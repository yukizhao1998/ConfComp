Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocked),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Completes),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Outward issue link (Incorporates),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Supercedes),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Fix AnalysisException messages at UNION/INTERSECT/EXCEPT/MINUS operations,SPARK-32131,13314100,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,小郭飞飞刀,小郭飞飞刀,小郭飞飞刀,29/Jun/20 16:45,01/Jul/20 06:40,13/Jul/23 08:46,01/Jul/20 06:40,2.1.3,2.2.3,2.3.4,2.4.6,3.0.0,,,,,,,,2.4.7,3.0.1,3.1.0,SQL,,,,0,,,"Union and set operations can only be performed on tables with the compatible column types,while when we have more than two column, the warning messages will have wrong column index.Steps to reproduce.

Step1:prepare test data
{code:java}
drop table if exists test1; 
drop table if exists test2; 
drop table if exists test3;
create table if not exists test1(id int, age int, name timestamp);
create table if not exists test2(id int, age timestamp, name timestamp);
create table if not exists test3(id int, age int, name int);
insert into test1 select 1,2,'2020-01-01 01:01:01';
insert into test2 select 1,'2020-01-01 01:01:01','2020-01-01 01:01:01'; 
insert into test3 select 1,3,4;
{code}
Step2:do query:
{code:java}
Query1:
select * from test1 except select * from test2;
Result1:
Error: org.apache.spark.sql.AnalysisException: Except can only be performed on tables with the compatible column types. timestamp <> int at the second column of the second table;; 'Except false :- Project [id#620, age#621, name#622] : +- SubqueryAlias `default`.`test1` : +- HiveTableRelation `default`.`test1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#620, age#621, name#622] +- Project [id#623, age#624, name#625] +- SubqueryAlias `default`.`test2` +- HiveTableRelation `default`.`test2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#623, age#624, name#625] (state=,code=0)
Query2:
select * from test1 except select * from test3;
Result2:
Error: org.apache.spark.sql.AnalysisException: Except can only be performed on tables with the compatible column types. int <> timestamp at the 2th column of the second table;; 'Except false :- Project [id#632, age#633, name#634] : +- SubqueryAlias `default`.`test1` : +- HiveTableRelation `default`.`test1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#632, age#633, name#634] +- Project [id#635, age#636, name#637] +- SubqueryAlias `default`.`test3` +- HiveTableRelation `default`.`test3`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#635, age#636, name#637] (state=,code=0)
{code}
the result of query1 is correct, while query2 have the wrong errors,it should be the third column

Here has the wrong column index.

+Error: org.apache.spark.sql.AnalysisException: Except can only be performed on tables with the compatible column types. int <> timestamp at the *2th* column of the second table+

We may need to change to the following

+Error: org.apache.spark.sql.AnalysisException: Except can only be performed on tables with the compatible column types. int <> timestamp at the *third* column of the second table+",,apachespark,dongjoon,小郭飞飞刀,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 01 06:40:51 UTC 2020,,,,,,,,,,"0|z0gazk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/20 16:54;apachespark;User 'GuoPhilipse' has created a pull request for this issue:
https://github.com/apache/spark/pull/28951;;;","29/Jun/20 16:54;apachespark;User 'GuoPhilipse' has created a pull request for this issue:
https://github.com/apache/spark/pull/28951;;;","01/Jul/20 05:08;dongjoon;I also verified that this bug exists at 2.1.3 ~ 2.3.7 and updated the affected versions.;;;","01/Jul/20 06:40;dongjoon;Issue resolved by pull request 28951
[https://github.com/apache/spark/pull/28951];;;",,,,,,,,,,,,,,,,,,,,
Spark 3.0 json load performance is unacceptable in comparison of Spark 2.4,SPARK-32130,13314087,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,maxgekk,lotus2you,lotus2you,29/Jun/20 15:53,12/Dec/22 18:10,13/Jul/23 08:46,01/Jul/20 22:47,3.0.0,,,,,,,,,,,,3.0.1,3.1.0,,Input/Output,,,,0,,,"We are planning to move to Spark 3 but the read performance of our json files is unacceptable. Following is the performance numbers when compared to Spark 2.4

 
 Spark 2.4

scala> spark.time(spark.read.json(""/data/20200528""))
 Time taken: {color:#ff0000}19691 ms{color}
 res61: org.apache.spark.sql.DataFrame = [created: bigint, id: string ... 5 more fields]
 scala> spark.time(res61.count())
 Time taken: {color:#0000ff}7113 ms{color}
 res64: Long = 2605349
 Spark 3.0
 scala> spark.time(spark.read.json(""/data/20200528""))
 20/06/29 08:06:53 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
 Time taken: {color:#ff0000}849652 ms{color}
 res0: org.apache.spark.sql.DataFrame = [created: bigint, id: string ... 5 more fields]
 scala> spark.time(res0.count())
 Time taken: {color:#0000ff}8201 ms{color}
 res2: Long = 2605349
  
  
 I am attaching a sample data (please delete is once you are able to reproduce the issue) that is much smaller than the actual size but the performance comparison can still be verified.

The sample tar contains bunch of json.gz files, each line of the file is self contained json doc as shown below

To reproduce the issue please untar the attachment - it will have multiple .json.gz files whose contents will look similar to following

 
{quote}{color:#0000ff}{""id"":""954e7819e91a11e981f60050569979b6"",""created"":1570463599492,""properties"":\{""WANAccessType"":""2"",""deviceClassifiers"":[""ARRIS HNC IGD"",""Annex F Gateway"",""Supports.Collect.Optimized.Workflow"",""Fast.Inform"",""Supports.TR98.Traceroute"",""InternetGatewayDevice:1.4"",""Motorola.ServiceType.IP"",""Supports Arris FastPath Speed Test"",""Arris.NVG468MQ.9.3.0h0"",""Wireless.Common.IGD.DualRadio"",""001E46.NVG468MQ.Is.WANIP"",""Device.Supports.HNC"",""Device.Type.RG"",""[Arris.NVG4xx.Missing.CA|http://arris.nvg4xx.missing.ca/]"",""Supports.TR98.IPPing"",""Arris.NVG468MQ.9.3.0+"",""Wireless"",""ARRIS HNC IGD EUROPA"",""Arris.NVG.Wireless"",""WLAN.Radios.Action.Common.TR098"",""VoiceService:1.0"",""ConnecticutDeviceTypes"",""Device.Supports.SpeedTest"",""Motorola.Device.Supports.VoIP"",""Arris.NVG468MQ"",""Motorola.device"",""CaptivePortal:1"",""Arris.NVG4xx"",""All.TR069.RG.Devices"",""TraceRoute:1"",""Arris.NVG4xx.9.3.0+"",""datamodel.igd"",""Arris.NVG4xxQ"",""IPPing:1"",""Device.ServiceType.IP"",""001E46.NVG468MQ.Is.WANEth"",""Arris.NVG468MQ.9.2.4+"",""broken.device.no.notification""],""deviceType"":""IGD"",""firstInform"":""1570463619543"",""groups"":[""Self-Service Diagnostics"",""SLF-SRVC_DGNSTCS000"",""TCW - NVG4xx - First Contact""],""hardwareVersion"":""NVG468MQ_0200240031004E"",""hncEnable"":""0"",""lastBoot"":""1587765844155"",""lastInform"":""1590624062260"",""lastPeriodic"":""1590624062260"",""manufacturerName"":""Motorola"",""modelName"":""NVG468MQ"",""productClass"":""NVG468MQ"",""protocolVersion"":""cwmp10"",""provisioningCode"":"""",""softwareVersion"":""9.3.0h0d55"",""tags"":[""default""],""timeZone"":""EST+5EDT,M3.2.0/2,M11.1.0/2"",""wan"":\{""ethDuplexMode"":""Full"",""ethSyncBitRate"":""1000""},""wifi"":[\\{""0"":{""Enable"":""1"",""SSID"":""Frontier3136"",""SSIDAdvertisementEnabled"":""1""},""1"":\\{""Enable"":""0"",""SSID"":""Guest3136"",""SSIDAdvertisementEnabled"":""1""},""2"":\\{""Enable"":""0"",""SSID"":""Frontier3136_D2"",""SSIDAdvertisementEnabled"":""1""},""3"":\\{""Enable"":""0"",""SSID"":""Frontier3136_D3"",""SSIDAdvertisementEnabled"":""1""},""4"":\\{""Enable"":""1"",""SSID"":""Frontier3136_5G"",""SSIDAdvertisementEnabled"":""1""},""5"":\\{""Enable"":""0"",""SSID"":""Guest3136_5G"",""SSIDAdvertisementEnabled"":""1""},""6"":\\{""Enable"":""1"",""SSID"":""Frontier3136_5G-TV"",""SSIDAdvertisementEnabled"":""0""},""7"":\\{""Enable"":""0"",""SSID"":""Frontier3136_5G_D2"",""SSIDAdvertisementEnabled"":""1""}}]},""ts"":1590624062260}{color}
{quote}
{quote}{color:#741b47}{""id"":""bf0448736d09e2e677ea383ef857d5bc"",""created"":1517843609967,""properties"":\{""WANAccessType"":""2"",""arrisNvgDbCheck"":""1:success"",""deviceClassifiers"":[""ARRIS HNC IGD"",""Annex F Gateway"",""Supports.Collect.Optimized.Workflow"",""Fast.Inform"",""InternetGatewayDevice:1.4"",""Supports.TR98.Traceroute"",""Supports Arris FastPath Speed Test"",""Motorola.ServiceType.IP"",""Arris.NVG468MQ.9.3.0h0"",""Wireless.Common.IGD.DualRadio"",""001E46.NVG468MQ.Is.WANIP"",""Device.Supports.HNC"",""Device.Type.RG"",""[Arris.NVG4xx.Missing.CA|http://arris.nvg4xx.missing.ca/]"",""Supports.TR98.IPPing"",""Arris.NVG468MQ.9.3.0+"",""Wireless"",""ARRIS HNC IGD EUROPA"",""Arris.NVG.Wireless"",""VoiceService:1.0"",""WLAN.Radios.Action.Common.TR098"",""ConnecticutDeviceTypes"",""Device.Supports.SpeedTest"",""Motorola.Device.Supports.VoIP"",""Arris.NVG468MQ"",""Motorola.device"",""CaptivePortal:1"",""Arris.NVG4xx"",""All.TR069.RG.Devices"",""TraceRoute:1"",""Arris.NVG4xx.9.3.0+"",""datamodel.igd"",""Arris.NVG4xxQ"",""IPPing:1"",""Device.ServiceType.IP"",""001E46.NVG468MQ.Is.WANEth"",""Arris.NVG468MQ.9.2.4+"",""broken.device.no.notification""],""deviceType"":""IGD"",""firstInform"":""1517843629132"",""groups"":[""Total Control"",""GPON_100M_100M"",""Self-Service Diagnostics"",""HSI"",""SLF-SRVC_DGNSTCS000"",""HS002"",""TTL_CNTRL000"",""GPN_100M_100M001""],""hardwareVersion"":""NVG468MQ_0200240031004E"",""hncEnable"":""0"",""lastBoot"":""1590196375084"",""lastInform"":""1590624060253"",""lastPeriodic"":""1590624060253"",""manufacturerName"":""Motorola"",""modelName"":""NVG468MQ"",""productClass"":""NVG468MQ"",""protocolVersion"":""cwmp10"",""provisioningCode"":"""",""softwareVersion"":""9.3.0h0d55"",""tags"":[""default""],""timeZone"":""EST+5EDT,M3.2.0/2,M11.1.0/2"",""wan"":\{""ethDuplexMode"":""Full"",""ethSyncBitRate"":""1000""},""wifi"":[\\{""0"":{""Enable"":""1"",""SSID"":""NE-TB12-GOAT-2G"",""SSIDAdvertisementEnabled"":""1""},""1"":\\{""Enable"":""1"",""SSID"":""TP-Link_extender_2.4GHz"",""SSIDAdvertisementEnabled"":""1""},""2"":\\{""Enable"":""0"",""SSID"":""Frontier5360_D2"",""SSIDAdvertisementEnabled"":""1""},""3"":\\{""Enable"":""0"",""SSID"":""Frontier5360_D3"",""SSIDAdvertisementEnabled"":""1""},""4"":\\{""Enable"":""1"",""SSID"":""NE-TB12-GOAT-5G"",""SSIDAdvertisementEnabled"":""1""},""5"":\\{""Enable"":""0"",""SSID"":""Guest5360_5G"",""SSIDAdvertisementEnabled"":""1""},""6"":\\{""Enable"":""1"",""SSID"":""Frontier5360_5G-TV"",""SSIDAdvertisementEnabled"":""0""},""7"":\\{""Enable"":""0"",""SSID"":""Frontier5360_5G_D2"",""SSIDAdvertisementEnabled"":""1""}}]},""ts"":1590624060253}{color}
{quote}
{quote}{color:#0000ff}{""id"":""1512b1b8526211e9acf100505699063c"",""created"":1553891682535,""properties"":\{""WANAccessType"":""2"",""arrisNvgDbCheck"":""1:success"",""deviceClassifiers"":[""ARRIS HNC IGD"",""Annex F Gateway"",""Supports.Collect.Optimized.Workflow"",""Fast.Inform"",""InternetGatewayDevice:1.4"",""Supports.TR98.Traceroute"",""Motorola.ServiceType.IP"",""Supports Arris FastPath Speed Test"",""Arris.NVG468MQ.9.3.0h0"",""Wireless.Common.IGD.DualRadio"",""001E46.NVG468MQ.Is.WANIP"",""Device.Supports.HNC"",""[Arris.NVG4xx.Missing.CA|http://arris.nvg4xx.missing.ca/]"",""Device.Type.RG"",""Supports.TR98.IPPing"",""Arris.NVG468MQ.9.3.0+"",""Wireless"",""ARRIS HNC IGD EUROPA"",""Arris.NVG.Wireless"",""WLAN.Radios.Action.Common.TR098"",""VoiceService:1.0"",""ConnecticutDeviceTypes"",""Device.Supports.SpeedTest"",""Motorola.Device.Supports.VoIP"",""Arris.NVG468MQ"",""Motorola.device"",""Arris.NVG4xx"",""CaptivePortal:1"",""All.TR069.RG.Devices"",""TraceRoute:1"",""Arris.NVG4xx.9.3.0+"",""datamodel.igd"",""Arris.NVG4xxQ"",""IPPing:1"",""Device.ServiceType.IP"",""001E46.NVG468MQ.Is.WANEth"",""Arris.NVG468MQ.9.2.4+"",""broken.device.no.notification""],""deviceType"":""IGD"",""firstInform"":""1553891708717"",""groups"":[""Total Control"",""HSI"",""Self-Service Diagnostics"",""SLF-SRVC_DGNSTCS000"",""HS004"",""TTL_CNTRL000"",""TCW - NVG4xx - First Contact"",""GPON_200M_200M"",""TCW Enabled"",""GPN_200M_200M000""],""hardwareVersion"":""NVG468MQ_0200240031004E"",""hncEnable"":""1"",""lastBoot"":""1590537703734"",""lastInform"":""1590624061415"",""lastPeriodic"":""1590624061415"",""manufacturerName"":""Motorola"",""modelName"":""NVG468MQ"",""productClass"":""NVG468MQ"",""protocolVersion"":""cwmp10"",""provisioningCode"":"""",""softwareVersion"":""9.3.0h0d55"",""tags"":[""default""],""timeZone"":""EST+5EDT,M3.2.0/2,M11.1.0/2"",""wan"":\{""ethDuplexMode"":""Full"",""ethSyncBitRate"":""1000""},""wifi"":[\\{""0"":{""Enable"":""1"",""SSID"":""Frontier7968"",""SSIDAdvertisementEnabled"":""1""},""1"":\\{""Enable"":""0"",""SSID"":""Guest7968"",""SSIDAdvertisementEnabled"":""1""},""2"":\\{""Enable"":""0"",""SSID"":""Frontier7968_D2"",""SSIDAdvertisementEnabled"":""1""},""3"":\\{""Enable"":""0"",""SSID"":""Frontier7968_D3"",""SSIDAdvertisementEnabled"":""1""},""4"":\\{""Enable"":""1"",""SSID"":""Frontier7968"",""SSIDAdvertisementEnabled"":""1""},""5"":\\{""Enable"":""0"",""SSID"":""Guest7968_5G"",""SSIDAdvertisementEnabled"":""1""},""6"":\\{""Enable"":""1"",""SSID"":""Frontier7968_5G-TV"",""SSIDAdvertisementEnabled"":""0""},""7"":\\{""Enable"":""0"",""SSID"":""Frontier7968_5G_D2"",""SSIDAdvertisementEnabled"":""1""}}]},""ts"":1590624061415}{color}
{quote}","20/06/29 07:52:19 WARN Utils: Your hostname, sanjeevs-MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 10.0.0.8 instead (on interface en0)
20/06/29 07:52:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
20/06/29 07:52:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
20/06/29 07:52:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Spark context Web UI available at http://10.0.0.8:4041
Spark context available as 'sc' (master = local[*], app id = local-1593442346864).
Spark session available as 'spark'.
Welcome to
 ____ __
 / __/__ ___ _____/ /__
 _\ \/ _ \/ _ `/ __/ '_/
 /___/ .__/\_,_/_/ /_/\_\ version 3.0.0
 /_/

Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_251)
Type in expressions to have them evaluated.
Type :help for more information.",apachespark,Chen Zhang,cloud_fan,dongjoon,gourav.sengupta,JinxinTang,kabhwan,lotus2you,maxgekk,medb,Samwel,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26246,,,,,,,,,,,"30/Jun/20 09:01;gourav.sengupta;SPARK 32130 - replication and findings.ipynb;https://issues.apache.org/jira/secure/attachment/13006732/SPARK+32130+-+replication+and+findings.ipynb","29/Jun/20 15:55;lotus2you;small-anon.tar;https://issues.apache.org/jira/secure/attachment/13006676/small-anon.tar",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 02 20:23:28 UTC 2020,,,,,,,,,,"0|z0gawo:",9223372036854775807,,,,,,,,,,,,,3.0.1,,,,,,,,,,"30/Jun/20 07:38;JinxinTang;[~lotus2you] Thank you for your feedback.

This is mainly due to lots of time is used to infer `Timestamp` json datatype becuse of  exception happens.

Please try use `.option(""inferTimestamp"", false)` as `spark.read.option(""inferTimestamp"", false).json(""..."")` or specify json schema.

cc [~maxgekk] FYI. https://issues.apache.org/jira/browse/SPARK-26246

 ;;;","30/Jun/20 08:16;kabhwan;I've feeling that ""opt-in"" approach is correct as it brings performance hit by default. Might be better to disable by default in Spark 3.0.1?;;;","30/Jun/20 09:01;gourav.sengupta;[~JinxinTang] and [~lotus2you]

I think that it is only the first time that the time taken by SPARK 3.x is more, and in the subsequent times, its less, as the schema would have already been determined. Setting the inferTimestamp to be False does not improve the performance as well.[^SPARK 32130 - replication and findings.ipynb][^SPARK 32130 - replication and findings.ipynb]

Note book with details and full reproducibility is attached. 

[~kabhwan] it might be of use to consult the entire community by writing to the user group if we are changing the default behaviour. Because it might affect a lot of production code.;;;","30/Jun/20 10:18;JinxinTang;[~gourav.sengupta] Nice notebook, is seems the row count() is 33447 instead of 11. it can be explained because 3s different is due to jvm warm up if record count is 11 instead of 33447.;;;","30/Jun/20 11:12;kabhwan;For me it's reproduced consistently. Please make sure you run the query in spark-shell with default setting of Spark 2.4.5 vs Spark 3.0.0. To remove any variant, don't add ""multiLine"" option.

spark.time(spark.read.json(""datadir"").count())

Spark 2.4.5: 3215ms
Spark 3.0.0: 13317ms

adding `.option(""inferTimestamp"", ""false"")` before .json leads the elapsed time closer - 4716 ms - still have a difference.;;;","30/Jun/20 11:28;kabhwan;So anyone can just reproduce via running spark-shell on Spark 3.0.0 and run the query, and shut down, and rerun spark-shell, and run another query.

{code}
spark.time(spark.read.option(""inferTimestamp"", ""false"").json(""datadir"").count())
{code}

{code}
spark.time(spark.read.option(""inferTimestamp"", ""true"").json(""datadir"").count())
{code}

{quote}
it might be of use to consult the entire community by writing to the user group if we are changing the default behaviour. Because it might affect a lot of production code.
{quote}

The default behavior was changed in Spark 3.0.0 - I agree it's bad user experience to make it back and forth, but if it contributes the performance silently, IMHO the option should be ideally turned off by default so that end users will turn on by their own intention.;;;","30/Jun/20 11:45;kabhwan;Looks like we already saw the difference but we missed to consider the performance regression. 

Please look at the table in PR description of https://github.com/apache/spark/pull/23653.

- inferTimestamp=default (true) & prefersDecimal=default (false) took 6.1 minutes
- inferTimestamp=false & prefersDecimal=default (false) took 49 seconds

The difference simply came from inferTimestamp and it was noticeable.;;;","30/Jun/20 13:18;lotus2you;I tried to load entire dataset using above suggestions and time did come close but Spark 2.4 still beats Spark 3 by ~3+ sec (may be in acceptable range but question is why?). I would definitely agree above comment that if the default behavior is changed the community MUST be notified otherwise it is a huge time waste for all. Here are the various results

*Spark 2.4* 

spark.time(spark.read.{color:#0747a6}option(""inferTimestamp"",""false""){color}.json(""/data/20200528/"").count)
 Time taken: {color:#ff0000}29706 ms{color}
 res0: Long = 2605349

 

spark.time(spark.read.{color:#0747a6}option(""inferTimestamp"",""false"").option(""prefersDecimal"",""false""{color}).json(""/data/20200528/"").count)
 Time taken: {color:#de350b}31431 ms{color}
 res0: Long = 2605349

 

*Spark 3.0*

spark.time(spark.read.{color:#0747a6}option(""inferTimestamp"",""false""{color}).json(""/data/20200528/"").count)
 Time taken: {color:#de350b}32826 ms{color}
 res0: Long = 2605349

 

spark.time(spark.read.{color:#0747a6}option(""inferTimestamp"",""false"").option(""prefersDecimal"",""false""{color}).json(""/data/20200528/"").count)
 Time taken: {color:#de350b}34011 ms{color}
 res0: Long = 2605349

 

{color:#de350b}*Note:*{color}
 # Make sure {color:#de350b}you never turn on prefersDecimal to true{color} even when inferTimestamp is false, it again takes huge amount of time.
 # Spark 3.0 + JDK 11 is slower than Spark 3.0 + JDK 8 by almost 6 sec.

 

 ;;;","30/Jun/20 20:12;srowen;So, is the issue that it's trying and failing to parse millions of things as timestamps, as part of schema inference, and almost all of that is wasted because they're not timestamps? is it possible to make the type inference for timestamps fail way faster on inputs that are obviously not dates (e.g. don't start with a number?) ;;;","01/Jul/20 00:17;kabhwan;There might be some tricks to make type inference for timestamps fail faster, but that would depend on the value and will be same (even worse for adding overhead on applying tricks) for worst case. Unless the performance becomes on par or similar on every cases, turning it on by default doesn't seem to be ideal.

[~dongjoon] [~cloud_fan] [~hyukjin.kwon] [~maxgekk] Could we please revisit this?;;;","01/Jul/20 09:01;cloud_fan;Even in Spark 2.4, the type inference takes much more time than the actual execution. It's always recommended to specify the schema manually to skip schema inference in production workloads.

Speaking of type inference, in general it's better to infer the type more accurately. But given the perf regression is huge, I think we can disable this feature by default. We can still investigate the fail-fast approach. cc [~maxgekk];;;","01/Jul/20 09:21;maxgekk;I would like to propose:
 # Add the SQL config spark.sql.legacy.json.inferTimestamps.enabled with false by default. The will control timestamp inference in JSON globally.
 # Keep inferTimestamps as an JSON option (because it has been already released) and set it to false by default.
 # If the JSON option inferTimestamps is set to any value, it overrides the SQL config.

If you are ok with the changes, I will open an PR for master and branch-3.0. cc [~Samwel] [~hyukjin.kwon];;;","01/Jul/20 09:36;cloud_fan;I'm not sure about 1. It's good to have but not necessary to fix this perf regression. You can open a PR against master branch to add it as a new feature though.;;;","01/Jul/20 10:01;Samwel;+1 to what [~cloud_fan] said. We should just keep the default hardcoded to ""false"" for 3.0, and then add the config to 3.1. We won't be able to flip it any time soon anyway, so there's no rush to add it.;;;","01/Jul/20 11:16;gurwls223;Yeah, we can disable it back by default considering the performance diff is huge.;;;","01/Jul/20 11:38;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28966;;;","01/Jul/20 11:38;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28966;;;","01/Jul/20 22:47;dongjoon;Issue resolved by pull request 28966
[https://github.com/apache/spark/pull/28966];;;","02/Jul/20 20:22;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28981;;;","02/Jul/20 20:23;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28981;;;",,,,
Scope Session.active in IncrementalExecution,SPARK-32126,13313915,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,dongjoon,dongjoon,29/Jun/20 04:34,29/Jun/20 04:38,13/Jul/23 08:46,29/Jun/20 04:37,3.0.0,,,,,,,,,,,,3.0.1,3.1.0,,Structured Streaming,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30798,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 29 04:37:14 UTC 2020,,,,,,,,,,"0|z0g9ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/20 04:35;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/28936;;;","29/Jun/20 04:37;dongjoon;Issue resolved by pull request 28936
[https://github.com/apache/spark/pull/28936];;;",,,,,,,,,,,,,,,,,,,,,,
[SHS] Failed to parse FetchFailed TaskEndReason from event log produce by Spark 2.4 ,SPARK-32124,13313890,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,warrenzhu25,warrenzhu25,warrenzhu25,28/Jun/20 22:46,01/Jul/20 08:22,13/Jul/23 08:46,29/Jun/20 04:07,3.0.0,,,,,,,,,,,,3.0.1,3.1.0,,Spark Core,,,,0,,,"When read event log produced by Spark 2.4.4, parsing TaskEndReason failed to due to missing field ""Map Index"".

 ",,apachespark,dongjoon,warrenzhu25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25341,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 01 08:22:58 UTC 2020,,,,,,,,,,"0|z0g9ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/20 22:48;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/28941;;;","29/Jun/20 04:07;dongjoon;Issue resolved by pull request 28941
[https://github.com/apache/spark/pull/28941];;;","01/Jul/20 08:22;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/28965;;;",,,,,,,,,,,,,,,,,,,,,
ExternalShuffleBlockResolverSuite failed on Windows,SPARK-32121,13313882,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,chengpan,chengpan,,28/Jun/20 19:27,12/Dec/22 18:11,13/Jul/23 08:46,02/Jul/20 10:23,3.0.0,3.0.1,,,,,,,,,,,3.0.1,3.1.0,,Tests,,,,0,,,"The method {code}ExecutorDiskUtils.createNormalizedInternedPathname{code} should consider the Windows file separator.
{code}
[ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.132 s <<< FAILURE! - in org.apache.spark.network.shuffle.ExternalShuffleBlockResolverSuite
[ERROR] testNormalizeAndInternPathname(org.apache.spark.network.shuffle.ExternalShuffleBlockResolverSuite)  Time elapsed: 0 s  <<< FAILURE!
org.junit.ComparisonFailure: expected:</foo[/bar/]baz> but was:</foo[\bar\]baz>
        at org.apache.spark.network.shuffle.ExternalShuffleBlockResolverSuite.assertPathsMatch(ExternalShuffleBlockResolverSuite.java:160)
        at org.apache.spark.network.shuffle.ExternalShuffleBlockResolverSuite.testNormalizeAndInternPathname(ExternalShuffleBlockResolverSuite.java:149)
{code}",Windows 10,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32149,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 02 10:23:18 UTC 2020,,,,,,,,,,"0|z0g9n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/20 19:40;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/28940;;;","01/Jul/20 15:09;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/28967;;;","02/Jul/20 10:23;gurwls223;Issue resolved by pull request 28940
[https://github.com/apache/spark/pull/28940];;;",,,,,,,,,,,,,,,,,,,,,
Incorrect results for SUBSTRING when overflow,SPARK-32115,13313817,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,XuanYuan,XuanYuan,XuanYuan,28/Jun/20 04:28,30/Jun/20 08:06,13/Jul/23 08:46,28/Jun/20 19:30,1.6.3,2.0.2,2.1.3,2.2.3,2.3.4,2.4.6,3.0.0,,,,,,2.4.7,3.0.1,3.1.0,SQL,,,,0,correctness,,"SQL query SELECT SUBSTRING(""abc"", -1207959552, -1207959552) incorrectly returns ""abc"" against expected output of """".
 This is a result of integer overflow in addition [https://github.com/apache/spark/blob/8c44d744631516a5cdaf63406e69a9dd11e5b878/common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java#L345]",,apachespark,dongjoon,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 08:06:58 UTC 2020,,,,,,,,,,"0|z0g98o:",9223372036854775807,,,,,,,,,,,,,2.4.7,3.0.1,,,,,,,,,"28/Jun/20 04:35;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/28937;;;","28/Jun/20 18:48;dongjoon;Thank you, @Yuanjian Li .;;;","28/Jun/20 18:54;dongjoon;I also verified that this is a long standing bug at 1.6.3 ~ 3.0.0 and Apache Hive 2.3.7 has no problem.
{code}
hive> SELECT SUBSTRING(""abc"", -1207959552, -1207959552);
OK
Time taken: 4.291 seconds, Fetched: 1 row(s)
{code};;;","28/Jun/20 18:56;dongjoon;Although this might be a rare case, but I raise this issue as a Blocker issue because this is a correctness issue .;;;","28/Jun/20 19:30;dongjoon;Issue resolved by pull request 28937
[https://github.com/apache/spark/pull/28937];;;","30/Jun/20 08:06;XuanYuan;Thank you for verifying! [~dongjoon];;;",,,,,,,,,,,,,,,,,,
-0.0 vs 0.0 is inconsistent,SPARK-32110,13313682,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,revans2,revans2,26/Jun/20 20:02,10/Dec/20 16:56,13/Jul/23 08:46,08/Dec/20 19:48,3.0.0,,,,,,,,,,,,3.0.2,3.1.0,,SQL,,,,0,,,"This is related to SPARK-26021 where some things were fixed but there is still a lot that is not consistent.

When parsing SQL {{-0.0}} is turned into {{0.0}}. This can produce quick results that appear to be correct but are totally inconsistent for the same operators.
{code:java}
scala> import spark.implicits._
import spark.implicits._

scala> spark.sql(""SELECT 0.0 = -0.0"").collect
res0: Array[org.apache.spark.sql.Row] = Array([true])

scala> Seq((0.0, -0.0)).toDF(""a"", ""b"").selectExpr(""a = b"").collect
res1: Array[org.apache.spark.sql.Row] = Array([false])
{code}
This also shows up in sorts
{code:java}
scala> Seq((0.0, -100.0), (-0.0, 100.0), (0.0, 100.0), (-0.0, -100.0)).toDF(""a"", ""b"").orderBy(""a"", ""b"").collect
res2: Array[org.apache.spark.sql.Row] = Array([-0.0,-100.0], [-0.0,100.0], [0.0,-100.0], [0.0,100.0])
{code}
But not for a equi-join or for an aggregate
{code:java}
scala> Seq((0.0, -0.0)).toDF(""a"", ""b"").join(Seq((-0.0, 0.0)).toDF(""r_a"", ""r_b""), $""a"" === $""r_a"").collect
res3: Array[org.apache.spark.sql.Row] = Array([0.0,-0.0,-0.0,0.0])

scala> Seq((0.0, 1.0), (-0.0, 1.0)).toDF(""a"", ""b"").groupBy(""a"").count.collect
res6: Array[org.apache.spark.sql.Row] = Array([0.0,2])
{code}
This can lead to some very odd results. Like an equi-join with a filter that logically should do nothing, but ends up filtering the result to nothing.
{code:java}
scala> Seq((0.0, -0.0)).toDF(""a"", ""b"").join(Seq((-0.0, 0.0)).toDF(""r_a"", ""r_b""), $""a"" === $""r_a"" && $""a"" <= $""r_a"").collect
res8: Array[org.apache.spark.sql.Row] = Array()

scala> Seq((0.0, -0.0)).toDF(""a"", ""b"").join(Seq((-0.0, 0.0)).toDF(""r_a"", ""r_b""), $""a"" === $""r_a"").collect
res9: Array[org.apache.spark.sql.Row] = Array([0.0,-0.0,-0.0,0.0])
{code}
Hive never normalizes -0.0 to 0.0 so this results in non-ieee complaint behavior everywhere, but at least it is consistently odd.

MySQL, Oracle, Postgres, and SQLite all appear to normalize the {{-0.0}} to {{0.0}}.

The root cause of this appears to be that the java implementation of {{Double.compare}} and {{Float.compare}} for open JDK places {{-0.0}} < {{0.0}}.

This is not documented in the java docs but it is clearly documented in the code, so it is not a ""bug"" that java is going to fix.

[https://github.com/openjdk/jdk/blob/a0a0539b0d3f9b6809c9759e697bfafd7b138ec1/src/java.base/share/classes/java/lang/Double.java#L1022-L1035]

It is also consistent with what is in the java docs for {{Double.equals}}
 [https://docs.oracle.com/javase/8/docs/api/java/lang/Double.html#equals-java.lang.Object-]

To be clear I am filing this mostly to document the current state rather than to think it needs to be fixed ASAP. It is a rare corner case, but ended up being really frustrating for me to debug what was happening.",,abellina,apachespark,cloud_fan,dongjoon,jlowe,maropu,revans2,Samwel,tanelk,tgraves,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 10 16:56:26 UTC 2020,,,,,,,,,,"0|z0g8eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/20 09:39;tanelk;there is also inconsistency in the code gen and the interpreted code paths for EqualTo and EqualNullSafe
{code:java}
org.scalatest.exceptions.GeneratorDrivenPropertyCheckFailedException: TestFailedException was thrown during property evaluation.   Message: Incorrect evaluation: (-0.0 = 0.0), interpret: false, codegen: true   Location: (ExpressionEvalHelper.scala:454)   Occurred when passed generated values (     arg0 = -0.0,     arg1 = 0.0   )
{code};;;","24/Aug/20 14:25;cloud_fan;I'd like to discuss the expected behavior here.

AFAIK Spark follows the java semantic: -0.0 is equal to 0.0, but they are different values and people can distinguish them via toString. While most databases simply normalize both -0.0 and 0.0 to 0, so they are exactly the same value (I checked PostgreSQL, MySQL, Oracle, SQL Server).

It's apparent that Spark has a bug and simply compare the UnsafeRow/Array/Map binaries. -0.0 and 0.0 are different values and has different binary representation, but they semantically equal. We need to look into row/array/map and recursively compare the elements.

That said, to fix this bug, we can't keep the fast code path to compare unsafe structure binaries, and need to introduce perf regression. IMO it's better than doing a breaking change and normalize -0.0/0.0 everywhere to follow other databases, but I'm open to other opinions.

cc [~maropu] [~viirya]  [~Samwel]

 ;;;","24/Aug/20 15:55;tanelk;Another thing to note is that -0.0 and 0.0 can have different hash values.
One place I could think of, where it is (kinda) relevant is in the HyperLogLog++. This test would fail in the HyperLogLogPlusPlusSuite:
{code:java}
  test(""add 0.0 and -0.0"") {
    val (hll, input, buffer) = createEstimator(0.05, DoubleType)
    input.setDouble(0, 0.0)
    hll.update(buffer, input)
    input.setDouble(0, -0.0)
    hll.update(buffer, input)
    evaluateEstimate(hll, buffer, 1);
  }
{code}

HLL++ makes a promise, that the estimate is never off by more than some %, so for very small cardinalities it should be precise.
This seems like a very minor issue, because HLL++ is supposed to be just a estimate, but the different hash values could cause some issues elsewhere.

Perhaps the exact equality check could be a controlled by a configuration option for those who really need it, avoiding the performance regression for most?;;;","24/Aug/20 18:01;viirya;[~cloud_fan] Is the idea to not change writing path to UnsafeRow/Array/Map binaries, but when we read values from them to compare, we need to normalize 0.0, -0.0? It sounds like significant perf regression.;;;","25/Aug/20 03:48;cloud_fan;> Is the idea to not change writing path to UnsafeRow/Array/Map binaries

Yes, otherwise we can't distinguish between -0.0 and 0.0 anymore, and thus a breaking change.

For grouping key, join key, window partition key, we normalize -0.0 to 0.0, so that Spark can still look at the binaries directly in hash map/sort buffer/etc. (see rule `NormalizeFloatingNumbers`)

One problem is that we need to catch all the places that compare float/double values. Fixing `DoubleType.ordering` and `CodeGenerator.genEqual/genComp` should cover most cases, but we need to take care of corner cases like HyperLogLog++.;;;","25/Aug/20 05:38;viirya;For `HyperLogLogPlusPlus`, seems its problem is that we use value representation to hash. So I think we also need to modify `HashExpression`?;;;","25/Aug/20 14:46;maropu;We cannot provide a boolean configuration (turning it off by default to avoid the breaking change) for normalizing it in write path? If we set true to it, we enable write path normalization then disable read path normalization (e.g., `NormalizeFloatingNumbers`). IMHO the write path normalization looks straightforward (and lower overhead?), and it might be still meaningful for users who want to use Spark like a database system.
;;;","12/Nov/20 11:01;Samwel;I think the path we went on so far is to normalize in any path that groups together values. HLL++ is in that category, so we should probably just normalize there as well. It's unfortunate that this is a bit like swatting flies, but the alternative is to have a flag (more complexity, more combinations to test which will probably never actually be tested) or to normalize always and lose the distinction even when reading data and writing it back verbatim (as a matter of principle we shouldn't be changing data when we don't transform it).;;;","08/Dec/20 13:58;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/30673;;;","08/Dec/20 19:48;dongjoon;Issue resolved by pull request 30673
[https://github.com/apache/spark/pull/30673];;;","09/Dec/20 14:56;revans2;Are we sure that this issue should be closed?

The PR https://github.com/apache/spark/pull/30673 fixed the issue with HLL++

I don't think the issues with equals and null safe equals described [here|https://issues.apache.org/jira/browse/SPARK-32110?focusedCommentId=17183107&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17183107] were fixed, but I am happy to be wrong.

Also nothing was done to address the odd behavior I called out initially (I was hoping to at least have it documented somewhere so users can know what the expected operation is, instead of being surprised);;;","09/Dec/20 18:04;dongjoon;How do you think about the above [~revans2]'s comment, [~cloud_fan]?;;;","10/Dec/20 13:50;cloud_fan;[~revans2] have you tried the latest master branch? I can't reproduce these issues anymore.;;;","10/Dec/20 13:54;cloud_fan;Similar problems were reported by others, e.g. https://issues.apache.org/jira/browse/SPARK-32764 . I think at this point HLL++ is the only issue and is fixed.;;;","10/Dec/20 15:16;revans2;I have not tried this again on the latest master branch.  I will try to find time today to build/rerun and see if things have changed at all.;;;","10/Dec/20 16:52;tanelk;[~cloud_fan] fixed issue I mentioned in the first comment in this PR: https://github.com/apache/spark/pull/29647;;;","10/Dec/20 16:56;revans2;Thanks [~tanelk] then resolving this is fine.  I would have liked some official documentation of what the expected behavior is, but just having it here is OK.;;;",,,,,,,
Spark support IPV6 in yarn mode,SPARK-32103,13313559,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,pavithraramachandran,jobitmathew,jobitmathew,26/Jun/20 07:07,10/Jul/20 20:55,13/Jul/23 08:46,10/Jul/20 20:55,3.1.0,,,,,,,,,,,,3.1.0,,,YARN,,,,0,,,"Spark support IPV6 in yarn mode

 ",,aaruna,apachespark,dongjoon,jobitmathew,pavithraramachandran,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 10 20:55:43 UTC 2020,,,,,,,,,,"0|z0g7nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/20 07:10;pavithraramachandran;i shall raise a pr soon;;;","26/Jun/20 07:25;apachespark;User 'PavithraRamachandran' has created a pull request for this issue:
https://github.com/apache/spark/pull/28931;;;","26/Jun/20 07:26;apachespark;User 'PavithraRamachandran' has created a pull request for this issue:
https://github.com/apache/spark/pull/28931;;;","10/Jul/20 20:55;dongjoon;Issue resolved by pull request 28931
[https://github.com/apache/spark/pull/28931];;;",,,,,,,,,,,,,,,,,,,,
Use iloc for positional slicing instead of direct slicing in createDataFrame with Arrow,SPARK-32098,13313420,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gurwls223,gurwls223,,25/Jun/20 10:44,12/Dec/22 17:51,13/Jul/23 08:46,25/Jun/20 18:18,2.4.6,3.0.0,,,,,,,,,,,2.4.7,3.0.1,3.1.0,PySpark,,,,0,correctness,,"When you use floats are index of pandas, it produces a wrong results:

{code}
>>> import pandas as pd
>>> spark.createDataFrame(pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.])).show()
+---+
|  a|
+---+
|  1|
|  1|
|  2|
+---+
{code}

This is because direct slicing uses the value as index when the index contains floats:

{code}
>>> pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.])[2:]
     a
2.0  1
3.0  2
4.0  3
>>> pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.]).iloc[2:]
     a
4.0  3
>>> pd.DataFrame({'a': [1,2,3]}, index=[2, 3, 4])[2:]
   a
4  3
{code}",,apachespark,bryanc,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 06 06:14:33 UTC 2020,,,,,,,,,,"0|z0g6so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/20 11:17;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28928;;;","25/Jun/20 11:18;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28928;;;","25/Jun/20 18:18;bryanc;Issue resolved by pull request 28928
[https://github.com/apache/spark/pull/28928];;;","06/Jul/20 06:14;dongjoon;Hi, [~hyukjin.kwon] and [~bryanc]. I'm trying to verify this at 2.4.6 with the above example, but there was no luck until now. Did I miss something?
{code}
$ ./bin/pyspark --conf spark.sql.execution.arrow.pyspark.enabled=true
Python 3.7.7 (default, Mar 21 2020, 21:07:30)
[Clang 11.0.0 (clang-1100.0.33.16)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
20/07/05 23:13:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.6
      /_/

Using Python version 3.7.7 (default, Mar 21 2020 21:07:30)
SparkSession available as 'spark'.
>>> import pandas as pd
>>> spark.createDataFrame(pd.DataFrame({'a': [1,2,3]}, index=[2., 3., 4.])).show()
+---+
|  a|
+---+
|  1|
|  2|
|  3|
+---+
{code};;;",,,,,,,,,,,,,,,,,,,,
[DataSource V2] Documentation on SupportsReportStatistics Outdated?,SPARK-32095,13313317,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,emkornfield,emkornfield,emkornfield,24/Jun/20 19:31,01/Jul/20 07:15,13/Jul/23 08:46,01/Jul/20 07:13,2.4.6,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,SQL,,,,0,,,"I was wondering if the documentation on SupportsReportStatistics [1][3] about its interaction with the planner and predicate pushdowns is still accurate. It says:

""Implementations that return more accurate statistics based on pushed operators will not improve query performance until the planner can push operators before getting stats.""

 

Is this still accurate? When looking through the code it seems like there is now functionality that explicitly wants the operators pushed down [2]. Is the documentation for SupportsReportStatistics referring to something other than [2] or should it be updated?

 

[[1]https://spark.apache.org/docs/2.4.6/api/java/org/apache/spark/sql/sources/v2/reader/SupportsReportStatistics.html|https://spark.apache.org/docs/2.4.6/api/java/org/apache/spark/sql/sources/v2/reader/SupportsReportStatistics.html]

[2] [https://github.com/apache/spark/blob/d0800fc8e2e71a79bf0f72c3e4bc608ae34053e7/sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala#L86]

[3][https://spark.apache.org/docs/3.0.0-preview/api/java/org/apache/spark/sql/connector/read/SupportsReportStatistics.html]",,apachespark,cloud_fan,emkornfield,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 01 07:13:53 UTC 2020,,,,,,,,,,"0|z0g65s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/20 21:10;apachespark;User 'emkornfield' has created a pull request for this issue:
https://github.com/apache/spark/pull/28925;;;","01/Jul/20 07:13;cloud_fan;Issue resolved by pull request 28925
[https://github.com/apache/spark/pull/28925];;;",,,,,,,,,,,,,,,,,,,,,,
Patch cloudpickle.py with typing module side-effect fix,SPARK-32094,13313315,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,codesue,codesue,24/Jun/20 19:19,12/Dec/22 18:10,13/Jul/23 08:46,17/Jul/20 02:49,2.4.6,3.0.0,,,,,,,,,,,3.1.0,,,PySpark,,,,1,,,"Pyspark's cloudpickle.py and versions of cloudpickle below 1.3.0 interfere with dill unpickling because they define types.ClassType, which is undefined in dill. This results in the following error:
{code:java}
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/apache_beam/internal/pickler.py"", line 279, in loads
    return dill.loads(s)
  File ""/usr/local/lib/python3.6/site-packages/dill/_dill.py"", line 317, in loads
    return load(file, ignore)
  File ""/usr/local/lib/python3.6/site-packages/dill/_dill.py"", line 305, in load
    obj = pik.load()
  File ""/usr/local/lib/python3.6/site-packages/dill/_dill.py"", line 577, in _load_type
    return _reverse_typemap[name]
KeyError: 'ClassType'{code}
(See [https://github.com/cloudpipe/cloudpickle/issues/82])

This was fixed for cloudpickle 1.3.0+ ([https://github.com/cloudpipe/cloudpickle/pull/337]), but PySpark's cloudpickle.py doesn't have this change yet.

 ",,apachespark,codesue,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 17 02:49:54 UTC 2020,,,,,,,,,,"0|z0g65c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/20 02:01;gurwls223;Are you interested in submitting a PR to upgrade?;;;","26/Jun/20 02:05;codesue;Yes! Will do.;;;","29/Jun/20 16:41;apachespark;User 'codesue' has created a pull request for this issue:
https://github.com/apache/spark/pull/28950;;;","29/Jun/20 16:41;apachespark;User 'codesue' has created a pull request for this issue:
https://github.com/apache/spark/pull/28950;;;","15/Jul/20 01:18;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29114;;;","17/Jul/20 02:49;gurwls223;Issue resolved by pull request 29114
[https://github.com/apache/spark/pull/29114];;;",,,,,,,,,,,,,,,,,,
CrossvalidatorModel does not save all submodels (it saves only 3),SPARK-32092,13313277,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xzrspark,ADR,ADR,24/Jun/20 15:25,26/Aug/20 23:12,13/Jul/23 08:46,22/Aug/20 14:29,2.4.0,2.4.5,,,,,,,,,,,3.0.1,3.1.0,,ML,PySpark,,,0,,,"When saving a CrossValidatorModel with more than 3 subModels and loading again, a different amount of subModels is returned. It seems every time 3 subModels are returned.

With less than two submodels (so 2 folds) writing plainly fails.

Issue seems to be (but I am not so familiar with the scala/java side)
 * python object is converted to scala/java
 * in scala we save subModels until numFolds:

 
{code:java}
val subModelsPath = new Path(path, ""subModels"") 
       for (splitIndex <- 0 until instance.getNumFolds) {
          val splitPath = new Path(subModelsPath, s""fold${splitIndex.toString}"")
          for (paramIndex <- 0 until instance.getEstimatorParamMaps.length) {
            val modelPath = new Path(splitPath, paramIndex.toString).toString
            instance.subModels(splitIndex)(paramIndex).asInstanceOf[MLWritable].save(modelPath)
          }
{code}
 * numFolds is not available on the CrossValidatorModel in pyspark
 * default numFolds is 3 so somehow it tries to save 3 subModels.

The first issue can be reproduced by following failing tests, where spark is a SparkSession and tmp_path is a (temporary) directory.
{code:java}
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.linalg import Vectors


def test_save_load_cross_validator(spark, tmp_path):
    temp_path = str(tmp_path)
    dataset = spark.createDataFrame(
        [
            (Vectors.dense([0.0]), 0.0),
            (Vectors.dense([0.4]), 1.0),
            (Vectors.dense([0.5]), 0.0),
            (Vectors.dense([0.6]), 1.0),
            (Vectors.dense([1.0]), 1.0),
        ]
        * 10,
        [""features"", ""label""],
    )

    lr = LogisticRegression()

    grid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()

    evaluator = BinaryClassificationEvaluator()

    cv = CrossValidator(
        estimator=lr,
        estimatorParamMaps=grid,
        evaluator=evaluator,
        collectSubModels=True,
        numFolds=4,
    )

    cvModel = cv.fit(dataset)

    # test save/load of CrossValidatorModel

    cvModelPath = temp_path + ""/cvModel""

    cvModel.write().overwrite().save(cvModelPath)

    loadedModel = CrossValidatorModel.load(cvModelPath)
    assert len(loadedModel.subModels) == len(cvModel.subModels)
{code}
 
The second as follows (will fail writing):
{code:java}
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.linalg import Vectors


def test_save_load_cross_validator(spark, tmp_path):
    temp_path = str(tmp_path)
    dataset = spark.createDataFrame(
        [
            (Vectors.dense([0.0]), 0.0),
            (Vectors.dense([0.4]), 1.0),
            (Vectors.dense([0.5]), 0.0),
            (Vectors.dense([0.6]), 1.0),
            (Vectors.dense([1.0]), 1.0),
        ]
        * 10,
        [""features"", ""label""],
    )

    lr = LogisticRegression()

    grid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()

    evaluator = BinaryClassificationEvaluator()

    cv = CrossValidator(
        estimator=lr,
        estimatorParamMaps=grid,
        evaluator=evaluator,
        collectSubModels=True,
        numFolds=2,
    )

    cvModel = cv.fit(dataset)

    # test save/load of CrossValidatorModel

    cvModelPath = temp_path + ""/cvModel""

    cvModel.write().overwrite().save(cvModelPath)

    loadedModel = CrossValidatorModel.load(cvModelPath)
    assert len(loadedModel.subModels) == len(cvModel.subModels)
{code}
 
 ","Ran on two systems:
 * Local pyspark installation (Windows): spark 2.4.5
 * Spark 2.4.0 on a cluster",ADR,apachespark,viirya,xzrspark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 26 23:12:55 UTC 2020,,,,,,,,,,"0|z0g5ww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/20 23:30;xzrspark;I think CrossValidatorModel.copy() is affected by a similar issue of losing numFolds too. I will attempt a fix.;;;","16/Aug/20 14:07;apachespark;User 'Louiszr' has created a pull request for this issue:
https://github.com/apache/spark/pull/29445;;;","16/Aug/20 14:07;apachespark;User 'Louiszr' has created a pull request for this issue:
https://github.com/apache/spark/pull/29445;;;","22/Aug/20 14:29;srowen;Issue resolved by pull request 29445
[https://github.com/apache/spark/pull/29445];;;","23/Aug/20 17:44;apachespark;User 'Louiszr' has created a pull request for this issue:
https://github.com/apache/spark/pull/29524;;;","23/Aug/20 17:44;apachespark;User 'Louiszr' has created a pull request for this issue:
https://github.com/apache/spark/pull/29524;;;","26/Aug/20 23:12;apachespark;User 'Louiszr' has created a pull request for this issue:
https://github.com/apache/spark/pull/29553;;;","26/Aug/20 23:12;apachespark;User 'Louiszr' has created a pull request for this issue:
https://github.com/apache/spark/pull/29553;;;",,,,,,,,,,,,,,,,
test of pyspark.sql.functions.timestamp_seconds failed if non-american timezone setting,SPARK-32088,13313219,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,小郭飞飞刀,huangtianhua,huangtianhua,24/Jun/20 09:38,12/Dec/22 18:10,13/Jul/23 08:46,27/Jun/20 02:06,3.1.0,,,,,,,,,,,,3.1.0,,,PySpark,,,,0,,,"The python test failed for aarch64 job, see https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-python-arm/405/console since the commit https://github.com/apache/spark/commit/f0e6d0ec13d9cdadf341d1b976623345bcdb1028#diff-c8de34467c555857b92875bf78bf9d49 merged:
**********************************************************************
File ""/home/jenkins/workspace/spark-master-test-python-arm/python/pyspark/sql/functions.py"", line 1435, in pyspark.sql.functions.timestamp_seconds
Failed example:
    time_df.select(timestamp_seconds(time_df.unix_time).alias('ts')).collect()
Expected:
    [Row(ts=datetime.datetime(2008, 12, 25, 7, 30))]
Got:
    [Row(ts=datetime.datetime(2008, 12, 25, 23, 30))]
**********************************************************************
   1 of   3 in pyspark.sql.functions.timestamp_seconds
***Test Failed*** 1 failures.

But this is not arm64-related issue, I took test on x86 instance with timezone setting of UTC, then the test failed too, so I think the expected datetime is timezone American/**, but seems we have not set the timezone when doing these timezone sensitive python tests.",,apachespark,dongjoon,huangtianhua,v_ganeshraju,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 01 02:00:07 UTC 2020,,,,,,,,,,"0|z0g5k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/20 01:59;gurwls223;[~huangtianhua] are you interested in a PR? I think you can do e.g.)

{code}
    >>> from pyspark.sql.functions import timestamp_seconds
    >>> spark.conf.set(""spark.sql.session.timeZone"", ""America/Los_Angeles"")
    >>> time_df = spark.createDataFrame([(1230219000,)], ['unix_time'])
    >>> time_df.select(timestamp_seconds(time_df.unix_time).alias('ts')).collect()
    [Row(ts=datetime.datetime(2008, 12, 25, 7, 30))]
    >>> spark.conf.unset(""spark.sql.session.timeZone"")
{code}.;;;","26/Jun/20 09:34;apachespark;User 'GuoPhilipse' has created a pull request for this issue:
https://github.com/apache/spark/pull/28932;;;","27/Jun/20 02:06;dongjoon;Issue resolved by pull request 28932
[https://github.com/apache/spark/pull/28932];;;","30/Jun/20 19:06;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28959;;;","01/Jul/20 02:00;huangtianhua;the test success by the modification https://github.com/apache/spark/pull/28959, [~maxgekk], thanks very much:);;;",,,,,,,,,,,,,,,,,,,
PySpark <> Beam pickling issues for collections.namedtuple,SPARK-32079,13313142,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gcasassaez,gcasassaez,24/Jun/20 01:30,12/Dec/22 18:11,13/Jul/23 08:46,25/Nov/21 23:21,3.3.0,,,,,,,,,,,,3.3.0,,,PySpark,,,,0,release-notes,,"PySpark monkeypatching namedtuple makes it difficult/impossible to depickle collections.namedtuple instances from outside of a pyspark environment.

 

When PySpark has been loaded into the environment, any time that you try to pickle a namedtuple, you are only able to unpickle it from an environment where the [hijack|https://github.com/apache/spark/blob/master/python/pyspark/serializers.py#L385] has been applied. 

This conflicts directly when trying to use Beam from a non-Spark environment (namingly Flink or Dataflow) making it impossible to use the pipeline if it has a namedtuple loaded somewhere. 

 
{code:python}
import collections
import dill
ColumnInfo = collections.namedtuple(
    ""ColumnInfo"",
    [
        ""name"",  # type: ColumnName  # pytype: disable=ignored-type-comment
        ""type"",  # type: Optional[ColumnType]  # pytype: disable=ignored-type-comment
    ])
dill.dumps(ColumnInfo('test', int))
{code}

{{b'\x80\x03cdill._dill\n_create_namedtuple\nq\x00X\n\x00\x00\x00ColumnInfoq\x01X\x04\x00\x00\x00nameq\x02X\x04\x00\x00\x00typeq\x03\x86q\x04X\x08\x00\x00\x00__main__q\x05\x87q\x06Rq\x07X\x04\x00\x00\x00testq\x08cdill._dill\n_load_type\nq\tX\x03\x00\x00\x00intq\n\x85q\x0bRq\x0c\x86q\r\x81q\x0e.'}}
{code:python}
import pyspark
import collections
import dill
ColumnInfo = collections.namedtuple(
    ""ColumnInfo"",
    [
        ""name"",  # type: ColumnName  # pytype: disable=ignored-type-comment
        ""type"",  # type: Optional[ColumnType]  # pytype: disable=ignored-type-comment
    ])
dill.dumps(ColumnInfo('test', int))
{code}
{{b'\x80\x03cpyspark.serializers\n_restore\nq\x00X\n\x00\x00\x00ColumnInfoq\x01X\x04\x00\x00\x00nameq\x02X\x04\x00\x00\x00typeq\x03\x86q\x04X\x04\x00\x00\x00testq\x05cdill._dill\n_load_type\nq\x06X\x03\x00\x00\x00intq\x07\x85q\x08Rq\t\x86q\n\x87q\x0bRq\x0c.'}}


Second pickled object can only be used from an environment with PySpark. ",,apachespark,gcasassaez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22674,SPARK-27810,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 25 23:21:25 UTC 2021,,,,,,,,,,"0|z0g534:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/21 03:22;gurwls223;im working on this.;;;","23/Nov/21 05:43;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/34688;;;","23/Nov/21 05:44;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/34688;;;","25/Nov/21 23:21;gurwls223;Issue resolved by pull request 34688
[https://github.com/apache/spark/pull/34688];;;",,,,,,,,,,,,,,,,,,,,
Spark 3 UI task launch time show in error time zone,SPARK-32068,13312967,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,JinxinTang,d87904488,d87904488,23/Jun/20 05:56,12/Dec/22 18:10,13/Jul/23 08:46,30/Jun/20 13:59,3.0.0,,,,,,,,,,,,3.0.1,3.1.0,,Web UI,,,,0,easyfix,,"For example,

In this link: history/app-20200623133209-0015/stages/ , stage submit time is correct (CST)

 

But in this link: history/app-20200623133209-0015/stages/stage/?id=0&attempt=0 , task launch time is incorrect(UTC)

 

The same problem exists in port 4040 Web UI.",,apachespark,d87904488,JinxinTang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/20 07:02;d87904488;correct.png;https://issues.apache.org/jira/secure/attachment/13006316/correct.png","24/Jun/20 07:02;d87904488;incorrect.png;https://issues.apache.org/jira/secure/attachment/13006317/incorrect.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 07:59:15 UTC 2020,,,,,,,,,,"0|z0g408:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/20 06:24;gurwls223;[~d87904488] can you attach the screenshots?;;;","24/Jun/20 07:03;d87904488;OK, I have uploaded it.;;;","24/Jun/20 07:56;apachespark;User 'TJX2014' has created a pull request for this issue:
https://github.com/apache/spark/pull/28918;;;","24/Jun/20 07:59;JinxinTang;Thank you for the issue, hope my PR can help.;;;",,,,,,,,,,,,,,,,,,,,
Reset listenerRegistered in SparkSession,SPARK-32062,13312939,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ulysses,ulysses,ulysses,23/Jun/20 02:20,24/Jun/20 04:52,13/Jul/23 08:46,24/Jun/20 04:52,3.1.0,,,,,,,,,,,,3.1.0,,,SQL,,,,0,,,,,apachespark,cloud_fan,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 04:52:18 UTC 2020,,,,,,,,,,"0|z0g3u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/20 02:30;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/28899;;;","23/Jun/20 02:30;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/28899;;;","24/Jun/20 04:52;cloud_fan;Issue resolved by pull request 28899
[https://github.com/apache/spark/pull/28899];;;",,,,,,,,,,,,,,,,,,,,,
SparkExecuteStatementOperation does not set CANCELED state correctly ,SPARK-32057,13312905,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smesseim,smesseim,smesseim,22/Jun/20 21:00,12/Dec/22 18:11,13/Jul/23 08:46,08/Jul/20 00:29,3.0.0,,,,,,,,,,,,3.0.1,3.1.0,,SQL,,,,0,,,"https://github.com/apache/spark/pull/28671 introduced changes that changed the way cleanup is done in SparkExecuteStatementOperation. In cancel(), cleanup (killing jobs) used to be done after setting state to CANCELED. Now, the order is reversed. Jobs are killed first, causing exception to be thrown inside execute(), so the status of the operation becomes ERROR before being set to CANCELED.

cc [~juliuszsompolski]",,apachespark,smesseim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31859,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 08 00:29:23 UTC 2020,,,,,,,,,,"0|z0g3nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/20 19:10;apachespark;User 'alismess-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/28912;;;","23/Jun/20 19:11;apachespark;User 'alismess-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/28912;;;","08/Jul/20 00:29;gurwls223;Issue resolved by pull request 28912
[https://github.com/apache/spark/pull/28912];;;",,,,,,,,,,,,,,,,,,,,,
Repartition by key should support partition coalesce for AQE,SPARK-32056,13312897,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,koert,koert,22/Jun/20 19:37,23/Oct/21 06:14,13/Jul/23 08:46,29/Jun/20 11:33,3.0.0,,,,,,,,,,,,3.1.0,,,SQL,,,,0,,,"when adaptive query execution is enabled the following expression should support coalescing of partitions:
{code:java}
dataframe.repartition(col(""somecolumn"")) {code}
currently it does not because it simply calls the repartition implementation where number of partitions is specified:
{code:java}
  def repartition(partitionExprs: Column*): Dataset[T] = {
    repartition(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }{code}
and repartition with the number of partitions specified does now allow for coalescing of partitions (since this breaks the user's expectation that it will have the number of partitions specified).

for more context see the discussion here:

[https://github.com/apache/spark/pull/27986]

a simple test to confirm that repartition by key does not support coalescing of partitions can be added in AdaptiveQueryExecSuite like this (it currently fails):
{code:java}
  test(""SPARK-32056 repartition has less partitions for small data when adaptiveExecutionEnabled"") {
    Seq(true, false).foreach { enableAQE =>
      withSQLConf(
        SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> enableAQE.toString,
        SQLConf.SHUFFLE_PARTITIONS.key -> ""50"",
        SQLConf.COALESCE_PARTITIONS_INITIAL_PARTITION_NUM.key -> ""50"",
        SQLConf.SHUFFLE_PARTITIONS.key -> ""50"") {
        val partitionsNum = (1 to 10).toDF.repartition($""value"")
          .rdd.collectPartitions().length
        if (enableAQE) {
          assert(partitionsNum < 50)
        } else {
          assert(partitionsNum === 50)
        }
      }
    }
  }
{code}
 ",spark release 3.0.0,apachespark,cloud_fan,koert,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31220,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 00:06:14 UTC 2020,,,,,,,,,,"0|z0g3lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/20 02:56;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/28900;;;","23/Jun/20 02:57;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/28900;;;","29/Jun/20 11:33;cloud_fan;Issue resolved by pull request 28900
[https://github.com/apache/spark/pull/28900];;;","30/Jun/20 00:05;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/28952;;;","30/Jun/20 00:06;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/28952;;;",,,,,,,,,,,,,,,,,,,
[SS] 2.4 Kafka continuous processing print mislead initial offsets log ,SPARK-32044,13312702,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,warrenzhu25,warrenzhu25,warrenzhu25,21/Jun/20 17:30,22/Jun/20 22:20,13/Jul/23 08:46,22/Jun/20 22:20,2.4.6,,,,,,,,,,,,2.4.7,,,Structured Streaming,,,,0,,,"When using structured streaming in continuous processing mode, after restart spark job, spark job can correctly pick up offsets in checkpoint location from last epoch. But it always print out below log:

20/06/12 00:58:09 INFO [stream execution thread for [id = 34e5b909-f9fe-422a-89c0-081251a68693, runId = 0246e19d-aaa1-4a5c-9091-bab1a0578a0a]] kafka010.KafkaContinuousReader: Initial offsets: \{""kafka_topic"":{""8"":51618236,""11"":51610655,""2"":51622889,""5"":51637171,""14"":51637346,""13"":51627784,""4"":51606960,""7"":51632475,""1"":51636129,""10"":51632212,""9"":51634107,""3"":51611013,""12"":51626567,""15"":51640774,""6"":51637823,""0"":51629106}}

This log is misleading as spark didn't use this one as initial offsets. Also, it results in unnecessary kafka offset fetch. This is caused by below code in KafkaContinuousReader
{code:java}
offset = start.orElse {
  val offsets = initialOffsets match {
    case EarliestOffsetRangeLimit =>
      KafkaSourceOffset(offsetReader.fetchEarliestOffsets())
    case LatestOffsetRangeLimit =>
      KafkaSourceOffset(offsetReader.fetchLatestOffsets(None))
    case SpecificOffsetRangeLimit(p) =>
      offsetReader.fetchSpecificOffsets(p, reportDataLoss)
  }
  logInfo(s""Initial offsets: $offsets"")
  offsets
}
{code}
 The code inside orElse block is always executed even when start has value.

 ",,apachespark,dongjoon,kabhwan,warrenzhu25,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 22 22:20:58 UTC 2020,,,,,,,,,,"0|z0g2e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/20 18:29;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/28887;;;","22/Jun/20 01:12;kabhwan;Fix version is reserved to mark the versions where the final fix has been landed. Let's leave it empty.;;;","22/Jun/20 22:20;dongjoon;Issue resolved by pull request 28887
[https://github.com/apache/spark/pull/28887];;;",,,,,,,,,,,,,,,,,,,,,
"Exchange reuse won't work in cases when DPP, subqueries are involved",SPARK-32041,13312653,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,prakharjain09,prakharjain09,21/Jun/20 05:34,21/Jun/21 04:54,13/Jul/23 08:46,21/Jun/21 04:54,2.4.6,3.0.0,,,,,,,,,,,3.2.0,,,SQL,,,,0,,,"When an Exchange node is repeated at multiple places in the PhysicalPlan, and if that exchange has some some DPP Subquery filter, then ReuseExchange doesn't work for such Exchange and different stages are launched to compute same thing.

Example:
{noformat}
// generate data
val factData = (1 to 100).map(i => (i%5, i%20, i))
factData.toDF(""store_id"", ""product_id"", ""units_sold"")
  .write
  .partitionBy(""store_id"")
  .format(""parquet"")
  .saveAsTable(""fact_stats"")

val dimData = Seq[(Int, String, String)](
  (1, ""AU"", ""US""),
  (2, ""CA"", ""US""),
  (3, ""KA"", ""IN""),
  (4, ""DL"", ""IN""),
  (5, ""GA"", ""PA""))

dimData.toDF(""store_id"", ""state_province"", ""country"")
  .write
  .format(""parquet"")
  .saveAsTable(""dim_stats"")

sql(""ANALYZE TABLE fact_stats COMPUTE STATISTICS FOR COLUMNS store_id"")
sql(""ANALYZE TABLE dim_stats COMPUTE STATISTICS FOR COLUMNS store_id"")

// Set Configs
spark.sql(""set spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly=true"")
spark.sql(""set spark.sql.autoBroadcastJoinThreshold=1000"")

val query = """"""
    With view1 as (
      SELECT product_id, f.store_id
      FROM fact_stats f JOIN dim_stats
      ON f.store_id = dim_stats.store_id WHERE dim_stats.country = 'IN')
    SELECT * FROM view1 v1 join view1 v2 WHERE v1.product_id = v2.product_id
""""""
val df = spark.sql(query)
println(df.queryExecution.executedPlan)

{noformat}
{noformat}
Plan:
 *(7) SortMergeJoin [product_id#1968|#1968], [product_id#2060|#2060], Inner
 :- *(3) Sort [product_id#1968 ASC NULLS FIRST|#1968 ASC NULLS FIRST], false, 0
 : +- Exchange hashpartitioning(product_id#1968, 5), true, [id=#1140|#1140]
 : +- *(2) Project [product_id#1968, store_id#1970|#1968, store_id#1970]
 : +- *(2) BroadcastHashJoin [store_id#1970|#1970], [store_id#1971|#1971], Inner, BuildRight
 : :- *(2) Project [product_id#1968, store_id#1970|#1968, store_id#1970]
 : : +- *(2) Filter isnotnull(product_id#1968)
 : : +- *(2) ColumnarToRow
 : : +- FileScan parquet default.fact_stats[product_id#1968,store_id#1970|#1968,store_id#1970] Batched: true, DataFilters: [isnotnull(product_id#1968)|#1968)], Format: Parquet, Location: InMemoryFileIndex[file:/home/prakhar/src/os/1_spark/sql/core/spark-warehouse/org.apache.spark.sql..., PartitionFilters: [isnotnull(store_id#1970), dynamicpruningexpression(store_id#1970 IN dynamicpruning#2067)|#1970), dynamicpruningexpression(store_id#1970 IN dynamicpruning#2067)], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:int>
 : : +- SubqueryBroadcast dynamicpruning#2067, 0, [store_id#1971|#1971], [id=#1131|#1131]
 : : +- ReusedExchange [store_id#1971|#1971], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#1021|#1021]
 : +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#1021|#1021]
 : +- *(1) Project [store_id#1971|#1971]
 : +- *(1) Filter ((isnotnull(country#1973) AND (country#1973 = IN)) AND isnotnull(store_id#1971))
 : +- *(1) ColumnarToRow
 : +- FileScan parquet default.dim_stats[store_id#1971,country#1973|#1971,country#1973] Batched: true, DataFilters: [isnotnull(country#1973), (country#1973 = IN), isnotnull(store_id#1971)|#1973), (country#1973 = IN), isnotnull(store_id#1971)], Format: Parquet, Location: InMemoryFileIndex[file:/home/prakhar/src/os/1_spark/sql/core/spark-warehouse/org.apache.spark.sql..., PartitionFilters: [], PushedFilters: [IsNotNull(country), EqualTo(country,IN), IsNotNull(store_id)], ReadSchema: struct<store_id:int,country:string>
 +- *(6) Sort [product_id#2060 ASC NULLS FIRST|#2060 ASC NULLS FIRST], false, 0
 +- ReusedExchange [product_id#2060, store_id#2062|#2060, store_id#2062], Exchange hashpartitioning(product_id#1968, 5), true, [id=#1026|#1026]
{noformat}
Issue:
 Note the last line of plan. Its a ReusedExchange which is pointing to id=1026. But There is no Exchange node in plan with ID 1026. ReusedExchange node is pointing to incorrect Child node (1026 instead of 1140) and so in actual, exchange reuse won't happen in this query.

Another query where issue is because of ReuseSubquery:
{noformat}
spark.sql(""set spark.sql.autoBroadcastJoinThreshold=-1"")

val query1 = """"""
                  | With view1 as (
                  |   SELECT product_id, units_sold
                  |   FROM fact_stats
                  |   WHERE store_id = (SELECT max(store_id) FROM dim_stats)
                  |         and units_sold = 2
                  | ), view2 as (
                  |   SELECT product_id, units_sold
                  |   FROM fact_stats
                  |   WHERE store_id = (SELECT max(store_id) FROM dim_stats)
                  |         and units_sold = 1
                  | )
                  |
                  | SELECT *
                  | FROM view1 v1 join view2 v2 join view2 v3
                  | WHERE v1.product_id = v2.product_id and v2.product_id = v3.product_id
""""""
// Here we are joining v2 with self. So it should use ReuseExchange. But final plan computes v2 twice.
val df = spark.sql(query1);
println(df.queryExecution.executedPlan){noformat}
Here we are joining v2 with self. So it should use ReuseExchange. But final plan computes v2 twice.

 ",,apachespark,cloud_fan,petertoth,prakharjain09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29375,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 21 04:54:09 UTC 2021,,,,,,,,,,"0|z0g23c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/20 05:46;apachespark;User 'prakharjain09' has created a pull request for this issue:
https://github.com/apache/spark/pull/28881;;;","21/Jun/20 05:47;apachespark;User 'prakharjain09' has created a pull request for this issue:
https://github.com/apache/spark/pull/28881;;;","22/Jun/20 08:25;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/28885;;;","22/Jun/20 08:26;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/28885;;;","26/Jan/21 13:19;petertoth;Let me reopen this ticket as this is not a duplicate of SPARK-29375 but more like a bug. The connection between this ticket, SPARK-29375 and SPARK-28940 is that my PR (https://github.com/apache/spark/pull/28885) fixes all of them. ;;;","21/Jun/21 04:54;cloud_fan;Issue resolved by pull request 28885
[https://github.com/apache/spark/pull/28885];;;",,,,,,,,,,,,,,,,,,
Regression in handling NaN values in COUNT(DISTINCT),SPARK-32038,13312568,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,viirya,mithun,mithun,19/Jun/20 23:44,24/Jun/20 08:37,13/Jul/23 08:46,22/Jun/20 11:59,3.0.0,,,,,,,,,,,,3.0.1,3.1.0,,SQL,,,,1,correctness,,"There seems to be a regression in Spark 3.0.0, with regard to how {{NaN}} values are normalized/handled in {{COUNT(DISTINCT ...)}}. Here is an illustration:
{code:scala}
case class Test( uid:String, score:Float)

val POS_NAN_1 = java.lang.Float.intBitsToFloat(0x7f800001)
val POS_NAN_2 = java.lang.Float.intBitsToFloat(0x7fffffff)

val rows = Seq(

 Test(""mithunr"",  Float.NaN),	
 Test(""mithunr"",  POS_NAN_1),
 Test(""mithunr"",  POS_NAN_2),
 Test(""abellina"", 1.0f),
 Test(""abellina"", 2.0f)

).toDF.createOrReplaceTempView(""mytable"")

spark.sql("" select uid, count(distinct score) from mytable group by 1 order by 1 asc "").show
{code}
Here are the results under Spark 3.0.0:
{code:java|title=Spark 3.0.0 (single aggregation)}
+--------+---------------------+
|     uid|count(DISTINCT score)|
+--------+---------------------+
|abellina|                    2|
| mithunr|                    3|
+--------+---------------------+
{code}
Note that the count against {{mithunr}} is {{3}}, accounting for each distinct value for {{NaN}}.
 The right results are returned when another aggregation is added to the GBY:
{code:scala|title=Spark 3.0.0 (multiple aggregations)}
scala> spark.sql("" select uid, count(distinct score), max(score) from mytable group by 1 order by 1 asc "").show
+--------+---------------------+----------+
|     uid|count(DISTINCT score)|max(score)|
+--------+---------------------+----------+
|abellina|                    2|       2.0|
| mithunr|                    1|       NaN|
+--------+---------------------+----------+
{code}
Also, note that Spark 2.4.6 normalizes the {{DISTINCT}} expression correctly:
{code:scala|title=Spark 2.4.6}
scala> spark.sql("" select uid, count(distinct score) from mytable group by 1 order by 1 asc "").show

+--------+---------------------+
|     uid|count(DISTINCT score)|
+--------+---------------------+
|abellina|                    2|
| mithunr|                    1|
+--------+---------------------+
{code}",,apachespark,dongjoon,mithun,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 08:37:28 UTC 2020,,,,,,,,,,"0|z0g1kg:",9223372036854775807,,,,,,,,,,,,,3.0.1,,,,,,,,,,"20/Jun/20 07:16;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/28876;;;","20/Jun/20 07:16;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/28876;;;","20/Jun/20 21:48;dongjoon;Since this is a correctness regression, I raised the priority to `Blocker` with target version `3.0.1`.;;;","22/Jun/20 11:59;dongjoon;Issue resolved by pull request 28876
[https://github.com/apache/spark/pull/28876];;;","22/Jun/20 16:49;mithun;[~viirya], [~dongjoon], thank you. I'm amazed at the quick resolution of this bug.;;;","24/Jun/20 08:36;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/28919;;;","24/Jun/20 08:37;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/28919;;;",,,,,,,,,,,,,,,,,
Inconsistent AWS environment variables in documentation,SPARK-32035,13312433,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,rjoerger,ondrej,ondrej,19/Jun/20 10:13,09/Jul/20 18:02,13/Jul/23 08:46,09/Jul/20 17:37,2.4.6,3.0.0,,,,,,,,,,,2.4.7,3.0.1,3.1.0,Documentation,,,,0,,,"Looking at the actual Scala code, the environment variables used to log into AWS are:
 - AWS_ACCESS_KEY_ID
 - AWS_SECRET_ACCESS_KEY
 - AWS_SESSION_TOKEN

These are the same that AWS uses in their libraries.

However, looking through the Spark documentation and comments, I see that these are not denoted correctly across the board:

docs/cloud-integration.md
 106:1. `spark-submit` reads the `AWS_ACCESS_KEY`, `AWS_SECRET_KEY` *<-- both different*
 107:and `AWS_SESSION_TOKEN` environment variables and sets the associated authentication options

docs/streaming-kinesis-integration.md
 232:- Set up the environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_KEY` with your AWS credentials. *<-- secret key different*

external/kinesis-asl/src/main/python/examples/streaming/kinesis_wordcount_asl.py
 34: $ export AWS_ACCESS_KEY_ID=<your-access-key>
 35: $ export AWS_SECRET_KEY=<your-secret-key> *<-- different*
 48: Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY *<-- secret key different*

core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala
 438: val keyId = System.getenv(""AWS_ACCESS_KEY_ID"")
 439: val accessKey = System.getenv(""AWS_SECRET_ACCESS_KEY"")
 448: val sessionToken = System.getenv(""AWS_SESSION_TOKEN"")

external/kinesis-asl/src/main/scala/org/apache/spark/examples/streaming/KinesisWordCountASL.scala
 53: * $ export AWS_ACCESS_KEY_ID=<your-access-key>
 54: * $ export AWS_SECRET_KEY=<your-secret-key> *<-- different*
 65: * Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY *<-- secret key different*

external/kinesis-asl/src/main/java/org/apache/spark/examples/streaming/JavaKinesisWordCountASL.java
 59: * $ export AWS_ACCESS_KEY_ID=[your-access-key]
 60: * $ export AWS_SECRET_KEY=<your-secret-key> *<-- different*
 71: * Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY *<-- secret key different*",,apachespark,dongjoon,ondrej,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 09 17:37:21 UTC 2020,,,,,,,,,,"0|z0g0qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/20 16:21;apachespark;User 'Moovlin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29058;;;","09/Jul/20 17:37;dongjoon;Issue resolved by pull request 29058
[https://github.com/apache/spark/pull/29058];;;",,,,,,,,,,,,,,,,,,,,,,
Port HIVE-14817: Shutdown the SessionManager timeoutChecker thread properly upon shutdown,SPARK-32034,13312415,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,19/Jun/20 08:45,22/Jun/20 08:14,13/Jul/23 08:46,21/Jun/20 23:29,3.0.0,3.1.0,,,,,,,,,,,2.4.7,3.0.1,3.1.0,SQL,,,,0,,,"When stopping the HiveServer2, the non-daemon thread stops the server from terminating

{code:java}
""HiveServer2-Background-Pool: Thread-79"" #79 prio=5 os_prio=31 tid=0x00007fde26138800 nid=0x13713 waiting on condition [0x0000700010c32000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hive.service.cli.session.SessionManager$1.sleepInterval(SessionManager.java:178)
	at org.apache.hive.service.cli.session.SessionManager$1.run(SessionManager.java:156)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


{code}


Also, causes issues as HIVE-14817 described
",,apachespark,dongjoon,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 22 03:28:41 UTC 2020,,,,,,,,,,"0|z0g0mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/20 08:55;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/28870;;;","19/Jun/20 08:56;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/28870;;;","21/Jun/20 23:29;dongjoon;Issue resolved by pull request 28870
[https://github.com/apache/spark/pull/28870];;;","22/Jun/20 03:27;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/28888;;;","22/Jun/20 03:28;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/28888;;;",,,,,,,,,,,,,,,,,,,
App id link in history summary page point to wrong application attempt,SPARK-32028,13312355,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zhli,zhli,zhli,19/Jun/20 04:54,01/Jul/20 06:10,13/Jul/23 08:46,24/Jun/20 02:45,2.4.4,3.0.0,3.1.0,,,,,,,,,,2.4.7,3.0.1,3.1.0,Web UI,,,,0,,,"App id link in history summary page url is wrong, for multi attempts case. for details, please see attached screen.",,apachespark,zhli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/20 04:54;zhli;multi_same.JPG;https://issues.apache.org/jira/secure/attachment/13006032/multi_same.JPG","19/Jun/20 04:54;zhli;wrong_attemptJPG.JPG;https://issues.apache.org/jira/secure/attachment/13006033/wrong_attemptJPG.JPG",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 29 16:20:39 UTC 2020,,,,,,,,,,"0|z0g094:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/20 05:08;apachespark;User 'zhli1142015' has created a pull request for this issue:
https://github.com/apache/spark/pull/28867;;;","19/Jun/20 05:09;apachespark;User 'zhli1142015' has created a pull request for this issue:
https://github.com/apache/spark/pull/28867;;;","24/Jun/20 02:45;srowen;Issue resolved by pull request 28867
[https://github.com/apache/spark/pull/28867];;;","29/Jun/20 16:20;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/28949;;;",,,,,,,,,,,,,,,,,,,,
CSV schema inference with boolean & integer ,SPARK-32025,13312295,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,planga82,bwallvera,bwallvera,18/Jun/20 19:54,12/Dec/22 18:10,13/Jul/23 08:46,26/Jun/20 01:42,2.4.6,,,,,,,,,,,,3.1.0,,,SQL,,,,0,,,"I have a dataset consisting of two small files in CSV format. 
{code:bash}
$ cat /example/f0.csv
col1
8589934592

$ cat /example/f1.csv
col1
43200000
true
{code}
 

When I try and load this in (py)spark and infer schema, my expectation is that the column is inferred to be a string. However, it is inferred as a boolean:
{code:python}
spark.read.csv(path=""file:///example/*.csv"", header=True, inferSchema=True, multiLine=True).show()
+----+
|col1|
+----+
|null|
|true|
|null|
+----+
{code}
Note that this seems to work correctly if multiLine is set to False (although we need to set it to True as this column may indeed span multiple lines in general).",,apachespark,bwallvera,planga82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 26 01:42:39 UTC 2020,,,,,,,,,,"0|z0fzvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/20 21:27;planga82;I'm looking for the problem, as a workaround you can define the schema to avoid the bug on infer schema automatically;;;","22/Jun/20 20:35;apachespark;User 'planga82' has created a pull request for this issue:
https://github.com/apache/spark/pull/28896;;;","22/Jun/20 20:36;apachespark;User 'planga82' has created a pull request for this issue:
https://github.com/apache/spark/pull/28896;;;","26/Jun/20 01:42;gurwls223;Issue resolved by pull request 28896
[https://github.com/apache/spark/pull/28896];;;",,,,,,,,,,,,,,,,,,,,
Disk usage tracker went negative in HistoryServerDiskManager,SPARK-32024,13312238,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zhli,zhli,zhli,18/Jun/20 13:54,08/Jul/20 23:37,13/Jul/23 08:46,08/Jul/20 13:00,2.4.4,3.0.0,3.1.0,,,,,,,,,,2.4.7,3.0.1,3.1.0,Web UI,,,,0,,,"After restart history server, we would see below error randomly.
h2. HTTP ERROR 500 java.lang.IllegalStateException: Disk usage tracker went negative (now = -****, delta = -****)
||URI:|/history/********/*/stages/|
||STATUS:|500|
||MESSAGE:|java.lang.IllegalStateException: Disk usage tracker went negative (now = -****, delta = -****)|
||SERVLET:|org.apache.spark.deploy.history.HistoryServer$$anon$1-6ce1f601|
||CAUSED BY:|java.lang.IllegalStateException: Disk usage tracker went negative (now = -****, delta = -****)|
h3. Caused by:

java.lang.IllegalStateException: Disk usage tracker went negative (now = -633925, delta = -38947) at org.apache.spark.deploy.history.HistoryServerDiskManager.org$apache$spark$deploy$history$HistoryServerDiskManager$$updateUsage(HistoryServerDiskManager.scala:258) at org.apache.spark.deploy.history.HistoryServerDiskManager$Lease.rollback(HistoryServerDiskManager.scala:316) at org.apache.spark.deploy.history.FsHistoryProvider.loadDiskStore(FsHistoryProvider.scala:1192) at org.apache.spark.deploy.history.FsHistoryProvider.getAppUI(FsHistoryProvider.scala:363) at org.apache.spark.deploy.history.HistoryServer.getAppUI(HistoryServer.scala:191) at org.apache.spark.deploy.history.ApplicationCache.$anonfun$loadApplicationEntry$2(ApplicationCache.scala:163) at org.apache.spark.deploy.history.ApplicationCache.time(ApplicationCache.scala:135) at org.apache.spark.deploy.history.ApplicationCache.org$apache$spark$deploy$history$ApplicationCache$$loadApplicationEntry(ApplicationCache.scala:161) at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:56) at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:52) at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599) at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379) at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342) at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257) at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000) at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004) at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874) at org.apache.spark.deploy.history.ApplicationCache.get(ApplicationCache.scala:89) at org.apache.spark.deploy.history.ApplicationCache.withSparkUI(ApplicationCache.scala:101) at org.apache.spark.deploy.history.HistoryServer.org$apache$spark$deploy$history$HistoryServer$$loadAppUi(HistoryServer.scala:248) at org.apache.spark.deploy.history.HistoryServer$$anon$1.doGet(HistoryServer.scala:101) at javax.servlet.http.HttpServlet.service(HttpServlet.java:687) at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:763) at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1631) at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95) at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1618) at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:549) at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233) at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1363) at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188) at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:489) at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186) at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1278) at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:767) at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:221) at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127) at org.sparkproject.jetty.server.Server.handle(Server.java:500) at org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383) at org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:547) at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:375) at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:273) at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311) at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:103) at org.sparkproject.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117) at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336) at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313) at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171) at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129) at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375) at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806) at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938) at java.lang.Thread.run(Thread.java:748)","System: Windows, Linux.

Config:

spark.history.retainedApplications 200

spark.history.store.maxDiskUsage 10g

spark.history.store.path /cache_hs",apachespark,kabhwan,zhli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 08 23:37:22 UTC 2020,,,,,,,,,,"0|z0fzj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/20 14:22;apachespark;User 'zhli1142015' has created a pull request for this issue:
https://github.com/apache/spark/pull/28859;;;","18/Jun/20 14:22;apachespark;User 'zhli1142015' has created a pull request for this issue:
https://github.com/apache/spark/pull/28859;;;","08/Jul/20 13:00;kabhwan;Issue resolved by pull request 28859
[https://github.com/apache/spark/pull/28859];;;","08/Jul/20 22:20;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/29046;;;","08/Jul/20 22:22;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/29046;;;","08/Jul/20 23:30;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/29047;;;","08/Jul/20 23:30;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/29047;;;","08/Jul/20 23:37;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/29048;;;","08/Jul/20 23:37;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/29048;;;",,,,,,,,,,,,,,,
make_interval does not accept seconds >100,SPARK-32021,13312194,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,juliuszsompolski,juliuszsompolski,18/Jun/20 11:23,20/Jun/20 18:12,13/Jul/23 08:46,20/Jun/20 02:35,3.0.0,,,,,,,,,,,,3.0.1,3.1.0,,SQL,,,,1,,,"In make_interval(years, months, weeks, days, hours, mins, secs), secs are defined as Decimal(8, 6), which turns into null if the value of the expression overflows 100 seconds.
Larger seconds values should be allowed.

This has been reported by Simba, who wants to use make_interval to implement translation for TIMESTAMP_ADD ODBC function in Spark 3.0.
ODBC {fn TIMESTAMPADD(SECOND, integer_exp, timestamp} fails when integer_exp returns seconds values >= 100.",,apachespark,dongjoon,juliuszsompolski,Samwel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 20 08:39:03 UTC 2020,,,,,,,,,,"0|z0fz9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/20 19:47;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28873;;;","20/Jun/20 02:35;dongjoon;This is resolved via https://github.com/apache/spark/pull/28873;;;","20/Jun/20 08:38;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28878;;;","20/Jun/20 08:39;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28878;;;",,,,,,,,,,,,,,,,,,,,
Fix UnsafeRow set overflowed decimal,SPARK-32018,13312081,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,allisonwang-db,allisonwang-db,17/Jun/20 23:59,18/Aug/20 04:03,13/Jul/23 08:46,16/Jul/20 18:11,2.4.6,2.4.7,3.0.0,3.0.1,,,,,,,,,3.1.0,,,SQL,,,,0,,,There is a bug that writing an overflowed decimal into UnsafeRow is fine but reading it out will throw ArithmeticException. This exception is thrown when calling {{getDecimal}} in UnsafeRow with input decimal's precision greater than the input precision. Setting the value of the overflowed decimal to null when writing into UnsafeRow should fix this issue.,,allisonwang-db,angerszhuuu,apachespark,cloud_fan,ksunitha,maropu,prashant,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 18 04:03:44 UTC 2020,,,,,,,,,,"0|z0fyk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/20 08:47;angerszhuuu;Work On this;;;","28/Jun/20 11:04;angerszhuuu;[~allisonwang-db] 

Can you show a test case to reproduce this?;;;","07/Jul/20 16:29;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29026;;;","07/Jul/20 16:30;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29026;;;","15/Jul/20 17:20;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29125;;;","15/Jul/20 17:21;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29125;;;","17/Jul/20 05:02;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29141;;;","17/Jul/20 05:03;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29141;;;","31/Jul/20 20:20;ksunitha;[@cloud-fan|https://github.com/cloud-fan], I noticed the back ports now.  This change is more far reaching in its impact as previous callers of UnsafeRow.getDecimal that would have thrown an exception earlier would now return null.

As an e.g, a caller like aggregate sum will need changes to account for this. Earlier cases where sum would throw error for overflow will *now return incorrect results*. The new tests that were added for sum overflow cases in the DataFrameSuite in master can be used to see repro.

IMO, it would be better to not back port the setDecimal change in isolation. wdyt?   Please share your thoughts.  Thanks. 

I added a comment on the pr but since it is closed, adding a comment here.;;;","06/Aug/20 22:03;ksunitha;The important issue is we should not return incorrect results. In general, it is not a good practice to back port a change to a stable branch and cause more queries to return incorrect results.

Just to reiterate:
 # This current PR that has back ported the UnsafeRow fix causes queries to return incorrect results. This is for v2.4.x and v3.0.x line. This change by itself has unsafe side effects and results in incorrect results being returned.
 # It does not matter whether you have whole stage on or off, ansi on or off, you will get more queries returning incorrect results.
 # Incorrect results is very serious and it is not good for Spark users to run into it for common operations like sum.

{code:java}
 
scala> val decStr = ""1"" + ""0"" * 19
decStr: String = 10000000000000000000
scala> val d3 = spark.range(0, 1, 1, 1).union(spark.range(0, 11, 1, 1))
d3: org.apache.spark.sql.Dataset[Long] = [id: bigint]
 
scala>  val d5 = d3.select(expr(s""cast('$decStr' as decimal (38, 18)) as d""),lit(1).as(""key"")).groupBy(""key"").agg(sum($""d"").alias(""sumd"")).select($""sumd"")
d5: org.apache.spark.sql.DataFrame = [sumd: decimal(38,18)]
scala> d5.show(false)   <----- INCORRECT RESULTS
+---------------------------------------+
|sumd                                   |
+---------------------------------------+
|20000000000000000000.000000000000000000|
+---------------------------------------+
{code}
 ;;;","06/Aug/20 22:06;ksunitha;I have added a summary of my comments from the  [https://github.com/apache/spark/pull/29125] discussion in above comment.  Thanks. ;;;","11/Aug/20 06:41;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/29404;;;","11/Aug/20 06:42;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/29404;;;","13/Aug/20 10:27;prashant;This issue is resolved as fixed with version 2.4.7. However, I am unable to find the fix in branch 2.4. ;;;","13/Aug/20 12:23;cloud_fan;[~Gengliang.Wang] we should create a new JIRA ticket for the new fix. The new fix is not applicable to 2.4 as 2.4 does not have ANSI mode.;;;","17/Aug/20 04:36;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/29448;;;","17/Aug/20 04:37;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/29448;;;","17/Aug/20 06:56;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/29450;;;","17/Aug/20 06:56;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/29450;;;","17/Aug/20 13:59;cloud_fan;The unsafe row bug fix has been reverted from 3.0/2.4, see the reason https://github.com/apache/spark/pull/29450#issuecomment-674799489;;;","18/Aug/20 04:03;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/29458;;;",,,
Thread leaks in pinned thread mode,SPARK-32010,13311850,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,17/Jun/20 01:19,12/Dec/22 18:11,13/Jul/23 08:46,30/Jul/20 01:15,3.1.0,,,,,,,,,,,,3.1.0,,,PySpark,SQL,,,0,,,"SPARK-22340 introduced a pin thread mode which guarantees you to sync Python thread and JVM thread.

However, looks like the JVM threads are not finished even when the Python thread is finished. It can be debugged via YourKit, and run multiple jobs with multiple threads at the same time.

Easiest reproducer is:

{code}
PYSPARK_PIN_THREAD=true ./bin/pyspark
{code}

{code}
>>> from threading import Thread
>>> Thread(target=lambda: spark.range(1000).collect()).start()
>>> Thread(target=lambda: spark.range(1000).collect()).start()
>>> Thread(target=lambda: spark.range(1000).collect()).start()
>>> spark._jvm._gateway_client.deque
deque([<py4j.clientserver.ClientServerConnection object at 0x119f7aba8>, <py4j.clientserver.ClientServerConnection object at 0x119fc9b70>, <py4j.clientserver.ClientServerConnection object at 0x119fc9e10>, <py4j.clientserver.ClientServerConnection object at 0x11a015358>, <py4j.clientserver.ClientServerConnection object at 0x119fc00f0>])
>>> Thread(target=lambda: spark.range(1000).collect()).start()
>>> spark._jvm._gateway_client.deque
deque([<py4j.clientserver.ClientServerConnection object at 0x119f7aba8>, <py4j.clientserver.ClientServerConnection object at 0x119fc9b70>, <py4j.clientserver.ClientServerConnection object at 0x119fc9e10>, <py4j.clientserver.ClientServerConnection object at 0x11a015358>, <py4j.clientserver.ClientServerConnection object at 0x119fc08d0>, <py4j.clientserver.ClientServerConnection object at 0x119fc00f0>])
{code}

The connection doesn't get closed, and it holds JVM thread running.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32011,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 30 01:15:43 UTC 2020,,,,,,,,,,"0|z0fx4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/20 01:34;gurwls223;I have a kind of a POC fix already. Will open a PR later.;;;","21/Jun/20 02:31;gurwls223;cc [~irashid] FYI;;;","01/Jul/20 15:11;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28968;;;","01/Jul/20 15:12;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28968;;;","30/Jul/20 01:15;gurwls223;Issue resolved by pull request 28968
[https://github.com/apache/spark/pull/28968];;;",,,,,,,,,,,,,,,,,,,
3.0.0 release build fails,SPARK-32008,13311813,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,Reamer,Reamer,16/Jun/20 20:35,17/Jun/20 08:40,13/Jul/23 08:46,17/Jun/20 08:40,3.0.0,,,,,,,,,,,,3.0.0,,,Build,Documentation,,,0,,,"Hi,
I try to build the spark release 3.0.0 by myself.

I got the following error.
{code}                                          
20/06/16 15:20:49 WARN PrefixSpan: Input data is not cached.
20/06/16 15:20:50 WARN Instrumentation: [b307b568] regParam is zero, which might cause numerical instability and overfitting.
Error: processing vignette 'sparkr-vignettes.Rmd' failed with diagnostics:
'vignetteInfo' is not an exported object from 'namespace:tools'
Execution halted
{code}

I can reproduce this error with a small Dockerfile.

EDIT: Issue is solved with ubuntu 20.04 as base image.
{code}
FROM ubuntu:20.04 as builder

ENV MVN_VERSION=3.6.3 \
    M2_HOME=/opt/apache-maven \
    MAVEN_HOME=/opt/apache-maven \
    MVN_HOME=/opt/apache-maven \
    MVN_SHA512=c35a1803a6e70a126e80b2b3ae33eed961f83ed74d18fcd16909b2d44d7dada3203f1ffe726c17ef8dcca2dcaa9fca676987befeadc9b9f759967a8cb77181c0 \
    MAVEN_OPTS=""-Xmx3g -XX:ReservedCodeCacheSize=1g"" \
    R_HOME=/usr/lib/R \
    GIT_REPO=https://github.com/apache/spark.git \
    GIT_BRANCH=v3.0.0 \
    SPARK_DISTRO_NAME=hadoop3.2 \
    SPARK_LOCAL_HOSTNAME=localhost

# Preparation
RUN /usr/bin/apt-get update && \
    # APT
    INSTALL_PKGS=""openjdk-8-jdk-headless git wget python3 python3-pip python3-setuptools r-base r-base-dev pandoc pandoc-citeproc libcurl4-openssl-dev libssl-dev libxml2-dev texlive qpdf language-pack-en"" && \
    DEBIAN_FRONTEND=noninteractive /usr/bin/apt-get -y install --no-install-recommends $INSTALL_PKGS && \
    rm -rf /var/lib/apt/lists/* && \
    Rscript -e ""install.packages(c('knitr', 'rmarkdown', 'devtools', 'testthat', 'e1071', 'survival'), repos='https://cloud.r-project.org/')"" && \
    # Maven
    /usr/bin/wget -nv -O apache-maven.tar.gz ""https://www.apache.org/dyn/mirrors/mirrors.cgi?action=download&filename=maven/maven-3/${MVN_VERSION}/binaries/apache-maven-${MVN_VERSION}-bin.tar.gz"" && \
    echo ""${MVN_SHA512} apache-maven.tar.gz"" > apache-maven.sha512 && \
    sha512sum --strict -c apache-maven.sha512 && \
    tar -xvzf apache-maven.tar.gz -C /opt && \
    rm -v apache-maven.sha512 apache-maven.tar.gz && \
    /bin/ln -vs /opt/apache-maven-${MVN_VERSION} /opt/apache-maven && \
    /bin/ln -vs /opt/apache-maven/bin/mvn /usr/bin/mvn

# Spark Distribution Build
RUN mkdir -p /workspace && \
    cd /workspace && \
    git clone --branch ${GIT_BRANCH} ${GIT_REPO} && \
    cd /workspace/spark && \
    ./dev/make-distribution.sh --name ${SPARK_DISTRO_NAME} --pip --r --tgz -Psparkr -Phadoop-3.2 -Phive-2.3 -Phive-thriftserver -Pyarn -Pkubernetes
{code}

I am very grateful to all helpers.",,Reamer,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 08:39:01 UTC 2020,,,,,,,,,,"0|z0fwww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/20 21:29;shivaram;It looks like the R vignette build failed and looking at the error message this seems related to https://github.com/rstudio/rmarkdown/issues/1831 -- I think it should work fine if you try to use R version >= 3.6;;;","17/Jun/20 08:39;Reamer;Hi [~shivaram]

Thanks for your fast response. I was able to build Spark by myself with ubuntu 20.04 as a base image.
ubuntu 20.04 delivers with R version >= 3.6

I'll edit the Dockerfile in my bug description.

;;;",,,,,,,,,,,,,,,,,,,,,,
Create date/timestamp formatters once before collect in `hiveResultString()`,SPARK-32006,13311797,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,16/Jun/20 18:52,17/Jun/20 06:29,13/Jul/23 08:46,17/Jun/20 06:29,3.0.1,3.1.0,,,,,,,,,,,3.1.0,,,SQL,,,,0,,,"Spark 2.4 re-uses one instance of SimpleDateFormat while formatting timestamps in toHiveString. Currently, toHiveString() creates timestampFormatter per each value. Even w/ caching, it causes additional overhead comparing to Spark 2.4. The ticket aims to create an instance of TimestampFormatter before collect in hiveResultString()",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 06:29:08 UTC 2020,,,,,,,,,,"0|z0fwtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/20 18:56;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28842;;;","17/Jun/20 06:29;cloud_fan;Issue resolved by pull request 28842
[https://github.com/apache/spark/pull/28842];;;",,,,,,,,,,,,,,,,,,,,,,
Shuffle files for lost executor are not unregistered if fetch failure occurs after executor is lost,SPARK-32003,13311756,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wypoon,wypoon,wypoon,16/Jun/20 15:56,04/Aug/20 19:38,13/Jul/23 08:46,22/Jul/20 14:57,2.4.6,3.0.0,,,,,,,,,,,2.4.7,3.0.1,3.1.0,Scheduler,,,,0,,,"A customer's cluster has a node that goes down while a Spark application is running. (They are running Spark on YARN with the external shuffle service enabled.) An executor is lost (apparently the only one running on the node). This executor lost event is handled in the DAGScheduler, which removes the executor from its BlockManagerMaster. At this point, there is no unregistering of shuffle files for the executor or the node. Soon after, tasks trying to fetch shuffle files output by that executor fail with FetchFailed (because the node is down, there is no NodeManager available to serve shuffle files). By right, such fetch failures should cause the shuffle files for the executor to be unregistered, but they do not.

Due to task failure, the stage is re-attempted. Tasks continue to fail due to fetch failure form the lost executor's shuffle output. This time, since the failed epoch for the executor is higher, the executor is removed again (this doesn't really do anything, the executor was already removed when it was lost) and this time the shuffle output is unregistered.

So it takes two stage attempts instead of one to clear the shuffle output. We get 4 attempts by default. The customer was unlucky and two nodes went down during the stage, i.e., the same problem happened twice. So they used up 4 stage attempts and the stage failed and thus the job. ",,apachespark,irashid,Ngone51,wypoon,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 04 19:38:36 UTC 2020,,,,,,,,,,"0|z0fwk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/20 16:31;wypoon;I will open a PR soon with a solution.;;;","17/Jun/20 04:56;apachespark;User 'wypoon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28848;;;","17/Jun/20 04:57;apachespark;User 'wypoon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28848;;;","22/Jul/20 05:14;apachespark;User 'wypoon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29182;;;","22/Jul/20 14:57;irashid;Fixed in master for 3.1 by https://github.com/apache/spark/pull/28848;;;","22/Jul/20 18:53;apachespark;User 'wypoon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29193;;;","22/Jul/20 18:54;apachespark;User 'wypoon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29193;;;","04/Aug/20 16:38;irashid;Fixed in 3.0.1 by https://github.com/apache/spark/pull/29193;;;","04/Aug/20 19:38;irashid;Fixed in 2.4.7 by https://github.com/apache/spark/pull/29182;;;",,,,,,,,,,,,,,,
Fix the flaky testcase for partially launched task in barrier-mode.,SPARK-32000,13311663,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Ngone51,sarutak,sarutak,16/Jun/20 08:41,17/Jun/20 17:00,13/Jul/23 08:46,17/Jun/20 17:00,3.1.0,,,,,,,,,,,,2.4.7,3.0.1,3.1.0,Spark Core,Tests,,,0,,,I noticed sometimes the testcase for SPARK-31485 fails.,,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 14:16:39 UTC 2020,,,,,,,,,,"0|z0fvzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/20 08:51;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/28839;;;","17/Jun/20 09:44;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/28849;;;","17/Jun/20 09:44;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/28849;;;","17/Jun/20 14:16;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/28851;;;",,,,,,,,,,,,,,,,,,,,
Should drop test_udtf table when SingleSessionSuite completed,SPARK-31997,13311632,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,16/Jun/20 05:17,12/Dec/22 18:11,13/Jul/23 08:46,16/Jun/20 10:21,3.0.0,,,,,,,,,,,,2.4.7,3.0.1,3.1.0,Tests,,,,0,,,"If we execute mvn test SingleSessionSuite and HiveThriftBinaryServerSuite in order,  the test case ""SPARK-11595 ADD JAR with input path having URL scheme""  in HiveThriftBinaryServerSuite will failed as following:

 
{code:java}
- SPARK-11595 ADD JAR with input path having URL scheme *** FAILED *** java.sql.SQLException: Error running query: org.apache.spark.sql.AnalysisException: Can not create the managed table('`default`.`test_udtf`'). The associated location('file:/home/yarn/spark_ut/spark_ut/baidu/inf-spark/spark-source/sql/hive-thriftserver/spark-warehouse/test_udtf') already exists.; at org.apache.hive.jdbc.HiveStatement.waitForOperationToComplete(HiveStatement.java:385) at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:254) at org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite.$anonfun$new$65(HiveThriftServer2Suites.scala:603) at org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite.$anonfun$new$65$adapted(HiveThriftServer2Suites.scala:603) at scala.collection.immutable.List.foreach(List.scala:392) at org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite.$anonfun$new$63(HiveThriftServer2Suites.scala:603) at org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite.$anonfun$new$63$adapted(HiveThriftServer2Suites.scala:573) at org.apache.spark.sql.hive.thriftserver.HiveThriftJdbcTest.$anonfun$withMultipleConnectionJdbcStatement$3(HiveThriftServer2Suites.scala:1074) at org.apache.spark.sql.hive.thriftserver.HiveThriftJdbcTest.$anonfun$withMultipleConnectionJdbcStatement$3$adapted(HiveThriftServer2Suites.scala:1074) at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
{code}
 

because SingleSessionSuite do `create table test_udtf` and not drop it when test  completed and HiveThriftBinaryServerSuite want to re-create this table.

If we execute mvn test HiveThriftBinaryServerSuite and SingleSessionSuite in order,both test suites will succeed, but we shouldn't rely on their execution order

 ",,apachespark,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 10:21:49 UTC 2020,,,,,,,,,,"0|z0fvso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/20 05:18;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/28838;;;","16/Jun/20 10:21;gurwls223;Issue resolved by pull request 28838
[https://github.com/apache/spark/pull/28838];;;",,,,,,,,,,,,,,,,,,,,,,
Docker image should use `https` urls for only mirrors that support it(SSL),SPARK-31994,13311453,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,prashant,prashant,prashant,15/Jun/20 10:04,15/Jun/20 18:27,13/Jul/23 08:46,15/Jun/20 18:26,3.0.0,3.1.0,,,,,,,,,,,3.0.1,3.1.0,,Kubernetes,,,,0,,,"It appears, that security.debian.org does not support https.
{code}
curl https://security.debian.org
curl: (35) LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to security.debian.org:443 
{code}

While building the image, it fails in the following way.
{code}
MacBook-Pro:spark prashantsharma$ bin/docker-image-tool.sh -r scrapcodes -t v3.1.0-1 build
Sending build context to Docker daemon  222.1MB
Step 1/18 : ARG java_image_tag=8-jre-slim
Step 2/18 : FROM openjdk:${java_image_tag}
 ---> 381b20190cf7
Step 3/18 : ARG spark_uid=185
 ---> Using cache
 ---> 65c06f86753c
Step 4/18 : RUN set -ex &&     sed -i 's/http:/https:/g' /etc/apt/sources.list &&     apt-get update &&     ln -s /lib /lib64 &&     apt install -y bash tini libc6 libpam-modules krb5-user libnss3 procps &&     mkdir -p /opt/spark &&     mkdir -p /opt/spark/examples &&     mkdir -p /opt/spark/work-dir &&     touch /opt/spark/RELEASE &&     rm /bin/sh &&     ln -sv /bin/bash /bin/sh &&     echo ""auth required pam_wheel.so use_uid"" >> /etc/pam.d/su &&     chgrp root /etc/passwd && chmod ug+rw /etc/passwd &&     rm -rf /var/cache/apt/*
 ---> Running in a3461dadd6eb
+ sed -i s/http:/https:/g /etc/apt/sources.list
+ apt-get update
Ign:1 https://security.debian.org/debian-security buster/updates InRelease
Err:2 https://security.debian.org/debian-security buster/updates Release
  Could not handshake: The TLS connection was non-properly terminated. [IP: 151.101.0.204 443]
Get:3 https://deb.debian.org/debian buster InRelease [121 kB]
Get:4 https://deb.debian.org/debian buster-updates InRelease [51.9 kB]
Get:5 https://deb.debian.org/debian buster/main amd64 Packages [7905 kB]
Get:6 https://deb.debian.org/debian buster-updates/main amd64 Packages [7868 B]
Reading package lists...
E: The repository 'https://security.debian.org/debian-security buster/updates Release' does not have a Release file.
The command '/bin/sh -c set -ex &&     sed -i 's/http:/https:/g' /etc/apt/sources.list &&     apt-get update &&     ln -s /lib /lib64 &&     apt install -y bash tini libc6 libpam-modules krb5-user libnss3 procps &&     mkdir -p /opt/spark &&     mkdir -p /opt/spark/examples &&     mkdir -p /opt/spark/work-dir &&     touch /opt/spark/RELEASE &&     rm /bin/sh &&     ln -sv /bin/bash /bin/sh &&     echo ""auth required pam_wheel.so use_uid"" >> /etc/pam.d/su &&     chgrp root /etc/passwd && chmod ug+rw /etc/passwd &&     rm -rf /var/cache/apt/*' returned a non-zero code: 100
Failed to build Spark JVM Docker image, please refer to Docker build output for details.
{code}

So, if we limit the https support to only deb.debian.org, that does the trick.",,apachespark,dongjoon,ffrodo,prashant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 18:26:46 UTC 2020,,,,,,,,,,"0|z0fups:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/20 10:16;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/28834;;;","15/Jun/20 10:17;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/28834;;;","15/Jun/20 18:26;dongjoon;Issue resolved by pull request 28834
[https://github.com/apache/spark/pull/28834];;;",,,,,,,,,,,,,,,,,,,,,
Generated code in 'concat_ws' fails to compile when splitting method is in effect,SPARK-31993,13311394,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,15/Jun/20 06:40,27/Jul/20 18:22,13/Jul/23 08:46,19/Jun/20 06:04,2.3.4,2.4.6,3.0.0,3.1.0,,,,,,,,,3.1.0,,,SQL,,,,0,,,"https://github.com/apache/spark/blob/a0187cd6b59a6b6bb2cadc6711bb663d4d35a844/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala#L88-L195

There're three parts of generated code in concat_ws (codes, varargCounts, varargBuilds) and all parts try to split method by itself, while `varargCounts` and `varargBuilds` refer on the generated code in `codes`, hence the overall generated code fails to compile if any of part succeeds to split.",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 08:17:20 UTC 2020,,,,,,,,,,"0|z0fuco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/20 08:16;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/28831;;;","15/Jun/20 08:17;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/28831;;;",,,,,,,,,,,,,,,,,,,,,,
Streaming's state store compatibility is broken,SPARK-31990,13311352,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,maropu,smilegator,smilegator,14/Jun/20 19:10,15/Jun/20 14:49,13/Jul/23 08:46,15/Jun/20 14:49,3.0.0,,,,,,,,,,,,3.0.1,3.1.0,,Structured Streaming,,,,0,correctness,,"[This line|https://github.com/apache/spark/pull/28062/files#diff-7a46f10c3cedbf013cf255564d9483cdR2458] of [https://github.com/apache/spark/pull/28062] changed the order of groupCols in dropDuplicates(). Thus, the executor JVM could probably crash, throw a random exception or even return a wrong answer when using the checkpoint written by the previous version. ",,apachespark,dbtsai,dongjoon,kabhwan,maropu,smilegator,XuanYuan,,,,,,,,,,,,,,,,,,,,,SPARK-31894,,,,,,,,SPARK-31905,SPARK-31292,,,,,SPARK-19497,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 14:49:39 UTC 2020,,,,,,,,,,"0|z0fu3c:",9223372036854775807,,,,,,,,,,,,,3.0.1,,,,,,,,,,"14/Jun/20 19:14;smilegator;cc [~maropu] [~sowen] [~sekikn];;;","14/Jun/20 19:23;smilegator;[~XuanYuan] just identified the problem when fixing the test failure in [https://github.com/apache/spark/pull/28707] . Thank you [~XuanYuan]!;;;","14/Jun/20 20:59;kabhwan;Nice finding!

One thing I'd like to add is, technically it can't be said SPARK-31292 is the root cause. The root cause is that we use toSet which doesn't guarantee order, where order should be preserved.

Paradoxically, ""Seq.distinct"" is more likely fit to the requirement, according to the scaladoc. Below is the description of ""Seq.distinct"":
{noformat}
def distinct: Seq[A]

Builds a new sequence from this sequence without any duplicate elements.

Note: will not terminate for infinite-sized collections.

returns 

A new sequence which contains the first occurrence of every element of this sequence.

Definition 

Classes SeqLike → GenSeqLike {noformat}
(NOTE: the description is changed in 2.13 - I don't know why. Would they change the implementation? If we don't believe Scala description of distinct then probably we can implement some utils which have functions with preserving order of the element.)

Though I have to say yes it may break backward compatibility, especially much more chance compared to the chance the algorithm of toSet affects the order. 

Looks like we should go through the hard decision - ""fix it to get it right"" vs ""leave it as it is unless problem occurs"".

 ;;;","15/Jun/20 00:49;maropu;Oh, nice catch, [~XuanYuan] ! I'll make a PR for that.;;;","15/Jun/20 01:23;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/28830;;;","15/Jun/20 01:24;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/28830;;;","15/Jun/20 02:32;dongjoon;I linked SPARK-31292 according to the PR description, but I also agree with [~kabhwan]'s opinion.
 [~maropu]'s new fix is also resemble to Scala's `distinct` implementation also. 
 [~smilegator]. Could you give us some reproducible example of the broken ordering, please?;;;","15/Jun/20 05:59;XuanYuan;[~maropu] Thanks for the quick fix!

[~dongjoon] This issue is found when investigating SPARK-31894, and it's also a blocker issue for it. I added a detailed analysis in SPARK-31894's [PR comments|https://github.com/apache/spark/pull/28707#issuecomment-643916110].

I think the simple revert fix in [https://github.com/apache/spark/pull/28830] can be merged without tests. I'll add a new integrated test in SPARK-31905.;;;","15/Jun/20 14:49;dongjoon;Issue resolved by pull request 28830
[https://github.com/apache/spark/pull/28830];;;",,,,,,,,,,,,,,,
"Test failure RebaseDateTimeSuite.""optimization of micros rebasing - Julian to Gregorian""",SPARK-31986,13311329,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,14/Jun/20 11:19,16/Jun/20 06:00,13/Jul/23 08:46,16/Jun/20 06:00,3.0.1,3.1.0,,,,,,,,,,,3.0.1,3.1.0,,SQL,,,,0,,,"The test fails on 1945-09-14 23:30:00.0. The failure can be reproduced by modifying the test:
{code:scala}
  test(""optimization of micros rebasing - Julian to Gregorian"") {
    outstandingZoneIds.filter(_.getId.contains(""Hong""))foreach { zid =>
      withClue(s""zone id = $zid"") {
        withDefaultTimeZone(zid) {
          val start = rebaseGregorianToJulianMicros(
            instantToMicros(LocalDateTime.of(1, 1, 1, 0, 0, 0).atZone(zid).toInstant))
          val end = rebaseGregorianToJulianMicros(
            instantToMicros(LocalDateTime.of(2100, 1, 1, 0, 0, 0).atZone(zid).toInstant))
          var micros = -761211000000000L
          do {
            val rebased = rebaseJulianToGregorianMicros(zid, micros)
            val rebasedAndOptimized = rebaseJulianToGregorianMicros(micros)
            assert(rebasedAndOptimized === rebased)
            micros += (MICROS_PER_DAY * 30 * (0.5 + Math.random())).toLong
          } while (micros <= end)
        }
      }
    }
  }
{code}
{code}
zone id = Asia/Hong_Kong -761211000000000 did not equal -761207400000000
ScalaTestFailureLocation: org.apache.spark.sql.catalyst.util.RebaseDateTimeSuite at (RebaseDateTimeSuite.scala:236)
Expected :-761207400000000
Actual   :zone id = Asia/Hong_Kong -761211000000000
{code}",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 06:00:33 UTC 2020,,,,,,,,,,"0|z0fty8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jun/20 11:21;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28816;;;","14/Jun/20 11:22;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28816;;;","16/Jun/20 06:00;cloud_fan;Issue resolved by pull request 28816
[https://github.com/apache/spark/pull/28816];;;",,,,,,,,,,,,,,,,,,,,,
Make micros rebasing functions via local timestamps pure,SPARK-31984,13311281,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,13/Jun/20 18:47,16/Jun/20 12:56,13/Jul/23 08:46,16/Jun/20 12:56,3.0.1,3.1.0,,,,,,,,,,,3.1.0,,,SQL,,,,0,,,"The functions rebaseGregorianToJulianMicros(zoneId: ZoneId, ...) and rebaseJulianToGregorianMicros(zoneId: ZoneId, ...) accept the zone id as the first parameter but use it only while forming ZonedDateTime and ignore in Java 7 GregorianCalendar. The Calendar instance uses the default JVM time zone internally. This causes the following problems:
# The functions depend on the global state variable. And calling the functions from different threads can return wrong results if the the default JVM time zone is changed during the execution.
# It is impossible to speed up generation of JSON files with diff/switch via parallelisation.
# The functions don't fully use the passed ZoneId.",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 12:56:59 UTC 2020,,,,,,,,,,"0|z0ftnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/20 19:11;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28824;;;","16/Jun/20 12:56;cloud_fan;Issue resolved by pull request 28824
[https://github.com/apache/spark/pull/28824];;;",,,,,,,,,,,,,,,,,,,,,,
Tables of structured streaming tab show wrong result for duration column,SPARK-31983,13311259,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rakson,rakson,rakson,13/Jun/20 14:46,15/Jun/20 01:44,13/Jul/23 08:46,14/Jun/20 21:44,3.0.0,,,,,,,,,,,,3.0.1,3.1.0,,SQL,Web UI,,,0,,,"Sorting result for duration column in tables of structured streaming tab is sometimes wrong. As we are sorting on string values. Consider ""3ms"" and ""12ms"".
",,apachespark,rakson,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 01:44:49 UTC 2020,,,,,,,,,,"0|z0ftio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/20 15:20;apachespark;User 'iRakson' has created a pull request for this issue:
https://github.com/apache/spark/pull/28823;;;","14/Jun/20 21:44;srowen;Resolved for 3.1 by https://github.com/apache/spark/pull/28752 - I will look at the backport;;;","14/Jun/20 21:45;apachespark;User 'iRakson' has created a pull request for this issue:
https://github.com/apache/spark/pull/28752;;;","15/Jun/20 01:44;sarutak;Resolved for 3.0 by [https://github.com/apache/spark/pull/28823] .;;;",,,,,,,,,,,,,,,,,,,,
Spark sequence() fails if start and end of range are identical dates,SPARK-31980,13311202,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,JinxinTang,DaveDeCaprio,DaveDeCaprio,12/Jun/20 20:45,20/Jun/20 07:40,13/Jul/23 08:46,20/Jun/20 02:25,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,2.4.5,2.4.6,,,,,,2.4.7,3.0.1,3.1.0,SQL,,,,0,,," 

The following Spark SQL query throws an exception
{code:java}
select sequence(cast(""2011-03-01"" as date), cast(""2011-03-01"" as date), interval 1 month)
{code}
The error is:

 

 
{noformat}
java.lang.ArrayIndexOutOfBoundsException: 1java.lang.ArrayIndexOutOfBoundsException: 1 at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:92) at org.apache.spark.sql.catalyst.expressions.Sequence$TemporalSequenceImpl.eval(collectionOperations.scala:2681) at org.apache.spark.sql.catalyst.expressions.Sequence.eval(collectionOperations.scala:2514) at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:389){noformat}
 ",Spark 2.4.4 standalone and on AWS EMR,apachespark,DaveDeCaprio,dongjoon,JinxinTang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 20 07:40:46 UTC 2020,,,,,,,,,,"0|z0ft60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/20 03:55;apachespark;User 'TJX2014' has created a pull request for this issue:
https://github.com/apache/spark/pull/28819;;;","13/Jun/20 03:56;apachespark;User 'TJX2014' has created a pull request for this issue:
https://github.com/apache/spark/pull/28819;;;","13/Jun/20 12:53;DaveDeCaprio;TJX2014 thanks for the quick turnaround there.  I was just about to pull the repo and start the fix! 

I'm also having problems with SPARK-31982  It's an unrelated issue but in the same section of code.;;;","16/Jun/20 02:51;JinxinTang;[~DaveDeCaprio] Maybe one of these PRs could rebase to solve this code section conflict.;;;","20/Jun/20 02:25;dongjoon;Issue resolved by pull request 28819
[https://github.com/apache/spark/pull/28819];;;","20/Jun/20 07:40;apachespark;User 'TJX2014' has created a pull request for this issue:
https://github.com/apache/spark/pull/28877;;;",,,,,,,,,,,,,,,,,,
write.partitionBy() creates duplicate subdirectories when user provides duplicate columns,SPARK-31968,13310908,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,JinxinTang,qxzzxq,qxzzxq,11/Jun/20 11:59,12/Dec/22 18:11,13/Jul/23 08:46,14/Jun/20 05:28,2.0.2,2.1.3,2.2.3,2.3.4,2.4.6,,,,,,,,2.4.7,3.0.1,3.1.0,SQL,,,,0,,,"I recently remarked that if there are duplicated elements in the argument of write.partitionBy(), then the same partition subdirectory will be created multiple times.

For example: 
{code:java}
import spark.implicits._

val df: DataFrame = Seq(
  (1, ""p1"", ""c1"", 1L),
  (2, ""p2"", ""c2"", 2L),
  (2, ""p1"", ""c2"", 2L),
  (3, ""p3"", ""c3"", 3L),
  (3, ""p2"", ""c3"", 3L),
  (3, ""p3"", ""c3"", 3L)
).toDF(""col1"", ""col2"", ""col3"", ""col4"")

df.write
  .partitionBy(""col1"", ""col1"")  // we have ""col1"" twice
  .mode(SaveMode.Overwrite)
  .csv(""output_dir""){code}
The above code will produce an output directory with this structure:

 
{code:java}
output_dir
  |
  |--col1=1
  |    |--col1=1
  |
  |--col1=2
  |    |--col1=2
  |
  |--col1=3
       |--col1=3{code}
And we won't be able to read the output

 
{code:java}
spark.read.csv(""output_dir"").show()
// Exception in thread ""main"" org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the partition schema: `col1`;{code}
 

I am not sure if partitioning a dataframe twice by the same column make sense in some real-world applications, but it will cause schema inference problems in tools like AWS Glue crawler.

Should Spark handle the deduplication of the partition columns? Or maybe throw an exception when duplicated columns are detected?

If this behaviour is unexpected, I will work on a fix. ",,apachespark,dongjoon,JinxinTang,qxzzxq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 03:38:06 UTC 2020,,,,,,,,,,"0|z0frco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/20 09:06;apachespark;User 'TJX2014' has created a pull request for this issue:
https://github.com/apache/spark/pull/28814;;;","14/Jun/20 05:17;dongjoon;Thanks. I also verified that the read-side exceptions occur at 2.0.2/2.1.3/2.2.3/2.3.4 with the following example.
{code}
Seq((1,2,3)).toDF(""a"", ""b"", ""c"").write.partitionBy(""b"", ""b"").csv(""/tmp/csv"")
spark.read.csv(""/tmp/csv"").show
{code};;;","14/Jun/20 05:28;dongjoon;Issue resolved by pull request 28814
[https://github.com/apache/spark/pull/28814];;;","14/Jun/20 14:33;qxzzxq;Thank you TJX2014 for the PR and thank you all for the responsiveness! :D;;;","15/Jun/20 01:27;dongjoon;Hi, [~hyukjin.kwon]. [~qxzzxq] is TJX2014?;;;","15/Jun/20 01:41;gurwls223;Oops, there's no confirmation here. Let me unassign it back.;;;","15/Jun/20 02:29;JinxinTang;Hi, [~dongjoon], [~hyukjin.kwon], I am  TJX2014,  Thank you very much.  ;;;","15/Jun/20 02:38;JinxinTang;[~qxzzxq] Thank you for this issue, it is my pleasure.;;;","15/Jun/20 03:38;dongjoon;Thanks, [~JinxinTang] and [~hyukjin.kwon]. :);;;",,,,,,,,,,,,,,,
Loading jobs UI page takes 40 seconds,SPARK-31967,13310840,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,11/Jun/20 07:58,14/Jun/20 19:21,13/Jul/23 08:46,13/Jun/20 00:36,2.4.6,3.0.1,,,,,,,,,,,2.4.7,3.0.1,3.1.0,Web UI,,,,0,,,"In the latest master branch, I find that the job list page becomes very slow.
To reproduce in local setup:

{code:java}
spark.read.parquet(""/tmp/p1"").createOrReplaceTempView(""t1"")
spark.read.parquet(""/tmp/p2"").createOrReplaceTempView(""t2"")
(1 to 1000).map(_ =>  spark.sql(""select * from t1, t2 where t1.value=t2.value"").show())
{code}

And that, open live UI: http://localhost:4040/
The loading time is about 40 seconds.

If we comment out the function call for `drawApplicationTimeline`, then the loading time is around 1 second.
",,apachespark,Gengliang.Wang,rakson,sarutak,smilegator,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31971,,,,,,"11/Jun/20 08:00;Gengliang.Wang;load_time.jpeg;https://issues.apache.org/jira/secure/attachment/13005457/load_time.jpeg","11/Jun/20 08:00;Gengliang.Wang;profile.png;https://issues.apache.org/jira/secure/attachment/13005456/profile.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 14 19:21:18 UTC 2020,,,,,,,,,,"0|z0fqxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/20 08:01;Gengliang.Wang;[~sarutak] could you take a look at it? It is very serious.;;;","11/Jun/20 08:12;sarutak;I'll investigate.;;;","11/Jun/20 08:31;sarutak;This issue can be related.

[https://github.com/visjs/vis-timeline/issues/379];;;","11/Jun/20 08:38;Gengliang.Wang;Thanks.
Also, not directly related to this topic, I don't think we need to render the timeline in the page unless the ""event timeline"" link is clicked..we can have follow-up improvements.;;;","11/Jun/20 09:47;sarutak;For now, I'm considering a paging solution like StagePage does.;;;","12/Jun/20 05:05;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/28806;;;","12/Jun/20 05:06;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/28806;;;","12/Jun/20 08:21;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/28811;;;","12/Jun/20 08:22;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/28811;;;","12/Jun/20 08:25;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/28813;;;","13/Jun/20 00:36;Gengliang.Wang;The issue is resoveld in https://github.com/apache/spark/pull/28806;;;","14/Jun/20 19:19;smilegator;Do we need to resolve the  same issue in 2.4.6?;;;","14/Jun/20 19:20;smilegator;nvm. This has been merged to 2.4 branch. ;;;","14/Jun/20 19:21;smilegator;[~Gengliang.Wang] Please also change the fix versions when you merged the PRs. ;;;",,,,,,,,,,
Run the tests in registerJavaFunction and registerJavaUDAF only when test classes are complied,SPARK-31965,13310803,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,11/Jun/20 03:08,12/Dec/22 17:50,13/Jul/23 08:46,11/Jun/20 04:16,2.4.6,3.0.0,,,,,,,,,,,3.0.0,,,PySpark,Tests,,,0,,,"If you do a plain package with sbt:

{code}
./build/sbt -DskipTests -Phive-thriftserver clean package
./run-tests --python-executable=python3 --testname=""pyspark.sql.udf UserDefinedFunction""
{code}

The doctests in registerJavaFunction and registerJavaUDAF fail because it requires some classes from the test compilation.

We should skip them conditionally",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 04:16:28 UTC 2020,,,,,,,,,,"0|z0fqpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/20 03:22;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28795;;;","11/Jun/20 03:23;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28795;;;","11/Jun/20 04:16;dongjoon;Issue resolved by pull request 28795
[https://github.com/apache/spark/pull/28795];;;",,,,,,,,,,,,,,,,,,,,,
Support both pandas 0.23 and 1.0,SPARK-31963,13310751,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,10/Jun/20 20:54,11/Jun/20 00:04,13/Jul/23 08:46,10/Jun/20 21:43,3.1.0,,,,,,,,,,,,3.1.0,,,PySpark,SQL,,,0,,,"This issue aims to fix a bug by supporting both pandas 0.23 and 1.0.

{code}
$ pip install pandas==0.23.2

$ python -c ""import pandas.CategoricalDtype""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ModuleNotFoundError: No module named 'pandas.CategoricalDtype'

$ python -c ""from pandas.api.types import CategoricalDtype""
{code}",,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25351,,,,,SPARK-31964,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 21:43:05 UTC 2020,,,,,,,,,,"0|z0fqe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/20 20:58;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28789;;;","10/Jun/20 20:59;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28789;;;","10/Jun/20 21:43;dongjoon;Issue resolved by pull request 28789
[https://github.com/apache/spark/pull/28789];;;",,,,,,,,,,,,,,,,,,,,,
"Test failure ""RebaseDateTimeSuite.optimization of micros rebasing - Gregorian to Julian""",SPARK-31959,13310719,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,10/Jun/20 17:53,16/Jun/20 05:21,13/Jul/23 08:46,15/Jun/20 15:09,3.0.1,3.1.0,,,,,,,,,,,3.0.1,3.1.0,,SQL,,,,0,,,See https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/123688/testReport/org.apache.spark.sql.catalyst.util/RebaseDateTimeSuite/optimization_of_micros_rebasing___Gregorian_to_Julian/,,apachespark,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 15:09:28 UTC 2020,,,,,,,,,,"0|z0fq74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/20 18:22;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28787;;;","12/Jun/20 07:56;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28809;;;","15/Jun/20 08:46;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28832;;;","15/Jun/20 08:46;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28832;;;","15/Jun/20 15:09;dongjoon;Issue resolved by pull request 28832
[https://github.com/apache/spark/pull/28832];;;",,,,,,,,,,,,,,,,,,,
normalize special floating numbers in subquery,SPARK-31958,13310689,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,10/Jun/20 15:18,11/Jun/20 06:39,13/Jul/23 08:46,11/Jun/20 06:39,3.0.0,,,,,,,,,,,,3.0.1,3.1.0,,SQL,,,,0,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 06:39:50 UTC 2020,,,,,,,,,,"0|z0fq0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/20 15:25;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/28785;;;","11/Jun/20 06:39;cloud_fan;Issue resolved by pull request 28785
[https://github.com/apache/spark/pull/28785];;;",,,,,,,,,,,,,,,,,,,,,,
Do not fail if there is no ambiguous self join,SPARK-31956,13310672,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,10/Jun/20 13:51,06/Aug/20 18:04,13/Jul/23 08:46,10/Jun/20 20:12,3.0.0,,,,,,,,,,,,3.0.1,3.1.0,,SQL,,,,0,,,,,apachespark,cloud_fan,dongjoon,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32551,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 02:17:03 UTC 2020,,,,,,,,,,"0|z0fpwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/20 13:53;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/28783;;;","10/Jun/20 20:12;dongjoon;Issue resolved by pull request 28783
[https://github.com/apache/spark/pull/28783];;;","10/Jun/20 20:15;kabhwan;Probably the correct fix version would be 3.0.1, as 3.0.0 RC3 vote is already passed.;;;","11/Jun/20 02:17;dongjoon;Thanks, [~kabhwan].;;;",,,,,,,,,,,,,,,,,,,,
Beeline discard the last line of the sql file when submited to  thriftserver via beeline,SPARK-31955,13310668,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,denglg,denglg,10/Jun/20 13:48,12/Dec/22 18:11,13/Jul/23 08:46,16/Jun/20 10:31,2.3.4,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"I submitted a sql file on beeline and the result returned is wrong. After many tests, it was found that the sql executed by Spark would discard the last line.This should be beeline's bug parsing sql file.",,denglg,dengzh,dongjoon,younggyuchun,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30034,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 11:02:43 UTC 2020,,,,,,,,,,"0|z0fpvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/20 14:43;younggyuchun;[~denglg] can you add screenshots or examples that discard the last line and the SQL file you are testing?;;;","10/Jun/20 15:28;denglg;{code:java}
// code placeholder
0: jdbc:hive2://hadoop.spark-sql.hadoo> select * from info_dev.beeline_test;
+-----+-------+--+
| id  | name  |
+-----+-------+--+
| 3   | ccc   |
| 2   | bbb   |
| 1   | aaa   |
+-----+-------+--+
3 rows selected (1.402 seconds)
{code}
Then sql file as bellows,
{code:java}
// code placeholder
[test@192.168.0.1 denglg]$ cat -A test2.sql 
select * from info_dev.beeline_test$
where name='bbb';[test@192.168.0.1 denglg]$ 
{code}
Result as bellows,
{code:java}
// code placeholder
0: jdbc:hive2://spark-sql.hadoo> select * from info_dev.beeline_test
0: jdbc:hive2://spark-sql.hadoo> where name='bbb';+-----+-------+--+
| id  | name  |
+-----+-------+--+
| 3   | ccc   |
| 2   | bbb   |
| 1   | aaa   |
+-----+-------+--+
3 rows selected (1.594 seconds)
{code}
As you can see，it got wrong result.;;;","10/Jun/20 17:50;younggyuchun;Try this rather than add a space between select statement and where statement:
select * from info_dev.beeline_test where name='bbb';;;","11/Jun/20 01:12;denglg;[~younggyuchun] No results.

 
{code:java}
// code placeholder
[test@192.168.0.1 denglg]$ cat -A test3.sql 
select * from info_dev.beeline_test where name='bbb'[test@192.168.0.1 denglg]$ 
{code}
{code:java}
// code placeholder
0: jdbc:hive2://spark-sql.hadoo> select * from info_dev.beeline_test where name='bbb'
Closing: 0: jdbc:hive2://spark-sql.hadoop.srv:10000/;principal=xxx?mapreduce.job.queuename=xxx
[test@192.168.0.1 denglg]$
{code}
 

 ;;;","11/Jun/20 12:33;younggyuchun;hmm... I will look into this;;;","11/Jun/20 14:36;younggyuchun;[~denglg] 

 

I cannot reproduce this locally. : 


Whole data:
{code:java}
0: jdbc:hive2://localhost:10000> select * from info_dev.beeline_test;
+------------------+--------------------+--+
| beeline_test.id  | beeline_test.name  |
+------------------+--------------------+--+
| 1                | aaa                |
| 2                | bbb                |
| 3                | ccc                |
| 1                | aaa                |
+------------------+--------------------+--+
4 rows selected (0.239 seconds)
0: jdbc:hive2://localhost:10000>
{code}
 

test2.sql:
{code:java}
jun562@CHUNYLT:~/spark-2.4.4-bin-hadoop2.7/bin$ cat test2.sql
select * from info_dev.beeline_test where name='bbb'
jun562@CHUNYLT:~/spark-2.4.4-bin-hadoop2.7/bin$
{code}
 

Execute a test2.sql file on Beeline by running a ""run"" command:
{code:java}
Connected to: Apache Hive (version 1.2.1)
Driver: Hive JDBC (version 1.2.1.spark2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://localhost:10000> !run /home/jun562/apache-hive-1.2.1-bin/bin/test2.sql
>>>  select * from info_dev.beeline_test where name='bbb';
+------------------+--------------------+--+
| beeline_test.id  | beeline_test.name  |
+------------------+--------------------+--+
| 2                | bbb                |
+------------------+--------------------+--+
1 row selected (0.406 seconds)
0: jdbc:hive2://localhost:10000>
{code}
 

Execute SQL on Beeline:
{code:java}
0: jdbc:hive2://localhost:10000> select * from info_dev.beeline_test where name='bbb';
+------------------+--------------------+--+
| beeline_test.id  | beeline_test.name  |
+------------------+--------------------+--+
| 2                | bbb                |
+------------------+--------------------+--+
1 row selected (0.233 seconds)
0: jdbc:hive2://localhost:10000>
{code}
 

cc [~dongjoon] [~hyukjin.kwon] [~srowen];;;","11/Jun/20 17:32;dongjoon;[~denglg]. Please add a new line character at the end of the line. Your script doesn't have it.
Thank you for investigating, [~younggyuchun].;;;","12/Jun/20 00:54;denglg;[~dongjoon] As  you said, EOL is the key to the problem. Sometimes, there is no newline character when SQL is submitted, or the newline character is removed by the third-party component. Maybe, spark or beeline should correctly parse SQL, whether or not there is EOL. Otherwise, users will be bothered by the wrong result.;;;","12/Jun/20 03:33;gurwls223;[~denglg] Please show the __exact__ reproducer. From reading [~younggyuchun], it doesn't look clear what issue you mean.
Also please check if the behaviours are consistent with beeline in Hive. If it also exists in Hive, this isn't a Spark specific issue.;;;","12/Jun/20 03:34;gurwls223;I am going to leave it resolved until enough information is provided to analyze further, for JIRA management purpose.;;;","12/Jun/20 03:49;yumwang;It seems this is a Hive issue. Maybe we should fix it on the Hive side.;;;","12/Jun/20 03:55;denglg;[~hyukjin.kwon] , the  difference between me and [~younggyuchun] 's sql file is that there is no EOL at the end of my script.

My example is the exact reproducer. ;;;","16/Jun/20 10:41;yumwang;The issue fixed by upgrading the beeline to 2.3.7. How to reproduce this issue:

*Prepare table*
{code:sql}
create table test_beeline using parquet as select id from range(5);
{code}

*Prepare SQL*
{code:sql}
echo -en ""select * from test_beeline\n where id=2;"" >> test.sql
{code}

*Spark 2.4*:
{noformat}
[root@spark-3267648 spark-2.4.4-bin-hadoop2.7]# bin/beeline -u ""jdbc:hive2://localhost:10000"" -f /root/spark-3.0.0-bin-hadoop3.2/test.sql
Connecting to jdbc:hive2://localhost:10000
log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Connected to: Spark SQL (version 3.0.0)
Driver: Hive JDBC (version 1.2.1.spark2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://localhost:10000> select * from test_beeline
0: jdbc:hive2://localhost:10000>  where id=2;+-----+--+
| id  |
+-----+--+
| 0   |
| 2   |
| 1   |
| 3   |
| 4   |
+-----+--+
5 rows selected (5.622 seconds)
0: jdbc:hive2://localhost:10000>  where id=2;
Closing: 0: jdbc:hive2://localhost:10000
{noformat}

*Spark 3.0*:
{noformat}
[root@spark-3267648 spark-3.0.0-bin-hadoop3.2]# bin/beeline -u ""jdbc:hive2://localhost:10000"" -f test.sql
log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Connecting to jdbc:hive2://localhost:10000
Connected to: Spark SQL (version 3.0.0)
Driver: Hive JDBC (version 2.3.7)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://localhost:10000> select * from test_beeline
. . . . . . . . . . . . . . . .>  where id=2;
+-----+
| id  |
+-----+
| 2   |
+-----+
1 row selected (7.749 seconds)
0: jdbc:hive2://localhost:10000> Closing: 0: jdbc:hive2://localhost:10000
{noformat}



;;;","16/Jun/20 10:48;dengzh;The issue seems have been fixed in  [HIVE-10541|https://issues.apache.org/jira/browse/HIVE-10541].;;;","16/Jun/20 11:02;denglg;[~dengzh], the version of beeline in spark2.4 and before is 1.2.1,  spark3.0 upgrade the beeline to 2.3.7. 

For spark，The issue was fixed in spark3.0.;;;",,,,,,,,,
The metric of MemoryBytesSpill is incorrect when doing Aggregate,SPARK-31952,13310597,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Ngone51,EdisonWang,EdisonWang,10/Jun/20 08:34,12/Jan/21 05:28,13/Jul/23 08:46,11/Jan/21 07:16,3.0.0,,,,,,,,,,,,3.0.2,3.1.1,,SQL,,,,0,,,"When doing Aggregate and spill occurs, the Spill(memory) metric is zero while Spill(disk) metric is right. As shown below

!image-2020-06-10-16-35-58-002.png!",,apachespark,cloud_fan,EdisonWang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/20 08:35;EdisonWang;image-2020-06-10-16-35-58-002.png;https://issues.apache.org/jira/secure/attachment/13005331/image-2020-06-10-16-35-58-002.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 12 03:08:49 UTC 2021,,,,,,,,,,"0|z0fpg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/20 09:05;apachespark;User 'WangGuangxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/28780;;;","10/Jun/20 09:06;apachespark;User 'WangGuangxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/28780;;;","05/Jan/21 08:53;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/31035;;;","05/Jan/21 08:54;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/31035;;;","11/Jan/21 07:16;cloud_fan;Issue resolved by pull request 31035
[https://github.com/apache/spark/pull/31035];;;","12/Jan/21 03:08;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/31140;;;",,,,,,,,,,,,,,,,,,
Handling the exception in SparkUI for getSparkUser method,SPARK-31941,13310352,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,saurabhc100,saurabhc100,saurabhc100,09/Jun/20 09:46,10/Jun/20 08:01,13/Jul/23 08:46,10/Jun/20 07:53,2.4.6,3.0.0,3.1.0,,,,,,,,,,2.4.7,3.0.1,3.1.0,Spark Core,,,,0,,,"After SPARK-31632 SparkException is thrown from  def applicationInfo(

{code:java}
  def applicationInfo(): v1.ApplicationInfo = {
    try {
      // The ApplicationInfo may not be available when Spark is starting up.
      store.view(classOf[ApplicationInfoWrapper]).max(1).iterator().next().info
    } catch {
      case _: NoSuchElementException =>
        throw new SparkException(""Failed to get the application information. "" +
          ""If you are starting up Spark, please wait a while until it's ready."")
    }
  }
{code}

Where as the caller for this method def getSparkUser in Spark UI is not handling SparkException in the catch


{code:java}
  def getSparkUser: String = {
    try {
      Option(store.applicationInfo().attempts.head.sparkUser)
        .orElse(store.environmentInfo().systemProperties.toMap.get(""user.name""))
        .getOrElse(""<unknown>"")
    } catch {
      case _: NoSuchElementException => ""<unknown>""
    }
  }
{code}

So On using this method (getSparkUser )we can get the application erred out.

So either we should thow


{code:java}
 throw new NoSuchElementException(""Failed to get the application information. "" +
          ""If you are starting up Spark, please wait a while until it's ready."")
{code}

or else add the scenario to catch spark exception in getSparkUser
case _: SparkException => ""<unknown>""


",,apachespark,sarutak,saurabhc100,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 08:01:35 UTC 2020,,,,,,,,,,"0|z0fnxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/20 09:50;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/28768;;;","09/Jun/20 09:51;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/28768;;;","10/Jun/20 08:01;sarutak;Fixed in https://github.com/apache/spark/pull/28768;;;",,,,,,,,,,,,,,,,,,,,,
Hadoop file system config should be effective in data source options ,SPARK-31935,13310242,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,08/Jun/20 23:16,02/Jul/20 04:07,13/Jul/23 08:46,11/Jun/20 21:18,2.4.6,3.0.0,,,,,,,,,,,2.4.7,3.0.1,3.1.0,SQL,,,,0,,,"Data source options should be propagated into the hadoop configuration of method `checkAndGlobPathIfNecessary`

From org.apache.hadoop.fs.FileSystem.java:
{code:java}
  public static FileSystem get(URI uri, Configuration conf) throws IOException {
    String scheme = uri.getScheme();
    String authority = uri.getAuthority();

    if (scheme == null && authority == null) {     // use default FS
      return get(conf);
    }

    if (scheme != null && authority == null) {     // no authority
      URI defaultUri = getDefaultUri(conf);
      if (scheme.equals(defaultUri.getScheme())    // if scheme matches default
          && defaultUri.getAuthority() != null) {  // & default has authority
        return get(defaultUri, conf);              // return default
      }
    }
    
    String disableCacheName = String.format(""fs.%s.impl.disable.cache"", scheme);
    if (conf.getBoolean(disableCacheName, false)) {
      return createFileSystem(uri, conf);
    }

    return CACHE.get(uri, conf);
  }
{code}

With this, we can specify URI schema and authority related configurations for scanning file systems.
",,apachespark,dongjoon,Gengliang.Wang,jiajia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 02 04:07:37 UTC 2020,,,,,,,,,,"0|z0fn94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/20 23:26;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/28760;;;","09/Jun/20 19:46;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/28771;;;","09/Jun/20 19:47;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/28771;;;","10/Jun/20 04:47;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/28776;;;","10/Jun/20 23:46;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28791;;;","11/Jun/20 03:29;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/28796;;;","11/Jun/20 21:18;dongjoon;Issue resolved by pull request 28776
[https://github.com/apache/spark/pull/28776];;;","29/Jun/20 15:15;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/28948;;;","29/Jun/20 15:16;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/28948;;;","02/Jul/20 04:06;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/28973;;;","02/Jul/20 04:07;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/28973;;;",,,,,,,,,,,,,
"local cache size exceeding ""spark.history.store.maxDiskUsage"" triggered ""java.io.IOException"" in history server on Windows",SPARK-31929,13310153,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Zhen Li,Zhen Li,Zhen Li,08/Jun/20 14:35,28/Aug/21 01:10,13/Jul/23 08:46,16/Jun/20 18:00,2.4.4,3.0.0,,,,,,,,,,,3.1.0,,,Web UI,,,,0,,,"h2.  
h2. HTTP ERROR 500

Problem accessing /history/app-20190711215551-0001/stages/. Reason:

Server Error

 
h3. Caused by:

java.io.IOException: Unable to delete file: d:\cache_hs\apps\app-20190711215551-0001.ldb\MANIFEST-000007 at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2381) at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1679) at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1575) at org.apache.spark.deploy.history.HistoryServerDiskManager.org$apache$spark$deploy$history$HistoryServerDiskManager$$deleteStore(HistoryServerDiskManager.scala:198) at org.apache.spark.deploy.history.HistoryServerDiskManager.$anonfun$release$1(HistoryServerDiskManager.scala:161) at scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23) at scala.Option.foreach(Option.scala:407) at org.apache.spark.deploy.history.HistoryServerDiskManager.release(HistoryServerDiskManager.scala:156) at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$loadDiskStore$1(FsHistoryProvider.scala:1163) at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$loadDiskStore$1$adapted(FsHistoryProvider.scala:1157) at scala.Option.foreach(Option.scala:407) at org.apache.spark.deploy.history.FsHistoryProvider.loadDiskStore(FsHistoryProvider.scala:1157) at org.apache.spark.deploy.history.FsHistoryProvider.getAppUI(FsHistoryProvider.scala:363) at org.apache.spark.deploy.history.HistoryServer.getAppUI(HistoryServer.scala:191) at org.apache.spark.deploy.history.ApplicationCache.$anonfun$loadApplicationEntry$2(ApplicationCache.scala:163) at org.apache.spark.deploy.history.ApplicationCache.time(ApplicationCache.scala:135) at org.apache.spark.deploy.history.ApplicationCache.org$apache$spark$deploy$history$ApplicationCache$$loadApplicationEntry(ApplicationCache.scala:161) at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:56) at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:52) at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599) at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379) at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342) at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257) at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000) at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004) at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874) at org.apache.spark.deploy.history.ApplicationCache.get(ApplicationCache.scala:89) at org.apache.spark.deploy.history.ApplicationCache.withSparkUI(ApplicationCache.scala:101) at org.apache.spark.deploy.history.HistoryServer.org$apache$spark$deploy$history$HistoryServer$$loadAppUi(HistoryServer.scala:248) at org.apache.spark.deploy.history.HistoryServer$$anon$1.doGet(HistoryServer.scala:101) at javax.servlet.http.HttpServlet.service(HttpServlet.java:687) at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:873) at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1623) at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95) at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610) at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:540) at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255) at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1345) at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203) at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480) at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201) at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247) at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144) at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:753) at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220) at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132) at org.sparkproject.jetty.server.Server.handle(Server.java:505) at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:370) at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:267) at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305) at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:103) at org.sparkproject.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117) at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333) at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310) at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168) at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126) at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366) at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698) at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804) at java.lang.Thread.run(Thread.java:748)
----
[Powered by Jetty:// 9.4.z-SNAPSHOT|http://eclipse.org/jetty]
----","System: Windows

Config: 

spark.history.retainedApplications 200

spark.history.store.maxDiskUsage 2g

spark.history.store.path d://cache_hs",apachespark,Zhen Li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-36603,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 18:00:50 UTC 2020,,,,,,,,,,"0|z0fmpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/20 10:06;apachespark;User 'zhli1142015' has created a pull request for this issue:
https://github.com/apache/spark/pull/28769;;;","09/Jun/20 10:06;apachespark;User 'zhli1142015' has created a pull request for this issue:
https://github.com/apache/spark/pull/28769;;;","16/Jun/20 18:00;srowen;Resolved by https://github.com/apache/spark/pull/28769;;;",,,,,,,,,,,,,,,,,,,,,
Summary.totalIterations greater than maxIters,SPARK-31925,13310037,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,huaxingao,podongfeng,podongfeng,08/Jun/20 07:05,15/Jun/20 13:49,13/Jul/23 08:46,15/Jun/20 13:49,2.4.6,3.0.0,,,,,,,,,,,3.1.0,,,ML,PySpark,,,0,release-notes,,"I am not sure whether it is a bug, but if we set *maxIter=n* in LiR/LiR/etc, the model.summary.totalIterations will return *n+1* if the training procedure does not drop out.

 

friendly ping [~huaxingao]",,apachespark,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,"Before Spark 3.1, if maxIter=n in LogisticRegression and LinearRegression, then the result summary's totalIterations returns n+1. It now correctly returns n, in 3.1. The objective history is unchanged.",false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 13:49:41 UTC 2020,,,,,,,,,,"0|z0flzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/20 03:16;podongfeng;also ping [~srowen]  [~weichenxu123];;;","10/Jun/20 03:18;srowen;Sounds like a small bug, unless one of the iterations is somehow an initial state.;;;","10/Jun/20 18:09;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/28786;;;","15/Jun/20 13:47;srowen;Just for completeness, I'll add release notes for this change, though it's essentially a bug fix.;;;","15/Jun/20 13:49;srowen;Resolved by https://github.com/apache/spark/pull/28786;;;",,,,,,,,,,,,,,,,,,,
Event log cannot be generated when some internal accumulators use unexpected types,SPARK-31923,13309903,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,06/Jun/20 23:12,12/Apr/21 18:38,13/Jul/23 08:46,08/Jun/20 23:54,2.3.0,2.3.1,2.3.2,2.3.3,2.3.4,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,2.4.5,2.4.6,2.4.7,3.0.1,3.1.0,Spark Core,,,,0,,,"A user may use internal accumulators by adding the ""internal.metrics."" prefix to the accumulator name to hide sensitive information from UI (Accumulators except internal ones will be shown in Spark UI).

However, *org.apache.spark.util.JsonProtocol.accumValueToJson* assumes an internal accumulator has only 3 possible types: int, long, and java.util.List[(BlockId, BlockStatus)]. When an internal accumulator uses an unexpected type, it will crash.

An event log that contains such accumulator will be dropped because it cannot be converted to JSON, and it will cause weird UI issue when rendering in Spark History Server. For example, if `SparkListenerTaskEnd` is dropped because of this issue, the user will see the task is still running even if it was finished.

It's better to make *accumValueToJson* more robust.

----
How to reproduce it:

- Enable Spark event log
- Run the following command:

{code}
scala> val accu = sc.doubleAccumulator(""internal.metrics.foo"")
accu: org.apache.spark.util.DoubleAccumulator = DoubleAccumulator(id: 0, name: Some(internal.metrics.foo), value: 0.0)

scala> sc.parallelize(1 to 1, 1).foreach { _ => accu.add(1.0) }
20/06/06 16:11:27 ERROR AsyncEventQueue: Listener EventLoggingListener threw an exception
java.lang.ClassCastException: java.lang.Double cannot be cast to java.util.List
	at org.apache.spark.util.JsonProtocol$.accumValueToJson(JsonProtocol.scala:330)
	at org.apache.spark.util.JsonProtocol$$anonfun$accumulableInfoToJson$3.apply(JsonProtocol.scala:306)
	at org.apache.spark.util.JsonProtocol$$anonfun$accumulableInfoToJson$3.apply(JsonProtocol.scala:306)
	at scala.Option.map(Option.scala:146)
	at org.apache.spark.util.JsonProtocol$.accumulableInfoToJson(JsonProtocol.scala:306)
	at org.apache.spark.util.JsonProtocol$$anonfun$accumulablesToJson$2.apply(JsonProtocol.scala:299)
	at org.apache.spark.util.JsonProtocol$$anonfun$accumulablesToJson$2.apply(JsonProtocol.scala:299)
	at scala.collection.immutable.List.map(List.scala:284)
	at org.apache.spark.util.JsonProtocol$.accumulablesToJson(JsonProtocol.scala:299)
	at org.apache.spark.util.JsonProtocol$.taskInfoToJson(JsonProtocol.scala:291)
	at org.apache.spark.util.JsonProtocol$.taskEndToJson(JsonProtocol.scala:145)
	at org.apache.spark.util.JsonProtocol$.sparkEventToJson(JsonProtocol.scala:76)
	at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:138)
	at org.apache.spark.scheduler.EventLoggingListener.onTaskEnd(EventLoggingListener.scala:158)
	at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:45)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:91)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)
{code}

 ",,apachespark,jystephan,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 12 18:38:53 UTC 2021,,,,,,,,,,"0|z0fl5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/20 23:23;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/28744;;;","08/Jun/20 20:41;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/28758;;;","12/Apr/21 17:44;jystephan;Hi [~zsxwing] :) 

Despite your patch, we're running in the same issue while using Spark 3.0.1. The stack trace is informative (for the line numbers we need to refer to this file [https://github.com/apache/spark/blob/v3.0.1/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala#L354]).

The problem is that we're given a class (java.util.Collections$SynchronizedSet) that enters the branch
{code:java}
case v: java.util.List[_] =>
{code}
but on the next line the cast v.asScala.toList fails. 

 
Question: Is there a workaround available through spark configurations? E.g. a flag to disable the metrics being collected here?



```

21/04/12 15:11:40 ERROR AsyncEventQueue: Listener EventLoggingListener threw an exception

java.lang.ClassCastException: java.util.Collections$SynchronizedSet cannot be cast to java.util.List at org.apache.spark.util.JsonProtocol$.accumValueToJson(JsonProtocol.scala:355) at org.apache.spark.util.JsonProtocol$.$anonfun$accumulableInfoToJson$4(JsonProtocol.scala:331) at scala.Option.map(Option.scala:230) at org.apache.spark.util.JsonProtocol$.accumulableInfoToJson(JsonProtocol.scala:331) at org.apache.spark.util.JsonProtocol$.$anonfun$accumulablesToJson$3(JsonProtocol.scala:324) at scala.collection.immutable.List.map(List.scala:290) at org.apache.spark.util.JsonProtocol$.accumulablesToJson(JsonProtocol.scala:324) at org.apache.spark.util.JsonProtocol$.taskInfoToJson(JsonProtocol.scala:316) at org.apache.spark.util.JsonProtocol$.taskEndToJson(JsonProtocol.scala:151) at org.apache.spark.util.JsonProtocol$.sparkEventToJson(JsonProtocol.scala:79) at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:97) at org.apache.spark.scheduler.EventLoggingListener.onTaskEnd(EventLoggingListener.scala:119) at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:45) at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28) 

```

 ;;;","12/Apr/21 18:02;zsxwing;Do you have a reproduction? It's weird to see `java.util.Collections$SynchronizedSet cannot be cast to java.util.List` since before calling `v.asScala.toList`, the pattern match `case v: java.util.List[_]` should not accept `java.util.Collections$SynchronizedSet`.;;;","12/Apr/21 18:38;jystephan;After further investigation, I confirmed this is a red herring - our end user was using Spark 3.0.0 after all. Thanks for checking. ;;;",,,,,,,,,,,,,,,,,,,
"""RpcEnv already stopped"" error when exit spark-shell with local-cluster mode",SPARK-31922,13309875,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,06/Jun/20 15:12,22/Jul/20 06:17,13/Jul/23 08:46,22/Jul/20 06:17,2.4.6,3.0.0,,,,,,,,,,,3.1.0,,,Spark Core,,,,0,,,"There's always an error from TransportRequestHandler when exiting spark-shell under local-cluster mode:

 
{code:java}
20/06/06 23:08:29 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.20/06/06 23:08:29 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped. at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:167) at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:150) at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:691) at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111) at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140) at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.lang.Thread.run(Thread.java:748)20/06/06 23:08:29 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped. at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:167) at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:150) at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:691) at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253) at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111) at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140) at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.lang.Thread.run(Thread.java:748)
{code}",,apachespark,dongjoon,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32236,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 22 06:17:04 UTC 2020,,,,,,,,,,"0|z0fkzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/20 15:12;Ngone51;I am working on this.;;;","07/Jun/20 07:34;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/28746;;;","07/Jun/20 07:35;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/28746;;;","22/Jul/20 06:17;dongjoon;Issue resolved by pull request 28746
[https://github.com/apache/spark/pull/28746];;;",,,,,,,,,,,,,,,,,,,,
"Wrong warning of ""WARN Master: App app-xxx requires more resource than any of Workers could have.""",SPARK-31921,13309873,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,06/Jun/20 14:24,09/Jun/20 16:23,13/Jul/23 08:46,09/Jun/20 16:21,3.0.0,,,,,,,,,,,,3.0.1,3.1.0,,Spark Core,,,,0,,,"When starting spark-shell using local cluster mode, e.g. ./bin/spark-shell --master ""local-cluster[2, 1, 1024]"", there will be a warning:

 
{code:java}
20/06/06 22:09:09 WARN Master: App app-20200606220908-0000 requires more resource than any of Workers could have.
{code}
which means the application can not get enough resources to launch at least one executor.

But that's not true since we can successfully complete a job.",,apachespark,dongjoon,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27371,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 16:21:30 UTC 2020,,,,,,,,,,"0|z0fkz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/20 14:45;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/28742;;;","06/Jun/20 14:46;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/28742;;;","09/Jun/20 16:21;dongjoon;Issue resolved by pull request 28742
[https://github.com/apache/spark/pull/28742];;;",,,,,,,,,,,,,,,,,,,,,
SparkR CRAN check gives a warning with R 4.0.0 on OSX,SPARK-31918,13309823,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gurwls223,shivaram,shivaram,06/Jun/20 02:58,12/Dec/22 18:10,13/Jul/23 08:46,24/Jun/20 02:04,2.4.6,3.0.0,,,,,,,,,,,2.4.7,3.0.1,3.1.0,SparkR,,,,0,,,"When the SparkR package is run through a CRAN check (i.e. with something like R CMD check --as-cran ~/Downloads/SparkR_2.4.6.tar.gz), we rebuild the SparkR vignette as a part of the checks.

However this seems to be failing with R 4.0.0 on OSX -- both on my local machine and on CRAN https://cran.r-project.org/web/checks/check_results_SparkR.html

cc [~felixcheung]",,apachespark,dongjoon,michaelchirico,shivaram,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32073,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 10 19:41:52 UTC 2020,,,,,,,,,,"0|z0fko0:",9223372036854775807,,,,,,,,,,,,,3.0.1,,,,,,,,,,"22/Jun/20 14:10;gurwls223;It affects Spark 3.0 too, and seems failing with a different message in my local:

{code}
* creating vignettes ... ERROR
--- re-building ‘sparkr-vignettes.Rmd’ using rmarkdown
Warning in engine$weave(file, quiet = quiet, encoding = enc) :
  Pandoc (>= 1.12.3) and/or pandoc-citeproc not available. Falling back to R Markdown v1.

Attaching package: 'SparkR'

The following objects are masked from 'package:stats':

    cov, filter, lag, na.omit, predict, sd, var, window

The following objects are masked from 'package:base':

    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,
    rank, rbind, sample, startsWith, subset, summary, transform, union

Picked up _JAVA_OPTIONS: -XX:-UsePerfData
Picked up _JAVA_OPTIONS: -XX:-UsePerfData
20/06/22 15:07:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).

[Stage 0:>                                                          (0 + 1) / 1]
20/06/22 15:07:43 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.spark.SparkException: R unexpectedly exited.
R worker produced errors: Error in lapply(part, FUN) : attempt to bind a variable to R_UnboundValue
{code}

Assuming the errors from R execution itself, the root cause might be same.;;;","22/Jun/20 14:50;gurwls223;Just to share what I investigated:

Seems the problem relates to {{processClosure}} via {{cleanClosure}} in SparkR.
 Looks like there is a problem [when the new environment is set to a function|https://github.com/apache/spark/blob/master/R/pkg/R/utils.R#L601] especially that includes generic S4 functions, given my observation.
 So, for example, if you skip it with the fix below:
{code:java}
diff --git a/R/pkg/R/utils.R b/R/pkg/R/utils.R
index 65db9c21d9d..60cad588f5e 100644
--- a/R/pkg/R/utils.R
+++ b/R/pkg/R/utils.R
@@ -529,7 +529,9 @@ processClosure <- function(node, oldEnv, defVars, checkedFuncs, newEnv) {
         # Namespaces other than ""SparkR"" will not be searched.
         if (!isNamespace(func.env) ||
             (getNamespaceName(func.env) == ""SparkR"" &&
-               !(nodeChar %in% getNamespaceExports(""SparkR"")))) {
+               !(nodeChar %in% getNamespaceExports(""SparkR"")) &&
+                  # Skip all generics under SparkR - R 4.0.0 looks having an issue.
+                  !isGeneric(nodeChar, func.env))) {
{code}
{code:java}
* checking re-building of vignette outputs ... OK
{code}
CRAN check passes with the current master branch in my local

For a minimal reproducer, with this diff:
{code:java}
diff --git a/R/pkg/R/RDD.R b/R/pkg/R/RDD.R
index 7a1d157bb8a..89250c37319 100644
--- a/R/pkg/R/RDD.R
+++ b/R/pkg/R/RDD.R
@@ -487,6 +487,7 @@ setMethod(""lapply"",
             func <- function(partIndex, part) {
               lapply(part, FUN)
             }
+            print(SparkR:::cleanClosure(func)(1, 2))
             lapplyPartitionsWithIndex(X, func)
           })
{code}
run:
{code:java}
createDataFrame(lapply(seq(100), function (e) list(value=e)))
{code}
When {{lapply}} is called against the RDD at {{createDataFrame}}, the cleaned closure's environment has SparkR's lapply as a S4 method and it leads to the error such as {{attempt to bind a variable to R_UnboundValue}}.

Hopefully this is the cause of the issue happening here, and not an issue in my env. cc [~felixcheung], [~dongjoon] FYI.;;;","22/Jun/20 22:43;shivaram;Thanks [~hyukjin.kwon]. It looks like there is another problem. From what I saw today, R 4.0.0 cannot load packages that were built with R 3.6.0.  Thus when SparkR workers try to start up with the pre-built SparkR package we see a failure.  I'm not really sure what is a good way to handle this. Options include
- Building the SparkR package using 4.0.0 (need to check if that works with R 3.6)
- Copy the package from the driver (where it is usually built) and make the SparkR workers use the package installed on the driver

Any other ideas?;;;","23/Jun/20 00:05;gurwls223;Ah, yeah. That one I read [it in the release notes|https://cran.r-project.org/doc/manuals/r-devel/NEWS.html]
I was freshly building and testing the package with R 4.0.1 so that was why the error messages were different ...

{quote}
> Packages need to be (re-)installed under this version (4.0.0) of *R*.
{quote}

I have two environments in my local. One is R 4.0.1, the other one is R 3.4.0. Although it officially says R 3.1+, we deprecated R < 3.4 at SPARK-26014.
I will test the first option out, and come back.

BTW, would you be able to test it out with a fresh build with R 4.0.0? If the issue I faced isn't my env issue, it looks tricky to handle ... [~dongjoon] do you have an existing SparkR dev env to test with R 4.0?;;;","23/Jun/20 01:33;dongjoon;Unfortunately, no~ I downgraded to R 3.5.2 on both my MacPro and MacBook.;;;","23/Jun/20 01:58;shivaram;[~hyukjin.kwon] I have R 4.0.2 and will try to do a fresh build from source of Spark 3.0.0 ;;;","23/Jun/20 02:36;shivaram;I can confirm that with build from source of Spark 3.0.0 and R 4.0.2, I see the following error while building vignettes.

{{R worker produced errors: Error in lapply(part, FUN) : attempt to bind a variable to R_UnboundValue}};;;","23/Jun/20 04:06;gurwls223;Okay, [~shivaram], the first option seems working although it shows a warning such as below. I built Spark 3.0.0 with R 4.0.1, and manually downgraded to R 3.6.3.

{code:java}
During startup - Warning message:
package ‘SparkR’ was built under R version 4.0.1
{code}

I removed unrelated comments I left above.;;;","23/Jun/20 04:26;shivaram;Thats great! [~hyukjin.kwon] -- so we can get around the installation issue if we can build on R 4.0.0. However I guess we will still have the the serialization issue. BTW does the serialization issue go away if we build in R 4.0.0 and run with R 3.6.3? 
;;;","23/Jun/20 04:30;gurwls223;I tested it manually with the fix I mentioned [here|https://issues.apache.org/jira/browse/SPARK-31918?focusedCommentId=17142127&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17142127] .. let me test that case too.

BTW, I just roughly tested instead of running the full tests. Some corner cases might not work when running SparkR built by R 4.0.1 on R 3.6.3.

Let me test a bit more closely and share the results later.;;;","23/Jun/20 06:30;gurwls223;With SparkR built by R 4.0.1 on R 3.6.3  as is, tests pass with one test failure, which I think it's not a big deal:

{code}
Warning message:
package ‘SparkR’ was built under R version 4.0.1
Spark package found in SPARK_HOME: /.../spark
══ testthat results  ═══════════════════════════════════════════════════════════
[ OK: 13 | SKIPPED: 0 | WARNINGS: 0 | FAILED: 0 ]
✔ |  OK F W S | Context
✔ |  11       | binary functions [3.7 s]
✔ |   4       | functions on binary files [3.7 s]
✔ |   2       | broadcast variables [0.8 s]
✔ |   5       | functions in client.R
✔ |  46       | test functions in sparkR.R [10.1 s]
✔ |   2       | include R packages [0.5 s]
✔ |   2       | JVM API [0.3 s]
✔ |  70       | MLlib classification algorithms, except for tree-based algorithms [93.1 s]
✔ |  70       | MLlib clustering algorithms [38.8 s]
✔ |   6       | MLlib frequent pattern mining [3.0 s]
✔ |   8       | MLlib recommendation algorithms [9.9 s]
✔ | 128       | MLlib regression algorithms, except for tree-based algorithms [63.9 s]
✔ |   8       | MLlib statistics algorithms [0.5 s]
✔ |  94       | MLlib tree-based algorithms [81.2 s]
✔ |  29       | parallelize() and collect() [0.5 s]
✔ | 428       | basic RDD functions [21.1 s]
✔ |  39       | SerDe functionality [2.1 s]
✔ |  20       | partitionBy, groupByKey, reduceByKey etc. [3.3 s]
✔ |   4       | functions in sparkR.R
✔ |  16       | SparkSQL Arrow optimization [20.3 s]
✔ |   6       | test show SparkDataFrame when eager execution is enabled. [1.3 s]
✖ | 1172 1     | SparkSQL functions [156.4 s]
────────────────────────────────────────────────────────────────────────────────
test_sparkSQL.R:2719: error: mutate(), transform(), rename() and names()
could not find function ""deparse1""
Backtrace:
 1. base::attach(airquality) tests/fulltests/test_sparkSQL.R:2719:2
 2. base::attach(airquality)
────────────────────────────────────────────────────────────────────────────────
✔ |  42       | Structured Streaming [520.2 s]
✔ |  16       | tests RDD function take() [0.9 s]
✔ |  14       | the textFile() function [2.6 s]
✔ |  46       | functions in utils.R [0.5 s]
✔ |   0     1 | Windows-specific tests
────────────────────────────────────────────────────────────────────────────────
test_Windows.R:22: skip: sparkJars tag in SparkContext
Reason: This test is only for Windows, skipped
────────────────────────────────────────────────────────────────────────────────

══ Results ═════════════════════════════════════════════════════════════════════
Duration: 1039.0 s
{code}

Seems like the test failure is due to missing {{deparse1}} which was added from R 4.0.0. I think we can just guide people to use https://github.com/r-lib/backports if this is an issue.
The test case itself doesn't look a big deal.

I will take a closer look to make it working in R 4.0.0.;;;","23/Jun/20 11:01;gurwls223;Ok.. I finally made all tests being passed. I will make a PR soon.;;;","23/Jun/20 11:45;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28907;;;","23/Jun/20 11:46;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28907;;;","24/Jun/20 02:04;gurwls223;Issue resolved by pull request 28907
[https://github.com/apache/spark/pull/28907];;;","10/Jul/20 14:27;michaelchirico;Hey folks, I just saw SparkR was removed from CRAN, I assume it's related to this issue?

https://cran.r-project.org/web/packages/SparkR/index.html

Is a new submission in the process as this issue was fixed?

Please let me know if there's any way I can help as well.;;;","10/Jul/20 19:41;shivaram;Yes – this is the reason that SparkR has been temporarily removed from CRAN. We need a new release to upload a new version and we have some efforts to release Spark 2.4.7 and Spark 3.0.1 that are ongoing AFAIK.

cc'ing the release managers [~holden] [~ruifengz] [~prashant];;;",,,,,,,
"StringConcat can overflow `length`, leads to StringIndexOutOfBoundsException",SPARK-31916,13309781,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,jstokes,jstokes,05/Jun/20 17:55,02/Jul/20 15:35,13/Jul/23 08:46,12/Jun/20 00:22,2.4.4,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,SQL,,,,0,,,"We have query plans that through multiple transformations can grow extremely long in length. These would eventually throw OutOfMemory exceptions (https://issues.apache.org/jira/browse/SPARK-26103 & related https://issues.apache.org/jira/browse/SPARK-25380).

 

We backported the changes from [https://github.com/apache/spark/pull/23169] into our distribution of Spark, based on 2.4.4, and attempted to use the added `spark.sql.maxPlanStringLength`. While this works in some cases, large query plans can still lead to issues stemming from `StringConcat` in sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/StringUtils.scala.

 

The following unit test exhibits the issue, which continues to fail in the master branch of spark:

 
{code:scala}
  test(""StringConcat doesn't overflow on many inputs"") {    
    val concat = new StringConcat(maxLength = 100)
    0.to(Integer.MAX_VALUE).foreach { _ =>      
      concat.append(""hello world"")    
     }    
    assert(concat.toString.length === 100)  
} 
{code}
 

Looking at the append method here: [https://github.com/apache/spark/blob/fc6af9d900ec6f6a1cbe8f987857a69e6ef600d1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/StringUtils.scala#L118-L128]

 

It seems like regardless of whether the string to be append is added fully to the internal buffer, added as a substring to reach `maxLength`, or not added at all the internal `length` field is incremented by the length of `s`. Eventually this will overflow an int and cause L123 to substring with a negative index.",,apachespark,brandonvin,jstokes,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32157,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 12 00:22:59 UTC 2020,,,,,,,,,,"0|z0fkeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/20 08:21;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/28750;;;","12/Jun/20 00:22;maropu;Resolved by https://github.com/apache/spark/pull/28750;;;",,,,,,,,,,,,,,,,,,,,,,
Resolve the grouping column properly per the case sensitivity in grouped and cogrouped pandas UDFs,SPARK-31915,13309774,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,05/Jun/20 17:11,12/Dec/22 17:51,13/Jul/23 08:46,10/Jun/20 22:54,3.0.0,,,,,,,,,,,,3.0.1,3.1.0,,PySpark,SQL,,,0,,,"
{code}
from pyspark.sql.functions import *

df = spark.createDataFrame([[1, 1]], [""column"", ""Score""])

@pandas_udf(""column integer, Score float"", PandasUDFType.GROUPED_MAP)
def my_pandas_udf(pdf):
    return pdf.assign(Score=0.5)

df.groupby('COLUMN').apply(my_pandas_udf).show()
{code}

{code}
pyspark.sql.utils.AnalysisException: Reference 'COLUMN' is ambiguous, could be: COLUMN, COLUMN.;
{code}

{code}
df1 = spark.createDataFrame([(1, 1)], (""column"", ""value""))
df2 = spark.createDataFrame([(1, 1)], (""column"", ""value""))

df1.groupby(""COLUMN"").cogroup(
    df2.groupby(""COLUMN"")
).applyInPandas(lambda r, l: r + l, df1.schema).show()
{code}

{code}
pyspark.sql.utils.AnalysisException: cannot resolve '`COLUMN`' given input columns: [COLUMN, COLUMN, value, value];;
'FlatMapCoGroupsInPandas ['COLUMN], ['COLUMN], <lambda>(column#9L, value#10L, column#13L, value#14L), [column#22L, value#23L]
:- Project [COLUMN#9L, column#9L, value#10L]
:  +- LogicalRDD [column#9L, value#10L], false
+- Project [COLUMN#13L, column#13L, value#14L]
   +- LogicalRDD [column#13L, value#14L], false
{code}",,apachespark,bryanc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 22:54:39 UTC 2020,,,,,,,,,,"0|z0fkd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/20 07:00;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28745;;;","07/Jun/20 07:01;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28745;;;","10/Jun/20 07:37;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28777;;;","10/Jun/20 22:54;bryanc;Issue resolved by pull request 28777
[https://github.com/apache/spark/pull/28777];;;",,,,,,,,,,,,,,,,,,,,
"Using S3A staging committer, pending uploads are committed more than once and listed incorrectly in _SUCCESS data",SPARK-31911,13309653,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,brandonvin,brandonvin,05/Jun/20 08:25,10/Mar/22 13:49,13/Jul/23 08:46,10/Mar/22 13:49,2.4.4,,,,,,,,,,,,2.4.7,3.0.1,,Input/Output,,,,0,,,"First of all thanks for the great work on the S3 committers. I was able set up the directory staging committer in my environment following docs at [https://github.com/apache/spark/blob/master/docs/cloud-integration.md#committing-work-into-cloud-storage-safely-and-fast] and tested one of my Spark applications using it. The Spark version is 2.4.4 with Hadoop 3.2.1 and the cloud committer bindings. The application writes multiple DataFrames to ORC/Parquet in S3, submitting them as write jobs to Spark in parallel.

I think I'm seeing a bug where the staging committer will complete pending uploads more than once. The main symptom how I discovered this is that the _SUCCESS data files under each table will contain overlapping file names that belong to separate tables. From my reading of the code, that's because the filenames in _SUCCESS reflect which multipart uploads were completed in the commit for that particular table.

An example:

Concurrently, fire off DataFrame.write.orc(""s3a://bucket/a"") and DataFrame.write.orc(""s3a://bucket/b""). Suppose each table has one partition so writes one partition file.

When the two writes are done,
 * /a/_SUCCESS contains two filenames: /a/part-0000 and /b/part-0000.
 * /b/_SUCCESS contains the same two filenames.

Setting S3A logs to debug, I see the commitJob operation belonging to table a includes completing the uploads of /a/part-0000 and /b/part-0000. Then again, commitJob for table b includes the same completions. I haven't had a problem yet, but I wonder if having these extra requests would become an issue at higher scale, where dozens of commits with hundreds of files may be happening concurrently in the application.

I believe this may be caused from the way the pendingSet files are stored in the staging directory. They are stored under one directory named by the jobID, in the Hadoop code. However, for all write jobs executed by the Spark application, the jobID passed to Hadoop is the same - the application ID. Maybe the staging commit algorithm was built on the assumption that each instance of the algorithm would use a unique random jobID.

[~stevel@apache.org] , [~rdblue] Having seen your names on most of this work (thank you), I would be interested to know your thoughts on this. Also it's my first time opening a bug here, so let me know if there's anything else I can do to help report the issue.",,brandonvin,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-17066,,,,,HADOOP-17318,,,,,SPARK-33230,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 10 13:48:34 UTC 2022,,,,,,,,,,"0|z0fjmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/20 08:52;brandonvin;Interesting, it looks like the staging committer supports a configuration parameter `spark.sql.sources.writeJobUUID` that takes precedence over the `spark.app.id` for determining the name of the pendingSet directory. This is very interesting because `spark.sql.sources.writeJobUUID` is not present anywhere in the Spark codebase. Should this be set to a random UUID for each write job in Spark?

https://github.com/apache/hadoop/blob/a6df05bf5e24d04852a35b096c44e79f843f4776/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/staging/StagingCommitter.java#L186-L208;;;","05/Jun/20 09:58;stevel@apache.org;thanks for this. Created HADOOP-17066

I'd like to see those debug logs; if you can, could you attach to that JIRA, or if not, email me as stevel at apache.org

At a guess > 1 job is 
- either using the same staging dir, so uploading twice
- or the directory under user.home we use for propagating those .pendingset files are the same

either way, serious bug. Will look at ASAP;;;","05/Jun/20 10:00;stevel@apache.org;BTW, given we complete the multipart uploads only once, I don't think we could actually write the files twice.

But: they could be committed by the wrong job, or aborted by the wrong job. And I'm surprised the second job commit didn't actually fail;;;","05/Jun/20 10:01;stevel@apache.org;FYI [~mackrorysd];;;","05/Jun/20 16:42;brandonvin;Thanks for looking [~stevel@apache.org], I attached logs to the Hadoop ticket.;;;","10/Jun/20 06:33;brandonvin;Some more notes:

My Spark app is configured with `spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2`. This seems relevant because the PendingSet files are managed with a wrapped FileOutputCommitter. Using v2 could exacerbate the problem, because the pending files are becoming visible to the driver immediately when a task finishes. Even so, if my understanding is correct, v1 would still have a race condition, but may present differently (not sure if worse or better).

An additional symptom of this bug is that data-writing tasks randomly fail with exceptions such as ""org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist"" while the task is committing its work. Of course, since spark attempts tasks 4 times by default, it usually gets through, but slows things down. This exception seems to be another symptom of the PendingSet files being cleaned up by the driver while tasks are still working on it?
{code:java}
20/06/10 04:45:36 WARN TaskSetManager: Lost task 7.0 in stage 15.0 (TID 4399, hostname, executor 1): org.apache.spark.SparkException: Task failed while writing rows.
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:257)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:177)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:123)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /tmp/s3a-test-staging/www-data/hooray/staging-uploads/_temporary/0/_temporary/attempt_20200610044421_0015_m_000007_4399 (inode 64406205) Holder DFSClient_NONMAPREDUCE_330704236_69 does not have any open files.
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2800)
        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.completeFileInternal(FSDirWriteFileOp.java:691)
        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.completeFile(FSDirWriteFileOp.java:677)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2843)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:928)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:607)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)
        at org.apache.hadoop.ipc.Client.call(Client.java:1491)
        at org.apache.hadoop.ipc.Client.call(Client.java:1388)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
        at com.sun.proxy.$Proxy28.complete(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:557)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
        at com.sun.proxy.$Proxy29.complete(Unknown Source)
        at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:949)
        at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:907)
        at org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:890)
        at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:845)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
        at org.apache.hadoop.util.JsonSerialization.writeJsonAsBytes(JsonSerialization.java:282)
        at org.apache.hadoop.util.JsonSerialization.save(JsonSerialization.java:269)
        at org.apache.hadoop.fs.s3a.commit.files.PendingSet.save(PendingSet.java:170)
        at org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter.commitTaskInternal(StagingCommitter.java:724)
        at org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter.commitTask(StagingCommitter.java:641)
        at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
        at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
        at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:245)
        at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:247)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:242)

{code}

In the above, ""hooray"" is the value of ""spark.sql.sources.writeJobUUID"", set to a constant value for every write job - effectively the same as if it has been unset, because then it defaults to the Spark app ID, which is also constant.

Finally, I've confirmed that setting `spark.sql.sources.writeJobUUID` to a random string for each write job fixes all issues for me. For example,
{noformat}
dataframe.write.option(""spark.sql.sources.writeJobUUID"", generateRandomUUID()).save(""s3a://...""){noformat}
With this change, I see no issues at all, it works beautifully. Maybe it's supposed to be set in spark too?;;;","09/Nov/20 19:25;stevel@apache.org;yeah, its the uuid. see SPARK-33230 for the problem: if you do two stages in the same second, you get the conflict.

the writeJobUUID was something spark used to set...it's been restored. S3A committers in Hadoop will add extra resilience here HADOOP-17318.

Your fix is only partial. It separates the jobs in the shared cluster FS, but individual tasks still use the hadoop job+task attempt IDs to identify their task-attempt-specific work directory. If two task attempts on the same host are executed from jobs launched in the same second, then you still get conflict. That is fixed in HADOOP-17318

;;;","10/Mar/22 13:48;stevel@apache.org;I'm going to close as fixed now; the spark changes will have done it.;;;",,,,,,,,,,,,,,,,
Add compatibility tests for streaming state store format,SPARK-31905,13309379,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,04/Jun/20 07:44,22/Jun/20 07:57,13/Jul/23 08:46,22/Jun/20 07:57,3.0.0,,,,,,,,,,,,3.1.0,,,Structured Streaming,,,,0,,,"After SPARK-31894, we have a validation checking for the streaming state store. It's better to add integrated tests in the PR builder as soon as the breaking changes introduced.",,apachespark,cloud_fan,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31990,,,,,,,SPARK-31894,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 22 07:57:20 UTC 2020,,,,,,,,,,"0|z0fhxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/20 07:51;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/28725;;;","04/Jun/20 07:52;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/28725;;;","22/Jun/20 07:57;cloud_fan;Issue resolved by pull request 28725
[https://github.com/apache/spark/pull/28725];;;",,,,,,,,,,,,,,,,,,,,,
Char and varchar partition columns throw MetaException,SPARK-31904,13309371,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,04/Jun/20 07:27,05/Jun/20 22:36,13/Jul/23 08:46,05/Jun/20 22:36,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"{code}
CREATE TABLE t1(a STRING, B VARCHAR(10), C CHAR(10)) STORED AS parquet;
CREATE TABLE t2 USING parquet PARTITIONED BY (b, c) AS SELECT * FROM t1;
SELECT * FROM t2 WHERE b = 'A';
{code}

Above SQL throws MetaException

{quote}
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.sql.hive.client.Shim_v0_13.getPartitionsByFilter(HiveShim.scala:810)
	... 114 more
Caused by: MetaException(message:Filtering is supported only on partition keys of type string, or integral types)
	at org.apache.hadoop.hive.metastore.parser.ExpressionTree$FilterBuilder.setError(ExpressionTree.java:184)
	at org.apache.hadoop.hive.metastore.parser.ExpressionTree$LeafNode.getJdoFilterPushdownParam(ExpressionTree.java:439)
	at org.apache.hadoop.hive.metastore.parser.ExpressionTree$LeafNode.generateJDOFilterOverPartitions(ExpressionTree.java:356)
	at org.apache.hadoop.hive.metastore.parser.ExpressionTree$LeafNode.generateJDOFilter(ExpressionTree.java:278)
	at org.apache.hadoop.hive.metastore.parser.ExpressionTree.generateJDOFilterFragment(ExpressionTree.java:583)
	at org.apache.hadoop.hive.metastore.ObjectStore.makeQueryFilterString(ObjectStore.java:3315)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsViaOrmFilter(ObjectStore.java:2768)
	at org.apache.hadoop.hive.metastore.ObjectStore.access$500(ObjectStore.java:182)
	at org.apache.hadoop.hive.metastore.ObjectStore$7.getJdoResult(ObjectStore.java:3248)
	at org.apache.hadoop.hive.metastore.ObjectStore$7.getJdoResult(ObjectStore.java:3232)
	at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2974)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilterInternal(ObjectStore.java:3250)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(ObjectStore.java:2906)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:101)
	at com.sun.proxy.$Proxy25.getPartitionsByFilter(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_filter(HiveMetaStore.java:5093)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy26.get_partitions_by_filter(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionsByFilter(HiveMetaStoreClient.java:1232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy27.listPartitionsByFilter(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByFilter(Hive.java:2679)
	... 119 more
{quote}",,apachespark,cltlfcjin,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 22:36:55 UTC 2020,,,,,,,,,,"0|z0fhvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/20 07:33;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/28724;;;","05/Jun/20 22:36;maropu;Resolved by [https://github.com/apache/spark/pull/28724];;;",,,,,,,,,,,,,,,,,,,,,,
toPandas with Arrow enabled doesn't show metrics in Query UI.,SPARK-31903,13309293,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,03/Jun/20 23:46,06/Jun/20 07:51,13/Jul/23 08:46,05/Jun/20 04:00,2.4.5,3.0.0,,,,,,,,,,,2.4.7,3.0.0,,PySpark,R,,,0,,,"When calling {{toPandas}}, usually Query UI shows each plan node's metric and corresponding Stage ID and Task ID:
{code:java}
>>> df = spark.createDataFrame([(1, 10, 'abc'), (2, 20, 'def')], schema=['x', 'y', 'z'])
>>> df.toPandas()
   x   y    z
0  1  10  abc
1  2  20  def
{code}
!Screen Shot 2020-06-03 at 4.47.07 PM.png!

but if Arrow execution is enabled, it shows only plan nodes and the duration is not correct:
{code:java}
>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)
>>> df.toPandas()
   x   y    z
0  1  10  abc
1  2  20  def{code}
 

!Screen Shot 2020-06-03 at 4.47.27 PM.png!",,apachespark,Gengliang.Wang,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/20 23:47;ueshin;Screen Shot 2020-06-03 at 4.47.07 PM.png;https://issues.apache.org/jira/secure/attachment/13004762/Screen+Shot+2020-06-03+at+4.47.07+PM.png","03/Jun/20 23:47;ueshin;Screen Shot 2020-06-03 at 4.47.27 PM.png;https://issues.apache.org/jira/secure/attachment/13004763/Screen+Shot+2020-06-03+at+4.47.27+PM.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 19:16:48 UTC 2020,,,,,,,,,,"0|z0fheg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/20 21:21;Gengliang.Wang;I tried investigating the issue, I think the SparkListenerSQLExecutionEnd event is sent too early if arrow enabled.
Here is what normal events looks like:

{code:java}
SparkListenerSQLExecutionStart
SparkListenerJobStart
SparkListenerTaskStart
...
SparkListenerTaskEnd
SparkListenerJobEnd
SparkListenerSQLExecutionEnd
{code}

However, with arrow enabled, the events looks like:
{code:java}
SparkListenerSQLExecutionStart
SparkListenerSQLExecutionEnd
SparkListenerJobStart
SparkListenerTaskStart
...
SparkListenerTaskEnd
SparkListenerJobEnd
{code}

The metrics are aggregated on receiving the event ""SparkListenerSQLExecutionEnd"", which is why there are no metrics in UI.;;;","04/Jun/20 22:29;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/28730;;;","04/Jun/20 22:30;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/28730;;;","05/Jun/20 19:16;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/28740;;;","05/Jun/20 19:16;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/28740;;;",,,,,,,,,,,,,,,,,,,
"df.explain(""mode"") should work in PySpark side as well for consistency",SPARK-31895,13309074,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,03/Jun/20 01:01,12/Dec/22 18:10,13/Jul/23 08:46,03/Jun/20 03:07,3.0.0,,,,,,,,,,,,3.0.0,,,PySpark,,,,0,,,"Scala:

{code}
scala> spark.range(10).explain(""cost"")
== Optimized Logical Plan ==
Range (0, 10, step=1, splits=Some(12)), Statistics(sizeInBytes=80.0 B)

== Physical Plan ==
*(1) Range (0, 10, step=1, splits=12)
{code}

PySpark:

{code}
>>> spark.range(10).explain(""cost"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../spark/python/pyspark/sql/dataframe.py"", line 333, in explain
    raise TypeError(err_msg)
TypeError: extended (optional) should be provided as bool, got <class 'str'>
{code}
",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30231,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 03:07:54 UTC 2020,,,,,,,,,,"0|z0fg1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/20 01:18;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28711;;;","03/Jun/20 01:19;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28711;;;","03/Jun/20 03:07;gurwls223;Issue resolved by pull request 28711
[https://github.com/apache/spark/pull/28711];;;",,,,,,,,,,,,,,,,,,,,,
Introduce UnsafeRow format validation for streaming state store,SPARK-31894,13309010,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,02/Jun/20 18:22,31/Jul/20 09:24,13/Jul/23 08:46,19/Jun/20 05:57,3.1.0,,,,,,,,,,,,3.1.0,,,Structured Streaming,,,,0,,,"Currently, Structured Streaming directly puts the UnsafeRow into StateStore without any schema validation. It's a dangerous behavior when users reusing the checkpoint file during migration. Any changes or bug fix related to the aggregate function may cause random exceptions, even the wrong answer, e.g SPARK-28067.

Here we introduce an UnsafeRow format validation for the state store.",,apachespark,cloud_fan,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31990,,,,,,,,,,,,,SPARK-31905,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 31 09:24:35 UTC 2020,,,,,,,,,,"0|z0ffnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/20 18:37;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/28707;;;","15/Jun/20 06:00;XuanYuan;This issue is blocking by SPARK-31990, see more details in https://github.com/apache/spark/pull/28707#issuecomment-643916110.;;;","19/Jun/20 05:57;cloud_fan;Issue resolved by pull request 28707
[https://github.com/apache/spark/pull/28707];;;","31/Jul/20 09:24;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/29315;;;",,,,,,,,,,,,,,,,,,,,
Support `java.time.Instant` in Parquet filter pushdown,SPARK-31888,13308770,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,01/Jun/20 17:23,02/Jun/20 12:23,13/Jul/23 08:46,02/Jun/20 11:54,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Currently, ParquetFilters supports only java.sql.Timestamp values of TimestampType, and explicitly casts Any to java.sql.Timestamp, see
https://github.com/apache/spark/blob/cb0db213736de5c5c02b09a2d5c3e17254708ce1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala#L180

So, any filters refer to timestamp values are not pushed down to Parquet when spark.sql.datetime.java8API.enabled is true.",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31488,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 11:54:27 UTC 2020,,,,,,,,,,"0|z0fe6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/20 17:31;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28696;;;","02/Jun/20 11:54;cloud_fan;Issue resolved by pull request 28696
[https://github.com/apache/spark/pull/28696];;;",,,,,,,,,,,,,,,,,,,,,,
Fix the wrong coloring of nodes in DAG-viz,SPARK-31886,13308703,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,01/Jun/20 11:07,03/Jun/20 08:17,13/Jul/23 08:46,03/Jun/20 08:17,3.0.0,3.1.0,,,,,,,,,,,,,,Web UI,,,,0,,,"In the Job Page and Stage Page, nodes which are associated with ""barrier mode"" in the DAG-viz will be colored pale green.

But, with some type of jobs, nodes which are not associated with the mode will also colored.

You can reproduce with the following operation.
{code:java}
sc.parallelize(1 to 10).barrier.mapPartitions(identity).repartition(1).collect() {code}",,apachespark,Gengliang.Wang,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 08:17:14 UTC 2020,,,,,,,,,,"0|z0fdrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/20 11:31;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/28694;;;","01/Jun/20 11:32;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/28694;;;","03/Jun/20 08:17;Gengliang.Wang;The issue is resolved in https://github.com/apache/spark/pull/28694;;;",,,,,,,,,,,,,,,,,,,,,
Incorrect filtering of old millis timestamp in parquet,SPARK-31885,13308698,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,apachespark,maxgekk,maxgekk,01/Jun/20 10:45,01/Jun/20 18:23,13/Jul/23 08:46,01/Jun/20 15:14,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"{code:scala}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.0-SNAPSHOT
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_242)
Type in expressions to have them evaluated.
Type :help for more information.

scala> spark.conf.set(""spark.sql.parquet.outputTimestampType"", ""TIMESTAMP_MILLIS"")
scala> spark.conf.set(""spark.sql.legacy.parquet.datetimeRebaseModeInWrite"", ""CORRECTED"")
scala> Seq(java.sql.Timestamp.valueOf(""1000-06-14 08:28:53.123"")).toDF(""ts"").write.mode(""overwrite"").parquet(""/Users/maximgekk/tmp/ts_millis_old_filter"")

scala> spark.read.parquet(""/Users/maximgekk/tmp/ts_millis_old_filter"").show(false)
+-----------------------+
|ts                     |
+-----------------------+
|1000-06-14 08:28:53.123|
+-----------------------+


scala> spark.read.parquet(""/Users/maximgekk/tmp/ts_millis_old_filter"").filter($""ts"" === ""1000-06-14 08:28:53.123"")
res6: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ts: timestamp]

scala> spark.read.parquet(""/Users/maximgekk/tmp/ts_millis_old_filter"").filter($""ts"" === ""1000-06-14 08:28:53.123"").show(false)
+---+
|ts |
+---+
+---+
{code}",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 01 18:23:47 UTC 2020,,,,,,,,,,"0|z0fdqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/20 10:48;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28693;;;","01/Jun/20 15:14;cloud_fan;Issue resolved by pull request 28693
[https://github.com/apache/spark/pull/28693];;;","01/Jun/20 18:23;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28699;;;","01/Jun/20 18:23;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28699;;;",,,,,,,,,,,,,,,,,,,,
DAG-viz is not rendered correctly with pagination.,SPARK-31882,13308628,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,01/Jun/20 04:12,29/Sep/20 06:10,13/Jul/23 08:46,03/Jun/20 08:24,2.4.4,3.0.0,3.1.0,,,,,,,,,,2.4.8,3.0.0,3.1.0,Web UI,,,,0,,,"Because DAG-viz for a job fetches link urls for each stage from the stage table, rendering can fail with pagination.

You can reproduce this issue with the following operation.
{code:java}
 sc.parallelize(1 to 10).map(value => (value ,value)).repartition(1).repartition(1).repartition(1).reduceByKey(_ + _).collect{code}
And then, visit the corresponding job page.
There are 5 stages so show <5 stages in the paged table.",,apachespark,Gengliang.Wang,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 12:13:53 UTC 2020,,,,,,,,,,"0|z0fdb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/20 04:42;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/28690;;;","03/Jun/20 08:24;Gengliang.Wang;This issue is resolved in https://github.com/apache/spark/pull/28690;;;","22/Sep/20 12:13;apachespark;User 'zhli1142015' has created a pull request for this issue:
https://github.com/apache/spark/pull/29833;;;","22/Sep/20 12:13;apachespark;User 'zhli1142015' has created a pull request for this issue:
https://github.com/apache/spark/pull/29833;;;",,,,,,,,,,,,,,,,,,,,
Display the canvas element icon for sorting column,SPARK-31871,13308481,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,liucht-inspur,liucht-inspur,liucht-inspur,30/May/20 10:52,02/May/23 05:37,13/Jul/23 08:46,17/Jun/20 17:10,2.4.3,2.4.4,2.4.5,2.4.6,,,,,,,,,2.4.7,,,Spark Core,Web UI,,,0,,,"In the history Server page and Executor page, due to the wrong canvas element image path,

The sorting icon cannot be displayed when the sequence is clicked. In order to improve the user experience, the error path code is modified",,apachespark,liucht-inspur,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-43337,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 18 08:01:41 UTC 2020,,,,,,,,,,"0|z0fceo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/May/20 11:25;apachespark;User 'liucht-inspur' has created a pull request for this issue:
https://github.com/apache/spark/pull/28680;;;","30/May/20 11:26;apachespark;User 'liucht-inspur' has created a pull request for this issue:
https://github.com/apache/spark/pull/28680;;;","11/Jun/20 12:34;apachespark;User 'liucht-inspur' has created a pull request for this issue:
https://github.com/apache/spark/pull/28799;;;","17/Jun/20 17:10;sarutak;This issue is resolved in [https://github.com/apache/spark/pull/28799] .;;;","18/Jun/20 08:00;apachespark;User 'liucht-inspur' has created a pull request for this issue:
https://github.com/apache/spark/pull/28855;;;","18/Jun/20 08:01;apachespark;User 'liucht-inspur' has created a pull request for this issue:
https://github.com/apache/spark/pull/28855;;;",,,,,,,,,,,,,,,,,,
"AdaptiveQueryExecSuite: ""Do not optimize skew join if introduce additional shuffle"" test has no skew join",SPARK-31870,13308478,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mauzhang,mauzhang,mauzhang,30/May/20 10:35,02/Jun/20 02:01,13/Jul/23 08:46,02/Jun/20 02:01,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,Tests,,,0,,,"Due to incorrect configurations of 

- spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes

- spark.sql.adaptive.advisoryPartitionSizeInBytes",,apachespark,cloud_fan,mauzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 02:01:31 UTC 2020,,,,,,,,,,"0|z0fce0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/May/20 10:48;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/28679;;;","02/Jun/20 02:01;cloud_fan;Issue resolved by pull request 28679
[https://github.com/apache/spark/pull/28679];;;",,,,,,,,,,,,,,,,,,,,,,
Fix complex AQE query stage not reused,SPARK-31865,13308188,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maryannxue,maryannxue,maryannxue,28/May/20 22:33,29/May/20 04:20,13/Jul/23 08:46,29/May/20 04:20,3.0.0,,,,,,,,,,,,3.1.0,,,SQL,,,,0,,,Complex query stage that contain sub stages not being reused at times due to dynamic plan changes.,,apachespark,cloud_fan,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 04:20:44 UTC 2020,,,,,,,,,,"0|z0falk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/May/20 22:41;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/28670;;;","28/May/20 22:41;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/28670;;;","29/May/20 04:20;cloud_fan;Issue resolved by pull request 28670
[https://github.com/apache/spark/pull/28670];;;",,,,,,,,,,,,,,,,,,,,,
"Thriftserver not setting active SparkSession, SQLConf.get not getting session configs correctly",SPARK-31863,13308161,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,28/May/20 21:31,30/May/20 06:24,13/Jul/23 08:46,30/May/20 06:23,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Thriftserver is not setting the active SparkSession.
Because of that, configuration obtained with SQLConf.get is not the session configuration.
This makes many configs set by ""set"" in the session not work correctly.",,apachespark,cloud_fan,juliuszsompolski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 30 06:23:38 UTC 2020,,,,,,,,,,"0|z0fafk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/May/20 22:57;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/28671;;;","28/May/20 22:57;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/28671;;;","28/May/20 22:57;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/28671;;;","30/May/20 06:23;cloud_fan;Issue resolved by pull request 28671
[https://github.com/apache/spark/pull/28671];;;",,,,,,,,,,,,,,,,,,,,
Thriftserver collecting timestamp not using spark.sql.session.timeZone,SPARK-31861,13308132,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,28/May/20 19:08,05/Jun/20 09:13,13/Jul/23 08:46,30/May/20 06:22,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"If JDBC client is in TimeZone PST, and sets spark.sql.session.timeZone to PST, and sends a query ""SELECT timestamp '2020-05-20 12:00:00'"", and the JVM timezone of the Spark cluster is e.g. CET, then
- the timestamp literal in the query is interpreted as 12:00:00 PST, i.e. 21:00:00 CET
- but currently when it's returned, the timestamps are collected from the query with a collect() in https://github.com/apache/spark/blob/master/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala#L299, and then in the end Timestamps are turned into strings using a t.toString() in https://github.com/apache/spark/blob/master/sql/hive-thriftserver/v2.3/src/main/java/org/apache/hive/service/cli/ColumnValue.java#L138 This will use the Spark cluster TimeZone. That results in ""21:00:00"" returned to the JDBC application.",,apachespark,cloud_fan,juliuszsompolski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 09:13:22 UTC 2020,,,,,,,,,,"0|z0fa94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/May/20 22:55;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/28671;;;","28/May/20 22:56;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/28671;;;","30/May/20 06:22;cloud_fan;Issue resolved by pull request 28671
[https://github.com/apache/spark/pull/28671];;;","05/Jun/20 09:13;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/28735;;;","05/Jun/20 09:13;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/28735;;;","05/Jun/20 09:13;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/28735;;;",,,,,,,,,,,,,,,,,,
Thriftserver with spark.sql.datetime.java8API.enabled=true,SPARK-31859,13308129,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,28/May/20 18:48,27/Jun/20 17:13,13/Jul/23 08:46,30/May/20 06:20,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"{code}
  test(""spark.sql.datetime.java8API.enabled=true"") {
    withJdbcStatement() { st =>
      st.execute(""set spark.sql.datetime.java8API.enabled=true"")
      val rs = st.executeQuery(""select timestamp '2020-05-28 00:00:00'"")
      rs.next()
      // scalastyle:off
      println(rs.getObject(1))
    }
  }
{code}
fails with 
{code}
HiveThriftBinaryServerSuite:
java.lang.IllegalArgumentException: Timestamp format must be yyyy-mm-dd hh:mm:ss[.fffffffff]
        at java.sql.Timestamp.valueOf(Timestamp.java:204)
        at org.apache.hive.jdbc.HiveBaseResultSet.evaluate(HiveBaseResultSet.java:444)
        at org.apache.hive.jdbc.HiveBaseResultSet.getColumnValue(HiveBaseResultSet.java:424)
        at org.apache.hive.jdbc.HiveBaseResultSet.getObject(HiveBaseResultSet.java:464
{code}

It seems it might be needed in HiveResult.toHiveString?
cc [~maxgekk]",,apachespark,cloud_fan,juliuszsompolski,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32057,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 09:12:13 UTC 2020,,,,,,,,,,"0|z0fa8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/May/20 19:12;juliuszsompolski;Actually, it's already in HiveResult.toHiveString...
What happens is:
- its taken from SparkRow in SparkExecuteStatement.addNonNullColumnValue  using ""to += from.getAs[Timestamp](ordinal)"". This appears to not complain that it's in fact an Instant, not Timestamp.
- in ColumnValue.timestampValue it gets turned into a String as value.toString(). That somehow also doesn't complain that the object is an Instant, not a Timestamp?
- this gets returned to the client as String, which complains that it cannot read it back into Timestamp..

I will fix it together with https://issues.apache.org/jira/browse/SPARK-31861;;;","28/May/20 19:38;maxgekk;[~juliuszsompolski] FYI [https://github.com/apache/spark/pull/27552] Maybe it should fix the issue but it was reverted by [https://github.com/apache/spark/pull/27733] for some (unclear to me) reasons. /cc [~cloud_fan];;;","28/May/20 22:55;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/28671;;;","30/May/20 06:20;cloud_fan;Issue resolved by pull request 28671
[https://github.com/apache/spark/pull/28671];;;","05/Jun/20 09:11;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/28735;;;","05/Jun/20 09:12;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/28735;;;",,,,,,,,,,,,,,,,,,
Different results of query execution with wholestage codegen on and off,SPARK-31854,13307957,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,asm0dey,asm0dey,28/May/20 08:39,01/Jun/20 11:12,13/Jul/23 08:46,01/Jun/20 04:52,2.0.2,2.1.3,2.2.3,2.3.4,2.4.5,3.0.0,,,,,,,2.4.7,3.0.0,,SQL,,,,0,,,"Preface: I'm creating Kotlin API for spark to take best parts from three worlds — spark scala, spark java and kotlin.

What is nice — it works in most scenarios.

But i've hit following cornercase:
{code:scala}
withSpark(props = mapOf(""spark.sql.codegen.wholeStage"" to true)) {
    dsOf(1, null, 2)
            .map { c(it) }
            .debugCodegen()
            .show()
}
{code}

c(it) is creation of unnamed tuple

It fails with exception
{code}
java.lang.NullPointerException: Null value appeared in non-nullable field:
top level Product or row object
If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.serializefromobject_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
…
{code}

I know, in Scala it won't work, so I could stop here. But it works in Kotlin if I turn wholestage codegen off!

Moreover, if we will dig into generated code (when wholestage codegen is on), we'll see that basically flow is following:
If one of elements in source dataset was null we wil throw NPE no matter what.

Flow is as follows:

{code}
private void serializefromobject_doConsume_0(org.jetbrains.spark.api.Arity1 serializefromobject_expr_0_0, boolean serializefromobject_exprIsNull_0_0) throws java.io.IOException {
    serializefromobject_doConsume_0(mapelements_value_1, mapelements_isNull_1);
        mapelements_isNull_1 = mapelements_resultIsNull_0;
            mapelements_resultIsNull_0 = mapelements_exprIsNull_0_0;
                private void mapelements_doConsume_0(java.lang.Integer mapelements_expr_0_0, boolean mapelements_exprIsNull_0_0) throws java.io.IOException {
                    mapelements_doConsume_0(deserializetoobject_value_0, deserializetoobject_isNull_0);
                        deserializetoobject_resultIsNull_0 = deserializetoobject_exprIsNull_0_0;
                            private void deserializetoobject_doConsume_0(InternalRow localtablescan_row_0, int deserializetoobject_expr_0_0, boolean deserializetoobject_exprIsNull_0_0) throws java.io.IOException {
                                deserializetoobject_doConsume_0(localtablescan_row_0, localtablescan_value_0, localtablescan_isNull_0);
                                    boolean localtablescan_isNull_0 = localtablescan_row_0.isNullAt(0);
        mapelements_isNull_1 = true;
{code}

You can find generated code in it's original view and slightly simplified and refacored version [here|https://gist.github.com/asm0dey/5c0fa4c985ab999b383d16257b515100]

I believe that Spark should not behave differently when wholestage codegen is on and off and differences in behavior look like a bug.

My Spark version is 3.0.0-preview2",,apachespark,asm0dey,cloud_fan,dongjoon,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 01 07:42:59 UTC 2020,,,,,,,,,,"0|z0f96g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/May/20 07:10;maropu;Could you show us a query to reproduce the issue in our env? If we don't have it, we cannot look into it. Anyway, thanks for the report.;;;","29/May/20 07:28;asm0dey;What do you mean by query? It is written in Kotlin and is on top of report. 

If you need alternative query in Scala it will be like following


{code:java}
spark.conf.set(""spark.sql.codegen.wholeStage"", false)

Seq(1.asInstanceOf[Integer], null.asInstanceOf[Integer], 3.asInstanceOf[Integer]).toDS().map(v=>(v,v)).show()
{code}

It also works when spark.sql.codegen.wholeStage is false and doesn't when it is on.
;;;","31/May/20 05:37;maropu;Thanks for your report. Yea. that should be a bug of the whole-stage codegen as you said.;;;","31/May/20 05:39;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/28681;;;","01/Jun/20 04:10;dongjoon;I also verified this at 2.0.2 ~ 2.3.4 and updated the affected versions.;;;","01/Jun/20 04:18;maropu;Thanks for the update, Dongjoon.;;;","01/Jun/20 04:52;cloud_fan;Issue resolved by pull request 28681
[https://github.com/apache/spark/pull/28681];;;","01/Jun/20 07:42;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/28691;;;","01/Jun/20 07:42;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/28691;;;",,,,,,,,,,,,,,,
Set HiveThriftServer2 with actual port while configured 0,SPARK-31833,13307627,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,27/May/20 06:55,29/May/20 04:14,13/Jul/23 08:46,29/May/20 04:14,2.3.4,2.4.5,3.0.0,3.1.0,,,,,,,,,3.0.0,,,SQL,,,,0,,,the portNum for the API getPortNum and the server log is wrong when configured 0 for any free port.,,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 04:14:35 UTC 2020,,,,,,,,,,"0|z0f76w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/May/20 07:13;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/28651;;;","27/May/20 07:14;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/28651;;;","29/May/20 04:14;cloud_fan;Issue resolved by pull request 28651
[https://github.com/apache/spark/pull/28651];;;",,,,,,,,,,,,,,,,,,,,,
Flaky test: org.apache.spark.sql.hive.thriftserver.HiveSessionImplSuite.(It is not a test it is a sbt.testing.SuiteSelector),SPARK-31831,13307611,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,frankyin-factual,kabhwan,kabhwan,27/May/20 05:04,26/Aug/20 14:15,13/Jul/23 08:46,09/Jul/20 21:32,3.1.0,,,,,,,,,,,,3.1.0,,,SQL,Tests,,,0,,,"I've seen the failures two times (not in a row but closely) which seems to require investigation.

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/123147/testReport
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/123150/testReport

{noformat}
org.mockito.exceptions.base.MockitoException:  ClassCastException occurred while creating the mockito mock :   class to mock : 'org.apache.hive.service.cli.session.SessionManager', loaded by classloader : 'sun.misc.Launcher$AppClassLoader@483bf400'   created class : 'org.mockito.codegen.SessionManager$MockitoMock$1696557705', loaded by classloader : 'net.bytebuddy.dynamic.loading.MultipleParentClassLoader@47ecf2c6'   proxy instance class : 'org.mockito.codegen.SessionManager$MockitoMock$1696557705', loaded by classloader : 'net.bytebuddy.dynamic.loading.MultipleParentClassLoader@47ecf2c6'   instance creation by : ObjenesisInstantiator  You might experience classloading issues, please ask the mockito mailing-list. 
 Stack Trace
sbt.ForkMain$ForkError: org.mockito.exceptions.base.MockitoException: 
ClassCastException occurred while creating the mockito mock :
  class to mock : 'org.apache.hive.service.cli.session.SessionManager', loaded by classloader : 'sun.misc.Launcher$AppClassLoader@483bf400'
  created class : 'org.mockito.codegen.SessionManager$MockitoMock$1696557705', loaded by classloader : 'net.bytebuddy.dynamic.loading.MultipleParentClassLoader@47ecf2c6'
  proxy instance class : 'org.mockito.codegen.SessionManager$MockitoMock$1696557705', loaded by classloader : 'net.bytebuddy.dynamic.loading.MultipleParentClassLoader@47ecf2c6'
  instance creation by : ObjenesisInstantiator

You might experience classloading issues, please ask the mockito mailing-list.

	at org.apache.spark.sql.hive.thriftserver.HiveSessionImplSuite.beforeAll(HiveSessionImplSuite.scala:44)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:59)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:317)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:510)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: sbt.ForkMain$ForkError: java.lang.ClassCastException: org.mockito.codegen.SessionManager$MockitoMock$1696557705 cannot be cast to org.mockito.internal.creation.bytebuddy.MockAccess
	at org.mockito.internal.creation.bytebuddy.SubclassByteBuddyMockMaker.createMock(SubclassByteBuddyMockMaker.java:48)
	at org.mockito.internal.creation.bytebuddy.ByteBuddyMockMaker.createMock(ByteBuddyMockMaker.java:25)
	at org.mockito.internal.util.MockUtil.createMock(MockUtil.java:35)
	at org.mockito.internal.MockitoCore.mock(MockitoCore.java:63)
	at org.mockito.Mockito.mock(Mockito.java:1908)
	at org.mockito.Mockito.mock(Mockito.java:1817)
	... 13 more
{noformat}",,apachespark,dongjoon,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 26 14:15:56 UTC 2020,,,,,,,,,,"0|z0f73k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/20 13:06;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/29039;;;","09/Jul/20 21:32;dongjoon;Issue resolved by pull request 29039
[https://github.com/apache/spark/pull/29039];;;","10/Jul/20 19:19;apachespark;User 'frankyin-factual' has created a pull request for this issue:
https://github.com/apache/spark/pull/29069;;;","16/Jul/20 04:54;apachespark;User 'frankyin-factual' has created a pull request for this issue:
https://github.com/apache/spark/pull/29129;;;","16/Jul/20 04:55;apachespark;User 'frankyin-factual' has created a pull request for this issue:
https://github.com/apache/spark/pull/29129;;;","26/Aug/20 14:15;apachespark;User 'alismess-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/29549;;;",,,,,,,,,,,,,,,,,,
Flaky JavaBeanDeserializationSuite,SPARK-31820,13307346,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,26/May/20 06:43,26/May/20 12:14,13/Jul/23 08:46,26/May/20 12:14,3.1.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"The test suite JavaBeanDeserializationSuite sometimes fails with:
{code}
sbt.ForkMain$ForkError: java.lang.AssertionError: expected:<[JavaBeanDeserializationSuite.RecordSpark22000[shortField=0,intField=0,longField=0,floatField=0.0,doubleField=0.0,stringField=0,booleanField=true,timestampField=2020-05-25 12:39:16.999,nullIntField=<null>], JavaBeanDeserializationSuite.RecordSpark22000[shortField=1,intField=1,longField=1,floatField=1.0,doubleField=1.0,stringField=1,booleanField=false,timestampField=2020-05-25 12:39:17.0,nullIntField=<null>], JavaBeanDeserializationSuite.RecordSpark22000[shortField=2,intField=2,longField=2,floatField=2.0,doubleField=2.0,stringField=2,booleanField=true,timestampField=2020-05-25 12:39:17.0,nullIntField=<null>], JavaBeanDeserializationSuite.RecordSpark22000[shortField=3,intField=3,longField=3,floatField=3.0,doubleField=3.0,stringField=3,booleanField=false,timestampField=2020-05-25 12:39:17.0,nullIntField=<null>], JavaBeanDeserializationSuite.RecordSpark22000[shortField=4,intField=4,longField=4,floatField=4.0,doubleField=4.0,stringField=4,booleanField=true,timestampField=2020-05-25 12:39:17.0,nullIntField=<null>]]> but was:<[JavaBeanDeserializationSuite.RecordSpark22000[shortField=0,intField=0,longField=0,floatField=0.0,doubleField=0.0,stringField=0,booleanField=true,timestampField=2020-05-25 12:39:16.999,nullIntField=<null>], JavaBeanDeserializationSuite.RecordSpark22000[shortField=1,intField=1,longField=1,floatField=1.0,doubleField=1.0,stringField=1,booleanField=false,timestampField=2020-05-25 12:39:17,nullIntField=<null>], JavaBeanDeserializationSuite.RecordSpark22000[shortField=2,intField=2,longField=2,floatField=2.0,doubleField=2.0,stringField=2,booleanField=true,timestampField=2020-05-25 12:39:17,nullIntField=<null>], JavaBeanDeserializationSuite.RecordSpark22000[shortField=3,intField=3,longField=3,floatField=3.0,doubleField=3.0,stringField=3,booleanField=false,timestampField=2020-05-25 12:39:17,nullIntField=<null>], JavaBeanDeserializationSuite.RecordSpark22000[shortField=4,intField=4,longField=4,floatField=4.0,doubleField=4.0,stringField=4,booleanField=true,timestampField=2020-05-25 12:39:17,nullIntField=<null>]]>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at test.org.apache.spark.sql.JavaBeanDeserializationSuite.testSpark22000(JavaBeanDeserializationSuite.java:165)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}

See https://github.com/apache/spark/pull/28630#issuecomment-633695723",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 12:14:02 UTC 2020,,,,,,,,,,"0|z0f5go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/May/20 06:55;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28639;;;","26/May/20 12:14;cloud_fan;Issue resolved by pull request 28639
[https://github.com/apache/spark/pull/28639];;;",,,,,,,,,,,,,,,,,,,,,,
Add a workaround for Java 8u251+/K8s 1.17 and update integration test cases,SPARK-31819,13307340,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,26/May/20 05:56,26/May/20 21:31,13/Jul/23 08:46,26/May/20 21:31,2.4.6,,,,,,,,,,,,2.4.6,,,Documentation,Kubernetes,Tests,,0,,,,,apachespark,dongjoon,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31786,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 21:31:50 UTC 2020,,,,,,,,,,"0|z0f5fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/May/20 05:59;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28638;;;","26/May/20 06:00;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28638;;;","26/May/20 15:56;smilegator;[~dongjoon]This is 2.4 only?;;;","26/May/20 21:31;dongjoon;Yes. I fixed master/branch-3.0 via SPARK-31786 .;;;","26/May/20 21:31;dongjoon;Issue resolved by pull request 28638
[https://github.com/apache/spark/pull/28638];;;",,,,,,,,,,,,,,,,,,,
Failure on pushing down filters with java.time.Instant values in ORC,SPARK-31818,13307227,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,25/May/20 12:23,26/May/20 01:36,13/Jul/23 08:46,26/May/20 01:36,3.0.0,3.0.1,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"When spark.sql.datetime.java8API.enabled is set to true, filters pushed down with java.time.Instant values to ORC datasource fails with the exception:
{code}
[info]   java.lang.IllegalArgumentException: Wrong value class java.time.Instant for TIMESTAMP.EQUALS leaf
[info]   at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$PredicateLeafImpl.checkLiteralType(SearchArgumentImpl.java:192)
[info]   at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$PredicateLeafImpl.<init>(SearchArgumentImpl.java:75)
[info]   at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$BuilderImpl.equals(SearchArgumentImpl.java:352)
[info]   at org.apache.spark.sql.execution.datasources.orc.OrcFilters$.buildLeafSearchArgument(OrcFilters.scala:234)
[info]   at org.apache.spark.sql.execution.datasources.orc.OrcFilters$.convertibleFiltersHelper$1(OrcFilters.scala:134)
[info]   at org.apache.spark.sql.execution.datasources.orc.OrcFilters$.$anonfun$convertibleFilters$4(OrcFilters.scala:137)
[info]   at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
{code}",,apachespark,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31489,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 01:36:44 UTC 2020,,,,,,,,,,"0|z0f4q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/May/20 12:51;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28636;;;","25/May/20 12:51;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28636;;;","26/May/20 01:36;dongjoon;Issue resolved by pull request 28636
[https://github.com/apache/spark/pull/28636];;;",,,,,,,,,,,,,,,,,,,,,
Fix recoverPartitions test in DDLSuite,SPARK-31810,13307162,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prakharjain09,prakharjain09,prakharjain09,25/May/20 07:32,12/Dec/22 18:10,13/Jul/23 08:46,26/May/20 05:15,2.3.4,2.4.5,3.0.0,,,,,,,,,,3.0.0,,,SQL,Tests,,,0,,,"The recoverPartitions test [[link|https://github.com/apache/spark/blob/v3.0.0-rc2/sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLSuite.scala#L1205]]] overrides RDD_PARALLEL_LISTING_THRESHOLD treating it as SQLConf. But RDD_PARALLEL_LISTING_THRESHOLD is not a SQLConf, so it doesn't get overriden in SharedSparkSession and so the existing test doesn't end up testing the required behaviour.",,apachespark,prakharjain09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 05:15:25 UTC 2020,,,,,,,,,,"0|z0f4c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/May/20 07:38;apachespark;User 'prakharjain09' has created a pull request for this issue:
https://github.com/apache/spark/pull/28634;;;","26/May/20 05:15;gurwls223;Fixed in https://github.com/apache/spark/pull/28634;;;",,,,,,,,,,,,,,,,,,,,,,
Make struct function's output name and class name pretty,SPARK-31808,13307124,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,25/May/20 03:55,12/Dec/22 17:51,13/Jul/23 08:46,26/May/20 03:36,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"{code}
scala> sql(""select struct(1)"").show()
+---------------------+
|named_struct(col1, 1)|
+---------------------+
|                  [1]|
+---------------------+
{code}

It should be:

{code}
scala> sql(""select struct(1)"").show()
+---------+
|struct(1)|
+---------+
|      [1]|
+---------+
{code}

",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/20 03:55;Screen Shot 2020-05-25 at 12.54.41 PM.png;https://issues.apache.org/jira/secure/attachment/13003898/Screen+Shot+2020-05-25+at+12.54.41+PM.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 03:36:42 UTC 2020,,,,,,,,,,"0|z0f43k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/May/20 06:38;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28633;;;","25/May/20 06:39;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28633;;;","26/May/20 03:36;dongjoon;Issue resolved by pull request 28633
[https://github.com/apache/spark/pull/28633];;;",,,,,,,,,,,,,,,,,,,,,
Error when creating UnionRDD of PairRDDs,SPARK-31788,13306596,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gurwls223,sanket991,sanket991,21/May/20 20:08,12/Dec/22 18:11,13/Jul/23 08:46,01/Jun/20 00:44,3.0.0,,,,,,,,,,,,3.0.0,,,DStreams,PySpark,Spark Core,,0,,,"Union RDD of Pair RDD's seems to have issues

SparkSession available as 'spark'.

{code}
rdd1 = sc.parallelize([1,2,3,4,5])
rdd2 = sc.parallelize([6,7,8,9,10])
pairRDD1 = rdd1.zip(rdd2)
unionRDD1 = sc.union([pairRDD1, pairRDD1])
{code}

{code}
Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/home/gs/spark/latest/python/pyspark/context.py"", line 870,
in union jrdds[i] = rdds[i]._jrdd
File ""/home/gs/spark/latest/python/lib/py4j-0.10.9-src.zip/py4j/java_collections.py"", line 238, in _setitem_ File ""/home/gs/spark/latest/python/lib/py4j-0.10.9-src.zip/py4j/java_collections.py"", line 221,
in __set_item File ""/home/gs/spark/latest/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py"", line 332, in get_return_value py4j.protocol.Py4JError: An error occurred while calling None.None. Trace: py4j.Py4JException: Cannot convert org.apache.spark.api.java.JavaPairRDD to org.apache.spark.api.java.JavaRDD at py4j.commands.ArrayCommand.convertArgument(ArrayCommand.java:166) at py4j.commands.ArrayCommand.setArray(ArrayCommand.java:144) at py4j.commands.ArrayCommand.execute(ArrayCommand.java:97) at py4j.GatewayConnection.run(GatewayConnection.java:238) at java.lang.Thread.run(Thread.java:748)
{code}


{code}
rdd3 = sc.parallelize([11,12,13,14,15])
pairRDD2 = rdd3.zip(rdd3)
unionRDD2 = sc.union([pairRDD1, pairRDD2])
{code}

{code}
Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/home/gs/spark/latest/python/pyspark/context.py"", line 870, in union jrdds[i] = rdds[i]._jrdd File ""/home/gs/spark/latest/python/lib/py4j-0.10.9-src.zip/py4j/java_collections.py"", line 238, in _setitem_ File ""/home/gs/spark/latest/python/lib/py4j-0.10.9-src.zip/py4j/java_collections.py"", line 221, in __set_item File ""/home/gs/spark/latest/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py"", line 332, in get_return_value py4j.protocol.Py4JError: An error occurred while calling None.None. Trace: py4j.Py4JException: Cannot convert org.apache.spark.api.java.JavaPairRDD to org.apache.spark.api.java.JavaRDD at py4j.commands.ArrayCommand.convertArgument(ArrayCommand.java:166) at py4j.commands.ArrayCommand.setArray(ArrayCommand.java:144) at py4j.commands.ArrayCommand.execute(ArrayCommand.java:97) at py4j.GatewayConnection.run(GatewayConnection.java:238) at java.lang.Thread.run(Thread.java:748)
{code}

2.4.5 does not have this regression as below:

{code}
rdd4 = sc.parallelize(range(5))
pairRDD3 = rdd4.zip(rdd4)
unionRDD3 = sc.union([pairRDD1, pairRDD3])
unionRDD3.collect()
{code}

{code}
[(1, 6), (2, 7), (3, 8), (4, 9), (5, 10), (0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]
{code}

",,apachespark,sanket991,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25737,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 01 00:44:28 UTC 2020,,,,,,,,,,"0|z0f0u8:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"21/May/20 21:47;sanket991;[https://github.com/apache/spark/commit/f83fedc9f20869ab4c62bb07bac50113d921207f] looks like it does not check for PairRDD type in pyspark;;;","21/May/20 22:02;tgraves;[~sanket991]the link you provided is not to public Apache Spark, can you change the reference?;;;","21/May/20 23:45;sanket991;Took a naive dig at it [https://github.com/apache/spark/pull/28603] seems to work, looking for reviews and improvement suggestions.;;;","21/May/20 23:47;apachespark;User 'redsanket' has created a pull request for this issue:
https://github.com/apache/spark/pull/28603;;;","21/May/20 23:48;apachespark;User 'redsanket' has created a pull request for this issue:
https://github.com/apache/spark/pull/28603;;;","25/May/20 01:31;gurwls223;Fixed in https://github.com/apache/spark/pull/28603;;;","27/May/20 01:35;gurwls223;Reverted at https://github.com/apache/spark/commit/7fb2275f009c8744560c3247decdc106a8bca86f;;;","27/May/20 02:40;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28648;;;","27/May/20 02:41;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28648;;;","01/Jun/20 00:44;gurwls223;Issue resolved by pull request 28648
[https://github.com/apache/spark/pull/28648];;;",,,,,,,,,,,,,,
Exception on submitting Spark-Pi to Kubernetes 1.17.3,SPARK-31786,13306511,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,maver1ck,maver1ck,21/May/20 15:19,19/Jan/21 21:59,13/Jul/23 08:46,23/May/20 18:10,2.4.5,3.0.0,,,,,,,,,,,3.0.0,,,Kubernetes,,,,0,,,"Hi,
I'm getting exception when submitting Spark-Pi app to Kubernetes cluster.

Kubernetes version: 1.17.3
JDK version: openjdk version ""1.8.0_252""

Exception:
{code}
 ./bin/spark-submit --master k8s://https://172.31.23.60:8443 --deploy-mode cluster --name spark-pi --conf spark.kubernetes.container.image=spark-py:2.4.5 --conf spark.kubernetes.executor.request.cores=0.1 --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark --conf spark.executor.instances=1 local:///opt/spark/examples/src/main/python/pi.py
log4j:WARN No appenders could be found for logger (io.fabric8.kubernetes.client.Config).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Exception in thread ""main"" io.fabric8.kubernetes.client.KubernetesClientException: Operation: [create]  for kind: [Pod]  with name: [null]  in namespace: [default]  failed.
        at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:64)
        at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:72)
        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:337)
        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:330)
        at org.apache.spark.deploy.k8s.submit.Client$$anonfun$run$2.apply(KubernetesClientApplication.scala:141)
        at org.apache.spark.deploy.k8s.submit.Client$$anonfun$run$2.apply(KubernetesClientApplication.scala:140)
        at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)
        at org.apache.spark.deploy.k8s.submit.Client.run(KubernetesClientApplication.scala:140)
        at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication$$anonfun$run$5.apply(KubernetesClientApplication.scala:250)
        at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication$$anonfun$run$5.apply(KubernetesClientApplication.scala:241)
        at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)
        at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:241)
        at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:204)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.net.SocketException: Broken pipe (Write failed)
        at java.net.SocketOutputStream.socketWrite0(Native Method)
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
        at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431)
        at sun.security.ssl.OutputRecord.write(OutputRecord.java:417)
        at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:894)
        at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:865)
        at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:123)
        at okio.Okio$1.write(Okio.java:79)
        at okio.AsyncTimeout$1.write(AsyncTimeout.java:180)
        at okio.RealBufferedSink.flush(RealBufferedSink.java:224)
        at okhttp3.internal.http2.Http2Writer.settings(Http2Writer.java:203)
        at okhttp3.internal.http2.Http2Connection.start(Http2Connection.java:515)
        at okhttp3.internal.http2.Http2Connection.start(Http2Connection.java:505)
        at okhttp3.internal.connection.RealConnection.startHttp2(RealConnection.java:298)
        at okhttp3.internal.connection.RealConnection.establishProtocol(RealConnection.java:287)
        at okhttp3.internal.connection.RealConnection.connect(RealConnection.java:168)
        at okhttp3.internal.connection.StreamAllocation.findConnection(StreamAllocation.java:257)
        at okhttp3.internal.connection.StreamAllocation.findHealthyConnection(StreamAllocation.java:135)
        at okhttp3.internal.connection.StreamAllocation.newStream(StreamAllocation.java:114)
        at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:42)
        at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
        at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
        at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93)
        at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
        at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
        at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)
        at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
        at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:126)
        at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
        at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
        at io.fabric8.kubernetes.client.utils.BackwardsCompatibilityInterceptor.intercept(BackwardsCompatibilityInterceptor.java:119)
        at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
        at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
        at io.fabric8.kubernetes.client.utils.ImpersonatorInterceptor.intercept(ImpersonatorInterceptor.java:68)
        at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
        at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
        at io.fabric8.kubernetes.client.utils.HttpClientUtils.lambda$createHttpClient$3(HttpClientUtils.java:112)
        at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
        at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
        at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:254)
        at okhttp3.RealCall.execute(RealCall.java:92)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:411)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:372)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:241)
        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:819)
        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:334)
        ... 17 more
20/05/21 15:17:21 INFO ShutdownHookManager: Shutdown hook called
20/05/21 15:17:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-aa4d1797-0e72-4a7f-8ea0-f2231371bbb1
{code}

I think problem is with Java version. When I downgraded to 8_u162 then I was able to submit app, but there was similar exception in driver container (which is build on newest openjdk image so u252)
What can I do to fix this bug ?",,akbordia,apachespark,dongjoon,felixzheng,maver1ck,smurarka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31819,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 19 21:59:31 UTC 2021,,,,,,,,,,"0|z0f0bc:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"21/May/20 17:44;maver1ck;I think this is related to:
[https://github.com/fabric8io/kubernetes-client/issues/2145];;;","21/May/20 22:09;dongjoon;Thank you for reporting, [~maver1ck].;;;","21/May/20 22:21;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28601;;;","21/May/20 22:22;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28601;;;","21/May/20 22:32;dongjoon;BTW, is there any chance for you to test Apache Spark 3.0 RC2 which is the latest binary?
- https://dist.apache.org/repos/dist/dev/spark/v3.0.0-rc2-bin/;;;","21/May/20 23:08;dongjoon;I also verified that Apache Spark 3.0.0-RC2 and 2.4.6-RC3 fails, too.
{code}
Exception in thread ""main"" io.fabric8.kubernetes.client.KubernetesClientException: Operation: [create]  for kind: [Pod]  with name: [null]  in namespace: [default]  failed.
{code};;;","21/May/20 23:13;dongjoon;[~holden] and [~dbtsai]. I raised this issue as a blocker for Apache Spark 3.0.0 and 2.4.6.;;;","22/May/20 03:37;felixzheng;It seems the same issue as https://github.com/fabric8io/kubernetes-client/issues/2212. I have tried out v4.9.2 in Flink and it works as expected.
JIRA: https://issues.apache.org/jira/browse/FLINK-17565 ;;;","22/May/20 16:18;dongjoon;Thank you for the addition info, [~felixzheng].;;;","23/May/20 18:10;dongjoon;I'm working on backporting.;;;","24/May/20 01:50;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28625;;;","24/May/20 01:50;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28625;;;","24/May/20 02:57;dongjoon;We are hitting jackson-core 2.10.0 incompatibility issue.
Since we cannot upgrade Jackson dependency in Apache Spark 2.4, I closed my PR and remove `2.4.6` from the target version.
cc [~holden] and [~dbtsai];;;","24/May/20 03:53;dongjoon;As a workaround for 2.4.6, users can set `HTTP2_DISABLE=true` before executing `spark-submit` and set the driver pod environment like the following.
{code}
spark.kubernetes.driverEnv.HTTP2_DISABLE=true
{code};;;","24/May/20 11:22;maver1ck;I think this should be added at least to documentation.;;;","24/May/20 20:02;dongjoon;Could you make a PR, [~maver1ck]?;;;","26/May/20 05:58;dongjoon;Okay. I'll create a PR for that, [~maver1ck].;;;","04/Jan/21 17:20;smurarka;[~maver1ck] / [~dongjoon] :
I am facing this issue. I am using Spark 2.4.7 . 

 

I have tried the settings mentioned in the above comments
spark.kubernetes.driverEnv.HTTP2_DISABLE=true


Following is the exception :


Exception in thread ""main"" io.fabric8.kubernetes.client.KubernetesClientException: Operation: [create] for kind: [Pod] with name: [null] in namespace: [spark-test] failed.
 at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:64)
 at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:72)
 at io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:337)
 at io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:330)
 at org.apache.spark.deploy.k8s.submit.Client$$anonfun$run$2.apply(KubernetesClientApplication.scala:141)
 at org.apache.spark.deploy.k8s.submit.Client$$anonfun$run$2.apply(KubernetesClientApplication.scala:140)
 at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)
 at org.apache.spark.deploy.k8s.submit.Client.run(KubernetesClientApplication.scala:140)
 at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication$$anonfun$run$5.apply(KubernetesClientApplication.scala:250)
 at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication$$anonfun$run$5.apply(KubernetesClientApplication.scala:241)
 at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)
 at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:241)
 at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:204)
 at [org.apache.spark.deploy.SparkSubmit.org|http://org.apache.spark.deploy.sparksubmit.org/]$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)
 at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
 at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
 at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
 at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: [java.net|http://java.net/].SocketTimeoutException: connect timed out
 at [java.net|http://java.net/].PlainSocketImpl.socketConnect(Native Method)
 at [java.net|http://java.net/].AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
 at [java.net|http://java.net/].AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
 at [java.net|http://java.net/].AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
 at [java.net|http://java.net/].SocksSocketImpl.connect(SocksSocketImpl.java:392)
 at [java.net|http://java.net/].Socket.connect(Socket.java:589)
 at okhttp3.internal.platform.Platform.connectSocket(Platform.java:129)
 at okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.java:246)
 at okhttp3.internal.connection.RealConnection.connect(RealConnection.java:166)
 at okhttp3.internal.connection.StreamAllocation.findConnection(StreamAllocation.java:257)
 at okhttp3.internal.connection.StreamAllocation.findHealthyConnection(StreamAllocation.java:135)
 at okhttp3.internal.connection.StreamAllocation.newStream(StreamAllocation.java:114)
 at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:42)
 at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
 at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
 at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93)
 at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
 at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
 at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)
 at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
 at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:126)
 at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
 at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
 at io.fabric8.kubernetes.client.utils.BackwardsCompatibilityInterceptor.intercept(BackwardsCompatibilityInterceptor.java:119)
 at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
 at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
 at io.fabric8.kubernetes.client.utils.ImpersonatorInterceptor.intercept(ImpersonatorInterceptor.java:68)
 at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
 at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
 at io.fabric8.kubernetes.client.utils.HttpClientUtils.lambda$createHttpClient$3(HttpClientUtils.java:110)
 at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
 at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
 at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:254)
 at okhttp3.RealCall.execute(RealCall.java:92)
 at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:411)
 at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:372)
 at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:241)
 at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:819)
 at io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:334)
 ... 17 more 
 ;;;","04/Jan/21 18:08;dongjoon;Did you do `export HTTP2_DISABLE=true` before `spark-submit`? HTTP2_DISABLE is required all places when you use `K8s client` and technically there exist two places.
 # Your Mac (Outside K8s cluster): `spark-submit`
 # Spark Driver Pod (Inside K8s cluster): spark.kubernetes.driverEnv.HTTP2_DISABLE=true;;;","04/Jan/21 18:40;smurarka;[~dongjoon] -> Yes I have used `export HTTP2_DISABLE=true` , but only on my machine .
 Should it on all nodes of Kubernetes?

Also , regarding you mentioned in second point , spark.kubernetes.driverEnv.HTTP2_DISABLE=true has to be used with spark-submit in form of --conf . 

Please let me know if my understanding is correct.

Also , since this is a workaround. What can be the long term solution. Should I consider Spark 3 instead of Spark 2.4.7?;;;","05/Jan/21 05:23;dongjoon;Yes, you are correct.
 # `export` is only required for your machine.
 # `–conf` should be used for `driverEnv`.

Yes, Spark 3.0 is better for K8s environment and Spark 3.1 is much better because of SPARK-33005 (`Kubernetes GA Preparation`). FYI, Apache Spark 3.1.0 RC1 is already created.
 - [https://github.com/apache/spark/tree/v3.1.0-rc1]

Apache Spark 3.1.0 will arrive this month.;;;","06/Jan/21 20:27;smurarka;sure [~dongjoon]. Thanks a lot for response.;;;","19/Jan/21 20:46;smurarka;[~dongjoon] 
To run spark in kubernetes . I see case ""$1"" in driver) shift 1 CMD=( ""$SPARK_HOME/bin/spark-submit"" --conf ""spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS"" --deploy-mode client 

Could you pls suggest Why deploy-mode client is mentioned in [entrypoint.sh|https://t.co/IPbK5sEIqM?amp=1] ? I am running spark submit using deploy mode cluster but inside this [entrypoint.sh|https://t.co/IPbK5sEIqM?amp=1] its mentioned like that.;;;","19/Jan/21 21:59;dongjoon;Hi, [~smurarka]. Apache Jira is not designed for Q&A. You had better use the official mailing list if you have an irrelevant question to this JIRA.;;;"
JsonProtocol doesn't write RDDInfo#isBarrier,SPARK-31764,13306008,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,19/May/20 21:11,01/Jun/20 21:34,13/Jul/23 08:46,27/May/20 21:37,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,JsonProtocol read RDDInfo#isBarrier but doesn't write it.,,apachespark,jiangxb1987,kabhwan,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 02:45:01 UTC 2020,,,,,,,,,,"0|z0ex7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/May/20 21:23;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/28583;;;","19/May/20 21:24;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/28583;;;","27/May/20 21:37;jiangxb1987;Resolved by https://github.com/apache/spark/pull/28583;;;","28/May/20 01:12;kabhwan;For me this looks to be a bug - the description of PR states the same. Is there a reason this issue is marked as an ""improvement"" and the fix only landed to master branch? 

The bug is also placed in branch-3.0: https://github.com/apache/spark/blob/branch-3.0/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala;;;","28/May/20 01:16;jiangxb1987;The issue is reported as ""Improvement"" and the affected version is ""3.1.0"", so it's merged to master only. [~sarutak] Do you think this should go to 3.0?;;;","28/May/20 02:33;sarutak;As you say, it's more proper to mark the label ""bug"" rather than ""improvement"" so I think it's better to go to 3.0.

(I don't remember why I marked the label ""bug"". Maybe, it's a mistake.)

I'll make a backporting PR.;;;","28/May/20 02:34;kabhwan;Thanks for confirming. :);;;","28/May/20 02:44;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/28660;;;","28/May/20 02:45;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/28660;;;",,,,,,,,,,,,,,,
DataFrame.inputFiles() not Available,SPARK-31763,13306000,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rakson,FelixKJose,FelixKJose,19/May/20 20:29,12/Dec/22 18:11,13/Jul/23 08:46,28/May/20 00:52,2.4.5,,,,,,,,,,,,3.1.0,,,PySpark,,,,0,,,"I have been trying to list inputFiles that compose my DataSet by using *PySpark* 

spark_session.read
 .format(sourceFileFormat)
 .load(S3A_FILESYSTEM_PREFIX + bucket + File.separator + sourceFolderPrefix)
 *.inputFiles();*

but I get an exception saying inputFiles attribute not present. But I was able to get this functionality with Spark Java. 

*So is this something missing in PySpark?*",,apachespark,FelixKJose,rakson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 00:52:36 UTC 2020,,,,,,,,,,"0|z0ex5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/May/20 07:21;gurwls223;Are you interested in opening a PR for this?;;;","26/May/20 13:19;rakson;Shall I open a PR for this?;;;","26/May/20 14:19;gurwls223;Please go ahead ;;;","27/May/20 08:48;apachespark;User 'iRakson' has created a pull request for this issue:
https://github.com/apache/spark/pull/28652;;;","28/May/20 00:52;gurwls223;Issue resolved by pull request 28652
[https://github.com/apache/spark/pull/28652];;;",,,,,,,,,,,,,,,,,,,
Sql Div operator can result in incorrect output for int_min,SPARK-31761,13305920,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sandeep.katta2007,kshukla,kshukla,19/May/20 12:42,12/Dec/22 18:10,13/Jul/23 08:46,24/May/20 05:52,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Input  in csv : -2147483648,-1  --> (_c0, _c1)
{code}
val res = df.selectExpr(""_c0 div _c1"")
res.collect
res1: Array[org.apache.spark.sql.Row] = Array([-2147483648])
{code}
The result should be 2147483648 instead.",,apachespark,cloud_fan,dongjoon,kabhwan,kshukla,Samwel,sandeep.katta2007,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 24 06:12:02 UTC 2020,,,,,,,,,,"0|z0ewo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/May/20 15:53;sandeep.katta2007;[~kshukla] thanks for raising this, I cross checked this in 2.4.4 it seems to work fine. 

 

I will analyse  w.r.t 3.0.0 and raise PR soon .;;;","19/May/20 16:08;kshukla;Thanks [~sandeep.katta2007] for the quick response. The commit I was on is 06e1c0d788407af6101bf66d28cd44ca943d7a42 and also that this is reproducible when reading from csv and not when values are passed as literals.;;;","20/May/20 15:43;sandeep.katta2007;[~sowen] [~hyukjin.kwon]  [~dongjoon]

 

I have executed the same query in spark-2.4.4 it works as per expectation.

 

As you can see from 2.4.4 plan, columns are casted to double,  so there won't be *Integer overflow.*

== Parsed Logical Plan ==
 'Project [cast(('col0 / 'col1) as bigint) AS CAST((col0 / col1) AS BIGINT)#4|#4]
 +- Relation[col0#0,col1#1|#0,col1#1] csv

== Analyzed Logical Plan ==
 CAST((col0 / col1) AS BIGINT): bigint
 Project [cast((cast(col0#0 as double) / cast(col1#1 as double)) as bigint) AS CAST((col0 / col1) AS BIGINT)#4L|#0 as double) / cast(col1#1 as double)) as bigint) AS CAST((col0 / col1) AS BIGINT)#4L]
 +- Relation[col0#0,col1#1|#0,col1#1] csv

== Optimized Logical Plan ==
 Project [cast((cast(col0#0 as double) / cast(col1#1 as double)) as bigint) AS CAST((col0 / col1) AS BIGINT)#4L|#0 as double) / cast(col1#1 as double)) as bigint) AS CAST((col0 / col1) AS BIGINT)#4L]
 +- Relation[col0#0,col1#1|#0,col1#1] csv

== Physical Plan ==
 *(1) Project [cast((cast(col0#0 as double) / cast(col1#1 as double)) as bigint) AS CAST((col0 / col1) AS BIGINT)#4L|#0 as double) / cast(col1#1 as double)) as bigint) AS CAST((col0 / col1) AS BIGINT)#4L]
 +- *(1) FileScan csv [col0#0,col1#1|#0,col1#1] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/opt/fordebug/divTest.csv|file:///opt/fordebug/divTest.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<col0:int,col1:int>
 *(1) Project [cast((cast(col0#0 as double) / cast(col1#1 as double)) as bigint) AS CAST((col0 / col1) AS BIGINT)#4L|#0 as double) / cast(col1#1 as double)) as bigint) AS CAST((col0 / col1) AS BIGINT)#4L]
 +- *(1) FileScan csv [col0#0,col1#1|#0,col1#1] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/opt/fordebug/divTest.csv|file:///opt/fordebug/divTest.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<col0:int,col1:int>

*Spark-3.0 Plan*

== Parsed Logical Plan ==
 'Project [('col0 div 'col1) AS (col0 div col1)#4|#4]
 +- RelationV2[col0#0, col1#1|#0, col1#1] csv [file:/opt/fordebug/divTest.csv|file:///opt/fordebug/divTest.csv]

== Analyzed Logical Plan ==
 (col0 div col1): int
 Project [(col0#0 div col1#1) AS (col0 div col1)#4|#0 div col1#1) AS (col0 div col1)#4]
 +- RelationV2[col0#0, col1#1|#0, col1#1] csv [file:/opt/fordebug/divTest.csv|file:///opt/fordebug/divTest.csv]

== Optimized Logical Plan ==
 Project [(col0#0 div col1#1) AS (col0 div col1)#4|#0 div col1#1) AS (col0 div col1)#4]
 +- RelationV2[col0#0, col1#1|#0, col1#1] csv [file:/opt/fordebug/divTest.csv|file:///opt/fordebug/divTest.csv]

== Physical Plan ==
 *(1) Project [(col0#0 div col1#1) AS (col0 div col1)#4|#0 div col1#1) AS (col0 div col1)#4]
 +- BatchScan[col0#0, col1#1|#0, col1#1] CSVScan Location: InMemoryFileIndex[file:/opt/fordebug/divTest.csv|file:///opt/fordebug/divTest.csv], ReadSchema: struct<col0:int,col1:int>

 

In Spark3 do I need to cast the columns as in spark-2.4,  or user should manually add cast to their query as per below example

 

val schema = ""col0 int,col1 int"";
 val df = spark.read.schema(schema).csv(""file:/opt/fordebug/divTest.csv"");
 val res = df.selectExpr(""col0 div col1"")
 val res = df.selectExpr(""Cast(col0 as Decimal) div col1 "")
 res.collect

 

please let us know your opinion ;;;","21/May/20 08:21;gurwls223;If it overflows, looks like it just should be guided by {{spark.sql.ansi.enabled}} and throws an exception on the overflow.;;;","21/May/20 08:23;gurwls223;[~sandeep.katta2007] feel free to open a followup PR against SPARK-16323 in order to document the overflow in the migration guide. cc [~cloud_fan] and [~mgaido] fyi;;;","21/May/20 15:53;cloud_fan;I think this is a breaking change and we should fix it.

The `div` operator always returns long type, so this should not overflow. The `IntegralDivide` should cast input to long and divide.;;;","21/May/20 15:54;cloud_fan;[~sandeep.katta2007] are you interested at fixing it?;;;","21/May/20 16:25;sandeep.katta2007;I can fix this, I will raise PR;;;","21/May/20 16:32;sandeep.katta2007;but to fix this do we need to revert https://issues.apache.org/jira/browse/SPARK-16323 or we just cast input to long and divide ?;;;","21/May/20 18:31;dongjoon;If possible, could you try to find a way not to revert the existing commit, [~sandeep.katta2007]?;;;","21/May/20 18:33;sandeep.katta2007;okay I will try to fix it ;;;","21/May/20 20:35;apachespark;User 'sandeep-katta' has created a pull request for this issue:
https://github.com/apache/spark/pull/28600;;;","21/May/20 20:35;apachespark;User 'sandeep-katta' has created a pull request for this issue:
https://github.com/apache/spark/pull/28600;;;","21/May/20 20:36;sandeep.katta2007;I have raised the Pull request [https://github.com/apache/spark/pull/28600];;;","22/May/20 02:57;kabhwan;Let's make sure priority is marked properly so that RC3 cannot be initiated without this - sounds like it's a blocker because it's a regression and correctness issue.;;;","22/May/20 03:54;cloud_fan;I've set it as blocker.;;;","24/May/20 05:52;gurwls223;Fixed in https://github.com/apache/spark/pull/28600;;;","24/May/20 06:12;apachespark;User 'sandeep-katta' has created a pull request for this issue:
https://github.com/apache/spark/pull/28628;;;",,,,,,
Support configurable max number of rotate logs for spark daemons,SPARK-31759,13305869,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,19/May/20 09:49,12/Dec/22 18:10,13/Jul/23 08:46,20/May/20 10:18,2.4.5,3.0.0,3.1.0,,,,,,,,,,3.1.0,,,Deploy,Spark Core,Spark Submit,,0,,,"in `spark-daemon.sh`, `spark_rotate_log` accepts $2 as a custom setting for the number of max rotate log files, but this part of code is actually never used.",,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 20 10:18:37 UTC 2020,,,,,,,,,,"0|z0ewco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/May/20 09:58;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/28580;;;","20/May/20 10:18;gurwls223;Issue resolved by pull request 28580
[https://github.com/apache/spark/pull/28580];;;",,,,,,,,,,,,,,,,,,,,,,
Use github URL instead of a broken link ,SPARK-31740,13305558,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,william,william,william,18/May/20 00:17,18/May/20 05:15,13/Jul/23 08:46,18/May/20 05:15,3.1.0,,,,,,,,,,,,2.4.6,3.0.0,,Kubernetes,Tests,,,0,,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 05:15:15 UTC 2020,,,,,,,,,,"0|z0eufc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/May/20 00:20;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28561;;;","18/May/20 00:21;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28561;;;","18/May/20 05:15;dongjoon;Issue resolved by pull request 28561
[https://github.com/apache/spark/pull/28561];;;",,,,,,,,,,,,,,,,,,,,,
Docstring syntax issues prevent proper compilation of documentation,SPARK-31739,13305537,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,DavidToneian,DavidToneian,DavidToneian,17/May/20 20:15,12/Dec/22 18:10,13/Jul/23 08:46,18/May/20 11:25,2.4.5,,,,,,,,,,,,3.0.0,3.1.0,,Documentation,,,,0,,,"Some docstrings contain mistakes, like missing or spurious spaces, which prevent the documentation from being rendered as intended.",,apachespark,DavidToneian,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 11:25:25 UTC 2020,,,,,,,,,,"0|z0euao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/May/20 20:20;DavidToneian;Proposed a fix in [PR #28559|https://github.com/apache/spark/pull/28559].;;;","17/May/20 20:22;apachespark;User 'DavidToneian' has created a pull request for this issue:
https://github.com/apache/spark/pull/28559;;;","18/May/20 11:25;gurwls223;Issue resolved by pull request 28559
[https://github.com/apache/spark/pull/28559];;;",,,,,,,,,,,,,,,,,,,,,
Make YarnClient.`specify a more specific type for the application` pass in Hadoop-3.2,SPARK-31733,13305388,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,dongjoon,dongjoon,16/May/20 14:45,26/Apr/23 17:35,13/Jul/23 08:46,26/Apr/23 17:35,3.0.0,,,,,,,,,,,,3.5.0,,,Tests,,,,0,,,,,dongjoon,gurwls223,snoot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 26 17:35:44 UTC 2023,,,,,,,,,,"0|z0etdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/23 04:12;snoot;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40877;;;","26/Apr/23 17:35;gurwls223;Issue resolved by pull request 40877
[https://github.com/apache/spark/pull/40877];;;",,,,,,,,,,,,,,,,,,,,,,
Inconsistent error messages of casting timestamp to int,SPARK-31727,13305289,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,15/May/20 20:47,18/May/20 05:04,13/Jul/23 08:46,18/May/20 05:04,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Casting 1900-05-05 18:34:56.1 to int produces different error messages for in the non-codegen and codegen mode:
# codegen: ""Casting -2198208303900000 to int causes overflow""
# non-codegen: ""Casting -2198208304 to int causes overflow""

The error message in the non-codegen contains intermediate results.",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 20:59:55 UTC 2020,,,,,,,,,,"0|z0esrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/May/20 20:59;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28549;;;",,,,,,,,,,,,,,,,,,,,,,,
Use a fallback version in HiveExternalCatalogVersionsSuite,SPARK-31716,13305108,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,15/May/20 05:32,14/Sep/20 10:36,13/Jul/23 08:46,15/May/20 07:31,2.4.6,3.0.0,3.1.0,,,,,,,,,,2.4.6,3.0.0,,SQL,Tests,,,0,,,"Currently, HiveExternalCatalogVersionsSuite is aborted in all Jenkins jobs except JDK11 Jenkins jobs which don't have old Spark releases supporting JDK11.

{code}
HiveExternalCatalogVersionsSuite:
org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite *** ABORTED ***
  Exception encountered when invoking run on a nested suite - Fail to get the lates Spark versions to test. (HiveExternalCatalogVersionsSuite.scala:180)
{code}",,apachespark,dongjoon,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32876,SPARK-31717,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 07:31:19 UTC 2020,,,,,,,,,,"0|z0ernc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/May/20 05:35;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28536;;;","15/May/20 07:31;dongjoon;Issue resolved by pull request 28536
[https://github.com/apache/spark/pull/28536];;;",,,,,,,,,,,,,,,,,,,,,,
Fix flaky SparkSQLEnvSuite that sometimes varies single derby instance standard,SPARK-31715,13305098,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,15/May/20 03:44,15/May/20 06:37,13/Jul/23 08:46,15/May/20 06:37,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,Tests,,,0,,,"
{code:java}
Caused by: sbt.ForkMain$ForkError: org.apache.derby.iapi.error.StandardException: Another instance of Derby may have already booted the database /home/jenkins/workspace/SparkPullRequestBuilder/sql/hive-thriftserver/metastore_db.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)
	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)
	at java.security.AccessController.doPrivileged(Native Method)
	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)
	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)
	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)
	at org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)
	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)
	at org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)
	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)
	at org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)
	at org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)
	at org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)
	... 138 more
{code}
",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 06:37:03 UTC 2020,,,,,,,,,,"0|z0erl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/May/20 03:58;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/28537;;;","15/May/20 06:37;cloud_fan;Issue resolved by pull request 28537
[https://github.com/apache/spark/pull/28537];;;",,,,,,,,,,,,,,,,,,,,,,
Make test-dependencies.sh detect version string correctly,SPARK-31713,13305057,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,14/May/20 21:44,15/May/20 02:29,13/Jul/23 08:46,15/May/20 02:29,2.4.6,3.0.0,,,,,,,,,,,2.4.6,3.0.0,,Project Infra,,,,0,,,"Currently, SBT jobs are broken like the following.
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-3.0-test-sbt-hadoop-3.2-hive-2.3/476/console
{code}
[error] running /home/jenkins/workspace/spark-branch-3.0-test-sbt-hadoop-3.2-hive-2.3/dev/test-dependencies.sh ; received return code 1
Build step 'Execute shell' marked build as failure
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28963,,,SPARK-31693,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 02:29:19 UTC 2020,,,,,,,,,,"0|z0erc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/May/20 21:49;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28532;;;","14/May/20 21:50;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28532;;;","15/May/20 02:29;dongjoon;Issue resolved by pull request 28532
[https://github.com/apache/spark/pull/28532];;;",,,,,,,,,,,,,,,,,,,,,
Fail casting numeric to timestamp by default,SPARK-31710,13304909,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,小郭飞飞刀,小郭飞飞刀,小郭飞飞刀,14/May/20 10:27,25/Nov/20 05:14,13/Jul/23 08:46,16/Jun/20 08:40,2.4.5,,,,,,,,,,,,3.1.0,,,SQL,,,,0,,,"Hi Team

Steps to reproduce.
{code:java}
create table test(id bigint);
insert into test select 1586318188000;
create table test1(id bigint) partitioned by (year string);
insert overwrite table test1 partition(year) select 234,cast(id as TIMESTAMP) from test;
{code}
let's check the result. 

Case 1:

*select * from test1;*

234 | 52238-06-04 13:06:400.0

--the result is wrong

Case 2:

*select 234,cast(id as TIMESTAMP) from test;*

 

java.lang.IllegalArgumentException: Timestamp format must be yyyy-mm-dd hh:mm:ss[.fffffffff]
 at java.sql.Timestamp.valueOf(Timestamp.java:237)
 at org.apache.hive.jdbc.HiveBaseResultSet.evaluate(HiveBaseResultSet.java:441)
 at org.apache.hive.jdbc.HiveBaseResultSet.getColumnValue(HiveBaseResultSet.java:421)
 at org.apache.hive.jdbc.HiveBaseResultSet.getString(HiveBaseResultSet.java:530)
 at org.apache.hive.beeline.Rows$Row.<init>(Rows.java:166)
 at org.apache.hive.beeline.BufferedRows.<init>(BufferedRows.java:43)
 at org.apache.hive.beeline.BeeLine.print(BeeLine.java:1756)
 at org.apache.hive.beeline.Commands.execute(Commands.java:826)
 at org.apache.hive.beeline.Commands.sql(Commands.java:670)
 at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:974)
 at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:810)
 at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:767)
 at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:480)
 at org.apache.hive.beeline.BeeLine.main(BeeLine.java:463)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.hadoop.util.RunJar.run(RunJar.java:226)
 at org.apache.hadoop.util.RunJar.main(RunJar.java:141)
 Error: Unrecognized column type:TIMESTAMP_TYPE (state=,code=0)

 

I try hive,it works well,and the convert is fine and correct
{code:java}
select 234,cast(id as TIMESTAMP) from test;
 234   2020-04-08 11:56:28
{code}
Two questions:

q1:

if we forbid this convert,should we keep all cases the same?

q2:

if we allow the convert in some cases, should we decide the long length, for the code seems to force to convert to ns with times*1000000 nomatter how long the data is,if it convert to timestamp with incorrect length, we can raise the error.
{code:java}
// // converting seconds to us
private[this] def longToTimestamp(t: Long): Long = t * 1000000L{code}
 

Thanks!

 ","hdp:2.7.7

spark:2.4.5",apachespark,Samwel,小郭飞飞刀,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 06 11:57:47 UTC 2020,,,,,,,,,,"0|z0eqf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/May/20 01:24;apachespark;User 'TJX2014' has created a pull request for this issue:
https://github.com/apache/spark/pull/28534;;;","18/May/20 08:42;apachespark;User 'GuoPhilipse' has created a pull request for this issue:
https://github.com/apache/spark/pull/28567;;;","18/May/20 08:42;apachespark;User 'GuoPhilipse' has created a pull request for this issue:
https://github.com/apache/spark/pull/28567;;;","18/May/20 08:54;apachespark;User 'GuoPhilipse' has created a pull request for this issue:
https://github.com/apache/spark/pull/28568;;;","18/May/20 10:04;apachespark;User 'GuoPhilipse' has created a pull request for this issue:
https://github.com/apache/spark/pull/28570;;;","18/May/20 10:05;apachespark;User 'GuoPhilipse' has created a pull request for this issue:
https://github.com/apache/spark/pull/28570;;;","20/May/20 10:17;apachespark;User 'GuoPhilipse' has created a pull request for this issue:
https://github.com/apache/spark/pull/28593;;;","16/Jun/20 13:59;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28843;;;","16/Jun/20 14:00;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28843;;;","06/Jul/20 11:57;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29012;;;",,,,,,,,,,,,,,
add back the support of streaming update mode,SPARK-31706,13304731,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,cloud_fan,cloud_fan,cloud_fan,13/May/20 17:19,20/May/20 03:45,13/Jul/23 08:46,20/May/20 03:45,3.0.0,,,,,,,,,,,,3.0.0,,,Structured Streaming,,,,0,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 20 03:45:37 UTC 2020,,,,,,,,,,"0|z0epbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/May/20 17:27;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/28523;;;","20/May/20 03:45;cloud_fan;Issue resolved by pull request 28523
[https://github.com/apache/spark/pull/28523];;;",,,,,,,,,,,,,,,,,,,,,,
Changes made by SPARK-26985 break reading parquet files correctly in BigEndian architectures (AIX + LinuxPPC64),SPARK-31703,13304654,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tinhto-000,miccagiann,miccagiann,13/May/20 13:04,22/Jun/21 06:52,13/Jul/23 08:46,12/Aug/20 06:39,2.4.5,3.0.0,,,,,,,,,,,2.4.7,3.0.1,3.1.0,Spark Core,,,,0,BigEndian,correctness,"tTrying to upgrade to Apache Spark 2.4.5 in our IBM systems (AIX and PowerPC) so as to be able to read data stored in parquet format, we notice that values associated with DOUBLE and DECIMAL types are parsed in the wrong form.

According toe parquet documentation, they always opt to store the values using little-endian representation for values:
 [https://github.com/apache/parquet-format/blob/master/Encodings.md]
{noformat}
The plain encoding is used whenever a more efficient encoding can not be used. It
stores the data in the following format:

BOOLEAN: Bit Packed, LSB first
INT32: 4 bytes little endian
INT64: 8 bytes little endian
INT96: 12 bytes little endian (deprecated)
FLOAT: 4 bytes IEEE little endian
DOUBLE: 8 bytes IEEE little endian
BYTE_ARRAY: length in 4 bytes little endian followed by the bytes contained in the array
FIXED_LEN_BYTE_ARRAY: the bytes contained in the array

For native types, this outputs the data as little endian. Floating
point types are encoded in IEEE.
For the byte array type, it encodes the length as a 4 byte little
endian, followed by the bytes.{noformat}","AIX 7.2
LinuxPPC64 with RedHat.",Anuja,apachespark,cloud_fan,dongjoon,maropu,miccagiann,nmarion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26985,,,,,,,,,,,"13/May/20 13:04;miccagiann;Data_problem_Spark.gif;https://issues.apache.org/jira/secure/attachment/13002820/Data_problem_Spark.gif",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 12 16:32:27 UTC 2020,,,,,,,,,,"0|z0eoug:",9223372036854775807,,,,,,,,,,,,,2.4.7,3.0.1,,,,,,,,,"07/Aug/20 02:54;apachespark;User 'tinhto-000' has created a pull request for this issue:
https://github.com/apache/spark/pull/29383;;;","07/Aug/20 02:55;apachespark;User 'tinhto-000' has created a pull request for this issue:
https://github.com/apache/spark/pull/29383;;;","09/Aug/20 01:59;dongjoon;I raised the priority to `Blocker` with `Target Version` 2.4.7 and 3.0.1.;;;","12/Aug/20 06:39;cloud_fan;Issue resolved by pull request 29383
[https://github.com/apache/spark/pull/29383];;;","12/Aug/20 16:31;apachespark;User 'tinhto-000' has created a pull request for this issue:
https://github.com/apache/spark/pull/29419;;;","12/Aug/20 16:32;apachespark;User 'tinhto-000' has created a pull request for this issue:
https://github.com/apache/spark/pull/29419;;;",,,,,,,,,,,,,,,,,,
HistoryServer should set Content-Type,SPARK-31697,13304537,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,13/May/20 06:59,13/May/20 08:47,13/Jul/23 08:46,13/May/20 08:47,3.1.0,,,,,,,,,,,,3.0.0,,,Web UI,,,,0,,,"I noticed that we will get html as plain text when we access to wrong URLs on HistoryServer.
{code:java}
<html>
      <head>
        <meta http-equiv=""Content-type"" content=""text/html; charset=utf-8""/><meta name=""viewport"" content=""width=device-width, initial-scale=1""/><link rel=""stylesheet"" href=""/static/bootstrap.min.css"" type=""text/css""/><link rel=""stylesheet"" href=""/static/vis-timeline-graph2d.min.css"" type=""text/css""/><link rel=""stylesheet"" href=""/static/webui.css"" type=""text/css""/><link rel=""stylesheet"" href=""/static/timeline-view.css"" type=""text/css""/><script src=""/static/sorttable.js""></script><script src=""/static/jquery-3.4.1.min.js""></script><script src=""/static/vis-timeline-graph2d.min.js""></script><script src=""/static/bootstrap.bundle.min.js""></script><script src=""/static/initialize-tooltips.js""></script><script src=""/static/table.js""></script><script src=""/static/timeline-view.js""></script><script src=""/static/log-view.js""></script><script src=""/static/webui.js""></script><script>setUIRoot('')</script>
        
        <link rel=""shortcut icon"" href=""/static/spark-logo-77x50px-hd.png""></link>
        <title>Not Found</title>
      </head>
      <body>
        <div class=""container-fluid"">
          <div class=""row"">
            <div class=""col-12"">
              <h3 style=""vertical-align: middle; display: inline-block;"">
                <a style=""text-decoration: none"" href=""/"">
                  <img src=""/static/spark-logo-77x50px-hd.png""/>
                  <span class=""version"" style=""margin-right: 15px;"">3.1.0-SNAPSHOT</span>
                </a>
                Not Found
              </h3>
            </div>
          </div>
          <div class=""row"">
            <div class=""col-12"">
              <div class=""row"">Application local-1589239 not found.</div>
            </div>
          </div>
        </div>
      </body>
    </html> {code}
 

The reason is Content-Type not set.
{code:java}
HTTP/1.1 404 Not Found
Date: Wed, 13 May 2020 06:59:29 GMT
Cache-Control: no-cache, no-store, must-revalidate
X-Frame-Options: SAMEORIGIN
X-XSS-Protection: 1; mode=block
X-Content-Type-Options: nosniff
Content-Length: 1778
Server: Jetty(9.4.18.v20190429) {code}",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 13 08:47:09 UTC 2020,,,,,,,,,,"0|z0eo4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/May/20 07:11;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/28519;;;","13/May/20 07:12;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/28519;;;","13/May/20 08:47;dongjoon;Issue resolved by pull request 28519
[https://github.com/apache/spark/pull/28519];;;",,,,,,,,,,,,,,,,,,,,,
Investigate AmpLab Jenkins server network issue,SPARK-31693,13304470,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,shaneknapp,dongjoon,dongjoon,12/May/20 22:56,12/Dec/22 18:11,13/Jul/23 08:46,14/Jan/21 19:04,2.4.6,3.0.0,3.1.0,,,,,,,,,,,,,Project Infra,,,,0,,,"Given the series of failures in Spark packaging Jenkins job, it seems that there is a network issue in AmbLab Jenkins cluster.

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20Packaging/job/spark-master-maven-snapshots/

- The node failed to talk to GitBox. (SPARK-31687) -> GitHub is okay.
- The node failed to download the maven mirror. (SPARK-31691) -> The primary host is okay.
- The node failed to communicate repository.apache.org. (Current master branch Jenkins job failure)
{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:3.0.0-M1:deploy (default-deploy) on project spark-parent_2.12: ArtifactDeployerException: Failed to retrieve remote metadata org.apache.spark:spark-parent_2.12:3.1.0-SNAPSHOT/maven-metadata.xml: Could not transfer metadata org.apache.spark:spark-parent_2.12:3.1.0-SNAPSHOT/maven-metadata.xml from/to apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots): Transfer failed for https://repository.apache.org/content/repositories/snapshots/org/apache/spark/spark-parent_2.12/3.1.0-SNAPSHOT/maven-metadata.xml: Connect to repository.apache.org:443 [repository.apache.org/207.244.88.140] failed: Connection timed out (Connection timed out) -> [Help 1]
{code}",,dongjoon,shaneknapp,wypoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31687,SPARK-31691,SPARK-31713,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 23 21:52:04 UTC 2020,,,,,,,,,,"0|z0enps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/May/20 22:58;dongjoon;Hi, [~shaneknapp]. How do you think the above?
cc [~srowen];;;","12/May/20 23:22;shaneknapp;weird.  nothing has changed on our end and i'm going to have to start debugging.

;;;","13/May/20 00:27;dongjoon;Thank you for taking a look at this~;;;","14/May/20 02:53;gurwls223;Seems it's blocking many other PRs ...;;;","14/May/20 03:07;shaneknapp;grrr.  ok, sorry.  today was my zoom meeting day.  i'll reboot the master
and all nodes tomorrow and see how that goes.

i really don't see how this is an issue on our end.




-- 
Shane Knapp
Computer Guy / Voice of Reason
UC Berkeley EECS Research / RISELab Staff Technical Lead
https://rise.cs.berkeley.edu
;;;","14/May/20 03:23;gurwls223;Thank you so much [~shaneknapp].;;;","14/May/20 20:24;shaneknapp;filed https://issues.apache.org/jira/browse/INFRA-20267

i don't think it's us.  i could be wrong as IANANE (i am not a network engineer).  :);;;","15/May/20 02:23;dongjoon;Thank you, [~shaneknapp]. 

Although this is another issue, `amp-jenkins-worker-05` has a corrupted Maven local repo and fails consistently.
{code}
Using `mvn` from path: /home/jenkins/workspace/SparkPullRequestBuilder/build/apache-maven-3.6.3/bin/mvn
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-install-plugin:3.0.0-M1:install (default-cli) on project spark-parent_2.12: ArtifactInstallerException: Failed to install metadata org.apache.spark:spark-parent_2.12/maven-metadata.xml: Could not parse metadata /home/jenkins/.m2/repository/org/apache/spark/spark-parent_2.12/maven-metadata-local.xml: in epilog non whitespace content is not allowed but got > (position: END_TAG seen ...</metadata>\n>... @13:2) -> [Help 1]
{code}

Could you nuke the local Maven repository directory in this machine. And maybe the other machine which fails consistently, too.;;;","18/May/20 03:10;gurwls223;Gentle ping [~shaneknapp], seems this is blocking the RC2 .. as it blocks to run the tests.;;;","18/May/20 16:01;shaneknapp;apache.org blacklisted us:

'''
That IP was banned 5 days ago for more than 1,000 download page views per 24 hours (1020 >= limit of 1000). Typically this is due to some misconfigured CI system hitting our systems to download packages instead of using a local cache.
'''

i asked them to un-ban us while i investigate the root cause.;;;","19/May/20 20:25;shaneknapp;i pinged the infra ticket again...  we're still blacklisted and i'm stumped as to why we had so many connections in such a short period.  i asked if they had any logs/data for me so i could (hopefully) track down the offending build.;;;","19/May/20 21:50;shaneknapp;ok, i had a nagging suspicion that i knew what it was, and i was proven right!

'''
Here's an example of the query string /dyn/closer.lua?action=download&filename=/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz that resulted in the ban. There are also a lot of connections to archive.apache.org which have not (yet) hit the ban limit. These artifacts should be cached locally as well if you are using them in a CI environment. I unblocked the IP for now, but it may get re-banned if the traffic continues, as it is automated.
'''

each and every build, a call is made to download maven 3.6.3.  i can have jenkins manage this, and have just about finished setting this up.

this will lessen the load by a LOT, and then we can cherry-pick other unnecessary calls later.;;;","19/May/20 21:55;shaneknapp;i'm also going to need to restart jenkins once i'm ready to deploy this.;;;","20/May/20 00:29;shaneknapp;this is done, and the maven builds (including the PRB) are all using a locally cached maven installation. in the future, we'll need to have a JIRA filed if we need to upgrade the version and avoid this from happening again! :)

 
{code:java}
exec: curl -s -L https://downloads.lightbend.com/zinc/0.3.15/zinc-0.3.15.tgz
exec: curl -s -L https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz
Using `mvn` from path: /home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.6.3/bin/mvn
{code};;;","20/May/20 05:45;dongjoon;Wow. Thank you so much, [~shaneknapp]! Got it. +1 for that.;;;","20/May/20 06:20;gurwls223;Awesome!!;;;","02/Jul/20 05:12;gurwls223;[~shaneknapp] .. seems this popped up again ;(.;;;","06/Jul/20 16:32;shaneknapp;please provide examples or links to examples...  all of the snapshot builds are currently green.;;;","23/Jul/20 21:52;wypoon;I'm seeing a problem with the .m2 cache on amp-jenkins-worker-06. In https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/126370/console
{noformat}
[EnvInject] - Loading node environment variables.
Building remotely on amp-jenkins-worker-06 (centos spark-test) in workspace /home/jenkins/workspace/SparkPullRequestBuilder
...
========================================================================
Running build tests
========================================================================
exec: curl -s -L https://downloads.lightbend.com/zinc/0.3.15/zinc-0.3.15.tgz
exec: curl -s -L https://downloads.lightbend.com/scala/2.12.10/scala-2.12.10.tgz
Using `mvn` from path: /home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.6.3/bin/mvn
Using `mvn` from path: /home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.6.3/bin/mvn
Performing Maven install for hadoop-2.7-hive-1.2
Using `mvn` from path: /home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.6.3/bin/mvn
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-install-plugin:3.0.0-M1:install (default-cli) on project spark-yarn_2.12: ArtifactInstallerException: Failed to install metadata org.apache.spark:spark-yarn_2.12/maven-metadata.xml: Could not parse metadata /home/jenkins/.m2/repository/org/apache/spark/spark-yarn_2.12/maven-metadata-local.xml: in epilog non whitespace content is not allowed but got t (position: END_TAG seen ...</metadata>\nt... @13:2) -> [Help 1]
{noformat}
;;;",,,,,
Hadoop confs passed via spark config are not set in URLStream Handler Factory,SPARK-31692,13304467,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,karup1990,karup1990,karup1990,12/May/20 22:15,03/May/22 21:22,13/Jul/23 08:46,14/May/20 06:22,2.3.4,2.4.5,3.0.0,,,,,,,,,,2.4.6,3.0.0,,SQL,,,,0,,,"Hadoop conf passed via spark config(as ""spark.hadoop.*"") are not set in URLStreamHandlerFactory",,apachespark,dongjoon,karup1990,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-39094,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 17:09:03 UTC 2020,,,,,,,,,,"0|z0enp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/May/20 22:23;apachespark;User 'karuppayya' has created a pull request for this issue:
https://github.com/apache/spark/pull/28516;;;","14/May/20 06:22;dongjoon;This is resolved via https://github.com/apache/spark/pull/28516;;;","14/May/20 17:09;apachespark;User 'karuppayya' has created a pull request for this issue:
https://github.com/apache/spark/pull/28529;;;",,,,,,,,,,,,,,,,,,,,,
release-build.sh should ignore a fallback output from `build/mvn`,SPARK-31691,13304456,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,12/May/20 20:53,12/May/20 22:57,13/Jul/23 08:46,12/May/20 21:27,2.4.5,3.0.0,,,,,,,,,,,2.4.6,3.0.0,,Project Infra,,,,0,,,"SPARK-28963 prints Falling back to archive.apache.org to download Maven in build/mvn.
This break dev/create-release/release-build.sh",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28963,,,SPARK-31693,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 21:27:17 UTC 2020,,,,,,,,,,"0|z0enmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/May/20 20:57;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28514;;;","12/May/20 20:57;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28514;;;","12/May/20 21:27;dongjoon;This is resolved via https://github.com/apache/spark/pull/28514;;;",,,,,,,,,,,,,,,,,,,,,
Use GitHub instead of GitBox in release script,SPARK-31687,13304402,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,12/May/20 16:16,12/May/20 22:56,13/Jul/23 08:46,12/May/20 20:07,3.0.0,,,,,,,,,,,,3.1.0,,,Project Infra,,,,0,,,"Currently, Spark Packaing jobs are broken.

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20Packaging/job/spark-master-maven-snapshots/2906/console

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20Packaging/job/spark-branch-3.0-maven-snapshots/105/console

 - https://amplab.cs.berkeley.edu/jenkins/view/Spark%20Packaging/job/spark-branch-2.4-maven-snapshots/439/console
{code:java}
fatal: unable to access 'https://gitbox.apache.org/repos/asf/spark.git/': Failed to connect to gitbox.apache.org port 443: Connection timed out{code}",,apachespark,dongjoon,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31693,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 20:07:24 UTC 2020,,,,,,,,,,"0|z0enao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/May/20 16:18;dongjoon;Hi, [~shaneknapp]. Could you take a look at this please?;;;","12/May/20 16:26;shaneknapp;did something change that i was unaware of?;;;","12/May/20 16:31;dongjoon;The code itself seems to be AmbLab script code which outside committers cannot touch.

It can be some rate-limiter issue from Apache GitBox repo.

Shall we use GitHub since it's more robust and stable?;;;","12/May/20 16:52;shaneknapp;actually, it looks to be code in [~pwendell]'s github repo:

https://github.com/pwendell/spark-utils/blob/master/make_release.sh;;;","12/May/20 18:25;dongjoon;Oh. Got it.;;;","12/May/20 18:30;dongjoon;After tracing Patrick's repo, I found that the root cause is inside Apache Spark git repo. Thanks, [~shaneknapp].;;;","12/May/20 19:24;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28513;;;","12/May/20 20:07;dongjoon;Issue resolved by pull request 28513
[https://github.com/apache/spark/pull/28513];;;",,,,,,,,,,,,,,,,
Overwrite partition failed with 'WRONG FS' when the target partition is not belong to the filesystem as same as the table ,SPARK-31684,13304280,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,12/May/20 06:15,19/May/20 14:09,13/Jul/23 08:46,19/May/20 14:09,2.1.3,2.2.3,2.3.4,2.4.5,3.0.0,3.1.0,,,,,,,3.1.0,,,SQL,,,,0,,,"With https://issues.apache.org/jira/browse/SPARK-18107, we will disable the underlying replace(overwrite) and instead do delete in spark side and only do copy in hive side to bypass the performance issue - https://issues.apache.org/jira/browse/HIVE-11940
 
Conditionally, if the table location and partition location do not belong to the same [[FileSystem]], We should not disable hive overwrite. Otherwise, hive will use the [[FileSystem]] instance belong to the table location to copy files, which will fail [[FileSystem#checkPath]]
see https://github.com/apache/hive/blob/rel/release-2.3.7/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java#L1648-L1659",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31675,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 14:09:15 UTC 2020,,,,,,,,,,"0|z0emjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/May/20 06:51;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/28511;;;","15/May/20 08:23;cloud_fan;This is not a new bug in 3.0, and shouldn't be marked as a blocker. I'm changing to major.;;;","19/May/20 14:09;cloud_fan;Issue resolved by pull request 28511
[https://github.com/apache/spark/pull/28511];;;",,,,,,,,,,,,,,,,,,,,,
Python multiclass logistic regression evaluate should return LogisticRegressionSummary,SPARK-31681,13304220,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,huaxingao,huaxingao,huaxingao,11/May/20 22:09,14/May/20 15:55,13/Jul/23 08:46,14/May/20 15:55,3.0.0,,,,,,,,,,,,3.0.0,,,ML,PySpark,,,0,release-notes,,"{code:java}
    def evaluate(self, dataset):
        ......
        java_blr_summary = self._call_java(""evaluate"", dataset)
        return BinaryLogisticRegressionSummary(java_blr_summary)
{code}

We should return LogisticRegressionSummary instead of BinaryLogisticRegressionSummary for multiclass LogisticRegression",,apachespark,huaxingao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,"In Spark 3.0, a multiclass logistic regression in Pyspark will now (correctly) return LogisticRegressionSummary, not the subclass BinaryLogisticRegressionSummary. The additional methods exposed by BinaryLogisticRegressionSummary would not work in this case anyway.",false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 15:55:28 UTC 2020,,,,,,,,,,"0|z0em68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/May/20 22:19;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/28503;;;","11/May/20 22:20;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/28503;;;","14/May/20 15:55;srowen;Resolved by https://github.com/apache/spark/pull/28503;;;",,,,,,,,,,,,,,,,,,,,,
QuantileDiscretizer raise error parameter splits given invalid value (splits array includes -0.0 and 0.0),SPARK-31676,13304053,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,11/May/20 09:22,16/May/20 02:01,13/Jul/23 08:46,14/May/20 14:28,2.4.5,3.0.0,,,,,,,,,,,2.4.6,3.0.0,,ML,,,,0,,,"Reproduce code

{code}

import scala.util.Random
val rng = new Random(3)

val a1 = Array.tabulate(200)(_=>rng.nextDouble * 2.0 - 1.0) ++ Array.fill(20)(0.0) ++ Array.fill(20)(-0.0)

import spark.implicits._
val df1 = sc.parallelize(a1, 2).toDF(""id"")

import org.apache.spark.ml.feature.QuantileDiscretizer
val qd = new QuantileDiscretizer().setInputCol(""id"").setOutputCol(""out"").setNumBuckets(200).setRelativeError(0.0)

val model = qd.fit(df1)

{code}

Raise error like:

  at org.apache.spark.ml.param.Param.validate(params.scala:76)
  at org.apache.spark.ml.param.ParamPair.<init>(params.scala:634)
  at org.apache.spark.ml.param.Param.$minus$greater(params.scala:85)
  at org.apache.spark.ml.param.Params.set(params.scala:713)
  at org.apache.spark.ml.param.Params.set$(params.scala:712)
  at org.apache.spark.ml.PipelineStage.set(Pipeline.scala:41)
  at org.apache.spark.ml.feature.Bucketizer.setSplits(Bucketizer.scala:77)
  at org.apache.spark.ml.feature.QuantileDiscretizer.fit(QuantileDiscretizer.scala:231)
  ... 49 elided
java.lang.IllegalArgumentException: quantileDiscretizer_479bb5a3ca99 parameter splits given invalid value [-Infinity,-0.9986765732730827,..., -0.0, 0.0, ..., 0.9907184077958491,Infinity]

0.0 > -0.0 is False, which break the paremater validation check.

",,apachespark,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 14:28:29 UTC 2020,,,,,,,,,,"0|z0el54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/May/20 09:43;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/28498;;;","14/May/20 14:28;srowen;Resolved by https://github.com/apache/spark/pull/28498;;;",,,,,,,,,,,,,,,,,,,,,,
Wrong error message in VectorAssembler  when column lengths can not be inferred,SPARK-31671,13303862,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,fan31415,fan31415,fan31415,09/May/20 17:33,16/May/20 02:01,13/Jul/23 08:46,11/May/20 23:24,2.4.4,,,,,,,,,,,,2.4.6,3.0.0,,ML,,,,0,,,"In VectorAssembler when input column lengths can not be inferred and handleInvalid = ""keep"", it will throw a runtime exception with message like below

_Can not infer column lengths with handleInvalid = ""keep"". *Consider using VectorSizeHint*_
 *_|to add metadata for columns: [column1, column2]_*

However, even if you set vector size hint for *column1*, the message remains, and will not change to  *[column2]* only. This is not consistent with the description in the error message.

This introduce difficulties when I try to resolve this exception, for I do not know which column required vectorSizeHint. This is especially troublesome when you have a large number of columns to deal with.

Here is a simple example:

 
{code:java}
// create a df without vector size
val df = Seq(
  (Vectors.dense(1.0), Vectors.dense(2.0))
).toDF(""n1"", ""n2"")

// only set vector size hint for n1 column
val hintedDf = new VectorSizeHint()
  .setInputCol(""n1"")
  .setSize(1)
  .transform(df)

// assemble n1, n2
val output = new VectorAssembler()
  .setInputCols(Array(""n1"", ""n2""))
  .setOutputCol(""features"")
  .setHandleInvalid(""keep"")
  .transform(hintedDf)

// because only n1 has vector size, the error message should tell us to set vector size for n2 too
output.show()
{code}
Expected error message:

 
{code:java}
Can not infer column lengths with handleInvalid = ""keep"". Consider using VectorSizeHint to add metadata for columns: [n2].
{code}
Actual error message:
{code:java}
Can not infer column lengths with handleInvalid = ""keep"". Consider using VectorSizeHint to add metadata for columns: [n1, n2].
{code}
I change one line in VectorAssembler.scala, so that it can work properly as expected. ",Mac OS  catalina,apachespark,fan31415,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 11 23:24:57 UTC 2020,,,,,,,,,,"0|z0ejyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/May/20 19:36;fan31415;The pull request to fix this issue [https://github.com/apache/spark/pull/28487];;;","09/May/20 19:37;apachespark;User 'fan31415' has created a pull request for this issue:
https://github.com/apache/spark/pull/28487;;;","09/May/20 19:37;apachespark;User 'fan31415' has created a pull request for this issue:
https://github.com/apache/spark/pull/28487;;;","11/May/20 23:24;srowen;Resolved by https://github.com/apache/spark/pull/28487;;;",,,,,,,,,,,,,,,,,,,,
Using complex type in Aggregation with cube failed Analysis error,SPARK-31670,13303852,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,09/May/20 14:52,21/Apr/21 00:15,13/Jul/23 08:46,02/Sep/20 13:49,2.4.0,3.0.0,,,,,,,,,,,3.1.0,,,SQL,,,,0,,,"Will wrong with below SQL
{code:java}
test(""TEST STRUCT FIELD WITH GROUP BY with CUBE"") {
  withTable(""t1"") {
    sql(
      """"""create table t1(
        |a string,
        |b int,
        |c array<struct<row_id:int,json_string:string>>)
        |using orc"""""".stripMargin)

    sql(
      """"""
        |select a, coalesce(get_json_object(each.json_string,'$.iType'),'-127') as iType, sum(b)
        |from t1
        |LATERAL VIEW explode(c) x AS each
        |group by a, get_json_object(each.json_string,'$.iType')
        |with cube
        |"""""".stripMargin).explain(true)
  }
}
{code}
Error 
{code:java}
expression 'x.`each`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;
Aggregate [a#230, get_json_object(each#222.json_string AS json_string#223, $.iType)#231, spark_grouping_id#229L], [a#230, coalesce(get_json_object(each#222.json_string, $.iType), -127) AS iType#218, sum(cast(b#220 as bigint)) AS sum(b)#226L]
+- Expand [List(a#219, b#220, c#221, each#222, a#227, get_json_object(each#222.json_string AS json_string#223, $.iType)#228, 0), List(a#219, b#220, c#221, each#222, a#227, null, 1), List(a#219, b#220, c#221, each#222, null, get_json_object(each#222.json_string AS json_string#223, $.iType)#228, 2), List(a#219, b#220, c#221, each#222, null, null, 3)], [a#219, b#220, c#221, each#222, a#230, get_json_object(each#222.json_string AS json_string#223, $.iType)#231, spark_grouping_id#229L]
   +- Project [a#219, b#220, c#221, each#222, a#219 AS a#227, get_json_object(each#222.json_string, $.iType) AS get_json_object(each#222.json_string AS json_string#223, $.iType)#228]
      +- Generate explode(c#221), false, x, [each#222]
         +- SubqueryAlias spark_catalog.default.t1
            +- Relation[a#219,b#220,c#221] orc
{code}",,angerszhuuu,apachespark,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34713,SPARK-34639,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 02 13:49:33 UTC 2020,,,,,,,,,,"0|z0ejwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/May/20 14:57;angerszhuuu;It is because for Complex type Field, it will add a Alias by default but with different exprId,

This make *ResolveGroupingAnalytics.constructAggregateExprs*. failed to replace expression by expand output attribute.;;;","09/May/20 15:20;angerszhuuu;raise a pr soon;;;","10/May/20 14:50;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/28490;;;","02/Sep/20 13:49;cloud_fan;Issue resolved by pull request 28490
[https://github.com/apache/spark/pull/28490];;;",,,,,,,,,,,,,,,,,,,,
RowEncoderSuite.encode/decode fails on 1000-02-29,SPARK-31669,13303823,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maxgekk,maxgekk,maxgekk,09/May/20 09:14,10/May/20 19:23,13/Jul/23 08:46,10/May/20 19:23,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,Tests,,,0,,,"Here is the failure https://github.com/apache/spark/pull/28481#issuecomment-626034381:
{code}
org.scalatest.exceptions.TestFailedException:  schema: struct<null:null,boolean:boolean,byte:tinyint,short:smallint,int:int,long:bigint,float:float,double:double,decimal:decimal(38,18),string:string,binary:binary,date:date,timestamp:timestamp,udt:examplepoint> input: [null,true,127,-32768,684257610,3148440411416190456,Infinity,null,2.036236359763072870,ം뵡所碆ᚯ᧳ꒁ밯झ᧱휽⑲岫遳翁㎊륣䓵읹씶읽Â␣⪸붵끂꩖⟭䶄裻乌⇇깍뵙࿻偁뷩셙녶퐾귘嫫䍧쩔ꆁ䠾ՠ訣췐つ亙⚓깠긄蚣꿞묌泓㘡ⵆ橾櫻膋뿽⮎㖍杘䊣臼穇붘켑镅抎灕쿿ァ쏍㤰酀旬槳鑻槸놛턌춅ꉪ陪⡉法耸郄篍㹏吡ط汢측䱣 莶婚ⳟ슿쓻̷흖〦湶ဎ銓霁叹롄ᯕ珅䅃卩慗銁묠쯟ሄ啕澻矌軈憃䑋餤I쒚ᡭ⪩⚋湐蒒ジ䝱綅媪㍉芸礮猱耳藁笲⽽壶젅溜穸⫾룚྿뇳Ѩ䍢넪谦⎠줊넳楼橨䖊ꪗꚔ鬜⋍羯ሾ삦毜਷뢍෍⛛᭟莽糸꟤픣좖뮋撜혍牭ӎ뢂験ꆪᩉ跙㌌ᔸꦐ〷旽k텁ଘ쩧媉❛뛽뷺㱂᪭挃ቿ셾⁞邞郰홋쀘ᜍ뉿ഁ꓌迭梽ዳ硟崤쑼놱뎬蓬覄挗뾱뉍枈懂⼞ܭ갸첟ᢍ燃Ò䦛∫㦿ᶡ랗ђ䓸쑾퀷ၓ鍖霃솄⨗얔嫚ꨵ캁큰࿩ߢ䌵Ⓡ扛郾ꌟ䫀㑈瓺냾෍厌ᇗ玹띏푏㻛䰁ᤠ邰굇뷉恃ᦜ⶞쾖戀諕돚裹聼鬽劙Ἱ䏐烗䢭뉁ꏼⴾ欆⛺坶磩̿꽦⾩綬跩玉谩嶂퇗떾心鵈짘쒸봐傱䦂殏┗ח듅宥㣠ꘙ㽟忽ଟ겚鄀梧ж䋁癫剫㠉繮੫ݽ櫌非剖䤖噹뫒圏쬧罍氒ញ梶印䶋蝗杨윇鬑䰡笤㜇梀큦먚碈蝠⒊쩔蹂ૡወ쵩襒ᇳ擴ꓙ踘짧㤫倍趯鱘剨궐ঔ⇮ᶄಂ꼉⨛插柬ᒠ뇯뉒Ⱛ돝ヘ枀冗ꈑ筚綃놪㞴滅䷀ቿ䋃絚孝⏍ɞԃ灚诔懠卮쬸υ뇺闭䆲葞颫頴渋皒夂Ⲧ蟹폊綘ꥄ悈匢觏奴둇⺮웧쭑析윘ⴉ㯒罧䔫妬滢顂⺀ᶠ洷㈋祵鼲꿓阤煳⪧耒襠Ὀ蒣尥鴴⿥涜⭕넳ⶬꑷ㢭憾휦蘀暫줔䐳Ӏ膲뜊꾓휔⤻染肽㉟Ὲ돋⏦턝໫⨋噴䡧☘蟾違숶籩헺Ꮼ͵ळ⣣ੲ憋ጴ癤Ś泣ࢨ뎜뚗꽳텭⽊ꦞ⍝臬슯챑捒ᐑ薯闌巡猰恝ᘱ퇨倶掫ύ矞㹿䱟᭵ජꞥ푥✠儦慮௚齵ệ艝傫⠤⾿챔លͬ츂궄裐편ἵ핗곐촂Ѷ鋟ໄ櫷諩艄掽᧡輎ଇ颁굺㒔企鲺脞稯흂휾ꆲ駊㲹恾暤ź沥咺ଅᖣ嶀㱎쐢꼕㮚ⴞĒ䯭튔㹶ꯜꇙ廦㏚颿垌빫ࠣ悰흥꧆괱鈋暶ᭇ燙㐇뿜閆䩋쾽䉄ៈᵅ칇ચ厑济갺캜㤩봫껫衴㎱롺藪夞䃡㮛픳餣୅최껐ꮾꃼ友Ῡ磗༩ꡐ흏崋䰖牀㨊䞋ᓊ㺧೏ᔣꥱ룛ᚁ἟爥呯ᩮၥ㴳᭿㗀籧鮶噿浦ٰ癝牻⬬䷗㽂醙ꨞੇ굾鏬⑰酚곥ℰ菁εⓤ嶐媒帊녲湙犉ܒ啹⾧孨䜸錸ஊ쐡ᾫ㊮夒䇏繍힂ᡗ奄輽섚肫쀺왗隬㨖ⲝⵙ껽狇貥෫孒톶鄜趿滃逅ꨎ䫻⁡箚美뮣湾梠贉遚줐㞻䴳떛齿楂ᣀ䟯再ꨬ驂䉭ꇜ,[B@3f1a4861,1970-01-01,1000-02-29 10:11:12.123,null]
{code}",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 10 19:23:08 UTC 2020,,,,,,,,,,"0|z0ejq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/May/20 18:19;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28486;;;","10/May/20 19:23;srowen;Resolved by https://github.com/apache/spark/pull/28486;;;",,,,,,,,,,,,,,,,,,,,,,
Saving and loading HashingTF leads to hash function changed,SPARK-31668,13303771,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,weichenxu123,weichenxu123,weichenxu123,09/May/20 01:50,12/May/20 15:57,13/Jul/23 08:46,12/May/20 15:57,3.0.0,3.0.1,3.1.0,,,,,,,,,,3.0.0,,,ML,,,,0,,,"If we use spark 2.x save HashingTF, and then use spark 3.0 load it, and then use spark 3.0 to save it again, and then use spark 3.0 to load it again, the hash function will be changed.

This bug is hard to debug, we need to fix it.",,apachespark,mengxr,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 15:57:03 UTC 2020,,,,,,,,,,"0|z0ejeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/May/20 02:26;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/28413;;;","12/May/20 15:57;mengxr;Issue resolved by pull request 28413
[https://github.com/apache/spark/pull/28413];;;",,,,,,,,,,,,,,,,,,,,,,
Grouping sets with having clause returns the wrong result,SPARK-31663,13303684,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,08/May/20 16:24,01/Jun/20 17:32,13/Jul/23 08:46,16/May/20 05:36,2.0.2,2.1.3,2.2.3,2.3.4,2.4.5,3.0.0,,,,,,,2.4.6,3.0.0,,SQL,,,,0,correctness,,"Grouping sets with having clause returns the wrong result when the condition of having contained conflicting naming. See the below example:
{code:java}
select sum(a) as b FROM VALUES (1, 10), (2, 20) AS T(a, b) group by GROUPING SETS ((b), (a, b)) having b > 10{code}
The `b` in `having b > 10` should be resolved as `T.b` not `sum(a)`, so the right result should be
{code:java}
+---+
|  b|
+---+
|  2|
|  2|
+---+{code}
instead of an empty result.

The root cause is similar to SPARK-31519, it's caused by we parsed HAVING as Filter(..., Agg(...)) and resolved these two parts in different rules. The CUBE and ROLLUP have the same issue.

Other systems worked as expected, I checked PostgreSQL 9.6 and MS SQL Server 2017.

 

For Apache Spark 2.0.2 ~ 2.3.4, the following query is tested.
{code:java}
spark-sql> select sum(a) as b from t group by b grouping sets(b) having b > 10;
Time taken: 0.194 seconds

hive> select sum(a) as b from t group by b grouping sets(b) having b > 10;
2
Time taken: 1.605 seconds, Fetched: 1 row(s) {code}",,angerszhuuu,apachespark,dongjoon,holden,waleedfateem,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 11 14:56:01 UTC 2020,,,,,,,,,,"0|z0eivc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/May/20 18:37;dongjoon;I confirmed that this is a correctness issue since 2.4.0.;;;","08/May/20 18:42;dongjoon;The issue is that `b` is interpreted differently in `group by b` and `group by grouping sets(b)`.
{code:java}
spark-sql> select sum(a) as b from t group by grouping sets(b) having b > 10;
Time taken: 0.231 seconds
spark-sql> select sum(a) as b from t group by b having b > 10;
2
Time taken: 0.174 seconds, Fetched 1 row(s) {code};;;","08/May/20 18:51;dongjoon;Also, with a changed syntax, this is reproduced in older versions like 2.3.x, too.;;;","08/May/20 18:56;dongjoon;Apache Spark 2.3.4 follows Hive syntaxes, but the result is also wrong while Hive is correct.
{code:java}
spark-sql> select sum(a) as b from t group by b grouping sets(b) having b > 10;
Time taken: 0.194 seconds

hive> select sum(a) as b from t group by b grouping sets(b) having b > 10;
2
Time taken: 1.605 seconds, Fetched: 1 row(s){code};;;","08/May/20 19:00;dongjoon;All 2.x versions are added into the affected versions. (cc [~holden]);;;","11/May/20 14:56;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/28501;;;",,,,,,,,,,,,,,,,,,
Upgrade snappy to version 1.1.7.5,SPARK-31655,13303256,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,07/May/20 03:14,01/Jun/20 17:32,13/Jul/23 08:46,07/May/20 19:02,3.0.0,,,,,,,,,,,,2.4.6,3.0.0,,Build,,,,0,,,Upgrade snappy to version 1.1.7.5,,angerszhuuu,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 04:15:39 UTC 2020,,,,,,,,,,"0|z0egjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/May/20 03:33;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/28472;;;","07/May/20 03:34;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/28472;;;","07/May/20 19:02;dongjoon;Issue resolved by pull request 28472
[https://github.com/apache/spark/pull/28472];;;","12/May/20 03:27;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/28506;;;","12/May/20 03:28;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/28506;;;","12/May/20 04:09;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/28507;;;","12/May/20 04:10;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/28507;;;","12/May/20 04:12;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/28508;;;","12/May/20 04:15;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/28509;;;",,,,,,,,,,,,,,,
setuptools needs to be installed before anything else,SPARK-31653,13303210,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,holden,holden,holden,06/May/20 21:52,06/May/20 21:57,13/Jul/23 08:46,06/May/20 21:57,2.4.6,,,,,,,,,,,,2.4.6,,,Build,,,,0,,,One of the early packages we install as part of the release build in the Dockerfile now requires setuptools to be pre-installed.,,apachespark,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 21:57:46 UTC 2020,,,,,,,,,,"0|z0eg94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/May/20 21:57;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28467;;;",,,,,,,,,,,,,,,,,,,,,,,
Improve handling the case where different barrier sync types in a single sync,SPARK-31651,13303072,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,06/May/20 09:34,19/May/20 07:19,13/Jul/23 08:46,19/May/20 07:18,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"Currently, we use cleanupBarrierStage when detecting different barrier sync types in a single sync. This cause a problem that a new `ContextBarrierState` can be created again if there's following requesters on the way. And those corresponding tasks will fail because of killing instead of different barrier sync types detected.

 

 ",,apachespark,jiangxb1987,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 07:18:40 UTC 2020,,,,,,,,,,"0|z0efeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/May/20 09:50;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/28462;;;","19/May/20 07:18;jiangxb1987;Resolved by https://github.com/apache/spark/pull/28462;;;",,,,,,,,,,,,,,,,,,,,,,
SQL UI doesn't show metrics and whole stage codegen in AQE,SPARK-31650,13303041,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,06/May/20 07:35,06/May/20 12:53,13/Jul/23 08:46,06/May/20 12:53,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"When enabling AQE with subqueris within a query, the SQL UI may doesn't show metrics and whole stage codegen.

Here's a reproduce demo:

 
{code:java}
spark.range(10000).toDF(""value"").write.parquet(""/tmp/p1"")

spark.range(10000).toDF(""value"").write.parquet(""/tmp/p2"")

spark.read.parquet(""/tmp/p1"").createOrReplaceTempView(""t1"")

spark.read.parquet(""/tmp/p2"").createOrReplaceTempView(""t2"")


spark.sql(""select * from t1 where value=(select Max(value) from t2)"").show()
{code}
 

 ",,apachespark,cloud_fan,ekoifman,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/May/20 07:35;Ngone51;before_aqe_ui.png;https://issues.apache.org/jira/secure/attachment/13002139/before_aqe_ui.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 12:53:25 UTC 2020,,,,,,,,,,"0|z0ef80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/May/20 07:55;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/28460;;;","06/May/20 12:53;cloud_fan;Issue resolved by pull request 28460
[https://github.com/apache/spark/pull/28460];;;",,,,,,,,,,,,,,,,,,,,,,
Deprecate 'spark.sql.optimizer.metadataOnly',SPARK-31647,13303015,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,06/May/20 03:41,12/Dec/22 17:50,13/Jul/23 08:46,07/May/20 00:02,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"This optimization can cause a potential correctness issue, see also SPARK-26709.
Also, it seems difficult to extend the optimization. Basically you should whitelist all available functions.

Seems we should rather deprecate and remove this.",,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 07 00:02:19 UTC 2020,,,,,,,,,,"0|z0ef28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/May/20 04:02;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28459;;;","06/May/20 04:02;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/28459;;;","07/May/20 00:02;maropu;Resolved by https://github.com/apache/spark/pull/28459;;;",,,,,,,,,,,,,,,,,,,,,
The ApplicationInfo in KVStore may be accessed before it's prepared,SPARK-31632,13302539,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,xccui,xccui,xccui,03/May/20 21:50,12/Dec/22 18:11,13/Jul/23 08:46,14/May/20 03:15,3.0.0,,,,,,,,,,,,2.4.6,3.0.0,,Spark Core,Web UI,,,0,,,"While starting some local tests, I occasionally encountered the following exceptions for Web UI.
{noformat}
23:00:29.845 WARN org.eclipse.jetty.server.HttpChannel: /jobs/
 java.util.NoSuchElementException
 at java.util.Collections$EmptyIterator.next(Collections.java:4191)
 at org.apache.spark.util.kvstore.InMemoryStore$InMemoryIterator.next(InMemoryStore.java:467)
 at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:39)
 at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:266)
 at org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:89)
 at org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:80)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1623)
 at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610)
 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:540)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1345)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
 at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:753)
 at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
 at org.eclipse.jetty.server.Server.handle(Server.java:505)
 at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)
 at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
 at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
 at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
 at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
 at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
 at java.lang.Thread.run(Thread.java:748){noformat}
*Reason*
 That is because {{AppStatusStore.applicationInfo()}} accesses an empty view (iterator) returned by {{InMemoryStore}}.

AppStatusStore
{code:java}
def applicationInfo(): v1.ApplicationInfo = {
    store.view(classOf[ApplicationInfoWrapper]).max(1).iterator().next().info
}
{code}
InMemoryStore
{code:java}
public <T> KVStoreView<T> view(Class<T> type){
    InstanceList<T> list = inMemoryLists.get(type);
    return list != null ? list.view() : emptyView();
 }
{code}
During the initialization of {{SparkContext}}, it first starts the Web UI (SparkContext: L475 _ui.foreach(_.bind())) and then setup the {{LiveListenerBus}} thread (SparkContext: L608 {{setupAndStartListenerBus()}}) for dispatching the {{SparkListenerApplicationStart}} event (which will trigger writing the requested {{ApplicationInfo}} to {{InMemoryStore}}).",,apachespark,xccui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 14 05:20:19 UTC 2020,,,,,,,,,,"0|z0ec48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/May/20 22:35;apachespark;User 'xccui' has created a pull request for this issue:
https://github.com/apache/spark/pull/28444;;;","14/May/20 03:15;gurwls223;Fixed in https://github.com/apache/spark/pull/28444;;;","13/Jun/20 06:57;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/28820;;;","13/Jun/20 06:58;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/28820;;;","14/Jun/20 05:20;gurwls223;Followup https://github.com/apache/spark/commit/c2e5012a0a76734acf94b8716ee293ba2f58ccb4 was merged into Spark 3.0.1 and 2.4.7.;;;",,,,,,,,,,,,,,,,,,,
"Fix test flakiness caused by MiniKdc which throws ""address in use"" BindException",SPARK-31631,13302525,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,03/May/20 16:04,07/May/20 12:05,13/Jul/23 08:46,07/May/20 05:38,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,Tests,,,,0,,,"
{code:java}
[info] org.apache.spark.sql.kafka010.KafkaDelegationTokenSuite *** ABORTED *** (15 seconds, 426 milliseconds)
[info]   java.net.BindException: Address already in use
[info]   at sun.nio.ch.Net.bind0(Native Method)
[info]   at sun.nio.ch.Net.bind(Net.java:433)
[info]   at sun.nio.ch.Net.bind(Net.java:425)
[info]   at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
[info]   at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
[info]   at org.apache.mina.transport.socket.nio.NioSocketAcceptor.open(NioSocketAcceptor.java:198)
[info]   at org.apache.mina.transport.socket.nio.NioSocketAcceptor.open(NioSocketAcceptor.java:51)
[info]   at org.apache.mina.core.polling.AbstractPollingIoAcceptor.registerHandles(AbstractPollingIoAcceptor.java:547)
[info]   at org.apache.mina.core.polling.AbstractPollingIoAcceptor.access$400(AbstractPollingIoAcceptor.java:68)
[info]   at org.apache.mina.core.polling.AbstractPollingIoAcceptor$Acceptor.run(AbstractPollingIoAcceptor.java:422)
[info]   at org.apache.mina.util.NamePreservingRunnable.run(NamePreservingRunnable.java:64)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:748)
{code}


This is an issue fixed in hadoop 2.8.0
https://issues.apache.org/jira/browse/HADOOP-12656

We may need apply the approach from HBASE first before we drop Hadoop 2.7.x

https://issues.apache.org/jira/browse/HBASE-14734




",,apachespark,maropu,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-12656,HBASE-14734,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 07 05:38:08 UTC 2020,,,,,,,,,,"0|z0ec14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/May/20 16:32;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/28442;;;","07/May/20 05:38;maropu;Resolved by https://github.com/apache/spark/pull/28442;;;",,,,,,,,,,,,,,,,,,,,,,
SHOW TBLPROPERTIES doesn't handle Session Catalog correctly,SPARK-31624,13302378,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,02/May/20 00:27,04/May/20 12:22,13/Jul/23 08:46,04/May/20 12:22,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,SHOW TBLPROPERTIES doesn't handle DataSource V2 tables that use the session catalog.,,apachespark,brkyvz,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 04 12:22:58 UTC 2020,,,,,,,,,,"0|z0eb4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/May/20 01:31;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/28434;;;","04/May/20 12:22;cloud_fan;Issue resolved by pull request 28434
[https://github.com/apache/spark/pull/28434];;;",,,,,,,,,,,,,,,,,,,,,,
Spark Master UI Fails to load if application is waiting for workers to launch driver,SPARK-31621,13302289,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,akbordia,akbordia,akbordia,01/May/20 11:03,05/May/20 14:00,13/Jul/23 08:46,05/May/20 14:00,3.0.0,,,,,,,,,,,,3.0.0,3.1.0,,Spark Core,Web UI,,,0,,,"*Steps to Reproduce:*
 # Run Spark Master and 1 Worker (Using local machine).
 # Submit an (long-running) application using spark-submit command: ./bin/spark-submit --master spark://127.0.0.1:7077  --deploy-mode cluster --class org.apache.spark.examples.SparkPi examples/target/original-spark-examples_2.12-3.1.0-SNAPSHOT.jar
 # Load Spark Master UI ([http://localhost:8080/]) which should launch successfully.
 # Submit one more application using the spark-submit command.
 # Load Spark Master UI again.

*Expected Behavior:* Spark Master UI should launch successfully.

*Actual Behavior*: Spark Master UI launch fails with HTTP Error 500 as shown in the below screenshot.

!SparkMasterError.png|width=530,height=411!",,akbordia,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/May/20 11:05;akbordia;SparkMasterError.png;https://issues.apache.org/jira/secure/attachment/13001808/SparkMasterError.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 01 11:15:34 UTC 2020,,,,,,,,,,"0|z0eako:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/May/20 11:15;apachespark;User 'akshatb1' has created a pull request for this issue:
https://github.com/apache/spark/pull/28429;;;",,,,,,,,,,,,,,,,,,,,,,,
"TreeNodeException: Binding attribute, tree: sum#19L",SPARK-31620,13302265,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,yumwang,yumwang,01/May/20 06:31,15/May/20 15:37,13/Jul/23 08:46,15/May/20 15:37,2.3.4,2.4.5,3.0.0,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"
{noformat}
scala> spark.sql(""create temporary view t1 as select * from values (1, 2) as t1(a, b)"")
res0: org.apache.spark.sql.DataFrame = []

scala> spark.sql(""create temporary view t2 as select * from values (3, 4) as t2(c, d)"")
res1: org.apache.spark.sql.DataFrame = []

scala> spark.sql(""select sum(if(c > (select a from t1), d, 0)) as csum from t2"").show
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: sum#19L
  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:75)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:74)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:309)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:309)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChild$2(TreeNode.scala:368)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$4(TreeNode.scala:427)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
  at scala.collection.TraversableLike.map(TraversableLike.scala:238)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
  at scala.collection.AbstractTraversable.map(Traversable.scala:108)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:427)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:298)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:74)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:96)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at scala.collection.TraversableLike.map(TraversableLike.scala:238)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
  at scala.collection.immutable.List.map(List.scala:298)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:96)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doConsumeWithoutKeys$4(HashAggregateExec.scala:348)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at scala.collection.TraversableLike.map(TraversableLike.scala:238)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
  at scala.collection.immutable.List.map(List.scala:298)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doConsumeWithoutKeys(HashAggregateExec.scala:347)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doConsume(HashAggregateExec.scala:175)
  at org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:221)
  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:192)
  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)
  at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)
  at org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)
  at org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)
  at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)
  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)
  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)
  at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithoutKeys(HashAggregateExec.scala:243)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:167)
  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)
  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:48)
  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:632)
  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:692)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:316)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:434)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)
  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)
  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3626)
  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2695)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2902)
  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:824)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:783)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:792)
  ... 47 elided
Caused by: java.lang.RuntimeException: Couldn't find sum#19L in [sum#13L,sum#12L]
  at scala.sys.package$.error(package.scala:30)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.$anonfun$applyOrElse$1(BoundAttribute.scala:81)
  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
  ... 140 more
{noformat}
",,angerszhuuu,apachespark,cloud_fan,Ngone51,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 15:37:02 UTC 2020,,,,,,,,,,"0|z0eafc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/May/20 16:35;angerszhuuu;It is because in  PlanSubqueries, we have scalar-subquery in Expression. 

then in transformAllExpression method, since expression changed, in {color:#FF0000}QueryPlan.mapExperssion(){color} method we will make a copy of the tree. For aggregate expression, when makeCopy, it will got a new DeclarativeAggregate expression with lazy val {color:#0747a6}inputAggBufferAttributes{color}  will be null. When we reuse this value in  *HashAggregateExec.doConsumeWithoutKeys,*  {color:#0747a6}inputAggBufferAttributes{color} will be re-initial with a new exprId, then error happended.

 

In right SparkPlan,  inputAggBufferAttributes is same as child's output. ;;;","08/May/20 03:39;angerszhuuu;cc [~cloud_fan];;;","11/May/20 02:18;Ngone51;I'm working on this now.;;;","11/May/20 07:22;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/28496;;;","11/May/20 07:23;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/28496;;;","15/May/20 15:37;cloud_fan;Issue resolved by pull request 28496
[https://github.com/apache/spark/pull/28496];;;",,,,,,,,,,,,,,,,,,
Fix spark.kubernetes.executor.podNamePrefix to work,SPARK-31601,13301714,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,29/Apr/20 06:24,17/May/20 18:26,13/Jul/23 08:46,30/Apr/20 05:10,2.4.5,3.0.0,,,,,,,,,,,2.4.6,3.0.0,,Kubernetes,Spark Core,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 30 05:10:10 UTC 2020,,,,,,,,,,"0|z0e70w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/20 03:05;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28401;;;","30/Apr/20 05:10;dongjoon;This is resolved via https://github.com/apache/spark/pull/28401;;;",,,,,,,,,,,,,,,,,,,,,,
LegacySimpleTimestampFormatter incorrectly interprets pre-Gregorian timestamps,SPARK-31598,13301610,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,28/Apr/20 16:41,09/Feb/21 01:54,13/Jul/23 08:46,01/May/20 15:59,3.0.0,3.1.0,,,,,,,,,,,3.0.0,3.1.0,,SQL,,,,0,,,"As per discussion with [~maxgekk]:

{{LegacySimpleTimestampFormatter#parse}} misinterprets pre-Gregorian timestamps:
{noformat}
scala> sql(""set spark.sql.legacy.timeParserPolicy=LEGACY"")
res0: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala> val df1 = Seq(""0002-01-01 00:00:00"", ""1000-01-01 00:00:00"", ""1800-01-01 00:00:00"").toDF(""expected"")
df1: org.apache.spark.sql.DataFrame = [expected: string]

scala> val df2 = df1.select('expected, to_timestamp('expected, ""yyyy-MM-dd HH:mm:ss"").as(""actual""))
df2: org.apache.spark.sql.DataFrame = [expected: string, actual: timestamp]

scala> df2.show(truncate=false)
+-------------------+-------------------+
|expected           |actual             |
+-------------------+-------------------+
|0002-01-01 00:00:00|0001-12-30 00:00:00|
|1000-01-01 00:00:00|1000-01-06 00:00:00|
|1800-01-01 00:00:00|1800-01-01 00:00:00|
+-------------------+-------------------+


scala> 
{noformat}
Legacy timestamp parsing with JSON and CSV files is correct, so apparently {{LegacyFastTimestampFormatter}} does not have this issue (need to double check).",,bersprockets,JinxinTang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 01 15:59:30 UTC 2020,,,,,,,,,,"0|z0e6e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/May/20 06:59;JinxinTang;same as : https://issues.apache.org/jira/browse/SPARK-31557

already fix in master latest code;;;","01/May/20 15:59;bersprockets;Thanks. SPARK-31557 was used for fixing both DATEs and TIMESTAMPs.;;;",,,,,,,,,,,,,,,,,,,,,,
Spark sql cli should allow unescaped quote mark in quoted string,SPARK-31595,13301519,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,adrian-wang,adrian-wang,28/Apr/20 10:31,06/May/20 04:35,13/Jul/23 08:46,06/May/20 04:35,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"spark-sql> select ""'"";
spark-sql> select '""';

In Spark parser if we pass a text of `select ""'"";`, there will be ParserCancellationException, which will be handled by PredictionMode.LL. By dropping `;` correctly we can avoid that retry.",,adrian-wang,Ankitraj,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 04:35:13 UTC 2020,,,,,,,,,,"0|z0e5ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/20 19:46;Ankitraj;[~adrian-wang], can i start working on this issue ?;;;","29/Apr/20 01:56;adrian-wang;[~Ankitraj] Thanks, I have already created a pull request on this.;;;","06/May/20 04:35;cloud_fan;Issue resolved by pull request 28393
[https://github.com/apache/spark/pull/28393];;;",,,,,,,,,,,,,,,,,,,,,
Metadata-only queries should not include subquery in partition filters,SPARK-31590,13301443,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dzcxzl,dzcxzl,dzcxzl,28/Apr/20 04:04,12/Dec/22 18:11,13/Jul/23 08:46,06/May/20 01:57,2.4.0,,,,,,,,,,,,2.4.6,3.0.0,,SQL,,,,0,,,"When using SPARK-23877, some sql execution errors.

code:
{code:scala}
        sql(""set spark.sql.optimizer.metadataOnly=true"")
        sql(""CREATE TABLE test_tbl (a INT,d STRING,h STRING) USING PARQUET PARTITIONED BY (d ,h)"")
        sql(""""""
            |INSERT OVERWRITE TABLE test_tbl PARTITION(d,h)
            |SELECT 1,'2020-01-01','23'
            |UNION ALL
            |SELECT 2,'2020-01-02','01'
            |UNION ALL
            |SELECT 3,'2020-01-02','02'
            """""".stripMargin)
        sql(
          s""""""
             |SELECT d, MAX(h) AS h
             |FROM test_tbl
             |WHERE d= (
             |  SELECT MAX(d) AS d
             |  FROM test_tbl
             |)
             |GROUP BY d
        """""".stripMargin).collect()
{code}
Exception:
{code:java}
java.lang.UnsupportedOperationException: Cannot evaluate expression: scalar-subquery#48 []

...
at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.prunePartitions(PartitioningAwareFileIndex.scala:180)
{code}
optimizedPlan:
{code:java}
Aggregate [d#245], [d#245, max(h#246) AS h#243]
+- Project [d#245, h#246]
   +- Filter (isnotnull(d#245) AND (d#245 = scalar-subquery#242 []))
      :  +- Aggregate [max(d#245) AS d#241]
      :     +- LocalRelation <empty>, [d#245]
      +- Relation[a#244,d#245,h#246] parquet
{code}",,apachespark,dzcxzl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 01:57:24 UTC 2020,,,,,,,,,,"0|z0e5cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/20 16:53;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/28383;;;","06/May/20 01:57;gurwls223;Issue resolved by pull request 28383
[https://github.com/apache/spark/pull/28383];;;",,,,,,,,,,,,,,,,,,,,,,
Use `r-lib/actions/setup-r` in GitHub Action,SPARK-31589,13301439,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,28/Apr/20 03:58,12/Dec/22 18:10,13/Jul/23 08:46,28/Apr/20 04:25,2.4.5,3.0.0,3.1.0,,,,,,,,,,2.4.6,3.0.0,,Project Infra,,,,0,,,"`r-lib/actions/setup-r` is more stabler and maintained 3rd party action.

I made this issue as `Bug` since the branch is currently broken.",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31587,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 28 04:25:14 UTC 2020,,,,,,,,,,"0|z0e5c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/20 04:25;gurwls223;Fixed in https://github.com/apache/spark/pull/28382;;;",,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when parsing event log with InMemoryStore,SPARK-31584,13301415,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Baohe Zhang,Baohe Zhang,Baohe Zhang,28/Apr/20 00:16,23/Jun/20 05:16,13/Jul/23 08:46,29/Apr/20 00:30,3.0.1,,,,,,,,,,,,3.0.1,3.1.0,,Web UI,,,,0,,,"I compiled with the current branch-3.0 source and tested it in mac os. A java.lang.NullPointerException will be thrown when below conditions are met: 
 # Using InMemoryStore as kvstore when parsing the event log file (e.g., when spark.history.store.path is unset). 
 # At least one stage in this event log has task number greater than spark.ui.retainedTasks (by default is 100000). In this case, kvstore needs to delete extra task records.
 # The job has more than one stage, so parentToChildrenMap in InMemoryStore.java will have more than one key.

The java.lang.NullPointerException is thrown in InMemoryStore.java :296. In the method deleteParentIndex().
{code:java}
    private void deleteParentIndex(Object key) {
      if (hasNaturalParentIndex) {
        for (NaturalKeys v : parentToChildrenMap.values()) {
          if (v.remove(asKey(key))) {
            // `v` can be empty after removing the natural key and we can remove it from
            // `parentToChildrenMap`. However, `parentToChildrenMap` is a ConcurrentMap and such
            // checking and deleting can be slow.
            // This method is to delete one object with certain key, let's make it simple here.
            break;
          }
        }
      }
    }{code}
In “if (v.remove(asKey(key)))”, if the key is not contained in v,  ""v.remove(asKey(key))"" will return null, and java will throw a NullPointerException when executing ""if (null)"".

An exception stack trace is attached.

This issue can be fixed by updating if statement to
{code:java}
if (v.remove(asKey(key)) != null){code}",,Baohe Zhang,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/20 00:17;Baohe Zhang;errorstack.txt;https://issues.apache.org/jira/secure/attachment/13001385/errorstack.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 29 00:30:24 UTC 2020,,,,,,,,,,"0|z0e56o:",9223372036854775807,,,,,,,,,,,,,3.0.0,3.0.1,3.1.0,,,,,,,,"29/Apr/20 00:30;Gengliang.Wang;The issue is resolved in https://github.com/apache/spark/pull/28378;;;",,,,,,,,,,,,,,,,,,,,,,,
Upgrade Apache ORC to 1.5.10,SPARK-31580,13301329,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,27/Apr/20 16:12,28/Apr/20 01:57,13/Jul/23 08:46,28/Apr/20 01:57,3.0.0,,,,,,,,,,,,3.0.0,,,Build,SQL,,,0,,,,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 28 01:57:10 UTC 2020,,,,,,,,,,"0|z0e4nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/20 01:57;dongjoon;Issue resolved by pull request 28373
[https://github.com/apache/spark/pull/28373];;;",,,,,,,,,,,,,,,,,,,,,,,
fix various problems when check name conflicts of CTE relations,SPARK-31577,13301285,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,27/Apr/20 13:58,27/Apr/20 23:48,13/Jul/23 08:46,27/Apr/20 23:48,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,,,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 27 23:48:31 UTC 2020,,,,,,,,,,"0|z0e4ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/20 23:48;dongjoon;This is resolved via https://github.com/apache/spark/pull/28371;;;",,,,,,,,,,,,,,,,,,,,,,,
Unable to return Hive data into Spark via Hive JDBC driver Caused by:  org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED,SPARK-31576,13301280,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,liuzhang,liuzhang,27/Apr/20 13:41,30/Apr/20 05:30,13/Jul/23 08:46,30/Apr/20 05:30,2.3.1,,,,,,,,,,,,,,,Spark Shell,Spark Submit,,,0,,,"I'm trying to fetch back data in Spark SQL using a JDBC connection to Hive. Unfortunately, when I try to query data that resides in every column I get the following error:

Caused by: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException [Error 10004]: Line 1:7 Invalid table alias or column reference 'test.aname': (possible column names are: aname, score, banji)
 at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:335)
 at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:199)

1)  On Hive create a simple table,its name is ""test""，it have three column(aname,score,banji),their type both are ""String""

2)important code:

object HiveDialect extends JdbcDialect

{ override def canHandle(url: String): Boolean = url.startsWith(""jdbc:hive2"")|| url.contains(""hive2"")                                                                                                                    override def quoteIdentifier(colName: String): String = s""`$colName`"" }

-------------------------------------------------------------------

object callOffRun {
 def main(args: Array[String]): Unit =

{ val spark = SparkSession.builder().enableHiveSupport().getOrCreate() JdbcDialects.registerDialect(HiveDialect)                                                                                 val props = new Properties()                                                                                                          props.put(""driver"",""org.apache.hive.jdbc.HiveDriver"")                                  props.put(""user"",""username"")                                                         props.put(""password"",""password"")                                                           props.put(""fetchsize"",""20"")                                                                                                     val table=spark.read .jdbc(""jdbc:hive2://xxxxxxxx:10000"",""test"",props)                          table.show() }

}

3)spark-submit ,After running,it have error

Caused by: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException [Error 10004]: Line 1:7 Invalid table alias or column reference 'test.aname': (possible column names are: aname, score, banji)

4)table.count() have result 

5) I try some method to print result,They all reported the same error

 ","hdp 3.0,hadoop 3.1.1，spark 2.3.1",liuzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 30 05:30:03 UTC 2020,,,,,,,,,,"0|z0e4co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/20 05:30;liuzhang;override def quoteIdentifier(colName: String): String = s""$colName"";;;",,,,,,,,,,,,,,,,,,,,,,,
Use fixed=TRUE where possible for internal efficiency,SPARK-31573,13301183,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,michaelchirico,michaelchirico,michaelchirico,27/Apr/20 07:38,12/Dec/22 18:10,13/Jul/23 08:46,28/Apr/20 08:26,2.4.5,,,,,,,,,,,,3.0.0,,,R,,,,0,,,"gsub('_', '', x) is more efficient if we signal there's no regex: gsub('_', '', x, fixed = TRUE)",,michaelchirico,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 28 08:26:14 UTC 2020,,,,,,,,,,"0|z0e3r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/20 08:26;gurwls223;Fixed in https://github.com/apache/spark/pull/28367;;;",,,,,,,,,,,,,,,,,,,,,,,
Flaky test: AllExecutionsPageSuite.SPARK-27019:correctly display SQL page when event reordering happens,SPARK-31564,13300957,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,25/Apr/20 11:56,25/Apr/20 17:27,13/Jul/23 08:46,25/Apr/20 17:27,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"the org.apache.spark.sql.execution.ui.AllExecutionsPageSuite.SPARK-27019:correctly display SQL page when event reordering happens is flaky for just checking the html not containing 1970. I will add a ticket to check and fix that.
In the specific failure https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/121799/testReport, it failed because

...
<td sorttable_customkey=""1587806019707"">
...
contained 1970

",,dongjoon,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27019,SPARK-27125,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 25 17:27:39 UTC 2020,,,,,,,,,,"0|z0e2cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/20 17:27;dongjoon;Issue resolved by pull request 28344
[https://github.com/apache/spark/pull/28344];;;",,,,,,,,,,,,,,,,,,,,,,,
Failure of InSet.sql for UTF8String collection,SPARK-31563,13300946,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,25/Apr/20 09:59,05/Jun/20 19:50,13/Jul/23 08:46,25/Apr/20 16:36,2.4.5,3.0.0,3.1.0,,,,,,,,,,2.4.6,3.0.0,,SQL,,,,0,,,"The InSet expression works on collections of internal Catalyst's types. We can see this in the optimization when In is replaced by InSet, and In's collection is evaluated to internal Catalyst's values: [https://github.com/apache/spark/blob/branch-2.4/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala#L253-L254]
{code:scala}
        if (newList.length > SQLConf.get.optimizerInSetConversionThreshold) {
          val hSet = newList.map(e => e.eval(EmptyRow))
          InSet(v, HashSet() ++ hSet)
        }
{code}
The code existed before the optimization https://github.com/apache/spark/pull/25754 that made another wrong assumption about collection types.

If InSet accepts only internal Catalyst's types, the following code shouldn't fail:
{code:scala}
InSet(Literal(""a""), Set(""a"", ""b"").map(UTF8String.fromString)).sql
{code}
but it fails with the exception:
{code}
Unsupported literal type class org.apache.spark.unsafe.types.UTF8String a
java.lang.RuntimeException: Unsupported literal type class org.apache.spark.unsafe.types.UTF8String a
	at org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:88)
	at org.apache.spark.sql.catalyst.expressions.InSet.$anonfun$sql$2(predicates.scala:522)
{code}
 ",,apachespark,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 19:50:56 UTC 2020,,,,,,,,,,"0|z0e2ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/20 10:00;maxgekk;I am working on the issue;;;","25/Apr/20 16:36;dongjoon;Issue resolved by pull request 28343
[https://github.com/apache/spark/pull/28343];;;","05/Jun/20 19:49;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28399;;;","05/Jun/20 19:50;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28343;;;",,,,,,,,,,,,,,,,,,,,
AM starts with initial fetched tokens in any attempt,SPARK-31559,13300914,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,25/Apr/20 01:16,17/May/20 18:13,13/Jul/23 08:46,12/May/20 00:26,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,YARN,,,0,,,"The issue is only occurred in yarn-cluster mode.

Submitter will obtain delegation tokens for yarn-cluster mode, and add these credentials to the launch context. AM will be launched with these credentials, and AM and driver are able to leverage these tokens.

In Yarn cluster mode, driver is launched in AM, which in turn initializes token manager (while initializing SparkContext) and obtain delegation tokens (+ schedule to renew) if both principal and keytab are available.

That said, even we provide principal and keytab to run application with yarn-cluster mode, AM always starts with initial tokens from launch context until token manager runs and obtains delegation tokens.

So there's a ""gap"", and if user codes (driver) access to external system with delegation tokens (e.g. HDFS) before initializing SparkContext, it cannot leverage the tokens token manager will obtain. It will make the application fail if AM is killed ""after"" the initial tokens are expired and relaunched.",,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 27 02:06:45 UTC 2020,,,,,,,,,,"0|z0e23c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/20 02:06;kabhwan;PR submitted: https://github.com/apache/spark/pull/28336;;;",,,,,,,,,,,,,,,,,,,,,,,
Legacy parser incorrectly interprets pre-Gregorian dates,SPARK-31557,13300909,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,25/Apr/20 00:05,01/May/22 01:24,13/Jul/23 08:46,27/Apr/20 05:04,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"With CSV:
{noformat}
scala> sql(""set spark.sql.legacy.timeParserPolicy=LEGACY"")
res0: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala> val seq = Seq(""0002-01-01"", ""1000-01-01"", ""1500-01-01"", ""1800-01-01"").map(x => s""$x,$x"")
seq: Seq[String] = List(0002-01-01,0002-01-01, 1000-01-01,1000-01-01, 1500-01-01,1500-01-01, 1800-01-01,1800-01-01)

scala> val ds = seq.toDF(""value"").as[String]
ds: org.apache.spark.sql.Dataset[String] = [value: string]

scala> spark.read.schema(""expected STRING, actual DATE"").csv(ds).show
+----------+----------+
|  expected|    actual|
+----------+----------+
|0002-01-01|0001-12-30|
|1000-01-01|1000-01-06|
|1500-01-01|1500-01-10|
|1800-01-01|1800-01-01|
+----------+----------+

scala> 
{noformat}
Similarly, with JSON:
{noformat}
scala> sql(""set spark.sql.legacy.timeParserPolicy=LEGACY"")
res0: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala> val seq = Seq(""0002-01-01"", ""1000-01-01"", ""1500-01-01"", ""1800-01-01"").map { x =>
  s""""""{""expected"": ""$x"", ""actual"": ""$x""}""""""
}

     |      | seq: Seq[String] = List({""expected"": ""0002-01-01"", ""actual"": ""0002-01-01""}, {""expected"": ""1000-01-01"", ""actual"": ""1000-01-01""}, {""expected"": ""1500-01-01"", ""actual"": ""1500-01-01""}, {""expected"": ""1800-01-01"", ""actual"": ""1800-01-01""})

scala> 
scala> val ds = seq.toDF(""value"").as[String]
ds: org.apache.spark.sql.Dataset[String] = [value: string]

scala> spark.read.schema(""expected STRING, actual DATE"").json(ds).show
+----------+----------+
|  expected|    actual|
+----------+----------+
|0002-01-01|0001-12-30|
|1000-01-01|1000-01-06|
|1500-01-01|1500-01-10|
|1800-01-01|1800-01-01|
+----------+----------+

scala> 
{noformat}",,apachespark,bersprockets,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 01 01:24:33 UTC 2022,,,,,,,,,,"0|z0e228:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/20 18:45;bersprockets;In case Jiras are no longer getting updated, here's a PR: https://github.com/apache/spark/pull/28345;;;","27/Apr/20 05:04;cloud_fan;Issue resolved by pull request 28345
[https://github.com/apache/spark/pull/28345];;;","30/Apr/20 04:23;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28408;;;","05/Jun/20 19:50;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28398;;;","01/May/22 01:24;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/28345;;;",,,,,,,,,,,,,,,,,,,
Wrong result of isInCollection for large collections,SPARK-31553,13300785,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,24/Apr/20 11:34,05/Jun/20 19:50,13/Jul/23 08:46,28/Apr/20 14:11,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,,,,0,correctness,,"If the size of a collection passed to isInCollection is bigger than spark.sql.optimizer.inSetConversionThreshold, the method can return wrong results for some inputs. For example:
{code:scala}
    val set = (0 to 20).map(_.toString).toSet
    val data = Seq(""1"").toDF(""x"")
    println(set.contains(""1""))
    data.select($""x"".isInCollection(set).as(""isInCollection"")).show()
{code}
{code}
true
+--------------+
|isInCollection|
+--------------+
|         false|
+--------------+
{code}",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29048,SPARK-12593,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 19:50:28 UTC 2020,,,,,,,,,,"0|z0e1ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/20 11:35;maxgekk;I am working on the issue;;;","28/Apr/20 14:11;cloud_fan;Issue resolved by pull request 28388
[https://github.com/apache/spark/pull/28388];;;","30/Apr/20 03:22;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28405;;;","30/Apr/20 03:23;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28405;;;","05/Jun/20 19:49;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28388;;;","05/Jun/20 19:50;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28328;;;",,,,,,,,,,,,,,,,,,
Fix potential ClassCastException in ScalaReflection arrayClassFor,SPARK-31552,13300745,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,24/Apr/20 08:51,26/Apr/20 06:14,13/Jul/23 08:46,25/Apr/20 01:14,2.0.2,2.1.3,2.2.3,2.3.4,2.4.5,3.0.0,3.1.0,,,,,,2.4.6,3.0.0,,SQL,,,,0,,,"arrayClassFor and dataTypeFor in ScalaReflection call each other circularly, the cases in dataTypeFor are not fully handled in arrayClassFor

For example:

{code:java}
scala> import scala.reflect.runtime.universe.TypeTag
scala> import org.apache.spark.sql._
scala> import org.apache.spark.sql.catalyst.encoders._
scala> import org.apache.spark.sql.types._
scala> implicit def newArrayEncoder[T <: Array[_] : TypeTag]: Encoder[T] = ExpressionEncoder()
newArrayEncoder: [T <: Array[_]](implicit evidence$1: reflect.runtime.universe.TypeTag[T])org.apache.spark.sql.Encoder[T]

scala> val decOne = Decimal(1, 38, 18)
decOne: org.apache.spark.sql.types.Decimal = 1E-18

scala>     val decTwo = Decimal(2, 38, 18)
decTwo: org.apache.spark.sql.types.Decimal = 2E-18

scala>     val decSpark = Array(decOne, decTwo)
decSpark: Array[org.apache.spark.sql.types.Decimal] = Array(1E-18, 2E-18)

scala> Seq(decSpark).toDF()
java.lang.ClassCastException: org.apache.spark.sql.types.DecimalType cannot be cast to org.apache.spark.sql.types.ObjectType
  at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$arrayClassFor$1(ScalaReflection.scala:131)
  at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69)
  at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:879)
  at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:878)
  at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:49)
  at org.apache.spark.sql.catalyst.ScalaReflection$.arrayClassFor(ScalaReflection.scala:120)
  at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$dataTypeFor$1(ScalaReflection.scala:105)
  at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69)
  at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:879)
  at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:878)
  at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:49)
  at org.apache.spark.sql.catalyst.ScalaReflection$.dataTypeFor(ScalaReflection.scala:88)
  at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerForType$1(ScalaReflection.scala:399)
  at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69)
  at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:879)
  at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:878)
  at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:49)
  at org.apache.spark.sql.catalyst.ScalaReflection$.serializerForType(ScalaReflection.scala:393)
  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:57)
  at newArrayEncoder(<console>:57)
  ... 53 elided

scala>
{code}
",,dongjoon,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 25 01:14:57 UTC 2020,,,,,,,,,,"0|z0e11s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/20 14:53;dongjoon;Hi, [~Qin Yao]. I updated the Affected Version by adding 2.0.2 ~ 2.4.5.;;;","25/Apr/20 01:14;dongjoon;This is resolved via https://github.com/apache/spark/pull/28324;;;",,,,,,,,,,,,,,,,,,,,,,
nondeterministic configurations with general meanings in sql configuration doc,SPARK-31550,13300712,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,24/Apr/20 05:28,12/Dec/22 18:10,13/Jul/23 08:46,27/Apr/20 08:09,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,Documentation,SQL,,,0,,,"spark.sql.session.timeZone

spark.sql.warehouse.dir

 

these 2 configs are nondeterministic and vary with environments",,JinxinTang,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 27 08:09:56 UTC 2020,,,,,,,,,,"0|z0e0ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/20 05:54;Qin Yao;Github webhook temporary downtime， PR in progress: [https://github.com/apache/spark/pull/28322];;;","27/Apr/20 08:09;gurwls223;Issue resolved by pull request 28322
[https://github.com/apache/spark/pull/28322];;;",,,,,,,,,,,,,,,,,,,,,,
Pyspark SparkContext.cancelJobGroup do not work correctly,SPARK-31549,13300700,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,weichenxu123,weichenxu123,weichenxu123,24/Apr/20 03:22,12/Dec/22 18:10,13/Jul/23 08:46,01/May/20 01:09,2.4.5,3.0.0,,,,,,,,,,,3.0.0,,,PySpark,,,,0,,,"Pyspark SparkContext.cancelJobGroup do not work correctly. This is an issue existing for a long time. This is because of pyspark thread didn't pinned to jvm thread when invoking java side methods, which leads to all pyspark API which related to java local thread variables do not work correctly. (Including `sc.setLocalProperty`, `sc.cancelJobGroup`, `sc.setJobDescription` and so on.)

This is serious issue. Now there's an experimental pyspark 'PIN_THREAD' mode added in spark-3.0 which address it, but the 'PIN_THREAD' mode exists two issue:
* It is disabled by default. We need to set additional environment variable to enable it.
* There's memory leak issue which haven't been addressed.

Now there's a series of project like hyperopt-spark, spark-joblib which rely on `sc.cancelJobGroup` API (use it to stop running jobs in their code). So it is critical to address this issue and we hope it work under default pyspark mode. An optional approach is implementing methods like `rdd.setGroupAndCollect`.

",,apachespark,mengxr,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32011,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 01 01:09:15 UTC 2020,,,,,,,,,,"0|z0e0rs:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"30/Apr/20 01:56;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/28395;;;","30/Apr/20 01:57;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/28395;;;","01/May/20 01:09;gurwls223;Issue resolved by pull request 28395
[https://github.com/apache/spark/pull/28395];;;",,,,,,,,,,,,,,,,,,,,,
"Backport SPARK-30199       Recover `spark.(ui|blockManager).port` from checkpoint",SPARK-31544,13300645,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,holden,holden,23/Apr/20 20:10,24/Apr/20 04:25,13/Jul/23 08:46,24/Apr/20 04:24,2.4.6,,,,,,,,,,,,2.4.6,,,DStreams,,,,0,,,"Backport SPARK-30199       Recover `spark.(ui|blockManager).port` from checkpoint

cc [~dongjoon] for if you think this is a good candidate",,dongjoon,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30199,,,,,SPARK-30199,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 24 04:25:00 UTC 2020,,,,,,,,,,"0|z0e0fk:",9223372036854775807,,,,,,,,,,,,,2.4.6,,,,,,,,,,"23/Apr/20 20:17;dongjoon;Got it. I'll make a PR.;;;","23/Apr/20 20:26;dongjoon;BTW, I kept the original authorship from the beginning. This will be the same for `branch-2.4`.;;;","23/Apr/20 20:38;dongjoon;I made a PR, [~holden].
- https://github.com/apache/spark/pull/28320;;;","23/Apr/20 20:41;holden;Thanks!;;;","24/Apr/20 04:25;dongjoon;This is resolved via https://github.com/apache/spark/pull/28320 . ;;;",,,,,,,,,,,,,,,,,,,
Backport SPARK-26095       Disable parallelization in make-distibution.sh. (Avoid build hanging),SPARK-31541,13300642,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,holden,holden,23/Apr/20 20:07,29/May/20 21:01,13/Jul/23 08:46,29/May/20 21:01,2.4.6,,,,,,,,,,,,2.4.6,,,Build,,,,0,,,Backport SPARK-26095       Disable parallelization in make-distibution.sh. (Avoid build hanging),,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26095,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 20:58:51 UTC 2020,,,,,,,,,,"0|z0e0ew:",9223372036854775807,,,,,,,,,,,,,2.4.6,,,,,,,,,,"23/Apr/20 20:18;holden;cc [~vanzin] for thoughts;;;","29/May/20 20:58;holden;Ran into issues with this during the RC7 build so I'm backporting this.;;;",,,,,,,,,,,,,,,,,,,,,,
Fix nested CTE substitution,SPARK-31535,13300619,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,petertoth,petertoth,petertoth,23/Apr/20 18:40,26/Apr/20 22:32,13/Jul/23 08:46,26/Apr/20 22:32,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,correctness,,"The following nested CTE should result empty result instead of {{1}}

{noformat}
WITH t(c) AS (SELECT 1)
SELECT * FROM t
WHERE c IN (
  WITH t(c) AS (SELECT 2)
  SELECT * FROM t
)
{noformat}",,dongjoon,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 26 22:32:12 UTC 2020,,,,,,,,,,"0|z0e09s:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"23/Apr/20 19:45;dongjoon;I added `correctness` label and raised the priority to `Blocker` with `Target Version: 3.0.0`. Thanks, [~petertoth] and [~cloud_fan]. ;;;","24/Apr/20 07:41;petertoth;Hmm, for some reason my PR ([https://github.com/apache/spark/pull/28318]) didn't get linked to this ticket automatically.;;;","26/Apr/20 22:32;dongjoon;Issue resolved by pull request 28318
[https://github.com/apache/spark/pull/28318];;;",,,,,,,,,,,,,,,,,,,,,
Text for tooltip should be escaped,SPARK-31534,13300604,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,23/Apr/20 16:53,04/Jun/21 20:46,13/Jul/23 08:46,26/Apr/20 20:21,3.0.0,3.1.0,,,,,,,,,,,3.0.0,3.1.0,,Web UI,,,,0,,,"Timeline View for application and job, and DAG Viz for job show tooltip but its text are not escaped for HTML so they cannot be shown properly.",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 04 20:46:07 UTC 2021,,,,,,,,,,"0|z0e06g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/20 20:21;dongjoon;This is resolved via https://github.com/apache/spark/pull/28317;;;","04/Jun/21 20:46;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/28359;;;",,,,,,,,,,,,,,,,,,,,,,
SparkSessionBuilder shoud not propagate static sql configurations to the existing active/default SparkSession,SPARK-31532,13300589,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,23/Apr/20 15:50,12/Dec/22 18:10,13/Jul/23 08:46,24/Apr/20 23:57,2.0.2,2.1.3,2.2.3,2.3.4,2.4.5,3.0.0,3.1.0,,,,,,2.4.6,,,SQL,,,,0,,,"Clearly, this is a bug.
{code:java}
scala> spark.sql(""set spark.sql.warehouse.dir"").show
+--------------------+--------------------+
|                 key|               value|
+--------------------+--------------------+
|spark.sql.warehou...|file:/Users/kenty...|
+--------------------+--------------------+


scala> spark.sql(""set spark.sql.warehouse.dir=2"");
org.apache.spark.sql.AnalysisException: Cannot modify the value of a static config: spark.sql.warehouse.dir;
  at org.apache.spark.sql.RuntimeConfig.requireNonStaticConf(RuntimeConfig.scala:154)
  at org.apache.spark.sql.RuntimeConfig.set(RuntimeConfig.scala:42)
  at org.apache.spark.sql.execution.command.SetCommand.$anonfun$x$7$6(SetCommand.scala:100)
  at org.apache.spark.sql.execution.command.SetCommand.run(SetCommand.scala:156)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
  at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3644)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3642)
  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:607)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:602)
  ... 47 elided

scala> import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SparkSession

scala> SparkSession.builder.config(""spark.sql.warehouse.dir"", ""xyz"").get
getClass   getOrCreate

scala> SparkSession.builder.config(""spark.sql.warehouse.dir"", ""xyz"").getOrCreate
20/04/23 23:49:13 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
res7: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6403d574

scala> spark.sql(""set spark.sql.warehouse.dir"").show
+--------------------+-----+
|                 key|value|
+--------------------+-----+
|spark.sql.warehou...|  xyz|
+--------------------+-----+


scala>
{code}
",,JinxinTang,maropu,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 24 23:57:08 UTC 2020,,,,,,,,,,"0|z0e034:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/20 04:14;gurwls223;The problem is that the static configuration was changed during runtime.;;;","24/Apr/20 04:35;gurwls223;PR is in progress at https://github.com/apache/spark/pull/28316;;;","24/Apr/20 23:57;maropu;Resolved by https://github.com/apache/spark/pull/28316;;;",,,,,,,,,,,,,,,,,,,,,
Hive metastore client initialization related configurations should be static ,SPARK-31522,13300312,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,22/Apr/20 15:53,23/Apr/20 15:08,13/Jul/23 08:46,23/Apr/20 15:08,2.0.2,2.1.3,2.2.3,2.3.4,2.4.5,3.0.0,3.1.0,,,,,,3.0.0,,,SQL,,,,0,,,"The following configurations defined in HiveUtils should be considered static:
 # spark.sql.hive.metastore.version - used to determine the hive version in Spark
 # spark.sql.hive.version - the fake of the above
 # spark.sql.hive.metastore.jars - hive metastore related jars location which is used by spark to create hive client
 # spark.sql.hive.metastore.sharedPrefixes and spark.sql.hive.metastore.barrierPrefixes -  packages of classes that are shared or separated between SparkContextLoader and hive client class loader

Those are used only once when creating the hive metastore client. They should be static in SQLConf for retrieving them correctly. We should avoid them being changed by users with SET/RESET command. They should ",,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 23 15:08:25 UTC 2020,,,,,,,,,,"0|z0dydk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/20 15:08;cloud_fan;Issue resolved by pull request 28302
[https://github.com/apache/spark/pull/28302];;;",,,,,,,,,,,,,,,,,,,,,,,
The fetch size is not correct when merging blocks into a merged block,SPARK-31521,13300274,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,22/Apr/20 13:51,12/Dec/22 18:10,13/Jul/23 08:46,25/Apr/20 05:12,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"When merging blocks into a merged block, we should count the size of that merged block as well.",,dongjoon,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 25 05:12:09 UTC 2020,,,,,,,,,,"0|z0dy54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/20 04:43;gurwls223;PR: https://github.com/apache/spark/pull/28301;;;","25/Apr/20 05:12;dongjoon;Issue resolved by pull request 28301
[https://github.com/apache/spark/pull/28301];;;",,,,,,,,,,,,,,,,,,,,,,
Cast in having aggregate expressions returns the wrong result,SPARK-31519,13300231,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,XuanYuan,XuanYuan,XuanYuan,22/Apr/20 09:56,12/Dec/22 18:10,13/Jul/23 08:46,28/Apr/20 08:12,2.2.3,2.3.4,2.4.5,3.0.0,,,,,,,,,2.4.6,3.0.0,,SQL,,,,0,correctness,,"Cast in having aggregate expressions returns the wrong result.

See the below tests: 
{code:java}

scala> spark.sql(""create temp view t(a, b) as values (1,10), (2, 20)"")
res0: org.apache.spark.sql.DataFrame = []

scala> val query = """"""
     | select sum(a) as b, '2020-01-01' as fake
     | from t
     | group by b
     | having b > 10;""""""

scala> spark.sql(query).show()
+---+----------+
|  b|      fake|
+---+----------+
|  2|2020-01-01|
+---+----------+

scala> val query = """"""
     | select sum(a) as b, cast('2020-01-01' as date) as fake
     | from t
     | group by b
     | having b > 10;""""""

scala> spark.sql(query).show()
+---+----+
|  b|fake|
+---+----+
+---+----+
{code}
The SQL parser in Spark creates Filter(..., Aggregate(...)) for the HAVING query, and Spark has a special analyzer rule ResolveAggregateFunctions to resolve the aggregate functions and grouping columns in the Filter operator.
 
It works for simple cases in a very tricky way as it relies on rule execution order:
1. Rule ResolveReferences hits the Aggregate operator and resolves attributes inside aggregate functions, but the function itself is still unresolved as it's an UnresolvedFunction. This stops resolving the Filter operator as the child Aggrege operator is still unresolved.
2. Rule ResolveFunctions resolves UnresolvedFunction. This makes the Aggrege operator resolved.
3. Rule ResolveAggregateFunctions resolves the Filter operator if its child is a resolved Aggregate. This rule can correctly resolve the grouping columns.
 
In the example query, I put a CAST, which needs to be resolved by rule ResolveTimeZone, which runs after ResolveAggregateFunctions. This breaks step 3 as the Aggregate operator is unresolved at that time. Then the analyzer starts next round and the Filter operator is resolved by ResolveReferences, which wrongly resolves the grouping columns.",,726575153@qq.com,apachespark,cloud_fan,dongjoon,JinxinTang,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31334,,,,,,,,SPARK-34012,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 30 02:33:57 UTC 2020,,,,,,,,,,"0|z0dxvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/20 04:43;gurwls223;PR: https://github.com/apache/spark/pull/28294;;;","28/Apr/20 08:12;cloud_fan;Issue resolved by pull request 28294
[https://github.com/apache/spark/pull/28294];;;","29/Apr/20 16:19;dongjoon;This is backported to branch-2.4 via https://github.com/apache/spark/pull/28397;;;","29/Apr/20 16:26;dongjoon;For the record, from 2.2.3 ~ 2.4.5, the following queries are used to see the wrong result. (2.0.2 and 2.1.3 has no problem with the following queries)
{code}
spark-sql> SELECT SUM(a) AS b, hour('2020-01-01 12:12:12') AS fake FROM VALUES (1, 10), (2, 20) AS T(a, b) GROUP BY b HAVING b > 10;
Time taken: 3.249 seconds
spark-sql> SELECT SUM(a) AS b, '2020-01-01 12:12:12' AS fake FROM VALUES (1, 10), (2, 20) AS T(a, b) GROUP BY b HAVING b > 10;
2	2020-01-01 12:12:12
Time taken: 0.505 seconds, Fetched 1 row(s)
{code};;;","30/Apr/20 02:33;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/28397;;;",,,,,,,,,,,,,,,,,,,
SparkR::orderBy with multiple columns descending produces error,SPARK-31517,13300202,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,michaelchirico,rossbowen,rossbowen,22/Apr/20 08:33,12/Dec/22 18:11,13/Jul/23 08:46,17/Dec/20 08:22,2.4.5,,,,,,,,,,,,3.1.0,3.2.0,,SparkR,,,,0,,,"When specifying two columns within an `orderBy()` function, to attempt to get an ordering by two columns in descending order, an error is returned.
{code:java}
library(magrittr) 
library(SparkR) 
cars <- cbind(model = rownames(mtcars), mtcars) 
carsDF <- createDataFrame(cars) 

carsDF %>% 
  mutate(rank = over(rank(), orderBy(windowPartitionBy(column(""cyl"")), desc(column(""mpg"")), desc(column(""disp""))))) %>% 
  head() {code}
This returns an error:
{code:java}
 Error in ns[[i]] : subscript out of bounds{code}
This seems to be related to the more general issue that the following code, excluding the use of the `desc()` function also fails:
{code:java}
carsDF %>% 
  mutate(rank = over(rank(), orderBy(windowPartitionBy(column(""cyl"")), column(""mpg""), column(""disp"")))) %>% 
  head(){code}
 ",Databricks Runtime 6.5,apachespark,michaelchirico,rossbowen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26199,,SPARK-26199,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 17 08:22:21 UTC 2020,,,,,,,,,,"0|z0dxp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/20 05:42;michaelchirico;> Error in ns[[i]] : subscript out of bounds

This error is coming from mutate

https://github.com/apache/spark/blob/2d3e9601b58fbe33aeedb106be7e2a1fafa2e1fd/R/pkg/R/DataFrame.R#L2294;;;","28/Apr/20 06:21;michaelchirico;Separating the window to its own step works:

window = over(rank(), orderBy(windowPartitionBy(column(""cyl"")), desc(column(""mpg"")), desc(column(""disp""))))
carsDF %>% 
  mutate(rank = window) %>% 
  head() 

So there's something in the logic of mutate that doesn't handle the nested call.;;;","28/Apr/20 06:30;michaelchirico;The issue is the use of deparse() in mutate; over(....) is longer than the default width.cutoff, so sapply() returns > 1 element.;;;","30/Apr/20 16:54;apachespark;User 'MichaelChirico' has created a pull request for this issue:
https://github.com/apache/spark/pull/28386;;;","17/Dec/20 08:22;gurwls223;Fixed in https://github.com/apache/spark/pull/28386;;;",,,,,,,,,,,,,,,,,,,
Canonicalize Cast should consider the value of needTimeZone,SPARK-31515,13300145,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,22/Apr/20 03:52,23/Apr/20 05:33,13/Jul/23 08:46,23/Apr/20 05:33,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,,"If we don't need to use `timeZone` information casting `from` type to `to` type, then the timeZoneId should not influence the canonicalize result. ",JinxinTang,maropu,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 23 05:33:46 UTC 2020,,,,,,,,,,"0|z0dxcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/20 07:20;JinxinTang;Could you please provide code piece or picture to depict this.;;;","23/Apr/20 05:33;maropu;Resolved by https://github.com/apache/spark/pull/28288#;;;",,,,,,,,,,,,,,,,,,,,,,
Make BytesToBytesMap iterator() thread-safe,SPARK-31511,13300069,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,21/Apr/20 19:28,08/Sep/20 05:02,13/Jul/23 08:46,22/Apr/20 01:17,1.4.1,1.5.2,1.6.3,2.0.2,2.1.3,2.2.3,2.3.4,2.4.5,3.0.0,,,,2.4.7,3.0.2,3.1.0,SQL,,,,0,correctness,,BytesToBytesMap currently has a thread safe and unsafe iterator method. This is somewhat confusing. We should make iterator() thread safe and remove the safeIterator() function.,,apachespark,dongjoon,hvanhovell,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 08 05:02:31 UTC 2020,,,,,,,,,,"0|z0dwvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/20 01:17;dongjoon;Issue resolved by pull request 28286
[https://github.com/apache/spark/pull/28286];;;","27/Aug/20 15:26;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/28286;;;","30/Aug/20 21:40;dongjoon;This is backported to branch-3.0 via https://github.com/apache/spark/commit/70c477e3fa6427c6ec9314b10ce4b0e7b9eed3ce ;;;","30/Aug/20 21:49;dongjoon;Thank you for updating issue type and labeling correctness. According to the commit log, this code came from Apache Spark 1.4.0, I updated `Affected Version` accordingly. Please let me know if that's not true.;;;","01/Sep/20 04:20;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/29605;;;","01/Sep/20 04:20;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/29605;;;","08/Sep/20 05:01;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/29669;;;","08/Sep/20 05:02;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/29669;;;",,,,,,,,,,,,,,,,
Output fields in formatted Explain should have determined order.,SPARK-31504,13299885,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,21/Apr/20 06:47,21/Apr/20 12:34,13/Jul/23 08:46,21/Apr/20 12:34,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Currently, formatted explain use `producedAttributes`, which is AttributeSet, to generate the ""Output"" of a leaf node. As a result, it will lead to the different output of formatted explain for the same plan.",,cloud_fan,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 21 12:34:27 UTC 2020,,,,,,,,,,"0|z0dvqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/20 12:34;cloud_fan;Issue resolved by pull request 28282
[https://github.com/apache/spark/pull/28282];;;",,,,,,,,,,,,,,,,,,,,,,,
fix the SQL string of the TRIM functions,SPARK-31503,13299883,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,21/Apr/20 06:43,22/Apr/20 16:54,13/Jul/23 08:46,21/Apr/20 18:23,2.3.4,2.4.5,3.0.0,,,,,,,,,,2.4.6,3.0.0,,SQL,,,,0,,,,,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 22 16:46:43 UTC 2020,,,,,,,,,,"0|z0dvq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/20 18:23;dongjoon;Issue resolved by pull request 28281
[https://github.com/apache/spark/pull/28281];;;","22/Apr/20 16:46;dongjoon;This is backported to branch-2.4 via https://github.com/apache/spark/pull/28299 .;;;",,,,,,,,,,,,,,,,,,,,,,
AQE update UI should not cause deadlock,SPARK-31501,13299813,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maryannxue,maryannxue,maryannxue,20/Apr/20 22:01,21/Apr/20 03:57,13/Jul/23 08:46,21/Apr/20 03:57,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,,,cloud_fan,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 21 03:57:23 UTC 2020,,,,,,,,,,"0|z0dvao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/20 03:57;cloud_fan;Issue resolved by pull request 28275
[https://github.com/apache/spark/pull/28275];;;",,,,,,,,,,,,,,,,,,,,,,,
collect_set() of BinaryType returns duplicate elements,SPARK-31500,13299785,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,planga82,ewasserman,ewasserman,20/Apr/20 19:16,01/May/20 13:17,13/Jul/23 08:46,01/May/20 13:17,2.0.2,2.1.3,2.2.3,2.3.4,2.4.5,,,,,,,,2.4.6,,,SQL,,,,0,correctness,,"The collect_set() aggregate function should produce a set of distinct elements. When the column argument's type is BinayType this is not the case.

 

Example:

{{import org.apache.spark.sql.functions._}}
 {{import org.apache.spark.sql.expressions.Window}}

{{case class R(id: String, value: String, bytes: Array[Byte])}}
 {{def makeR(id: String, value: String) = R(id, value, value.getBytes)}}
 {{val df = Seq(makeR(""a"", ""dog""), makeR(""a"", ""cat""), makeR(""a"", ""cat""), makeR(""b"", ""fish"")).toDF()}}

 

{{// In the example below ""bytesSet"" erroneously has duplicates but ""stringSet"" does not (as expected).}}

{{df.agg(collect_set('value) as ""stringSet"", collect_set('bytes) as ""byteSet"").show(truncate=false)}}

 

{{// The same problem is displayed when using window functions.}}
 {{val win = Window.partitionBy('id).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)}}
 {{val result = df.select(}}
  collect_set('value).over(win) as ""stringSet"",
  collect_set('bytes).over(win) as ""bytesSet""
 {{)}}
 {{.select('stringSet, 'bytesSet, size('stringSet) as ""stringSetSize"", size('bytesSet) as ""bytesSetSize"")}}
 {{.show()}}",,apachespark,dongjoon,ewasserman,maropu,planga82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 01 13:17:34 UTC 2020,,,,,,,,,,"0|z0dv4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/20 19:06;planga82;Hi [~ewasserman],

This is a scala base problem, equality between arrays is not behaving as expected.

[https://blog.bruchez.name/2013/05/scala-array-comparison-without-phd.html]

I'm going to work to find a solution, but here is a workaround, change the definition of the case class and put Seq instead of Array and it will work as expected.
{code:java}
case class R(id: String, value: String, bytes: Seq[Byte]){code}
 ;;;","26/Apr/20 18:29;planga82;[https://github.com/apache/spark/pull/28351];;;","30/Apr/20 10:54;apachespark;User 'planga82' has created a pull request for this issue:
https://github.com/apache/spark/pull/28351;;;","30/Apr/20 10:55;apachespark;User 'planga82' has created a pull request for this issue:
https://github.com/apache/spark/pull/28351;;;","01/May/20 00:36;dongjoon;I checked the following code snippet from the PR unit test with Apache Spark 2.0.2 ~ 2.4.5 and update the Affected Version.
{code}
val bytesTest1 = ""test1"".getBytes
val bytesTest2 = ""test2"".getBytes
val df = Seq(bytesTest1, bytesTest1, bytesTest2).toDF(""a"")
val ret = df.select(collect_set($""a"")).collect().map(r => r.getAs[Seq[_]](0)).head
{code};;;","01/May/20 13:17;maropu;Resolved by https://github.com/apache/spark/pull/28351;;;",,,,,,,,,,,,,,,,,,
Pyspark CrossValidator/TrainValidationSplit with pipeline estimator cannot save and load model,SPARK-31497,13299626,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,20/Apr/20 09:50,27/Apr/20 04:06,13/Jul/23 08:46,27/Apr/20 04:06,2.4.5,,,,,,,,,,,,3.0.0,,,ML,PySpark,,,0,,,"Pyspark CrossValidator/TrainValidationSplit with pipeline estimator cannot save and load model.

Reproduce code run in pyspark shell:

1) Train model and save model in pyspark:
{code:python}

from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.feature import HashingTF, Tokenizer
from pyspark.ml.tuning import CrossValidator, CrossValidatorModel, ParamGridBuilder

training = spark.createDataFrame([
    (0, ""a b c d e spark"", 1.0),
    (1, ""b d"", 0.0),
    (2, ""spark f g h"", 1.0),
    (3, ""hadoop mapreduce"", 0.0),
    (4, ""b spark who"", 1.0),
    (5, ""g d a y"", 0.0),
    (6, ""spark fly"", 1.0),
    (7, ""was mapreduce"", 0.0),
    (8, ""e spark program"", 1.0),
    (9, ""a e c l"", 0.0),
    (10, ""spark compile"", 1.0),
    (11, ""hadoop software"", 0.0)
], [""id"", ""text"", ""label""])

# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.
tokenizer = Tokenizer(inputCol=""text"", outputCol=""words"")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=""features"")
lr = LogisticRegression(maxIter=10)
pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])

paramGrid = ParamGridBuilder() \
    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \
    .addGrid(lr.regParam, [0.1, 0.01]) \
    .build()
crossval = CrossValidator(estimator=pipeline,
                          estimatorParamMaps=paramGrid,
                          evaluator=BinaryClassificationEvaluator(),
                          numFolds=2)  # use 3+ folds in practice

# Run cross-validation, and choose the best set of parameters.
cvModel = crossval.fit(training)

cvModel.save('/tmp/cv_model001') # save model failed. Rase error.
{code}

2): Train crossvalidation model in scala with similar code above, and save to '/tmp/model_cv_scala001', run following code in pyspark:
{code:python}
from pyspark.ml.tuning import CrossValidatorModel
CrossValidatorModel.load('/tmp/model_cv_scala001') # raise error
{code}
",,mengxr,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31548,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 27 04:06:09 UTC 2020,,,,,,,,,,"0|z0du54:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"27/Apr/20 04:06;mengxr;Issue resolved by pull request 28279
[https://github.com/apache/spark/pull/28279];;;",,,,,,,,,,,,,,,,,,,,,,,
Failure on pushing down filters with java.time.LocalDate values in ORC,SPARK-31489,13299511,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,19/Apr/20 17:46,05/Jun/20 19:49,13/Jul/23 08:46,26/Apr/20 22:49,3.0.0,3.0.1,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"When spark.sql.datetime.java8API.enabled is set to true, filters pushed down with java.time.LocalDate values to ORC datasource fails with the exception:
{code}
Wrong value class java.time.LocalDate for DATE.EQUALS leaf
java.lang.IllegalArgumentException: Wrong value class java.time.LocalDate for DATE.EQUALS leaf
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$PredicateLeafImpl.checkLiteralType(SearchArgumentImpl.java:192)
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$PredicateLeafImpl.<init>(SearchArgumentImpl.java:75)
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$BuilderImpl.equals(SearchArgumentImpl.java:352)
	at org.apache.spark.sql.execution.datasources.orc.OrcFilters$.buildLeafSearchArgument(OrcFilters.scala:229)
{code}",,apachespark,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31818,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 19:49:38 UTC 2020,,,,,,,,,,"0|z0dtg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/20 22:49;dongjoon;Issue resolved by pull request 28261
[https://github.com/apache/spark/pull/28261];;;","05/Jun/20 19:49;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28272;;;",,,,,,,,,,,,,,,,,,,,,,
Support `java.time.LocalDate` in Parquet filter pushdown,SPARK-31488,13299491,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,19/Apr/20 15:56,26/Jul/20 05:52,13/Jul/23 08:46,24/Apr/20 02:22,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Currently, ParquetFilters supports only java.sql.Date values of DateType, and explicitly casts Any to java.sql.Date, see
https://github.com/apache/spark/blob/cb0db213736de5c5c02b09a2d5c3e17254708ce1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala#L176

So, any filters refer to date values are not pushed down to Parquet when spark.sql.datetime.java8API.enabled is true.",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31888,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 26 05:52:00 UTC 2020,,,,,,,,,,"0|z0dtbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/20 02:22;cloud_fan;Issue resolved by pull request 28259
[https://github.com/apache/spark/pull/28259];;;","05/Jun/20 19:50;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/28272;;;","26/Jul/20 05:52;apachespark;User 'abhishekd0907' has created a pull request for this issue:
https://github.com/apache/spark/pull/29242;;;",,,,,,,,,,,,,,,,,,,,,
Barrier stage can hang if only partial tasks launched,SPARK-31485,13299471,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,19/Apr/20 13:40,07/May/20 13:39,13/Jul/23 08:46,27/Apr/20 06:10,2.4.0,,,,,,,,,,,,2.4.6,,,Spark Core,,,,0,,,"The issue can be reproduced by following test:

 
{code:java}
initLocalClusterSparkContext(2)
val rdd0 = sc.parallelize(Seq(0, 1, 2, 3), 2)
val dep = new OneToOneDependency[Int](rdd0)
val rdd = new MyRDD(sc, 2, List(dep), Seq(Seq(""executor_h_0""),Seq(""executor_h_0"")))
rdd.barrier().mapPartitions { iter =>
  BarrierTaskContext.get().barrier()
  iter
}.collect()
{code}
 ",,apachespark,cloud_fan,Ngone51,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 07 13:39:53 UTC 2020,,,,,,,,,,"0|z0dt7c:",9223372036854775807,,,,,holden,,,,,,,,2.4.6,3.0.0,,,,,,,,,"27/Apr/20 06:10;cloud_fan;Issue resolved by pull request 28357
[https://github.com/apache/spark/pull/28357];;;","07/May/20 13:39;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/28476;;;",,,,,,,,,,,,,,,,,,,,,,
Broadcast stage in AQE did not timeout,SPARK-31475,13299196,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maryannxue,maryannxue,maryannxue,17/Apr/20 18:28,20/Apr/20 18:56,13/Jul/23 08:46,20/Apr/20 18:56,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,,,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-04-17 18:28:27.0,,,,,,,,,,"0|z0druo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AQE should set active session during execution,SPARK-31473,13299180,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maryannxue,maryannxue,maryannxue,17/Apr/20 17:25,18/Apr/20 07:09,13/Jul/23 08:46,18/Apr/20 07:09,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,,,dongjoon,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 07:09:15 UTC 2020,,,,,,,,,,"0|z0drr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/20 07:09;dongjoon;Issue resolved by pull request 28247
[https://github.com/apache/spark/pull/28247];;;",,,,,,,,,,,,,,,,,,,,,,,
allGather() may return null messages ,SPARK-31472,13299147,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,17/Apr/20 14:09,23/Apr/20 14:44,13/Jul/23 08:46,23/Apr/20 14:44,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"{code:java}
[info] BarrierTaskContextSuite:
[info] - share messages with allGather() call *** FAILED *** (18 seconds, 705 milliseconds)
[info]   org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(0, 2) finished unsuccessfully.
[info] java.lang.NullPointerException
[info] 	at scala.collection.mutable.ArrayOps$ofRef$.length$extension(ArrayOps.scala:204)
[info] 	at scala.collection.mutable.ArrayOps$ofRef.length(ArrayOps.scala:204)
[info] 	at scala.collection.IndexedSeqOptimized.toList(IndexedSeqOptimized.scala:285)
[info] 	at scala.collection.IndexedSeqOptimized.toList$(IndexedSeqOptimized.scala:284)
[info] 	at scala.collection.mutable.ArrayOps$ofRef.toList(ArrayOps.scala:198)
[info] 	at org.apache.spark.scheduler.BarrierTaskContextSuite.$anonfun$new$4(BarrierTaskContextSuite.scala:68)
[info] 	at org.apache.spark.rdd.RDDBarrier.$anonfun$mapPartitions$2(RDDBarrier.scala:51)
[info] 	at org.apache.spark.rdd.RDDBarrier.$anonfun$mapPartitions$2$adapted(RDDBarrier.scala:51)
[info] 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[info] 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
[info] 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
[info] 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[info] 	at org.apache.spark.scheduler.Task.run(Task.scala:127)
[info] 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:460)
[info] 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
[info] 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:463)
[info] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info] 	at java.lang.Thread.run(Thread.java:748)
[info]   at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2094)
[info]   at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2043)
[info]   at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2042)
[info]   at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[info]   at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[info]   at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[info]   at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2042)
[info]   at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1831)
[info]   at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2271)
[info]   at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2223)
[info]   at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2212)
[info]   at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[info]   at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:822)
[info]   at org.apache.spark.SparkContext.runJob(SparkContext.scala:2108)
[info]   at org.apache.spark.SparkContext.runJob(SparkContext.scala:2129)
[info]   at org.apache.spark.SparkContext.runJob(SparkContext.scala:2148)
[info]   at org.apache.spark.SparkContext.runJob(SparkContext.scala:2173)
[info]   at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
[info]   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[info]   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[info]   at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
[info]   at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
[info]   at org.apache.spark.scheduler.BarrierTaskContextSuite.$anonfun$new$3(BarrierTaskContextSuite.scala:71)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:151)
[info]   at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
[info]   at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:286)
[info]   at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
[info]   at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:58)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:58)
[info]   at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:393)
[info]   at scala.collection.immutable.List.foreach(List.scala:392)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:381)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:376)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:458)
[info]   at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
[info]   at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
[info]   at org.scalatest.Suite.run(Suite.scala:1124)
[info]   at org.scalatest.Suite.run$(Suite.scala:1106)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
[info]   at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:518)
[info]   at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
[info]   at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:58)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:58)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:317)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:510)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:296)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:286)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:748)
{code}
 
{code:java}
 {code}
 ",,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-04-17 14:09:26.0,,,,,,,,,,"0|z0drjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null types should be implicitly casted to Decimal types,SPARK-31468,13299007,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,17/Apr/20 08:21,17/Apr/20 23:20,13/Jul/23 08:46,17/Apr/20 14:11,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"A query below fails in master/branch-3.0 and passed in v2.4.5;
{code}
scala> Seq(BigDecimal(10)).toDF(""v1"").selectExpr(""v1 = NULL"").explain(true)
org.apache.spark.sql.AnalysisException: cannot resolve '(`v1` = NULL)' due to data type mismatch: differing types in '(`v1` = NULL)' (decimal(38,18) and null).; line 1 pos 0;
'Project [(v1#5 = null) AS (v1 = NULL)#7]
+- Project [value#2 AS v1#5]
   +- LocalRelation [value#2]
...
{code}",,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 14:11:55 UTC 2020,,,,,,,,,,"0|z0dqoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/20 14:11;cloud_fan;Issue resolved by pull request 28241
[https://github.com/apache/spark/pull/28241];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix test issue with table named `test` in hive/SQLQuerySuite,SPARK-31467,13299006,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hzfeiwang,hzfeiwang,hzfeiwang,17/Apr/20 08:18,05/May/20 06:16,13/Jul/23 08:46,05/May/20 06:16,3.1.0,,,,,,,,,,,,3.0.0,,,Tests,,,,0,,,"If we add ut in hive/SQLQuerySuite and use table named `test`. We may meet these exceptions.
{code:java}
 org.apache.spark.sql.AnalysisException: Inserting into an RDD-based table is not allowed.;;
[info] 'InsertIntoTable Project [_1#1403 AS key#1406, _2#1404 AS value#1407], Map(name -> Some(n1)), true, false
[info] +- Project [col1#3850]
[info]    +- LocalRelation [col1#3850]
{code}


{code:java}
org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException: Table or view 'test' already exists in database 'default';
[info]   at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$doCreateTable$1.apply$mcV$sp(HiveExternalCatalog.scala:226)
[info]   at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$doCreateTable$1.apply(HiveExternalCatalog.scala:216)
[info]   at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$doCreateTable$1.apply(HiveExternalCatalog.scala:216)
[info]   at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
[info]   at org.apache.spark.sql.hive.HiveExternalCatalog.doCreateTable(HiveExternalCatalog.scala:216)
{code}

",,hzfeiwang,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 05 06:16:53 UTC 2020,,,,,,,,,,"0|z0dqog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/May/20 06:16;maropu;Resolved b https://github.com/apache/spark/pull/28239;;;",,,,,,,,,,,,,,,,,,,,,,,
The usage of getopts and case statement is wrong in do-release.sh,SPARK-31462,13298877,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,16/Apr/20 17:59,16/Apr/20 23:25,13/Jul/23 08:46,16/Apr/20 19:56,3.1.0,,,,,,,,,,,,3.0.0,,,Project Infra,,,,0,,,"In the current master, do-release.sh contains the following code.

{code}
while getopts ""bn"" opt; do
  case $opt in
    b) GIT_BRANCH=$OPTARG ;;
    n) DRY_RUN=1 ;;
    ?) error ""Invalid option: $OPTARG"" ;;
  esac
done
{code}

There are 3 wrong usage in getopts and case statement.
1. To set  $OPTARG to an argument passed for the option ""b"", the parameter for getopts should be ""b:"".
2. To set $OPTARG to the invalid option name passed, the parameter for getopts starts with "":"".
3. It's minor but to match the character ""?"", it's better to escape like ""\?"".",,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 16 19:56:16 UTC 2020,,,,,,,,,,"0|z0dpvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/20 19:56;dongjoon;This is resolved via https://github.com/apache/spark/pull/28234;;;",,,,,,,,,,,,,,,,,,,,,,,
"If shutdownhook is added with priority Integer.MIN_VALUE, it's supposed to be called the last, but it gets called before other positive priority shutdownhook",SPARK-31456,13298688,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,olkuznsmith,nancysilence,nancysilence,16/Apr/20 03:27,16/May/20 02:02,13/Jul/23 08:46,11/May/20 20:12,1.6.3,2.0.2,2.1.3,2.2.3,2.3.4,2.4.5,,,,,,,2.4.6,3.0.0,,Spark Core,,,,0,,,"https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala

Since shutdownHookManager use below method to do the comparison. 
override def compareTo(other: SparkShutdownHook): Int = {
    other.priority - priority
  }

Which will cause :
(Int)(25 - Integer.MIN_VALUE) < 0
Then the shutdownhook with Integer.Min_VALUE would not be called the last. ",macOS Mojave 10.14.6,apachespark,dongjoon,nancysilence,olkuznsmith,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 11 20:12:08 UTC 2020,,,,,,,,,,"0|z0dops:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/May/20 22:51;olkuznsmith;I will work on that.;;;","10/May/20 23:08;apachespark;User 'oleg-smith' has created a pull request for this issue:
https://github.com/apache/spark/pull/28494;;;","11/May/20 20:12;dongjoon;Issue resolved by pull request 28494
[https://github.com/apache/spark/pull/28494];;;",,,,,,,,,,,,,,,,,,,,,
Make ExpressionEncoder thread safe,SPARK-31450,13298503,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,15/Apr/20 08:36,16/Nov/20 20:16,13/Jul/23 08:46,17/Apr/20 01:48,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,ExpressionEncoder is currently not thread-safe because it contains stateful objects that are required for converting objects to internal rows and vise versa. We have been working around this by (excessively) cloning ExpressionEncoders which is not free. I propose that we move the stateful bits of the expression encoder into two helper classes that will take care of the conversions.,,dongjoon,hvanhovell,navinvishy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 16 20:16:53 UTC 2020,,,,,,,,,,"0|z0dnko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/20 01:48;dongjoon;Issue resolved by pull request 28223
[https://github.com/apache/spark/pull/28223];;;","16/Nov/20 20:16;navinvishy;[~hvanhovell]  [~dongjoon] I was in the process of migrating some code from Spark 2.4 to Spark 3 and noticed that this required a change in our code. We use the following process to go from a Thrift type T to InternalRow(reading thrift files on HDFS into a Dataframe):
 # We construct a Spark schema by inspecting the thrift metadata.
 # We convert a thrift object to a GenericRow using the thrift metadata to read columns.
 # We then construct an ExpressionEncoder[Row] and use it to create an InternalRow as follows:
{code:java}
val schema: StructType = ... // infer thrift schema
val encoder: ExpressionEncoder[Row] = RowEncoder(schema)
val genericRow: GenericRow = toGenericRow(thriftObject, schema)
val internalRow: InternalRow = encoder.toRow(genericRow)
{code}

The above steps are used to implement
{code:java}
protected def buildReader(
      sparkSession: SparkSession,
      dataSchema: StructType,
      partitionSchema: StructType,
      requiredSchema: StructType,
      filters: Seq[Filter],
      options: Map[String, String],
      hadoopConf: Configuration): PartitionedFile => Iterator[InternalRow]
{code}
in trait org.apache.spark.sql.execution.datasources.FileFormat where we need an Iterator[InternalRow].

With the change in this ticket, I would have to replace 
{code:java}
val internalRow: InternalRow = encoder.toRow(genericRow)  
{code}
with
{code:java}
val serializer = encoder.createSerializer()
val internalRow: InternalRow = serializer(genericRow){code}
Since this is marked as an internal API in the PR, I was wondering if there is a way to implement this so that it is compatible with both Spark 2.4 and Spark 3. 

My goal is to not require a code change if possible. It seems to me that since I know the schema of the thrift type it should be possible to construct an InternalRow, but I don't see a way to do this in the code base.;;;",,,,,,,,,,,,,,,,,,,,,,
Difference in Storage Levels used in cache() and persist() for pyspark dataframes,SPARK-31448,13298468,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,abhishekd0907,abhishekd0907,abhishekd0907,15/Apr/20 06:25,12/Dec/22 18:11,13/Jul/23 08:46,15/Sep/20 13:43,2.4.3,,,,,,,,,,,,3.1.0,,,PySpark,,,,0,,,"There is a difference in default storage level *MEMORY_AND_DISK* in pyspark and scala.

*Scala*: StorageLevel(true, true, false, true)

*Pyspark:* StorageLevel(True, True, False, False)

 

*Problem Description:* 

Calling *df.cache()*  for pyspark dataframe directly invokes Scala method cache() and Storage Level used is StorageLevel(true, true, false, true).

But calling *df.persist()* for pyspark dataframe sets the newStorageLevel=StorageLevel(true, true, false, false) inside pyspark and then invokes Scala function persist(newStorageLevel).

*Possible Fix:*
Invoke pyspark function persist inside pyspark function cache instead of calling the scala function directly.

I can raise a PR for this fix if someone can confirm that this is a bug and the possible fix is the correct approach.",,abhishekd0907,apachespark,tianshi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 13:43:59 UTC 2020,,,,,,,,,,"0|z0dncw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/20 07:10;abhishekd0907;Any update on this?;;;","06/May/20 01:35;tianshi;I found the following comment in StorageLevel.py in Spark 2.4.3: 

_"".. note:: The following four storage level constants are deprecated in 2.0, since the records_
 _will always be serialized in Python.""_

[https://github.com/apache/spark/blob/v2.4.3/python/pyspark/storagelevel.py#L61]

So I would assume the counterpart in Scala is 

val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)

[https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/StorageLevel.scala#L162]

 

val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) means the data is deserialized. Does that help?;;;","06/May/20 04:12;gurwls223;The diff is intentional. See also SPARK-2014;;;","07/May/20 12:31;abhishekd0907;Let me try to explain the problem more. 

Please look at this code in pyspark/dataframe.py: 
{code:java}
    @since(1.3)
    def cache(self):
        """"""Persists the :class:`DataFrame` with the default storage level (C{MEMORY_AND_DISK}).
        .. note:: The default storage level has changed to C{MEMORY_AND_DISK} to match Scala in 2.0.
        """"""
        self.is_cached = True
        self._jdf.cache()
        return self
{code}
Cache method in pyspark data frame directly calls scala's cache method. Hence Storage level used is based on Scala defaults i.e. StorageLevel(true, true, false, true)  with deserialized equal to true. But since, data from python is already serialized by the Pickle library, we should be using storage level with deserialized = false for pyspark dataframes.

But if you look at cache method in pyspark/rdd.py, it sets the storage level in pyspark only and then calls the scala method with parameter. Hence correct storage level is used in this case with deserialzied = false.
{code:java}
def cache(self):
        """"""
        Persist this RDD with the default storage level (C{MEMORY_ONLY}).
        """"""
        self.is_cached = True
        self.persist(StorageLevel.MEMORY_ONLY)
        return self{code}
 We need to implement a similar way in cache method in dataframe.py to avoid using the scala defaults of deserialized = true

 

 ;;;","21/May/20 10:09;abhishekd0907;[~tianshi] [~hyukjin.kwon]

Any update on this?;;;","26/Jul/20 05:55;apachespark;User 'abhishekd0907' has created a pull request for this issue:
https://github.com/apache/spark/pull/29242;;;","15/Sep/20 13:43;srowen;Issue resolved by pull request 29242
[https://github.com/apache/spark/pull/29242];;;",,,,,,,,,,,,,,,,,
Make html elements for a paged table possible to have different id attribute.,SPARK-31446,13298363,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,14/Apr/20 18:17,17/Apr/20 20:45,13/Jul/23 08:46,16/Apr/20 23:26,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,Web UI,,,,0,,,"Some pages have a paged table and  page navigations above / below the table.
But corresponding HTML elements between the two page navigations for a table have the same id attribute. Every id element should be unique.
For example, there are two `form-completedJob-table-page` id in JobsPage.",,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 16 23:26:07 UTC 2020,,,,,,,,,,"0|z0dmpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/20 23:26;dongjoon;This is resolved via https://github.com/apache/spark/pull/28217;;;",,,,,,,,,,,,,,,,,,,,,,,
Support duplicated column names for toPandas with Arrow execution.,SPARK-31441,13298166,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,14/Apr/20 01:48,12/Dec/22 18:10,13/Jul/23 08:46,14/Apr/20 05:10,2.4.5,3.0.0,,,,,,,,,,,3.0.0,,,PySpark,,,,0,,,"When we execute {{toPandas()}} with Arrow execution, it fails if the column names have duplicates.

{code:python}
>>> spark.sql(""select 1 v, 1 v"").toPandas()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/path/to/lib/python3.7/site-packages/pyspark/sql/dataframe.py"", line 2132, in toPandas
    pdf = table.to_pandas()
  File ""pyarrow/array.pxi"", line 441, in pyarrow.lib._PandasConvertible.to_pandas
  File ""pyarrow/table.pxi"", line 1367, in pyarrow.lib.Table._to_pandas
  File ""/Users/ueshin/workspace/databricks-koalas/miniconda/envs/databricks-koalas_3.7/lib/python3.7/site-packages/pyarrow/pandas_compat.py"", line 653, in table_to_blockmanager
    columns = _deserialize_column_index(table, all_columns, column_indexes)
  File ""/path/to/lib/python3.7/site-packages/pyarrow/pandas_compat.py"", line 704, in _deserialize_column_index
    columns = _flatten_single_level_multiindex(columns)
  File ""/path/to/lib/python3.7/site-packages/pyarrow/pandas_compat.py"", line 937, in _flatten_single_level_multiindex
    raise ValueError('Found non-unique column index')
ValueError: Found non-unique column index
{code}",,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 05:10:03 UTC 2020,,,,,,,,,,"0|z0dlh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/20 05:10;gurwls223;Issue resolved by pull request 28210
[https://github.com/apache/spark/pull/28210];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix NPE when BlockManagerSource is used after BlockManagerMaster stops,SPARK-31422,13297746,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,11/Apr/20 06:24,11/Apr/20 15:28,13/Jul/23 08:46,11/Apr/20 15:28,2.3.4,2.4.5,3.0.0,,,,,,,,,,2.4.6,3.0.0,,Spark Core,,,,0,,,"In `SparkEnv.stop`, the following stop sequence is used.
{code}
blockManager.stop()
blockManager.master.stop()
metricsSystem.stop()
{code}

During `metricsSystem.stop`, some sink can invoke `BlockManagerSource` and ends up with NPE.
{code}
sinks.foreach(_.stop)
registry.removeMatching((_: String, _: Metric) => true)
{code}

{code}
java.lang.NullPointerException
	at org.apache.spark.storage.BlockManagerMaster.getStorageStatus(BlockManagerMaster.scala:170)
	at org.apache.spark.storage.BlockManagerSource$$anonfun$10.apply(BlockManagerSource.scala:63)
	at org.apache.spark.storage.BlockManagerSource$$anonfun$10.apply(BlockManagerSource.scala:63)
	at org.apache.spark.storage.BlockManagerSource$$anon$1.getValue(BlockManagerSource.scala:31)
	at org.apache.spark.storage.BlockManagerSource$$anon$1.getValue(BlockManagerSource.scala:30)
{code}

Since SparkContext registers and forgets `BlockManagerSource` without deregistering, we had better avoid NPE inside `BlockManagerMaster`.
{code}
    _env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager))
{code}
",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 11 15:28:45 UTC 2020,,,,,,,,,,"0|z0divs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/20 15:28;dongjoon;Issue resolved by pull request 28187
[https://github.com/apache/spark/pull/28187];;;",,,,,,,,,,,,,,,,,,,,,,,
Infinite timeline redraw in job details page,SPARK-31420,13297711,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,Gengliang.Wang,Gengliang.Wang,10/Apr/20 22:57,17/Apr/20 21:35,13/Jul/23 08:46,16/Apr/20 17:37,2.0.2,2.1.3,2.2.3,2.3.4,2.4.5,3.0.0,3.1.0,,,,,,2.4.6,3.0.0,,Web UI,,,,0,,,"In the job page, the timeline section keeps changing the position style and shaking. We can see that there is a warning ""infinite loop in redraw"" from the console, which can be related to https://github.com/visjs/vis-timeline/issues/17

I am using the history server with the events under ""core/src/test/resources/spark-events"" to reproduce.
I have also uploaded a screen recording.",,dongjoon,Gengliang.Wang,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/20 22:57;Gengliang.Wang;timeline.mov;https://issues.apache.org/jira/secure/attachment/12999599/timeline.mov",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 13 23:40:28 UTC 2020,,,,,,,,,,"0|z0dio0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/20 22:58;Gengliang.Wang;[~sarutak] I looked into it but I am not familiar with this part. Could you please check it?;;;","11/Apr/20 06:39;sarutak;O.K, I'll look into this.;;;","12/Apr/20 00:39;dongjoon;Hi, does this bug exist only in `branch-3.0` and `master`?;;;","12/Apr/20 07:46;sarutak;Maybe, all the versions which use vis.js (>= 1.4.0) have this issue.;;;","13/Apr/20 23:40;dongjoon;Thank you for confirming, [~sarutak]!;;;",,,,,,,,,,,,,,,,,,,
file source backward compatibility after calendar switch,SPARK-31404,13297532,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,cloud_fan,cloud_fan,10/Apr/20 03:33,20/Sep/22 17:18,13/Jul/23 08:46,17/May/20 02:33,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"In Spark 3.0, we switch to the Proleptic Gregorian calendar by using the Java 8 datetime APIs. This makes Spark follow the ISO and SQL standard, but introduces some backward compatibility problems:
1. may read wrong data from the data files written by Spark 2.4
2. may have perf regression",,cloud_fan,connectsachit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26651,SPARK-30951,,,SPARK-34675,,"18/May/20 05:27;cloud_fan;Switch to Java 8 time API in Spark 3.0.pdf;https://issues.apache.org/jira/secure/attachment/13003245/Switch+to+Java+8+time+API+in+Spark+3.0.pdf",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 20 17:18:13 UTC 2022,,,,,,,,,,"0|z0dhk8:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"20/Sep/22 17:18;connectsachit;Hi [~cloud_fan] ,

Could you please confirm if we need to use below properties to ensure it can read data written by spark2.4x


spark.conf.set(""spark.sql.parquet.int96RebaseModeInRead"",""CORRECTED"")
spark.conf.set(""spark.sql.parquet.int96RebaseModeInWrite"",""CORRECTED"")


Regards
Sachit;;;",,,,,,,,,,,,,,,,,,,,,,,
Closure cleaner broken in Scala 2.12,SPARK-31399,13297412,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,rednaxelafx,cloud_fan,cloud_fan,09/Apr/20 15:01,19/May/20 19:30,13/Jul/23 08:46,18/May/20 05:33,2.4.5,3.0.0,,,,,,,,,,,2.4.6,3.0.0,,Spark Core,,,,0,,,"The `ClosureCleaner` only support Scala functions and it uses the following check to catch closures
{code}
  // Check whether a class represents a Scala closure
  private def isClosure(cls: Class[_]): Boolean = {
    cls.getName.contains(""$anonfun$"")
  }
{code}

This doesn't work in 3.0 any more as we upgrade to Scala 2.12 and most Scala functions become Java lambdas.

As an example, the following code works well in Spark 2.4 Spark Shell:
{code}
scala> :pa
// Entering paste mode (ctrl-D to finish)

import org.apache.spark.sql.functions.lit

case class Foo(id: String)
val col = lit(""123"")
val df = sc.range(0,10,1,1).map { _ => Foo("""") }

// Exiting paste mode, now interpreting.

import org.apache.spark.sql.functions.lit
defined class Foo
col: org.apache.spark.sql.Column = 123
df: org.apache.spark.rdd.RDD[Foo] = MapPartitionsRDD[5] at map at <pastie>:20
{code}

But fails in 3.0
{code}
scala> :pa
// Entering paste mode (ctrl-D to finish)

import org.apache.spark.sql.functions.lit

case class Foo(id: String)
val col = lit(""123"")
val df = sc.range(0,10,1,1).map { _ => Foo("""") }

// Exiting paste mode, now interpreting.

org.apache.spark.SparkException: Task not serializable
  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:396)
  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:386)
  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)
  at org.apache.spark.SparkContext.clean(SparkContext.scala:2371)
  at org.apache.spark.rdd.RDD.$anonfun$map$1(RDD.scala:422)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
  at org.apache.spark.rdd.RDD.map(RDD.scala:421)
  ... 39 elided
Caused by: java.io.NotSerializableException: org.apache.spark.sql.Column
Serialization stack:
	- object not serializable (class: org.apache.spark.sql.Column, value: 123)
	- field (class: $iw, name: col, type: class org.apache.spark.sql.Column)
	- object (class $iw, $iw@2d87ac2b)
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 1)
	- field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;)
	- object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class $iw, functionalInterfaceMethod=scala/Function1.apply:(Ljava/lang/Object;)Ljava/lang/Object;, implementation=invokeStatic $anonfun$df$1$adapted:(L$iw;Ljava/lang/Object;)LFoo;, instantiatedMethodType=(Ljava/lang/Object;)LFoo;, numCaptured=1])
	- writeReplace data (class: java.lang.invoke.SerializedLambda)
	- object (class $Lambda$2438/170049100, $Lambda$2438/170049100@d6b8c43)
  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)
  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)
  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:393)
  ... 47 more
{code}

**Apache Spark 2.4.5 with Scala 2.12**
{code}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.5
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_242)
Type in expressions to have them evaluated.
Type :help for more information.

scala> :pa
// Entering paste mode (ctrl-D to finish)

import org.apache.spark.sql.functions.lit

case class Foo(id: String)
val col = lit(""123"")
val df = sc.range(0,10,1,1).map { _ => Foo("""") }

// Exiting paste mode, now interpreting.

org.apache.spark.SparkException: Task not serializable
  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:403)
  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:393)
  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)
  at org.apache.spark.SparkContext.clean(SparkContext.scala:2326)
  at org.apache.spark.rdd.RDD.$anonfun$map$1(RDD.scala:393)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
  at org.apache.spark.rdd.RDD.map(RDD.scala:392)
  ... 45 elided
Caused by: java.io.NotSerializableException: org.apache.spark.sql.Column
Serialization stack:
	- object not serializable (class: org.apache.spark.sql.Column, value: 123)
	- field (class: $iw, name: col, type: class org.apache.spark.sql.Column)
	- object (class $iw, $iw@73534675)
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 1)
	- field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;)
	- object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class $iw, functionalInterfaceMethod=scala/Function1.apply:(Ljava/lang/Object;)Ljava/lang/Object;, implementation=invokeStatic $anonfun$df$1$adapted:(L$iw;Ljava/lang/Object;)LFoo;, instantiatedMethodType=(Ljava/lang/Object;)LFoo;, numCaptured=1])
	- writeReplace data (class: java.lang.invoke.SerializedLambda)
	- object (class $Lambda$1952/356563238, $Lambda$1952/356563238@6ca95b1e)
  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:400)
  ... 53 more
{code}",,advancedxy,apachespark,cloud_fan,dbtsai,dongjoon,holden,javier_ivanov,joshrosen,maryannxue,Ngone51,Qin Yao,rednaxelafx,rxin,sfcoy,smilegator,Tagar,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 06:21:41 UTC 2020,,,,,,,,,,"0|z0dgtk:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"09/Apr/20 15:05;cloud_fan;cc [~rxin] [~sowen] [~dongjoon] [~tgraves];;;","09/Apr/20 16:02;smilegator;cc [~zsxwing];;;","09/Apr/20 20:42;dongjoon;Thank you for pinging me, [~cloud_fan].
cc [~dbtsai] and [~holden];;;","10/Apr/20 05:56;rxin;This is bad... [~sowen] and [~joshrosen]  did you look into this in the past?;;;","10/Apr/20 08:43;joshrosen;h3. My rough first impression

I think the problem is that Spark 3.x isn't performing full cleaning of lambdas: the old cleaning logic (which clones closures and nulls out unreferenced fields) only seems to run for non-lambdas ([source|https://github.com/apache/spark/blob/e42a3945acd614a26c7941a9eed161b500fb4520/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala#L259-L260]); the lambda cleaner runs a [subset|https://github.com/apache/spark/blame/e42a3945acd614a26c7941a9eed161b500fb4520/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala#L374-L382] of the old cleaning logic to check for {{return}} statements, skipping the rest of the cleaning steps.
h3. Different cases to consider 

Some closures might contain _entirely_ spurious {{$outer}} references, where in theory we could just omit the entire {{$outer}} object (as opposed to cloning it and nulling a subset of its fields). These situations can happen because the Scala compiler's (escape?) analysis isn't perfect.
 * In 2.11 these will closures will be serializable because the ClosureCleaner can null the {{$outer}} reference.
 * In 2.12 these will fail to serialize because we can't perform that type of cleaning.
 * In the past we discovered some cases where Scala 2.12 would generate _more_ unnecessary {{$outer}} references than 2.11, so a closure which could have been serialized _even without any cleaning_ on 2.11 would require cleaning to serialize on 2.12. SPARK-14540 describes a few examples of this: I think the known cases have been fixed in newer 2.12 versions.
 * In other cases, however, _both_ 2.11 and 2.12 generate entirely spurious references. These aren't behavior regressions w.r.t capture (both versions capture the same things), but such cases will be broken on 2.12 unless we add full cleaning support for lambdas (or modify improve Scala 2.12's analysis beyond 2.11's so the compiler never generate the unnecessary reference in the first place).

In other cases, however, we can't entirely null out all captured references because we need to transitively access some fields or methods in those references. Even if the compiler analysis was perfect, I think these cases will still require cleaning to be serializable.
h3. Example of unnecessary $outer capture in both 2.11 and 2.12

Here's an example of a closure which over-captures in both 2.11 and 2.12. Here I'm deliberately using {{sc.emptyRDD}} instead of {{spark.range}} because {{range}} makes it own closure cleaner calls and that clutters up the logs:
{code:java}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.4-SNAPSHOT
      /_/

Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_191)
Type in expressions to have them evaluated.
Type :help for more information.

scala> :paste
// Entering paste mode (ctrl-D to finish)

sc.setLogLevel(""DEBUG"")
case class Foo(id: String)
val nonSerializableObj = new Object
val df = sc.emptyRDD[Int].map { _ => Foo("""") }

// Exiting paste mode, now interpreting.

20/04/10 00:56:24 DEBUG ClosureCleaner: +++ Cleaning closure <function1> ($line14.$read$$iw$$iw$$anonfun$1) +++
20/04/10 00:56:24 DEBUG ClosureCleaner:  + declared fields: 2
20/04/10 00:56:24 DEBUG ClosureCleaner:      public static final long $line14.$read$$iw$$iw$$anonfun$1.serialVersionUID
20/04/10 00:56:24 DEBUG ClosureCleaner:      private final $line14.$read$$iw$$iw $line14.$read$$iw$$iw$$anonfun$1.$outer
20/04/10 00:56:24 DEBUG ClosureCleaner:  + declared methods: 2
20/04/10 00:56:24 DEBUG ClosureCleaner:      public final java.lang.Object $line14.$read$$iw$$iw$$anonfun$1.apply(java.lang.Object)
20/04/10 00:56:24 DEBUG ClosureCleaner:      public final $line14.$read$$iw$$iw$Foo $line14.$read$$iw$$iw$$anonfun$1.apply(int)
20/04/10 00:56:24 DEBUG ClosureCleaner:  + inner classes: 0
20/04/10 00:56:24 DEBUG ClosureCleaner:  + outer classes: 1
20/04/10 00:56:24 DEBUG ClosureCleaner:      $line14.$read$$iw$$iw
20/04/10 00:56:24 DEBUG ClosureCleaner:  + outer objects: 1
20/04/10 00:56:24 DEBUG ClosureCleaner:      $line14.$read$$iw$$iw@163cd4a6
20/04/10 00:56:24 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
20/04/10 00:56:24 DEBUG ClosureCleaner:  + fields accessed by starting closure: 2
20/04/10 00:56:24 DEBUG ClosureCleaner:      (class $line14.$read$$iw$$iw,Set())
20/04/10 00:56:24 DEBUG ClosureCleaner:      (class java.lang.Object,Set())
20/04/10 00:56:24 DEBUG ClosureCleaner:  + outermost object is a REPL line object, so we clone it: (class $line14.$read$$iw$$iw,$line14.$read$$iw$$iw@163cd4a6)
20/04/10 00:56:24 DEBUG ClosureCleaner:  + cloning the object $line14.$read$$iw$$iw@163cd4a6 of class $line14.$read$$iw$$iw
20/04/10 00:56:24 DEBUG ClosureCleaner:  +++ closure <function1> ($line14.$read$$iw$$iw$$anonfun$1) is now cleaned +++
defined class Foo
nonSerializableObj: Object = java.lang.Object@f1d5f3
df: org.apache.spark.rdd.RDD[Foo] = MapPartitionsRDD[1] at map at <console>:16
{code}
Here we have an outer object of type {{$line14.$read$$iw$$iw}} but the ""{{fields accessed by starting closure:""}} log output shows that the closure doesn't actually reference any of the outer object's fields.

In 3.0 this fails because the closure cleaning doesn't omit the outer object reference:
{code:java}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT
      /_/

Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_191)
Type in expressions to have them evaluated.
Type :help for more information.

scala> :paste
// Entering paste mode (ctrl-D to finish)

sc.setLogLevel(""DEBUG"")
case class Foo(id: String)
val nonSerializableObj = new Object
val df = sc.emptyRDD[Int].map { _ => Foo("""") }

// Exiting paste mode, now interpreting.

20/04/10 01:00:34 DEBUG ClosureCleaner: Cleaning lambda: $anonfun$df$1$adapted
20/04/10 01:00:34 DEBUG ClosureCleaner:  +++ Lambda closure ($anonfun$df$1$adapted) is now cleaned +++
org.apache.spark.SparkException: Task not serializable
  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:396)
  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:386)
  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)
  at org.apache.spark.SparkContext.clean(SparkContext.scala:2379)
  at org.apache.spark.rdd.RDD.$anonfun$map$1(RDD.scala:396)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)
  at org.apache.spark.rdd.RDD.map(RDD.scala:395)
  ... 41 elided
Caused by: java.io.NotSerializableException: java.lang.Object
Serialization stack:
	- object not serializable (class: java.lang.Object, value: java.lang.Object@76c1ede3)
	- field (class: $iw, name: nonSerializableObj, type: class java.lang.Object)
	- object (class $iw, $iw@463a0302)
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 1)
	- field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;)
	- object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class $iw, functionalInterfaceMethod=scala/Function1.apply:(Ljava/lang/Object;)Ljava/lang/Object;, implementation=invokeStatic $anonfun$df$1$adapted:(L$iw;Ljava/lang/Object;)LFoo;, instantiatedMethodType=(Ljava/lang/Object;)LFoo;, numCaptured=1])
	- writeReplace data (class: java.lang.invoke.SerializedLambda)
	- object (class $Lambda$2029/1917668362, $Lambda$2029/1917668362@60af08b2)
  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)
  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)
  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:393)
  ... 49 more
 {code}
h3. Example of necessary reference to cleanable $outer object

Here's a similar example, except this time the closure needs to reference a subset of the outer object's fields:
{code:java}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.4-SNAPSHOT
      /_/

Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_191)
Type in expressions to have them evaluated.
Type :help for more information.

scala> :paste
// Entering paste mode (ctrl-D to finish)

sc.setLogLevel(""DEBUG"")
val constant = 1
val nonSerializableObj = new Object
val df = sc.emptyRDD[Int].map { _ => constant }

// Exiting paste mode, now interpreting.

20/04/10 01:06:30 DEBUG ClosureCleaner: +++ Cleaning closure <function1> ($line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1) +++
20/04/10 01:06:30 DEBUG ClosureCleaner:  + declared fields: 2
20/04/10 01:06:30 DEBUG ClosureCleaner:      public static final long $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.serialVersionUID
20/04/10 01:06:30 DEBUG ClosureCleaner:      private final $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.$outer
20/04/10 01:06:30 DEBUG ClosureCleaner:  + declared methods: 3
20/04/10 01:06:30 DEBUG ClosureCleaner:      public final int $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(int)
20/04/10 01:06:30 DEBUG ClosureCleaner:      public final java.lang.Object $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(java.lang.Object)
20/04/10 01:06:30 DEBUG ClosureCleaner:      public int $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply$mcII$sp(int)
20/04/10 01:06:30 DEBUG ClosureCleaner:  + inner classes: 0
20/04/10 01:06:30 DEBUG ClosureCleaner:  + outer classes: 1
20/04/10 01:06:30 DEBUG ClosureCleaner:      $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw
20/04/10 01:06:30 DEBUG ClosureCleaner:  + outer objects: 1
20/04/10 01:06:30 DEBUG ClosureCleaner:      $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw@7bc239db
20/04/10 01:06:30 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
20/04/10 01:06:30 DEBUG ClosureCleaner:  + fields accessed by starting closure: 2
20/04/10 01:06:30 DEBUG ClosureCleaner:      (class $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw,Set(constant))
20/04/10 01:06:30 DEBUG ClosureCleaner:      (class java.lang.Object,Set())
20/04/10 01:06:30 DEBUG ClosureCleaner:  + outermost object is a REPL line object, so we clone it: (class $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw,$line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw@7bc239db)
20/04/10 01:06:30 DEBUG ClosureCleaner:  + cloning the object $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw@7bc239db of class $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw
20/04/10 01:06:30 DEBUG ClosureCleaner:  +++ closure <function1> ($line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1) is now cleaned +++
constant: Int = 1
nonSerializableObj: Object = java.lang.Object@5dd5422f
df: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at <console>:27
{code}
Here Spark has cloned both the closure _and_ the outer object, nulling the cloned outer object's unaccessed fields.

As expected, this fails in 2.12:
{code:java}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT
      /_/

Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_191)
Type in expressions to have them evaluated.
Type :help for more information.

scala> :paste
// Entering paste mode (ctrl-D to finish)

sc.setLogLevel(""DEBUG"")
val constant = 1
val nonSerializableObj = new Object
val df = sc.emptyRDD[Int].map { _ => constant }

// Exiting paste mode, now interpreting.

20/04/10 01:07:49 DEBUG ClosureCleaner: Cleaning lambda: $anonfun$df$1
20/04/10 01:07:49 DEBUG ClosureCleaner:  +++ Lambda closure ($anonfun$df$1) is now cleaned +++
org.apache.spark.SparkException: Task not serializable
  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:396)
  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:386)
  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)
  at org.apache.spark.SparkContext.clean(SparkContext.scala:2379)
  at org.apache.spark.rdd.RDD.$anonfun$map$1(RDD.scala:396)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)
  at org.apache.spark.rdd.RDD.map(RDD.scala:395)
  ... 47 elided
Caused by: java.io.NotSerializableException: java.lang.Object
Serialization stack:
	- object not serializable (class: java.lang.Object, value: java.lang.Object@291fbdc9)
	- field (class: $iw, name: nonSerializableObj, type: class java.lang.Object)
	- object (class $iw, $iw@6f3b4c9a)
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 1)
	- field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;)
	- object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class $iw, functionalInterfaceMethod=scala/runtime/java8/JFunction1$mcII$sp.apply$mcII$sp:(I)I, implementation=invokeStatic $anonfun$df$1:(L$iw;I)I, instantiatedMethodType=(I)I, numCaptured=1])
	- writeReplace data (class: java.lang.invoke.SerializedLambda)
	- object (class $Lambda$1853/1591026569, $Lambda$1853/1591026569@1cf38112)
  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)
  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)
  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)
  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:393)
  ... 55 more
 {code}
h3. Brainstorming on possible fixes and workarounds

Some rough ideas on how Spark / Scala could fix this (not all of them good, but listed for completeness / brainstorming):
 * If the Scala compiler didn't over-capture then more closures would be serializable without any cleaning. However:
 ** It requires a new Scala release and requires users to upgrade to it.
 ** It's probably pretty hard to do, especially without impacting compile performance.
 ** It isn't a complete fix because it only addresses the ""completely spurious capture"" case, not the ""capture but only need a subset of things along the reference chains"" cases.
 ** Even if a closure is serializable without cleaning, unreferenced outer object fields might add a lot of bloat to the closure which slows down deserialization (which is performed on a per-task basis).
 * If Spark implemented ""full"" closure cleaning for lambdas then we might be able to achieve near-complete parity.
 ** I'm unsure of technical feasibility, but this is definitely worth investigating further: this seems like the most user-friendly fix.
 ** If we do this, we should re-introduce some of the ClosureCleaner suite tests which were removed in [https://github.com/apache/spark/commit/8bc304f97ee693b57f33fa6708eb63e2d641c609] 

Some workarounds which require users to change their code:
 * Users could mark unserializable / unused closure fields as {{@transient}}:
 ** In my examples above, marking the {{nonSerializableObj}} field as {{@transient}} would allow the closure to be serialized without cleaning (and avoids size bloat).
 * Moving the failures to compile-time:
 ** This ""non-serializable closure"" issue is especially painful because it shows up at compile-time, not runtime: if users' code doesn't have full unit / integration test coverage then finding these issues can be an unpleasant game of wack-a-mole of tracking down broken jobs (hopefully in a test / staging environment and not production).
 ** If we could somehow move these failures to compile time (whether through macros or a compiler plugin) then users would have a much more reasonable porting devloop since we'd be moving further in the direction of ""if the code compiles then it'll probably run successfully"". It looks like Spores can do this: [https://scalacenter.github.io/spores/java-serialization.html]
 ** In notebooks and REPLs the compile-time and run-time phases are interleaved, so these approaches wouldn't be of as much help there.;;;","10/Apr/20 16:48;dongjoon;According to the above information,
- I added the failure result in Apache Spark 2.4.5 with Scala 2.12 additionally into the JIRA description.
- Update the title from `in Spark 3.0` to `in Scala 2.12`.
- Add `2.4.5` as `Affected Version` (but not a target version);;;","15/Apr/20 20:15;smilegator;[~rednaxelafx] will help this ticket and do more investigation. ;;;","26/Apr/20 19:29;dongjoon;Hi, [~smilegator] and [~rednaxelafx]. Is there any update for this Blocker issue? Thank you for any update in advance!;;;","30/Apr/20 06:29;rednaxelafx;Hi [~dongjoon], I've been working on a fix of this issue and will send out a WIP PR as soon as possible. I've pretty much done an analysis of the situation in parallel to [~joshrosen]'s analysis above and have arrived at very similar conclusions.

The fact is, Scala 2.12+'s indylambda (aka LMF-based closures) does still have an equivalent of an {{""$outer""}}, just under a different name. Thus the logic inside the {{ClosureCleaner}} for Scala 2.11 support has to be ported basically verbatim to Scala 2.12+/indylambda. That's exactly what I'm working on right now, and it's the main contents of the WIP PR.

A separate issue is that the test coverage of {{ClosureCleaner}} in the Spark repo is very insufficient. Neither {{ClosureCleanerSuite}} nor {{ClosureCleanerSuite2}} cover anything related to the Scala REPL. There needs to be a separate suite, similar to {{ReplSuite}}, that fires up an actual Scala REPL and trigger ClosureCleaner in it to bridge the gap in test coverage. I will do that as a second step of the PR, and once the new test suite is in, the PR can be considered complete and ready for final review.;;;","30/Apr/20 15:08;dongjoon;Thank you so much, [~rednaxelafx].;;;","06/May/20 11:51;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/28463;;;","18/May/20 05:33;cloud_fan;Issue resolved by pull request 28463
[https://github.com/apache/spark/pull/28463];;;","19/May/20 06:21;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/28577;;;","19/May/20 06:21;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/28577;;;",,,,,,,,,,
HiveThriftServer2Listener update methods fail with unknown operation/session id,SPARK-31387,13297125,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smesseim,smesseim,smesseim,08/Apr/20 16:33,22/May/20 04:05,13/Jul/23 08:46,20/May/20 17:31,2.3.4,2.4.5,3.0.0,,,,,,,,,,3.1.0,,,SQL,,,,0,,,"HiveThriftServer2Listener update methods, such as onSessionClosed and onOperationError throw a NullPointerException (in Spark 3) or a NoSuchElementException (in Spark 2) when the input session/operation id is unknown. In Spark 2, this can cause control flow issues with the caller of the listener. In Spark 3, the listener is called by a ListenerBus which catches the exception, but it would still be nicer if an invalid update is logged and does not throw an exception.",,apachespark,dongjoon,smesseim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 20 17:31:12 UTC 2020,,,,,,,,,,"0|z0df1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/May/20 19:16;dongjoon;This is reverted from branch-3.0/master inevitability because this breaks all Maven jobs in both branches. Please see the comments on the original PR.;;;","15/May/20 13:44;apachespark;User 'alismess-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/28544;;;","20/May/20 17:31;dongjoon;Issue resolved by pull request 28544
[https://github.com/apache/spark/pull/28544];;;",,,,,,,,,,,,,,,,,,,,,
Show a better error message for different python and pip installation mistake,SPARK-31382,13297008,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,08/Apr/20 07:53,12/Dec/22 18:10,13/Jul/23 08:46,09/Apr/20 02:05,2.4.5,3.0.0,,,,,,,,,,,2.4.6,3.0.0,,PySpark,,,,0,,,See https://stackoverflow.com/questions/46286436/running-pyspark-after-pip-install-pyspark/49587560,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-32082,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 09 02:05:40 UTC 2020,,,,,,,,,,"0|z0debs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/20 02:05;gurwls223;Issue resolved by pull request 28152
[https://github.com/apache/spark/pull/28152];;;",,,,,,,,,,,,,,,,,,,,,,,
stage level scheduling dynamic allocation bug with initial num executors,SPARK-31378,13296845,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,07/Apr/20 14:38,07/Apr/20 21:56,13/Jul/23 08:46,07/Apr/20 21:56,3.1.0,,,,,,,,,,,,3.1.0,,,Spark Core,,,,0,,,"I found a bug in the stage level scheduling dynamic allocation code when you have a non default profile and it has an initial number of executors the same as what the number of executors needed for the first job, then we don't properly request the executors.",,dongjoon,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27495,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 07 21:56:47 UTC 2020,,,,,,,,,,"0|z0ddbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/20 21:56;dongjoon;This is resolved via https://github.com/apache/spark/pull/28146;;;",,,,,,,,,,,,,,,,,,,,,,,
Fix hung-up issue in StagePage,SPARK-31360,13296509,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,06/Apr/20 12:14,06/Apr/20 19:50,13/Jul/23 08:46,06/Apr/20 19:38,3.1.0,,,,,,,,,,,,3.1.0,,,Web UI,,,,0,,,"StagePage will be hung-up with following operations.

1. Run a job with shuffle.
scala> sc.parallelize(1 to 10).map(x => (x, x)).reduceByKey(_ + _).collect

2. Visit StagePage for the stage writing shuffle data and check `Shuffle Write Time`.

3. Run a job with no shuffle.
scala> sc.parallelize(1 to 10).collect

4. Visit StagePage for the last stage.

This issue is caused by following reason.

In stagepage.js, an array `optionalColumns` has indices for columns for optional metrics.
If a stage doesn't perform shuffle read or write, the corresponding indices are removed from the array.
StagePage doesn't try to create column for such metrics, even if the state of corresponding optional metrics are preserved as ""visible"".
But, if a stage doesn't perform both shuffle read and write, the index for `Shuffle Write Time` isn't removed.
In that case, StagePage tries to create a column for `Shuffle Write Time` even though there are no metrics for shuffle write, leading hungup.",,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31073,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 06 19:38:36 UTC 2020,,,,,,,,,,"0|z0dbhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/20 19:38;dongjoon;Issue resolved by pull request 28136
[https://github.com/apache/spark/pull/28136];;;",,,,,,,,,,,,,,,,,,,,,,,
SparkSession Lifecycle methods to fix memory leak,SPARK-31354,13296362,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vinooganesh,vinooganesh,vinooganesh,05/Apr/20 14:36,03/Jul/20 03:32,13/Jul/23 08:46,21/May/20 16:07,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"Follow up to https://issues.apache.org/jira/browse/SPARK-27958 after discussion on [https://github.com/apache/spark/pull/24807]. 

 

Let's instead expose methods that allow the user to manually clean up (terminate) a SparkSession, that also remove the listenerState from the context. ",,cloud_fan,Jackey Lee,vinooganesh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32165,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 16:07:01 UTC 2020,,,,,,,,,,"0|z0dal4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/May/20 16:07;cloud_fan;Issue resolved by pull request 28128
[https://github.com/apache/spark/pull/28128];;;",,,,,,,,,,,,,,,,,,,,,,,
Remove SaveMode check in v2 FileWriteBuilder,SPARK-31321,13295440,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,01/Apr/20 07:40,02/Apr/20 16:52,13/Jul/23 08:46,02/Apr/20 08:35,3.1.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"SaveMode is never assigned, so it will fail when calling `validateInputs`",,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 08:35:38 UTC 2020,,,,,,,,,,"0|z0d5fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/20 08:35;cloud_fan;Issue resolved by pull request 28090
[https://github.com/apache/spark/pull/28090];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix release script for 3.0.0,SPARK-31320,13295409,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,01/Apr/20 03:53,12/Dec/22 18:10,13/Jul/23 08:46,01/Apr/20 07:45,3.0.0,,,,,,,,,,,,3.0.0,,,Project Infra,,,,0,,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 01 07:45:54 UTC 2020,,,,,,,,,,"0|z0d58o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/20 07:45;gurwls223;Fixed in https://github.com/apache/spark/pull/28088;;;",,,,,,,,,,,,,,,,,,,,,,,
Revert SPARK-29285 to fix shuffle regression caused by creating temporary file eagerly,SPARK-31314,13295204,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,31/Mar/20 09:09,28/Jul/21 07:29,13/Jul/23 08:46,31/Mar/20 11:37,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"In SPARK-29285, we change to create shuffle temporary eagerly. This is helpful for not to fail the entire task in the scenario of occasional disk failure.

But for the applications that many tasks don't actually create shuffle files, it caused overhead. See the below benchmark:

Env: Spark local-cluster[2, 4, 19968], each queries run 5 round, each round 5 times.
Data: TPC-DS scale=99 generate by spark-tpcds-datagen

Results:
|| ||Base||Revert||
|Q20|Vector(4.096865667, 2.76231748, 2.722007606, 2.514433591, 2.400373579) Median 2.722007606|Vector(3.763185446, 2.586498463, 2.593472842, 2.320522846, 2.224627274) Median 2.586498463|
|Q33|Vector(5.872176321, 4.854397586, 4.568787136, 4.393378146, 4.423996818) Median 4.568787136|Vector(5.38746785, 4.361236877, 4.082311276, 3.867206824, 3.783188024) Median 4.082311276|
|Q52|Vector(3.978870321, 3.225437871, 3.282411608, 2.869674887, 2.644490664) Median 3.225437871|Vector(4.000381522, 3.196025108, 3.248787619, 2.767444508, 2.606163423) Median 3.196025108|
|Q56|Vector(6.238045133, 4.820535173, 4.609965579, 4.313509894, 4.221256227) Median 4.609965579|Vector(6.241611339, 4.225592467, 4.195202502, 3.757085755, 3.657525982) Median 4.195202502|",,chengbing.liu,cloud_fan,XuanYuan,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29285,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 28 07:04:52 UTC 2021,,,,,,,,,,"0|z0d3zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/20 11:37;cloud_fan;Issue resolved by pull request 28072
[https://github.com/apache/spark/pull/28072];;;","28/Jul/21 07:04;zhuqi;cc [~XuanYuan] [~cloud_fan] [~Ngone51] 

Since this has been reverted, i meet the disk failure in our production clusters, how can we handle the disk failed problem without this.

There are many disks in yarn clusters, but if one disk failure happend, we just retry the task, if we can avoid retry to the same failed disk in one node? Or if spark has some disk blacklist solution now?

And reverted solution causes that applications with many tasks don't actually create shuffle files, it caused overhead, if we can get a workaround solution to avoid create when tasks don't need temp shuffle files, i still think we should handle this.

The logs are: 
{code:java}
DAGScheduler: ShuffleMapStage 521 (insertInto at Tools.scala:147) failed in 4.995 s due to Job aborted due to stage failure: Task 30 in stage 521.0 failed 4 times, most recent failure: Lost task 30.3 in stage 521.0 (TID 127941, ********** 91): java.io.FileNotFoundException: /data2/yarn/local/usercache/aa/appcache/*****/blockmgr-eb5ca215-a7af-41be-87ee-89fd7e3b1de5/0e/temp_shuffle_45279ef1-5143-4632-9df0-d7ee1f50c026 (Input/output error)
 at java.io.FileOutputStream.open0(Native Method)
 at java.io.FileOutputStream.open(FileOutputStream.java:270)
 at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
 at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:103)
 at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:116)
 at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:237)
 at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)
 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
 at org.apache.spark.scheduler.Task.run(Task.scala:121)
 at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
 at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 at java.lang.Thread.run(Thread.java:745)
{code}
Thanks.

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,
Transforming Hive simple UDF (using JAR) expression may incur CNFE in later evaluation,SPARK-31312,13295185,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,31/Mar/20 07:42,07/Apr/20 13:26,13/Jul/23 08:46,31/Mar/20 16:32,2.4.5,3.0.0,,,,,,,,,,,2.4.6,3.0.0,,SQL,,,,0,,,"In SPARK-26560, we ensured that Hive UDF using JAR is executed regardless of current thread context classloader.

[~cloud_fan] pointed out another potential issue in post-review of SPARK-26560 - quoting the comment:

{quote}
Found a potential problem: here we call HiveSimpleUDF.dateType (which is a lazy val), to force to load the class with the corrected class loader.

However, if the expression gets transformed later, which copies HiveSimpleUDF, then calling HiveSimpleUDF.dataType will re-trigger the class loading, and at that time there is no guarantee that the corrected classloader is used.

I think we should materialize the loaded class in HiveSimpleUDF.
{quote}

This JIRA issue is to track the effort of verifying the potential issue and fixing the issue.",,dongjoon,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26560,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 06:29:35 UTC 2020,,,,,,,,,,"0|z0d3v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/20 18:40;dongjoon;Hi, [~kabhwan]. Due to SPARK-26560, this occurs only for 2.4.5 and 3.0.0?
;;;","01/Apr/20 23:58;kabhwan;No, it wasn't triggered by SPARK-26560 and should be long-lived bug same as SPARK-26560, although I haven't encountered the bug in practice so I can't say which version is the oldest.

Btw, while I always appreciate your details and diligence on leaving information, it's a different story if that is required to others. That would become a burden to do actual work, spending more time on ""boilerplate"" than the origin work. There should be a ""balance"", and the balance should be decided by consensus of community.

Let's discuss more in dev@ mailing list.;;;","02/Apr/20 06:25;dongjoon;It's for informimg the users (and the downstream distributors) the risk and to recommend upgrade their versions. If we set 2.4.5 only, that can be also considered as a bug occurred at 2.4.5 .

If we set 2.3.x at least, all 2.4.0 ~ 2.4.4 users also understand the risk.;;;","02/Apr/20 06:29;dongjoon;Since I saw your opinion, I'll not ping you about that again.;;;",,,,,,,,,,,,,,,,,,,,
Fix wrong examples and help messages for Kinesis integration,SPARK-31293,13294752,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sekikn,sekikn,sekikn,28/Mar/20 13:47,29/Mar/20 21:32,13/Jul/23 08:46,29/Mar/20 21:31,2.3.4,2.4.5,3.0.0,,,,,,,,,,2.4.6,3.0.0,,Documentation,DStreams,,,0,,,"There are some minor mistakes in the examples and the help messages for Kinesis integration. For example, {{KinesisWordCountASL.scala}} takes three arguments but its example is taking four, while {{kinesis_wordcount_asl.py}} takes four but its example is taking three.",,dongjoon,sekikn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 29 21:31:14 UTC 2020,,,,,,,,,,"0|z0d17c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/20 21:31;dongjoon;Issue resolved by pull request 28063
[https://github.com/apache/spark/pull/28063];;;",,,,,,,,,,,,,,,,,,,,,,,
Flaky test: org.apache.spark.sql.hive.thriftserver.CliSuite,SPARK-31289,13294529,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,27/Mar/20 18:29,16/May/20 09:12,13/Jul/23 08:46,16/May/20 09:12,3.1.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"
{code:java}
Caused by: MetaException(message:Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=/home/jenkins/workspace/SparkPullRequestBuilder/target/tmp/spark-17bf6d71-1e68-4e56-b656-f1b2fd2e15fb;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
2020-03-27 09:20:11.949 - stderr> java.sql.SQLException: Failed to create database '/home/jenkins/workspace/SparkPullRequestBuilder/target/tmp/spark-17bf6d71-1e68-4e56-b656-f1b2fd2e15fb', see the next exception for details.
{code}



{code:java}
Caused by: ERROR XBM0A: The database directory '/home/jenkins/workspace/SparkPullRequestBuilder@4/target/tmp/spark-84c8ff0e-214f-416c-9d44-ab19f864a79b' exists. However, it does not contain the expected 'service.properties' file. Perhaps Derby was brought down in the middle of creating this database. You may want to delete this directory and try creating the database again.
{code}

",,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 16 09:12:09 UTC 2020,,,,,,,,,,"0|z0czts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/May/20 09:12;cloud_fan;Issue resolved by pull request 28055
[https://github.com/apache/spark/pull/28055];;;",,,,,,,,,,,,,,,,,,,,,,,
"groupby().applyInPandas, groupby().cogroup().applyInPandas and mapInPandas should ignore type hints",SPARK-31287,13294444,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,27/Mar/20 11:54,12/Dec/22 18:11,13/Jul/23 08:46,29/Mar/20 05:00,3.1.0,,,,,,,,,,,,3.0.0,,,PySpark,SQL,,,0,,,"Setting type hints in pandas function API should not matter at this moment. However, currently it tries to infer type hint when it's set.

{code}
import pandas as pd

def pandas_plus_one(v: pd.DataFrame) -> pd.DataFrame:
    return v + 1

spark.range(10).groupby('id').applyInPandas(pandas_plus_one, schema=""id long"").show()
{code}

{code}
Traceback (most recent call last):
  File ""/.../spark/python/pyspark/sql/utils.py"", line 98, in deco
    return f(*a, **kw)
  File ""/.../spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py"", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o34.flatMapGroupsInPandas.
: java.lang.IllegalArgumentException: requirement failed: Must pass a grouped map udf
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.sql.RelationalGroupedDataset.flatMapGroupsInPandas(RelationalGroupedDataset.scala:541)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../spark/python/pyspark/sql/pandas/group_ops.py"", line 182, in applyInPandas
    jdf = self._jgd.flatMapGroupsInPandas(udf_column._jc.expr())
  File ""/.../spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1305, in __call__
  File ""/.../spark/python/pyspark/sql/utils.py"", line 102, in deco
    raise converted
pyspark.sql.utils.IllegalArgumentException: requirement failed: Must pass a grouped map udf
{code}

Looks {{groupby().cogroup().applyInPandas}} and {{mapInPandas}} also have the same issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 29 05:00:09 UTC 2020,,,,,,,,,,"0|z0czaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/20 05:00;gurwls223;Issue resolved by pull request 28052
[https://github.com/apache/spark/pull/28052];;;",,,,,,,,,,,,,,,,,,,,,,,
numOutputRows shows value from last micro batch when there is no new data,SPARK-31278,13294287,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,26/Mar/20 19:39,08/Apr/20 01:02,13/Jul/23 08:46,08/Apr/20 00:21,3.0.0,,,,,,,,,,,,3.0.0,,,Structured Streaming,,,,0,,,"In Structured Streaming, we provide progress updates every 10 seconds when a stream doesn't have any new data upstream. When providing this progress though, we zero out the input information but not the output information.",,brkyvz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 00:20:30 UTC 2020,,,,,,,,,,"0|z0cyc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/20 00:20;brkyvz;Resolved by [https://github.com/apache/spark/pull/28040];;;",,,,,,,,,,,,,,,,,,,,,,,
fix web ui for driver side SQL metrics,SPARK-31271,13294207,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,26/Mar/20 13:47,30/Mar/20 03:12,13/Jul/23 08:46,27/Mar/20 22:46,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,,,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 27 22:46:14 UTC 2020,,,,,,,,,,"0|z0cxu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/20 22:46;dongjoon;Issue resolved by pull request 28037
[https://github.com/apache/spark/pull/28037];;;",,,,,,,,,,,,,,,,,,,,,,,
Flaky test: WholeStageCodegenSparkSubmitSuite.Generated code on driver should not embed platform-specific constant,SPARK-31267,13294172,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tianshi,gsomogyi,gsomogyi,26/Mar/20 10:51,12/Dec/22 18:10,13/Jul/23 08:46,04/May/20 05:51,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,Tests,,,0,,,"https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/120363/testReport/
{code}
Error Message
org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to failAfter did not complete within 1 minute.
Stacktrace
sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to failAfter did not complete within 1 minute.
	at java.lang.Thread.getStackTrace(Thread.java:1559)
	at org.scalatest.concurrent.TimeLimits.failAfterImpl(TimeLimits.scala:234)
	at org.scalatest.concurrent.TimeLimits.failAfterImpl$(TimeLimits.scala:233)
	at org.apache.spark.deploy.SparkSubmitSuite$.failAfterImpl(SparkSubmitSuite.scala:1416)
	at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:230)
	at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:229)
	at org.apache.spark.deploy.SparkSubmitSuite$.failAfter(SparkSubmitSuite.scala:1416)
	at org.apache.spark.deploy.SparkSubmitSuite$.runSparkSubmit(SparkSubmitSuite.scala:1435)
	at org.apache.spark.sql.execution.WholeStageCodegenSparkSubmitSuite.$anonfun$new$1(WholeStageCodegenSparkSubmitSuite.scala:53)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:151)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:286)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:58)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)
	at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:58)
	at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:393)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:381)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:376)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:458)
	at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite.run(Suite.scala:1124)
	at org.scalatest.Suite.run$(Suite.scala:1106)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:518)
	at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:58)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:58)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:317)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:510)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: sbt.ForkMain$ForkError: java.lang.InterruptedException: null
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:502)
	at java.lang.UNIXProcess.waitFor(UNIXProcess.java:395)
	at org.apache.spark.deploy.SparkSubmitSuite$.$anonfun$runSparkSubmit$2(SparkSubmitSuite.scala:1435)
	at scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)
	at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)
	at org.scalatest.concurrent.TimeLimits.failAfterImpl(TimeLimits.scala:239)
	... 53 more
{code}
",,apachespark,gsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 04 05:51:56 UTC 2020,,,,,,,,,,"0|z0cxmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/May/20 00:16;apachespark;User 'tianshizz' has created a pull request for this issue:
https://github.com/apache/spark/pull/28438;;;","03/May/20 00:16;apachespark;User 'tianshizz' has created a pull request for this issue:
https://github.com/apache/spark/pull/28438;;;","04/May/20 05:51;gurwls223;Fixed in https://github.com/apache/spark/pull/28438;;;",,,,,,,,,,,,,,,,,,,,,
"Test case import another test case contains bracketed comments, can't display bracketed comments in golden files well.",SPARK-31262,13294073,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,beliefer,beliefer,beliefer,26/Mar/20 03:14,27/Mar/20 00:11,13/Jul/23 08:46,26/Mar/20 23:10,3.1.0,,,,,,,,,,,,3.0.0,,,SQL,Tests,,,0,,,"The content of 
{code:java}
nested-comments.sql 
{code} show below:

{code:java}
-- This test case just used to test imported bracketed comments.

-- the first case of bracketed comment
--QUERY-DELIMITER-START
/* This is the first example of bracketed comment.
SELECT 'ommented out content' AS first;
*/
SELECT 'selected content' AS first;
--QUERY-DELIMITER-END
{code}
The test case 
{code:java}
comments.sql 
{code} imports 
{code:java}
nested-comments.sql
{code}
 below:

{code:java}
--IMPORT nested-comments.sql
{code}

The output will be:

{code:java}
-- !query
/* This is the first example of bracketed comment.
SELECT 'ommented out content' AS first
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException

mismatched input '/' expecting {'(', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 
'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', '
ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 0)

== SQL ==
/* This is the first example of bracketed comment.
^^^
SELECT 'ommented out content' AS first


-- !query
*/
SELECT 'selected content' AS first
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException

extraneous input '*/' expecting {'(', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', 'ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 0)

== SQL ==
*/
^^^
SELECT 'selected content' AS first
{code}

",,beliefer,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 23:10:55 UTC 2020,,,,,,,,,,"0|z0cx3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/20 23:10;maropu;Resolved by [https://github.com/apache/spark/pull/28018#];;;",,,,,,,,,,,,,,,,,,,,,,,
Avoid npe when reading bad csv input with `columnNameCorruptRecord` specified,SPARK-31261,13294068,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhenhuawang,zhenhuawang,zhenhuawang,26/Mar/20 02:42,12/Dec/22 18:11,13/Jul/23 08:46,29/Mar/20 04:31,2.4.5,3.0.0,3.1.0,,,,,,,,,,2.4.6,3.0.0,,SQL,,,,0,,,,,zhenhuawang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 29 04:31:18 UTC 2020,,,,,,,,,,"0|z0cx2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/20 04:31;gurwls223;Issue resolved by pull request 28029
[https://github.com/apache/spark/pull/28029];;;",,,,,,,,,,,,,,,,,,,,,,,
Dropna doesn't work for struct columns,SPARK-31256,13294039,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,msouder,msouder,25/Mar/20 22:56,20/Apr/20 05:01,13/Jul/23 08:46,20/Apr/20 05:01,2.4.5,,,,,,,,,,,,2.4.6,3.0.0,,PySpark,,,,0,,,"Dropna using a subset with a column from a struct drops the entire data frame.
{code:python}
import pyspark.sql.functions as F

df = spark.createDataFrame([(5, 80, 'Alice'), (10, None, 'Bob'), (15, 80, None)], schema=['age', 'height', 'name'])
df.show()
+---+------+-----+
|age|height| name|
+---+------+-----+
|  5|    80|Alice|
| 10|  null|  Bob|
| 15|    80| null|
+---+------+-----+

# this works just fine
df.dropna(subset=['name']).show()
+---+------+-----+
|age|height| name|
+---+------+-----+
|  5|    80|Alice|
| 10|  null|  Bob|
+---+------+-----+

# now add a struct column
df_with_struct = df.withColumn('struct_col', F.struct('age', 'height', 'name'))
df_with_struct.show(truncate=False)
+---+------+-----+--------------+
|age|height|name |struct_col    |
+---+------+-----+--------------+
|5  |80    |Alice|[5, 80, Alice]|
|10 |null  |Bob  |[10,, Bob]    |
|15 |80    |null |[15, 80,]     |
+---+------+-----+--------------+

# now dropna drops the whole dataframe when you use struct_col
df_with_struct.dropna(subset=['struct_col.name']).show(truncate=False)
+---+------+----+----------+
|age|height|name|struct_col|
+---+------+----+----------+
+---+------+----+----------+
{code}
 I've tested the above code in Spark 2.4.4 with python 3.7.4 and Spark 2.3.1 with python 3.6.8 and in both, the result looks like:
{code:python}
df_with_struct.dropna(subset=['struct_col.name']).show(truncate=False)
+---+------+-----+--------------+
|age|height|name |struct_col    |
+---+------+-----+--------------+
|5  |80    |Alice|[5, 80, Alice]|
|10 |null  |Bob  |[10,, Bob]    |
+---+------+-----+--------------+
{code}","Spark 2.4.5

Python 3.7.4",bryanc,imback82,JinxinTang,ksunitha,msouder,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 19 21:28:05 UTC 2020,,,,,,,,,,"0|z0cww8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/20 22:35;ksunitha;I can repro the issue using the Scala api on trunk. 

It looks like SPARK-30065 explicitly removed the support for nested column resolution in drop.    The change went into trunk and as well as the 2.4.5 branch.

E.g in the test:

https://github.com/apache/spark/blame/master/sql/core/src/test/scala/org/apache/spark/sql/DataFrameNaFunctionsSuite.scala#L302

cc [~cloud_fan], [~imback82]    This seems to be a regression. Is there a reason to remove this behavior?

 ;;;","14/Apr/20 22:47;imback82;Let me look into this.;;;","19/Apr/20 21:28;imback82;I created [https://github.com/apache/spark/pull/28266] to address this.;;;",,,,,,,,,,,,,,,,,,,,,
`HiveResult.toHiveString` does not use the current session time zone,SPARK-31254,13293927,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,25/Mar/20 20:21,26/Mar/20 09:51,13/Jul/23 08:46,26/Mar/20 09:51,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Currently, date/timestamp formatters in `HiveResult.toHiveString` are initialized once on instantiation of the `HiveResult` object, and pick up the session time zone. If the sessions time zone is changed, the formatters still use the previous one.

See the discussion there https://github.com/apache/spark/pull/23391#discussion_r397347820",,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 09:51:40 UTC 2020,,,,,,,,,,"0|z0cwrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/20 09:51;cloud_fan;Issue resolved by pull request 28024
[https://github.com/apache/spark/pull/28024];;;",,,,,,,,,,,,,,,,,,,,,,,
Use Minio instead of Ceph in K8S DepsTestsSuite,SPARK-31244,13293721,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,25/Mar/20 06:11,17/May/20 18:24,13/Jul/23 08:46,25/Mar/20 19:38,3.0.0,,,,,,,,,,,,3.0.0,,,Kubernetes,Spark Core,Tests,,0,,,"`DepsTestsSuite` is using `ceph` for S3 storage. However, it's not robust on `minikube` version. Also, the image size is almost 1GB. 
{code}
ceph/daemon                               v4.0.3-stable-4.0-nautilus-centos-7-x86_64   a6a05ccdf924        6 months ago        852MB
ceph/daemon                               v4.0.11-stable-4.0-nautilus-centos-7         87f695550d8e        12 hours ago        901MB
{code}

{code}
$ minikube version
minikube version: v1.8.2

$ minikube -p minikube docker-env | source

$ docker run -it --rm -e NETWORK_AUTO_DETECT=4 -e RGW_FRONTEND_PORT=8000 -e SREE_PORT=5001 -e CEPH_DEMO_UID=nano -e CEPH_DAEMON=demo ceph/daemon:v4.0.3-stable-4.0-nautilus-centos-7-x86_64 /bin/sh
2020-03-25 04:26:21  /opt/ceph-container/bin/entrypoint.sh: ERROR- it looks like we have not been able to discover the network settings

$ docker run -it --rm -e NETWORK_AUTO_DETECT=4 -e RGW_FRONTEND_PORT=8000 -e SREE_PORT=5001 -e CEPH_DEMO_UID=nano -e CEPH_DAEMON=demo ceph/daemon:v4.0.11-stable-4.0-nautilus-centos-7 /bin/sh
2020-03-25 04:20:30  /opt/ceph-container/bin/entrypoint.sh: ERROR- it looks like we have not been able to discover the network settings
{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 19:38:54 UTC 2020,,,,,,,,,,"0|z0cvi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/20 19:38;dongjoon;Issue resolved by pull request 28015
[https://github.com/apache/spark/pull/28015];;;",,,,,,,,,,,,,,,,,,,,,,,
ResetCommand should not wipe out all configs,SPARK-31234,13293536,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,Qin Yao,Qin Yao,Qin Yao,24/Mar/20 13:12,20/Apr/20 20:09,13/Jul/23 08:46,20/Apr/20 20:09,2.0.2,2.1.3,2.2.3,2.3.4,2.4.5,3.1.0,,,,,,,2.4.6,3.0.0,,SQL,,,,0,,,"Currently, ResetCommand clear all configurations, including sql configs, static sql configs and spark context level configs.
for example:
```
spark-sql> set xyz=abc;
xyz	abc
spark-sql> set;
spark.app.id	local-1585055396930
spark.app.name	SparkSQL::10.242.189.214
spark.driver.host	10.242.189.214
spark.driver.port	65094
spark.executor.id	driver
spark.jars
spark.master	local[*]
spark.sql.catalogImplementation	hive
spark.sql.hive.version	1.2.1
spark.submit.deployMode	client
xyz	abc
spark-sql> reset;
spark-sql> set;
spark-sql> set spark.sql.hive.version;
spark.sql.hive.version	1.2.1
spark-sql> set spark.app.id;
spark.app.id	<undefined>
```",,cloud_fan,Qin Yao,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 19 18:57:28 UTC 2020,,,,,,,,,,"0|z0cud4:",9223372036854775807,,,,,,,,,,,,,2.4.6,,,,,,,,,,"26/Mar/20 07:20;cloud_fan;Issue resolved by pull request 28003
[https://github.com/apache/spark/pull/28003];;;","19/Apr/20 18:57;smilegator;I reopened the ticket and target it to Spark 2.4.6.;;;",,,,,,,,,,,,,,,,,,,,,,
Support setuptools 46.1.0+ in PySpark packaging,SPARK-31231,13293465,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gurwls223,gurwls223,,24/Mar/20 07:35,12/Dec/22 18:10,13/Jul/23 08:46,03/Apr/20 23:10,2.4.5,3.0.0,3.1.0,,,,,,,,,,2.4.6,3.0.0,3.1.0,PySpark,,,,0,,,"PIP packaging test started to fail (see https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/120218/testReport/) as of  setuptools 46.1.0 release.

In https://github.com/pypa/setuptools/issues/1424, they decided to don't keep the modes in {{package_data}}. In PySpark pip installation, we keep the executable scripts in {{package_data}} https://github.com/apache/spark/blob/master/python/setup.py#L199-L200, and expose their symbolic links as executable scripts.

So, the symbolic links (or copied scripts) executes the scripts copied from {{package_data}}, which didn't keep the modes:

{code}
/tmp/tmp.UmkEGNFdKF/3.6/bin/spark-submit: line 27: /tmp/tmp.UmkEGNFdKF/3.6/lib/python3.6/site-packages/pyspark/bin/spark-class: Permission denied
/tmp/tmp.UmkEGNFdKF/3.6/bin/spark-submit: line 27: exec: /tmp/tmp.UmkEGNFdKF/3.6/lib/python3.6/site-packages/pyspark/bin/spark-class: cannot execute: Permission denied
{code}

The current issue is being tracked at https://github.com/pypa/setuptools/issues/2041
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 23:10:23 UTC 2020,,,,,,,,,,"0|z0ctwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/20 23:10;gurwls223;This was fixed in setuptools https://github.com/pypa/setuptools/pull/2046;;;",,,,,,,,,,,,,,,,,,,,,,,
Fix SizeBasedCoalesce in tests,SPARK-31226,13293252,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,23/Mar/20 09:07,11/Jul/20 21:48,13/Jul/23 08:46,11/Jul/20 21:48,2.4.0,3.0.0,,,,,,,,,,,3.1.0,,,SQL,Tests,,,0,,,"In spark UT, 

SizeBasedCoalecse's logic is wrong",,angerszhuuu,dongjoon,sathyaprakashg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 11 21:48:42 UTC 2020,,,,,,,,,,"0|z0cslk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/20 21:48;dongjoon;Issue resolved by pull request 27988
[https://github.com/apache/spark/pull/27988];;;",,,,,,,,,,,,,,,,,,,,,,,
Rebase all dates/timestamps in conversion in Java types,SPARK-31221,13293227,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,23/Mar/20 05:27,24/Mar/20 14:35,13/Jul/23 08:46,24/Mar/20 14:35,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Currently, the fromJavaDate(), toJavaDate(), toJavaTimestamp() and fromJavaTimestamp() methods of DateTimeUtils perform rebase only dates before Gregorian cutover date 1582-10-15 assuming that Gregorian calendar has the same behavior in Java 7 and Java 8 API. The assumption is incorrect, in particular, in getting zone offsets, for instance:
{code:scala}
scala> java.time.ZoneId.systemDefault
res16: java.time.ZoneId = America/Los_Angeles
scala> java.sql.Timestamp.valueOf(""1883-11-10 00:00:00"").getTimezoneOffset / 60.0
warning: there was one deprecation warning; re-run with -deprecation for details
res17: Double = 8.0
scala> java.time.ZoneId.of(""America/Los_Angeles"").getRules.getOffset(java.time.LocalDateTime.parse(""1883-11-10T00:00:00""))
res18: java.time.ZoneOffset = -07:52:58
{code}
Java 7 is not accurate, America/Los_Angeles changed time zone shift from
{code}
-7:52:58
{code}
to
{code}
-8:00 
{code}
The ticket aims to perform rebasing for any dates/timestamps independently from calendar cutover date.",,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 24 14:35:09 UTC 2020,,,,,,,,,,"0|z0csg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/20 14:35;cloud_fan;Issue resolved by pull request 27980
[https://github.com/apache/spark/pull/27980];;;",,,,,,,,,,,,,,,,,,,,,,,
repartition obeys spark.sql.adaptive.coalescePartitions.initialPartitionNum when spark.sql.adaptive.enabled,SPARK-31220,13293226,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,23/Mar/20 05:17,22/Jun/20 19:38,13/Jul/23 08:46,09/Jun/20 16:07,3.0.0,,,,,,,,,,,,3.0.1,3.1.0,,SQL,,,,1,,,"
{code:scala}
spark.sql(""CREATE TABLE spark_31220(id int)"")
spark.sql(""set spark.sql.adaptive.coalescePartitions.initialPartitionNum=1000"")
spark.sql(""set spark.sql.adaptive.enabled=true"")
{code}

{noformat}
scala> spark.sql(""SELECT id from spark_31220 GROUP BY id"").explain
== Physical Plan ==
AdaptiveSparkPlan(isFinalPlan=false)
+- HashAggregate(keys=[id#5], functions=[])
   +- Exchange hashpartitioning(id#5, 1000), true, [id=#171]
      +- HashAggregate(keys=[id#5], functions=[])
         +- FileScan parquet default.spark_31220[id#5] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/root/opensource/apache-spark/spark-warehouse/spark_31220], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int>



scala> spark.sql(""SELECT id from spark_31220 DISTRIBUTE BY id"").explain
== Physical Plan ==
AdaptiveSparkPlan(isFinalPlan=false)
+- Exchange hashpartitioning(id#5, 200), false, [id=#179]
   +- FileScan parquet default.spark_31220[id#5] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/root/opensource/apache-spark/spark-warehouse/spark_31220], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int>
{noformat}

",,cloud_fan,dongjoon,koert,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31841,,,,,,,,SPARK-32056,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 16:25:25 UTC 2020,,,,,,,,,,"0|z0csfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/20 16:07;cloud_fan;Issue resolved by pull request 27986
[https://github.com/apache/spark/pull/27986];;;","09/Jun/20 16:25;dongjoon;Hi, [~cloud_fan]. I updated the fixed version from 3.0.0 to (3.0.1 and 3.1.0) since 3.0.0 RC3 vote is still valid. Let's adjust this back to 3.0.0 if RC3 vote fails.;;;",,,,,,,,,,,,,,,,,,,,,,
YarnShuffleService doesn't close idle netty channel,SPARK-31219,13293219,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mauzhang,mauzhang,mauzhang,23/Mar/20 03:27,17/May/20 18:30,13/Jul/23 08:46,30/Mar/20 17:46,2.4.5,3.0.0,,,,,,,,,,,3.0.0,3.1.0,,Shuffle,Spark Core,,,0,,,"Recently, we find our YarnShuffleService has a lot of [half-open connections|https://blog.stephencleary.com/2009/05/detection-of-half-open-dropped.html] where shuffle servers' connections are active while clients have already closed. 

For example, from server's `ss -nt sport = :7337` output we have
{code:java}
ESTAB 0 0 server:7337 client:port

{code}
However, on client `ss -nt dport =: 7337 | grep server` would return nothing.

Looking at the code,  `YarnShuffleService` creates a `TransportContext` with `closeIdleConnections` set to false.
{code:java}
public class YarnShuffleService extends AuxiliaryService {
  ...
  @Override  protected void serviceInit(Configuration conf) throws Exception { 
    ...     
    transportContext = new TransportContext(transportConf, blockHandler); 
    ...
  }
  ...
}

public class TransportContext implements Closeable {
  ...

  public TransportContext(TransportConf conf, RpcHandler rpcHandler) {       
    this(conf, rpcHandler, false, false);  
  }
  public TransportContext(TransportConf conf, RpcHandler rpcHandler, boolean closeIdleConnections) {    
    this(conf, rpcHandler, closeIdleConnections, false);  
  }
  ...
}{code}
Hence, it's possible the channel  may never get closed at server side if the server misses the event that the client has closed it.

I find that parameter is true for `ExternalShuffleService`.

Is there any reason for the difference here ?  Can we enable closeIdleConnections in YarnShuffleService or at least add a configuration to enable it ?

 ",,mauzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-03-23 03:27:39.0,,,,,,,,,,"0|z0cse8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure on loading 1000-02-29 from parquet saved by Spark 2.4.5,SPARK-31211,13293084,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,21/Mar/20 10:13,25/Mar/20 02:00,13/Jul/23 08:46,23/Mar/20 06:47,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Save valid date in Julian calendar by Spark 2.4.5 in a leap year, for instance 1000-02-29:
{code}
$ export TZ=""America/Los_Angeles""
{code}
{code:scala}
scala> spark.conf.set(""spark.sql.session.timeZone"", ""America/Los_Angeles"")
scala> df.write.mode(""overwrite"").format(""avro"").save(""/Users/maxim/tmp/before_1582/2_4_5_date_avro_leap"")

scala> val df = Seq(java.sql.Date.valueOf(""1000-02-29"")).toDF(""dateS"").select($""dateS"".as(""date""))
df: org.apache.spark.sql.DataFrame = [date: date]

scala> df.show
+----------+
|      date|
+----------+
|1000-02-29|
+----------+

scala> df.write.mode(""overwrite"").parquet(""/Users/maxim/tmp/before_1582/2_4_5_date_leap"")
{code}

Load the parquet files back by Spark 3.1.0-SNAPSHOT:
{code:scala}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.0-SNAPSHOT
      /_/

Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_231)
Type in expressions to have them evaluated.
Type :help for more information.

scala> spark.read.parquet(""/Users/maxim/tmp/before_1582/2_4_5_date_leap"").show
+----------+
|      date|
+----------+
|1000-03-06|
+----------+


scala> spark.conf.set(""spark.sql.legacy.parquet.rebaseDateTime.enabled"", true)

scala> spark.read.parquet(""/Users/maxim/tmp/before_1582/2_4_5_date_leap"").show
20/03/21 03:03:59 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.time.DateTimeException: Invalid date 'February 29' as '1000' is not a leap year
	at java.time.LocalDate.create(LocalDate.java:429)
	at java.time.LocalDate.of(LocalDate.java:269)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.rebaseJulianToGregorianDays(DateTimeUtils.scala:1008)
{code}",,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 06:47:41 UTC 2020,,,,,,,,,,"0|z0crk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/20 06:47;cloud_fan;Issue resolved by pull request 27974
[https://github.com/apache/spark/pull/27974];;;",,,,,,,,,,,,,,,,,,,,,,,
toPandas fails on simple query (collect() works),SPARK-31186,13292620,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,michaelchirico,michaelchirico,19/Mar/20 05:02,12/Dec/22 18:11,13/Jul/23 08:46,27/Mar/20 03:12,2.4.4,,,,,,,,,,,,2.4.6,3.0.0,,PySpark,,,,0,,,"My pandas is 0.25.1.

I ran the following simple code (cross joins are enabled):

{code:python}
spark.sql('''
select t1.*, t2.* from (
  select explode(sequence(1, 3)) v
) t1 left join (
  select explode(sequence(1, 3)) v
) t2
''').toPandas()
{code}

and got a ValueError from pandas:

> ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().

Collect works fine:

{code:python}
spark.sql('''
select * from (
  select explode(sequence(1, 3)) v
) t1 left join (
  select explode(sequence(1, 3)) v
) t2
''').collect()
# [Row(v=1, v=1),
#  Row(v=1, v=2),
#  Row(v=1, v=3),
#  Row(v=2, v=1),
#  Row(v=2, v=2),
#  Row(v=2, v=3),
#  Row(v=3, v=1),
#  Row(v=3, v=2),
#  Row(v=3, v=3)]
{code}

I imagine it's related to the duplicate column names, but this doesn't fail:

{code:python}
spark.sql(""select 1 v, 1 v"").toPandas()
# v	v
# 0	1	1
{code}

Also no issue for multiple rows:

spark.sql(""select 1 v, 1 v union all select 1 v, 2 v"").toPandas()

It also works when not using a cross join but a janky programatically-generated union all query:

{code:python}
cond = []
for ii in range(3):
    for jj in range(3):
        cond.append(f'select {ii+1} v, {jj+1} v')
spark.sql(' union all '.join(cond)).toPandas()
{code}

As near as I can tell, the output is identical to the explode output, making this issue all the more peculiar, as I thought toPandas() is applied to the output of collect(), so if collect() gives the same output, how can toPandas() fail in one case and not the other? Further, the lazy DataFrame is the same: DataFrame[v: int, v: int] in both cases. I must be missing something.",,bryanc,michaelchirico,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 27 03:12:47 UTC 2020,,,,,,,,,,"0|z0copc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/20 03:12;gurwls223;Issue resolved by pull request 28025
[https://github.com/apache/spark/pull/28025];;;",,,,,,,,,,,,,,,,,,,,,,,
"sql(""INSERT INTO v2DataSource ..."").collect() double inserts",SPARK-31178,13292312,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,brkyvz,brkyvz,brkyvz,18/Mar/20 01:21,19/Mar/20 01:12,13/Jul/23 08:46,19/Mar/20 01:12,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"The following unit test fails in DataSourceV2SQLSuite:
{code:java}
test(""do not double insert on INSERT INTO collect()"") {
  import testImplicits._
  val t1 = s""${catalogAndNamespace}tbl""
  sql(s""CREATE TABLE $t1 (id bigint, data string) USING $v2Format"")
  val tmpView = ""test_data""
  val df = Seq((1L, ""a""), (2L, ""b""), (3L, ""c"")).toDF(""id"", ""data"")
  df.createOrReplaceTempView(tmpView)
  sql(s""INSERT INTO TABLE $t1 SELECT * FROM $tmpView"").collect()

  verifyTable(t1, df)
} {code}
The INSERT INTO is double inserting when "".collect()"" is called. I think this is because the V2 SparkPlans are not commands, and doExecute on a Spark plan can be called multiple times causing data to be inserted multiple times.",,brkyvz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 19 01:12:33 UTC 2020,,,,,,,,,,"0|z0cmsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/20 01:22;brkyvz;cc [~wenchen] [~rdblue];;;","19/Mar/20 01:12;brkyvz;Resolved by [https://github.com/apache/spark/pull/27941];;;",,,,,,,,,,,,,,,,,,,,,,
Spark Cli does not respect hive-site.xml and spark.sql.warehouse.dir,SPARK-31170,13292104,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,17/Mar/20 03:45,11/Dec/20 00:31,13/Jul/23 08:46,27/Mar/20 04:26,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"In Spark CLI, we create a hive CliSessionState and it does not load the hive-site.xml. So the configurations in hive-site.xml will not take effects like other spark-hive integration apps.

Also, the warehouse directory is not correctly picked. If the `default` database does not exist, the CliSessionState will create one during the first time it talks to the metastore. The `Location` of the default DB will be neither the value of spark.sql.warehousr.dir nor the user-specified value of hive.metastore.warehourse.dir, but the default value of hive.metastore.warehourse.dir which will always be `/user/hive/warehouse`.",,cloud_fan,dongjoon,nchammas,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33740,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 22 04:29:04 UTC 2020,,,,,,,,,,"0|z0clio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/20 15:30;cloud_fan;Issue resolved by pull request 27933
[https://github.com/apache/spark/pull/27933];;;","19/Mar/20 23:19;dongjoon;This is reverted because this broke all `hive-1.2` profile Jenkins jobs (2 SBT/2 Maven).;;;","27/Mar/20 04:26;cloud_fan;Issue resolved by pull request 27969
[https://github.com/apache/spark/pull/27969];;;","21/Apr/20 19:19;nchammas;Isn't this also an issue in Spark 2.4.5?
{code:java}
$ spark-sql
...
20/04/21 15:12:26 INFO metastore: Mestastore configuration hive.metastore.warehouse.dir changed from /user/hive/warehouse to file:/Users/myusername/spark-warehouse
...
spark-sql> create table test(a int);
...
20/04/21 15:12:48 WARN HiveMetaStore: Location: file:/user/hive/warehouse/test specified for non-external table:test
20/04/21 15:12:48 INFO FileUtils: Creating directory if it doesn't exist: file:/user/hive/warehouse/test
Error in query: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:file:/user/hive/warehouse/test is not a directory or unable to create one);{code}
If I understood the problem correctly, then the fix should perhaps also be backported to 2.4.x.;;;","22/Apr/20 04:29;cloud_fan;Yea it's a long-standing bug, but the fix is non-trivial, seems a bit risky for 2.4. cc [~dongjoon] [~holdenkarau];;;",,,,,,,,,,,,,,,,,,,
"UNION map<null, null> and other maps should not fail",SPARK-31166,13292004,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,16/Mar/20 15:35,23/Mar/20 17:51,13/Jul/23 08:46,17/Mar/20 04:36,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31229,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 17 04:36:05 UTC 2020,,,,,,,,,,"0|z0ckwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/20 04:36;cloud_fan;Issue resolved by pull request 27926
[https://github.com/apache/spark/pull/27926];;;",,,,,,,,,,,,,,,,,,,,,,,
Inconsistent rdd and output partitioning for bucket table when output doesn't contain all bucket columns,SPARK-31164,13291928,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhenhuawang,zhenhuawang,zhenhuawang,16/Mar/20 09:35,17/Mar/20 19:12,13/Jul/23 08:46,17/Mar/20 12:20,2.2.3,2.3.4,2.4.5,3.0.0,,,,,,,,,2.4.6,3.0.0,,SQL,,,,0,,,"For a bucketed table, when deciding output partitioning, if the output doesn't contain all bucket columns, the result is `UnknownPartitioning`. But when generating rdd, current Spark uses `createBucketedReadRDD` because it doesn't check if the output contains all bucket columns. So the rdd and its output partitioning are inconsistent.",,dongjoon,zhenhuawang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 17 06:39:37 UTC 2020,,,,,,,,,,"0|z0ckfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/20 19:06;dongjoon;Hi, [~zhenhuawang]. Could you check the older releases like 2.3.4 or 2.2.x together in `Affected Versions`?;;;","17/Mar/20 06:39;zhenhuawang;Thanks [~dongjoon], I added 2.3.4 and 2.2.3;;;",,,,,,,,,,,,,,,,,,,,,,
acl/permission should handle non-existed path when truncating table,SPARK-31163,13291883,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,16/Mar/20 06:14,17/Mar/20 01:47,13/Jul/23 08:46,16/Mar/20 18:46,3.0.0,,,,,,,,,,,,2.4.6,3.0.0,,SQL,,,,0,,,TruncateTableCommand with acl/permission should handle non-existed path in case of path is missing.,,dongjoon,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30312,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 16 18:46:23 UTC 2020,,,,,,,,,,"0|z0ck5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/20 18:46;dongjoon;Issue resolved by pull request 27923
[https://github.com/apache/spark/pull/27923];;;",,,,,,,,,,,,,,,,,,,,,,,
Remove pydocstyle tests,SPARK-31155,13291695,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nchammas,nchammas,nchammas,14/Mar/20 08:22,12/Dec/22 18:10,13/Jul/23 08:46,17/Mar/20 01:42,3.0.0,,,,,,,,,,,,3.0.0,,,Build,Documentation,,,0,,,"pydocstyle tests have been running neither on Jenkins nor on Github.

We also seem to be in a [bad place|https://github.com/apache/spark/pull/27912#issuecomment-599167117] to re-enable them.",,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 17 01:42:26 UTC 2020,,,,,,,,,,"0|z0cizs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/20 01:42;gurwls223;Issue resolved by pull request 27912
[https://github.com/apache/spark/pull/27912];;;",,,,,,,,,,,,,,,,,,,,,,,
Cleanup several failures in lint-python,SPARK-31153,13291681,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nchammas,nchammas,nchammas,14/Mar/20 06:21,12/Dec/22 18:10,13/Jul/23 08:46,16/Mar/20 01:05,3.0.0,,,,,,,,,,,,3.0.0,,,Build,PySpark,,,0,,,"Don't understand how this script runs fine on the build server. Perhaps we've just been getting lucky?

Will detail the issues on the PR.",,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 16 01:05:14 UTC 2020,,,,,,,,,,"0|z0ciwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/20 01:05;gurwls223;Issue resolved by pull request 27917
[https://github.com/apache/spark/pull/27917];;;",,,,,,,,,,,,,,,,,,,,,,,
Leverage the helper method for aliasing in all SQL expressions,SPARK-31146,13291508,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,13/Mar/20 08:19,12/Dec/22 17:50,13/Jul/23 08:46,16/Mar/20 18:23,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,SPARK-30184 added the helper function for aliases. We should leverage and update all SQL functions.,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 16 18:23:27 UTC 2020,,,,,,,,,,"0|z0chu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/20 18:23;dongjoon;Issue resolved by pull request 27901
[https://github.com/apache/spark/pull/27901];;;",,,,,,,,,,,,,,,,,,,,,,,
Use the same version of `commons-io` in SBT,SPARK-31130,13291236,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,12/Mar/20 07:13,12/Mar/20 16:09,13/Jul/23 08:46,12/Mar/20 16:09,2.4.5,3.0.0,,,,,,,,,,,2.4.6,3.0.0,,Build,,,,0,,,"Maven always uses 2.4 version while SBT uses 2.5 at `hadoop-3.2` profile.
{code}
<commons-io.version>2.4</commons-io.version>
{code}

**branch-3.0**
{code}
$ build/sbt -Phadoop-3.2 ""core/dependencyTree"" | grep commons-io:commons-io | head -n1
    [info]   | | +-commons-io:commons-io:2.5
{code}

**branch-2.4**
{code}
$ build/sbt -Phadoop-3.1 ""core/dependencyTree"" | grep commons-io:commons-io | head -n1
[info]   | | +-commons-io:commons-io:2.5
{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-15261,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 16:09:04 UTC 2020,,,,,,,,,,"0|z0cg5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/20 16:09;dongjoon;Issue resolved by pull request 27886
[https://github.com/apache/spark/pull/27886];;;",,,,,,,,,,,,,,,,,,,,,,,
IntervalBenchmark and DateTimeBenchmark fails to run,SPARK-31129,13291234,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,12/Mar/20 06:53,12/Mar/20 20:00,13/Jul/23 08:46,12/Mar/20 20:00,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,Tests,,,,0,,,"[error] Caused by: java.time.format.DateTimeParseException: Text '2019-01-27 11:02:01.0' could not be parsed at index 20
[error] 	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2046)
[error] 	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)
[error] 	at org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:71)
[error] 	... 19 more",,dongjoon,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 20:00:12 UTC 2020,,,,,,,,,,"0|z0cg5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/20 20:00;dongjoon;Issue resolved by pull request 27885
[https://github.com/apache/spark/pull/27885];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix Uncaught TypeError in streaming statistics page,SPARK-31128,13291208,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,12/Mar/20 01:51,13/Mar/20 03:02,13/Jul/23 08:46,13/Mar/20 03:02,3.0.0,,,,,,,,,,,,3.0.0,,,Web UI,,,,0,,,"There is a minor issue in https://github.com/apache/spark/pull/26201
In the streaming statistics page, there is such error 
```
streaming-page.js:211 Uncaught TypeError: Cannot read property 'top' of undefined
    at SVGCircleElement.<anonymous> (streaming-page.js:211)
    at SVGCircleElement.__onclick (d3.min.js:1)
```
in the console after clicking the timeline graph.
We should fix it.",,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 13 03:02:44 UTC 2020,,,,,,,,,,"0|z0cfzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/20 03:02;Gengliang.Wang;The issue is resolved in https://github.com/apache/spark/pull/27883;;;",,,,,,,,,,,,,,,,,,,,,,,
Upgrade Kafka to 2.4.1,SPARK-31126,13291192,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,11/Mar/20 23:20,12/Mar/20 02:26,13/Jul/23 08:46,12/Mar/20 02:26,3.0.0,,,,,,,,,,,,3.0.0,,,Structured Streaming,,,,0,,,,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-8933,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 02:26:50 UTC 2020,,,,,,,,,,"0|z0cfw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/20 02:26;dongjoon;Issue resolved by pull request 27881
[https://github.com/apache/spark/pull/27881];;;",,,,,,,,,,,,,,,,,,,,,,,
Drop does not work after join with aliases,SPARK-31123,13291083,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,mikelsanvi,mikelsanvi,11/Mar/20 13:10,06/Jun/23 14:08,13/Jul/23 08:46,17/Mar/20 19:34,2.4.2,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,," 

Hi,

I am seeing a really strange behaviour in drop method after a join with aliases. It doesn't seem to find the column when I reference to it using dataframe(""columnName"") syntax, but it does work with other combinators like select
{code:java}
case class Record(a: String, dup: String)
case class Record2(b: String, dup: String)

val df = Seq(Record(""a"", ""dup"")).toDF
val df2 = Seq(Record2(""a"", ""dup"")).toDF 
val joined = df.alias(""a"").join(df2.alias(""b""), df(""a"") === df2(""b""))
val dupCol = df(""dup"")
joined.drop(dupCol) // Does not drop anything
joined.drop(func.col(""a.dup"")) // It drops the column  
joined.select(dupCol) // It selects the column
{code}
 

 

 ",,imback82,mikelsanvi,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-43439,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 16 17:16:03 UTC 2020,,,,,,,,,,"0|z0cf7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/20 01:57;viirya;I tested with current master branch. Looks it is resolved:
{code:java}
scala> joined.show
+---+---+---+---+
|  a|dup|  b|dup|
+---+---+---+---+
|  a|dup|  a|dup|
+---+---+---+---+

scala> joined.drop(dupCol).show
+---+---+---+
|  a|  b|dup|
+---+---+---+
|  a|  a|dup|
+---+---+---+


{code};;;","16/Mar/20 09:09;mikelsanvi;Hi L.C.,

did you perform the join using aliases? otherwise the issue won't happen;;;","16/Mar/20 17:16;viirya;I copied your example code in the ticket description.;;;",,,,,,,,,,,,,,,,,,,,,
"Support DDL ""SHOW VIEWS""",SPARK-31113,13290977,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,EricWu,smilegator,smilegator,11/Mar/20 04:03,03/May/20 11:24,13/Jul/23 08:46,07/Apr/20 16:25,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"It is nice to have a `SHOW VIEWS` command similar to Hive ([https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ShowViews]).

Although this is adding a new feature, this resolved the regression bug in the below.

According to the report, [https://github.com/apache/spark/pull/27897#issuecomment-602519201],
{quote}Tableau and PowerBI, which use the Simba ODBC driver, do not work with Spark 3.0 with Hive 2.3.

When we switch to Hive 2.3 profile, Simba driver assumes that {{SHOW VIEWS}} is supported, and issues it, failing the {{SQLGetTables}} call when it fails.
{quote}
 ",,dongjoon,EricWu,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31033,,,,,,,,,,SPARK-23710,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 07 16:25:53 UTC 2020,,,,,,,,,,"0|z0cek8:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"11/Mar/20 04:03;smilegator;cc [~EricWu] Could you try this?;;;","11/Mar/20 05:55;EricWu;Sure, I'm working on this! Thanks [~smilegator];;;","07/Apr/20 16:25;dongjoon;Issue resolved by pull request 27897
[https://github.com/apache/spark/pull/27897];;;",,,,,,,,,,,,,,,,,,,,,
Fix interval output issue in ExtractBenchmark ,SPARK-31111,13290846,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,10/Mar/20 15:55,12/Mar/20 06:32,13/Jul/23 08:46,11/Mar/20 12:24,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,Tests,,,0,,,,,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 11 12:24:09 UTC 2020,,,,,,,,,,"0|z0cdr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/20 12:24;cloud_fan;Issue resolved by pull request 27867
[https://github.com/apache/spark/pull/27867];;;",,,,,,,,,,,,,,,,,,,,,,,
spark-sql fails to parse when contains comment,SPARK-31102,13290704,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,javier_ivanov,yumwang,yumwang,10/Mar/20 03:07,19/May/20 02:45,13/Jul/23 08:46,19/May/20 02:45,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"{code:sql}
select
  1,
  -- two
  2;
{code}


{noformat}
spark-sql> select
         >   1,
         >   -- two
         >   2;
Error in query:
mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DATA', 'DATABASE', DATABASES, 'DAY', 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SCHEMA', 'SECOND', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', '+', '-', '*', 'DIV', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 3, pos 2)

== SQL ==
select
  1,

--^^^
{noformat}

",,apachespark,javier_ivanov,maropu,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30049,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 02:45:06 UTC 2020,,,,,,,,,,"0|z0ccvk:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"10/Mar/20 03:09;yumwang;cc [~javier_ivanov];;;","10/Mar/20 22:16;javier_ivanov;Hey [~yumwang] I am checking this. Thanks!;;;","16/Mar/20 01:53;javier_ivanov;[~yumwang] I have checked 2 things:
 # The insideComment flag doesn't change to false with a newline.
 # Some tests had line continuation using ""\"" this is not possible in Spark nor hive.

I will raise a PR for this.;;;","16/Mar/20 02:02;maropu;Looks nice , thanks for the work.;;;","18/May/20 03:53;apachespark;User 'javierivanov' has created a pull request for this issue:
https://github.com/apache/spark/pull/28565;;;","18/May/20 03:53;apachespark;User 'javierivanov' has created a pull request for this issue:
https://github.com/apache/spark/pull/28565;;;","19/May/20 02:45;maropu;Resolved by [https://github.com/apache/spark/pull/27920];;;",,,,,,,,,,,,,,,,,
Upgrade Janino to 3.0.16,SPARK-31101,13290680,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,10/Mar/20 00:48,20/Aug/20 20:28,13/Jul/23 08:46,22/Mar/20 02:10,3.0.0,,,,,,,,,,,,2.4.6,3.0.0,,Build,,,,0,,,"We got some report on failure on user's query which Janino throws error on compiling generated code. The issue is here: janino-compiler/janino#113 It contains the information of generated code, symptom (error), and analysis of the bug, so please refer the link for more details.
Janino 3.0.16 contains the PR janino-compiler/janino#114 which would enable Janino to succeed to compile user's query properly.",,dongjoon,kabhwan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 22 02:10:55 UTC 2020,,,,,,,,,,"0|z0ccq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/20 02:10;dongjoon;Issue resolved by pull request 27932
[https://github.com/apache/spark/pull/27932];;;",,,,,,,,,,,,,,,,,,,,,,,
Expired SSL cert on Spark 2.4.5 download,SPARK-31097,13290618,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shaneknapp,heuermh,heuermh,09/Mar/20 18:43,09/Mar/20 21:48,13/Jul/23 08:46,09/Mar/20 21:35,2.4.5,,,,,,,,,,,,,,,Project Infra,,,,0,,,"{code:java}
+ curl -L 'https://www.apache.org/dyn/mirrors/mirrors.cgi?action=download&filename=spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz' -o spark-2.4.5-bin-hadoop2.7.tgz
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
curl: (60) SSL certificate problem: certificate has expired
More details here: https://curl.haxx.se/docs/sslcerts.html
{code}
https://amplab.cs.berkeley.edu/jenkins/job/ADAM-prb/3091/HADOOP_VERSION=2.7.5,SCALAVER=2.11,SPARK_VERSION=2.4.5,label=ubuntu/console",,heuermh,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 09 21:48:07 UTC 2020,,,,,,,,,,"0|z0cccg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/20 20:07;shaneknapp;odd.  i replicated the environment that jenkins uses when executing curl and was unable to reproduce it...  it should be using the anaconda-provided certs:

 
{noformat}
* successfully set certificate verify locations:
*   CAfile: /home/jenkins/anaconda2/ssl/cacert.pem
  CApath: none{noformat}
 

could you do me a favor and add a -v to each curl call in scripts/jenkins-test and push to that existing PR?  i'll limit the nodes that job is capable of building on to amp-jenkins-staging-worker-02.;;;","09/Mar/20 20:09;heuermh;Will do, thank you for your help, [~shaneknapp] 

 ;;;","09/Mar/20 20:50;shaneknapp;same worker, same curl command, different build:

[https://amplab.cs.berkeley.edu/jenkins/job/ADAM-prb/HADOOP_VERSION=2.7.5,SCALAVER=2.11,SPARK_VERSION=2.4.5,label=ubuntu/3092/consoleFull]

 

and it worked just fine!  ¯\_(ツ)_/¯

after this job finishes, we'll see what happens to the next job that's pinned to the impacted worker.;;;","09/Mar/20 21:35;shaneknapp;i am unable to reproduce this failure, and curl's verbose output from the latest build (https://amplab.cs.berkeley.edu/jenkins/job/ADAM-prb/HADOOP_VERSION=2.7.5,SCALAVER=2.12,SPARK_VERSION=2.4.5,label=amp-jenkins-staging-worker-02/3094/console) show that it's using the proper and valid cert.;;;","09/Mar/20 21:36;heuermh;Please close as CannotReproduce.

 ;;;","09/Mar/20 21:37;heuermh;Thank you, [~shaneknapp]!;;;","09/Mar/20 21:42;shaneknapp;it seems that i am unable to change the close reason, or reopen the ticket.  google searches are telling my to edit the ticket workflow, but that's a path i am unwilling to venture down.;;;","09/Mar/20 21:48;heuermh;Ah, don't worry about it. My comment was only meant to come before yours ;);;;",,,,,,,,,,,,,,,,
Upgrade netty-all to 4.1.47.Final,SPARK-31095,13290548,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,visvijay,visvijay,09/Mar/20 14:38,28/Jun/20 07:48,13/Jul/23 08:46,11/Mar/20 00:51,2.4.5,3.0.0,3.1.0,,,,,,,,,,2.4.6,3.0.0,,Build,,,,0,security,,"Upgrade version of io.netty_netty-all to 4.1.44.Final [CVE-2019-20445|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20445]",,dongjoon,ouyangxc.zte,visvijay,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 28 07:48:36 UTC 2020,,,,,,,,,,"0|z0cbww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/20 21:55;dongjoon;Hi, [~visvijay]. You should not set `Fix Version`. Please see the contribution guide.
- https://spark.apache.org/contributing.html;;;","11/Mar/20 00:51;dongjoon;Issue resolved by pull request 27869
[https://github.com/apache/spark/pull/27869];;;","11/Mar/20 00:56;dongjoon;For `branch-2.4`, https://github.com/apache/spark/pull/27870 is created.;;;","28/Jun/20 07:48;ouyangxc.zte;Hello  [~dongjoon], Can netty-all upgrade solve CVE-2020-9480 security vulnerability metioned on the Spark official website? Thanks!;;;",,,,,,,,,,,,,,,,,,,,
MapOutputTrackerMaster.getMapLocation can't handle last mapIndex,SPARK-31082,13290440,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,09/Mar/20 02:38,09/Mar/20 11:30,13/Jul/23 08:46,09/Mar/20 11:30,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"Currently, for the last mapIndex, MapOutputTrackerMaster.getMapLocation always returns empty locations.",,cloud_fan,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 09 11:30:37 UTC 2020,,,,,,,,,,"0|z0cb8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/20 11:30;cloud_fan;Issue resolved by pull request 27850
[https://github.com/apache/spark/pull/27850];;;",,,,,,,,,,,,,,,,,,,,,,,
IllegalArgumentException in BroadcastExchangeExec,SPARK-31068,13289974,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,06/Mar/20 07:30,16/Mar/20 01:21,13/Jul/23 08:46,16/Mar/20 01:21,3.0.0,,,,,,,,,,,,3.1.0,,,SQL,,,,0,,,"{code}
Caused by: org.apache.spark.SparkException: Failed to materialize query stage: BroadcastQueryStage 0
+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true], input[1, bigint, true], input[2, int, true]))
   +- *(1) Project [guid#138126, session_skey#138127L, seqnum#138132]
      +- *(1) Filter ((((isnotnull(session_start_dt#138129) && (session_start_dt#138129 = 2020-01-01)) && isnotnull(seqnum#138132)) && isnotnull(session_skey#138127L)) && isnotnull(guid#138126))
         +- *(1) FileScan parquet p_soj_cl_t.clav_events[guid#138126, session_skey#138127L, session_start_dt#138129, seqnum#138132] DataFilters: [isnotnull(session_start_dt#138129), (session_start_dt#138129 = 2020-01-01), isnotnull(seqnum#138..., Format: Parquet, Location: TahoeLogFileIndex[hdfs://hermes-rno/workspaces/P_SOJ_CL_T/clav_events], PartitionFilters: [], PushedFilters: [IsNotNull(session_start_dt), EqualTo(session_start_dt,2020-01-01), IsNotNull(seqnum), IsNotNull(..., ReadSchema: struct<guid:string,session_skey:bigint,session_start_dt:string,seqnum:int>, SelectedBucketsCount: 1000 out of 1000, UsedIndexes: []

	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$anonfun$generateFinalPlan$3.apply(AdaptiveSparkPlanExec.scala:230)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$anonfun$generateFinalPlan$3.apply(AdaptiveSparkPlanExec.scala:225)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.generateFinalPlan(AdaptiveSparkPlanExec.scala:225)
	... 48 more
Caused by: java.lang.IllegalArgumentException: Initial capacity 670166426 exceeds maximum capacity of 536870912
at org.apache.spark.unsafe.map.BytesToBytesMap.<init>(BytesToBytesMap.java:196)
	at org.apache.spark.unsafe.map.BytesToBytesMap.<init>(BytesToBytesMap.java:219)
	at org.apache.spark.sql.execution.joins.UnsafeHashedRelation$.apply(HashedRelation.scala:340)
	at org.apache.spark.sql.execution.joins.HashedRelation$.apply(HashedRelation.scala:123)
	at org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:964)
	at org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:952)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$9.apply(BroadcastExchangeExec.scala:220)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$9.apply(BroadcastExchangeExec.scala:207)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:128)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:206)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:172)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	... 3 more
{code}",,cltlfcjin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 16 01:21:19 UTC 2020,,,,,,,,,,"0|z0c8dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/20 01:21;srowen;Issue resolved by pull request 27828
[https://github.com/apache/spark/pull/27828];;;",,,,,,,,,,,,,,,,,,,,,,,
Disable useless and uncleaned hive SessionState initialization parts,SPARK-31066,13289962,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,06/Mar/20 06:33,18/Mar/20 02:16,13/Jul/23 08:46,12/Mar/20 11:01,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"As a common usage and according to the spark doc, users may often just copy their hive-site.xml to Spark directly from hive projects. Sometimes, the config file is not that clean for spark and may cause some side effects

for example, hive.session.history.enabled will create a log for the hive jobs but useless for spark and also it will not be deleted on JVM exit.",,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 11:01:11 UTC 2020,,,,,,,,,,"0|z0c8ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/20 11:01;cloud_fan;Issue resolved by pull request 27827
[https://github.com/apache/spark/pull/27827];;;",,,,,,,,,,,,,,,,,,,,,,,
Empty string values cause schema_of_json() to return a schema not usable by from_json(),SPARK-31065,13289960,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,nchammas,nchammas,06/Mar/20 05:45,12/Dec/22 18:11,13/Jul/23 08:46,10/Mar/20 07:42,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,2.4.5,3.0.0,,,,,,3.0.0,,,SQL,,,,0,,,"Here's a reproduction:
  
{code:python}
from pyspark.sql.functions import from_json, schema_of_json
json = '{""a"": """"}'

df = spark.createDataFrame([(json,)], schema=['json'])
df.show()

# chokes with org.apache.spark.sql.catalyst.parser.ParseException
json_schema = schema_of_json(json)
df.select(from_json('json', json_schema))

# works fine
json_schema = spark.read.json(df.rdd.map(lambda x: x[0])).schema
df.select(from_json('json', json_schema))
{code}
The output:
{code:java}
>>> from pyspark.sql.functions import from_json, schema_of_json
>>> json = '{""a"": """"}'
>>> 
>>> df = spark.createDataFrame([(json,)], schema=['json'])
>>> df.show()
+---------+
|     json|
+---------+
|{""a"": """"}|
+---------+

>>> 
>>> # chokes with org.apache.spark.sql.catalyst.parser.ParseException
>>> json_schema = schema_of_json(json)
>>> df.select(from_json('json', json_schema))
Traceback (most recent call last):
  File "".../site-packages/pyspark/sql/utils.py"", line 63, in deco
    return f(*a, **kw)
  File "".../site-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py"", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.sql.functions.from_json.
: org.apache.spark.sql.catalyst.parser.ParseException: 
extraneous input '<' expecting {'SELECT', 'FROM', 'ADD', 'AS', 'ALL', 'ANY', 'DISTINCT', 'WHERE', 'GROUP', 'BY', 'GROUPING', 'SETS', 'CUBE', 'ROLLUP', 'ORDER', 'HAVING', 'LIMIT', 'AT', 'OR', 'AND', 'IN', NOT, 'NO', 'EXISTS', 'BETWEEN', 'LIKE', RLIKE, 'IS', 'NULL', 'TRUE', 'FALSE', 'NULLS', 'ASC', 'DESC', 'FOR', 'INTERVAL', 'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'JOIN', 'CROSS', 'OUTER', 'INNER', 'LEFT', 'SEMI', 'RIGHT', 'FULL', 'NATURAL', 'ON', 'PIVOT', 'LATERAL', 'WINDOW', 'OVER', 'PARTITION', 'RANGE', 'ROWS', 'UNBOUNDED', 'PRECEDING', 'FOLLOWING', 'CURRENT', 'FIRST', 'AFTER', 'LAST', 'ROW', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'DIRECTORY', 'VIEW', 'REPLACE', 'INSERT', 'DELETE', 'INTO', 'DESCRIBE', 'EXPLAIN', 'FORMAT', 'LOGICAL', 'CODEGEN', 'COST', 'CAST', 'SHOW', 'TABLES', 'COLUMNS', 'COLUMN', 'USE', 'PARTITIONS', 'FUNCTIONS', 'DROP', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'TO', 'TABLESAMPLE', 'STRATIFY', 'ALTER', 'RENAME', 'ARRAY', 'MAP', 'STRUCT', 'COMMENT', 'SET', 'RESET', 'DATA', 'START', 'TRANSACTION', 'COMMIT', 'ROLLBACK', 'MACRO', 'IGNORE', 'BOTH', 'LEADING', 'TRAILING', 'IF', 'POSITION', 'EXTRACT', 'DIV', 'PERCENT', 'BUCKET', 'OUT', 'OF', 'SORT', 'CLUSTER', 'DISTRIBUTE', 'OVERWRITE', 'TRANSFORM', 'REDUCE', 'SERDE', 'SERDEPROPERTIES', 'RECORDREADER', 'RECORDWRITER', 'DELIMITED', 'FIELDS', 'TERMINATED', 'COLLECTION', 'ITEMS', 'KEYS', 'ESCAPED', 'LINES', 'SEPARATED', 'FUNCTION', 'EXTENDED', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'LAZY', 'FORMATTED', 'GLOBAL', TEMPORARY, 'OPTIONS', 'UNSET', 'TBLPROPERTIES', 'DBPROPERTIES', 'BUCKETS', 'SKEWED', 'STORED', 'DIRECTORIES', 'LOCATION', 'EXCHANGE', 'ARCHIVE', 'UNARCHIVE', 'FILEFORMAT', 'TOUCH', 'COMPACT', 'CONCATENATE', 'CHANGE', 'CASCADE', 'RESTRICT', 'CLUSTERED', 'SORTED', 'PURGE', 'INPUTFORMAT', 'OUTPUTFORMAT', DATABASE, DATABASES, 'DFS', 'TRUNCATE', 'ANALYZE', 'COMPUTE', 'LIST', 'STATISTICS', 'PARTITIONED', 'EXTERNAL', 'DEFINED', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'REPAIR', 'RECOVER', 'EXPORT', 'IMPORT', 'LOAD', 'ROLE', 'ROLES', 'COMPACTIONS', 'PRINCIPALS', 'TRANSACTIONS', 'INDEX', 'INDEXES', 'LOCKS', 'OPTION', 'ANTI', 'LOCAL', 'INPATH', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 6)

== SQL ==
struct<a:null>
------^^^

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableSchema(ParseDriver.scala:64)
	at org.apache.spark.sql.types.DataType$.fromDDL(DataType.scala:123)
	at org.apache.spark.sql.catalyst.expressions.JsonExprUtils$.evalSchemaExpr(jsonExpressions.scala:777)
	at org.apache.spark.sql.catalyst.expressions.JsonToStructs.<init>(jsonExpressions.scala:527)
	at org.apache.spark.sql.functions$.from_json(functions.scala:3606)
	at org.apache.spark.sql.functions.from_json(functions.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../site-packages/pyspark/sql/functions.py"", line 2284, in from_json
    jc = sc._jvm.functions.from_json(_to_java_column(col), schema, options)
  File "".../site-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__
  File "".../site-packages/pyspark/sql/utils.py"", line 73, in deco
    raise ParseException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.ParseException: ""\nextraneous input '<' expecting {'SELECT', 'FROM', 'ADD', 'AS', 'ALL', 'ANY', 'DISTINCT', 'WHERE', 'GROUP', 'BY', 'GROUPING', 'SETS', 'CUBE', 'ROLLUP', 'ORDER', 'HAVING', 'LIMIT', 'AT', 'OR', 'AND', 'IN', NOT, 'NO', 'EXISTS', 'BETWEEN', 'LIKE', RLIKE, 'IS', 'NULL', 'TRUE', 'FALSE', 'NULLS', 'ASC', 'DESC', 'FOR', 'INTERVAL', 'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'JOIN', 'CROSS', 'OUTER', 'INNER', 'LEFT', 'SEMI', 'RIGHT', 'FULL', 'NATURAL', 'ON', 'PIVOT', 'LATERAL', 'WINDOW', 'OVER', 'PARTITION', 'RANGE', 'ROWS', 'UNBOUNDED', 'PRECEDING', 'FOLLOWING', 'CURRENT', 'FIRST', 'AFTER', 'LAST', 'ROW', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'DIRECTORY', 'VIEW', 'REPLACE', 'INSERT', 'DELETE', 'INTO', 'DESCRIBE', 'EXPLAIN', 'FORMAT', 'LOGICAL', 'CODEGEN', 'COST', 'CAST', 'SHOW', 'TABLES', 'COLUMNS', 'COLUMN', 'USE', 'PARTITIONS', 'FUNCTIONS', 'DROP', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'TO', 'TABLESAMPLE', 'STRATIFY', 'ALTER', 'RENAME', 'ARRAY', 'MAP', 'STRUCT', 'COMMENT', 'SET', 'RESET', 'DATA', 'START', 'TRANSACTION', 'COMMIT', 'ROLLBACK', 'MACRO', 'IGNORE', 'BOTH', 'LEADING', 'TRAILING', 'IF', 'POSITION', 'EXTRACT', 'DIV', 'PERCENT', 'BUCKET', 'OUT', 'OF', 'SORT', 'CLUSTER', 'DISTRIBUTE', 'OVERWRITE', 'TRANSFORM', 'REDUCE', 'SERDE', 'SERDEPROPERTIES', 'RECORDREADER', 'RECORDWRITER', 'DELIMITED', 'FIELDS', 'TERMINATED', 'COLLECTION', 'ITEMS', 'KEYS', 'ESCAPED', 'LINES', 'SEPARATED', 'FUNCTION', 'EXTENDED', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'LAZY', 'FORMATTED', 'GLOBAL', TEMPORARY, 'OPTIONS', 'UNSET', 'TBLPROPERTIES', 'DBPROPERTIES', 'BUCKETS', 'SKEWED', 'STORED', 'DIRECTORIES', 'LOCATION', 'EXCHANGE', 'ARCHIVE', 'UNARCHIVE', 'FILEFORMAT', 'TOUCH', 'COMPACT', 'CONCATENATE', 'CHANGE', 'CASCADE', 'RESTRICT', 'CLUSTERED', 'SORTED', 'PURGE', 'INPUTFORMAT', 'OUTPUTFORMAT', DATABASE, DATABASES, 'DFS', 'TRUNCATE', 'ANALYZE', 'COMPUTE', 'LIST', 'STATISTICS', 'PARTITIONED', 'EXTERNAL', 'DEFINED', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'REPAIR', 'RECOVER', 'EXPORT', 'IMPORT', 'LOAD', 'ROLE', 'ROLES', 'COMPACTIONS', 'PRINCIPALS', 'TRANSACTIONS', 'INDEX', 'INDEXES', 'LOCKS', 'OPTION', 'ANTI', 'LOCAL', 'INPATH', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 6)\n\n== SQL ==\nstruct<a:null>\n------^^^\n""
>>> 
>>> # works fine
>>> json_schema = spark.read.json(df.rdd.map(lambda x: x[0])).schema
>>> df.select(from_json('json', json_schema))
DataFrame[jsontostructs(json): struct<a:string>]
{code}
I assumed that {{schema_of_json()}} would use the same code path that underlies {{spark.read.json()}}, but I guess there is something different that trips up the parser.",,dongjoon,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 10 07:42:05 UTC 2020,,,,,,,,,,"0|z0c8a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/20 05:48;nchammas;cc [~hyukjin.kwon];;;","06/Mar/20 06:44;nchammas;Confirmed this issue is also present on {{branch-3.0}} as of commit {{9b48f3358d3efb523715a5f258e5ed83e28692f6}}.;;;","09/Mar/20 01:52;gurwls223;There seems two issues. The first is in the JSON schema inference (https://github.com/apache/spark/blob/c1986204e59f1e8cc4b611d5a578cb248cb74c28/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JsonInferSchema.scala#L115-L122). Seems it treats empty strings as nulls. We might need an option to treat this as StringType.

Another issue is that {{null}} type is not properly being supported in SQL parser (which is used to parse the DDL type string from {{schema_of_json}}). Hive supports {{null}} type as {{void}} keyword so we might need to support this as a proper string to parse as a {{null}} type.;;;","09/Mar/20 05:16;nchammas;Thanks for looking into it.

I have a silly question: Why isn't {{schema_of_json()}} simply syntactic sugar for {{spark.read.json()}}?

For example:
{code:python}
def _schema_of_json(string):
    df = spark.read.json(spark.sparkContext.parallelize([string]))
    return df.schema{code}
Perhaps there are practical reasons not to do this, but conceptually speaking this kind of equivalence should hold. Yet this bug report demonstrates that they are not equivalent.
{code:python}
from pyspark.sql.functions import from_json, schema_of_json
json = '{""a"": """"}'

df = spark.createDataFrame([(json,)], schema=['json'])
df.show()

# chokes with org.apache.spark.sql.catalyst.parser.ParseException
json_schema = schema_of_json(json)
df.select(from_json('json', json_schema))

# works fine
json_schema = _schema_of_json(json)
df.select(from_json('json', json_schema)) {code};;;","09/Mar/20 06:05;gurwls223;Oh, nice catch. Yes, {{null}} types inferred for each field is replaced to {{string}} type through https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JsonInferSchema.scala#L380 which isn't called in {{schema_of_json}}.

Let me make a quick fix.;;;","09/Mar/20 17:20;dongjoon;Thank you, [~nchammas] and [~hyukjin.kwon].;;;","10/Mar/20 07:42;dongjoon;Issue resolved by pull request 27854
[https://github.com/apache/spark/pull/27854];;;",,,,,,,,,,,,,,,,,
Impossible to change the provider of a table in the HiveMetaStore,SPARK-31061,13289932,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,06/Mar/20 00:49,02/Jul/20 10:09,13/Jul/23 08:46,06/Mar/20 07:55,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Currently, it's impossible to alter the datasource of a table in the HiveMetaStore by using alterTable, as the HiveExternalCatalog doesn't change the provider table property during an alterTable command. This is required to support changing table formats when using commands like REPLACE TABLE.",,apachespark,brkyvz,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 02 10:09:51 UTC 2020,,,,,,,,,,"0|z0c840:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/20 07:55;cloud_fan;Issue resolved by pull request 27822
[https://github.com/apache/spark/pull/27822];;;","02/Jul/20 10:09;apachespark;User 'TJX2014' has created a pull request for this issue:
https://github.com/apache/spark/pull/28980;;;","02/Jul/20 10:09;apachespark;User 'TJX2014' has created a pull request for this issue:
https://github.com/apache/spark/pull/28980;;;","02/Jul/20 10:09;apachespark;User 'TJX2014' has created a pull request for this issue:
https://github.com/apache/spark/pull/28980;;;","02/Jul/20 10:09;apachespark;User 'TJX2014' has created a pull request for this issue:
https://github.com/apache/spark/pull/28980;;;",,,,,,,,,,,,,,,,,,,
"Fix flaky test ""DAGSchedulerSuite.shuffle fetch failed on speculative task, but original task succeed""",SPARK-31052,13289802,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,05/Mar/20 13:17,05/Mar/20 19:02,13/Jul/23 08:46,05/Mar/20 19:02,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"Test ""shuffle fetch failed on speculative task, but original task succeed"" is flaky.",,jiangxb1987,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30388,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 19:02:21 UTC 2020,,,,,,,,,,"0|z0c7b4:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"05/Mar/20 19:02;jiangxb1987;Fixed by https://github.com/apache/spark/pull/27809;;;",,,,,,,,,,,,,,,,,,,,,,,
Disable flaky KafkaDelegationTokenSuite,SPARK-31050,13289722,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,05/Mar/20 07:58,05/Mar/20 14:24,13/Jul/23 08:46,05/Mar/20 08:22,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,Structured Streaming,,,0,,,Disable flaky KafkaDelegationTokenSuite since it's too flaky.,,dongjoon,gsomogyi,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30541,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 08:22:10 UTC 2020,,,,,,,,,,"0|z0c6tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/20 08:22;dongjoon;Issue resolved by pull request 27789
[https://github.com/apache/spark/pull/27789];;;",,,,,,,,,,,,,,,,,,,,,,,
Spark 3.0 built against hadoop2.7 can't start standalone master,SPARK-31043,13289617,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,tgraves,tgraves,04/Mar/20 19:22,12/Dec/22 18:11,13/Jul/23 08:46,06/Mar/20 02:22,3.0.0,,,,,,,,,,,,3.0.0,,,Build,,,,0,,,"trying to start a standalone master when building spark branch 3.0 with hadoop2.7 fails with:

 
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/w3c/dom/ElementTraversal
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:757)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at 
[java.net|http://java.net/]
.URLClassLoader.defineClass(URLClassLoader.java:468)
        at 
[java.net|http://java.net/]
.URLClassLoader.access$100(URLClassLoader.java:74)
        at 
[java.net|http://java.net/]
.URLClassLoader$1.run(URLClassLoader.java:369)
...
Caused by: java.lang.ClassNotFoundException: org.w3c.dom.ElementTraversal
        at 
[java.net|http://java.net/]
.URLClassLoader.findClass(URLClassLoader.java:382)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
        ... 42 more",,nchammas,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30994,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 09 05:18:31 UTC 2020,,,,,,,,,,"0|z0c660:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/20 19:27;tgraves;I'm working on tracing down what broke this.

[~srowen] looks like  [SPARK-30994][CORE] Update xerces to 2.12.0 broke this. When I revert that it works again.;;;","04/Mar/20 19:29;srowen;Weird but I think I have to revert it . It isn't essential enough as an update. I messed up the change in a way that didn't get it tested by the pr builder properly;;;","04/Mar/20 21:25;srowen;Hm, I'm not seeing failures for the 3.0 branch or master after this change, for Hadoop 2.7.
It sure does look suspiciously related as that class is XML-related.
However that class is also not in Xerces, but in xml-apis.
You don't by chance have old and new Xerces in your deployment somehow?

Anyway this makes me nervous enough relative to the gain, that unless you have a reason to think it's a fluke, I think I'm going to revert it.;;;","04/Mar/20 21:59;tgraves;A couple of my colleagues actually ran into this and reported it to me. I built and saw the same thing. I did a clean when building, but I'll run again just to verify.

I was building with:

build/mvn -Phadoop-2.7 -Pyarn -Pkinesis-asl -Pkubernetes -Pmesos -Phadoop-cloud -Pspark-ganglia-lgpl clean package -DskipTests 2>&1 | tee out

I reverted the one xerces version change commit and rebuilt with command above and the error went away.

One thing is that I don't have hadoop env variables set - not sure if you do or have them in path such that it might be picking up jars from there.

Yeah I actually started looking for other things because it was complaining about xml-apis so thought the xerces change was weird that it caused but haven't investigated further;;;","04/Mar/20 22:17;tgraves;rebuilt and still see the error. The full exception in the master log is:

 

java.lang.NoClassDefFoundError: org/w3c/dom/ElementTraversal
 at java.lang.ClassLoader.defineClass1(Native Method)
 at java.lang.ClassLoader.defineClass(ClassLoader.java:757)
 at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
 at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
 at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
 at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
 at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
 at java.security.AccessController.doPrivileged(Native Method)
 at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
 at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
 at org.apache.xerces.parsers.AbstractDOMParser.startDocument(Unknown Source)
 at org.apache.xerces.xinclude.XIncludeHandler.startDocument(Unknown Source)
 at org.apache.xerces.impl.dtd.XMLDTDValidator.startDocument(Unknown Source)
 at org.apache.xerces.impl.XMLDocumentScannerImpl.startEntity(Unknown Source)
 at org.apache.xerces.impl.XMLVersionDetector.startDocumentParsing(Unknown Source)
 at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
 at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
 at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
 at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
 at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
 at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
 at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2482)
 at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2470)
 at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2541)
 at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2494)
 at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2407)
 at org.apache.hadoop.conf.Configuration.get(Configuration.java:981)
 at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1031)
 at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1432)
 at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:72)
 at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
 at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
 at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
 at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
 at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
 at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
 at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
 at org.apache.spark.deploy.master.Master$.startRpcEnvAndEndpoint(Master.scala:1137)
 at org.apache.spark.deploy.master.Master$.main(Master.scala:1122)
 at org.apache.spark.deploy.master.Master.main(Master.scala)
Caused by: java.lang.ClassNotFoundException: org.w3c.dom.ElementTraversal
 at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
 at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
 ... 44 more;;;","06/Mar/20 02:22;gurwls223;I believe it was fixed as of https://github.com/apache/spark/pull/27808;;;","06/Mar/20 06:46;nchammas;FWIW I was seeing the same {{java.lang.NoClassDefFoundError: org/w3c/dom/ElementTraversal}} issue on {{branch-3.0}} and pulling the latest changes fixed it for me too.;;;","09/Mar/20 01:53;gurwls223;[~nchammas], can you show the reproducible steps for that?;;;","09/Mar/20 05:18;nchammas;It's working for me now (per my comment), but when I was seeing the issue simply starting a PySpark shell was enough to throw that error. Do you still need a reproduction?;;;",,,,,,,,,,,,,,,
ShuffleBlockFetcherIterator may can't create request for last group,SPARK-31034,13289446,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,04/Mar/20 07:17,05/Mar/20 15:07,13/Jul/23 08:46,05/Mar/20 13:58,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"When the size of all blocks is less than targetRemoteRequestSize and the size of last block group is less than maxBlocksInFlightPerAddress, ShuffleBlockFetcherIterator will not create a request for the last group.",,cloud_fan,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 13:58:37 UTC 2020,,,,,,,,,,"0|z0c540:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/20 13:58;cloud_fan;Issue resolved by pull request 27786
[https://github.com/apache/spark/pull/27786];;;",,,,,,,,,,,,,,,,,,,,,,,
Occasional class not found error in user's Future code using global ExecutionContext,SPARK-31029,13289372,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shanyu,shanyu,shanyu,04/Mar/20 02:11,19/Jun/20 15:01,13/Jul/23 08:46,19/Jun/20 15:01,2.4.5,,,,,,,,,,,,3.1.0,,,Spark Core,YARN,,,0,,,"*Problem:*
When running tpc-ds test (https://github.com/databricks/spark-sql-perf), occasionally we see error related to class not found:

2020-02-04 20:00:26,673 ERROR yarn.ApplicationMaster: User class threw exception: scala.ScalaReflectionException: class com.databricks.spark.sql.perf.ExperimentRun in JavaMirror with 
sun.misc.Launcher$AppClassLoader@28ba21f3 of type class sun.misc.Launcher$AppClassLoader with classpath [...] 
and parent being sun.misc.Launcher$ExtClassLoader@3ff5d147 of type class sun.misc.Launcher$ExtClassLoader with classpath [...] 
and parent being primordial classloader with boot classpath [...] not found.

*Root cause:*
Spark driver starts ApplicationMaster in the main thread, which starts a user thread and set MutableURLClassLoader to that thread's ContextClassLoader.
	userClassThread = startUserApplication()

The main thread then setup YarnSchedulerBackend RPC endpoints, which handles these calls using scala Future with the default global ExecutionContext:
    - doRequestTotalExecutors
    - doKillExecutors

If main thread starts a future to handle doKillExecutors() before user thread does then the default thread pool thread's ContextClassLoader would be the default (AppClassLoader). 
If user thread starts a future first then the thread pool thread will have MutableURLClassLoader.

So if user's code uses a future which references a user provided class (only MutableURLClassLoader can load), and before the future if there are executor lost, you will see errors related to class not found.

*Proposed Solution:*
We can potentially solve this problem in one of two ways:
1) Set the same class loader (userClassLoader) to both the main thread and user thread in ApplicationMaster.scala

2) Do not use ""ExecutionContext.Implicits.global"" in YarnSchedulerBackend",,shanyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-03-04 02:11:04.0,,,,,,,,,,"0|z0c4xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Star(*) expression fails when used with fully qualified column names for v2 tables,SPARK-31015,13289100,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,imback82,imback82,03/Mar/20 03:51,03/Mar/20 17:07,13/Jul/23 08:46,03/Mar/20 17:07,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"For a v2 table,
{code:java}
CREATE TABLE testcat.ns1.ns2.tbl (id bigint, name string) USING foo
{code}

The following works
{code:java}
SELECT testcat.ns1.ns2.tbl.id FROM testcat.ns1.ns2.tbl
{code}
but this fails to resolve
{code:java}
SELECT testcat.ns1.ns2.tbl.* FROM testcat.ns1.ns2.tbl
[info]   org.apache.spark.sql.AnalysisException: cannot resolve 'testcat.ns1.ns2.tbl.*' given input columns 'id, name';
{code}
",,cloud_fan,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 17:07:10 UTC 2020,,,,,,,,,,"0|z0c3ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/20 17:07;cloud_fan;Issue resolved by pull request 27766
[https://github.com/apache/spark/pull/27766];;;",,,,,,,,,,,,,,,,,,,,,,,
InMemoryStore: CountingRemoveIfForEach misses to remove key from parentToChildrenMap,SPARK-31014,13289096,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kabhwan,kabhwan,kabhwan,03/Mar/20 03:10,08/Mar/20 00:15,13/Jul/23 08:46,07/Mar/20 20:04,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"SPARK-30964 introduces the secondary index which defines the relationship between parent - children and able to operate all children for given parent faster.

This change is not applied to CountingRemoveIfForEach, so there's a chance ""countingRemoveAllByIndexValues"" missed to remove key from parentToChildrenMap.",,dongjoon,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30964,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 07 20:04:47 UTC 2020,,,,,,,,,,"0|z0c3o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/20 20:04;dongjoon;This is resolved via the followings.
- https://github.com/apache/spark/pull/27765 (master)
- https://github.com/apache/spark/pull/27825 (branch-3.0);;;",,,,,,,,,,,,,,,,,,,,,,,
Failed to register signal handler for PWR,SPARK-31011,13288877,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kabhwan,gsomogyi,gsomogyi,02/Mar/20 13:33,12/Dec/22 18:10,13/Jul/23 08:46,12/Mar/20 03:27,3.1.0,,,,,,,,,,,,3.1.0,,,Spark Core,,,,0,,,"I've just tried to test something on standalone mode but the application fails.

Environment:
 * MacOS Catalina 10.15.3 (19D76)
 * Scala 2.12.10
 * Java 1.8.0_241-b07

Steps to reproduce:
 * Compile Spark (mvn -DskipTests clean install -Dskip)
 * ./sbin/start-master.sh
 * ./sbin/start-slave.sh spark://host:7077
 * submit an empty application

Error:
{code:java}
20/03/02 14:25:44 INFO SignalUtils: Registering signal handler for PWR
20/03/02 14:25:44 WARN SignalUtils: Failed to register signal handler for PWR
java.lang.IllegalArgumentException: Unknown signal: PWR
	at sun.misc.Signal.<init>(Signal.java:143)
	at org.apache.spark.util.SignalUtils$.$anonfun$register$1(SignalUtils.scala:64)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.util.SignalUtils$.register(SignalUtils.scala:62)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend.onStart(CoarseGrainedExecutorBackend.scala:85)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:120)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:203)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}",,dongjoon,gsomogyi,kabhwan,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20628,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 03:27:29 UTC 2020,,,,,,,,,,"0|z0c2lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/20 13:36;gsomogyi;[~holden] since you've added this recently any idea?

 ;;;","03/Mar/20 03:52;gurwls223;[~gsomogyi], can you show the commands you used for ""Compile Spark"" and ""submit an empty application""? I would like to reproduce and take a look.;;;","03/Mar/20 04:11;kabhwan;[https://en.wikipedia.org/wiki/Signal_(IPC)]

According to the wikipedia, SIGPWR is NOT listed in the POSIX specification, so we may need to handle such case, at least provide better message instead of random message.

But in anyway, looks like it should just leave warn message and doesn't break functionality other than effectively disabling decommission. If the application is failed, that's something we should take a look. (Please add OS version as well.);;;","03/Mar/20 14:17;gsomogyi;[~hyukjin.kwon] [~kabhwan] added OS info + cmd to compile Spark. The application looks like is not failing but not sure what kind of feature is missing. It can be a silent feature killer...

Anyway, since it's not making the apps failing I've lowered the prio.;;;","12/Mar/20 03:27;dongjoon;Issue resolved by pull request 27832
[https://github.com/apache/spark/pull/27832];;;",,,,,,,,,,,,,,,,,,,
Fix incorrect use of assume() in tests,SPARK-31003,13288599,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,02/Mar/20 02:46,03/Mar/20 04:21,13/Jul/23 08:46,02/Mar/20 23:24,3.0.0,,,,,,,,,,,,2.4.6,3.0.0,,Tests,,,,0,,,"[~ahirreddy] and I found some cases where {{assume}} and {{assert}} were mixed up in test suites.

If an {{assume}} call fails then it will cause the test to be marked as skipped instead of failed: this is often used to skip tests if certain prerequisites are missing (e.g. to disable certain tests on Windows). If {{assume}} is mistakenly used in place of {{assert}} then this can cause logically-failing tests to be skipped, masking bugs.

This patch fixes several such cases, replacing certain {{assume}} calls with {{assert}}. ",,dongjoon,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 23:24:28 UTC 2020,,,,,,,,,,"0|z0c1y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/20 23:24;dongjoon;This is resolved via https://github.com/apache/spark/pull/27754;;;",,,,,,,,,,,,,,,,,,,,,,,
ClassCastException when a generator having nested inner generators,SPARK-30998,13288483,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,01/Mar/20 05:02,03/Mar/20 20:33,13/Jul/23 08:46,03/Mar/20 14:49,2.0.2,2.1.3,2.2.3,2.3.4,2.4.5,3.0.0,3.1.0,,,,,,2.4.6,3.0.0,,SQL,,,,0,,,"A query below failed in the master/branch-3.0/branch-2.4;

{code}
scala> sql(""select array(array(1, 2), array(3)) ar"").select(explode(explode($""ar""))).show()
20/03/01 13:51:56 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)/ 1]
java.lang.ClassCastException: scala.collection.mutable.ArrayOps$ofRef cannot be cast to org.apache.spark.sql.catalyst.util.ArrayData
	at org.apache.spark.sql.catalyst.expressions.ExplodeBase.eval(generators.scala:313)
	at org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$8(GenerateExec.scala:108)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
	at scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
    ...
{code}
",,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 14:49:28 UTC 2020,,,,,,,,,,"0|z0c18g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/20 14:49;maropu;Resolved by [https://github.com/apache/spark/pull/27750] and [https://github.com/apache/spark/pull/27769];;;",,,,,,,,,,,,,,,,,,,,,,,
An analysis failure in generators with aggregate functions,SPARK-30997,13288482,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,01/Mar/20 04:29,03/Mar/20 22:28,13/Jul/23 08:46,03/Mar/20 20:25,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"We have supported generators in SQL aggregate expressions by SPARK-28782.
But, the generator(explode) query with aggregate functions in DataFrame failed as follows;


{code}
// SPARK-28782: Generator support in aggregate expressions
scala> spark.range(3).toDF(""id"").createOrReplaceTempView(""t"")
scala> sql(""select explode(array(min(id), max(id))) from t"").show()
+---+
|col|
+---+
|  0|
|  2|
+---+

// A failure case handled in this pr
scala> spark.range(3).select(explode(array(min($""id""), max($""id"")))).show()
org.apache.spark.sql.AnalysisException:
The query operator `Generate` contains one or more unsupported
expression types Aggregate, Window or Generate.
Invalid expressions: [min(`id`), max(`id`)];;
Project [col#46L]
+- Generate explode(array(min(id#42L), max(id#42L))), false, [col#46L]
   +- Range (0, 3, step=1, splits=Some(4))

  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:49)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:48)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:129)
{code}

The root cause is that `ExtractGenerator` wrongly replaces a project w/ aggregate functions
before `GlobalAggregates` replaces it with an aggregate as follows;

{code}
scala> sql(""SET spark.sql.optimizer.planChangeLog.level=warn"")
scala> spark.range(3).select(explode(array(min($""id""), max($""id"")))).show()

20/03/01 12:51:58 WARN HiveSessionStateBuilder$$anon$1: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences ===
!'Project [explode(array(min('id), max('id))) AS List()]   'Project [explode(array(min(id#72L), max(id#72L))) AS List()]
 +- Range (0, 3, step=1, splits=Some(4))                   +- Range (0, 3, step=1, splits=Some(4))
           
20/03/01 12:51:58 WARN HiveSessionStateBuilder$$anon$1: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractGenerator ===
!'Project [explode(array(min(id#72L), max(id#72L))) AS List()]   Project [col#76L]
!+- Range (0, 3, step=1, splits=Some(4))                         +- Generate explode(array(min(id#72L), max(id#72L))), false, [col#76L]
!                                                                   +- Range (0, 3, step=1, splits=Some(4))
           
20/03/01 12:51:58 WARN HiveSessionStateBuilder$$anon$1: 
=== Result of Batch Resolution ===
!'Project [explode(array(min('id), max('id))) AS List()]   Project [col#76L]
!+- Range (0, 3, step=1, splits=Some(4))                   +- Generate explode(array(min(id#72L), max(id#72L))), false, [col#76L]
!                                                             +- Range (0, 3, step=1, splits=Some(4))
          
// the analysis failed here...
{code}

",,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28782,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 20:25:48 UTC 2020,,,,,,,,,,"0|z0c188:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/20 20:25;dongjoon;Issue resolved by pull request 27749
[https://github.com/apache/spark/pull/27749];;;",,,,,,,,,,,,,,,,,,,,,,,
GenerateUnsafeRowJoiner corrupts the value if the datatype is UDF and its sql type has fixed length,SPARK-30993,13288450,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,29/Feb/20 14:11,08/Jun/21 16:45,13/Jul/23 08:46,02/Mar/20 19:26,2.3.0,2.3.1,2.3.2,2.3.3,2.3.4,2.4.5,3.0.0,,,,,,2.4.6,3.0.0,,SQL,,,,0,correctness,,"This is reported by user mailing list, though the mail thread is regarding suspect of the behavior of mapGroupsWithState.

[https://lists.apache.org/thread.html/r08b44a7afac4e4c971633d30b4e5d11bd7c0d6e28180e03b874ea58b%40%3Cuser.spark.apache.org%3E]

The actual culprit is, there're a couple of methods which don't handle UDT and it makes GenerateUnsafeRowJoiner to generate incorrect code. Specifically, the issue occurs when the sql type of UDT has fixed length - GenerateUnsafeRowJoiner has the logic to update the offset position for all variable-length data, and due to this bug, UDT field with fixed length is being treated as variable-length data and its value is modified.",,apachespark,dongjoon,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30986,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 08 16:45:27 UTC 2021,,,,,,,,,,"0|z0c114:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Feb/20 14:13;kabhwan;Will submit a PR soon. Btw, it looks to be correctness issue, and if it is, priority may need to be raised to critical+ (I'd defer decision of priority to the committers.);;;","29/Feb/20 14:14;kabhwan;The reporter uses Spark 2.3.0, and validated it exists on Spark 2.4.0 as well.;;;","29/Feb/20 14:39;kabhwan;During review phase I'll check which versions are affected (recent version for each minor version). Maybe tomorrow. It would be really appreciated someone could help checking it.;;;","01/Mar/20 03:23;kabhwan;Just confirmed the problem persists in branch-2.3 and branch-2.4. I guess the bug is ancient one, as there has been no code change around the spot.;;;","01/Mar/20 23:51;dongjoon;Thank you, [~kabhwan]!;;;","02/Mar/20 19:26;dongjoon;This is resolved via https://github.com/apache/spark/pull/27747 in `branch-3.0`.
And, https://github.com/apache/spark/pull/27761 is under review.;;;","08/Jun/21 16:44;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/32824;;;","08/Jun/21 16:45;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/32824;;;",,,,,,,,,,,,,,,,
ResourceDiscoveryPluginSuite sometimes fails,SPARK-30987,13288331,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,28/Feb/20 16:31,29/Feb/20 02:11,13/Jul/23 08:46,28/Feb/20 19:51,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"{color:#1d1c1d}I noted that {color}{{ResourceDiscoveryPluginSuite}}{color:#1d1c1d} is quite flack recently. And I did some investigation locally and it looks it’s a pure problem of the time:{color}
[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/119030/testReport/]
[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/119005/testReport/]
[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/119029/testReport/]
{color:#1d1c1d}and many other places...{color}",,jiangxb1987,Ngone51,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 28 19:51:26 UTC 2020,,,,,,,,,,"0|z0c0ao:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"28/Feb/20 16:32;tgraves;all of these fail in just starting up the local-cluster. I can't reproduce locally. My guess here is that the jenkins box is being overloaded and the wait timeout is not enough.

 

We increased timeout some other places this was used with SPARK-29139, so I think we should try increasing the timeout on these as well.;;;","28/Feb/20 19:51;jiangxb1987;Resolved by https://github.com/apache/spark/pull/27738;;;",,,,,,,,,,,,,,,,,,,,,,
ResourceProfile and Builder should be private in spark 3.0,SPARK-30977,13288125,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,27/Feb/20 18:49,29/Feb/20 02:12,13/Jul/23 08:46,29/Feb/20 02:12,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"it looks like ResourceProfile and ResourceProfileBuilder accidentally got opened up - they should be private[spark] until the stage level scheduling feature is complete, which won't make the 3.0 release.  So make them private in 3.0 branch",,dongjoon,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 29 02:12:44 UTC 2020,,,,,,,,,,"0|z0bz28:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"27/Feb/20 18:53;tgraves;I'm working on this should have pr by end of day;;;","29/Feb/20 02:12;dongjoon;Issue resolved by pull request 27737
[https://github.com/apache/spark/pull/27737];;;",,,,,,,,,,,,,,,,,,,,,,
ScriptTransformationExec should wait for the termination of process when scriptOutputReader hasNext return false,SPARK-30973,13288052,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sunke,sunke,sunke,27/Feb/20 13:25,14/May/20 14:04,13/Jul/23 08:46,14/May/20 13:56,2.4.4,2.4.5,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,,,cloud_fan,sunke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 13:56:31 UTC 2020,,,,,,,,,,"0|z0bym0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/May/20 13:56;cloud_fan;Issue resolved by pull request 27724
[https://github.com/apache/spark/pull/27724];;;",,,,,,,,,,,,,,,,,,,,,,,
PruneHiveTablePartitions should be executed as earlyScanPushDownRules,SPARK-30972,13288029,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,27/Feb/20 12:17,28/Feb/20 05:22,13/Jul/23 08:46,28/Feb/20 05:22,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Similar to PruneFileSourcePartitions, PruneHiveTablePartitions should also be executed as earlyScanPushDownRules to eliminate the impact on statistic computation later.",,cloud_fan,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 28 05:22:15 UTC 2020,,,,,,,,,,"0|z0bygw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/20 05:22;cloud_fan;Issue resolved by pull request 27723
[https://github.com/apache/spark/pull/27723];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix NPE in resolving k8s master url,SPARK-30970,13287979,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,27/Feb/20 09:34,28/Feb/20 19:16,13/Jul/23 08:46,28/Feb/20 08:04,2.3.4,2.4.5,3.0.0,,,,,,,,,,2.4.6,3.0.0,,Kubernetes,Spark Core,,,0,,,"{code:java}
```
bin/spark-sql --master  k8s:///https://kubernetes.docker.internal:6443 --conf spark.kubernetes.container.image=yaooqinn/spark:v2.4.4
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.spark.util.Utils$.checkAndGetK8sMasterUrl(Utils.scala:2739)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:261)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:774)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
```
{code}
Althrough k8s:///https://kubernetes.docker.internal:6443 is a wrong master url but should not throw npe",,dongjoon,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 28 08:04:29 UTC 2020,,,,,,,,,,"0|z0by5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/20 10:12;dongjoon;Since the root cause is the user mistakes, the prevention logic will be a minor bug fix.;;;","27/Feb/20 10:13;dongjoon;BTW, [~Qin Yao]. Could you check 2.3.4 behavior and update the Affected Version if needed?;;;","28/Feb/20 08:04;dongjoon;This is resolved via https://github.com/apache/spark/pull/27721;;;",,,,,,,,,,,,,,,,,,,,,
Upgrade aws-java-sdk-sts to 1.11.655,SPARK-30968,13287958,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,27/Feb/20 07:54,13/Jun/20 16:31,13/Jul/23 08:46,28/Feb/20 01:06,3.0.0,,,,,,,,,,,,2.4.6,3.0.0,,Build,,,,0,release-notes,,,,dongjoon,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,"With AWS SDK upgrade to 1.11.655, we strongly encourage the users that use S3N file system (open-source NativeS3FileSystem that is based on jets3t library) on Hadoop 2.7.3 to upgrade to use AWS Signature V4 and set the bucket endpoint or migrate to S3A (“s3a://” prefix) - jets3t library uses AWS v2 by default and s3.amazonaws.com as an endpoint. Otherwise, the 403 Forbidden error may be thrown in the following cases:
If a user accesses an S3 path that contains “+” characters and uses the legacy S3N file system, e.g. s3n://bucket/path/+file.
If a user has configured AWS V2 signature to sign requests to S3 with S3N file system.
Note that if you use S3AFileSystem, e.g. (“s3a://bucket/path”) to access S3 in S3Select or SQS connectors, then everything will work as expected.
",false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 13 16:31:12 UTC 2020,,,,,,,,,,"0|z0by14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/20 01:06;dongjoon;Issue resolved by pull request 27720
[https://github.com/apache/spark/pull/27720];;;","13/Jun/20 16:31;smilegator;Added the release note based on the issue we identified. ;;;",,,,,,,,,,,,,,,,,,,,,,
do not set default era for DateTimeFormatter,SPARK-30958,13287704,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,26/Feb/20 09:45,12/Dec/22 18:10,13/Jul/23 08:46,16/Mar/20 07:49,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 16 07:49:21 UTC 2020,,,,,,,,,,"0|z0bwgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/20 07:49;gurwls223;Issue resolved by pull request 27707
[https://github.com/apache/spark/pull/27707];;;",,,,,,,,,,,,,,,,,,,,,,,
Exclude Generate output when aliasing in nested column pruning,SPARK-30955,13287667,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,26/Feb/20 05:48,12/Dec/22 18:10,13/Jul/23 08:46,28/Feb/20 03:30,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"When aliasing in nested column pruning on Project on top of Generate, we should exclude Generate outputs.",,petertoth,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 28 03:30:38 UTC 2020,,,,,,,,,,"0|z0bw8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/20 07:43;petertoth;This affects only 3.0, doesn't it?;;;","26/Feb/20 08:08;viirya;Thanks for pointing it. Yea, I re-checked and that code isn't in branch-2.4.;;;","28/Feb/20 03:30;gurwls223;Issue resolved by pull request 27702
[https://github.com/apache/spark/pull/27702];;;",,,,,,,,,,,,,,,,,,,,,
Potential data loss for legacy applications after switch to proleptic Gregorian calendar,SPARK-30951,13287647,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,maxgekk,bersprockets,bersprockets,26/Feb/20 01:43,12/Dec/22 18:10,13/Jul/23 08:46,20/Mar/20 07:33,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,release-notes,,"tl;dr: We recently discovered some Spark 2.x sites that have lots of data containing dates before October 15, 1582. This could be an issue when such sites try to upgrade to Spark 3.0.

From SPARK-26651:
{quote}""The changes might impact on the results for dates and timestamps before October 15, 1582 (Gregorian)
{quote}
We recently discovered that some large scale Spark 2.x applications rely on dates before October 15, 1582.

Two cases came up recently:
 * An application that uses a commercial third-party library to encode sensitive dates. On insert, the library encodes the actual date as some other date. On select, the library decodes the date back to the original date. The encoded value could be any date, including one before October 15, 1582 (e.g., ""0602-04-04"").
 * An application that uses a specific unlikely date (e.g., ""1200-01-01"") as a marker to indicate ""unknown date"" (in lieu of null)

Both sites ran into problems after another component in their system was upgraded to use the proleptic Gregorian calendar. Spark applications that read files created by the upgraded component were interpreting encoded or marker dates incorrectly, and vice versa. Also, their data now had a mix of calendars (hybrid and proleptic Gregorian) with no metadata to indicate which file used which calendar.

Both sites had enormous amounts of existing data, so re-encoding the dates using some other scheme was not a feasible solution.

This is relevant to Spark 3:

Any Spark 2 application that uses such date-encoding schemes may run into trouble when run on Spark 3. The application may not properly interpret the dates previously written by Spark 2. Also, once the Spark 3 version of the application writes data, the tables will have a mix of calendars (hybrid and proleptic gregorian) with no metadata to indicate which file uses which calendar.

Similarly, sites might run with mixed Spark versions, resulting in data written by one version that cannot be interpreted by the other. And as above, the tables will now have a mix of calendars with no way to detect which file uses which calendar.

As with the two real-life example cases, these applications may have enormous amounts of legacy data, so re-encoding the dates using some other scheme may not be feasible.

We might want to consider a configuration setting to allow the user to specify the calendar for storing and retrieving date and timestamp values (not sure how such a flag would affect other date and timestamp-related functions). I realize the change is far bigger than just adding a configuration setting.

Here's a quick example of where trouble may happen, using the real-life case of the marker date.

In Spark 2.4:
{noformat}
scala> spark.read.orc(s""$home/data/datefile"").filter(""dt == '1200-01-01'"").count
res0: Long = 1
scala>
{noformat}
In Spark 3.0 (reading from the same legacy file):
{noformat}
scala> spark.read.orc(s""$home/data/datefile"").filter(""dt == '1200-01-01'"").count
res0: Long = 0
scala> 
{noformat}
By the way, Hive had a similar problem. Hive switched from hybrid calendar to proleptic Gregorian calendar between 2.x and 3.x. After some upgrade headaches related to dates before 1582, the Hive community made the following changes:
 * When writing date or timestamp data to ORC, Parquet, and Avro files, Hive checks a configuration setting to determine which calendar to use.
 * When writing date or timestamp data to ORC, Parquet, and Avro files, Hive stores the calendar type in the metadata.
 * When reading date or timestamp data from ORC, Parquet, and Avro files, Hive checks the metadata for the calendar type.
 * When reading date or timestamp data from ORC, Parquet, and Avro files that lack calendar metadata, Hive's behavior is determined by a configuration setting. This allows Hive to read legacy data (note: if the data already consists of a mix of calendar types with no metadata, there is no good solution).",,bersprockets,cloud_fan,dongjoon,petertoth,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26651,SPARK-31404,,,,,SPARK-34675,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 10 03:40:20 UTC 2020,,,,,,,,,,"0|z0bw40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/20 04:29;gurwls223;FYI [~cloud_fan], [~maxgekk], [~XuanYuan];;;","04/Mar/20 02:33;XuanYuan;[~bersprockets] Thanks for the report and detailed analysis. I think it's the issue we are solving in: [https://github.com/apache/spark/pull/27537];;;","04/Mar/20 10:38;cloud_fan;[~bersprockets] You are making a good point here. It's very hard to roll out the calendar switching smoothly, but we at least should give users a way to read their legacy data.

The hive approach looks good to me. [~maxgekk] can we implement something like that?;;;","13/Mar/20 21:15;dongjoon;I marked as `correctness` since the query result is different in any way. BTW, did we mention this can be a correctness issue in the migration guide?;;;","16/Mar/20 08:27;cloud_fan;[~dongjoon] different query result doesn't always mean correctness issue. In this case, it's well documented (in the migration guide) that datetime operations before 1582 will have slightly different results due to the calendar switch.

This PR reports the missing support of legacy data files, but it's not a correctness issue. It's a general problem of file formats like Parquet/Avro where the date/timestamp type is not well defined (missing calendar information). For example, if we use Spark 2.4 to read parquet files written by Hive 3.x, we will also get unexpected results as the calendar is different.

On the other hand, the Proleptic Gregorian calendar is the de-facto calendar of our world, so it's reasonable to assume the calendar should be Proleptic Gregorian if file formats don't define it explicitly. That said, the parquet files written Spark 2.x was wrong, instead of Spark 3.0 having a correctness issue.

Think about if we fix a correctness issue of a SQL function in 3.0, but users already have a data file containing the result of this SQL function, written by Spark 2.4. Users would get unexpected result but it doesn't mean 3.0 has a correctness issue.;;;","20/Mar/20 03:26;dongjoon;Thanks. According to [~cloud_fan] comment, `correctness` label is removed. However, it seems that we need more documents like the above [~cloud_fan]'s comment.;;;","20/Mar/20 07:33;cloud_fan;I'm closing it as all the sub-tasks are done. Now users can turn on legacy configs to read/write legacy data in Parquet/Avro. For ORC, it follows the Java `Timestamp`/`Date` semantic and Spark still respects it in 3.0, so there is no legacy data as nothing changed in 3.0.

We didn't add special metadata to Parquet/Avro files as we think it may not worth the complexity. Feel free to reopen this ticket if you think the metadata is necessary.;;;","24/Mar/20 20:09;bersprockets;[~cloud_fan]
{quote}
For ORC, it follows the Java `Timestamp`/`Date` semantic and Spark still respects it in 3.0, so there is no legacy data as nothing changed in 3.0.
{quote}
Sorry if I misunderstand, but my  example case (above in description) uses ORC, and I can still reproduce with latest master vs. spark 2.4.;;;","24/Mar/20 20:22;dongjoon;Hi, [~bersprockets]. 
Could you file a new JIRA about that ORC issue as a subtask of this JIRA issue?;;;","24/Mar/20 20:54;bersprockets;I added a subtask for ORC. The issue is only for date type. Timestamp type seems fine, as far as I can tell.;;;","24/Mar/20 21:15;dongjoon;Thank you so much, [~bersprockets]!;;;","29/Mar/20 01:27;dongjoon;I added `release-notes` label because `spark.sql.legacy.parquet.rebaseDateTime.enabled` is `false` by default.
cc [~rxin];;;","01/Apr/20 17:46;bersprockets;Since there is some inherit danger in a migrating user accidentally mixing old and new Calendar types in the same table (with no way to distinguish which row has which, other than maybe file timestamps), I propose it makes more sense for the rebaseDateTime configs to be true by default.;;;","01/Apr/20 22:54;dongjoon;I'm +1 basically. Please make a Jira issue and PR for that. We can discuss on there.;;;","01/Apr/20 22:58;dongjoon;The decision will depend on how much people are exposed to that situation because the performance difference is not small.;;;","02/Apr/20 07:43;cloud_fan;Theoretically, Parquet spec implicitly requires Gregorian calendar by referring to the Java 8 time API: https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#timestamp

That said, Spark 2.x writes ""wrong"" datetime values to Parquet, and I don't think we should keep this ""wrong"" behavior by default in 3.0. Besides, you will hit the mixed calendar Parquet files anyway if the data is written by multiple systems (e.g. Spark and Hive).

I'd suggest users turn on the legacy config only if they have legacy datetime values in Parquet that are before 1582. To make users easier to realize the existence of these legacy data, we can fail by default when reading datetime values before 1582 from parquet files. ;;;","03/Apr/20 17:16;bersprockets;{quote}
we can fail by default when reading datetime values before 1582 from parquet files.
{quote}
That sounds reasonable. I can make a PR, but if someone beats me to it, I won't complain.;;;","10/Apr/20 03:40;cloud_fan;FYI I've created a ticket for the fail-by-default behavior: https://issues.apache.org/jira/browse/SPARK-31405;;;",,,,,,
Fix the warning for requiring cores to be limiting resource,SPARK-30942,13287334,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,24/Feb/20 20:06,26/Feb/20 02:50,13/Jul/23 08:46,25/Feb/20 16:57,3.0.0,,,,,,,,,,,,3.1.0,,,Spark Core,,,,0,,,"[https://github.com/apache/spark/pull/27615]

Changed a warning to only show when dynamic allocation enabled. In reality other things rely on the ""slots"", so the warning should always be present if custom resources are specified.",,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29148,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-02-24 20:06:07.0,,,,,,,,,,"0|z0bu6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark Row can be instantiated with duplicate field names,SPARK-30941,13287258,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,droher,droher,24/Feb/20 16:38,12/Dec/22 18:10,13/Jul/23 08:46,09/Mar/20 18:07,1.6.3,2.0.2,2.1.3,2.2.3,2.3.4,2.4.5,3.0.0,,,,,,2.4.6,3.0.0,,PySpark,,,,0,correctness,,"It is possible to create a Row that has fields with the same name when calling `collect()` after a join. Given that the Row constructor itself doesn't allow this, this seems to be undesired behavior.

This can possibly cause correctness issues because different ways of getting values produce different results: {{__get_item__}} will return the leftmost value, while {{asDict()}} will return the rightmost value (because the former uses an index search and the latter uses a dictionary generator).

{{>>> manual_output_row = Row(a=1, b=1, b=2)}}
 \{{ File ""<stdin>"", line 1}}
 {{SyntaxError: keyword argument repeated}}

{{>>> input_rows = Row(a=1, b=1), Row(a=1, b=2)}}
 {{>>> df1, df2 = (spark.createDataFrame([r]) for r in input_rows)}}
 {{>>> df3 = df1.join(df2, ""a"")}}
 {{>>> output_row = df3.collect()[0]}}
 {{>>> output_row}}
 {{Row(a=1, b=1, b=2)}}
 {{>>> output_row[""b""]}}
 {{1}}
 {{>>> output_row.asDict()[""b""]}}
 {{2}} 

**SPARK 1.6.3**
{code}
>>> from pyspark.sql.types import Row
>>> input_rows = Row(a=1, b=1), Row(a=1, b=2)
>>> df1, df2 = (sqlContext.createDataFrame([r]) for r in input_rows)
>>> df3 = df1.join(df2, ""a"")
>>> output_row = df3.collect()[0]
>>> output_row
Row(a=1, b=1, b=2)
>>> output_row[""b""]
1
>>> output_row.asDict()[""b""]
2
>>> sc.version
u'1.6.3'
{code}",,bryanc,dongjoon,droher,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 09 18:07:50 UTC 2020,,,,,,,,,,"0|z0btxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/20 05:05;gurwls223;Probably we should rather document this limitation in {{asDict}} API.;;;","09/Mar/20 18:05;dongjoon;I updated the Affected Versions because this is a behavior from Apache Spark 1.x.;;;","09/Mar/20 18:07;dongjoon;Issue resolved by pull request 27853
[https://github.com/apache/spark/pull/27853];;;",,,,,,,,,,,,,,,,,,,,,
StringIndexer setOutputCols does not set output cols,SPARK-30939,13287225,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,srowen,srowen,24/Feb/20 14:36,25/Feb/20 02:18,13/Jul/23 08:46,25/Feb/20 02:18,3.0.0,,,,,,,,,,,,3.0.0,,,ML,,,,0,,,"(Credit to Brooke Wenig for finding it). Quoting:

"".. The python code works completely fine, but the scala code is outputting

{code}
strIdx_8278ae6d55b3__output, strIdx_8278ae6d55b3__output, strIdx_8278ae6d55b3__output, strIdx_8278ae6d55b3__output, strIdx_8278ae6d55b3__output, strIdx_8278ae6d55b3__output, strIdx_8278ae6d55b3__output
{code}

for the output of the string indexer, instead of using the column names specified in here:

{code}
val stringIndexer = new StringIndexer()
  .setInputCols(categoricalCols)
  .setOutputCols(indexOutputCols)
  .setHandleInvalid(""skip"")
{code}

I was expecting the resulting column names to be

{code}
indexOutputCols: Array[String] = Array(host_is_superhostIndex, cancellation_policyIndex, instant_bookableIndex, neighbourhood_cleansedIndex, property_typeIndex, room_typeIndex, bed_typeIndex)
{code}


Indeed I'm pretty sure this is the bug:

{code}
  private def validateAndTransformField(
      schema: StructType,
      inputColName: String,
      outputColName: String): StructField = {
    val inputDataType = schema(inputColName).dataType
    require(inputDataType == StringType || inputDataType.isInstanceOf[NumericType],
      s""The input column $inputColName must be either string type or numeric type, "" +
        s""but got $inputDataType."")
    require(schema.fields.forall(_.name != outputColName),
      s""Output column $outputColName already exists."")
    NominalAttribute.defaultAttr.withName($(outputCol)).toStructField()
  }
{code}

The last line does not use the transformed output col name, but the default single output col parameter.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11215,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 25 02:18:40 UTC 2020,,,,,,,,,,"0|z0btqg:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"25/Feb/20 02:18;srowen;Issue resolved by pull request 27684
[https://github.com/apache/spark/pull/27684];;;",,,,,,,,,,,,,,,,,,,,,,,
Overflow/round errors in conversions of milliseconds to/from microseconds,SPARK-30925,13287042,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,23/Feb/20 09:04,26/Feb/20 06:04,13/Jul/23 08:46,24/Feb/20 06:20,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Spark has special methods in DataTimeUtils for converting microseconds from/to milliseconds - `fromMillis` and `toMillis()`. The methods handle arithmetic overflow and round negative values. The ticket aims to review all places in Spark SQL where microseconds are converted from/to milliseconds, and replace them by util methods from DateTimeUtils.",,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 24 06:20:06 UTC 2020,,,,,,,,,,"0|z0bsls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/20 06:20;cloud_fan;Issue resolved by pull request 27676
[https://github.com/apache/spark/pull/27676];;;",,,,,,,,,,,,,,,,,,,,,,,
Add additional validation into Merge Into,SPARK-30924,13287029,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,23/Feb/20 02:29,24/Feb/20 07:25,13/Jul/23 08:46,24/Feb/20 07:25,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Merge Into is currently missing additional validation around:

 1. The lack of any WHEN statements

 2. Single use of UPDATE/DELETE

 3. The first WHEN MATCHED statement needs to have a condition if there are two WHEN MATCHED statements.",,brkyvz,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 24 07:25:26 UTC 2020,,,,,,,,,,"0|z0bsiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/20 07:25;cloud_fan;Issue resolved by pull request 27677
[https://github.com/apache/spark/pull/27677];;;",,,,,,,,,,,,,,,,,,,,,,,
Error using VectorAssembler after Pandas GROUPED_AGG UDF,SPARK-30921,13286923,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,tkellogg,tkellogg,22/Feb/20 00:35,12/Dec/22 18:10,13/Jul/23 08:46,06/Apr/20 00:40,2.4.4,,,,,,,,,,,,3.0.0,,,ML,PySpark,,,0,,,"Using VectorAssembler after a Pandas GROUPED_AGG and join causes an opaque error:

Caused by: java.lang.UnsupportedOperationException: Cannot evaluate expression: apply_impl(input[1, struct<t:bigint,val:bigint>, true].val)

However, inserting a .cache() between the VectorAssembler and join seems to prevent VectorAssembler & Pandas UDF from interacting to cause this error.

 

{{E py4j.protocol.Py4JJavaError: An error occurred while calling o259.collectToPython.}}
{{E : org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:}}
{{E Exchange hashpartitioning(foo_id_SummaryAggregator_AOG2FHR#34L, 4)}}
{{E +- *(4) Filter AtLeastNNulls(n, apply_impl(foo_explode_SummaryAggregator_AOG2FHR#20.val),apply_impl(foo_explode_SummaryAggregator_AOG2FHR#20.val))}}
{{E +- Generate explode(foo#11), [foo_id_SummaryAggregator_AOG2FHR#34L], true, [foo_explode_SummaryAggregator_AOG2FHR#20]}}
{{E +- *(3) Project [foo#11, monotonically_increasing_id() AS foo_id_SummaryAggregator_AOG2FHR#34L]}}
{{E +- Scan ExistingRDD[foo#11,id#12L]}}
{{E }}
{{E at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)}}
{{E at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)}}
{{E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)}}
{{E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)}}
{{E at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)}}
{{E at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)}}
{{E at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)}}
{{E at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)}}
{{E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)}}
{{E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)}}
{{E at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)}}
{{E at org.apache.spark.sql.execution.python.AggregateInPandasExec.doExecute(AggregateInPandasExec.scala:80)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)}}
{{E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)}}
{{E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)}}
{{E at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)}}
{{E at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)}}
{{E at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)}}
{{E at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)}}
{{E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)}}
{{E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)}}
{{E at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)}}
{{E at org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:383)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)}}
{{E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)}}
{{E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)}}
{{E at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)}}
{{E at org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:386)}}
{{E at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)}}
{{E at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)}}
{{E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)}}
{{E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)}}
{{E at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)}}
{{E at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)}}
{{E at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)}}
{{E at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)}}
{{E at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)}}
{{E at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)}}
{{E at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)}}
{{E at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)}}
{{E at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)}}
{{E at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)}}
{{E at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)}}
{{E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)}}
{{E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)}}
{{E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)}}
{{E at java.lang.reflect.Method.invoke(Method.java:498)}}
{{E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)}}
{{E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)}}
{{E at py4j.Gateway.invoke(Gateway.java:282)}}
{{E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)}}
{{E at py4j.commands.CallCommand.execute(CallCommand.java:79)}}
{{E at py4j.GatewayConnection.run(GatewayConnection.java:238)}}
{{E at java.lang.Thread.run(Thread.java:748)}}
{{E Caused by: java.lang.UnsupportedOperationException: Cannot evaluate expression: apply_impl(input[1, struct<t:bigint,val:bigint>, true].val)}}
{{E at org.apache.spark.sql.catalyst.expressions.Unevaluable$class.doGenCode(Expression.scala:261)}}
{{E at org.apache.spark.sql.catalyst.expressions.PythonUDF.doGenCode(PythonUDF.scala:50)}}
{{E at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)}}
{{E at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)}}
{{E at scala.Option.getOrElse(Option.scala:121)}}
{{E at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)}}
{{E at org.apache.spark.sql.catalyst.expressions.AtLeastNNonNulls$$anonfun$4.apply(nullExpressions.scala:402)}}
{{E at org.apache.spark.sql.catalyst.expressions.AtLeastNNonNulls$$anonfun$4.apply(nullExpressions.scala:401)}}
{{E at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)}}
{{E at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)}}
{{E at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)}}
{{E at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)}}
{{E at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)}}
{{E at scala.collection.AbstractTraversable.map(Traversable.scala:104)}}
{{E at org.apache.spark.sql.catalyst.expressions.AtLeastNNonNulls.doGenCode(nullExpressions.scala:401)}}
{{E at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)}}
{{E at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)}}
{{E at scala.Option.getOrElse(Option.scala:121)}}
{{E at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)}}
{{E at org.apache.spark.sql.execution.FilterExec.org$apache$spark$sql$execution$FilterExec$$genPredicate$1(basicPhysicalOperators.scala:139)}}
{{E at org.apache.spark.sql.execution.FilterExec$$anonfun$13.apply(basicPhysicalOperators.scala:179)}}
{{E at org.apache.spark.sql.execution.FilterExec$$anonfun$13.apply(basicPhysicalOperators.scala:163)}}
{{E at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)}}
{{E at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)}}
{{E at scala.collection.immutable.List.foreach(List.scala:392)}}
{{E at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)}}
{{E at scala.collection.immutable.List.map(List.scala:296)}}
{{E at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:163)}}
{{E at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)}}
{{E at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:374)}}
{{E at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:403)}}
{{E at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)}}
{{E at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)}}
{{E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)}}
{{E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)}}
{{E at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)}}
{{E at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:374)}}
{{E at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:125)}}
{{E at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)}}
{{E at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)}}
{{E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)}}
{{E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)}}
{{E at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)}}
{{E at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:85)}}
{{E at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)}}
{{E at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)}}
{{E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)}}
{{E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)}}
{{E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)}}
{{E at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)}}
{{E at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)}}
{{E at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)}}
{{E at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)}}
{{E at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)}}
{{E ... 69 more}}","numpy==1.16.4
pandas==0.23.4
py4j==0.10.7
pyarrow==0.8.0
pyspark==2.4.4
scikit-learn==0.19.1
scipy==1.1.0",tkellogg,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/20 00:35;tkellogg;test_dyn_pandas_function.py;https://issues.apache.org/jira/secure/attachment/12994151/test_dyn_pandas_function.py",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 06 00:40:16 UTC 2020,,,,,,,,,,"0|z0brvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/20 00:40;gurwls223;Fixed in https://github.com/apache/spark/pull/28089;;;",,,,,,,,,,,,,,,,,,,,,,,
Thrift RowBasedSet serialization throws NullPointerException on NULL BigDecimal,SPARK-30904,13286539,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cjstuart,cjstuart,cjstuart,20/Feb/20 18:22,26/Feb/20 03:37,13/Jul/23 08:46,22/Feb/20 04:50,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"Adding the following test to {{SparkThriftServerProtocolVersionsSuite}} reproduces the issue:
{code:scala}
test(s""$version get null as decimal"") {
  testExecuteStatementWithProtocolVersion(version,
    ""SELECT cast(null as decimal)"") { rs =>
    assert(rs.next())
    assert(rs.getBigDecimal(1) === null)
  }
}{code}

The bug was introduced in https://github.com/apache/spark/commit/163f4a4",,cjstuart,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 22 04:46:41 UTC 2020,,,,,,,,,,"0|z0bpi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/20 04:46;yumwang;Issue resolved by pull request 27654
https://github.com/apache/spark/pull/27654;;;",,,,,,,,,,,,,,,,,,,,,,,
Fail fast on duplicate columns when analyze columns,SPARK-30903,13286516,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,20/Feb/20 16:03,24/Feb/20 01:42,13/Jul/23 08:46,23/Feb/20 00:56,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,We should fail fast on duplicate columns when analyze columns to avoid duplicate computation on the column.,,maropu,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 23 00:56:42 UTC 2020,,,,,,,,,,"0|z0bpd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/20 00:56;maropu;Resolved by [https://github.com/apache/spark/pull/27651];;;",,,,,,,,,,,,,,,,,,,,,,,
default table provider should be decided by catalog implementations,SPARK-30902,13286505,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,20/Feb/20 15:34,12/Dec/22 18:10,13/Jul/23 08:46,28/Feb/20 06:15,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 28 06:15:06 UTC 2020,,,,,,,,,,"0|z0bpao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/20 06:15;gurwls223;Issue resolved by pull request 27650
[https://github.com/apache/spark/pull/27650];;;",,,,,,,,,,,,,,,,,,,,,,,
Expressions should not change its data type/nullability after it's created,SPARK-30893,13286461,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,cloud_fan,cloud_fan,20/Feb/20 12:37,12/Dec/22 18:10,13/Jul/23 08:46,11/Mar/20 02:28,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"This is a problem because the configuration can change between different phases of planning, and this can silently break a query plan which can lead to crashes or data corruption, if data type/nullability gets changed.",,anuragmantri,cloud_fan,dongjoon,maropu,petertoth,rakson,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 15 04:16:43 UTC 2020,,,,,,,,,,"0|z0bp0w:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"21/Feb/20 05:51;cloud_fan;My assumption is: after you create a `df`, calling `df.collect` multiple times should always return the same result even if you change some configs.

If this is not what we expect, then we should only fix the data type changes and leave the beavior ones.

cc [~hvanhovell] [~viirya] [~maropu] what's your opinion?;;;","21/Feb/20 06:14;maropu;Yea, I've already seen someone opening PRs for this issue and the idea looks pretty reasonable to me..... we shouldn't change results in that case because the change looks error-prone in applications. But, I think this change has a big impact on the existing Spark users. We can simply change the behaviour even on major release, e.g., 3.0?;;;","21/Feb/20 07:12;viirya;hmm, an action like `df.collect` should re-evaluate if it is not cached, right? If so, changing a config causing the results different sounds reasonable too? Although I agree that it is error-prone in practice. We should avoid inconsistency among different SQL configs, for example some configs can change behaviors across `df.collect` calls but some are not. If there is a consistent rule about whether SQL configs can change behaviors across different calls of same action, it is better.;;;","21/Feb/20 07:13;cloud_fan;For data type and nullability, I think we should fix before 3.0, as they can lead to data corruption.

For other behaviors, we can have more discussion and wait for 3.1;;;","21/Feb/20 14:05;maropu;Ah, I see. As for the second case, I think we need to share the idea in the dev-mailing list and collect more info. to minimize the impact on applications. As viirya said above, the consistent behaviour is important, so we need to carefully check behaviours config-by-config...;;;","24/Feb/20 07:43;gurwls223;I am sure there are already multiple inconsistent instances out there. Probably some configurations would need more destructive fixes. Are they worthwhile? I am not sure.
It seems to me a bit unlikely users set a different configurations that change behaviours between queries.
For these data type related instances, they look easy to fix so probably fine for now. I am not so supportive of fixing other instances.;;;","12/Oct/20 19:55;anuragmantri;As mentioned above in [~cloud_fan]'s comment, should we backport the nullability and datatype issues from this umbrella to branch-2.4 as they may cause corruption? 

CC: [~viirya], [~dongjoon];;;","12/Oct/20 22:35;dongjoon;[~anuragmantri]. Which comment are you referring specifically? (cc [~dbtsai]);;;","12/Oct/20 22:41;anuragmantri;Sorry for not being clear. I was referring to this comment.

 

??For data type and nullability, I think we should fix before 3.0, as they can lead to data corruption.??

??For other behaviors, we can have more discussion and wait for 3.1??;;;","12/Oct/20 23:40;maropu;You find any data corruption in branch-2.4? If so, I think we should backport the PRs that are necessary to fix it.;;;","13/Oct/20 00:37;anuragmantri;[~maropu], [~dongjoon] - I went through the PRs for individual issues in this Umbrella and looked at the code changes. Only [SPARK-30894|https://issues.apache.org/jira/browse/SPARK-30894] seems to affect branch-2.4. I've commented on that Jira separately asking the original author if we can backport this to branch-2.4. ;;;","15/Oct/20 04:16;dongjoon;Thank you for investigation, [~anuragmantri].;;;",,,,,,,,,,,,
V1 table name should be fully qualified if catalog name is provided,SPARK-30885,13286385,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,imback82,imback82,20/Feb/20 06:43,04/Mar/20 10:10,13/Jul/23 08:46,25/Feb/20 05:39,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"For the following example,

{code:java}
sql(""CREATE TABLE t USING json AS SELECT 1 AS i"")
sql(""SELECT * FROM spark_catalog.t"")
{code}
`spark_catalog.t` is expanded to `spark_catalog.default.t` assuming that the current namespace is `default`. However, this is not consistent with V2 behavior where namespace should be provided if the catalog name is also provided.
",,cloud_fan,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 25 05:39:27 UTC 2020,,,,,,,,,,"0|z0bok0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/20 05:39;cloud_fan;Issue resolved by pull request 27642
[https://github.com/apache/spark/pull/27642];;;",,,,,,,,,,,,,,,,,,,,,,,
Upgrade to Py4J 0.10.9,SPARK-30884,13286383,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,20/Feb/20 05:41,20/Feb/20 17:11,13/Jul/23 08:46,20/Feb/20 17:10,3.0.0,,,,,,,,,,,,3.0.0,,,PySpark,,,,0,,,"This issue aims to upgrade Py4J from 0.10.8.1 to 0.10.9.
Py4J 0.10.9 is released with the following fixes.
- https://www.py4j.org/changelog.html#py4j-0-10-9",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25891,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 20 17:10:15 UTC 2020,,,,,,,,,,"0|z0bojk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/20 17:10;dongjoon;Issue resolved by pull request 27641
[https://github.com/apache/spark/pull/27641];;;",,,,,,,,,,,,,,,,,,,,,,,
Throw Exception if runHive(sql) failed,SPARK-30868,13286041,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Jackey Lee,Jackey Lee,Jackey Lee,18/Feb/20 17:25,28/Apr/20 08:07,13/Jul/23 08:46,28/Apr/20 08:05,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"At present, HiveClientImpl.runHive will not throw an exception when it runs incorrectly, which will cause it to fail to feedback error information normally.
Example
{code:scala}
spark.sql(""add jar file:///tmp/test.jar"")
spark.sql(""show databases"").show()
{code}
/tmp/test.jar doesn't exist, thus add jar is failed. However this code will run completely without causing application failure.",,Ankitraj,Jackey Lee,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 28 08:07:31 UTC 2020,,,,,,,,,,"0|z0bmfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/20 04:35;Ankitraj;i am working on this;;;","19/Feb/20 06:58;Ankitraj;[~Jackey Lee], you mean after add jar failed .show() statement also should throw exception ?;;;","20/Feb/20 01:49;Jackey Lee;[~Ankitraj] No, I think we should just throw Exception once hive run statement failed. Otherwise, the user cannot detect that this statement has failed.

Any jira you have created for this?;;;","20/Feb/20 09:30;Ankitraj;no i have not created any jira.;;;","20/Feb/20 09:42;Ankitraj;[~srowen] and [~cloud_fan] please give your suggestion for this jira, according to me current behaviour is correct.;;;","20/Feb/20 10:06;Jackey Lee;So your opinion is to return normal results even if the statement runs wrong? A bit weird, it is really unamiable.;;;","20/Feb/20 10:40;Ankitraj;According to me user should handle Exception case,during exception handling they can terminate application or can write log and continue further execution.

Anyhow you already raised a PR let them review then it will clear to us. ;;;","20/Feb/20 11:18;Jackey Lee;Can't agree more. It is up to the user to choose how to handle the exception. What Spark does is to inform the user about the exception, not to mask it.;;;","21/Feb/20 10:04;Ankitraj;[~Jackey Lee], your point is correct , Thank you for raising pr :) ;;;","28/Apr/20 08:07;yumwang;Issue resolved by pull request 27644
https://github.com/apache/spark/pull/27644;;;",,,,,,,,,,,,,,
Docstring syntax issues prevent proper compilation of documentation,SPARK-30859,13285840,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,DavidToneian,DavidToneian,17/Feb/20 19:16,12/Dec/22 18:11,13/Jul/23 08:46,18/Feb/20 07:48,2.4.5,,,,,,,,,,,,3.0.0,,,Documentation,,,,0,,,"Some docstrings contain mistakes, like missing newlines or spurious spaces, which prevent the documentation from being rendered as intended.",,DavidToneian,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Tue Feb 18 07:48:12 UTC 2020,,,,,,,,,,"0|z0bl6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/20 19:25;DavidToneian;I have proposed a fix in [PR #27613|https://github.com/apache/spark/pull/27613].;;;","18/Feb/20 07:48;gurwls223;Fixed in https://github.com/apache/spark/pull/27613;;;",,,,,,,,,,,,,,,,,,,,,,
Wrong truncations of timestamps before the epoch to hours and days,SPARK-30857,13285830,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,17/Feb/20 17:24,18/Feb/20 20:19,13/Jul/23 08:46,18/Feb/20 13:31,2.3.0,2.3.4,2.4.5,,,,,,,,,,2.4.6,,,SQL,,,,0,correctness,,"Truncations to seconds and minutes of timestamps after the epoch are correct:
{code:sql}
spark-sql> select date_trunc('HOUR', '2020-02-11 00:01:02.123'), date_trunc('HOUR', '2020-02-11 00:01:02.789');
2020-02-11 00:00:00	2020-02-11 00:00:00
{code}
but truncations of timestamps before the epoch are incorrect:
{code:sql}
spark-sql> select date_trunc('HOUR', '1960-02-11 00:01:02.123'), date_trunc('HOUR', '1960-02-11 00:01:02.789');
1960-02-11 01:00:00	1960-02-11 01:00:00
{code}
The result must be *1960-02-11 00:00:00 1960-02-11 00:00:00*

The same for the DAY level:
{code:sql}
spark-sql> select date_trunc('DAY', '1960-02-11 00:01:02.123'), date_trunc('DAY', '1960-02-11 00:01:02.789');
1960-02-12 00:00:00	1960-02-12 00:00:00
{code}
The result must be *1960-02-11 00:00:00 1960-02-11 00:00:00*",,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22829,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 18 13:31:49 UTC 2020,,,,,,,,,,"0|z0bl4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/20 13:31;cloud_fan;Issue resolved by pull request 27612
[https://github.com/apache/spark/pull/27612];;;",,,,,,,,,,,,,,,,,,,,,,,
SQLContext retains reference to unusable instance after SparkContext restarted,SPARK-30856,13285813,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,afavaro,afavaro,afavaro,17/Feb/20 16:26,12/Dec/22 18:10,13/Jul/23 08:46,20/Feb/20 03:22,2.4.5,,,,,,,,,,,,3.1.0,,,PySpark,SQL,,,0,,,"When the underlying SQLContext is instantiated for a SparkSession, the instance is saved as a class attribute and returned from subsequent calls to SQLContext.getOrCreate(). If the SparkContext is stopped and a new one started, the SQLContext class attribute is never cleared so any code which calls SQLContext.getOrCreate() will get a SQLContext with a reference to the old, unusable SparkContext.

A similar issue was identified and fixed for SparkSession in SPARK-19055, but the fix did not change SQLContext as well. I ran into this because mllib still [uses|https://github.com/apache/spark/blob/master/python/pyspark/mllib/common.py#L105] SQLContext.getOrCreate() under the hood.

I've already written a fix for this, which I'll be sharing in a PR, that clears the class attribute on SQLContext when the SparkSession is stopped. Another option would be to deprecate SQLContext.getOrCreate() entirely since the corresponding Scala [method|https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/SQLContext.html#getOrCreate-org.apache.spark.SparkContext-] is itself deprecated. That seems like a larger change for a relatively minor issue, however.",,afavaro,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 20 03:22:48 UTC 2020,,,,,,,,,,"0|z0bl0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/20 03:22;gurwls223;Issue resolved by pull request 27610
[https://github.com/apache/spark/pull/27610];;;",,,,,,,,,,,,,,,,,,,,,,,
Take productPrefix into account in MurmurHash3.productHash,SPARK-30847,13285671,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gurwls223,gurwls223,,17/Feb/20 04:25,12/Dec/22 17:51,13/Jul/23 08:46,18/Feb/20 06:53,2.4.5,3.0.0,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"See the issue in https://github.com/scala/bug/issues/10495 and fix https://github.com/scala/scala/pull/7648.

Currently, different expressions with same children can produce the same hash as below:

{code}
scala> spark.range(1).selectExpr(""id - 1"").queryExecution.analyzed.semanticHash()
res0: Int = -565572825

scala> spark.range(1).selectExpr(""id + 1"").queryExecution.analyzed.semanticHash()
res1: Int = -565572825
{code}

The reason seems to be it doesn't take the product's class itself",,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30848,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 18 06:53:19 UTC 2020,,,,,,,,,,"0|z0bk5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/20 06:53;cloud_fan;Issue resolved by pull request 27601
[https://github.com/apache/spark/pull/27601];;;",,,,,,,,,,,,,,,,,,,,,,,
spark-submit pyspark app on yarn uploads local pyspark archives,SPARK-30845,13285648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shanyu,shanyu,shanyu,16/Feb/20 20:59,08/Jun/20 20:56,13/Jul/23 08:46,08/Jun/20 20:56,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,2.4.5,,,,,,,3.1.0,,,Spark Submit,,,,0,,,"Use spark-submit to submit a pyspark app on Yarn, and set this in spark-env.sh:

{code:bash}
export PYSPARK_ARCHIVES_PATH=local:/opt/spark/python/lib/pyspark.zip,local:/opt/spark/python/lib/py4j-0.10.7-src.zip
{code}

You can see that these local archives are still uploaded to Yarn distributed cache.

yarn.Client: Uploading resource file:/opt/spark/python/lib/pyspark.zip -> hdfs://myhdfs/user/test1/.sparkStaging/application_1581024490249_0001/pyspark.zip
",,shanyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-02-16 20:59:12.0,,,,,,,,,,"0|z0bk08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong results of getting time components before 1582 year,SPARK-30843,13285559,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,16/Feb/20 07:58,19/Feb/20 05:18,13/Jul/23 08:46,17/Feb/20 09:01,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Here are example that show the issues:
{code:sql}
spark-sql> select hour(timestamp '0010-01-01 00:00:00');
23
spark-sql> select minute(timestamp '0010-01-01 00:00:00');
52
spark-sql> select second(timestamp '0010-01-01 00:00:00');
58
spark-sql> select date_part('milliseconds', timestamp '0010-01-01 00:00:00');
58000.000
spark-sql> select date_part('microseconds', timestamp '0010-01-01 00:00:00');
58000000
{code}
The expected results must be:
* hour = 0
* minute = 0
* second = 0
* milliseconds = 0
* microseconds = 0",,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 17 09:01:59 UTC 2020,,,,,,,,,,"0|z0bjgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/20 09:01;cloud_fan;Issue resolved by pull request 27596
[https://github.com/apache/spark/pull/27596];;;",,,,,,,,,,,,,,,,,,,,,,,
spark-master-test-k8s is broken,SPARK-30837,13285436,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,dongjoon,dongjoon,14/Feb/20 21:53,12/Dec/22 18:10,13/Jul/23 08:46,20/Feb/20 01:46,3.1.0,,,,,,,,,,,,,,,Kubernetes,Project Infra,Spark Core,,0,,,"- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20K8s%20Builds/job/spark-master-test-k8s/471/console

{code}
+ /home/jenkins/bin/session_lock_resource.py minikube
  File ""/home/jenkins/bin/session_lock_resource.py"", line 140
    child_body_func = lambda(success_callback): _lock_and_wait(
                            ^
SyntaxError: invalid syntax
{code}",,dongjoon,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 20 01:46:42 UTC 2020,,,,,,,,,,"0|z0bip4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/20 21:58;dongjoon;Hi, [~shaneknapp].
`session_lock_resource.py` seems to be outside our source repository. Could you fix that? Maybe, it's Python 2 vs 3 issue?
Also, cc [~hyukjin.kwon];;;","14/Feb/20 22:01;shaneknapp;yeah it's on my list of things to add to the jenkins infra repo.  i'll fix it now.;;;","14/Feb/20 22:25;shaneknapp;and fixed...  i triggered a job, too:

[https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-k8s/472/];;;","15/Feb/20 00:12;dongjoon;Thank you! :D;;;","20/Feb/20 01:46;gurwls223;Thanks! I see green light :-). resolving it.;;;",,,,,,,,,,,,,,,,,,,
LIKE returns wrong result from external table using parquet,SPARK-30826,13285260,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,14/Feb/20 06:24,24/Feb/20 22:35,13/Jul/23 08:46,15/Feb/20 12:13,2.0.2,2.1.3,2.2.3,2.3.4,2.4.5,3.0.0,,,,,,,2.4.6,3.0.0,,SQL,,,,0,correctness,,"# Write a parquet file with a column in upper case:
{code:scala}
Seq(""42"").toDF(""COL"").write.parquet(path)
{code}
# Create an external table on top of the written parquet files with a column in lower case
{code:sql}
CREATE TABLE t1 (col STRING)
USING parquet
OPTIONS (path '$path')
{code}
# Read the table using LIKE
{code:sql}
SELECT * FROM t1 WHERE col LIKE '4%'
{code}
It returns empty set but must be one row with 42.
",,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 15 12:13:38 UTC 2020,,,,,,,,,,"0|z0bhm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/20 12:13;cloud_fan;Issue resolved by pull request 27574
[https://github.com/apache/spark/pull/27574];;;",,,,,,,,,,,,,,,,,,,,,,,
"%PYTHONPATH% not set in python/docs/make2.bat, resulting in failed/wrong documentation builds",SPARK-30823,13285193,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,DavidToneian,DavidToneian,DavidToneian,13/Feb/20 21:06,07/May/20 17:59,13/Jul/23 08:46,15/Feb/20 22:23,2.4.5,,,,,,,,,,,,2.4.6,,,Documentation,PySpark,Windows,,0,,,"When building the PySpark documentation on Windows, by changing directory to {{python/docs}} and running {{make.bat}} (which runs {{make2.bat}}), the majority of the documentation may not be built if {{pyspark}} is not in the default {{%PYTHONPATH%}}. Sphinx then reports that {{pyspark}} (and possibly dependencies) cannot be imported.

If {{pyspark}} is in the default {{%PYTHONPATH%}}, I suppose it is that version of {{pyspark}} – as opposed to the version found above the {{python/docs}} directory – that is considered when building the documentation, which may result in documentation that does not correspond to the development version one is trying to build.

{{python/docs/Makefile}} avoids this issue by setting
 ??export PYTHONPATH=$(realpath ..):$(realpath ../lib/py4j-0.10.8.1-src.zip)??
 on line 10, but {{make2.bat}} does no such thing. The fix consist of adding
 ??set PYTHONPATH=..;..\lib\py4j-0.10.8.1-src.zip??
 to {{make2.bat}}.

See [GitHub PR #27569|https://github.com/apache/spark/pull/27569].",Tested on Windows 10.,DavidToneian,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 15 22:23:23 UTC 2020,,,,,,,,,,"0|z0bh74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/20 22:23;DavidToneian;Merged in [{{b2134ee}}|https://github.com/apache/spark/commit/b2134ee73cfad4d78aaf8f0a2898011ac0308e48].;;;",,,,,,,,,,,,,,,,,,,,,,,
Pyspark queries fail if terminated with a semicolon,SPARK-30822,13285191,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,samsetegne,samsetegne,samsetegne,13/Feb/20 20:54,25/Mar/20 07:55,13/Jul/23 08:46,25/Mar/20 07:50,3.0.0,,,,,,,,,,,,3.0.0,,,PySpark,SQL,,,0,,,"When a user submits a directly executable SQL statement terminated with a semicolon, they receive a `org.apache.spark.sql.catalyst.parser.ParseException` of `mismatched input "";""`. SQL-92 describes a direct SQL statement as having the format of `<directly executable statement> <semicolon>` and the majority of SQL implementations either require the semicolon as a statement terminator, or make it optional (meaning not raising an exception when it's included, seemingly in recognition that it's a common behavior).",,cloud_fan,samsetegne,,,,,,,,,,,,,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,SPARK-24260,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 07:50:32 UTC 2020,,,,,,,,,,"0|z0bh6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/20 07:50;cloud_fan;Issue resolved by pull request 27567
[https://github.com/apache/spark/pull/27567];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix dev-run-integration-tests.sh to ignore empty param correctly,SPARK-30816,13285166,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dongjoon,dongjoon,dongjoon,13/Feb/20 19:01,17/May/20 18:26,13/Jul/23 08:46,13/Feb/20 19:42,2.4.5,3.0.0,,,,,,,,,,,3.0.0,,,Kubernetes,Spark Core,Tests,,0,,,,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 13 19:42:32 UTC 2020,,,,,,,,,,"0|z0bh14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/20 19:42;dongjoon;Issue resolved by pull request 27566
[https://github.com/apache/spark/pull/27566];;;",,,,,,,,,,,,,,,,,,,,,,,
Add Columns references should be able to resolve each other,SPARK-30814,13285156,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,brkyvz,brkyvz,13/Feb/20 18:43,19/Feb/20 08:46,13/Jul/23 08:46,19/Feb/20 05:43,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"In ResolveAlterTableChanges, we have checks that make sure that positional arguments exist and are normalized around case sensitivity for ALTER TABLE ADD COLUMNS. However, we missed the case, where a column in ADD COLUMNS can depend on the position of a column that is just being added.

For example for the schema:
{code:java}
root:
  - a: string
  - b: long
 {code}
 

The following should work:
{code:java}
ALTER TABLE ... ADD COLUMNS (x int AFTER a, y int AFTER x) {code}
Currently, the above statement will throw an error saying that AFTER x cannot be resolved, because x doesn't exist yet.",,brkyvz,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 13 19:02:59 UTC 2020,,,,,,,,,,"0|z0bgyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/20 18:44;brkyvz;cc [~cloud_fan] [~imback82], can we prioritize this over REPLACE COLUMNS if possible?;;;","13/Feb/20 19:02;imback82;Yea, I can work on this.;;;",,,,,,,,,,,,,,,,,,,,,,
Matrices.sprand mistakes in comments ,SPARK-30813,13285109,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,xiaochang-wu,xiaochang-wu,xiaochang-wu,13/Feb/20 15:03,02/Mar/20 14:57,13/Jul/23 08:46,02/Mar/20 14:57,3.0.0,,,,,,,,,,,,2.4.6,3.0.0,,ML,,,,0,,,"/**
 * Generate a `SparseMatrix` consisting of `i.i.d.` *gaussian random* numbers.

>> *should be ""uniform random"" here*

* @param numRows number of rows of the matrix
 * @param numCols number of columns of the matrix
 * @param density the desired density for the matrix
 * @param rng a random number generator
 * @return `Matrix` with size `numRows` x `numCols` and values in U(0, 1)
 */
 @Since(""2.0.0"")
 def sprand(numRows: Int, numCols: Int, density: Double, rng: Random): Matrix =
 SparseMatrix.sprand(numRows, numCols, density, rng)",,xiaochang-wu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 14:57:30 UTC 2020,,,,,,,,,,"0|z0bgog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/20 14:57;srowen;Issue resolved by pull request 27564
[https://github.com/apache/spark/pull/27564];;;",,,,,,,,,,,,,,,,,,,,,,,
CTE that refers to non-existent table with same name causes StackOverflowError,SPARK-30811,13285088,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,13/Feb/20 13:45,19/Feb/20 18:19,13/Jul/23 08:46,18/Feb/20 22:03,2.0.2,2.1.3,2.2.3,2.3.4,2.4.5,,,,,,,,2.4.6,,,SQL,,,,0,,,"The following query causes a StackOverflowError:
{noformat}
WITH t AS (SELECT 1 FROM nonexist.t) SELECT * FROM t
{noformat}

This only happens when the CTE refers to a non-existent table with the same name and a database qualifier. This is caused by a couple of things:
 * {{CTESubstitution}} runs analysis on the CTE, but this does not throw an exception because the table has a database qualifier. The reason is that we don't fail is because we re-attempt to resolve the relation in a later rule.
 * {{CTESubstitution}} replace logic does not check if the table it is replacing has a database, it shouldn't replace the relation if it does. So now we will happily replace {{nonexist.t}} with {{t}}.
 * {{CTESubstitution}} transforms down, this means it will keep replacing {{t}} with itself, creating an infinite recursion.

This is not an issue for master/3.0.",,dongjoon,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 19 18:19:17 UTC 2020,,,,,,,,,,"0|z0bgjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/20 22:03;dongjoon;Issue resolved by pull request 27562
[https://github.com/apache/spark/pull/27562];;;","19/Feb/20 18:19;dongjoon;The test cases are landed to `master/branch-3.0` to prevent future regression.
- https://github.com/apache/spark/pull/27635;;;",,,,,,,,,,,,,,,,,,,,,,
Allows to parse a Dataset having different column from 'value' in csv(dataset) API,SPARK-30810,13285055,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,13/Feb/20 11:20,12/Dec/22 17:51,13/Jul/23 08:46,14/Feb/20 10:37,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"{code}
val ds = spark.range(10).selectExpr(""'a, b, c' AS text"").as[String]
spark.read.csv(ds).show()
{code}

{code}
org.apache.spark.sql.AnalysisException: cannot resolve '`value`' given input columns: [text];;
'Filter (length(trim('value, None)) > 0)
+- Project [a, b, c AS text#2]
   +- Range (0, 10, step=1, splits=Some(2))
{code}

It fails to create a CSV parsed DataFrame from a String Dataset.
",,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 14 10:37:20 UTC 2020,,,,,,,,,,"0|z0bgcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/20 10:37;cloud_fan;Issue resolved by pull request 27561
[https://github.com/apache/spark/pull/27561];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix the home page link to Scala API document,SPARK-30803,13284950,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,XuanYuan,dongjoon,dongjoon,13/Feb/20 01:19,24/Dec/20 03:46,13/Jul/23 08:46,16/Feb/20 16:00,3.0.0,,,,,,,,,,,,3.0.0,,,Documentation,,,,0,,,This only happens in 3.0.0.,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 16 14:20:10 UTC 2020,,,,,,,,,,"0|z0bfp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/20 16:00;srowen;Issue resolved by pull request 27549
[https://github.com/apache/spark/pull/27549];;;","16/Dec/20 14:19;apachespark;User 'kozakana' has created a pull request for this issue:
https://github.com/apache/spark/pull/30803;;;","16/Dec/20 14:20;apachespark;User 'kozakana' has created a pull request for this issue:
https://github.com/apache/spark/pull/30803;;;",,,,,,,,,,,,,,,,,,,,,
"""spark_catalog.t"" should not be resolved to temp view",SPARK-30799,13284832,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,12/Feb/20 14:57,19/Feb/20 05:20,13/Jul/23 08:46,17/Feb/20 04:19,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 17 04:19:33 UTC 2020,,,,,,,,,,"0|z0beyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/20 04:19;cloud_fan;Issue resolved by pull request 27550
[https://github.com/apache/spark/pull/27550];;;",,,,,,,,,,,,,,,,,,,,,,,
Set tradition user/group/other permission to ACL entries when setting up ACLs in truncate table,SPARK-30797,13284762,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,12/Feb/20 09:24,13/Feb/20 01:50,13/Jul/23 08:46,12/Feb/20 22:28,2.4.5,3.0.0,,,,,,,,,,,2.4.6,3.0.0,,SQL,,,,0,,,"This is a follow-up to the SPARK-30312. In SPARK-30312, the patch proposed to preserve path permission when truncating table. When setting up original ACLs, we need to set user/group/other permission as ACL entries too, otherwise if the path doesn't have default user/group/other ACL entries, ACL API will complain an error Invalid ACL: the user, group and other entries are required.",,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30312,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 12 22:28:25 UTC 2020,,,,,,,,,,"0|z0bejc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/20 22:28;dongjoon;Issue resolved by pull request 27548
[https://github.com/apache/spark/pull/27548];;;",,,,,,,,,,,,,,,,,,,,,,,
Spark SQL codegen's code() interpolator should treat escapes like Scala's StringContext.s(),SPARK-30795,13284702,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,12/Feb/20 01:17,12/Dec/22 18:11,13/Jul/23 08:46,12/Feb/20 06:21,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,2.4.5,3.0.0,,,,,,3.1.0,,,SQL,,,,0,,,"The {{code()}} string interpolator in Spark SQL's code generator should treat escapes like Scala's builtin {{StringContext.s()}} interpolator, i.e. it should treat escapes in the code parts, and should not treat escapes in the input arguments.

For example,
{code}
val arg = ""This is an argument.""
val str = s""This is string part 1. $arg This is string part 2.""
val code = code""This is string part 1. $arg This is string part 2.""
assert(code.toString == str)
{code}
We should expect the {{code()}} interpolator produce the same thing as the {{StringContext.s()}} interpolator, where only escapes in the string parts should be treated, while the args should be kept verbatim.

But in the current implementation, due to the eager folding of code parts and literal input args, the escape treatment is incorrectly done on both code parts and literal args.
That causes a problem when an arg contains escape sequences and wants to preserve that in the final produced code string. For example, in {{Like}} expression's codegen, there's an ugly workaround for this bug:
{code}
      // We need double escape to avoid org.codehaus.commons.compiler.CompileException.
      // '\\' will cause exception 'Single quote must be backslash-escaped in character literal'.
      // '\""' will cause exception 'Line break in literal not allowed'.
      val newEscapeChar = if (escapeChar == '\""' || escapeChar == '\\') {
        s""""""\\\\\\$escapeChar""""""
      } else {
        escapeChar
      }
{code}",,rednaxelafx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 12 06:21:58 UTC 2020,,,,,,,,,,"0|z0be60:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"12/Feb/20 06:21;gurwls223;Fixed in https://github.com/apache/spark/pull/27544;;;",,,,,,,,,,,,,,,,,,,,,,,
Wrong truncations of timestamps before the epoch to minutes and seconds,SPARK-30793,13284639,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,11/Feb/20 18:21,18/Feb/20 04:29,13/Jul/23 08:46,17/Feb/20 15:15,2.3.0,2.3.4,2.4.5,3.0.0,,,,,,,,,2.4.6,3.0.0,,SQL,,,,0,correctness,,"Truncations to seconds and minutes of timestamps after the epoch are correct:
{code:sql}
spark-sql> select date_trunc('SECOND', '2020-02-11 00:01:02.123'), date_trunc('SECOND', '2020-02-11 00:01:02.789');
2020-02-11 00:01:02	2020-02-11 00:01:02
{code}
but truncations of timestamps before the epoch are incorrect:
{code:sql}
spark-sql> select date_trunc('SECOND', '1960-02-11 00:01:02.123'), date_trunc('SECOND', '1960-02-11 00:01:02.789');
1960-02-11 00:01:03	1960-02-11 00:01:03
{code}
The result must be *1960-02-11 00:01:02	1960-02-11 00:01:02*

The same for the MINUTE level:
{code:sql}
spark-sql> select date_trunc('MINUTE', '1960-02-11 00:01:01'), date_trunc('MINUTE', '1960-02-11 00:01:50');
1960-02-11 00:02:00	1960-02-11 00:02:00
{code}
The result must be 1960-02-11 00:01:00	1960-02-11 00:01:00",,cloud_fan,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22829,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 17 17:34:18 UTC 2020,,,,,,,,,,"0|z0bds0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/20 15:15;cloud_fan;Issue resolved by pull request 27543
[https://github.com/apache/spark/pull/27543];;;","17/Feb/20 17:34;dongjoon;I linked SPARK-22829 and added `2.3.4` to `Affected Versions` since it's added at 2.3.0.;;;",,,,,,,,,,,,,,,,,,,,,,
Block replication is not retried on other BlockManagers when it fails on 1 of the peers,SPARK-30786,13284513,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prakharjain09,prakharjain09,prakharjain09,11/Feb/20 07:50,19/Feb/20 12:24,13/Jul/23 08:46,19/Feb/20 12:24,2.3.4,2.4.5,3.0.0,,,,,,,,,,3.1.0,,,Block Manager,Spark Core,,,0,,,"When we cache an RDD with replication > 1, Firstly the RDD block is cached locally on one of the BlockManager and then it is replicated to (replication-1) number of BlockManagers. While replicating a block, if replication fails on one of the peers, it is supposed to retry the replication on some other peer (based on ""spark.storage.maxReplicationFailures"" config). But currently this doesn't happen because of some issue.

Logs of 1 of the executor which is trying to replicate:
{noformat}
20/02/10 09:01:47 INFO Executor: Starting executor ID 1 on host wn11-prakha.mvqvy0u1catevlxn5wwhjss34f.bx.internal.cloudapp.net
.
.
.
20/02/10 09:06:45 INFO Executor: Running task 244.0 in stage 3.0 (TID 550)
20/02/10 09:06:45 DEBUG BlockManager: Getting local block rdd_13_244
20/02/10 09:06:45 DEBUG BlockManager: Block rdd_13_244 was not found
20/02/10 09:06:45 DEBUG BlockManager: Getting remote block rdd_13_244
20/02/10 09:06:45 DEBUG BlockManager: Block rdd_13_244 not found
20/02/10 09:06:46 INFO MemoryStore: Block rdd_13_244 stored as values in memory (estimated size 33.3 MB, free 44.2 MB)
20/02/10 09:06:46 DEBUG BlockManager: Told master about block rdd_13_244
20/02/10 09:06:46 DEBUG BlockManager: Put block rdd_13_244 locally took  947 ms
20/02/10 09:06:46 DEBUG BlockManager: Level for block rdd_13_244 is StorageLevel(memory, deserialized, 3 replicas)
20/02/10 09:06:46 TRACE BlockManager: Trying to replicate rdd_13_244 of 34908552 bytes to BlockManagerId(2, wn10-prakha.mvqvy0u1catevlxn5wwhjss34f.bx.internal.cloudapp.net, 36711, None)
20/02/10 09:06:47 TRACE BlockManager: Replicated rdd_13_244 of 34908552 bytes to BlockManagerId(2, wn10-prakha.mvqvy0u1catevlxn5wwhjss34f.bx.internal.cloudapp.net, 36711, None) in 205.849858 ms
20/02/10 09:06:47 TRACE BlockManager: Trying to replicate rdd_13_244 of 34908552 bytes to BlockManagerId(5, wn2-prakha.mvqvy0u1catevlxn5wwhjss34f.bx.internal.cloudapp.net, 36463, None)
20/02/10 09:06:47 TRACE BlockManager: Replicated rdd_13_244 of 34908552 bytes to BlockManagerId(5, wn2-prakha.mvqvy0u1catevlxn5wwhjss34f.bx.internal.cloudapp.net, 36463, None) in 180.501504 ms
20/02/10 09:06:47 DEBUG BlockManager: Replicating rdd_13_244 of 34908552 bytes to 2 peer(s) took 387.381168 ms
20/02/10 09:06:47 DEBUG BlockManager: block rdd_13_244 replicated to BlockManagerId(5, wn2-prakha.mvqvy0u1catevlxn5wwhjss34f.bx.internal.cloudapp.net, 36463, None), BlockManagerId(2, wn10-prakha.mvqvy0u1catevlxn5wwhjss34f.bx.internal.cloudapp.net, 36711, None)
20/02/10 09:06:47 DEBUG BlockManager: Put block rdd_13_244 remotely took  423 ms
20/02/10 09:06:47 DEBUG BlockManager: Putting block rdd_13_244 with replication took  1371 ms
20/02/10 09:06:47 DEBUG BlockManager: Getting local block rdd_13_244
20/02/10 09:06:47 DEBUG BlockManager: Level for block rdd_13_244 is StorageLevel(memory, deserialized, 3 replicas)
20/02/10 09:06:47 INFO Executor: Finished task 244.0 in stage 3.0 (TID 550). 2253 bytes result sent to driver

{noformat}


Logs of other executor where the block is being replicated to:
{noformat}
20/02/10 09:01:47 INFO Executor: Starting executor ID 5 on host wn2-prakha.mvqvy0u1catevlxn5wwhjss34f.bx.internal.cloudapp.net
.
.
.
20/02/10 09:06:47 INFO MemoryStore: Will not store rdd_13_244
20/02/10 09:06:47 WARN MemoryStore: Not enough space to cache rdd_13_244 in memory! (computed 4.2 MB so far)
20/02/10 09:06:47 INFO MemoryStore: Memory use = 4.9 GB (blocks) + 7.3 MB (scratch space shared across 2 tasks(s)) = 4.9 GB. Storage limit = 4.9 GB.
20/02/10 09:06:47 DEBUG BlockManager: Put block rdd_13_244 locally took  12 ms
20/02/10 09:06:47 WARN BlockManager: Block rdd_13_244 could not be removed as it was not found on disk or in memory
20/02/10 09:06:47 WARN BlockManager: Putting block rdd_13_244 failed
20/02/10 09:06:47 DEBUG BlockManager: Putting block rdd_13_244 without replication took  13 ms
{noformat}

Note here that the block replication failed in Executor-5 with log line ""Not enough space to cache rdd_13_244 in memory!"". But Executor-1 shows that block is successfully replicated to executor-5 - ""Replicated rdd_13_244 of 34908552 bytes to BlockManagerId(5, wn2-prakha.mvqvy0u1catevlxn5wwhjss34f.bx.internal.cloudapp.net, 36463, None)"" and so it never retries the replication on some other executor.



Sample code:
{noformat}
sc.setLogLevel(""INFO"")
def randomString(length: Int) = {
  val r = new scala.util.Random
  val sb = new StringBuilder
  for (i <- 1 to length) \{ sb.append(r.nextPrintableChar) }
  sb.toString
}
 
val df = sc.parallelize(1 to 300000, 300).map\{x => randomString(100000)}.toDF
import org.apache.spark.storage.StorageLevel
df.persist(StorageLevel(false, true, false, true, 3))
df.count()
{noformat}",,cloud_fan,mannswinky,prakharjain09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 19 12:24:15 UTC 2020,,,,,,,,,,"0|z0bd00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/20 07:57;prakharjain09;I am working on this.;;;","19/Feb/20 12:24;cloud_fan;Issue resolved by pull request 27539
[https://github.com/apache/spark/pull/27539];;;",,,,,,,,,,,,,,,,,,,,,,
Create table like should keep tracksPartitionsInCatalog same with source table,SPARK-30785,13284499,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,11/Feb/20 06:02,19/Feb/20 07:22,13/Jul/23 08:46,19/Feb/20 07:22,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Table generated by CTL a partitioned table is a partitioned table. But when run ALTER TABLE ADD PARTITION, it will throw AnalysisException: ALTER TABLE ADD PARTITION is not allowed. That's because the default value of {{tracksPartitionsInCatalog}} from CTL always is {{false}}.",,cloud_fan,cltlfcjin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 19 07:22:11 UTC 2020,,,,,,,,,,"0|z0bcww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/20 07:22;cloud_fan;Issue resolved by pull request 27538
[https://github.com/apache/spark/pull/27538];;;",,,,,,,,,,,,,,,,,,,,,,,
Column resolution doesn't respect current catalog/namespace for v2 tables.,SPARK-30782,13284420,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,imback82,imback82,10/Feb/20 22:04,26/Feb/20 18:23,13/Jul/23 08:46,26/Feb/20 16:35,3.0.0,3.1.0,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"For v1 tables, you can perform the following:
{code:sql}
SELECT default.t.id FROM t;
{code}

For v2 tables, the following fails:
{code:sql}
USE testcat.ns1.ns2;
SELECT testcat.ns1.ns2.t.id FROM t;

org.apache.spark.sql.AnalysisException: cannot resolve '`testcat.ns1.ns2.t.id`' given input columns: [t.id, t.point]; line 1 pos 7;
{code}",,cloud_fan,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 26 16:35:37 UTC 2020,,,,,,,,,,"0|z0bcfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/20 16:35;cloud_fan;Issue resolved by pull request 27532
[https://github.com/apache/spark/pull/27532];;;",,,,,,,,,,,,,,,,,,,,,,,
PySpark test_arrow tests fail with Pandas >= 1.0.0,SPARK-30777,13284386,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,bryanc,bryanc,10/Feb/20 19:03,12/Dec/22 18:10,13/Jul/23 08:46,11/Feb/20 01:05,3.0.0,,,,,,,,,,,,3.0.0,,,PySpark,Tests,,,0,,,"Some tests fail using Pandas 1.0.0 and above due to removal of  attr ""ix"" from DataFrame",,bryanc,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33216,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 11 01:05:59 UTC 2020,,,,,,,,,,"0|z0bc7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/20 19:08;dongjoon;Hi, [~bryanc]. Is this a blocker for 3.0.0 release? Then, please set `Target Version` at least.;;;","10/Feb/20 19:18;bryanc;I'm working on the patch;;;","10/Feb/20 19:19;bryanc;[~dongjoon] I don't think it's a blocker, only the tests fail and our minimum pandas version that we test with is 0.23;;;","10/Feb/20 19:34;dongjoon;Got it~;;;","11/Feb/20 01:05;gurwls223;Fixed in https://github.com/apache/spark/pull/27529;;;",,,,,,,,,,,,,,,,,,,
avoid tuple assignment because it will circumvent the transient tag,SPARK-30772,13284259,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,podongfeng,podongfeng,podongfeng,10/Feb/20 11:37,16/Feb/20 16:02,13/Jul/23 08:46,16/Feb/20 16:02,3.1.0,,,,,,,,,,,,3.1.0,,,ML,SQL,,,0,,,tuple assignment to transient object will circumvent the transient tag,,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 16 16:02:26 UTC 2020,,,,,,,,,,"0|z0bbfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/20 16:02;srowen;Issue resolved by pull request 27523
[https://github.com/apache/spark/pull/27523];;;",,,,,,,,,,,,,,,,,,,,,,,
Wrong truncation of old timestamps to hours and days,SPARK-30766,13284155,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,09/Feb/20 17:28,14/Feb/20 14:35,13/Jul/23 08:46,14/Feb/20 14:35,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"The date_trunc() function incorrectly truncates old timestamps to HOUR and DAY:
{code:scala}
scala> import  org.apache.spark.sql.functions._
import org.apache.spark.sql.functions._

scala> :paste
// Entering paste mode (ctrl-D to finish)

Seq(""0010-01-01 01:02:03.123456"").toDF()
    .select($""value"".cast(""timestamp"").as(""ts""))
    .select(date_trunc(""HOUR"", $""ts"").cast(""string""))
    .show(false)

// Exiting paste mode, now interpreting.

+------------------------------------+
|CAST(date_trunc(HOUR, ts) AS STRING)|
+------------------------------------+
|0010-01-01 01:30:17                 |
+------------------------------------+
{code}
the result must be *0010-01-01 01:00:00*
{code:scala}
scala> :paste
// Entering paste mode (ctrl-D to finish)

Seq(""0010-01-01 01:02:03.123456"").toDF()
    .select($""value"".cast(""timestamp"").as(""ts""))
    .select(date_trunc(""DAY"", $""ts"").cast(""string""))
    .show(false)

// Exiting paste mode, now interpreting.

+-----------------------------------+
|CAST(date_trunc(DAY, ts) AS STRING)|
+-----------------------------------+
|0010-01-01 23:30:17                |
+-----------------------------------+
{code}
the result must be *0010-01-01 00:00:00*",,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 14 14:35:36 UTC 2020,,,,,,,,,,"0|z0bas8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/20 14:35;cloud_fan;Issue resolved by pull request 27512
[https://github.com/apache/spark/pull/27512];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix java.lang.IndexOutOfBoundsException No group 1 for regexp_extract,SPARK-30763,13284126,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,beliefer,beliefer,beliefer,09/Feb/20 10:23,25/Feb/20 03:11,13/Jul/23 08:46,19/Feb/20 12:36,2.4.5,3.0.0,,,,,,,,,,,2.4.6,3.0.0,,SQL,,,,0,,,"The current implement of regexp_extract will throws a unprocessed exception show below:

SELECT regexp_extract('1a 2b 14m', '
d+')

 
{code:java}
[info]   org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 22.0 failed 1 times, most recent failure: Lost task 1.0 in stage 22.0 (TID 33, 192.168.1.6, executor driver): java.lang.IndexOutOfBoundsException: No group 1
[info] at java.util.regex.Matcher.group(Matcher.java:538)
[info] at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[info] at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[info] at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
[info] at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[info] at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
[info] at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1804)
[info] at org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1227)
[info] at org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1227)
[info] at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2156)
[info] at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[info] at org.apache.spark.scheduler.Task.run(Task.scala:127)
[info] at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
[info] at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
[info] at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
[info] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info] at java.lang.Thread.run(Thread.java:748)
{code}
 

I think should treat this exception well.",,beliefer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 12 14:34:58 UTC 2020,,,,,,,,,,"0|z0bals:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/20 14:34;beliefer;Hive exception when idx > regex group count.


{code:java}
FAILED: SemanticException [Error 10014]: Line 1:7 Wrong arguments ‘2’: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public java.lang.String org.apache.hadoop.hive.ql.udf.UDFRegExpExtract.evaluate(java.lang.String,java.lang.String,java.lang.Integer) on object org.apache.hadoop.hive.ql.udf.UDFRegExpExtract@2cf5e0f0 of class org.apache.hadoop.hive.ql.udf.UDFRegExpExtract with arguments {x=a3&x=18abc&x=2&y=3&x=4:java.lang.String, x=([0-9]+)[a-z]:java.lang.String, 2:java.lang.Integer} of size 3{code};;;",,,,,,,,,,,,,,,,,,,,,,,
The cache in StringRegexExpression is not initialized for foldable patterns,SPARK-30759,13284095,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maxgekk,maxgekk,maxgekk,08/Feb/20 20:10,26/Feb/20 22:30,13/Jul/23 08:46,10/Feb/20 20:52,1.6.3,2.0.2,2.1.3,2.2.3,2.3.4,2.4.5,,,,,,,2.4.6,3.0.0,,SQL,,,,0,,,"In the case of foldable patterns, the cache in StringRegexExpression should be evaluated once but in fact it is compiled every time. Here is the example:
{code:sql}
SELECT '%SystemDrive%\Users\John' _FUNC_ '%SystemDrive%\\Users.*';
{code}
the code https://github.com/apache/spark/blob/8aebc80e0e67bcb1aa300b8c8b1a209159237632/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala#L45-L48:
{code:scala}
  // try cache the pattern for Literal
  private lazy val cache: Pattern = pattern match {
    case Literal(value: String, StringType) => compile(value)
    case _ => null
  }
{code}
The attached screenshot shows that foldable expression doesn't fall to the first case.",,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/20 20:10;maxgekk;Screen Shot 2020-02-08 at 22.45.50.png;https://issues.apache.org/jira/secure/attachment/12992964/Screen+Shot+2020-02-08+at+22.45.50.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 10 20:52:07 UTC 2020,,,,,,,,,,"0|z0baew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/20 19:42;dongjoon;Hi, [~maxgekk]. As you wrote on the PR, this is a behavior since 1.0.0. So, I convert this into an improvement JIRA. That will be easier to understand this issue impact and contribution.

If you PR doesn't have another improvement aspect (Literal to Foldable), the scope might be considered differently.;;;","10/Feb/20 20:52;dongjoon;Issue resolved by pull request 27502
[https://github.com/apache/spark/pull/27502];;;",,,,,,,,,,,,,,,,,,,,,,
`ThriftServerWithSparkContextSuite` fails always on spark-branch-3.0-test-sbt-hadoop-2.7-hive-2.3,SPARK-30756,13284038,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gurwls223,dongjoon,dongjoon,07/Feb/20 23:34,12/Dec/22 18:10,13/Jul/23 08:46,11/Feb/20 06:50,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,Tests,,,0,,,"This is a release blocker for 3.0.0.

The same test profile Hadoop-2.7/Hive-2.3 fails on `branch-3.0` while it succeeds in `master`.
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-3.0-test-sbt-hadoop-2.7-hive-2.3/

The failure comes from `ThriftServerWithSparkContextSuite`.
{code}
info] org.apache.spark.sql.hive.thriftserver.ThriftServerWithSparkContextSuite *** ABORTED *** (40 seconds, 707 milliseconds)
[info]   org.apache.hive.service.ServiceException: Failed to Start HiveServer2
{code}",,dongjoon,shaneknapp,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 11 06:51:07 UTC 2020,,,,,,,,,,"0|z0ba28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/20 23:34;dongjoon;Hi, [~yumwang]. Could you take a look please?;;;","08/Feb/20 05:34;yumwang;Hi [~shaneknapp] Do you have some special configuration? I can not reproduce it on my local machine.;;;","11/Feb/20 00:33;shaneknapp;unsure what you mean by ""special configuration""...

if having access to the jenkins workers helps, let me know and i can get that sorted.;;;","11/Feb/20 06:50;gurwls223;Issue resolved by pull request 27513
[https://github.com/apache/spark/pull/27513];;;","11/Feb/20 06:51;gurwls223;I will reopen if this isn't fixed.;;;",,,,,,,,,,,,,,,,,,,
Wrong result of to_utc_timestamp() on daylight saving day,SPARK-30752,13283750,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,06/Feb/20 17:03,10/Feb/20 06:29,13/Jul/23 08:46,07/Feb/20 18:37,2.4.4,3.0.0,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"The to_utc_timestamp() function returns wrong result when:
* JVM system time zone is PST
* the session local time zone is UTC
* fromZone is Asia/Hong_Kong
for the local date '2019-11-03T12:00:00', the result must be '2019-11-03T04:00:00'
{code}
scala> import java.util.TimeZone
import java.util.TimeZone

scala> import org.apache.spark.sql.catalyst.util.DateTimeUtils._
import org.apache.spark.sql.catalyst.util.DateTimeUtils._

scala> import org.apache.spark.sql.functions._
import org.apache.spark.sql.functions._

scala> TimeZone.setDefault(getTimeZone(""PST""))
scala> spark.conf.set(""spark.sql.session.timeZone"", ""UTC"")
scala> val df = Seq(""2019-11-03T12:00:00"").toDF(""localTs"")
df: org.apache.spark.sql.DataFrame = [localTs: string]

scala> df.select(to_utc_timestamp(col(""localTs""), ""Asia/Hong_Kong"")).show
+-----------------------------------------+
|to_utc_timestamp(localTs, Asia/Hong_Kong)|
+-----------------------------------------+
|                      2019-11-03 03:00:00|
+-----------------------------------------+
{code}
 
See https://www.worldtimebuddy.com/?qm=1&lid=8,1819729,100&h=8&date=2019-11-2&sln=21-22",,cloud_fan,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30730,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 07 18:37:29 UTC 2020,,,,,,,,,,"0|z0b8a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/20 17:14;maxgekk;[~dongjoon] FYI, 2.4 has the bug.;;;","06/Feb/20 18:08;dongjoon;Thanks, bit you don't need to ping me from today because I'm not a release manager any more, [~maxgekk]. :);;;","06/Feb/20 18:09;dongjoon;The next release 2.4.6 is scheduled but there is no release manager for that yet.;;;","07/Feb/20 18:37;cloud_fan;Issue resolved by pull request 27474
[https://github.com/apache/spark/pull/27474];;;",,,,,,,,,,,,,,,,,,,,
"Use specific image version in ""Launcher client dependencies"" test",SPARK-30738,13283426,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,05/Feb/20 09:57,17/May/20 18:25,13/Jul/23 08:46,05/Feb/20 19:02,3.0.0,,,,,,,,,,,,3.0.0,,,Kubernetes,Spark Core,Tests,,0,,,,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28465,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 05 19:02:24 UTC 2020,,,,,,,,,,"0|z0b6a8:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"05/Feb/20 19:02;dongjoon;Issue resolved by pull request 27465
[https://github.com/apache/spark/pull/27465];;;",,,,,,,,,,,,,,,,,,,,,,,
Update deprecated Mkdocs option,SPARK-30731,13283331,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,nchammas,nchammas,nchammas,04/Feb/20 22:48,26/Feb/20 02:45,13/Jul/23 08:46,19/Feb/20 19:17,3.0.0,,,,,,,,,,,,2.4.6,,,Documentation,,,,0,,,,,dongjoon,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 19 19:17:13 UTC 2020,,,,,,,,,,"0|z0b5p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/20 19:17;dongjoon;This is resolved via https://github.com/apache/spark/pull/27626;;;",,,,,,,,,,,,,,,,,,,,,,,
fix DataFrameAggregateSuite when enabling AQE,SPARK-30721,13283139,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,maryannxue,maryannxue,04/Feb/20 03:47,05/Feb/20 20:37,13/Jul/23 08:46,05/Feb/20 20:37,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"This is a follow up for [https://github.com/apache/spark/pull/26813#discussion_r373044512].

We need to fix test DataFrameAggregateSuite with AQE on.",,cloud_fan,dongjoon,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 05 20:37:26 UTC 2020,,,,,,,,,,"0|z0b4ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/20 03:48;maryannxue;cc [~cloud_fan], [~Jk_Self];;;","04/Feb/20 11:07;cloud_fan;This is actually a false alert. We can turn off WSC with AQE, the DataFrameAggregateSuite is not updated properly to pass with AQE.;;;","05/Feb/20 20:37;dongjoon;Issue resolved by pull request 27451
[https://github.com/apache/spark/pull/27451];;;",,,,,,,,,,,,,,,,,,,,,
AQE subquery map should cache `SubqueryExec` instead of `ExecSubqueryExpression`,SPARK-30717,13283026,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maryannxue,maryannxue,maryannxue,03/Feb/20 18:15,04/Feb/20 05:04,13/Jul/23 08:46,04/Feb/20 05:04,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,,,cloud_fan,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 04 05:04:05 UTC 2020,,,,,,,,,,"0|z0b440:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/20 05:04;cloud_fan;Issue resolved by pull request 27446
[https://github.com/apache/spark/pull/27446];;;",,,,,,,,,,,,,,,,,,,,,,,
Use jekyll-redirect-from 0.15.0 instead of the latest,SPARK-30704,13282821,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,02/Feb/20 07:48,03/Feb/20 03:29,13/Jul/23 08:46,02/Feb/20 08:48,2.4.5,3.0.0,,,,,,,,,,,2.4.5,3.0.0,,Project Infra,,,,0,,,"We use Ruby 2.3 in our release docker image.

The latest version of `jekyll-redirect-from 0.16.0` causes a failure on Ruby 2.3.
- https://github.com/jekyll/jekyll-redirect-from/releases/tag/v0.16.0

{code}
root@dc0bc546e377:/# gem install jekyll-redirect-from
ERROR:  Error installing jekyll-redirect-from:
	jekyll-redirect-from requires Ruby version >= 2.4.0.
{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 02 08:48:08 UTC 2020,,,,,,,,,,"0|z0b2ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/20 08:48;dongjoon;Issue resolved by pull request 27434
[https://github.com/apache/spark/pull/27434];;;",,,,,,,,,,,,,,,,,,,,,,,
Handle database and namespace exceptions in catalog.isView,SPARK-30697,13282721,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,01/Feb/20 00:26,03/Feb/20 06:24,13/Jul/23 08:46,03/Feb/20 06:24,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,The non-existence of a database shouldn't throw a NoSuchDatabaseException from catalog.isView,,brkyvz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 03 06:24:27 UTC 2020,,,,,,,,,,"0|z0b288:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"03/Feb/20 06:24;brkyvz;Resolved by [#27423|https://github.com/apache/spark/pull/27423];;;",,,,,,,,,,,,,,,,,,,,,,,
numpy is a dependency for building PySpark API docs,SPARK-30672,13282304,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nchammas,nchammas,nchammas,30/Jan/20 02:46,12/Dec/22 18:10,13/Jul/23 08:46,30/Jan/20 04:05,3.0.0,,,,,,,,,,,,3.0.0,,,Build,PySpark,,,0,,,As described here: https://github.com/apache/spark/pull/27376#discussion_r372550656,,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 30 04:05:14 UTC 2020,,,,,,,,,,"0|z0aznk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/20 04:05;gurwls223;Issue resolved by pull request 27390
[https://github.com/apache/spark/pull/27390];;;",,,,,,,,,,,,,,,,,,,,,,,
"to_timestamp failed to parse 2020-01-27T20:06:11.847-0800 using pattern ""yyyy-MM-dd'T'HH:mm:ss.SSSz""",SPARK-30668,13282069,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,maxgekk,smilegator,smilegator,28/Jan/20 22:11,05/Mar/20 10:20,13/Jul/23 08:46,05/Mar/20 08:57,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"{code:java}
SELECT to_timestamp(""2020-01-27T20:06:11.847-0800"", ""yyyy-MM-dd'T'HH:mm:ss.SSSz"")
{code}

This can return a valid value in Spark 2.4 but return NULL in the latest master

**2.4.5 RC2**
{code}
scala> sql(""""""SELECT to_timestamp(""2020-01-27T20:06:11.847-0800"", ""yyyy-MM-dd'T'HH:mm:ss.SSSz"")"""""").show
+----------------------------------------------------------------------------+
|to_timestamp('2020-01-27T20:06:11.847-0800', 'yyyy-MM-dd\'T\'HH:mm:ss.SSSz')|
+----------------------------------------------------------------------------+
|                                                         2020-01-27 20:06:11|
+----------------------------------------------------------------------------+
{code}

**2.2.3 ~ 2.4.4** (2.0.2 ~ 2.1.3 doesn't have `to_timestamp`).
{code}
spark-sql> SELECT to_timestamp(""2020-01-27T20:06:11.847-0800"", ""yyyy-MM-dd'T'HH:mm:ss.SSSz"");
2020-01-27 20:06:11
{code}",,cloud_fan,dongjoon,hvanhovell,maxgekk,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26503,SPARK-27438,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 08:57:46 UTC 2020,,,,,,,,,,"0|z0ay7c:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"28/Jan/20 22:15;smilegator;cc [~maxgekk];;;","29/Jan/20 03:19;maxgekk;Date/timestamp parsing is based on Java 8 DateTimeFormat in Spark 3.0 which may have different notion of pattern letters (see [https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html]):
{code}
   V       time-zone ID                zone-id           America/Los_Angeles; Z; -08:30
   z       time-zone name              zone-name         Pacific Standard Time; PST
   O       localized zone-offset       offset-O          GMT+8; GMT+08:00; UTC-08:00;
   X       zone-offset 'Z' for zero    offset-X          Z; -08; -0830; -08:30; -083015; -08:30:15;
   x       zone-offset                 offset-x          +0000; -08; -0830; -08:30; -083015; -08:30:15;
   Z       zone-offset                 offset-Z          +0000; -0800; -08:00;
{code}
As you can see 'z' is for time zone name, but you is going to parse zone offsets. You can use 'x' or 'Z' in the pattern instead of 'z':
{code}
scala> spark.sql(""""""SELECT to_timestamp(""2020-01-27T20:06:11.847-0800"", ""yyyy-MM-dd'T'HH:mm:ss.SSSZ"")"""""").show(false)
+----------------------------------------------------------------------------+
|to_timestamp('2020-01-27T20:06:11.847-0800', 'yyyy-MM-dd\'T\'HH:mm:ss.SSSZ')|
+----------------------------------------------------------------------------+
|2020-01-28 07:06:11.847                                                     |
+----------------------------------------------------------------------------+
{code}

Parsing in Spark 2.4 is based on SimpleDateFormat (see https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html) where 'z' has slightly different meaning.;;;","29/Jan/20 06:56;smilegator;This will make the migration very painful. This is not mentioned in the migration guide. It will also generate different query results. Do we have a simple way to remove such a behavior change? For example, converting the pattern for users?;;;","29/Jan/20 06:57;smilegator;Can we let users choose different parsing mechanisms between SimpleDateFormat and DateTimeFormat? ;;;","29/Jan/20 07:14;maxgekk;> This is not mentioned in the migration guide.

It is mentioned:
{code}
    - The `unix_timestamp`, `date_format`, `to_unix_timestamp`, `from_unixtime`, `to_date`, `to_timestamp` functions. New implementation supports pattern formats as described here https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html and performs strict checking of its input. For example, the `2015-07-22 10:00:00` timestamp cannot be parse if pattern is `yyyy-MM-dd` because the parser does not consume whole input. Another example is the `31/01/2015 00:00` input cannot be parsed by the `dd/MM/yyyy hh:mm` pattern because `hh` supposes hours in the range `1-12`.
{code}
 
> Do we have a simple way to remove such a behavior change? 

The change is related to the migration to Proleptic Gregorian calendar. To remove the behavior, you need to revert most of https://issues.apache.org/jira/browse/SPARK-26651 and maybe more.

> For example, converting the pattern for users?

Even it is possible to convert patterns, the result can be different for old dates due to the calendar system.

> Can we let users choose different parsing mechanisms between SimpleDateFormat and DateTimeFormat?

No, a flag was removed 1 year ago, see https://issues.apache.org/jira/browse/SPARK-26503 and see https://github.com/apache/spark/pull/23391#discussion_r244414750;;;","29/Jan/20 08:02;smilegator;[~hvanhovell] Making it configurable looks necessary. Today, Michael hit this when they tried the master branch. ;;;","29/Jan/20 08:18;maxgekk;[~marmbrus] The doc for `to_timestamp` points out DateTimeFormatter: https://github.com/apache/spark/blob/d69ed9afdf2bd8d03aaf835292b92692ec8189e9/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L2964, please, have a look at its doc https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html to see which patterns for time zone and zone offset it supports /cc [~srowen] [~dongjoon];;;","29/Jan/20 08:49;hvanhovell;I don't think we should revert the proleptic gregorian patch, the previous behavior was kind of broken.

[~maxgekk] can we move back to the previous behavior by using the old parser? And perhaps feature flag that bit, or make it configurable.;;;","29/Jan/20 09:12;maxgekk;We can try to revert this [https://github.com/apache/spark/pull/23495] ;;;","02/Feb/20 18:38;maxgekk;[~dongjoon] Output of your example is surprised - it doesn't contain the fractional part of seconds. 847 is printed on the master if we change the pattern z -> Z. It seems 2.4.x parses the input incorrectly:
{code}
scala> val ts = sql(""""""SELECT to_timestamp(""2020-01-27T20:06:11.847-0800"", ""yyyy-MM-dd'T'HH:mm:ss.SSSz"")"""""").first()(0)
ts: Any = 2020-01-28 07:06:11.0

scala> ts.asInstanceOf[java.sql.Timestamp].getNanos
res11: Int = 0
{code};;;","02/Feb/20 19:35;maxgekk;[~dongjoon] I have found the reason [https://github.com/apache/spark/pull/24420] . Theoretically, it can be backported to 2.4.x. but it could support parsing in millisecond precision. To support parsing in microseconds, need to switch on the parser added to 2.4 by [https://github.com/apache/spark/pull/26507];;;","02/Feb/20 19:36;dongjoon;[~maxgekk]. You should be the last man who surprised.  It's SPARK-27438, isn't it?;;;","02/Feb/20 19:38;dongjoon;[~maxgekk]. We don't backport improvement. You registered that issue as `Improvement`. Also, I think so.;;;","02/Feb/20 19:40;dongjoon;Given the current circumstance, this seems to be considered as a bug introduced by a new improvement patch. This is irrelevant to `2.4.x`.;;;","02/Feb/20 19:53;maxgekk;> as a bug introduced by a new improvement patch. 

Don't understand this. The behavior of to_timestamp exists from the beginning. I am not sure that we can classify it as a bug.

We could improve the description of the function, at least. At the moment, it says nothing about precision in 2.4.x, and the example of supported pattern yyyy-MM-dd HH:mm:ss.SSSS doesn't make sense.;;;","02/Feb/20 22:03;dongjoon;[~maxgekk]. What I meant was the scope of this `BUG` issue (SPARK-30668) is only 3.0.0. 
It's because only `3.0.0` returns `NULL`. In 2.4.x world, we don't return `NULL` for that kind of case. 
> The behavior of to_timestamp exists from the beginning. ;;;","03/Feb/20 07:08;cloud_fan;I checked the doc in [Spark 2.4|https://github.com/apache/spark/blob/branch-2.4/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L2967], and it says the pattern string follows java.text.SimpleDateFormat, so I think this is a breaking change.

AFAIK we fixed several bugs by switching to the java.time.format.DateTimeFormatter, so it should be OK to do it in 3.0. We can make the migration more smooth by
1. providing a legacy config to restore the old behavior
2. when we use the new formatter, fall back to the old formatter if the new one fails to parse. This can at least fix the problem reported by this ticket.

thoughts?;;;","03/Feb/20 07:41;maxgekk;Behind of the removed config _spark.sql.legacy.timeParser.enabled_, there are 2 more fallbacks to behaviors since Spark 1.5, see LegacyFallbackDateFormatter:
1. *s.toInt* - In Spark 1.5.0, we store the data as number of days since epoch in string. So, we just convert it to Int.
2. *DateTimeUtils.millisToDays(DateTimeUtils.stringToTime(s).getTime)* - the way used in 2.0 and 1.x
3. FastDateFormat or *SimpleDateFormat*
Should we allow users to switch to SimpleDateFormat only or other legacy ways too?

 ;;;","03/Feb/20 07:58;cloud_fan;[~maxgekk] can we do them one by one? The SimpleDateFormat fallback is well justified by the examples in this ticket. We should look at the other 2 fallbacks closely as well.;;;","05/Feb/20 11:13;cloud_fan;Issue resolved by pull request 27441
[https://github.com/apache/spark/pull/27441];;;","06/Feb/20 18:58;smilegator;I think this is still not resolved. Spark 3.0 should not silently return a wrong result for a query whose pattern was right in the previous versions. I did not see the fallback mentioned in [~cloud_fan];;;","05/Mar/20 08:57;cloud_fan;Issue resolved by pull request 27537
[https://github.com/apache/spark/pull/27537];;;",,
Limit after on streaming dataframe before streaming agg returns wrong results,SPARK-30658,13281890,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,28/Jan/20 09:12,01/Feb/20 00:42,13/Jul/23 08:46,01/Feb/20 00:29,2.3.0,2.3.1,2.3.2,2.3.3,2.3.4,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,,,3.0.0,,,Structured Streaming,,,,0,,,"Limit before a streaming aggregate (i.e. {{df.limit(5).groupBy().count()}}) in complete mode was not being planned as a streaming limit. The planner rule planned a logical limit with a stateful streaming limit plan only if the query is in append mode. As a result, instead of allowing max 5 rows across batches, the planned streaming query was allowing 5 rows in every batch thus producing incorrect results.",,dongjoon,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 01 00:42:14 UTC 2020,,,,,,,,,,"0|z0ax3k:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"29/Jan/20 04:44;dongjoon;Hi, [~tdas]. Can we have `2.4.5` at `Target Version`, too?;;;","01/Feb/20 00:30;tdas;I am a little afraid to backport this because this is hacky change in the incremental planner which is already quite complicated to reason about;;;","01/Feb/20 00:30;tdas;Fixed in this PR https://github.com/apache/spark/pull/27373;;;","01/Feb/20 00:42;dongjoon;Got it. Thank you for confirmation, [~tdas].;;;",,,,,,,,,,,,,,,,,,,,
Streaming limit after streaming dropDuplicates can throw error,SPARK-30657,13281887,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,28/Jan/20 09:08,01/Feb/20 22:32,13/Jul/23 08:46,01/Feb/20 00:30,2.3.0,2.3.1,2.3.2,2.3.3,2.3.4,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,,,3.0.0,,,Structured Streaming,,,,0,,,"{{LocalLimitExec}} does not consume the iterator of the child plan. So if there is a limit after a stateful operator like streaming dedup in append mode (e.g. {{streamingdf.dropDuplicates().limit(5}})), the state changes of streaming duplicate may not be committed (most stateful ops commit state changes only after the generated iterator is fully consumed). This leads to the next batch failing with {{java.lang.IllegalStateException: Error reading delta file .../N.delta does not exist}} as the state store delta file was never generated.",,dongjoon,tdas,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 01 22:32:09 UTC 2020,,,,,,,,,,"0|z0ax2w:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"29/Jan/20 04:44;dongjoon;Hi, [~tdas]. Can we have `2.4.5` at `Target Version`, too?;;;","01/Feb/20 00:35;tdas;This fix by itself (separate from the fix for SPARK-30658) may be backported. The solution that I did to always inject StreamingLocalLimitExec is safe from correctness point of view, but is a little risky from the performance point of view (which I tried to minimize using the optimization). With 2.4.4+, unless this is a serious bug that affects many users, I dont think we should backport this. And i dont think limit on streaming is that extensively used such that this is big bug (it has not been reported for 1.5 years). 

What do you think [~zsxwing];;;","01/Feb/20 22:32;zsxwing;[~tdas] Make sense. Agreed that the risk is high but the benefit is pretty low. We can backport it later whenever needed.;;;",,,,,,,,,,,,,,,,,,,,,
EXPLAIN EXTENDED does not show detail information for aggregate operators,SPARK-30651,13281740,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,EricWu,EricWu,EricWu,27/Jan/20 15:47,13/Feb/20 01:54,13/Jul/23 08:46,12/Feb/20 18:13,3.1.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,Currently EXPLAIN FORMATTED only report input attributes of HashAggregate/ObjectHashAggregate/SortAggregate. While EXPLAIN EXTENDED provides more information. We need to enhance EXPLAIN FORMATTED to follow the original behavior.,,cloud_fan,dkbiswal,EricWu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 12 18:13:44 UTC 2020,,,,,,,,,,"0|z0aw68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/20 18:13;cloud_fan;Issue resolved by pull request 27368
[https://github.com/apache/spark/pull/27368];;;",,,,,,,,,,,,,,,,,,,,,,,
collect() support Unicode charactes tests fails on Windows,SPARK-30645,13281569,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,zero323,zero323,26/Jan/20 00:46,12/Dec/22 18:10,13/Jul/23 08:46,26/Jan/20 04:01,3.0.0,,,,,,,,,,,,2.4.5,3.0.0,,SparkR,Tests,,,0,,,"As-is [test_that(""collect() support Unicode characters""|https://github.com/apache/spark/blob/d5b92b24c41b047c64a4d89cc4061ebf534f0995/R/pkg/tests/fulltests/test_sparkSQL.R#L850-L869] case seems to be system dependent, and doesn't work properly on Windows with CP1252 English locale:

 
{code:r}
library(SparkR)
SparkR::sparkR.session()
Sys.info()
#           sysname           release           version 
#         ""Windows""      ""Server x64""     ""build 17763"" 
#          nodename           machine             login 
# ""WIN-5BLT6Q610KH""          ""x86-64""   ""Administrator"" 
#              user    effective_user 
#   ""Administrator""   ""Administrator"" 

Sys.getlocale()

# [1] ""LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MONETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252""

lines <- c(""{\""name\"":\""안녕하세요\""}"",
           ""{\""name\"":\""您好\"", \""age\"":30}"",
           ""{\""name\"":\""こんにちは\"", \""age\"":19}"",
           ""{\""name\"":\""Xin chào\""}"")

system(paste0(""cat "", jsonPath))
# {""name"":""<U+C548><U+B155><U+D558><U+C138><U+C694>""}
# {""name"":""<U+60A8><U+597D>"", ""age"":30}
# {""name"":""<U+3053><U+3093><U+306B><U+3061><U+306F>"", ""age"":19}
# {""name"":""Xin chào""}
# [1] 0


jsonPath <- tempfile(pattern = ""sparkr-test"", fileext = "".tmp"")
writeLines(lines, jsonPath)

df <- read.df(jsonPath, ""json"")


printSchema(df)
# root
#  |-- _corrupt_record: string (nullable = true)
#  |-- age: long (nullable = true)
#  |-- name: string (nullable = true)

head(df)
#              _corrupt_record age                                     name
# 1                       <NA>  NA <U+C548><U+B155><U+D558><U+C138><U+C694>
# 2                       <NA>  30                         <U+60A8><U+597D>
# 3                       <NA>  19 <U+3053><U+3093><U+306B><U+3061><U+306F>
# 4 {""name"":""Xin ch<U+FFFD>o""}  NA                                     <NA>

{code}

Problem becomes visible on AppVoyer when testthat is updated to 2.x, but somehow silenced when testthat 1.x is used.",,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23435,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 26 04:01:48 UTC 2020,,,,,,,,,,"0|z0av48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/20 04:01;gurwls223;Issue resolved by pull request 27362
[https://github.com/apache/spark/pull/27362];;;",,,,,,,,,,,,,,,,,,,,,,,
Unable to add packages on spark-packages.org,SPARK-30636,13281449,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,smilegator,smilegator,24/Jan/20 18:08,05/Feb/20 18:07,13/Jul/23 08:46,05/Feb/20 18:07,2.4.4,,,,,,,,,,,,3.0.0,,,Project Infra,,,,0,,,Unable to add new packages to spark-packages.org. ,,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-01-24 18:08:40.0,,,,,,,,,,"0|z0audk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Codegen fails when xxHash seed is not an integer,SPARK-30633,13281361,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Cording,Cording,Cording,24/Jan/20 10:47,27/Jan/20 19:22,13/Jul/23 08:46,27/Jan/20 18:33,2.4.4,,,,,,,,,,,,2.4.5,3.0.0,,SQL,,,,0,,,"If the seed for xxHash is not an integer the generated code does not compile.

Steps to reproduce:
{code:java}
import org.apache.spark.sql.catalyst.expressions.XxHash64
import org.apache.spark.sql.Column

val file = ""...""
val column = col(""..."")

val df = spark.read.csv(file)

def xxHash(seed: Long, cols: Column*): Column = new Column(
   XxHash64(cols.map(_.expr), seed)
)

val seed = (Math.pow(2, 32)+1).toLong
df.select(xxHash(seed, column)).show()
{code}
Appending an L to the seed when the datatype is long fixes the issue.",,Cording,dongjoon,oshevchenko,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 27 18:33:39 UTC 2020,,,,,,,,,,"0|z0atu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/20 18:33;dongjoon;Issue resolved by pull request 27354
[https://github.com/apache/spark/pull/27354];;;",,,,,,,,,,,,,,,,,,,,,,,
Spark external shuffle allow disable of separate event loop group,SPARK-30623,13281229,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,tgraves,tgraves,23/Jan/20 16:12,17/May/20 18:30,13/Jul/23 08:46,26/Mar/20 05:01,3.0.0,,,,,,,,,,,,3.0.0,,,Shuffle,Spark Core,,,0,,,"In SPARK-24355 changes were made to add a separate event loop group for processing ChunkFetchRequests , this  allow fors the other threads to handle regular connection requests when the configuration value is set. This however seems to have added some latency (see pr 22173 comments at the end).

To help with this we could make sure the secondary event loop group isn't used when the configuration of spark.shuffle.server.chunkFetchHandlerThreadsPercent isn't explicitly set. This should result in getting the same behavior as before.",,cloud_fan,csingh,dongjoon,mshen,Steven Rand,tgraves,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24355,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 05:01:50 UTC 2020,,,,,,,,,,"0|z0at0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/20 04:51;dongjoon;Hi, [~tgraves]. Since SPARK-24355 is fixed at 3.0.0, I removed `2.4.4` from the `Affected Version`. Please let me know if I'm wrong.;;;","26/Mar/20 05:01;cloud_fan;Issue resolved by pull request 27665
[https://github.com/apache/spark/pull/27665];;;",,,,,,,,,,,,,,,,,,,,,,
commands should return dummy statistics,SPARK-30622,13281206,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,23/Jan/20 14:15,31/Jan/20 03:43,13/Jul/23 08:46,30/Jan/20 18:27,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,,,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 30 18:27:52 UTC 2020,,,,,,,,,,"0|z0asvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/20 18:27;dongjoon;Issue resolved by pull request 27344
[https://github.com/apache/spark/pull/27344];;;",,,,,,,,,,,,,,,,,,,,,,,
can't resolve qualified column name with v2 tables,SPARK-30612,13281107,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,cloud_fan,cloud_fan,23/Jan/20 03:42,24/Feb/20 05:38,13/Jul/23 08:46,06/Feb/20 06:33,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"When running queries with qualified columns like `SELECT t.a FROM t`, it fails to resolve for v2 tables.

v1 table is fine as we always wrap the v1 relation with a `SubqueryAlias`. We should do the same for v2 tables.",,brkyvz,cloud_fan,colin,imback82,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 06 06:33:00 UTC 2020,,,,,,,,,,"0|z0as9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/20 03:43;cloud_fan;Hi [~imback82] do you have time to work on it? Thanks;;;","23/Jan/20 04:00;imback82;[~cloud_fan] Yes, I will work on this.;;;","23/Jan/20 18:14;brkyvz;SPARK-30314 should help make this work easier;;;","24/Jan/20 04:16;imback82;Thanks [~brkyvz]

There are two approaches we can take. One is to wrap v2 table with `SubqueryAlias`. Another is to update `DataSourceV2Relation`'s output (Seq[AttributeReference]) to have qualifier directly (after SPARK-30314). Which route should I take?

 ;;;","24/Jan/20 04:34;brkyvz;I prefer SubqueryAlias. We need to support all degrees of the user provided
identifier I believe:

SELECT testcat.ns1.ns2.tbl.foo FROM testcat.ns1.ns2.tbl

SELECT ns1.ns2.tbl.foo FROM testcat.ns1.ns2.tbl

SELECT ns2.tbl.foo FROM testcat.ns1.ns2.tbl

SELECT tbl.foo FROM testcat.ns1.ns2.tbl

should all work.

However I'm not sure if

SELECT spark_catalog.default.tbl.foo FROM tbl

should work. Are my assumptions correct?


;;;","24/Jan/20 15:46;cloud_fan;I think the example from [~brkyvz] is right. The column name qualifier should only refer to what specified in the table name.;;;","06/Feb/20 06:33;cloud_fan;Issue resolved by pull request 27391
[https://github.com/apache/spark/pull/27391];;;",,,,,,,,,,,,,,,,,
Applying the `like` function with 2 parameters fails,SPARK-30606,13281040,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,22/Jan/20 19:14,22/Jan/20 23:40,13/Jul/23 08:46,22/Jan/20 23:40,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"The `Like` expression is registered as the `like` function. Applying the function with 2 parameters fails with the exception:
{code}
spark-sql> select like('Spark', 'S%');

Invalid arguments for function like; line 1 pos 7
org.apache.spark.sql.AnalysisException: Invalid arguments for function like; line 1 pos 7
	at org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.$anonfun$expression$7(FunctionRegistry.scala:618)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.$anonfun$expression$4(FunctionRegistry.scala:602)
	at org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:121)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupFunction(SessionCatalog.scala:1412)
{code}",,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28083,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 22 23:40:46 UTC 2020,,,,,,,,,,"0|z0aruo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/20 21:40;dongjoon;Since this is introduced by SPARK-28083, this is only for 3.0.0, right?;;;","22/Jan/20 23:40;dongjoon;Issue resolved by pull request 27323
[https://github.com/apache/spark/pull/27323];;;",,,,,,,,,,,,,,,,,,,,,,
HostLocal Block size missed in log total bytes,SPARK-30604,13280995,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,Udbhav Agrawal,Udbhav Agrawal,Udbhav Agrawal,22/Jan/20 15:39,17/May/20 18:30,13/Jul/23 08:46,22/Jan/20 22:21,3.0.0,,,,,,,,,,,,3.0.0,,,Shuffle,Spark Core,,,0,,,,,dongjoon,Udbhav Agrawal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27651,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 22 22:21:06 UTC 2020,,,,,,,,,,"0|z0arkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/20 15:39;Udbhav Agrawal;I will raise a PR to fix;;;","22/Jan/20 17:52;dongjoon;This is caused by SPARK-27651;;;","22/Jan/20 22:21;dongjoon;Issue resolved by pull request 27320
[https://github.com/apache/spark/pull/27320];;;",,,,,,,,,,,,,,,,,,,,,
can't use more than five type-safe user-defined aggregation in select statement,SPARK-30590,13280635,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,mantovani,mantovani,21/Jan/20 01:36,12/Dec/22 18:11,13/Jul/23 08:46,27/Feb/20 06:33,2.2.3,2.3.4,2.4.4,3.0.0,,,,,,,,,3.0.0,,,Spark Core,,,,0,,," How to reproduce:
{code:scala}
val df = Seq((1,2,3,4,5,6)).toDF(""a"",""b"",""c"",""d"",""e"",""f"")

import org.apache.spark.sql.expressions.Aggregator
import org.apache.spark.sql.Encoder
import org.apache.spark.sql.Encoders
import org.apache.spark.sql.Row

case class FooAgg(s:Int) extends Aggregator[Row, Int, Int] {
  def zero:Int = s
  def reduce(b: Int, r: Row): Int = b + r.getAs[Int](0)
  def merge(b1: Int, b2: Int): Int = b1 + b2
  def finish(b: Int): Int = b
  def bufferEncoder: Encoder[Int] = Encoders.scalaInt
  def outputEncoder: Encoder[Int] = Encoders.scalaInt
}

val fooAgg = (i:Int) => FooAgg(i).toColumn.name(s""foo_agg_$i"")

scala> df.select(fooAgg(1),fooAgg(2),fooAgg(3),fooAgg(4),fooAgg(5)).show
+---------+---------+---------+---------+---------+
|foo_agg_1|foo_agg_2|foo_agg_3|foo_agg_4|foo_agg_5|
+---------+---------+---------+---------+---------+
|        3|        5|        7|        9|       11|
+---------+---------+---------+---------+---------+

{code}
With 6 arguments we have error:
{code:scala}
scala> df.select(fooAgg(1),fooAgg(2),fooAgg(3),fooAgg(4),fooAgg(5),fooAgg(6)).show

org.apache.spark.sql.AnalysisException: unresolved operator 'Aggregate [fooagg(FooAgg(1), None, None, None, input[0, int, false] AS value#114, assertnotnull(cast(value#114 as int)), input[0, int, false] AS value#113, IntegerType, IntegerType, false) AS foo_agg_1#116, fooagg(FooAgg(2), None, None, None, input[0, int, false] AS value#119, assertnotnull(cast(value#119 as int)), input[0, int, false] AS value#118, IntegerType, IntegerType, false) AS foo_agg_2#121, fooagg(FooAgg(3), None, None, None, input[0, int, false] AS value#124, assertnotnull(cast(value#124 as int)), input[0, int, false] AS value#123, IntegerType, IntegerType, false) AS foo_agg_3#126, fooagg(FooAgg(4), None, None, None, input[0, int, false] AS value#129, assertnotnull(cast(value#129 as int)), input[0, int, false] AS value#128, IntegerType, IntegerType, false) AS foo_agg_4#131, fooagg(FooAgg(5), None, None, None, input[0, int, false] AS value#134, assertnotnull(cast(value#134 as int)), input[0, int, false] AS value#133, IntegerType, IntegerType, false) AS foo_agg_5#136, fooagg(FooAgg(6), None, None, None, input[0, int, false] AS value#139, assertnotnull(cast(value#139 as int)), input[0, int, false] AS value#138, IntegerType, IntegerType, false) AS foo_agg_6#141];;
'Aggregate [fooagg(FooAgg(1), None, None, None, input[0, int, false] AS value#114, assertnotnull(cast(value#114 as int)), input[0, int, false] AS value#113, IntegerType, IntegerType, false) AS foo_agg_1#116, fooagg(FooAgg(2), None, None, None, input[0, int, false] AS value#119, assertnotnull(cast(value#119 as int)), input[0, int, false] AS value#118, IntegerType, IntegerType, false) AS foo_agg_2#121, fooagg(FooAgg(3), None, None, None, input[0, int, false] AS value#124, assertnotnull(cast(value#124 as int)), input[0, int, false] AS value#123, IntegerType, IntegerType, false) AS foo_agg_3#126, fooagg(FooAgg(4), None, None, None, input[0, int, false] AS value#129, assertnotnull(cast(value#129 as int)), input[0, int, false] AS value#128, IntegerType, IntegerType, false) AS foo_agg_4#131, fooagg(FooAgg(5), None, None, None, input[0, int, false] AS value#134, assertnotnull(cast(value#134 as int)), input[0, int, false] AS value#133, IntegerType, IntegerType, false) AS foo_agg_5#136, fooagg(FooAgg(6), None, None, None, input[0, int, false] AS value#139, assertnotnull(cast(value#139 as int)), input[0, int, false] AS value#138, IntegerType, IntegerType, false) AS foo_agg_6#141]
+- Project [_1#6 AS a#13, _2#7 AS b#14, _3#8 AS c#15, _4#9 AS d#16, _5#10 AS e#17, _6#11 AS F#18]
 +- LocalRelation [_1#6, _2#7, _3#8, _4#9, _5#10, _6#11]

at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:43)
 at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)
 at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$3.apply(CheckAnalysis.scala:431)
 at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$3.apply(CheckAnalysis.scala:430)
 at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
 at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:430)
 at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
 at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
 at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)

 at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
 at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
 at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)
 at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)
 at org.apache.spark.sql.Dataset.select(Dataset.scala:1340)
 ... 50 elided
{code}
 

 

 ",,cloud_fan,mantovani,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Thu Feb 27 06:33:27 UTC 2020,,,,,,,,,,"0|z0apcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/20 02:06;gurwls223;Seems fixed in the master:

{code}
scala> df.select(fooAgg(1),fooAgg(2),fooAgg(3),fooAgg(4),fooAgg(5)).show
+---------+---------+---------+---------+---------+
|foo_agg_1|foo_agg_2|foo_agg_3|foo_agg_4|foo_agg_5|
+---------+---------+---------+---------+---------+
|        3|        5|        7|        9|       11|
+---------+---------+---------+---------+---------+
{code}

It would be great if we can identify which JIRA fixed it and see if we can backport.;;;","23/Jan/20 05:47;mantovani;[~hyukjin.kwon] You tried with 5 parameters which works, you should try with 6 to get the exception:

 
{code:java}
scala> df.select(fooAgg(1),fooAgg(2),fooAgg(3),fooAgg(4),fooAgg(5),fooAgg(6)).show
org.apache.spark.sql.AnalysisException: unresolved operator 'Aggregate [fooagg(FooAgg(1), None, None, None, input[0, int, false] AS value#114, assertnotnull(cast(value#114 as int)), input[0, int, false] AS value#113, IntegerType, IntegerType, false) AS foo_agg_1#116, fooagg(FooAgg(2), None, None, None, input[0, int, false] AS value#119, assertnotnull(cast(value#119 as int)), input[0, int, false] AS value#118, IntegerType, IntegerType, false) AS foo_agg_2#121, fooagg(FooAgg(3), None, None, None, input[0, int, false] AS value#124, assertnotnull(cast(value#124 as int)), input[0, int, false] AS value#123, IntegerType, IntegerType, false) AS foo_agg_3#126, fooagg(FooAgg(4), None, None, None, input[0, int, false] AS value#129, assertnotnull(cast(value#129 as int)), input[0, int, false] AS value#128, IntegerType, IntegerType, false) AS foo_agg_4#131, fooagg(FooAgg(5), None, None, None, input[0, int, false] AS value#134, assertnotnull(cast(value#134 as int)), input[0, int, false] AS value#133, IntegerType, IntegerType, false) AS foo_agg_5#136, fooagg(FooAgg(6), None, None, None, input[0, int, false] AS value#139, assertnotnull(cast(value#139 as int)), input[0, int, false] AS value#138, IntegerType, IntegerType, false) AS foo_agg_6#141];;
 'Aggregate [fooagg(FooAgg(1), None, None, None, input[0, int, false] AS value#114, assertnotnull(cast(value#114 as int)), input[0, int, false] AS value#113, IntegerType, IntegerType, false) AS foo_agg_1#116, fooagg(FooAgg(2), None, None, None, input[0, int, false] AS value#119, assertnotnull(cast(value#119 as int)), input[0, int, false] AS value#118, IntegerType, IntegerType, false) AS foo_agg_2#121, fooagg(FooAgg(3), None, None, None, input[0, int, false] AS value#124, assertnotnull(cast(value#124 as int)), input[0, int, false] AS value#123, IntegerType, IntegerType, false) AS foo_agg_3#126, fooagg(FooAgg(4), None, None, None, input[0, int, false] AS value#129, assertnotnull(cast(value#129 as int)), input[0, int, false] AS value#128, IntegerType, IntegerType, false) AS foo_agg_4#131, fooagg(FooAgg(5), None, None, None, input[0, int, false] AS value#134, assertnotnull(cast(value#134 as int)), input[0, int, false] AS value#133, IntegerType, IntegerType, false) AS foo_agg_5#136, fooagg(FooAgg(6), None, None, None, input[0, int, false] AS value#139, assertnotnull(cast(value#139 as int)), input[0, int, false] AS value#138, IntegerType, IntegerType, false) AS foo_agg_6#141]
{code}
 ;;;","23/Jan/20 07:14;gurwls223;Ah, thanks. I rushed to read. This issue still persists in the master as well.;;;","27/Feb/20 06:33;cloud_fan;Issue resolved by pull request 27499
[https://github.com/apache/spark/pull/27499];;;",,,,,,,,,,,,,,,,,,,,
Spark UI is not showing Aggregated Metrics by Executor in stage page,SPARK-30582,13280509,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,saurabhc100,saurabhc100,saurabhc100,20/Jan/20 09:32,29/Jan/20 14:51,13/Jul/23 08:46,29/Jan/20 14:51,3.0.0,,,,,,,,,,,,3.0.0,,,Web UI,,,,0,,,"There are scenarios where Spark History Server is located behind the VPC. So whenever api calls hit to get the executor Summary(allexecutors). There can be delay in getting the response of executor summary and in mean time ""stage-page-template.html"" is loaded and the response of executor Summary is not added to the stage-page-template.html.

As the result of which Aggregated Metrics by Executor in stage page is showing blank.

This scenario can be easily found in the cases when there is some proxy-server which is responsible for sending the request and response to spark History server.
 This can be reproduced in Knox/In-house proxy servers which are used to send and receive response to Spark History Server.

Alternative scenario to test this case, Open the spark UI in developer mode in browser add some breakpoint in stagepage.js, this will add some delay in getting the response and now if we check the spark UI for stage Aggregated Metrics by Executor in stage page is showing blank.",,saurabhc100,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/20 09:32;saurabhc100;SparkUIStagePage.mov;https://issues.apache.org/jira/secure/attachment/12991368/SparkUIStagePage.mov",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 29 14:51:08 UTC 2020,,,,,,,,,,"0|z0aokw:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"29/Jan/20 14:50;srowen;[~saurabhc100] it's OK now as it's resolved, but generally do not set Target or Fix version.;;;","29/Jan/20 14:51;srowen;Issue resolved by pull request 27292
[https://github.com/apache/spark/pull/27292];;;",,,,,,,,,,,,,,,,,,,,,,
Add a fallback Maven repository,SPARK-30572,13280430,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,19/Jan/20 21:23,22/Jan/20 01:10,13/Jul/23 08:46,20/Jan/20 01:43,2.4.4,3.0.0,,,,,,,,,,,2.4.5,3.0.0,,Build,,,,0,,,"This issue aims to add a fallback Maven repository when a mirror to `central` fail.
For example, when we use Google Maven Central in GitHub Action as a mirror of `central`,
this will be used when Google Maven Central is out of sync due to its late sync cycle.",,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30596,SPARK-30601,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 20 01:43:33 UTC 2020,,,,,,,,,,"0|z0ao3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/20 01:43;dongjoon;Issue resolved by pull request 27281
[https://github.com/apache/spark/pull/27281];;;",,,,,,,,,,,,,,,,,,,,,,,
Regression in the wide schema benchmark,SPARK-30564,13280307,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,petertoth,maxgekk,maxgekk,18/Jan/20 17:17,16/Apr/20 22:56,13/Jul/23 08:46,16/Apr/20 08:55,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"New results of WideSchemaBenchmark generated in the PR: https://github.com/apache/spark/pull/27078 show regressions up to 2 times.
Before:
{code}
2500 select expressions                        103 /  107          0.0   102962705.0       0.1X
{code}
https://github.com/apache/spark/pull/27078/files#diff-8d27bbf2f73a68bf0c2025f0702f7332L11
After:
{code}
2500 select expressions                             211            214           4          0.0   210927791.0       0.0X
{code}
https://github.com/apache/spark/pull/27078/files#diff-8d27bbf2f73a68bf0c2025f0702f7332R11",,maropu,maxgekk,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 16 08:55:58 UTC 2020,,,,,,,,,,"0|z0anc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/20 17:21;maxgekk;Here, regression is ~8 times [https://github.com/apache/spark/pull/27078/files#diff-8d27bbf2f73a68bf0c2025f0702f7332R74];;;","18/Jan/20 17:22;maxgekk;[~viirya] Please, take a look at this. Maybe it can be interesting for you.;;;","16/Apr/20 08:55;maropu;Resolved by https://github.com/apache/spark/pull/28083;;;",,,,,,,,,,,,,,,,,,,,,
Regressions in Join benchmarks,SPARK-30563,13280304,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,maxgekk,maxgekk,18/Jan/20 17:06,12/Dec/22 18:11,13/Jul/23 08:46,06/Mar/20 01:31,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Regenerated benchmark results in the https://github.com/apache/spark/pull/27078 shows many regressions in JoinBenchmark. The benchmarked queries slowed down by up to 3 times, see
old results:
https://github.com/apache/spark/pull/27078/files#diff-d5cbaab2b49ee9fddfa0e229de8f607dL10
new results:
https://github.com/apache/spark/pull/27078/files#diff-d5cbaab2b49ee9fddfa0e229de8f607dR10

One of the difference in queries is using the `NoOp` datasource in new queries.",,maxgekk,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 06 01:31:56 UTC 2020,,,,,,,,,,"0|z0anbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/20 09:06;petertoth;[~maxgekk], [~dongjoon], [~hyukjin.kwon] it looks like the change in the {{JoinBenchmark}} ([https://github.com/apache/spark/commit/f5118f81e395bde0cd8253dbef6a9e6455c3958a#diff-da1033f4d10b6046046202dd8f85e3f7L49-R49]) causes this regression. If we used {{df.groupBy().count().noop()}} and measure the same as previously there won't be any regression in this suite. Please see the results running the fixed benchmark on my machine: [https://github.com/peter-toth/spark/commit/207d15d1801cfcf9c40635a481d4aa7192911548]

This is because lots of rows are returned and we spend a lot of time in [this loop|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala#L438-L442] even if {{NoopWriter}} does nothing. 

Another very minor improvement regarding `NoOp` datasource could be to turn off using commit coordinator in {{NoopBatchWrite}}.

Shall I open a PR with these changes (excluding the non-official benchmark result)?

 ;;;","04/Mar/20 10:10;maxgekk;> we spend a lot of time in this loop even

The loop just forces materialization of joined rows. By df.groupBy().count(), you skip some steps in join, it seems. I think in most cases, users need results of join but not just count on top of it.;;;","04/Mar/20 10:13;maxgekk;[~petertoth] If you think it is possible to avoid some overhead of NoOp datasource, please, open a PR.;;;","06/Mar/20 01:31;gurwls223;Fixed in https://github.com/apache/spark/pull/27791;;;",,,,,,,,,,,,,,,,,,,,
Copy sparkContext.localproperties to child thread inSubqueryExec.executionContext,SPARK-30556,13280187,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ajithshetty,ajithshetty,ajithshetty,17/Jan/20 17:08,20/Feb/20 04:16,13/Jul/23 08:46,23/Jan/20 02:21,2.3.4,2.4.4,3.0.0,,,,,,,,,,2.4.5,3.0.0,,SQL,,,,0,,,"Local properties set via sparkContext are not available as TaskContext properties when executing  jobs and threadpools have idle threads which are reused

Explanation:
When SubqueryExec, the {{relationFuture}} is evaluated via a separate thread. The threads inherit the {{localProperties}} from sparkContext as they are the child threads.
These threads are controlled via the executionContext (thread pools). Each Thread pool has a default {{keepAliveSeconds}} of 60 seconds for idle threads.
Scenarios where the thread pool has threads which are idle and reused for a subsequent new query, the thread local properties will not be inherited from spark context (thread properties are inherited only on thread creation) hence end up having old or no properties set. This will cause taskset properties to be missing when properties are transferred by child thread via {{sparkContext.runJob/submitJob}}",,ajithshetty,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 23 17:02:56 UTC 2020,,,,,,,,,,"0|z0amlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/20 19:26;dongjoon;Hi, [~ajithshetty].
This looks important. Could you check the older Spark versions like 2.3.x and 2.2.x, too? ;;;","23/Jan/20 02:21;dongjoon;Issue resolved by pull request 27267
[https://github.com/apache/spark/pull/27267];;;","23/Jan/20 10:29;ajithshetty;Raised backport PR for branch 2.4 [https://github.com/apache/spark/pull/27340];;;","23/Jan/20 10:29;ajithshetty;Yes, it exist in lower version like 2.3.x too;;;","23/Jan/20 17:02;dongjoon;Thank you for confirming, [~ajithshetty].
This is backported to branch-2.4 via https://github.com/apache/spark/pull/27340;;;",,,,,,,,,,,,,,,,,,,
MERGE INTO insert action should only access columns from source table,SPARK-30555,13280180,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,17/Jan/20 16:33,26/Dec/20 11:46,13/Jul/23 08:46,22/Jan/20 14:43,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33917,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 22 14:43:45 UTC 2020,,,,,,,,,,"0|z0amjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/20 14:43;cloud_fan;Issue resolved by pull request 27265
[https://github.com/apache/spark/pull/27265];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix structured-streaming java example error,SPARK-30553,13280168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,bettermouse,bettermouse,bettermouse,17/Jan/20 15:48,22/Jan/20 05:45,13/Jul/23 08:46,22/Jan/20 05:38,2.1.1,2.2.3,2.3.4,2.4.4,,,,,,,,,2.4.5,3.0.0,,Documentation,,,,0,,,"[http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking]

I write code according to this by java and scala.

java
{code:java}
    public static void main(String[] args) throws StreamingQueryException {
        SparkSession spark = SparkSession.builder().appName(""test"").master(""local[*]"")
                .config(""spark.sql.shuffle.partitions"", 1)
                .getOrCreate();        Dataset<Row> lines = spark.readStream().format(""socket"")
                .option(""host"", ""skynet"")
                .option(""includeTimestamp"", true)
                .option(""port"", 8888).load();
        Dataset<Row> words = lines.select(""timestamp"", ""value"");
        Dataset<Row> count = words.withWatermark(""timestamp"", ""10 seconds"")
                .groupBy(functions.window(words.col(""timestamp""), ""10 seconds"", ""10 seconds"")
                        , words.col(""value"")).count();
        StreamingQuery start = count.writeStream()
                .outputMode(""update"")
                .format(""console"").start();
        start.awaitTermination();    }
{code}
scala

 
{code:java}
 def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder.appName(""test"").
      master(""local[*]"").
      config(""spark.sql.shuffle.partitions"", 1)
      .getOrCreate
    import spark.implicits._
    val lines = spark.readStream.format(""socket"").
      option(""host"", ""skynet"").option(""includeTimestamp"", true).
      option(""port"", 8888).load
    val words = lines.select(""timestamp"", ""value"")
    val count = words.withWatermark(""timestamp"", ""10 seconds"").
      groupBy(window($""timestamp"", ""10 seconds"", ""10 seconds""), $""value"")
      .count()
    val start = count.writeStream.outputMode(""update"").format(""console"").start
    start.awaitTermination()
  }
{code}
This is according to official documents. written in Java I found metrics ""stateOnCurrentVersionSizeBytes"" always increase .but scala is ok.

 

java

 
{code:java}
== Physical Plan ==
WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@4176a001
+- *(4) HashAggregate(keys=[window#11, value#0], functions=[count(1)], output=[window#11, value#0, count#10L])
   +- StateStoreSave [window#11, value#0], state info [ checkpoint = file:/C:/Users/chenhao/AppData/Local/Temp/temporary-63acf9b1-9249-40db-ab33-9dcadf5736aa/state, runId = d38b8fee-6cd0-441c-87da-a4e3660856a3, opId = 0, ver = 5, numPartitions = 1], Update, 1579274372624, 2
      +- *(3) HashAggregate(keys=[window#11, value#0], functions=[merge_count(1)], output=[window#11, value#0, count#21L])
         +- StateStoreRestore [window#11, value#0], state info [ checkpoint = file:/C:/Users/chenhao/AppData/Local/Temp/temporary-63acf9b1-9249-40db-ab33-9dcadf5736aa/state, runId = d38b8fee-6cd0-441c-87da-a4e3660856a3, opId = 0, ver = 5, numPartitions = 1], 2
            +- *(2) HashAggregate(keys=[window#11, value#0], functions=[merge_count(1)], output=[window#11, value#0, count#21L])
               +- Exchange hashpartitioning(window#11, value#0, 1)
                  +- *(1) HashAggregate(keys=[window#11, value#0], functions=[partial_count(1)], output=[window#11, value#0, count#21L])
                     +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#1, TimestampType, LongType) - 0) as double) / 1.0E7)) as double) = (cast((precisetimestampconversion(timestamp#1, TimestampType, LongType) - 0) as double) / 1.0E7)) THEN (CEIL((cast((precisetimestampconversion(timestamp#1, TimestampType, LongType) - 0) as double) / 1.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#1, TimestampType, LongType) - 0) as double) / 1.0E7)) END + 0) - 1) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#1, TimestampType, LongType) - 0) as double) / 1.0E7)) as double) = (cast((precisetimestampconversion(timestamp#1, TimestampType, LongType) - 0) as double) / 1.0E7)) THEN (CEIL((cast((precisetimestampconversion(timestamp#1, TimestampType, LongType) - 0) as double) / 1.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#1, TimestampType, LongType) - 0) as double) / 1.0E7)) END + 0) - 1) * 10000000) + 10000000), LongType, TimestampType)) AS window#11, value#0]
                        +- *(1) Filter isnotnull(timestamp#1)
                           +- EventTimeWatermark timestamp#1: timestamp, interval 10 seconds
                              +- LocalTableScan <empty>, [timestamp#1, value#0]

{code}
 

 

scala 

 

 
{code:java}
WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@4149892c
+- *(4) HashAggregate(keys=[window#11-T10000ms, value#0], functions=[count(1)], output=[window#6-T10000ms, value#0, count#10L])
   +- StateStoreSave [window#11-T10000ms, value#0], state info [ checkpoint = file:/C:/Users/chenhao/AppData/Local/Temp/temporary-8b17f74b-0963-4fee-82cd-2c1e63a75a98/state, runId = dac4413d-5a82-4d61-b134-c81bfab704d8, opId = 0, ver = 7, numPartitions = 1], Update, 1579275214256, 2
      +- *(3) HashAggregate(keys=[window#11-T10000ms, value#0], functions=[merge_count(1)], output=[window#11-T10000ms, value#0, count#21L])
         +- StateStoreRestore [window#11-T10000ms, value#0], state info [ checkpoint = file:/C:/Users/chenhao/AppData/Local/Temp/temporary-8b17f74b-0963-4fee-82cd-2c1e63a75a98/state, runId = dac4413d-5a82-4d61-b134-c81bfab704d8, opId = 0, ver = 7, numPartitions = 1], 2
            +- *(2) HashAggregate(keys=[window#11-T10000ms, value#0], functions=[merge_count(1)], output=[window#11-T10000ms, value#0, count#21L])
               +- Exchange hashpartitioning(window#11-T10000ms, value#0, 1)
                  +- *(1) HashAggregate(keys=[window#11-T10000ms, value#0], functions=[partial_count(1)], output=[window#11-T10000ms, value#0, count#21L])
                     +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#1-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) as double) = (cast((precisetimestampconversion(timestamp#1-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) THEN (CEIL((cast((precisetimestampconversion(timestamp#1-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#1-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) END + 0) - 1) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#1-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) as double) = (cast((precisetimestampconversion(timestamp#1-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) THEN (CEIL((cast((precisetimestampconversion(timestamp#1-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#1-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) END + 0) - 1) * 10000000) + 10000000), LongType, TimestampType)) AS window#11-T10000ms, value#0]
                        +- *(1) Filter isnotnull(timestamp#1-T10000ms)
                           +- EventTimeWatermark timestamp#1: timestamp, interval 10 seconds
                              +- LocalTableScan <empty>, [timestamp#1, value#0]
{code}
 

 you also can debug in statefulOperators.scala  
{code:java}
  protected def removeKeysOlderThanWatermark(
      storeManager: StreamingAggregationStateManager,
      store: StateStore): Unit = {
    if (watermarkPredicateForKeys.nonEmpty) {
      storeManager.keys(store).foreach { keyRow =>
        if (watermarkPredicateForKeys.get.eval(keyRow)) {
          storeManager.remove(store, keyRow)  //this line
        }
      }
    }
  }
}

{code}
you will find java does not remove old state.

 I think java should write like this
{code:java}
        SparkSession spark = SparkSession.builder().appName(""test"").master(""local[*]"")
                .config(""spark.sql.shuffle.partitions"", 1)
                .getOrCreate();        Dataset<Row> lines = spark.readStream().format(""socket"")
                .option(""host"", ""skynet"")
                .option(""includeTimestamp"",true)
                .option(""port"", 8888).load();
        Dataset<Row> words = lines.select(""timestamp"", ""value"");
        Dataset<Row> wordsWatermark = words.withWatermark(""timestamp"", ""10 seconds"");
        Dataset<Row> count = wordsWatermark
                .groupBy(functions.window(wordsWatermark.col(""timestamp""), ""10 seconds"", ""10 seconds"")
                        , wordsWatermark.col(""value"")).count();
        StreamingQuery start = count.writeStream()
                .outputMode(""update"")
                .format(""console"").start();
        start.awaitTermination();    }
{code}",,bettermouse,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18669,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 22 05:38:53 UTC 2020,,,,,,,,,,"0|z0amh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/20 05:38;dongjoon;Issue resolved by pull request 27268
[https://github.com/apache/spark/pull/27268];;;",,,,,,,,,,,,,,,,,,,,,,,
Flaky test: org.apache.spark.sql.kafka010.KafkaDelegationTokenSuite,SPARK-30541,13280064,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gsomogyi,kabhwan,kabhwan,17/Jan/20 07:42,24/Mar/20 08:36,13/Jul/23 08:46,22/Mar/20 02:00,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,Structured Streaming,,,0,,,"The test suite has been failing intermittently as of now:

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/116862/testReport/]

 

org.apache.spark.sql.kafka010.KafkaDelegationTokenSuite.(It is not a test it is a sbt.testing.SuiteSelector)
  
{noformat}
Error Details
org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 3939 times over 1.0001223535333332 minutes. Last failure message: KeeperErrorCode = AuthFailed for /brokers/ids.

Stack Trace
sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 3939 times over 1.0001223535333332 minutes. Last failure message: KeeperErrorCode = AuthFailed for /brokers/ids.
	at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432)
	at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439)
	at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391)
	at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:479)
	at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:337)
	at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:336)
	at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:479)
	at org.apache.spark.sql.kafka010.KafkaTestUtils.setup(KafkaTestUtils.scala:292)
	at org.apache.spark.sql.kafka010.KafkaDelegationTokenSuite.beforeAll(KafkaDelegationTokenSuite.scala:49)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:58)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:317)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:510)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: sbt.ForkMain$ForkError: org.apache.zookeeper.KeeperException$AuthFailedException: KeeperErrorCode = AuthFailed for /brokers/ids
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:130)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:54)
	at kafka.zookeeper.AsyncResponse.resultException(ZooKeeperClient.scala:554)
	at kafka.zk.KafkaZkClient.getChildren(KafkaZkClient.scala:719)
	at kafka.zk.KafkaZkClient.getSortedBrokerList(KafkaZkClient.scala:455)
	at kafka.zk.KafkaZkClient.getAllBrokersInCluster(KafkaZkClient.scala:404)
	at org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$setup$3(KafkaTestUtils.scala:293)
	at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395)
	at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409)
	... 20 more
{noformat}",,dongjoon,gsomogyi,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31050,,,,,,,,,"06/Mar/20 21:29;gsomogyi;consoleText_NOK.txt;https://issues.apache.org/jira/secure/attachment/12995911/consoleText_NOK.txt","06/Mar/20 21:29;gsomogyi;consoleText_OK.txt;https://issues.apache.org/jira/secure/attachment/12995912/consoleText_OK.txt","06/Mar/20 21:29;gsomogyi;unit-tests_NOK.log;https://issues.apache.org/jira/secure/attachment/12995913/unit-tests_NOK.log","06/Mar/20 21:29;gsomogyi;unit-tests_OK.log;https://issues.apache.org/jira/secure/attachment/12995914/unit-tests_OK.log",,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 22 02:00:29 UTC 2020,,,,,,,,,,"0|z0alu0:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"05/Mar/20 08:19;dongjoon;Please see the PR. This is decided to be a blocker for 3.0.0.;;;","06/Mar/20 21:27;gsomogyi;As I see there are 2 problems:
 * Kafka broker is not coming up in time: I've had discussion with the Kafka guys and their solution to this is to execute their test with retry
 * Client not found in Kerberos database: which is reproduced here: [https://github.com/apache/spark/pull/27810]

I'm going to attach the logs for the second problem here not because jenkins results going to disappear.

 ;;;","06/Mar/20 21:30;gsomogyi;Going to check them next week...;;;","11/Mar/20 10:12;gsomogyi;The first problem is obvious, Kafka is not coming up all the time consistently. I think not much to do there unless Kafka community is fixing the issue. As a temporary solution retry can be used in the test.

 
The second problem is also coming from the Kafka side:
{code:java}
[info]   Cause: org.apache.kafka.common.KafkaException: javax.security.auth.login.LoginException: Client not found in Kerberos database (6) - Client not found in Kerberos database
{code}

When I've reproduced the issue locally I've realised that:
* KDC didn't throw any exception while the mentioned user created
* The keytab file is readable and able to do kinit with it

Maybe it's another flaky behaviour on the Kafka side?!

All in all since the broker is flaky and KafkaAdminClient shown also some flakyness my suggestion is to use testRetry until the mentioned problems are not solved in Kafka.
;;;","22/Mar/20 02:00;dongjoon;Issue resolved by pull request 27877
[https://github.com/apache/spark/pull/27877];;;",,,,,,,,,,,,,,,,,,,
toPandas gets wrong dtypes when applied on empty DF when Arrow enabled,SPARK-30537,13280033,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pralabhkumar,gurwls223,,17/Jan/20 02:55,12/Dec/22 18:11,13/Jul/23 08:46,23/Nov/21 00:05,2.4.4,3.0.0,,,,,,,,,,,3.3.0,,,PySpark,SQL,,,0,,,Same issue with SPARK-29188 persists when Arrow optimization is enabled.,,apachespark,bryanc,pralabhkumar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29188,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 23 00:05:54 UTC 2021,,,,,,,,,,"0|z0aln4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/21 04:59;pralabhkumar;[~hyukjin.kwon]

 

I would like to work on this , please let me know if I can work on this ;;;","22/Oct/21 05:13;gurwls223;Please go ahead!;;;","22/Oct/21 05:41;pralabhkumar;Thx [~hyukjin.kwon] , working on this ;;;","27/Oct/21 12:36;apachespark;User 'pralabhkumar' has created a pull request for this issue:
https://github.com/apache/spark/pull/34401;;;","27/Oct/21 12:37;apachespark;User 'pralabhkumar' has created a pull request for this issue:
https://github.com/apache/spark/pull/34401;;;","23/Nov/21 00:05;gurwls223;Issue resolved by pull request 34401
[https://github.com/apache/spark/pull/34401];;;",,,,,,,,,,,,,,,,,,
DataFrameStatFunctions.approxQuantile doesn't work with TABLE.COLUMN syntax,SPARK-30532,13279965,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,okachaiev,chris_suchanek,chris_suchanek,16/Jan/20 19:24,30/Mar/20 05:43,13/Jul/23 08:46,30/Mar/20 05:38,2.4.4,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"The DataFrameStatFunctions.approxQuantile doesn't work with fully qualified column name (i.e TABLE_NAME.COLUMN_NAME) which is often the way you refer to the column when working with joined dataframes having ambiguous column names.


See code below for example.
{code:java}

import scala.util.Random
val l = (0 to 1000).map(_ => Random.nextGaussian() * 1000)
val df1 = sc.parallelize(l).toDF(""num"").as(""tt1"")
val df2 = sc.parallelize(l).toDF(""num"").as(""tt2"")
val dfx = df2.crossJoin(df1)

dfx.stat.approxQuantile(""tt1.num"", Array(0.1), 0.0)
// throws: java.lang.IllegalArgumentException: Field ""tt1.num"" does not exist.
Available fields: num

dfx.stat.approxQuantile(""num"", Array(0.1), 0.0)
// throws: org.apache.spark.sql.AnalysisException: Reference 'num' is ambiguous, could be: tt2.num, tt1.num.;{code}
 

 ",,chris_suchanek,cloud_fan,okachaiev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 30 05:38:02 UTC 2020,,,,,,,,,,"0|z0al80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/20 07:17;okachaiev;The same applies to other stats functions: {{cov}}, {{corr}} and {{freqItems}}.

I'm working on PR to fix all of them (the problem is similar in all cases).;;;","30/Mar/20 05:38;cloud_fan;Issue resolved by pull request 27916
[https://github.com/apache/spark/pull/27916];;;",,,,,,,,,,,,,,,,,,,,,,
"CSV load followed by ""is null"" filter produces incorrect results",SPARK-30530,13279866,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,jlowe,jlowe,16/Jan/20 16:23,21/Jan/20 01:20,13/Jul/23 08:46,19/Jan/20 09:23,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Trying to filter on is null from values loaded from a CSV file has regressed recently and now produces incorrect results.

Given a CSV file with the contents:
{noformat:title=floats.csv}
100.0,1.0,
200.0,,
300.0,3.0,
1.0,4.0,
,4.0,
500.0,,
,6.0,
-500.0,50.5
 {noformat}
Filtering this data for the first column being null should return exactly two rows, but it is returning extraneous rows with nulls:
{noformat}
scala> val schema = StructType(Array(StructField(""floats"", FloatType, true),StructField(""more_floats"", FloatType, true)))
schema: org.apache.spark.sql.types.StructType = StructType(StructField(floats,FloatType,true), StructField(more_floats,FloatType,true))

scala> val df = spark.read.schema(schema).csv(""floats.csv"")
df: org.apache.spark.sql.DataFrame = [floats: float, more_floats: float]

scala> df.filter(""floats is null"").show
+------+-----------+
|floats|more_floats|
+------+-----------+
|  null|       null|
|  null|       null|
|  null|       null|
|  null|       null|
|  null|        4.0|
|  null|       null|
|  null|        6.0|
+------+-----------+
{noformat}",,cloud_fan,jlowe,maxgekk,mithun,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30323,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 19 09:23:49 UTC 2020,,,,,,,,,,"0|z0akzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/20 16:28;jlowe;The regressed behavior was introduced by this commit:

{noformat}
commit 4e50f0291f032b4a5c0b46ed01fdef14e4cbb050
Author: Maxim Gekk <max.gekk@gmail.com>
Date: Thu Jan 16 13:10:08 2020 +0900

[SPARK-30323][SQL] Support filters pushdown in CSV datasource
{noformat}

[~maxgekk] would you take a look?
;;;","16/Jan/20 16:44;maxgekk;[~jlowe] Thank you for the bug report. I will take a look at it.;;;","17/Jan/20 08:32;maxgekk;[~jlowe] I prepared a fix for the issue. [~hyukjin.kwon] [~cloud_fan] Could you review it, please.;;;","19/Jan/20 09:23;cloud_fan;Issue resolved by pull request 27239
[https://github.com/apache/spark/pull/27239];;;",,,,,,,,,,,,,,,,,,,,
Potential performance regression with DPP subquery duplication,SPARK-30528,13279811,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maryannxue,mayurb31,mayurb31,16/Jan/20 12:13,17/May/20 17:58,13/Jul/23 08:46,13/Feb/20 11:44,3.0.0,,,,,,,,,,,,3.0.0,,,Optimizer,SQL,,,1,performance,,"In DPP, heuristics to decide if DPP is going to benefit relies on the sizes of the tables in the right subtree of the join. This might not be a correct estimate especially when the detailed column level stats are not available.
{code:java}
    // the pruning overhead is the total size in bytes of all scan relations
    val overhead = otherPlan.collectLeaves().map(_.stats.sizeInBytes).sum.toFloat
    filterRatio * partPlan.stats.sizeInBytes.toFloat > overhead.toFloat
{code}
Also, DPP executes the entire right side of the join as a subquery because of which multiple scans happen for the tables in the right subtree of the join. This can cause issues when join is non-Broadcast Hash Join (BHJ) and reuse of the subquery result does not happen. Also, I couldn’t figure out, why do the results from the subquery get re-used only for BHJ?

 

Consider a query,
{code:java}
SELECT * 
FROM   store_sales_partitioned 
       JOIN (SELECT * 
             FROM   store_returns_partitioned, 
                    date_dim 
             WHERE  sr_returned_date_sk = d_date_sk) ret_date 
         ON ss_sold_date_sk = d_date_sk 
WHERE  d_fy_quarter_seq > 0 
{code}
DPP will kick-in for both the join. (Please check the image plan.png attached below for the plan)

Some of the observations -
 * Based on heuristics, DPP would go ahead with pruning if the cost of scanning the tables in the right sub-tree of the join is less than the benefit due to pruning. This is due to the reason that multiple scans will be needed for an SMJ. But heuristics simply checks if the benefits offset the cost of multiple scans and do not take into consideration other operations like Join, etc in the right subtree which can be quite expensive. This issue will be particularly prominent when detailed column level stats are not available. In the example above, a decision that pruningHasBenefit was made on the basis of sizes of the tables store_returns_partitioned and date_dim but did not take into consideration the join between them before the join happens with the store_sales_partitioned table.

 * Multiple scans are needed when the join is SMJ as the reuse of the exchanges does not happen. This is because Aggregate gets added on top of the right subtree to be executed as a subquery in order to prune only required columns. Here, scanning all the columns as the right subtree of the join would, and reusing the same exchange might be more helpful as it avoids duplicate scans.

This was just a representative example, but in-general for cases such as in the image cases.png below, DPP can cause performance issues.

 

Also, for the cases when there are multiple DPP compatible join conditions in the same join, the entire right subtree of the join would be executed as a subquery that many times. Consider an example,
{code:java}
SELECT * 
FROM   partitionedtable 
       JOIN nonpartitionedtable 
         ON partcol1 = col1 
            AND partcol2 = col2 
WHERE  nonpartitionedtable.id > 0 
{code}
Here the right subtree of the join (scan of table nonpartitionedtable) would be executed twice as a subquery, once each for the every join condition. These two subqueries should be aggregated and executed only once as they are almost the same apart from the columns that they prune. Check the image dup_subquery.png attached below for the details.",,cloud_fan,karup1990,maryannxue,mayurb31,prakharjain09,rajesh.balamohan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/20 12:14;mayurb31;cases.png;https://issues.apache.org/jira/secure/attachment/12991121/cases.png","20/Jan/20 07:14;mayurb31;dup_subquery.png;https://issues.apache.org/jira/secure/attachment/12991358/dup_subquery.png","16/Jan/20 12:14;mayurb31;plan.png;https://issues.apache.org/jira/secure/attachment/12991122/plan.png",,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 13 11:44:15 UTC 2020,,,,,,,,,,"0|z0aknk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/20 07:16;mayurb31;[~maryannxue] can you please comment on this?;;;","21/Jan/20 17:03;maryannxue;Good point, [~mayurb31]!

1. Heuristics: yes, we should improve the cost estimate for the filter plan. As a quick workaround, though, you could set 

""spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio"" to ""0.0"", which would disable this kind of DPP if columns stats are not available.

2. Reuse: yes, that's a dilemma here: first of all we wanna reduce the result set returned to the driver for pruning values, and that's why the Aggregate is added. It might kill some potential opportunities for exchange reuse if the filter plan contains another join, but that kind of potential opportunity is not fully guaranteed even if we didn't push down the Aggregate, for the  join in the filter plan can turn out to be a BHJ. However, `pruningHasBenefit` had intended to cost the entire DPP subquery as overhead without considering reuse, so this takes us back to point 1: we should improve the costing of heuristics so that this kind of DPP should not be triggered at all if the scan + join would be just too much work.

3. Can you attach the query plan instead of UI for your last example? I think the reuse did not happen because the first subquery selects `col1` while the second `col2`? ;;;","22/Jan/20 06:08;mayurb31;Thanks for the explanation [~maryannxue]
 # Should DPP be turned off by default till the heuristics are improved or keep having it turned on by default but don't do DPP when the column level stats are not available? Because for some cases this can be really disastrous.
 # Can we use the bloom filter to store the pruning values (for non-Broadcast Hash Join)? This will have multiple advantages -
 ## The size of the result returned to the driver would be way smaller
 ## Faster lookups compared to hashSet
 ## Reuse of the exchange will happen (because we won't be adding Aggregate on top)
 ## Duplicate subqueries because of multiple join conditions on partitioned columns will get removed (cases like example 3 in the description above)

         This will require more thoughts though. Let me know if this sounds feasible and useful, then I can get back with more details and can pick it up as well. 

       3. Yes, one of the subqueries selects `col1` and the other selects `col2`.
{code:java}
== Physical Plan ==                                                             
*(5) SortMergeJoin [partcol1#2L, partcol2#3], [col1#5L, col2#6], Inner
:- *(2) Sort [partcol1#2L ASC NULLS FIRST, partcol2#3 ASC NULLS FIRST], false, 0
:  +- Exchange hashpartitioning(partcol1#2L, partcol2#3, 200), true, [id=#103]
:     +- *(1) ColumnarToRow
:        +- FileScan parquet default.partitionedtable[id#0L,name#1,partCol1#2L,partCol2#3] Batched: true, DataFilters: [], Format: Parquet, Location: PrunedInMemoryFileIndex[file:/~/src/spark/bin/sp..., PartitionFilters: [isnotnull(partCol2#3), isnotnull(partCol1#2L), dynamicpruningexpression(partCol1#2L IN subquery#..., PushedFilters: [], ReadSchema: struct<id:bigint,name:string>
:              :- Subquery subquery#19, [id=#49]
:              :  +- *(2) HashAggregate(keys=[col1#5L], functions=[])
:              :     +- Exchange hashpartitioning(col1#5L, 200), true, [id=#45]
:              :        +- *(1) HashAggregate(keys=[col1#5L], functions=[])
:              :           +- *(1) Project [col1#5L]
:              :              +- *(1) Filter (((isnotnull(id#4L) AND (id#4L > 0)) AND isnotnull(col2#6)) AND isnotnull(col1#5L))
:              :                 +- *(1) ColumnarToRow
:              :                    +- FileScan parquet default.nonpartitionedtable[id#4L,col1#5L,col2#6] Batched: true, DataFilters: [isnotnull(id#4L), (id#4L > 0), isnotnull(col2#6), isnotnull(col1#5L)], Format: Parquet, Location: InMemoryFileIndex[file:/~/src/spark/bin/spark-wa..., PartitionFilters: [], PushedFilters: [IsNotNull(id), GreaterThan(id,0), IsNotNull(col2), IsNotNull(col1)], ReadSchema: struct<id:bigint,col1:bigint,col2:string>
:              +- Subquery subquery#21, [id=#82]
:                 +- *(2) HashAggregate(keys=[col2#6], functions=[])
:                    +- Exchange hashpartitioning(col2#6, 200), true, [id=#78]
:                       +- *(1) HashAggregate(keys=[col2#6], functions=[])
:                          +- *(1) Project [col2#6]
:                             +- *(1) Filter (((isnotnull(id#4L) AND (id#4L > 0)) AND isnotnull(col2#6)) AND isnotnull(col1#5L))
:                                +- *(1) ColumnarToRow
:                                   +- FileScan parquet default.nonpartitionedtable[id#4L,col1#5L,col2#6] Batched: true, DataFilters: [isnotnull(id#4L), (id#4L > 0), isnotnull(col2#6), isnotnull(col1#5L)], Format: Parquet, Location: InMemoryFileIndex[file:/~/src/spark/bin/spark-wa..., PartitionFilters: [], PushedFilters: [IsNotNull(id), GreaterThan(id,0), IsNotNull(col2), IsNotNull(col1)], ReadSchema: struct<id:bigint,col1:bigint,col2:string>
+- *(4) Sort [col1#5L ASC NULLS FIRST, col2#6 ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(col1#5L, col2#6, 200), true, [id=#113]
      +- *(3) Project [id#4L, col1#5L, col2#6, name#7]
         +- *(3) Filter (((isnotnull(id#4L) AND (id#4L > 0)) AND isnotnull(col2#6)) AND isnotnull(col1#5L))
            +- *(3) ColumnarToRow
               +- FileScan parquet default.nonpartitionedtable[id#4L,col1#5L,col2#6,name#7] Batched: true, DataFilters: [isnotnull(id#4L), (id#4L > 0), isnotnull(col2#6), isnotnull(col1#5L)], Format: Parquet, Location: InMemoryFileIndex[file:/~/src/spark/bin/spark-wa..., PartitionFilters: [], PushedFilters: [IsNotNull(id), GreaterThan(id,0), IsNotNull(col2), IsNotNull(col1)], ReadSchema: struct<id:bigint,col1:bigint,col2:string,name:string> 
{code}
 

           If we don't decide to go with removing Aggregate (not using BloomFilter), should we combine such DPP subqueries into a                     single sub-query? We can avoid duplicate computation this way.;;;","22/Jan/20 16:39;maryannxue;# Turning off the non-broadcast-reuse DPP by default is an option.
 # So it really depends: bloom filter would reduce the partition filtering rate, thus getting you less benefit, and the penalties are (without extra aggregate):
 ## Duplicate scans or other ops like joins, if the filter plan is partially reused;
 ## Otherwise, you could implement sth. just like broadcast reuse to fully reuse the shuffle exchange of the filter plan, but you need to be aware that you are now serializing the two child operators' computation of an SMJ.
 # Yes, you could.;;;","13/Feb/20 11:44;cloud_fan;Issue resolved by pull request 27551
[https://github.com/apache/spark/pull/27551];;;",,,,,,,,,,,,,,,,,,,
Use a dedicated boss event group loop in the netty pipeline for external shuffle service,SPARK-30512,13279433,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csingh,csingh,csingh,14/Jan/20 19:54,17/May/20 18:30,13/Jul/23 08:46,29/Jan/20 21:10,3.0.0,,,,,,,,,,,,2.4.5,3.0.0,,Shuffle,Spark Core,,,0,,,"We have been seeing a large number of SASL authentication (RPC requests) timing out with the external shuffle service.
 The issue and all the analysis we did is described here:
 [https://github.com/netty/netty/issues/9890]

I added a {{LoggingHandler}} to netty pipeline and realized that even the channel registration is delayed by 30 seconds. 
 In the Spark External Shuffle service, the boss event group and the worker event group are same which is causing this delay.
{code:java}
    EventLoopGroup bossGroup =
      NettyUtils.createEventLoop(ioMode, conf.serverThreads(), conf.getModuleName() + ""-server"");
    EventLoopGroup workerGroup = bossGroup;

    bootstrap = new ServerBootstrap()
      .group(bossGroup, workerGroup)
      .channel(NettyUtils.getServerChannelClass(ioMode))
      .option(ChannelOption.ALLOCATOR, allocator)
      .childOption(ChannelOption.ALLOCATOR, allocator);
{code}
When the load at the shuffle service increases, since the worker threads are busy with existing channels, registering new channels gets delayed.

The fix is simple. I created a dedicated boss thread event loop group with 1 thread.
{code:java}
    EventLoopGroup bossGroup = NettyUtils.createEventLoop(ioMode, 1,
      conf.getModuleName() + ""-boss"");
    EventLoopGroup workerGroup =  NettyUtils.createEventLoop(ioMode, conf.serverThreads(),
    conf.getModuleName() + ""-server"");

    bootstrap = new ServerBootstrap()
      .group(bossGroup, workerGroup)
      .channel(NettyUtils.getServerChannelClass(ioMode))
      .option(ChannelOption.ALLOCATOR, allocator)
{code}
This fixed the issue.
 We just need 1 thread in the boss group because there is only a single server bootstrap.

 ",,csingh,mshen,sbeeram,siddarthasagar,Steven Rand,tgraves,zhenhuawang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24355,SPARK-29206,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 29 21:10:53 UTC 2020,,,,,,,,,,"0|z0aibs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/20 19:57;csingh;Please assign the issue to me so I can open up a PR.;;;","29/Jan/20 21:10;tgraves;this could be pulled back into branch-2.X as well;;;",,,,,,,,,,,,,,,,,,,,,,
Spark marks intentionally killed speculative tasks as pending leads to holding idle executors,SPARK-30511,13279428,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zebingl,zebingl,zebingl,14/Jan/20 19:32,17/May/20 17:48,13/Jul/23 08:46,31/Jan/20 14:50,2.3.0,,,,,,,,,,,,3.0.0,,,Scheduler,Spark Core,,,0,,,"*TL;DR*
 When speculative tasks fail/get killed, they are still considered as pending and count towards the calculation of number of needed executors.
h3. Symptom

In one of our production job (where it's running 4 tasks per executor), we found that it was holding 6 executors at the end with only 2 tasks running (1 speculative). With more logging enabled, we found the job printed:
{code:java}
pendingTasks is 0 pendingSpeculativeTasks is 17 totalRunningTasks is 2
{code}
 while the job only had 1 speculative task running and 16 speculative tasks intentionally killed because of corresponding original tasks had finished.

An easy repro of the issue (`--conf spark.speculation=true --conf spark.executor.cores=4 --conf spark.dynamicAllocation.maxExecutors=1000` in cluster mode):
{code:java}
val n = 4000
val someRDD = sc.parallelize(1 to n, n)
someRDD.mapPartitionsWithIndex( (index: Int, it: Iterator[Int]) => {
if (index < 300 && index >= 150) {
    Thread.sleep(index * 1000) // Fake running tasks
} else if (index == 300) {
    Thread.sleep(1000 * 1000) // Fake long running tasks
}
it.toList.map(x => index + "", "" + x).iterator
}).collect
{code}
You will see when running the last task, we would be hold 38 executors (see attachment), which is exactly (152 + 3) / 4 = 38.
h3. The Bug

Upon examining the code of _pendingSpeculativeTasks_: 
{code:java}
stageAttemptToNumSpeculativeTasks.map { case (stageAttempt, numTasks) =>
  numTasks - stageAttemptToSpeculativeTaskIndices.get(stageAttempt).map(_.size).getOrElse(0)
}.sum
{code}
where _stageAttemptToNumSpeculativeTasks(stageAttempt)_ is incremented on _onSpeculativeTaskSubmitted_, but never decremented.  _stageAttemptToNumSpeculativeTasks -= stageAttempt_ is performed on stage completion. *This means Spark is marking ended speculative tasks as pending, which leads to Spark to hold more executors that it actually needs!*

I will have a PR ready to fix this issue, along with SPARK-28403 too

 

 

 ",,zebingl,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28403,,,,,,,,,,,,,,,,SPARK-27082,,,,,,"15/Jan/20 19:14;zebingl;Screen Shot 2020-01-15 at 11.13.17.png;https://issues.apache.org/jira/secure/attachment/12991024/Screen+Shot+2020-01-15+at+11.13.17.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-01-14 19:32:10.0,,,,,,,,,,"0|z0aiaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecation log warning is not printed in Avro schema inferring,SPARK-30509,13279306,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maxgekk,maxgekk,maxgekk,14/Jan/20 10:15,14/Jan/20 19:48,13/Jul/23 08:46,14/Jan/20 19:48,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"The bug can be reproduced by the test:
{code}
  test(""log a warning of ignoreExtension deprecation"") {
    val logAppender = new LogAppender
    withTempPath { dir =>
      Seq((""a"", 1, 2), (""b"", 1, 2), (""c"", 2, 1), (""d"", 2, 1))
        .toDF(""value"", ""p1"", ""p2"")
        .repartition(2)
        .write
        .format(""avro"")
        .option(""header"", true)
        .save(dir.getCanonicalPath)
      withLogAppender(logAppender) {
        spark
          .read
          .format(""avro"")
          .option(AvroOptions.ignoreExtensionKey, false)
          .option(""header"", true)
          .load(dir.getCanonicalPath)
          .count()
      }
      val deprecatedEvents = logAppender.loggingEvents
        .filter(_.getRenderedMessage.contains(
          s""Option ${AvroOptions.ignoreExtensionKey} is deprecated""))
      assert(deprecatedEvents.size === 1)
    }
  }
{code}",,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 14 19:48:31 UTC 2020,,,,,,,,,,"0|z0ahjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/20 19:48;dongjoon;Issue resolved by pull request 27200
[https://github.com/apache/spark/pull/27200];;;",,,,,,,,,,,,,,,,,,,,,,,
OneVsRest and OneVsRestModel _from_java and _to_java should handle weightCol,SPARK-30504,13279079,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zero323,zero323,zero323,13/Jan/20 12:35,15/Jan/20 14:45,13/Jul/23 08:46,15/Jan/20 14:42,3.0.0,,,,,,,,,,,,3.0.0,,,ML,PySpark,,,0,,,"Current behaviour

{code:python}
from pyspark.ml.classification import LogisticRegression, OneVsRest, OneVsRestModel
from pyspark.ml.linalg import DenseVector

df = spark.createDataFrame([(0, 1, DenseVector([1.0, 0.0])), (0, 1, DenseVector([1.0, 0.0]))], (""label"", ""w"", ""features""))

ovr = OneVsRest(classifier=LogisticRegression()).setWeightCol(""w"")
ovrm = ovr.fit(df)
ovr.getWeightCol()
## 'w'
ovrm.getWeightCol()
## 'w'

ovr.write().overwrite().save(""/tmp/ovr"")
ovr_ = OneVsRest.load(""/tmp/ovr"")
ovr_.getWeightCol()
## KeyError   
## ...
## KeyError: Param(parent='OneVsRest_5145d56b6bd1', name='weightCol', doc='weight column name. ...)

ovrm.write().overwrite().save(""/tmp/ovrm"")
ovrm_ = OneVsRestModel.load(""/tmp/ovrm"")
ovrm_ .getWeightCol()
## KeyError   
## ...
## KeyError: Param(parent='OneVsRestModel_598c6d900fad', name='weightCol', doc='weight column name ...
{code}

Expected behaviour:

{{OneVsRest}} and {{OneVsRestModel}} loaded from disk should have {{weightCol}}.",,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 15 14:42:43 UTC 2020,,,,,,,,,,"0|z0ag5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/20 14:42;srowen;Issue resolved by pull request 27190
[https://github.com/apache/spark/pull/27190];;;",,,,,,,,,,,,,,,,,,,,,,,
OnlineLDAOptimizer does not handle persistance correctly,SPARK-30503,13279061,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,podongfeng,podongfeng,podongfeng,13/Jan/20 12:08,23/Jan/20 08:40,13/Jul/23 08:46,22/Jan/20 14:24,2.4.4,3.0.0,,,,,,,,,,,3.0.0,,,GraphX,ML,,,0,,,"It seems that in {{OnlineLDAOptimizer, }}{{PeriodicGraphCheckpointer}} can not unpersit edges correctly.
{code:java}
scala> import org.apache.spark.ml.clustering.LDA
import org.apache.spark.ml.clustering.LDA

scala> val dataset = spark.read.format(""libsvm"").load(""data/mllib/sample_lda_libsvm_data.txt"")
20/01/13 20:00:30 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan. dataset: org.apache.spark.sql.DataFrame = [label: double, features: vector]

scala> val lda = new LDA().setK(10).setMaxIter(100).setOptimizer(""em"")
lda: org.apache.spark.ml.clustering.LDA = lda_0e9a6cf09801

scala> sc.getPersistentRDDs
res0: scala.collection.Map[Int,org.apache.spark.rdd.RDD[_]] = Map()

scala> val model = lda.fit(dataset)
model: org.apache.spark.ml.clustering.LDAModel = DistributedLDAModel: uid=lda_0e9a6cf09801, k=10, numFeatures=11

scala> sc.getPersistentRDDs
res1: scala.collection.Map[Int,org.apache.spark.rdd.RDD[_]] = Map(809 -> EdgeRDD MapPartitionsRDD[809] at mapPartitions at EdgeRDDImpl.scala:119, 1337 -> EdgeRDD MapPartitionsRDD[1337] at mapPartitions at EdgeRDDImpl.scala:119, 977 -> EdgeRDD MapPartitionsRDD[977] at mapPartitions at EdgeRDDImpl.scala:119, 1073 -> EdgeRDD MapPartitionsRDD[1073] at mapPartitions at EdgeRDDImpl.scala:119, 449 -> EdgeRDD MapPartitionsRDD[449] at mapPartitions at EdgeRDDImpl.scala:119, 1793 -> EdgeRDD MapPartitionsRDD[1793] at mapPartitions at EdgeRDDImpl.scala:119, 185 -> EdgeRDD MapPartitionsRDD[185] at mapPartitions at EdgeRDDImpl.scala:119, 1001 -> EdgeRDD MapPartitionsRDD[1001] at mapPartitions at EdgeRDDImpl.scala:119, 1601 -> EdgeRDD MapPartitionsRDD[1601] at mapPartitions a...

scala> sc.getPersistentRDDs.size
res2: Int = 106

scala> sc.getPersistentRDDs.foreach(println)
(809,EdgeRDD MapPartitionsRDD[809] at mapPartitions at EdgeRDDImpl.scala:119)
(1337,EdgeRDD MapPartitionsRDD[1337] at mapPartitions at EdgeRDDImpl.scala:119)
(977,EdgeRDD MapPartitionsRDD[977] at mapPartitions at EdgeRDDImpl.scala:119)
(1073,EdgeRDD MapPartitionsRDD[1073] at mapPartitions at EdgeRDDImpl.scala:119)
(449,EdgeRDD MapPartitionsRDD[449] at mapPartitions at EdgeRDDImpl.scala:119)
(1793,EdgeRDD MapPartitionsRDD[1793] at mapPartitions at EdgeRDDImpl.scala:119)
(185,EdgeRDD MapPartitionsRDD[185] at mapPartitions at EdgeRDDImpl.scala:119)
(1001,EdgeRDD MapPartitionsRDD[1001] at mapPartitions at EdgeRDDImpl.scala:119)
(1601,EdgeRDD MapPartitionsRDD[1601] at mapPartitions at EdgeRDDImpl.scala:119)
(1529,EdgeRDD MapPartitionsRDD[1529] at mapPartitions at EdgeRDDImpl.scala:119)
(1265,EdgeRDD MapPartitionsRDD[1265] at mapPartitions at EdgeRDDImpl.scala:119)
(257,EdgeRDD MapPartitionsRDD[257] at mapPartitions at EdgeRDDImpl.scala:119)
(1409,EdgeRDD MapPartitionsRDD[1409] at mapPartitions at EdgeRDDImpl.scala:119)
(1985,EdgeRDD MapPartitionsRDD[1985] at mapPartitions at EdgeRDDImpl.scala:119)
(785,EdgeRDD MapPartitionsRDD[785] at mapPartitions at EdgeRDDImpl.scala:119)
(1313,EdgeRDD MapPartitionsRDD[1313] at mapPartitions at EdgeRDDImpl.scala:119)
(1577,EdgeRDD MapPartitionsRDD[1577] at mapPartitions at EdgeRDDImpl.scala:119)
(881,EdgeRDD MapPartitionsRDD[881] at mapPartitions at EdgeRDDImpl.scala:119)
(29,VertexRDD, VertexRDD ZippedPartitionsRDD2[29] at zipPartitions at VertexRDD.scala:322)
(2105,EdgeRDD MapPartitionsRDD[2105] at mapPartitions at EdgeRDDImpl.scala:119)
(353,EdgeRDD MapPartitionsRDD[353] at mapPartitions at EdgeRDDImpl.scala:119)
(905,EdgeRDD MapPartitionsRDD[905] at mapPartitions at EdgeRDDImpl.scala:119)
(1169,EdgeRDD MapPartitionsRDD[1169] at mapPartitions at EdgeRDDImpl.scala:119)
(89,EdgeRDD MapPartitionsRDD[89] at mapPartitions at EdgeRDDImpl.scala:119)
(1433,EdgeRDD MapPartitionsRDD[1433] at mapPartitions at EdgeRDDImpl.scala:119)
(1697,EdgeRDD MapPartitionsRDD[1697] at mapPartitions at EdgeRDDImpl.scala:119)
(233,EdgeRDD MapPartitionsRDD[233] at mapPartitions at EdgeRDDImpl.scala:119)
(761,EdgeRDD MapPartitionsRDD[761] at mapPartitions at EdgeRDDImpl.scala:119)
(2441,EdgeRDD MapPartitionsRDD[2441] at mapPartitions at EdgeRDDImpl.scala:119)
(2249,EdgeRDD MapPartitionsRDD[2249] at mapPartitions at EdgeRDDImpl.scala:119)
(1217,EdgeRDD MapPartitionsRDD[1217] at mapPartitions at EdgeRDDImpl.scala:119)
(137,EdgeRDD MapPartitionsRDD[137] at mapPartitions at EdgeRDDImpl.scala:119)
(2414,VertexRDD, VertexRDD ZippedPartitionsRDD2[2414] at zipPartitions at VertexRDD.scala:322)
(65,EdgeRDD MapPartitionsRDD[65] at mapPartitions at EdgeRDDImpl.scala:119)
(329,EdgeRDD MapPartitionsRDD[329] at mapPartitions at EdgeRDDImpl.scala:119)
(665,EdgeRDD MapPartitionsRDD[665] at mapPartitions at EdgeRDDImpl.scala:119)
(1457,EdgeRDD MapPartitionsRDD[1457] at mapPartitions at EdgeRDDImpl.scala:119)
(2345,EdgeRDD MapPartitionsRDD[2345] at mapPartitions at EdgeRDDImpl.scala:119)
(1121,EdgeRDD MapPartitionsRDD[1121] at mapPartitions at EdgeRDDImpl.scala:119)
(593,EdgeRDD MapPartitionsRDD[593] at mapPartitions at EdgeRDDImpl.scala:119)
(857,EdgeRDD MapPartitionsRDD[857] at mapPartitions at EdgeRDDImpl.scala:119)
(1361,EdgeRDD MapPartitionsRDD[1361] at mapPartitions at EdgeRDDImpl.scala:119)
(1937,EdgeRDD MapPartitionsRDD[1937] at mapPartitions at EdgeRDDImpl.scala:119)
(1889,EdgeRDD MapPartitionsRDD[1889] at mapPartitions at EdgeRDDImpl.scala:119)
(2153,EdgeRDD MapPartitionsRDD[2153] at mapPartitions at EdgeRDDImpl.scala:119)
(569,EdgeRDD MapPartitionsRDD[569] at mapPartitions at EdgeRDDImpl.scala:119)
(1241,EdgeRDD MapPartitionsRDD[1241] at mapPartitions at EdgeRDDImpl.scala:119)
(2057,EdgeRDD MapPartitionsRDD[2057] at mapPartitions at EdgeRDDImpl.scala:119)
(953,EdgeRDD MapPartitionsRDD[953] at mapPartitions at EdgeRDDImpl.scala:119)
(425,EdgeRDD MapPartitionsRDD[425] at mapPartitions at EdgeRDDImpl.scala:119)
(2033,EdgeRDD MapPartitionsRDD[2033] at mapPartitions at EdgeRDDImpl.scala:119)
(32,EdgeRDD MapPartitionsRDD[32] at mapPartitions at EdgeRDDImpl.scala:119)
(161,EdgeRDD MapPartitionsRDD[161] at mapPartitions at EdgeRDDImpl.scala:119)
(689,EdgeRDD MapPartitionsRDD[689] at mapPartitions at EdgeRDDImpl.scala:119)
(2225,EdgeRDD MapPartitionsRDD[2225] at mapPartitions at EdgeRDDImpl.scala:119)
(2393,EdgeRDD MapPartitionsRDD[2393] at mapPartitions at EdgeRDDImpl.scala:119)
(281,EdgeRDD MapPartitionsRDD[281] at mapPartitions at EdgeRDDImpl.scala:119)
(545,EdgeRDD MapPartitionsRDD[545] at mapPartitions at EdgeRDDImpl.scala:119)
(641,EdgeRDD MapPartitionsRDD[641] at mapPartitions at EdgeRDDImpl.scala:119)
(713,EdgeRDD MapPartitionsRDD[713] at mapPartitions at EdgeRDDImpl.scala:119)
(1865,EdgeRDD MapPartitionsRDD[1865] at mapPartitions at EdgeRDDImpl.scala:119)
(113,EdgeRDD MapPartitionsRDD[113] at mapPartitions at EdgeRDDImpl.scala:119)
(377,EdgeRDD MapPartitionsRDD[377] at mapPartitions at EdgeRDDImpl.scala:119)
(737,EdgeRDD MapPartitionsRDD[737] at mapPartitions at EdgeRDDImpl.scala:119)
(2129,EdgeRDD MapPartitionsRDD[2129] at mapPartitions at EdgeRDDImpl.scala:119)
(521,EdgeRDD MapPartitionsRDD[521] at mapPartitions at EdgeRDDImpl.scala:119)
(1841,EdgeRDD MapPartitionsRDD[1841] at mapPartitions at EdgeRDDImpl.scala:119)
(2369,EdgeRDD MapPartitionsRDD[2369] at mapPartitions at EdgeRDDImpl.scala:119)
(2390,VertexRDD, VertexRDD ZippedPartitionsRDD2[2390] at zipPartitions at VertexRDD.scala:322)
(473,EdgeRDD MapPartitionsRDD[473] at mapPartitions at EdgeRDDImpl.scala:119)
(209,EdgeRDD MapPartitionsRDD[209] at mapPartitions at EdgeRDDImpl.scala:119)
(617,EdgeRDD MapPartitionsRDD[617] at mapPartitions at EdgeRDDImpl.scala:119)
(1145,EdgeRDD MapPartitionsRDD[1145] at mapPartitions at EdgeRDDImpl.scala:119)
(1049,EdgeRDD MapPartitionsRDD[1049] at mapPartitions at EdgeRDDImpl.scala:119)
(1961,EdgeRDD MapPartitionsRDD[1961] at mapPartitions at EdgeRDDImpl.scala:119)
(1025,EdgeRDD MapPartitionsRDD[1025] at mapPartitions at EdgeRDDImpl.scala:119)
(497,EdgeRDD MapPartitionsRDD[497] at mapPartitions at EdgeRDDImpl.scala:119)
(1649,EdgeRDD MapPartitionsRDD[1649] at mapPartitions at EdgeRDDImpl.scala:119)
(1553,EdgeRDD MapPartitionsRDD[1553] at mapPartitions at EdgeRDDImpl.scala:119)
(1817,EdgeRDD MapPartitionsRDD[1817] at mapPartitions at EdgeRDDImpl.scala:119)
(1913,EdgeRDD MapPartitionsRDD[1913] at mapPartitions at EdgeRDDImpl.scala:119)
(1289,EdgeRDD MapPartitionsRDD[1289] at mapPartitions at EdgeRDDImpl.scala:119)
(1385,EdgeRDD MapPartitionsRDD[1385] at mapPartitions at EdgeRDDImpl.scala:119)
(1721,EdgeRDD MapPartitionsRDD[1721] at mapPartitions at EdgeRDDImpl.scala:119)
(2273,EdgeRDD MapPartitionsRDD[2273] at mapPartitions at EdgeRDDImpl.scala:119)
(1481,EdgeRDD MapPartitionsRDD[1481] at mapPartitions at EdgeRDDImpl.scala:119)
(1745,EdgeRDD MapPartitionsRDD[1745] at mapPartitions at EdgeRDDImpl.scala:119)
(401,EdgeRDD MapPartitionsRDD[401] at mapPartitions at EdgeRDDImpl.scala:119)
(2009,EdgeRDD MapPartitionsRDD[2009] at mapPartitions at EdgeRDDImpl.scala:119)
(2081,EdgeRDD MapPartitionsRDD[2081] at mapPartitions at EdgeRDDImpl.scala:119)
(929,EdgeRDD MapPartitionsRDD[929] at mapPartitions at EdgeRDDImpl.scala:119)
(1193,EdgeRDD MapPartitionsRDD[1193] at mapPartitions at EdgeRDDImpl.scala:119)
(833,EdgeRDD MapPartitionsRDD[833] at mapPartitions at EdgeRDDImpl.scala:119)
(36,EdgeRDD MapPartitionsRDD[36] at mapPartitionsWithIndex at GraphImpl.scala:106)
(1097,EdgeRDD MapPartitionsRDD[1097] at mapPartitions at EdgeRDDImpl.scala:119)
(1625,EdgeRDD MapPartitionsRDD[1625] at mapPartitions at EdgeRDDImpl.scala:119)
(1673,EdgeRDD MapPartitionsRDD[1673] at mapPartitions at EdgeRDDImpl.scala:119)
(305,EdgeRDD MapPartitionsRDD[305] at mapPartitions at EdgeRDDImpl.scala:119)
(2201,EdgeRDD MapPartitionsRDD[2201] at mapPartitions at EdgeRDDImpl.scala:119)
(2417,EdgeRDD MapPartitionsRDD[2417] at mapPartitions at EdgeRDDImpl.scala:119)
(1505,EdgeRDD MapPartitionsRDD[1505] at mapPartitions at EdgeRDDImpl.scala:119)
(2321,EdgeRDD MapPartitionsRDD[2321] at mapPartitions at EdgeRDDImpl.scala:119)
(2438,VertexRDD, VertexRDD ZippedPartitionsRDD2[2438] at zipPartitions at VertexRDD.scala:322)
(2297,EdgeRDD MapPartitionsRDD[2297] at mapPartitions at EdgeRDDImpl.scala:119)
(1769,EdgeRDD MapPartitionsRDD[1769] at mapPartitions at EdgeRDDImpl.scala:119)
(2177,EdgeRDD MapPartitionsRDD[2177] at mapPartitions at EdgeRDDImpl.scala:119)
 {code}",,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 22 14:24:35 UTC 2020,,,,,,,,,,"0|z0ag1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/20 14:24;srowen;Issue resolved by pull request 27261
[https://github.com/apache/spark/pull/27261];;;",,,,,,,,,,,,,,,,,,,,,,,
Pyspark on kubernetes does not support --py-files from remote storage in cluster mode,SPARK-30496,13278987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,navdeepniku,navdeepniku,13/Jan/20 03:48,17/May/20 18:23,13/Jul/23 08:46,13/Jan/20 11:06,2.4.4,,,,,,,,,,,,3.0.0,,,Kubernetes,PySpark,Spark Core,,0,,,"The following spark-submit on yarn works fine downloading the file from remote storage and putting it into PYTHONPATH,

spark-submit --master yarn --deploy-mode cluster --py-files s3://bucket/packages.zip s3://bucket/etl.py

 

While, the same fails on k8s with import errors for packages in the zip file. The following is set to PYTHONPATH on k8s, which has a link to s3 file which can't be supported by PYTHONPATH
{noformat}
PYTHONPATH='/opt/spark/python/lib/pyspark.zip:/opt/spark/python/lib/py4j-*.zip:s3://bucket/packages.zip'{noformat}
 ",,navdeepniku,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24736,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-01-13 03:48:20.0,,,,,,,,,,"0|z0afl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to disable 'spark.security.credentials.${service}.enabled' in Structured streaming while connecting to a kafka cluster,SPARK-30495,13278980,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,act_coder,act_coder,13/Jan/20 03:07,17/Apr/20 16:36,13/Jul/23 08:46,15/Jan/20 19:47,3.0.0,,,,,,,,,,,,3.0.0,,,Structured Streaming,,,,0,,,"Trying to read data from a secured Kafka cluster using spark structured
 streaming. Also, using the below library to read the data -
 +*""spark-sql-kafka-0-10_2.12"":""3.0.0-preview""*+ since it has the feature to
 specify our custom group id (instead of spark setting its own custom group
 id)

+*Dependency used in code:*+

        <groupId>org.apache.spark</groupId>
         <artifactId>spark-sql-kafka-0-10_2.12</artifactId>
         <version>3.0.0-preview</version>

 

+*Logs:*+

Getting the below error - even after specifying the required JAAS
 configuration in spark options.

Caused by: java.lang.IllegalArgumentException: requirement failed:
 *Delegation token must exist for this connector*. at
 scala.Predef$.require(Predef.scala:281) at

org.apache.spark.kafka010.KafkaTokenUtil$.isConnectorUsingCurrentToken(KafkaTokenUtil.scala:299)
 at
 org.apache.spark.sql.kafka010.KafkaDataConsumer.getOrRetrieveConsumer(KafkaDataConsumer.scala:533)
 at
 org.apache.spark.sql.kafka010.KafkaDataConsumer.$anonfun$get$1(KafkaDataConsumer.scala:275)

 

+*Spark configuration used to read from Kafka:*+

val kafkaDF = sparkSession.readStream
 .format(""kafka"")
 .option(""kafka.bootstrap.servers"", bootStrapServer)
 .option(""subscribe"", kafkaTopic )
 
//Setting JAAS Configuration
.option(""kafka.sasl.jaas.config"", KAFKA_JAAS_SASL)
 .option(""kafka.sasl.mechanism"", ""PLAIN"")
 .option(""kafka.security.protocol"", ""SASL_SSL"")

// Setting custom consumer group id
.option(""kafka.group.id"", ""test_cg"")
 .load()

 

Following document specifies that we can disable the feature of obtaining
 delegation token -
 [https://spark.apache.org/docs/3.0.0-preview/structured-streaming-kafka-integration.html]


Tried setting this property *spark.security.credentials.kafka.enabled to*
 *false in spark config,* but it is still failing with the same error.",,act_coder,gsomogyi,kabhwan,ladoe00,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 16:36:17 UTC 2020,,,,,,,,,,"0|z0afjk:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,"13/Jan/20 03:29;kabhwan;There seems to be a bug reside on Spark SQL Kafka module - SASL_JAAS available doesn't not always mean there's delegation token. We should also check the value of ""tokenauth"" and apply delegation-token related logic only when the value is ""true"".

[~gsomogyi] Could you confirm whether this is a real bug or not? Once you confirm this is a bug, please update type to ""bug"" and update title accordingly as well. Thanks in advance!;;;","13/Jan/20 09:29;gsomogyi;[~act_coder] thanks for reporting, you've found a bug.
[~kabhwan] you've pinpointed the problem properly. The actual code assumes delegation token usage when SASL_JAAS_CONFIG set.
Started to work on this...;;;","13/Jan/20 12:27;act_coder;[~gsomogyi], [~kabhwan] - Thanks..!  
Can i change the Jira type to BUG from QUESTION ?;;;","14/Jan/20 19:17;kabhwan;Adjusted priority as it's a regression. May need higher priority though, but given we have a PR closer to merge, major seems OK.;;;","15/Jan/20 19:47;vanzin;Issue resolved by pull request 27191
[https://github.com/apache/spark/pull/27191];;;","17/Apr/20 16:36;ladoe00;Anyone knows when this fix will make it to a public build?  I see that it is part of the RC1 tag, but I can't find any RC1 binaries in maven.  I am using 3.0.0 preview2 and this bug is a blocker for us.  Any idea when 3.0.0 will be officially out?;;;",,,,,,,,,,,,,,,,,,
Duplicates cached RDD when create or replace an existing view,SPARK-30494,13278975,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,13/Jan/20 02:29,25/Mar/20 02:26,13/Jul/23 08:46,23/Mar/20 05:24,2.0.2,2.1.3,2.2.3,2.3.4,2.4.5,3.0.0,,,,,,,2.4.6,3.0.0,,SQL,,,,0,,,"We can reproduce by below commands:
{code}
beeline> create or replace temporary view temp1 as select 1
beeline> cache table temp1
beeline> create or replace temporary view temp1 as select 1, 2
beeline> cache table temp1
{code}

The cached RDD for plan ""select 1"" stays in memory forever until the session close. This cached data cannot be used since the view temp1 has been replaced by another plan. It's a memory leak.

assert(spark.sharedState.cacheManager.lookupCachedData(sql(""select 1, 2"")).isDefined)
assert(spark.sharedState.cacheManager.lookupCachedData(sql(""select 1"")).isDefined)",,aaruna,cltlfcjin,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 05:24:50 UTC 2020,,,,,,,,,,"0|z0afig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/20 02:34;cltlfcjin;I will file a PR soon.;;;","21/Mar/20 22:39;dongjoon;Hi, [~cltlfcjin]. How about older versions, 2.3.4 and 2.4.5?;;;","23/Mar/20 05:24;dongjoon;This is resolved via https://github.com/apache/spark/pull/27185;;;",,,,,,,,,,,,,,,,,,,,,
"pyspark.ml.classification.OneVsRestModel shouldn't have setClassifier, setLabelCol and setWeightCol methods",SPARK-30493,13278962,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,zero323,zero323,12/Jan/20 23:22,13/Jan/20 17:12,13/Jul/23 08:46,13/Jan/20 11:04,3.0.0,,,,,,,,,,,,3.0.0,,,ML,PySpark,,,0,,,"These methods don't makes sense in a model, and not present in the Scala counterpart.

Problem introduced with SPARK-29093.",,podongfeng,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 13 11:04:03 UTC 2020,,,,,,,,,,"0|z0affk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/20 11:04;podongfeng;Issue resolved by pull request 27181
[https://github.com/apache/spark/pull/27181];;;",,,,,,,,,,,,,,,,,,,,,,,
Make build delete pyspark.zip file properly,SPARK-30489,13278811,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,jeff.w.evans,jeff.w.evans,jeff.w.evans,10/Jan/20 23:01,11/Jan/20 01:04,13/Jul/23 08:46,11/Jan/20 01:02,2.0.2,2.1.3,2.2.3,2.3.4,2.4.4,3.0.0,,,,,,,2.4.5,3.0.0,,Build,,,,0,,,"The build uses Ant tasks to delete, then recreate, the {{pyspark.zip}} file within {{python/lib}}.  The only problem is the Ant task definition for the delete operation is incorrect (it uses {{dir}} instead of {{file}}), so it doesn't actually get deleted by this task.",,dongjoon,jeff.w.evans,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 11 01:02:06 UTC 2020,,,,,,,,,,"0|z0aei0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/20 01:02;dongjoon;This is resolved via https://github.com/apache/spark/pull/27171;;;",,,,,,,,,,,,,,,,,,,,,,,
Bump lz4-java version to 1.7.1,SPARK-30486,13278732,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,10/Jan/20 14:56,20/Jan/20 05:08,13/Jul/23 08:46,20/Jan/20 03:05,3.0.0,,,,,,,,,,,,3.0.0,,,Build,Spark Core,,,1,,,"lz4-java v1.7.0 has an issue on older macOS (e.g., v10.12 and v10.13). Since v1.7.1 will be released in the end of next week, we need to upgrade: https://github.com/lz4/lz4-java/issues/156#issuecomment-573063299",,ajithshetty,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 20 03:05:52 UTC 2020,,,,,,,,,,"0|z0ae0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/20 03:05;dongjoon;Issue resolved by pull request 27271
[https://github.com/apache/spark/pull/27271];;;",,,,,,,,,,,,,,,,,,,,,,,
"Pyspark test ""test_memory_limit"" fails consistently",SPARK-30480,13278615,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,kabhwan,kabhwan,10/Jan/20 05:11,12/Dec/22 18:10,13/Jul/23 08:46,13/Jan/20 09:47,3.0.0,,,,,,,,,,,,3.0.0,,,PySpark,,,,0,,,"I'm seeing consistent pyspark test failures on multiple PRs ([#26955|https://github.com/apache/spark/pull/26955], [#26201|https://github.com/apache/spark/pull/26201], [#27064|https://github.com/apache/spark/pull/27064]), and they all failed from ""test_memory_limit"".

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/116422/testReport]

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/116438/testReport]

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/116429/testReport]

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/116366/testReport]

 ",,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 13 09:47:46 UTC 2020,,,,,,,,,,"0|z0adag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/20 06:34;gurwls223;Fixed in [https://github.com/apache/spark/pull/27162];;;","10/Jan/20 13:40;gurwls223;Reverted at [https://github.com/apache/spark/commit/d0983af38ffb123fa440bc5fcf3912db7658dd28];;;","13/Jan/20 09:47;gurwls223;Issue resolved by pull request 27186
[https://github.com/apache/spark/pull/27186];;;",,,,,,,,,,,,,,,,,,,,,
Structured Streaming _spark_metadata fills up Spark Driver memory when having lots of objects,SPARK-30462,13278181,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,Vladimir Yankov,Vladimir Yankov,08/Jan/20 14:57,20/Aug/20 09:31,13/Jul/23 08:46,20/Aug/20 09:31,2.4.3,2.4.4,3.0.0,,,,,,,,,,3.1.0,,,Structured Streaming,,,,4,,,"Hi,

With the current implementation of the Spark Structured Streaming it does not seem to be possible to have a constantly running stream, writing millions of files, without increasing the spark driver's memory to dozens of GB's.

In our scenario we are using Spark structured streaming to consume messages from a Kafka cluster, transform them, and write them as compressed Parquet files in an S3 Objectstore Service.
Each 30 seconds a new batch of the spark-streaming is writing hundreds of objects, which respectively results within time to millions of objects in S3.
As all written objects are recorded in the _spark_metadata, the size of the compact files there grows to GB's that eventually fill up the Spark Driver's memory and lead to OOM errors.

We need the functionality to configure the spark structured streaming to run without loading all the historically accumulated metadata in its memory. 
Regularly resetting the _spark_metadata and the checkpoint folders is not an option in our use-case, as we are using the information from the _spark_metadata to have a register of the objects for faster querying and search of the written objects.",,apachespark,apolyakov,dongjoon,FelixKJose,gschiavon,kabhwan,okulev,roczei,SparkSiva,uditme,Vatkov,Vladimir Yankov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24295,SPARK-29995,SPARK-27188,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 20 09:31:58 UTC 2020,,,,,,,,,,"0|z0aaxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/20 02:31;kabhwan;I'm seeing many requests to fix up Spark metadata for streaming file sink, which I feel we may need to consider this as critical.

Not exactly same symptoms (so the necessary fix might be different) but you can find similar issues from SPARK-24295, SPARK-29995 as well. Same reason. I'll link these three issues.;;;","17/Jan/20 08:00;SparkSiva;Hi All,

I have two structured streaming jobs which should write data to the same base directory.

As __spark___metadata directory will be created by default for one job, second job cannot use the same directory as base path as already _spark__metadata directory is created by other job, It is throwing exception.

Is there any workaround for this, other than creating separate base path's for both the jobs.

Is it possible to create the __spark__metadata directory else where or disable without any data loss.

If I had to change the base path for both the jobs, then my whole framework will get impacted, So i don't want to do that.

(SPARK-30542) - I have created a separate ticket for this.;;;","23/Jun/20 08:13;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/28904;;;","23/Jun/20 08:14;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/28904;;;","20/Aug/20 09:31;dongjoon;Issue resolved by pull request 28904
[https://github.com/apache/spark/pull/28904];;;",,,,,,,,,,,,,,,,,,,
Fix ignoreMissingFiles/ignoreCorruptFiles in DSv2,SPARK-30459,13278172,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,08/Jan/20 14:30,13/Jan/20 05:38,13/Jul/23 08:46,09/Jan/20 19:41,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"ignoreMissingFiles/ignoreCorruptFiles in DSv2 is wrong comparing to DSv1, as it stop immediately once it finds a missing or corrupt file while in DSv1 it will skip and continue to read next files.",,dongjoon,Gengliang.Wang,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 13 05:38:30 UTC 2020,,,,,,,,,,"0|z0aavk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/20 19:41;Gengliang.Wang;This issue is resolved in https://github.com/apache/spark/pull/27136;;;","13/Jan/20 05:20;dongjoon;[~Gengliang.Wang]. I set the fixed version to `3.0.0`. This is only needed and merged to `master`, right?;;;","13/Jan/20 05:38;Gengliang.Wang;[~dongjoon]Thanks, yes it is.
I will set the fixed version next time :);;;",,,,,,,,,,,,,,,,,,,,,
The Executor Computing Time in Time Line of Stage Page is Wrong,SPARK-30458,13278140,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sddyljsx,sddyljsx,sddyljsx,08/Jan/20 11:24,12/Jan/20 04:11,13/Jul/23 08:46,12/Jan/20 04:09,2.0.2,2.1.3,2.2.3,2.3.4,2.4.4,3.0.0,,,,,,,2.4.5,3.0.0,,Web UI,,,,0,,,"The Executor Computing Time in Time Line of Stage Page is Wrong. It includes the Scheduler Delay Time, while the Proportion excludes the Scheduler Delay

val executorOverhead = serializationTime + deserializationTime
val executorRunTime = if (taskInfo.duration.isDefined) {
 totalExecutionTime - executorOverhead - gettingResultTime
} else {
 metricsOpt.map(_.executorRunTime).getOrElse(
 totalExecutionTime - executorOverhead - gettingResultTime)
}
val executorComputingTime = executorRunTime - shuffleReadTime - shuffleWriteTime
val executorComputingTimeProportion =
 math.max(100 - schedulerDelayProportion - shuffleReadTimeProportion -
 shuffleWriteTimeProportion - serializationTimeProportion -
 deserializationTimeProportion - gettingResultTimeProportion, 0)",,dongjoon,sddyljsx,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/20 11:25;sddyljsx;Snipaste_2020-01-08_19-04-33.png;https://issues.apache.org/jira/secure/attachment/12990185/Snipaste_2020-01-08_19-04-33.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 12 04:09:28 UTC 2020,,,,,,,,,,"0|z0aaog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/20 04:03;dongjoon;[~sddyljsx]. I saw that you are using `3.1.0` at `Affects Version/s:` repeatedly. However, as you know, the correct next version is `3.0.0`. Please don't use `3.1.0` until we have a `branch-3.0`. Thanks.;;;","12/Jan/20 04:09;dongjoon;Issue resolved by pull request 27135
[https://github.com/apache/spark/pull/27135];;;",,,,,,,,,,,,,,,,,,,,,,
Exclude .git folder for python linter,SPARK-30450,13277993,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ericfchang,ericfchang,ericfchang,07/Jan/20 19:06,09/Jan/20 03:38,13/Jul/23 08:46,08/Jan/20 02:57,3.0.0,,,,,,,,,,,,2.4.5,3.0.0,,Spark Core,,,,0,,,The python linter shouldn't include the .git folder. ,,dongjoon,ericfchang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 08 02:57:06 UTC 2020,,,,,,,,,,"0|z0a9rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/20 02:57;dongjoon;Issue resolved by pull request 27121
[https://github.com/apache/spark/pull/27121];;;",,,,,,,,,,,,,,,,,,,,,,,
accelerator aware scheduling enforce cores as limiting resource,SPARK-30448,13277966,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,07/Jan/20 16:48,10/Jan/20 14:33,13/Jul/23 08:46,10/Jan/20 14:33,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"For the first version of accelerator aware scheduling(SPARK-27495), the SPIP had a condition that we can support dynamic allocation because we were going to have a strict requirement that we don't waste any resources. This means that the number of number of slots each executor has could be calculated from the number of cores and task cpus just as is done today.

Somewhere along the line of development we relaxed that and only warn when we are wasting resources. This breaks the dynamic allocation logic if the limiting resource is no longer the cores.  This means we will request less executors then we really need to run everything.

We have to enforce that cores is always the limiting resource so we should throw if its not.

I guess we could only make this a requirement with dynamic allocation on, but to make the behavior consistent I would say we just require it across the board.",,jiangxb1987,mengxr,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30446,,SPARK-24615,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 07 18:55:33 UTC 2020,,,,,,,,,,"0|z0a9ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/20 16:59;tgraves;note there are other calculations throughout spark code that calculate the number of slots so I think its best for now just to require cores to be limiting resource;;;","07/Jan/20 18:55;tgraves;Note this actually overlaps with https://issues.apache.org/jira/browse/SPARK-30446 since with this change some of those checks don't make sense.;;;",,,,,,,,,,,,,,,,,,,,,,
Constant propagation nullability issue,SPARK-30447,13277957,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,07/Jan/20 15:46,02/Mar/20 22:16,13/Jul/23 08:46,10/Jan/20 12:45,2.3.0,2.3.1,2.3.2,2.3.3,2.3.4,2.4.4,3.0.0,,,,,,2.4.5,3.0.0,,SQL,,,,0,correctness,,"There is a bug in constant propagation due to null handling:

SELECT * FROM t WHERE NOT(c = 1 AND c + 1 = 1) returns those rows where c is null due to 1 + 1 = 1 propagation, but it shouldn't.",,dongjoon,maropu,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 12 03:03:28 UTC 2020,,,,,,,,,,"0|z0a9js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/20 12:45;maropu;Resolved by https://github.com/apache/spark/pull/27119;;;","10/Jan/20 19:02;dongjoon;Hi, [~petertoth].
Could you check the old Spark versions (2.3.4/2.2.3) and update `Affected Versions` please?;;;","11/Jan/20 13:58;petertoth;[~dongjoon], 2.3.4 is affected, 2.2.3 isn't.;;;","12/Jan/20 03:03;dongjoon;Thank you, [~petertoth]!;;;",,,,,,,,,,,,,,,,,,,,
Accelerator aware scheduling handle setting configs to 0 better,SPARK-30445,13277930,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,07/Jan/20 13:52,08/Jan/20 17:14,13/Jul/23 08:46,08/Jan/20 17:14,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"If you set the resource configs to 0, it errors with divide by zero. While I think ideally the user should just remove the configs we should handle the 0 better.

 

{color:#1d1c1d}$ spark-submit --conf spark.driver.resource.gpu.amount=0 {color}*--conf spark.executor.resource.gpu.amount=0*{color:#1d1c1d} {color}*--conf spark.task.resource.gpu.amount=0*{color:#1d1c1d} --conf spark.driver.resource.gpu.discoveryScript=/shared/tools/get_gpu_resources.sh --conf spark.executor.resource.gpu.discoveryScript=/shared/tools/get_gpu_resources.sh test.py{color}
{color:#1d1c1d}20/01/07 05:36:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable{color}
{color:#1d1c1d}Using Spark’s default log4j profile: org/apache/spark/log4j-defaults.properties{color}
{color:#1d1c1d}20/01/07 05:36:43 INFO SparkContext: {color}*Running Spark version 3.0.0-preview*
{color:#1d1c1d}20/01/07 05:36:43 INFO ResourceUtils: =============================================================={color}
{color:#1d1c1d}20/01/07 05:36:43 INFO ResourceUtils: Resources for spark.driver:{color}
*gpu -> [name: gpu, addresses: 0]*
{color:#1d1c1d}20/01/07 05:36:43 INFO ResourceUtils: =============================================================={color}
{color:#1d1c1d}20/01/07 05:36:43 INFO SparkContext: Submitted application: test.py{color}
{color:#1d1c1d}......{color}
{color:#1d1c1d}20/01/07 05:36:43 ERROR SparkContext: Error initializing SparkContext.{color}
*java.lang.ArithmeticException: / by zero*
{color:#1d1c1d}at org.apache.spark.SparkContext$.$anonfun$createTaskScheduler$3(SparkContext.scala:2793){color}
{color:#1d1c1d}at org.apache.spark.SparkContext$.$anonfun$createTaskScheduler$3$adapted(SparkContext.scala:2775){color}
{color:#1d1c1d}at scala.collection.Iterator.foreach(Iterator.scala:941){color}
{color:#1d1c1d}at scala.collection.Iterator.foreach$(Iterator.scala:941){color}
{color:#1d1c1d}at scala.collection.AbstractIterator.foreach(Iterator.scala:1429){color}",,dongjoon,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 08 17:14:24 UTC 2020,,,,,,,,,,"0|z0a9ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/20 17:14;dongjoon;Issue resolved by pull request 27118
[https://github.com/apache/spark/pull/27118];;;",,,,,,,,,,,,,,,,,,,,,,,
Flaky test: org.apache.spark.scheduler.TaskSetManagerSuite.reset,SPARK-30440,13277711,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ajithshetty,kabhwan,kabhwan,06/Jan/20 14:05,09/Jan/20 00:57,13/Jul/23 08:46,09/Jan/20 00:28,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,Tests,,,0,,,"[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/116126/testReport]

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/116159/testReport/]
{noformat}
 org.apache.spark.scheduler.TaskSetManagerSuite.reset Error Detailsorg.scalatest.exceptions.TestFailedException: task0.isDefined was true, but task1.isDefined was false Stack Tracesbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: task0.isDefined was true, but task1.isDefined was false
	at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530)
	at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:503)
	at org.apache.spark.scheduler.TaskSetManagerSuite.$anonfun$new$107(TaskSetManagerSuite.scala:1933)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:149)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:286)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)
	at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:56)
	at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:393)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:381)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:376)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:458)
	at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite.run(Suite.scala:1124)
	at org.scalatest.Suite.run$(Suite.scala:1106)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:518)
	at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:56)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:317)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:510)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748) {noformat}",,ajithshetty,dongjoon,kabhwan,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 09 00:28:51 UTC 2020,,,,,,,,,,"0|z0a814:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/20 06:57;kabhwan;cc. [~Ngone51] since the failing test is added from SPARK-30359;;;","07/Jan/20 07:25;Ngone51;[~kabhwan] thanks for reporting it. Let me take a look.;;;","07/Jan/20 07:37;ajithshetty;Found a race between reviveOffers in org.apache.spark.scheduler.TaskSchedulerImpl#submitTasks and org.apache.spark.scheduler.TaskSetManager#resourceOffer, in the testcase made PR for same

[https://github.com/apache/spark/pull/27115];;;","09/Jan/20 00:28;dongjoon;Issue resolved by pull request 27115
[https://github.com/apache/spark/pull/27115];;;",,,,,,,,,,,,,,,,,,,,
update Spark SQL guide of Supported Hive Features,SPARK-30435,13277673,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,06/Jan/20 10:20,30/Jan/20 04:56,13/Jul/23 08:46,30/Jan/20 04:56,3.0.0,,,,,,,,,,,,3.0.0,,,Documentation,,,,0,,,,,angerszhuuu,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 30 04:56:18 UTC 2020,,,,,,,,,,"0|z0a7so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/20 04:56;dongjoon;Issue resolved by pull request 27106
[https://github.com/apache/spark/pull/27106];;;",,,,,,,,,,,,,,,,,,,,,,,
WideSchemaBenchmark fails with OOM,SPARK-30429,13277641,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,maxgekk,maxgekk,06/Jan/20 07:41,08/Jan/20 02:46,13/Jul/23 08:46,08/Jan/20 02:46,3.0.0,,,,,,,,,,,,3.0.0,,,SQL,,,,0,,,"Run WideSchemaBenchmark on the master (commit bc16bb1dd095c9e1c8deabf6ac0d528441a81d88) via:
{code}
SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt ""sql/test:runMain org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark""
{code}
This fails with:
{code}
Caused by: java.lang.reflect.InvocationTargetException
[error] 	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
[error] 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[error] 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
[error] 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$makeCopy$7(TreeNode.scala:468)
[error] 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
[error] 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$makeCopy$1(TreeNode.scala:467)
[error] 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
[error] 	... 132 more
[error] Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
[error] 	at java.util.Arrays.copyOfRange(Arrays.java:3664)
[error] 	at java.lang.String.<init>(String.java:207)
[error] 	at java.lang.StringBuilder.toString(StringBuilder.java:407)
[error] 	at org.apache.spark.sql.types.StructType.catalogString(StructType.scala:411)
[error] 	at org.apache.spark.sql.types.StructType.$anonfun$catalogString$1(StructType.scala:410)
[error] 	at org.apache.spark.sql.types.StructType$$Lambda$2441/1040526643.apply(Unknown Source)
{code}
Full stack dump is attached.",,dongjoon,maxgekk,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/20 07:42;maxgekk;WideSchemaBenchmark_console.txt;https://issues.apache.org/jira/secure/attachment/12989982/WideSchemaBenchmark_console.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 08 02:46:40 UTC 2020,,,,,,,,,,"0|z0a7lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/20 19:27;dongjoon;Thank you for reporting, [~maxgekk].
Do you have any idea about what patch causes this regression?;;;","06/Jan/20 21:44;maxgekk;[~dongjoon] I ran git bisect. Let see what it will find during this night.;;;","07/Jan/20 06:43;maxgekk;Bisect have found the first bad commit. I specified the recent master as a bad commit and 62551cceebf6aca8b6bd8164cd2ed85564726f6c as the good commit.
{code}
cb5ea201df5fae8aacb653ffb4147b9288bca1e9 is the first bad commit
commit cb5ea201df5fae8aacb653ffb4147b9288bca1e9
Author: Liang-Chi Hsieh <viirya@gmail.com>
Date:   Thu Oct 25 19:27:45 2018 +0800

    [SPARK-25746][SQL] Refactoring ExpressionEncoder to get rid of flat flag
    ...
       Closes #22749 from viirya/SPARK-24762-refactor.

    Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
    Signed-off-by: Wenchen Fan <wenchen@databricks.com>

:040000 040000 11961d7665e9097c682cdf6d51163ad4b3ffdf90 cb82a04e8a2fa1505c2db36c9c6578544e502601 M      sql
bisect run success
{code}
/cc [~cloud_fan] [~viirya];;;","07/Jan/20 07:16;dongjoon;Thank you so much for the investigation!
cc [~smilegator];;;","07/Jan/20 08:19;viirya;Thanks for pinging me. Looking into this.;;;","08/Jan/20 02:46;dongjoon;Issue resolved by pull request 27117
[https://github.com/apache/spark/pull/27117];;;",,,,,,,,,,,,,,,,,,
Fix the disorder of structured-streaming-kafka-integration page,SPARK-30426,13277619,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,06/Jan/20 03:33,06/Jan/20 05:25,13/Jul/23 08:46,06/Jan/20 04:26,3.0.0,,,,,,,,,,,,3.0.0,,,Documentation,Structured Streaming,,,0,,,,,cloud_fan,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 06 04:26:42 UTC 2020,,,,,,,,,,"0|z0a7go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/20 04:26;cloud_fan;Issue resolved by pull request 27098
[https://github.com/apache/spark/pull/27098];;;",,,,,,,,,,,,,,,,,,,,,,,
SPARK-29976 calculation of slots wrong for Standalone Mode,SPARK-30417,13277413,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yuchen.huo,tgraves,tgraves,03/Jan/20 18:49,08/Jan/20 19:36,13/Jul/23 08:46,08/Jan/20 19:31,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"In SPARK-29976 we added a config to determine if we should allow speculation when the number of tasks is less then the number of slots on a single executor.  The problem is that for standalone mode (and  mesos coarse grained) the EXECUTOR_CORES config is not set properly by default. In those modes the number of executor cores is all the cores of the Worker.    The default of EXECUTOR_CORES is 1.

The calculation:

{color:#000080}val {color}{color:#660e7a}speculationTasksLessEqToSlots {color}= {color:#660e7a}numTasks {color}<= ({color:#660e7a}conf{color}.get({color:#660e7a}EXECUTOR_CORES{color}) / sched.{color:#660e7a}CPUS_PER_TASK{color})

If someone set the cpus per task > 1 then this would end up being false even if 1 task.  Note that the default case where cpus per task is 1 and executor cores is 1 it works out ok but is only applied if 1 task vs number of slots on the executor.

Here we really don't know the number of executor cores for standalone mode or mesos so I think a decent solution is to just use 1 in those cases and document the difference.

Something like max({color:#660e7a}conf{color}.get({color:#660e7a}EXECUTOR_CORES{color}) / sched.{color:#660e7a}CPUS_PER_TASK{color}, 1)

 ",,jiangxb1987,tgraves,yuchen.huo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29976,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 08 19:31:41 UTC 2020,,,,,,,,,,"0|z0a674:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/20 18:50;tgraves;[~yuchen.huo] is this something you could work on?;;;","07/Jan/20 22:32;yuchen.huo;[~tgraves] Sure. Is there a more stable way to get the number of cores the executor is using instead of checking the value of EXECUTOR_CORES which might not be set?

 

cc [~jiangxb1987];;;","07/Jan/20 22:49;tgraves;The only way for standalone mode would be to look at what each executor registers with. Theoretically different executors could have different number of cores.  There are actually other issues (SPARK-30299 for instance) with this in the code as well that I think we need a global solution for.  So perhaps for this Jira we do the easy thing like I suggested and then we have have a separate Jira to look at handling this better in the future.;;;","07/Jan/20 23:06;jiangxb1987;Good catch! `max(conf.get(EXECUTOR_CORES) / sched.CPUS_PER_TASK, 1)` seems good enough for me. Thanks!;;;","08/Jan/20 19:31;jiangxb1987;Resolved by https://github.com/apache/spark/pull/27126;;;",,,,,,,,,,,,,,,,,,,
OneForOneStreamManager use AtomicLong,SPARK-30406,13277147,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ajithshetty,ajithshetty,ajithshetty,02/Jan/20 07:10,03/Jan/20 17:42,13/Jul/23 08:46,03/Jan/20 17:42,3.0.0,,,,,,,,,,,,3.0.0,,,Spark Core,,,,0,,,"Using compound operations as well as increments and decrements on primitive fields are not atomic operations. Here when volatile primitive field is incremented or decremented,  we run into data loss if threads interleave in steps of update. 

 

Refer: [https://wiki.sei.cmu.edu/confluence/display/java/VNA02-J.+Ensure+that+compound+operations+on+shared+variables+are+atomic]",,ajithshetty,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 03 17:42:15 UTC 2020,,,,,,,,,,"0|z0a4k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/20 17:42;srowen;Issue resolved by pull request 27071
[https://github.com/apache/spark/pull/27071];;;",,,,,,,,,,,,,,,,,,,,,,,
