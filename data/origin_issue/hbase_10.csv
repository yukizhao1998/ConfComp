Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Dependent),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Outward issue link (Incorporates),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Regression),Inward issue link (Required),Inward issue link (Required),Inward issue link (Required),Outward issue link (Required),Outward issue link (Required),Outward issue link (Required),Inward issue link (Supercedes),Inward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (Hadoop Flags),Custom field (Hadoop Flags),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Region orphaned after failure during split,HBASE-3403,12494333,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,tlipcon,tlipcon,30/Dec/10 22:54,20/Nov/15 12:42,14/Jul/23 06:06,07/Jan/11 20:18,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"ERROR: Region hdfs://haus01.sf.cloudera.com:11020/hbase-normal/usertable/2ad8df700eea55f70e02ea89178a65a2 on HDFS, but not listed in META or deployed on any region server.
ERROR: Found inconsistency in table usertable

Not sure how I got into this state, will look through logs.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/11 22:38;stack;3403-v2.txt;https://issues.apache.org/jira/secure/attachment/12467678/3403-v2.txt","04/Jan/11 23:58;stack;3403.txt;https://issues.apache.org/jira/secure/attachment/12467493/3403.txt","31/Dec/10 01:02;tlipcon;broken-split.txt;https://issues.apache.org/jira/secure/attachment/12467201/broken-split.txt","31/Dec/10 02:16;tlipcon;hbck-fix-missing-in-meta.txt;https://issues.apache.org/jira/secure/attachment/12467208/hbck-fix-missing-in-meta.txt","03/Jan/11 21:56;tlipcon;master-logs.txt.gz;https://issues.apache.org/jira/secure/attachment/12467371/master-logs.txt.gz",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26838,Reviewed,,,,Fri Nov 20 12:42:03 UTC 2015,,,,,,,,,,"0|i0hm53:",100849,,,,,,,,,,,,,,,,,,,,,"31/Dec/10 01:02;tlipcon;It seems this region was involved in a split (starts at 2010-12-30 11:55:15,035). Here are all the logs for the parent (1acb10b8287adbd1a2ace40331366a8c) and the two daughters;;;","31/Dec/10 01:09;tlipcon;Worth noting that I kill -9ed all the servers at just about 11:56:46,879.;;;","31/Dec/10 01:39;tlipcon;I think the RS crashed somewhere between offlining the parent and opening one of the new daughters, where this comment is:
{code}
    // The is the point of no return.  We are committed to the split now.  We
    // have still the daughter regions to open but meta has been changed.
    // If we fail from here on out, we can not rollback so, we'll just abort.
    // The meta has been changed though so there will need to be a fixup run
    // during processing of the crashed server by master (TODO: Verify this in place).
{code}

When the master processed the RS failure, the daughter region wasn't yet in META, so it didn't count them as unassigned during processing the server shutdown.

Perhaps MetaReader.getServerUserRegions() needs to take into account offline parents. If it finds an offline parent, it should add the two daughters to a list of potentially mid-split regions. Then it can issue a get() for each of those regions, and if it doesn't find a row, it may need to be counted as unassigned... seems a bit messy :);;;","31/Dec/10 01:42;tlipcon;(fwiw hbck -fix cannot fix this class of issues);;;","31/Dec/10 02:16;tlipcon;Here's an hbck fix that will re-add a region to META. It should probably be improved to actually double-check that META has a hole for the region where it's adding. Otherwise it could potentially do more harm than good!;;;","31/Dec/10 04:39;stack;There is fixup for this condition in the server shutdown recovery code.  See the fixupDaughters methods in the ServerShutdown handler.  I noticed recently that it doesn't seem to be working properly (I noted it in issue comment but didn't open explicit issue to cover this case since couldn't repro).  I added extra logging to see if could figure why the fixup code wasn't cutting in.  Mind posting master log?   Good on you Todd.;;;","03/Jan/11 21:56;tlipcon;Here are some logs from the master;;;","03/Jan/11 23:15;stack;The log does not have important transition points for the above cited problematic region nor for the few that are stuck PENDING_CLOSE on end of the log.  Let me try reproduce.  Thanks Todd.;;;","04/Jan/11 00:26;tlipcon;Stack and I looked at this. Here's the issue:
- when we offline the parent in MetaEditor, we clear out SERVER_QUALIFIER and SERVER_STARTCODE
- so, when we process the server shutdown in ServerShutdownHandler, it never makes it into the list of regions that were on that server.
- hence, we never call processDeadRegion, and it gets stuck.

Stack is going to try to write a unit test.;;;","04/Jan/11 23:58;stack;Patch that fixes issue where we are clearing server from .META. in parent of split so if crash, we fail to do fixup if daughters didn't get added properly.

Adds ugly test that bends over backwards to recreate the scenario (we have some work to do to expose our innards for mocking).;;;","05/Jan/11 21:56;tlipcon;+    // Turn off the meta scanner so it don't remove parent on us.
+    cluster.getMaster().catalogJanitorSwitch(false);

Is this a potential issue? ie the catalog janitor shouldn't remove the parent if the daughters aren't ready yet, should it? Or is this just for the simulation done in the test?

- naming-wise, can we use ""setCatalogJanitorEnabled(false)"" just like our other accessors? Also .onOff() -> setEnabled()?

- Does this change introduce a new bug? Consider the following scenario:
1. Region A exists
2. Region A splits to B and C, A is marked offline
3. B and C both open just fine
4. B splits to E and F, successfully
5. server crashes

When we process the server shutdown, we end up re-creating B.


;;;","05/Jan/11 23:23;stack;bq. + cluster.getMaster().catalogJanitorSwitch(false);

I added above switch to avoid the unlikely but *perhaps* possible case of split, compaction in each daughter, and run of catalogjanitor happens before we get our edit of .META. in.  Just trying to do all I can to avoid a debug of failed tests up on hudson.

NP on changing name of method.  Will do.

bq. Does this change introduce a new bug?

Yes.  That could happen.  Unlikely, but perhaps.  Let me spin a new patch.


;;;","06/Jan/11 22:38;stack;Made Todd suggested changes as well as handling of the possible scenario he describes.  The fixup code was copied from 0.20.  In 0.20, it was not possible to get into the state Todd postulates above but in 0.90, when fixup is done in shutdown handler and not in the 'catalogJanitor', it could happen.  I added test that manufactures the postulated condition and we seem to be doing right thing.;;;","07/Jan/11 18:56;tlipcon;Looks pretty good! A few nits:

- CatalogJanitor.setEnabled() - why does it return its argument? that seems kind of silly... Same with HMaster.setCatalogJanitorEnabled. 
- typo: ""used testing"" -> ""Used for testing""
- typo: ""start the scan at the what""
- rename: isDaughter -> foundDaughter, IsDaughterVisitor -> FindDaughterVisitor might make more sense?;;;","07/Jan/11 20:18;stack;Committed branch and trunk after making the new Todd suggested changes.   Thanks for review Todd.;;;","09/Jan/11 05:26;stack;Really committed to TRUNK (I'd only committed on branch as noticed by Ted Yu.  Thanks Hawkeye Ted).;;;","09/Jan/11 08:03;hudson;Integrated in HBase-TRUNK #1711 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1711/])
    HBASE-3403 Region orphaned after failure during split
;;;","10/Jan/11 22:37;stack;Duh, forgot to add Test to 0.90.  Just did.  Thanks Ted Yu for noticing.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web UI shows two META regions,HBASE-3402,12494332,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,tlipcon,tlipcon,30/Dec/10 22:51,20/Nov/15 12:42,14/Jul/23 06:06,05/Jan/11 00:46,0.90.0,,,,,,,,,,,0.90.0,,master,regionserver,,,,0,,,"Running 0.90@r1052112 I see two regions for META on the same server. Both have start key '-' and end key '-'.

Things seem to work OK, but it's very strange.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/10 22:51;tlipcon;two-metas.png;https://issues.apache.org/jira/secure/attachment/12467193/two-metas.png",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26837,,,,,Fri Nov 20 12:42:50 UTC 2015,,,,,,,,,,"0|i0hm4v:",100848,,,,,,,,,,,,,,,,,,,,,"05/Jan/11 00:44;stack;This looks like a dumb one:

{code}
diff --git a/src/main/resources/hbase-webapps/master/table.jsp b/src/main/resources/hbase-webapps/master/table.jsp
index b433e20..f312004 100644
--- a/src/main/resources/hbase-webapps/master/table.jsp
+++ b/src/main/resources/hbase-webapps/master/table.jsp
@@ -100,7 +100,7 @@
   // NOTE: Presumes one meta region only.
   HRegionInfo meta = HRegionInfo.FIRST_META_REGIONINFO;
   HServerAddress metaLocation = master.getCatalogTracker().getMetaLocation();
-  for (int i = 0; i <= 1; i++) {
+  for (int i = 0; i < 1; i++) {
     int infoPort = master.getServerManager().getHServerInfo(metaLocation).getInfoPort();
     String url = ""http://"" + metaLocation.getHostname() + "":"" + infoPort + ""/"";
 %>
{code}

I'll just commit.;;;","05/Jan/11 00:46;stack;Committed trunk and branch after verifying it works.;;;","05/Jan/11 21:41;hudson;Integrated in HBase-TRUNK #1703 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1703/])
    ;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Region IPC operations should be high priority,HBASE-3401,12494320,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,tlipcon,tlipcon,tlipcon,30/Dec/10 19:55,20/Nov/15 12:43,14/Jul/23 06:06,06/Jan/11 19:51,0.90.1,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"I manufactured an imbalanced cluster so one region server had 300 regions and the others had very few. I then ran balancer while hitting the high-load region server with YCSB. I observed that the rate of load shedding was VERY slow since the closeRegion IPCs were getting stuck at the back of the IPC queue.

All of these important master->RS RPC calls should be set to high priority.",,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/11 02:45;tlipcon;hbase-3401.txt;https://issues.apache.org/jira/secure/attachment/12467394/hbase-3401.txt","30/Dec/10 21:37;tlipcon;hbase-3401.txt;https://issues.apache.org/jira/secure/attachment/12467189/hbase-3401.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26836,Reviewed,,,,Fri Nov 20 12:43:38 UTC 2015,,,,,,,,,,"0|i0hm4n:",100847,,,,,,,,,,,,,,,,,,,,,"30/Dec/10 21:37;tlipcon;Here's a patch which adds a new QosPriority annotation which can be used to prioritize things more easily right in the interface.

We can eventually extend this to be more generalized, but this should work for now and make it more obvious when adding new RPCs how to prioritize them.;;;","30/Dec/10 22:35;tlipcon;Tested on cluster for basic operation, though have not tested that scenario again yet.;;;","31/Dec/10 03:05;tlipcon;oops, this patch doesn't work... will upload a new one soon.;;;","03/Jan/11 23:35;stack;Thats pretty fancy Todd using annotations.  You going to upload a new version?  Why don't it work?;;;","04/Jan/11 02:45;tlipcon;Old patch didn't work because annotations aren't inherited from interfaces into the methods in the actual class, which is where I was trying to read them from.

New patch moves the whole shebang into HRegionServer and appears to actually work ;-);;;","04/Jan/11 05:10;stack;I don't think I've seen this construct before: 

{code}
+  private @interface QosPriority {
+    int priority() default 0;
+  }
{code}

No curlies around the default 0?  This how annotations do 'interfaces'?

Will this all work if someone subclasses and overrides HRS methods?

Otherwise, +1 on commit.  Looks good to me.
;;;","04/Jan/11 19:56;tlipcon;Stack: yea, the syntax for defining an annotation is wacky. I had to look it up mysefl.

bq. Will this all work if someone subclasses and overrides HRS methods?

Yes, it should - getAnnotation() will return inherited annotations as well. Though perhaps it should be made a protected @interface instead of private for subclassers?

Though, I thought we were discouraging subclassing these days in favor of CPs?;;;","05/Jan/11 00:51;stack;bq. Though, I thought we were discouraging subclassing these days in favor of CPs?

Yes.  Doesn't sound like it an issue anyways.

+1 on commit.;;;","05/Jan/11 00:53;stack;Adding fix in 0.90.0;;;","07/Jan/11 22:55;hudson;Integrated in HBase-TRUNK #1708 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1708/])
    ;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Coprocessor Support for Generic Interfaces,HBASE-3400,12494280,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ekohlwey,ekohlwey,ekohlwey,29/Dec/10 23:59,20/Nov/15 12:43,14/Jul/23 06:06,04/Feb/11 18:26,0.92.0,,,,,,,,,,,0.92.0,,Coprocessors,,,,,0,,,"Coprocessors currently do not support generic interfaces because type erasure makes their generic parameters appear as Objects to Invocation.java.

This can be overcome by writing out the parameters using their own types (rather than the type parameters), and then separately writing the class names for the type parameters. While it would be ideal to implement this in Invocation.java, some other code seems to be relying on its write order and doing so breaks other RPC code. The modification can, however, be implemented in Exec.java instead.

The included patch modifies Invocation.java's fields to that they are protected scope, and fully implements the read and write methods for Exec rather than using the parent method for the parent fields. ExecResult is also modified to accommodate generic returns in the same way.",,ghelmling,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/11 22:15;ekohlwey;HBASE-3400-4.patch;https://issues.apache.org/jira/secure/attachment/12467768/HBASE-3400-4.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26835,Reviewed,,,,Fri Nov 20 12:43:58 UTC 2015,,,,,,,,,,"0|i0hm4f:",100846,,,,,,,,,,,,,,,,,,,,,"30/Dec/10 01:28;ghelmling;Thanks for the patch, Ed!

It looks like the diff contains some of your changes from HBASE-3316, which are now in trunk.  Can you redo the diff from the latest trunk and also post to review.hbase.org?  Thanks again.
;;;","31/Dec/10 21:20;ekohlwey;This patch should be better ;);;;","31/Dec/10 21:27;hbasereviewboard;Message from: ekohlwey@gmail.com

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1405/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Coprocessors currently do not support generic interfaces because type erasure makes their generic parameters appear as Objects to Invocation.java.
This can be overcome by writing out the parameters using their own types (rather than the type parameters), and then separately writing the class names for the type parameters. While it would be ideal to implement this in Invocation.java, some other code seems to be relying on its write order and doing so breaks other RPC code. The modification can, however, be implemented in Exec.java instead.

The included patch modifies Invocation.java's fields to that they are protected scope, and fully implements the read and write methods for Exec rather than using the parent method for the parent fields. ExecResult is also modified to accommodate generic returns in the same way.


This addresses bug HBASE-3400.
    http://issues.apache.org/jira/browse/HBASE-3400


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/client/coprocessor/Exec.java c127ea3 
  src/main/java/org/apache/hadoop/hbase/client/coprocessor/ExecResult.java be46cd2 
  src/main/java/org/apache/hadoop/hbase/ipc/Invocation.java 9609652 
  src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java 75f76e8 

Diff: http://review.cloudera.org/r/1405/diff


Testing
-------

Integration test included in patch. Demonstrates generic interface using objects, arrays, and primitives, and checks that all primitive classes work as well.


Thanks,

ekohlwey


;;;","01/Jan/11 01:45;ekohlwey;One more try- added missing test classes.;;;","01/Jan/11 01:46;hbasereviewboard;Message from: ekohlwey@gmail.com

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1405/
-----------------------------------------------------------

(Updated 2010-12-31 17:45:56.197941)


Review request for hbase.


Changes
-------

Added classes that were missing for tests.


Summary
-------

Coprocessors currently do not support generic interfaces because type erasure makes their generic parameters appear as Objects to Invocation.java.
This can be overcome by writing out the parameters using their own types (rather than the type parameters), and then separately writing the class names for the type parameters. While it would be ideal to implement this in Invocation.java, some other code seems to be relying on its write order and doing so breaks other RPC code. The modification can, however, be implemented in Exec.java instead.

The included patch modifies Invocation.java's fields to that they are protected scope, and fully implements the read and write methods for Exec rather than using the parent method for the parent fields. ExecResult is also modified to accommodate generic returns in the same way.


This addresses bug HBASE-3400.
    http://issues.apache.org/jira/browse/HBASE-3400


Diffs (updated)
-----

  src/main/java/org/apache/hadoop/hbase/client/coprocessor/Exec.java c127ea3 
  src/main/java/org/apache/hadoop/hbase/client/coprocessor/ExecResult.java be46cd2 
  src/main/java/org/apache/hadoop/hbase/ipc/Invocation.java 9609652 
  src/test/java/org/apache/hadoop/hbase/coprocessor/GenericEndpoint.java PRE-CREATION 
  src/test/java/org/apache/hadoop/hbase/coprocessor/GenericProtocol.java PRE-CREATION 
  src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java 75f76e8 

Diff: http://review.cloudera.org/r/1405/diff


Testing
-------

Integration test included in patch. Demonstrates generic interface using objects, arrays, and primitives, and checks that all primitive classes work as well.


Thanks,

ekohlwey


;;;","07/Jan/11 22:15;ekohlwey;Yet another patch.

This modification fixes an issue where Exec was not properly handling primitives.

Not posted to review.hbase.org as the site appears to be down.;;;","04/Feb/11 18:26;ghelmling;Committed to trunk.  Thanks for the patch, Ed!;;;","04/Feb/11 22:58;hudson;Integrated in HBase-TRUNK #1732 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1732/])
    HBASE-3400  Coprocessor Support for Generic Interfaces
;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update backport of InputSampler,HBASE-3392,12494051,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,tlipcon,tlipcon,tlipcon,25/Dec/10 04:41,20/Nov/15 12:41,14/Jul/23 06:06,04/Jan/11 00:57,0.90.0,,,,,,,,,,,0.90.0,,mapreduce,,,,,0,,,"We have a backport copy of o.a.h.mapreduce.lib.partition.InputSampler in HBase which contains a bug, MAPREDUCE-1820. This JIRA is to update our copy to the version in MR trunk.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Dec/10 04:42;tlipcon;hbase-3392.txt;https://issues.apache.org/jira/secure/attachment/12466957/hbase-3392.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26830,Reviewed,,,,Fri Nov 20 12:41:29 UTC 2015,,,,,,,,,,"0|i0hm3b:",100841,,,,,,,,,,,,,,,,,,,,,"25/Dec/10 04:42;tlipcon;Produced patch by copying new source from MR trunk.;;;","27/Dec/10 18:51;stack;Bringing into 0.90.1.  Will commit to 0.90 if the current RC doesn't make the grade.;;;","04/Jan/11 00:28;stack;+1;;;","04/Jan/11 00:57;tlipcon;Committed to branch and trunk;;;","04/Jan/11 04:02;hudson;Integrated in HBase-TRUNK #1701 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1701/])
    HBASE-3392. Update backport of InputSampler to reflect MAPREDUCE-1820
;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE processRegionInTransition(AssignmentManager.java:264) doing rolling-restart.sh,HBASE-3388,12493922,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,22/Dec/10 20:24,20/Nov/15 12:43,14/Jul/23 06:06,05/Jan/11 00:03,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Trying to run rolling-restart got this:

{code}
2010-12-22 20:16:48,579 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.
java.lang.NullPointerException
    at org.apache.hadoop.hbase.master.AssignmentManager.processRegionInTransition(AssignmentManager.java:264)
    at org.apache.hadoop.hbase.master.AssignmentManager.processFailover(AssignmentManager.java:223)
    at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:395)
    at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:277)
2010-12-22 20:16:48,581 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
2010-12-22 20:16:48,581 DEBUG org.apache.hadoop.hbase.master.HMaster: Stopping service threads
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26827,,,,,Fri Nov 20 12:43:04 UTC 2015,,,,,,,,,,"0|i0hm2n:",100838,,,,,,,,,,,,,,,,,,,,,"22/Dec/10 20:27;stack;Here's patch:

{code}
diff --git a/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java b/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
index ea064f2..2b345fb 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
@@ -261,8 +261,13 @@ public class AssignmentManager extends ZooKeeperListener {
   throws KeeperException, IOException {
     RegionTransitionData data = ZKAssign.getData(watcher, encodedRegionName);
     if (data == null) return false;
-    HRegionInfo hri = (regionInfo != null)? regionInfo:
-      MetaReader.getRegion(catalogTracker, data.getRegionName()).getFirst();
+    HRegionInfo hri = regionInfo;
+    if (hri == null) {
+      Pair<HRegionInfo, HServerAddress> p =
+        MetaReader.getRegion(catalogTracker, data.getRegionName());
+      if (p == null) return false;
+      hri = p.getFirst();
+    }
     processRegionsInTransition(data, hri);
     return true;
   }
{code};;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","05/Jan/11 00:03;stack;This has already been applied.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pair does not deep check arrays for equality. ,HBASE-3387,12493917,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jesse_yates,jesse_yates,jesse_yates,22/Dec/10 19:26,12/Jun/22 00:41,14/Jul/23 06:06,29/Dec/10 00:10,0.90.1,,,,,,,,,,,,,util,,,,,0,,,"Pair does not deep check arrays for equality. It merely does x.equals(y) for the sent Object. However, with any type of array this is merely going to compare the array pointers, rather than the underlying data structure.

It requires a rewriting of the private equals method in Pair to check for elements being an array, then checking the underlying elements.",Any (discovered in Ubuntu 10.10 using TRUNK). ,ekohlwey,hammer,jesse_yates,nspiegelberg,,,,,,,,,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,HBASE-3644,,,,,,,,,,,,,,,,,"18/Jan/11 23:29;jesse_yates;HBASE-3387-revert.patch;https://issues.apache.org/jira/secure/attachment/12468699/HBASE-3387-revert.patch","28/Dec/10 22:18;jesse_yates;HBASE-3387.patch;https://issues.apache.org/jira/secure/attachment/12467073/HBASE-3387.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26826,Reviewed,,,,Fri Jan 21 19:04:48 UTC 2011,,,,,,,,,,"0|i0hm2f:",100837,"Pair now does a deep equals on arrays, rather than a shallow, pointer compare.",,,,,,,,,,,,,,,,,,,,"22/Dec/10 19:29;jesse_yates;Fix (and tests) for the issue. Does deep check on an array, if it is an array.;;;","22/Dec/10 19:51;stack;Patch looks good Jesse.  Thanks for making it.   Minor comments are that you changed a bunch of formatting -- 90% of it is improvement over what was there previous -- but you messed up the license formatting on the top of the file.  Also, minor stylistic comment is that you wrote the equals with a load of indentation when you could have done without by doing the 'other' test first.  For example:

If instead of a #133 that looks like this:

{code}
+    if (x != null && y != null) {
{code}

.. you instead did:

{code}
if (x == null || y == null) return false;
{code}

... you'd save an indent and IMO the code is easier to read?

What you think?  If you'd like to fix the above, go for it... otherwise, i'll fix the license myself and commit as is.

Thanks.;;;","22/Dec/10 20:26;jesse_yates;Stack, I see what you are saying about the && vs ||. I think the original way involves and extra line (was shooting or concise), but I prefer your way; I'll switch it up.

Sorry about the reformatting- old eclipse autoformat habits die hard- though I was using trying to use the hbase std format. Do you want me to switch it back to the old version or just keep it?;;;","22/Dec/10 20:55;stack;Your formatting improvements were near all moving the file closer to the hbase 'standard' format so they are all good.  The only mess was the way it did license at head of the file.  Fix if you can.  Don't worry if you can't.  If'll fix on commit.  Thanks.;;;","24/Dec/10 02:37;jesse_yates;Updated patch based on stack's comment's for || vs. && and (hopefully) license. 

Sorry about the late update - hotel internet was crapping out :-(;;;","28/Dec/10 21:30;stack;@Jesse When I try to apply this patch I get this:

{code}
stack:trunk Stack$ patch -p1 < HBASE-3387.patch 
patching file src/main/java/org/apache/hadoop/hbase/util/Pair.java
patch: **** malformed patch at line 34:   * @param <T1>
{code}

Does it apply for you?;;;","28/Dec/10 22:18;jesse_yates;Sorry about the non-working patch - turns out it was b/c git diff (at least in my setup) appends a/ and b/ to the directories of the original and new versions. This means that the patch will necessarily fail. Went back through, edited by hand and double checked that it works this time. Sorry again!;;;","29/Dec/10 00:05;stack;Committed to TRUNK.  Thanks for the patch Jesse.  Will leave it in 0.90.1 bucket so it gets committed there too.;;;","29/Dec/10 00:05;stack;Reopening to commit 0.90.1;;;","29/Dec/10 00:10;stack;Committed to branch after tagging RC2;;;","29/Dec/10 03:16;hudson;Integrated in HBase-TRUNK #1699 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1699/])
    HBASE-3387 Pair does not deep check arrays for equality
;;;","17/Jan/11 06:02;nspiegelberg;This JIRA patch is a big consistency problem and should be reverted!  http://www.ibm.com/developerworks/java/library/j-jtp05273.html  

Basically, java containers assume a.equals(b) == false if a.hashcode() != b.hashcode().  Furthermore,

byte[] a = {0,1,2}, b = {0,1,2};
false == a.equals(b)
true == Pair.newPair(a,a).equals(Pair.newPair(b,b))

Was this patch introduced to fix any bug in the existing subsystem?;;;","17/Jan/11 15:41;jesse_yates;This was introduced to correct behavior in software on my side (e.g. non-HBase core) that for all intents and purposes should have worked, but didn't because of the previous definition of Pair.equals().

As far as the equals/hashCode issue, we could also overwrite the hashCode in Pair to ensure that the equals and hashCode methods work out to the same equality.

This change was made because they underlying arrays really should check the underlying types in Pairs, to check if they really would be the same thing. If we fix the hashCode method, then I think we end up with a system that is really more flexible than just comparing the pointer addresses.

. Is there somewhere in HBase where this breaks the current model and I just missed it?;;;","17/Jan/11 18:01;ekohlwey;@Nicolas Spiegelberg
Is the issue just that the hashCode contract isn't kept here? I'd suggest that thats a separate bug (missing hashCode), but not an issue such that the patch needs to be reverted.;;;","17/Jan/11 21:01;nspiegelberg;Although the hasCode contract was the original issue that I noticed, a deeper issue that this is a fundamental violation of equivalence relationships (http://en.wikipedia.org/wiki/Equivalence_relation ).  In particular, overriding Java functionality for one special class is making Java's equality operations non-symmetric.  For example:

byte[] a = {0,1,2}, b = {0,1,2};
System.out.println( a.equals(b) );  // false
Pair<byte[], byte[]> pa = Pair.newPair(a,a); pb = Pair.newPair(b,b);
System.out.println(pa.equals(pb); // true
System.out.println(pa.getFirst().equals(pb.getFirst()) && pa.getSecond().equals(pb.getSecond())); // false

The current equals override makes it such that a == c && b == d is not symmetric with (a,b) == (c,d). 

I understand your problem with needing a deep comparison.  However, I think if you want deep comparisons, you need to take another action other than overriding a contracted Java API.  Example ideas:

1. Use org.apache.hadoop.hbase.util.Bytes.ByteArrayComparator for your collection's comparator or hand-roll your own.
2. Create a DeepEqual template class that utilizes java.util.Arrays.deepEquals & deepHashCode.  Then create a Pair<DeepEqual<T>, DeepEqual<T>>

However, the Pair class must be refactored to restore its equivalence relations.  The Pair/Tuple class is a well-known idiom and should act the same here as it does in C++, Python , and other languages.;;;","18/Jan/11 06:27;stack;Thanks for catching this Nicolas.  Jesse you want to have a go at it?  Otherwise I will.;;;","18/Jan/11 06:28;stack;Upping priority some.;;;","18/Jan/11 13:18;jesse_yates;Yeah, I got it- up by this afternoon.

Does it make any sense to add in a DeepCheck class (as discussed above) or should I just keep something like that application side?;;;","18/Jan/11 23:29;jesse_yates;Revert patch for HBASE-3387 issue to roll back the changes as per discussion.;;;","20/Jan/11 00:45;stack;I just applied your revert patch to trunk and 0.90 branch Jesse.  Thanks for providing it Jesse.   Thanks Nicolas for catching this.  I should have.;;;","20/Jan/11 03:03;hudson;Integrated in HBase-TRUNK #1714 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1714/])
    HBASE-3387 Pair does not deep check arrays for equality -- REVERTED
;;;","21/Jan/11 19:04;stack;Moving out of 0.90.1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in TableRecordReaderImpl.restart,HBASE-3386,12493916,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,jdcryans,jdcryans,jdcryans,22/Dec/10 19:22,20/Nov/15 12:41,14/Jul/23 06:06,22/Dec/10 19:55,,,,,,,,,,,,0.90.0,,,,,,,0,,,"I was running some testing on a already busy cluster and my clients in the mappers sometimes aren't able to create an HTable because of ConnectionLoseException. The code logs that as an error but still continues with a null HTable then this happens:

{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.restart(TableRecordReaderImpl.java:58)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.init(TableRecordReaderImpl.java:67)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReader.init(TableRecordReader.java:57)
	at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.createRecordReader(TableInputFormatBase.java:108)
{noformat}

Needs to be prettier.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/10 19:48;jdcryans;HBASE-3386.patch;https://issues.apache.org/jira/secure/attachment/12466829/HBASE-3386.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26825,Reviewed,,,,Fri Nov 20 12:41:51 UTC 2015,,,,,,,,,,"0|i0hm27:",100836,,,,,,,,,,,,,,,,,,,,,"22/Dec/10 19:48;jdcryans;We can't throw an IOE when we fail to set the HTable, so instead I do it in TableInputFormatBase and try to give a nice message.;;;","22/Dec/10 19:52;stack;+1;;;","22/Dec/10 19:55;jdcryans;Committed to branch and trunk, thanks for reviewing Stack.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move User-Triggered Compactions to Store,HBASE-3384,12493838,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,nspiegelberg,nspiegelberg,nspiegelberg,22/Dec/10 07:32,20/Nov/15 12:41,14/Jul/23 06:06,17/Jan/11 21:45,0.90.0,0.90.1,0.92.0,,,,,,,,,0.90.0,,,,,,,0,,,There are currently miscellaneous small race conditions or workflows that would ignore a user-triggered major compaction.  Moving the major compaction flag to the Store level and adding a couple small checks should fix these issues.,,khemani,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2770,,,,,,,,,,,,,,,,,"22/Dec/10 07:33;nspiegelberg;fixForceMajor.patch;https://issues.apache.org/jira/secure/attachment/12466795/fixForceMajor.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26824,,,,,Fri Nov 20 12:41:29 UTC 2015,,,,,,,,,,"0|i0hm1z:",100835,,,,,,,,,,,,,,,,,,,,,"22/Dec/10 21:10;stack;+1;;;","28/Dec/10 02:44;hudson;Integrated in HBase-TRUNK #1698 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1698/])
    HBASE-3384 : Move User-Triggered Compactions to Store
;;;","21/Jan/11 16:13;stack;This made it into 0.90.0.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[0.90RC1] bin/hbase script displays ""no such file"" warning on target/cached_classpath.txt",HBASE-3383,12493837,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,jdcryans,tatsuya6502,tatsuya6502,22/Dec/10 07:18,20/Nov/15 12:43,14/Jul/23 06:06,22/Dec/10 23:03,0.90.0,,,,,,,,,,,0.90.0,,shell,,,,,0,,,"HBase version: 0.90 release-candidate-1

bin/hbase blindly creates ""target"" directory and fails to generate ""target/cached_classpath.txt"" if it's in the binary (pre-built) distribution. 

{code}
$ bin/hbase shell
cat: ... /hbase-0.90.0/bin/../target/cached_classpath.txt: No such file or directory
HBase Shell; enter 'help<RETURN>' for list of supported commands.
{code}

{code:title=bin/hbase}

112: add_maven_deps_to_classpath() {
113:   # The maven build dir is called 'target'
114:   target=""${HBASE_HOME}/target""
115:   if [ ! -d ""${HBASE_HOME}/target"" ]
116:   then
117:     mkdir ""${target}""
118:   fi
119:   # Need to generate classpath from maven pom. This is costly so generate it
120:   # and cache it. Save the file into our target dir so a mvn clean will get
121:   # clean it up and force us create a new one.
122:   f=""${target}/cached_classpath.txt""
123:   if [ ! -f ""${f}"" ]
124:   then
125:     ${MVN} -f ""${HBASE_HOME}/pom.xml"" dependency:build-classpath -Dmdep.outputFile=""${f}"" &> /dev/null
126:   fi
127:   CLASSPATH=${CLASSPATH}:`cat ""${f}""`
{code}

Maybe we can simply skip this process if ""target"" directory doesn't exist.
",Ubuntu 10.10 Server,larsfrancke,mingjielai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/10 22:42;jdcryans;HBASE-3383.patch;https://issues.apache.org/jira/secure/attachment/12466844/HBASE-3383.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26823,Reviewed,,,,Fri Nov 20 12:43:17 UTC 2015,,,,,,,,,,"0|i0hm1r:",100834,,,,,,,,,,,,,,,,,,,,,"22/Dec/10 15:27;stack;This is a little bit involved.  We used to detect difference between a binary hbase install and a checkout by looking for presence of pom.xml.  Now that you can build a 'binary' bundle -- our binaries now include src, pom.xml, etc -- our little test no longer works.  Not sure what to do here now a binary hbase package can turn into a build environment.  Let me look.;;;","22/Dec/10 22:42;jdcryans;Here's a try, I only do the check like Tatsuya said and cleaned up some ruby testing related lines that we have to get rid of since we had to revert the jruby version.;;;","22/Dec/10 22:56;stack;Did you check the CLASSPATH?  If binary tgz, then at top level there will be hbase*jar.   But if we then do a build and target dir appears, we are now a 'dev' env.  In dev env, the content of classes should be before the top level jar.  Is that the case?

Otherwise, the patch looks good to me.  +1;;;","22/Dec/10 23:01;jdcryans;I did a check and the classes folder comes before the jars on the classpath, I also did a test to make sure it worked well by outputting some debug message while creating an HTable.;;;","22/Dec/10 23:03;jdcryans;Committed to branch and trunk, thanks for the review Stack.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    HBASE-3383  [0.90RC1] bin/hbase script displays ""no such file"" warning on
               target/cached_classpath.txt
;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Interrupt of a region open comes across as a successful open,HBASE-3381,12493805,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,21/Dec/10 20:52,20/Nov/15 12:42,14/Jul/23 06:06,22/Dec/10 18:16,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Meta was offline when below happened:

{code}
2010-12-21 19:45:23,023 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x12d0a53c540000e Attempting to transition node 337038b50e467fbd6b031f278bbd9c22 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2010-12-21 19:45:23,046 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x12d0a53c540000e Successfully transitioned node 337038b50e467fbd6b031f278bbd9c22 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2010-12-21 19:45:26,379 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Interrupting thread Thread[PostOpenDeployTasks:337038b50e467fbd6b031f278bbd9c22,5,main]
2010-12-21 19:45:26,379 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x12d0a53c540000e Attempting to transition node 337038b50e467fbd6b031f278bbd9c22 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2010-12-21 19:45:26,381 WARN org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Exception running postOpenDeployTasks; region=337038b50e467fbd6b031f278bbd9c22
org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException: Interrupted
    at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:364)
    at org.apache.hadoop.hbase.catalog.MetaEditor.updateRegionLocation(MetaEditor.java:146)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1331)
    at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$PostOpenDeployTasksThread.run(OpenRegionHandler.java:195)
...
{code}

So, we timed out trying to open the region but rather than close the region because edit failed, we missed seeing the InterruptedException.

Here is suggested fix:

{code}
diff --git a/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java b/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java
index 7bf680d..2b0078c 100644
--- a/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java
+++ b/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java
@@ -339,7 +339,7 @@ public class MetaReader {
     get.addFamily(HConstants.CATALOG_FAMILY);
     byte [] meta = getCatalogRegionNameForRegion(regionName);
     Result r = catalogTracker.waitForMetaServerConnectionDefault().get(meta, get);
-    if(r == null || r.isEmpty()) {
+    if (r == null || r.isEmpty()) {
       return null;
     }
     return metaRowToRegionPair(r);
{code}

Let me try it.

W/o this, what we see is hbck showing that region is on server X but in .META. it shows as being on Y (its pre-balance server)",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/10 23:49;stack;3381.txt;https://issues.apache.org/jira/secure/attachment/12466779/3381.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26821,Reviewed,,,,Fri Nov 20 12:42:24 UTC 2015,,,,,,,,,,"0|i0hm1j:",100833,,,,,,,,,,,,,,,,,,,,,"21/Dec/10 21:48;stack;Jon pointed out that the above change is nought.  Here is what I meant:

{code}
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java b/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
index d3c78e1..0730fe1 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
@@ -165,7 +165,7 @@ public class OpenRegionHandler extends EventHandler {
     }
     // Was there an exception opening the region?  This should trigger on
     // InterruptedException too.  If so, we failed.
-    return t.getException() == null;
+    return !t.interrupted() && t.getException() == null;
   }
 
   /**
{code}

In the lines above, if we timeout, we interrupt the worker thread only we don't wait... we just move on to the test for exception.  I add the interrupted so if interrupted then that'll count as failed open.

Testing.;;;","21/Dec/10 23:49;stack;Here is the patch.  I've not been able to repro the condition during last few hours of testing so would like to commit this (need a +1 -- Jon?).  While in here, I did some cleanup of hbck messages and stopped it claiming error when offlined split parent.  Also added logging around fixup of case where parent offlining edit got in but not daughter addtions; needed debugging.;;;","22/Dec/10 00:00;streamy;+1 on all but the last change in HBCK.

{noformat}

@@ -397,7 +399,8 @@ public class HBaseFsck {
       return;
     } else if (inMeta && !shouldBeDeployed && !isDeployed) {
       // offline regions shouldn't cause complaints
-      LOG.debug(""Region "" + descriptiveName + "" offline, ignoring."");
+      LOG.debug(""Region "" + descriptiveName + "" offline, splitParent="" + splitParent +
+        "", ignoring."");
       return;
     } else if (recentlyModified) {
       LOG.info(""Region "" + descriptiveName + "" was recently modified -- skipping"");
{noformat}

This version of HBCK changes the {{shouldBeDeployed}} to use the new ZK-based table disabling...

{noformat}
    boolean shouldBeDeployed = inMeta && !isTableDisabled(hbi.metaEntry);
{noformat}

and...

{noformat}
  private boolean isTableDisabled(HRegionInfo regionInfo) {
    return disabledTables.contains(regionInfo.getTableDesc().getName());
  }
{noformat}

I think there should be (yet another) boolean in HBCK that is offlineInMeta.  And then separate checks for the possible good/bad states of that flag.  (and this log message should remain as it was since it's not relevant for splits).;;;","22/Dec/10 00:19;stack;Committing to branch and trunk but changed the hbck emissions slightly after discussion with Jon.;;;","22/Dec/10 17:40;stack;What I committed was crap.  More often than not, we'd get woken up because the worker thread was done but we'd then interrupt the worker thread though it was on its way out.  Made for confusing logging.

I have a new patch that I've been testing.  Will put it up in a sec.;;;","22/Dec/10 17:42;stack;Here is evidence the mechanism is basically working (though this is not really what ths specific issue is about..... still need evidence that that is working):

{code}
2010-12-22 17:17:25,352 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Interrupting thread Thread[PostOpenDeployTasks:9872342bec238398a94dee894ff29a6f,5,main]
2010-12-22 17:17:25,352 WARN org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Exception running postOpenDeployTasks; region=9872342bec238398a94dee894ff29a6f
org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException: Interrupted
    at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:364)
    at org.apache.hadoop.hbase.catalog.MetaEditor.updateRegionLocation(MetaEditor.java:146)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1331)
    at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$PostOpenDeployTasksThread.run(OpenRegionHandler.java:205)
2010-12-22 17:17:25,352 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing TestTable,0612723473,1293034036869.9872342bec238398a94dee894ff29a6f.: disabling compactions & flushes
2010-12-22 17:17:25,352 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates disabled for region TestTable,0612723473,1293034036869.9872342bec238398a94dee894ff29a6f.
2010-12-22 17:17:25,352 DEBUG org.apache.hadoop.hbase.regionserver.Store: closed info
...
{code}
;;;","22/Dec/10 17:54;stack;Sorry, the above stack trace IS evidence that latest spin on this patch is working (Its below).  We were stuck in CatalogManager waiting on .META. to come back and it was going on too long so the worker thread was interrupted... and the open of the region closed up (See above the 'Interrupting thread' message and then the stack trace is actually out of the worker thread named PostOpenDeployTasksThread).

Here is patch I'm committing:

{code}
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java b/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
index d3c78e1..28bdfb9 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
@@ -160,8 +160,18 @@ public class OpenRegionHandler extends EventHandler {
     // Is thread still alive?  We may have left above loop because server is
     // stopping or we timed out the edit.  Is so, interrupt it.
     if (t.isAlive()) {
-      LOG.debug(""Interrupting thread "" + t);
-      t.interrupt();
+      if (!signaller.get()) {
+        // Thread still running; interrupt
+        LOG.debug(""Interrupting thread "" + t);
+        t.interrupt();
+      }
+      try {
+        t.join();
+      } catch (InterruptedException ie) {
+        LOG.warn(""Interrupted joining "" +
+          r.getRegionInfo().getRegionNameAsString(), ie);
+        Thread.currentThread().interrupt();
+      }
     }
     // Was there an exception opening the region?  This should trigger on
     // InterruptedException too.  If so, we failed.
{code}

Only interrupt if our signaller has NOT been set (it will not be set if we are stuck trying to update meta).  Then join on the thread so we have chance to pick up any exceptions... so we return the right result out of this method.;;;","22/Dec/10 18:16;stack;Re-resolving after testing second attempt at a fix.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master failover can split logs of live servers,HBASE-3380,12493779,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,streamy,jdcryans,jdcryans,21/Dec/10 18:38,20/Nov/15 12:43,14/Jul/23 06:06,22/Dec/10 18:33,,,,,,,,,,,,0.90.0,,,,,,,0,,,"The reason why TestMasterFailover fails is that when it does the master failover, the new master doesn't wait long enough for all region servers to checkin so it goes ahead and split logs... which doesn't work because of the way lease timeouts work:

{noformat}
2010-12-21 07:30:36,977 DEBUG [Master:0;vesta.apache.org:33170] wal.HLogSplitter(256): Splitting hlog 1 of 1:
 hdfs://localhost:49187/user/hudson/.logs/vesta.apache.org,38743,1292916616340/vesta.apache.org%3A38743.1292916617204, length=0
2010-12-21 07:30:36,977 DEBUG [WriterThread-1] wal.HLogSplitter$WriterThread(619): Writer thread Thread[WriterThread-1,5,main]: starting
2010-12-21 07:30:36,977 DEBUG [WriterThread-2] wal.HLogSplitter$WriterThread(619): Writer thread Thread[WriterThread-2,5,main]: starting
2010-12-21 07:30:36,977 INFO  [Master:0;vesta.apache.org:33170] util.FSUtils(625): Recovering file
 hdfs://localhost:49187/user/hudson/.logs/vesta.apache.org,38743,1292916616340/vesta.apache.org%3A38743.1292916617204
2010-12-21 07:30:36,979 WARN  [IPC Server handler 8 on 49187] namenode.FSNamesystem(1122): DIR* NameSystem.startFile:
 failed to create file /user/hudson/.logs/vesta.apache.org,38743,1292916616340/vesta.apache.org%3A38743.1292916617204 for
 DFSClient_hb_m_vesta.apache.org:33170_1292916630791 on client 127.0.0.1, because this file is already being created by
 DFSClient_hb_rs_vesta.apache.org,38743,1292916616340_1292916617166 on 127.0.0.1
...
2010-12-21 07:33:44,332 WARN  [Master:0;vesta.apache.org:33170] util.FSUtils(644): Waited 187354ms for lease recovery on
 hdfs://localhost:49187/user/hudson/.logs/vesta.apache.org,38743,1292916616340/vesta.apache.org%3A38743.1292916617204:
 org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file
 /user/hudson/.logs/vesta.apache.org,38743,1292916616340/vesta.apache.org%3A38743.1292916617204
 for DFSClient_hb_m_vesta.apache.org:33170_1292916630791 on client 127.0.0.1, because this file is already
 being created by DFSClient_hb_rs_vesta.apache.org,38743,1292916616340_1292916617166 on 127.0.0.1
{noformat}

I think that we should always check in ZK the number of live region servers before waiting for them to check in, this way we know how many we should expect during failover. There's also a case where we still want to timeout, since RS can die during that time, but we should wait a bit longer.",,larsfrancke,nkeywal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-4610,,,,,,,,,,,,,"21/Dec/10 19:36;streamy;HBASE-3380-v1.patch;https://issues.apache.org/jira/secure/attachment/12466753/HBASE-3380-v1.patch","22/Dec/10 00:53;streamy;HBASE-3380-v2.patch;https://issues.apache.org/jira/secure/attachment/12466780/HBASE-3380-v2.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26820,Reviewed,,,,Fri Nov 20 12:43:07 UTC 2015,,,,,,,,,,"0|i0hm1b:",100832,,,,,,,,,,,,,,,,,,,,,"21/Dec/10 19:19;streamy;bq. the new master doesn't wait long enough for all region servers to checkin so it goes ahead and split logs
we should add a configuration param, something like {{hbase.master.wait.on.regionservers.mintostart}}.  it will at least be useful for tests and i think it would be generally useful as well.

bq. I think that we should always check in ZK the number of live region servers before waiting for them to check in
What we need to do is move away from relying on the heartbeat for check-in.  I'd prefer to punt on this fix until 0.92 and fix the test with above.  As for this bug happening in a production cluster, we've actually taken to increasing {{hbase.master.wait.on.regionservers.interval}} from default of 3 seconds up to like 20 seconds.  I think a default of 10 seconds is better (will add a delay in some cases but because of failover / interaction with heartbeat period, we need to have it larger than that period).;;;","21/Dec/10 19:36;streamy;Adds new config params into the waitOnRS method and changes the default timeout from 3 seconds to 9 seconds (but at a 3 second interval to reduce expected wait).  Also added min/max limits so you have the option of setting this stuff and not being forced to wait (useful in tests and in general, I think).

I can do a simpler patch to just try and fix failing test (I think changing defaults is enough) but I've wanted to add these configs anyways.  In 0.92 we'll probably still need these configs, we'll just rely on ZK nodes not periodic RPCs so we should be able to reduce the timeout from 9 seconds.;;;","21/Dec/10 19:49;stack;This could work for 0.90.0.  My main issue is the 9 second wait period.  Doesn't it mean 9 seconds pass before we actually go do something?

Wouldn't J-Ds suggestion of looking into zk and using number found there be better for 0.90.0?;;;","21/Dec/10 20:00;streamy;It'd work but it's a halfway hack to the real fix we should do in 0.92.

My points is that I think it is a good idea in general to raise this timeout (while adding configs so you don't necessarily have to wait since smallish clusters could use the max param if they want to speed this up)... we have run into the situation where not all RS came in the 3 second window (which really is too small given heartbeat frequency) so I've already ramped up the config timeout for fb production.

But I'm fine implementing the halfway fix if my argument still doesn't win you over :);;;","21/Dec/10 20:11;stack;I'm worried about out of the box having a 9 second pause before get going.  3 seconds maybe too short yes.  Should be 1 1/2 heartbeat or something.    Yeah in 0.92 we'll purge heartbeat but lets get 0.90 out first (smile).  The longer timeout might be enough to fix the failing test.  The check of whats up on zk would help guard against splitting live server logs.... ugly.;;;","21/Dec/10 20:12;kannanm;<<< I think that we should always check in ZK the number of live region servers before waiting for them to check in, this way we know how many we should expect during failover. There's also a case where we still want to timeout, since RS can die during that time, but we should wait a bit longer.>>>

Instead of waiting on them to check in, should splitLog() do one additional sanity test to make sure the directory it is about to split doesn't have a corresponding node in ZK (under /hbase/rs)?

;;;","21/Dec/10 20:17;streamy;All great ideas.  Been wanting to punt on 0.92 and do it right there.

I think it will be sufficient for 0.90 and this jira to use my patch but change timeout to 4500ms, interval to 1500ms.  Only 50% increase in waiting (but because of interval + timeout you might actually wait less).  And I like these new configs if you aren't against them.

So I propose my patch w/ change of defaults to 1500/4500.;;;","21/Dec/10 22:24;jdcryans;bq. should splitLog() do one additional sanity test to make sure the directory it is about to split doesn't have a corresponding node in ZK (under /hbase/rs)?

Currently that class doesn't talk with ZK, and since it's called MasterFileSystem I think it would make that class less cohesive but we could pass the list from HMaster. I think this could be done on top of counting the number of RS to expect during failover.

bq. So I propose my patch w/ change of defaults to 1500/4500.

I don't mind the changes as it helps on cold starts, but I think that in the failover case calling regionServerTracker.getOnlineServers().size() would be simple and reliable.;;;","21/Dec/10 22:30;streamy;bq. I don't mind the changes as it helps on cold starts, but I think that in the failover case calling regionServerTracker.getOnlineServers().size() would be simple and reliable.

I don't think it's necessarily simple and I would prefer not to do halfway fixes where we are still actually relying on heartbeats.

The simplest implementation of this fix would create a new bug where the RS has actually died but znode is not gone yet.  So master will wait until it reaches N regionservers but will only ever get heartbeats from N-1.  So then we have a new maxTimeout parameter to deal with this case (and now you're basically where we are with my patch but without added benefits of the new configs).

I think these timeout changes are all that is necessary to fix both cases until we remove heartbeats in 0.92.;;;","21/Dec/10 22:43;jdcryans;bq. The simplest implementation of this fix would create a new bug where the RS has actually died but znode is not gone yet. So master will wait until it reaches N regionservers but will only ever get heartbeats from N-1. So then we have a new maxTimeout parameter to deal with this case (and now you're basically where we are with my patch but without added benefits of the new configs).

I think you demonstrate that both solutions work together, and that by looking at ZK you possibly don't have to wait as long.;;;","22/Dec/10 00:53;streamy;Committing this to see if it fixes hudson after getting +1 from jd on irc.;;;","22/Dec/10 18:33;stack;Resolving.  Hudson 0.90 tests passed last night.;;;","17/Oct/11 22:41;streamy;So it looks like we thought we'd do a proper fix for 0.92, but do we have one?  There's some good config params that were committed as part of this JIRA into 0.90 that are now not available in 0.92.

Should this be committed to 0.92 and trunk?  I'd like to at least bring these config params over since they are pretty nice (and will make a more elegant solution to stuff like HBASE-4603).;;;","17/Oct/11 22:49;yuzhihong@gmail.com;+1 on bringing over the parameters.;;;","17/Oct/11 23:26;jdcryans;Let's do it, +1.;;;","17/Oct/11 23:36;streamy;What's the best practice here?  Should I just commit this to 92 and trunk and make a note here?  Should I open a new jira since this is so old?

(Thanks for input guys);;;","17/Oct/11 23:39;jdcryans;Since it's almost a year old I'd prefer a new jira.;;;","17/Oct/11 23:39;yuzhihong@gmail.com;Please open new JIRA where we may come up with better idea.;;;","17/Oct/11 23:47;streamy;Heartbeats still exist so I'm not sure much is different in 92 since we tackled this, right?

I will open a new JIRA though.;;;","08/Dec/11 04:05;hudson;Integrated in HBase-0.92 #176 (See [https://builds.apache.org/job/HBase-0.92/176/])
    HBASE-4610  Port HBASE-3380 (Master failover can split logs of live servers) to 92/trunk

tedyu : 
Files : 
* /hbase/branches/0.92/CHANGES.txt
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
* /hbase/branches/0.92/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java
;;;","08/Dec/11 05:55;hudson;Integrated in HBase-0.92-security #33 (See [https://builds.apache.org/job/HBase-0.92-security/33/])
    HBASE-4610  Port HBASE-3380 (Master failover can split logs of live servers) to 92/trunk

tedyu : 
Files : 
* /hbase/branches/0.92/CHANGES.txt
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
* /hbase/branches/0.92/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java
;;;","08/Dec/11 06:47;hudson;Integrated in HBase-TRUNK-security #25 (See [https://builds.apache.org/job/HBase-TRUNK-security/25/])
    HBASE-4610  Port HBASE-3380 (Master failover can split logs of live servers) to 92/trunk

tedyu : 
Files : 
* /hbase/trunk/CHANGES.txt
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java
;;;","08/Dec/11 08:44;hudson;Integrated in HBase-TRUNK #2526 (See [https://builds.apache.org/job/HBase-TRUNK/2526/])
    HBASE-4610  Port HBASE-3380 (Master failover can split logs of live servers) to 92/trunk

tedyu : 
Files : 
* /hbase/trunk/CHANGES.txt
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java
;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Our jruby jar has *GPL jars in it; fix",HBASE-3374,12493621,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,19/Dec/10 06:15,20/Nov/15 12:40,14/Jul/23 06:06,03/Mar/11 23:48,,,,,,,,,,,,0.92.0,,,,,,,0,,,"The latest JRuby's complete jar bundles *GPL jars (JNA and JFFI among others).   It looks like the functionality we depend on -- the shell in particular -- makes use of these dirty jars so they are hard to strip.  They came in because we (I!) just updated our JRuby w/o checking in on what updates contained.  JRuby has been doing this for a while now (1.1.x added the first LGPL).  You have to go all the ways back to the original HBase checkin, HBASE-487, of JRuby -- 1.0.3 -- to get a JRuby w/o *GPL jars.

Plan is to try and revert our JRuby all the ways down to 1.0.3 before shipping 0.90.0.  Thats what this issue is about.

We should also look into moving off JRuby in the medium to long-term.  Its kinda awkward sticking on an old version that is no longer supported.  I'll open an issue for that.",,apurtell,jghoman,larsfrancke,wattsteve,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/10 00:38;stack;jruby.txt;https://issues.apache.org/jira/secure/attachment/12466672/jruby.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26816,,,,,Fri Nov 20 12:40:54 UTC 2015,,,,,,,,,,"0|i0hlzz:",100826,The lads over in jruby land say 1.6RC2 should have the licensing fixup.  Lets get it into TRUNK when ready.  I don't think this a 0.90.x issue.  Moving it out.,,,,,,,,,,,,,,,,,,,,"20/Dec/10 09:38;larsfrancke;I'm sorry. I might have been the one who upgraded that in one of the Maven cleanups without taking a close look.;;;","20/Dec/10 15:27;stack;@Lars It was not you.  The damage was already done many times over.;;;","20/Dec/10 23:56;stack;So, 1.0.3. is still available in maven repos.  Thats good.  java_import is not available in 1.0.3.  Thats the 'safe' way of doing imports --- see http://jira.codehaus.org/browse/JRUBY-3171.  Using raw 'import' seems to work but dumps out the following before shell starts if the referenced import is used more than once -- e.g. if we're using HConstants defines in a few places in .rb code:

{code}
(eval):1 warning: already initialized constant HConstants
{code}

Fully specifying java classes with packages in all places they are used seems to get rid of the above.  I'm working on a patch to do that.

We will have to remove TestShell.  It uses facility only available in later jrubys.

Looking at the way the shell was refactored by Alexei, its kinda nice the way he did it.  All of the java access is done in one or two files apart from the bulk of the ruby code; makes it easier doing this fixup.;;;","21/Dec/10 00:38;stack;Here is what I applied to 0.90.  I applied same to trunk but took a little fixup.

It looks to be all working.  I exercised a bunch of shell facility and I think I got all the weird imports and references.  Lets open new issues if there is anything that I forgot... Shell seems fine.  Maybe a little slower than it used to be.;;;","21/Dec/10 00:44;stack;I did a bit more testing of shell.. enable, disable, create, alter, etc.  Seems to be doing like it used to.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","04/Jan/11 03:22;wattsteve;Thanks for turning this issue around so quickly. Awesome work.;;;","23/Jan/11 21:40;headius;Woah there cowboys!

Downgrading to JRuby 1.0.3 is a *TERRIBLE* idea. 1.0.3 does not include a compiler, and performs somewhere in the neighborhood of 50-100 times worse than current JRuby versions. It also has much worse compatibility with Ruby, is not compatible at all with Ruby 1.8.7 or 1.9, and has many other leaks, bugs, and flaws.

We would happily work with you guys (and Apache proper) to either publish a version of JRuby that does not include the offending libraries, or try to get the creator of those libraries (who is a JRuby committer) to relicense them.

I would STRONGLY recommend undoing the massive damage you will cause by downgrading to JRuby 1.0.3 and working with us to get you a ""safe"" version of a more current JRuby.;;;","23/Jan/11 21:52;ryanobjc;hi Charles,

The patch has already been submitted, the deed is done.

In our survey of the code, there were about 4 different libraries embedded and included in JRuby, some of LGPL some of GPLv3.

Some of them look pretty fundamental, such as the dependency on: http://code.google.com/p/jvm-language-runtime

As you probably know, the ASF has pretty strict rules regarding licensing, so undoing the downgrade is not possible until a JRuby that has ASF compatible licensing is available. ;;;","23/Jan/11 21:55;ryanobjc;Based on COPYING from:
https://github.com/jruby/jruby/blob/master/COPYING

The following libraries are a problem:

 build_lib/jffi*jar (http://kenai.com/projects/jffi) and
 build_lib/jgrapht-jdk1.5.jar (http://jgrapht.sourceforge.net) are
 distributed under the GPL v3.

 build_lib/jna*jar (http://jna.dev.java.net) are distributed
 under the LGPL-2.1+ license.

 build_lib/invokedynamic.jar and build_lib/jsr292-mock.jar
 (http://code.google.com/p/jvm-language-runtime) are distributed
 under the LGPL license.


Also there are a few broken links such as http://projectkenai.com/projects/jaffl which are a little concerning.



;;;","23/Jan/11 21:59;headius;Ryan: I understand what ASF needs, and that's exactly what I want to provide.

Of the libraries that are GPL, according to our COPYING file:

* jvm-language-runtime is not shipped with JRuby binaries. It is used only because it provides a mock JSR-292 invokedynamic library for compile-time on JVMs that do not have JSR-292 (Java 6 and lower).
* jffi could potentially be relicensed, or simply removed; it is for native library access, and JRuby would work correctly without it.
* jgrapht-jdk15 is only used by a newer in-development compiler that is not used normally, and could be removed. We would like to explore replacements.
* we do not ship JNA anymore

Of these, only jffi and jgrapht are in the binaries we distribute, and it's likely JRuby would work with them just yanked out. If any of the other libraries are included in our binaries, it's a bug in our build process.

If we provide a build of JRuby that does not include those GPLed libraries, would that satisfy ASF's requirements?;;;","23/Jan/11 22:33;headius;FYI, I have taken this to be a JRuby issue, and filed the following: http://jira.codehaus.org/browse/JRUBY-5410

I want us to publish a *GPL-free maven artifact for JRuby 1.6.0. This should be usable by Apache and other projects that can't include GPLed code.

I am talking to the maintainer of jffi (and jaffl, which was incorrectly listed as MIT licensed in COPYING) about adding an Apache-friendly license.

I added a jar-jruby-nogpl target to our build.xml and fixed COPYING to properly reflect jaffl licensing and jffi and jaffl homepages

I also confirmed that simply removing jffi, jaffl, and jgrapht from jruby.jar does not damage general Ruby behavior. As mentioned above, some native or system-level capabilities are disabled or work differently, but IRB, RubyGems, and Rails all function correctly.;;;","23/Jan/11 23:45;headius;The author of jffi and jaffl has asked if Sun's CDDL would be an acceptable license under which he could dual-license jffi, jaffl, and other projects. Is CDDL acceptable?;;;","24/Jan/11 00:15;headius;My reading of Apache's position on CDDL (from http://www.apache.org/legal/3party.html#criteriaandcategories) is that CDDL would be acceptable if the software/library in question is distributed only in binary form and an appropriate note of the so-licensed software is made in the LICENSE file. This appears to puts CDDL in the same category as CPL, under which license JRuby itself is allowed for inclusion in HBase.

So I think CDDL would be acceptable...correct?;;;","24/Jan/11 00:41;apurtell;Alright Charles, leaving the meta issue of whether Ruby syntax is really what people want long term for the shell, reopening this issue perhaps to upgrade to an ASF-clean JRuby for the next HBase release if JRuby sorts the licensing issues out.;;;","24/Jan/11 16:18;stack;@Charles Yes, CDDL looks fine.  We can add appropriate notices np.  Thanks for taking this on.;;;","25/Jan/11 05:56;headius;Andrew, stack: Thank you for giving us an opportunity to fix this :)

An update for you.

Due to the oldness of CDDL, the author of jffi and jaffl (Wayne Meissner, also one of the primary -- or the primary -- contributors to jna) has opted to add EPL as an additional license. This is roughly in line with our CPL license in JRuby and falls into the same category as CPL and CDDL as far as inclusion in Apache projects.

We will also strip jgrapht from JRuby 1.6.0.RC2, so the binaries for JRuby 1.6 final will not have any GPL or LGPL-only library dependencies.

I'm very glad we were able to get this sorted out.;;;","25/Jan/11 06:01;stack;@Charles sweet!;;;","26/Jan/11 01:05;apurtell;@Charles Thanks, looking forward to JRuby 1.6.;;;","27/Jan/11 05:41;headius;A final update, and then hopefully this will be all set for JRuby 1.6RC2:

Wayne has decided to go ahead and just add Apache-2.0 to his jffi and jaffl projects. I guess that should settle things nicely :);;;","27/Jan/11 06:14;stack;@Charles That'll work (smile).;;;","27/Jan/11 15:43;wattsteve;@Charles Thanks for the contributions. This is some awesome cross-project synergy. 

@stack et al - I'll do another legal scan of HBase for 0.90.1 once its out and will let you know if I find any other issues. Thanks for resolving this so quickly.;;;","08/Feb/11 18:34;headius;The JRuby bug (JRUBY-5410) has been resolved! JRuby 1.6.0.RC2 will ship with no [L]GPL-only libraries, now that jaffl, jffi, and jnr-netdb are licensed Apache-2.0 and jgrapht has been removed from our binary distribution.

Thanks for your patience!;;;","08/Feb/11 18:42;stack;Thanks @Charles.  We'll wait on RC2 release then bring it in and close this issue.  Good stuff.;;;","03/Mar/11 23:33;stack;Closing.  Opened HBASE-3600 to accomodate update to jruby 1.6.0RC2 (Charles, if you see this, I notice that 1.6.0RC2 is out but its not in a maven repo -- you know why?  Usually RCs make it to maven it would seem.  Thanks, and thanks again for fixing licensing so we could use the new stuff again);;;","03/Mar/11 23:48;stack;Actually closing.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race in TestReplication can make it fail,HBASE-3371,12493561,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,17/Dec/10 23:33,20/Nov/15 12:42,14/Jul/23 06:06,18/Dec/10 00:18,,,,,,,,,,,,0.90.0,,,,,,,0,,,I found that truncating in TestReplication.setUp would sometime fail when during testStartStop the log would be rolled and not added to ReplicationSource's queue.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26815,,,,,Fri Nov 20 12:42:04 UTC 2015,,,,,,,,,,"0|i0hlzj:",100824,,,,,,,,,,,,,,,,,,,,,"17/Dec/10 23:33;jdcryans;Easy fix, force roll the logs when starting setUp:

{noformat}

+    // Starting and stopping replication can make us miss new logs,
+    // rolling like this makes sure the most recent one gets added to the queue
+    for ( JVMClusterUtil.RegionServerThread r :
+        utility1.getHBaseCluster().getRegionServerThreads()) {
+      r.getRegionServer().getWAL().rollWriter();
+    }

{noformat};;;","18/Dec/10 00:18;jdcryans;Committed to branch and trunk.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReplicationSource.openReader fails to locate HLogs when they aren't split yet,HBASE-3370,12493478,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,17/Dec/10 01:06,20/Nov/15 12:43,14/Jul/23 06:06,18/Dec/10 00:17,,,,,,,,,,,,0.90.0,,,,,,,0,,,"In ReplicationSource.openReader there's a special handling of HLogs location during a failover and it's currently broken as it will retry 10 times and if the log splitting isn't done by then then it will skip the file forever.

First issue is that it looks at the chain of failovers but not the oldest one.

Second issue is that the location is found starting from the current RS's folder, instead of the dead RS's folder.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/10 01:09;jdcryans;HBASE-3370.patch;https://issues.apache.org/jira/secure/attachment/12466433/HBASE-3370.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26814,,,,,Fri Nov 20 12:43:09 UTC 2015,,,,,,,,,,"0|i0hlzb:",100823,,,,,,,,,,,,,,,,,,,,,"17/Dec/10 01:09;jdcryans;Fix for the issue.;;;","18/Dec/10 00:17;jdcryans;Committed to branch and trunk.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We should use HBaseClient.call() that has parallelism built in ,HBASE-3369,12493472,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,ryanobjc,ryanobjc,16/Dec/10 23:19,12/Jun/22 00:41,14/Jul/23 06:06,19/Jul/14 00:39,,,,,,,,,,,,,,,,,,,0,,,"the HBaseClient supports a call like so:

  public Writable[] call(Writable[] params, InetSocketAddress[] addresses)

And we are able to dispatch and retrieve multiple requests without threads.  We should try to use this instead of using thread pools.",,apurtell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26813,,,,,Sat Jul 19 00:39:42 UTC 2014,,,,,,,,,,"0|i02dgv:",11783,,,,,,,,,,,,,,,,,,,,,"19/Jul/14 00:39;apurtell;The client has come a long way since this golden oldie;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed log split not retried,HBASE-3367,12493396,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,,jdcryans,jdcryans,16/Dec/10 05:43,20/Nov/15 12:41,14/Jul/23 06:06,18/Dec/10 00:17,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Found this running TestReplication:

{noformat}
2010-12-15 17:58:33,639 DEBUG [MASTER_SERVER_OPERATIONS-h17.sfo.stumble.net:58644-0] wal.HLogSplitter(299)
: Closed hdfs://localhost:58631/user/jdcryans/test/211477a0a924abda419b5579c7a83452/recovered.edits/0000000000000000002
2010-12-15 17:58:33,642 ERROR [MASTER_SERVER_OPERATIONS-h17.sfo.stumble.net:58644-0] master.MasterFileSystem(197):
 Failed splitting hdfs://localhost:58631/user/jdcryans/.logs/h17.sfo.stumble.net,58647,1292464631034
java.io.IOException: Discovered orphan hlog after split. Maybe HRegionServer was not dead when we started
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:290)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:151)
        at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:193)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:96)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
2010-12-15 17:58:33,686 INFO  [MASTER_SERVER_OPERATIONS-h17.sfo.stumble.net:58644-0] handler.ServerShutdownHandler(144):
 Reassigning 8 region(s) that h17.sfo.stumble.net,58647,1292464631034 was carrying (skipping 0 regions(s) that are already in transition)
{noformat}

What I see is that there was an orphan HLog, but the exception was eaten in MasterFileSystem.splitLog (it just logs as an error) and then it proceeds to reassign the regions. There is potential data loss.

Another bad side effect is that those HLogs never get archived, and stay in .logs",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/10 01:14;jdcryans;HBASE-3367.patch;https://issues.apache.org/jira/secure/attachment/12466435/HBASE-3367.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26811,Reviewed,,,,Fri Nov 20 12:41:09 UTC 2015,,,,,,,,,,"0|i0hlyv:",100821,,,,,,,,,,,,,,,,,,,,,"17/Dec/10 01:14;jdcryans;Hackish fix, not sure how handlers are supposed to be retried so instead I retry once when catching the exception.;;;","17/Dec/10 05:55;stack;+1

If it don't work on single retry, then it deserves to fail I'd say.  Patch looks good J-D.;;;","18/Dec/10 00:17;jdcryans;Committed to branch and trunk.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WALObservers should be notified before the lock,HBASE-3366,12493386,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,16/Dec/10 01:37,20/Nov/15 12:42,14/Jul/23 06:06,18/Dec/10 00:17,,,,,,,,,,,,0.90.0,,,,,,,0,,,"TestReplication failed on the last 0.90 build because the log of the machine that was killed was rolled and got a few edits in before it was put in a ZK queue.

This fix is pretty easy, the notification should be sent before the lock is acquired instead of after that. It shouldn't have been like that since the beginning but I guess I missed it when I narrowed the lock.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/10 01:10;jdcryans;HBASE-3366.patch;https://issues.apache.org/jira/secure/attachment/12466434/HBASE-3366.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26810,Reviewed,,,,Fri Nov 20 12:42:55 UTC 2015,,,,,,,,,,"0|i0hlyn:",100820,,,,,,,,,,,,,,,,,,,,,"17/Dec/10 01:10;jdcryans;Patch for the issue.;;;","17/Dec/10 06:04;stack;You are saying that the newly rolled file, the one that is created inside the synchronized block, that it got some edits and these edits were not ""observed"" because notification of roll happened after the new file was created? 

If so, +1 on your patch.;;;","17/Dec/10 19:57;jdcryans;bq  that it got some edits and these edits were not ""observed"" 

Actually it's the roll that isn't observed because the process died after rolling but before notifying and in between new edits came in the new log.;;;","18/Dec/10 00:17;jdcryans;Committed to branch and trunk.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOFE contacting crashed RS causes Master abort,HBASE-3365,12493378,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,16/Dec/10 00:20,20/Nov/15 12:40,14/Jul/23 06:06,16/Dec/10 00:33,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Just got this testing:

{code}
2010-12-16 00:05:02,863 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region TestTable,0071897074,1292373519828.8cec43d5df41ea830b08180f688f2819. to sv2borg181,60020,1292457487454
2010-12-16 00:05:02,867 FATAL org.apache.hadoop.hbase.master.HMaster: Remote unexpected exception
java.io.IOException: Call to sv2borg185/10.20.20.185:60020 failed on local exception: java.io.EOFException
    at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:788)
    at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:757)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
    at $Proxy7.closeRegion(Unknown Source)
    at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:589)
    at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1085)
    at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1032)
    at org.apache.hadoop.hbase.master.AssignmentManager.balance(AssignmentManager.java:1791)
    at org.apache.hadoop.hbase.master.HMaster.balance(HMaster.java:688)
    at org.apache.hadoop.hbase.master.HMaster$1.chore(HMaster.java:579)
    at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
Caused by: java.io.EOFException
    at java.io.DataInputStream.readInt(DataInputStream.java:375)
    at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:521)
    at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:459)
2010-12-16 00:05:02,868 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/10 00:27;stack;3365.txt;https://issues.apache.org/jira/secure/attachment/12466354/3365.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26809,,,,,Fri Nov 20 12:40:32 UTC 2015,,,,,,,,,,"0|i0hlyf:",100819,,,,,,,,,,,,,,,,,,,,,"16/Dec/10 00:27;stack;Small patch that adds EOFE as possible exception sending close region.

Will just apply.;;;","16/Dec/10 00:33;stack;Committed branch and trunk (after missing EOFE import).;;;","16/Dec/10 06:09;stack;Oh, I took a look at sending of open region via rpc to see if it needed this fix too but it works differently (any failed open is left for the RIT timeout to handle).;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReplicationZookeeper.lockOtherRS doesn't return false when lock exists,HBASE-3364,12493375,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,15/Dec/10 23:28,20/Nov/15 12:43,14/Jul/23 06:06,16/Dec/10 00:46,,,,,,,,,,,,0.90.0,,,,,,,0,,,"ReplicationZookeeper.lockOtherRS is missing a ""return false"" when handling the KeeperException.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26808,,,,,Fri Nov 20 12:43:37 UTC 2015,,,,,,,,,,"0|i0hly7:",100818,,,,,,,,,,,,,,,,,,,,,"16/Dec/10 00:46;jdcryans;Committed.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If .META. offline between OPENING and OPENED, then wrong server location in .META. is possible",HBASE-3362,12493361,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,stack,stack,15/Dec/10 20:50,20/Nov/15 12:42,14/Jul/23 06:06,17/Dec/10 05:42,,,,,,,,,,,,0.90.0,,,,,,,0,,,"This is a good one.  It happened to me testing OOME in split logging.

* Balancer moves region to new location, regionservrer X.
* New location regionserver X successfully opens the region and then goes to update .META.
* At this point, the server carrying .META. crashes.
* Regionserver X is stuck waiting on .META. to come back online.  It takes so long master times out the region-in-transition
* Master assigns the region elsewhere to regionserver Y
* It opens successfully on regionserver Y and then it also parks waiting on .META. coming online
* .META. comes online
* The two servers X and Y race to update .META.

I saw case where server X edit went in after server Ys edit which means that lookups in .META. get the wrong server.  HBCK can detect this situation.

RegionServer X when it wakes up coreeclty notices that its lost control of the region but the damage is done -- where damage is .META. edit.

Chatting with Jon, he suggested that regionserver X should 'rollback' the .META. edit -- do explicit delete of what it added.  This would work I think but chatting more, I'll make a fix that keeps updating the zookeeper OPENING state while edit goes on in a separate thread.  Our continuous setting of OPENING will make it so region-in-transition does not timeout.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26807,Reviewed,,,,Fri Nov 20 12:42:30 UTC 2015,,,,,,,,,,"0|i0hlxr:",100816,,,,,,,,,,,,,,,,,,,,,"16/Dec/10 00:17;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1298/
-----------------------------------------------------------

Review request for hbase and Jonathan Gray.


Summary
-------

M src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
 Removed stale comments and TODOs.

 Added a 'version' datamenber, the znode edit version which we keep across open process.

 Refactored the setting of OPENING out into a method that is used in multiple places 
 now rather than repeat code.  Did this in new tickleOpening method.

 Added new PostOpenDeployTasksThread which we run to do the postOpenDeployTasks.
 While its running we update OPENING state if its running a while.


This addresses bug hbase-3362.
    http://issues.apache.org/jira/browse/hbase-3362


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java 1049707 

Diff: http://review.cloudera.org/r/1298/diff


Testing
-------

Ran it on my cluster. Seems to work as the old code did.


Thanks,

stack


;;;","16/Dec/10 08:17;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1298/#review2083
-----------------------------------------------------------

Ship it!


a few small comments.  i think the loop should change as described in my comment (busy loop w/ call to currentTimeMillis as i read it).  otherwise +1, good stuff.  we need some tickle util class soon :)


trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
<http://review.cloudera.org/r/1298/#comment6529>

    ""on this server"" should probably be left in comment to be clear what this is checking



trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
<http://review.cloudera.org/r/1298/#comment6530>

    We were not previously but we should probably log this condition



trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
<http://review.cloudera.org/r/1298/#comment6531>

    This is a busy wait loop?
    
    Should we add a wait/notify on something passed to the thread and w/ a timeout of the period?
    
    And then we should probably also have some kind of max timeout.  Even if minutes, there could be weird cluster state where the RS misses META availability but someone else might handle it properly, so max timeout might be good?



trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
<http://review.cloudera.org/r/1298/#comment6533>

    whitespace



trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
<http://review.cloudera.org/r/1298/#comment6532>

    maybe this should be warn.  i think i'd want to see it and also logging of stack trace (i don't see logging of it elsewhere)


- Jonathan



;;;","16/Dec/10 17:59;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-12-16 00:14:36, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java, line 96
bq.  > <http://review.cloudera.org/r/1298/diff/1/?file=18309#file18309line96>
bq.  >
bq.  >     We were not previously but we should probably log this condition

We do in the method?


bq.  On 2010-12-16 00:14:36, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java, line 173
bq.  > <http://review.cloudera.org/r/1298/diff/1/?file=18309#file18309line173>
bq.  >
bq.  >     This is a busy wait loop?
bq.  >     
bq.  >     Should we add a wait/notify on something passed to the thread and w/ a timeout of the period?
bq.  >     
bq.  >     And then we should probably also have some kind of max timeout.  Even if minutes, there could be weird cluster state where the RS misses META availability but someone else might handle it properly, so max timeout might be good?

I need to add a small sleep.  I'd rather do this than wait/notify.  t.isAlive should be enough.  Regards max timeout, I should add check if server is stopped ... and for max timeout, what you think?  Ten minutes?  Then abort?


bq.  On 2010-12-16 00:14:36, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java, line 238
bq.  > <http://review.cloudera.org/r/1298/diff/1/?file=18309#file18309line238>
bq.  >
bq.  >     maybe this should be warn.  i think i'd want to see it and also logging of stack trace (i don't see logging of it elsewhere)

For sure.


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1298/#review2083
-----------------------------------------------------------



;;;","16/Dec/10 18:03;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>


bq.  On 2010-12-16 00:14:36, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java, line 96
bq.  > <http://review.cloudera.org/r/1298/diff/1/?file=18309#file18309line96>
bq.  >
bq.  >     We were not previously but we should probably log this condition
bq.  
bq.  stack wrote:
bq.      We do in the method?

yeah you're right, thought that was only on the exception.  nvm.


bq.  On 2010-12-16 00:14:36, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java, line 173
bq.  > <http://review.cloudera.org/r/1298/diff/1/?file=18309#file18309line173>
bq.  >
bq.  >     This is a busy wait loop?
bq.  >     
bq.  >     Should we add a wait/notify on something passed to the thread and w/ a timeout of the period?
bq.  >     
bq.  >     And then we should probably also have some kind of max timeout.  Even if minutes, there could be weird cluster state where the RS misses META availability but someone else might handle it properly, so max timeout might be good?
bq.  
bq.  stack wrote:
bq.      I need to add a small sleep.  I'd rather do this than wait/notify.  t.isAlive should be enough.  Regards max timeout, I should add check if server is stopped ... and for max timeout, what you think?  Ten minutes?  Then abort?

I was thinking 5 minutes.

How long you going to sleep for?  That seems like an unideal way to do this.  I would prefer wait/notify and have timeout on wait be this 1/3 period, but small sleep could work.  If really small, we're in busy loop again.  If too big, we increase how long we have to wait.  This is on critical path of every single region open.

If we go down path of threads doing work, I don't see why we don't want to use wait/notify to let the blocked thread know when it's done.


- Jonathan


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1298/#review2083
-----------------------------------------------------------



;;;","16/Dec/10 18:17;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-12-16 00:14:36, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java, line 173
bq.  > <http://review.cloudera.org/r/1298/diff/1/?file=18309#file18309line173>
bq.  >
bq.  >     This is a busy wait loop?
bq.  >     
bq.  >     Should we add a wait/notify on something passed to the thread and w/ a timeout of the period?
bq.  >     
bq.  >     And then we should probably also have some kind of max timeout.  Even if minutes, there could be weird cluster state where the RS misses META availability but someone else might handle it properly, so max timeout might be good?
bq.  
bq.  stack wrote:
bq.      I need to add a small sleep.  I'd rather do this than wait/notify.  t.isAlive should be enough.  Regards max timeout, I should add check if server is stopped ... and for max timeout, what you think?  Ten minutes?  Then abort?
bq.  
bq.  Jonathan Gray wrote:
bq.      I was thinking 5 minutes.
bq.      
bq.      How long you going to sleep for?  That seems like an unideal way to do this.  I would prefer wait/notify and have timeout on wait be this 1/3 period, but small sleep could work.  If really small, we're in busy loop again.  If too big, we increase how long we have to wait.  This is on critical path of every single region open.
bq.      
bq.      If we go down path of threads doing work, I don't see why we don't want to use wait/notify to let the blocked thread know when it's done.

5 minute is not enough.  IIRC, it was > 5 minutes before the region came back online.  Let me see.

I want to avoid mother thread depending on daughter thread signaling it to stop... seems redundant when I'm watching the daughter with the isAlive already.

The sleep would be short.  1ms or so.  Normally we'd not trip into the sleep.  The operation will have compeleted before we have chance to sleep.  It'd only sleep when no progress can be made.

I'll add wait/notify for you to get this patch cleared past review, np.


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1298/#review2083
-----------------------------------------------------------



;;;","17/Dec/10 01:03;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1298/
-----------------------------------------------------------

(Updated 2010-12-16 17:01:04.757304)


Review request for hbase and Jonathan Gray.


Changes
-------

I implemented Jon's suggestions and then some.  Not pretty but works in my local and cluster testing.


Summary
-------

M src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
 Removed stale comments and TODOs.

 Added a 'version' datamenber, the znode edit version which we keep across open process.

 Refactored the setting of OPENING out into a method that is used in multiple places 
 now rather than repeat code.  Did this in new tickleOpening method.

 Added new PostOpenDeployTasksThread which we run to do the postOpenDeployTasks.
 While its running we update OPENING state if its running a while.


This addresses bug hbase-3362.
    http://issues.apache.org/jira/browse/hbase-3362


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java 1050086 

Diff: http://review.cloudera.org/r/1298/diff


Testing
-------

Ran it on my cluster. Seems to work as the old code did.


Thanks,

stack


;;;","17/Dec/10 01:15;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1298/#review2103
-----------------------------------------------------------

Ship it!


it's getting pretty crazy but this looks good.

it's unfortunate we have all these extra node transitioning methods inside this class.  this pattern of doing node transitions and tracking expected version is very common and we'll probably have more of it so we should look at doing some kind of generic abstraction for that pattern soon.

+1 for commit, thanks for the changes


trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
<http://review.cloudera.org/r/1298/#comment6561>

    typo 'initalizes' but good comment



trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
<http://review.cloudera.org/r/1298/#comment6562>

    interesting thing is... we only use this progressable if we do a log replay.  in that case, a region open is not really idempotent as we treat it here.
    
    outside scope of this jira but something to think about.


- Jonathan



;;;","17/Dec/10 05:42;stack;Committed branch and trunk.  Thanks for review Jon.  I tested it more up on cluster and seems to be doing at least what old stuff used to.  Yes to pulling out this code and generalizing it.  For instance guava SimpleTimeLimiter looks useful but has its own semantics and you pass Callable.  Might be worth going that route with a generalized soln.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReplicationLogCleaner is enabled by default in 0.90 -- causes NPE,HBASE-3360,12493258,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,stack,stack,15/Dec/10 00:35,20/Nov/15 12:42,14/Jul/23 06:06,15/Dec/10 19:48,,,,,,,,,,,,0.90.0,,,,,,,0,,,"{code}
2010-12-15 00:33:17,706 ERROR org.apache.hadoop.hbase.master.LogCleaner: Caught exception
java.lang.NullPointerException
        at org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner.isLogDeletable(ReplicationLogCleaner.java:59)
        at org.apache.hadoop.hbase.master.LogCleaner.chore(LogCleaner.java:138)
        at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
        at org.apache.hadoop.hbase.master.LogCleaner.run(LogCleaner.java:165)
{code}

Assigning J-D at his request.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/10 19:48;jdcryans;HBASE-3360.patch;https://issues.apache.org/jira/secure/attachment/12466341/HBASE-3360.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26806,Reviewed,,,,Fri Nov 20 12:42:41 UTC 2015,,,,,,,,,,"0|i0hlxj:",100815,,,,,,,,,,,,,,,,,,,,,"15/Dec/10 01:34;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1293/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Patch that removes ReplicationLogCleaner from hbase-default.xml and instead injects from the Replication class. There's also some cleanup on how HConstants are used.


This addresses bug HBASE-3360.
    http://issues.apache.org/jira/browse/HBASE-3360


Diffs
-----

  /trunk/src/main/java/org/apache/hadoop/hbase/HConstants.java 1049375 
  /trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1049375 
  /trunk/src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java 1049375 
  /trunk/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java 1049375 
  /trunk/src/main/resources/hbase-default.xml 1049375 

Diff: http://review.cloudera.org/r/1293/diff


Testing
-------


Thanks,

Jean-Daniel


;;;","15/Dec/10 18:34;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1293/#review2074
-----------------------------------------------------------

Ship it!


+1


/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.cloudera.org/r/1293/#comment6510>

    Don't need this (found by J-D reviewing this over my shoulder)



/trunk/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java
<http://review.cloudera.org/r/1293/#comment6511>

    Call this 'decoraateMasterConfiguration' or something other than instrument.


- stack



;;;","15/Dec/10 19:48;jdcryans;Patch I'm committing.;;;","15/Dec/10 19:48;jdcryans;Committed to branch and trunk, thanks for the review Stack.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogRoller not added as a WAL listener when replication is enabled,HBASE-3359,12493256,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,15/Dec/10 00:16,20/Nov/15 12:41,14/Jul/23 06:06,15/Dec/10 00:26,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Another thing I broke when porting to the new master, LogRoller is started but never added to HLog meaning that only hourly rolls work. Easy fix in HRS:

{code}
     this.hlogRoller = new LogRoller(this, this);
     listeners.add(this.hlogRoller);
     if (this.replicationHandler != null) {
-      listeners = new ArrayList<WALObserver>();
       // Replication handler is an implementation of WALActionsListener.
       listeners.add(this.replicationHandler);
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26805,Reviewed,,,,Fri Nov 20 12:41:06 UTC 2015,,,,,,,,,,"0|i0hlxb:",100814,,,,,,,,,,,,,,,,,,,,,"15/Dec/10 00:19;stack;+1;;;","15/Dec/10 00:26;jdcryans;Committed to branch and trunk, thanks for looking at it Stack.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Recovered replication queue wait on themselves when terminating,HBASE-3358,12493255,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,15/Dec/10 00:14,20/Nov/15 12:43,14/Jul/23 06:06,15/Dec/10 00:26,,,,,,,,,,,,0.90.0,,,,,,,0,,,"When I ported the replication code on the new master I broke a few things, and it seems that the threads created for recovered queues never properly terminate because they join on themselves. Easy fix in ReplicationSource:

{code}
-      this.terminate(""Finished recovering the queue"");
+      LOG.info(""Finished recovering the queue"");
+      this.running = false;
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26804,Reviewed,,,,Fri Nov 20 12:43:52 UTC 2015,,,,,,,,,,"0|i0hlx3:",100813,,,,,,,,,,,,,,,,,,,,,"15/Dec/10 00:20;stack;+1;;;","15/Dec/10 00:26;jdcryans;Committed to branch and trunk, thanks for looking at it Stack.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"lockID parameter not used in HRegion.get(get, lockid)",HBASE-3357,12493244,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,larsgeorge,larsgeorge,14/Dec/10 21:53,12/Jun/22 00:40,14/Jul/23 06:06,19/Jul/14 00:38,,,,,,,,,,,,,,,,,,,0,,,"Not sure what the intention is, but the lockId is never used in HRegion's

{code}
  public Result get(final Get get, final Integer lockid) throws IOException
{code}

If we are not locking on Get's then we can drop this parameter?",,apurtell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26803,,,,,Sat Jul 19 00:38:35 UTC 2014,,,,,,,,,,"0|i02dgn:",11782,,,,,,,,,,,,,,,,,,,,,"14/Dec/10 22:41;ryanobjc;we no longer require nor accept locks for gets.  Never did for scans.

Changing it requires a major protocol revision, so it has been punted, but this could go into 0.92.;;;","19/Jul/14 00:38;apurtell;Fixed since user level locks were removed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add more checks in replication if RS is stopped,HBASE-3356,12493237,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,14/Dec/10 19:51,20/Nov/15 12:40,14/Jul/23 06:06,14/Dec/10 21:15,,,,,,,,,,,,0.90.0,,,,,,,0,,,"In a few places in the replication code I abort the region server if I get ZK exceptions, which often happens when closing down a region server cleanly (wasn't the case before). It results in a bunch of exceptions being thrown for nothing, adding more checks should resolve this easily.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/10 20:19;jdcryans;HBASE-3356.patch;https://issues.apache.org/jira/secure/attachment/12466244/HBASE-3356.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26802,,,,,Fri Nov 20 12:40:51 UTC 2015,,,,,,,,,,"0|i0hlwv:",100812,,,,,,,,,,,,,,,,,,,,,"14/Dec/10 20:19;jdcryans;Here's the places that I found were throwing exceptions.;;;","14/Dec/10 21:15;jdcryans;Committed to branch and trunk.;;;","14/Dec/10 22:19;stack;+1 Patch adds check on stopped and J-D explained that his catch of SessionExpired is 'safe' because shutdown is being done elsewhere in code base.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stopping a stopped cluster leaks an HMaster,HBASE-3355,12493229,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,14/Dec/10 19:08,20/Nov/15 12:41,14/Jul/23 06:06,14/Dec/10 21:15,,,,,,,,,,,,0.90.0,,,,,,,0,,,"This is a very annoying bug, I have two clusters running on the same machine so I often stop the wrong one (both are for testing). When it happens, it leaves a HMaster running with this:

{noformat}

""main"" prio=10 tid=0x0000000041dd3800 nid=0x55d5 in Object.wait() [0x00007f3c7165a000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00007f3bac94f528> (a java.lang.Object)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:382)
	- locked <0x00007f3bac94f528> (a java.lang.Object)
	at org.apache.hadoop.hbase.client.HBaseAdmin.<init>(HBaseAdmin.java:90)
	at org.apache.hadoop.hbase.master.HMasterCommandLine.stopMaster(HMasterCommandLine.java:160)
	at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:104)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
	at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1058)
{noformat}

And if I happen to restart that cluster right away, IT KILLS IT!",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/10 19:42;jdcryans;HBASE-3355.patch;https://issues.apache.org/jira/secure/attachment/12466239/HBASE-3355.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26801,Reviewed,,,,Fri Nov 20 12:41:55 UTC 2015,,,,,,,,,,"0|i0hlwn:",100811,,,,,,,,,,,,,,,,,,,,,"14/Dec/10 19:42;jdcryans;This patch limits the retries to 1. Works better for me.;;;","14/Dec/10 20:20;stack;+1;;;","14/Dec/10 21:15;jdcryans;Committed to branch and trunk!;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
table.jsp doesn't handle entries in META without server info,HBASE-3353,12493153,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,tlipcon,tlipcon,tlipcon,14/Dec/10 03:35,20/Nov/15 12:42,14/Jul/23 06:06,14/Dec/10 03:58,0.90.0,,,,,,,,,,,0.90.0,,master,,,,,0,,,"I have a table where one of the rows in META is missing server info. table.jsp doesn't check for this case, so it throws an NPE, which is then dumped to the .out log rather than anywhere where someone might find it. We should catch this case.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/10 03:53;tlipcon;hbase-3353.txt;https://issues.apache.org/jira/secure/attachment/12466192/hbase-3353.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26800,Reviewed,,,,Fri Nov 20 12:42:52 UTC 2015,,,,,,,,,,"0|i0hlw7:",100809,,,,,,,,,,,,,,,,,,,,,"14/Dec/10 03:55;stack;+1;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
enabling a non-existent table from shell prints no error,HBASE-3352,12493140,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,tlipcon,tlipcon,14/Dec/10 00:16,20/Nov/15 12:41,14/Jul/23 06:06,14/Dec/10 03:40,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"hbase(main):001:0> enable 'testtable'
0 row(s) in 0.3120 seconds

Only thing is that I don't have a table called 'testtable'",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/10 03:38;stack;3352.txt;https://issues.apache.org/jira/secure/attachment/12466189/3352.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26799,,,,,Fri Nov 20 12:41:47 UTC 2015,,,,,,,,,,"0|i0hlvz:",100808,,,,,,,,,,,,,,,,,,,,,"14/Dec/10 03:38;stack;Small test that table exists added to enable/disable;;;","14/Dec/10 03:40;stack;Committed small patch to trunk and branch.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReplicationZookeeper goes to ZK every time a znode is modified,HBASE-3351,12493138,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,13/Dec/10 23:46,20/Nov/15 12:41,14/Jul/23 06:06,14/Dec/10 04:21,,,,,,,,,,,,0.90.0,,,,,,,0,,,"While debugging other issues, I found that ReplicationAdmin.ReplicationStatusTracker is doing a ZK request every time a znode changes. Also reading the replication state shouldn't go to ZK if we already maintain a local variable that's updated with the tracker.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/10 00:23;jdcryans;HBASE-3351_3326.patch;https://issues.apache.org/jira/secure/attachment/12466182/HBASE-3351_3326.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26798,Reviewed,,,,Fri Nov 20 12:41:00 UTC 2015,,,,,,,,,,"0|i0hlvr:",100807,,,,,,,,,,,,,,,,,,,,,"14/Dec/10 00:23;jdcryans;Attaching a patch that covers both this issue and HBASE-3326 (Replication state's znode should be created else it defaults to false) because they are very intertwined. 

In this patch I fix HBASE-3326 simply by creating the znode and then I add the better tracking of the state znode which had an impact in more user-side classes since they had to provide an Abortable in order to be able to start the tracker in any given situation (before the client wasn't tracking the state znode). In the end, I think this is much better.;;;","14/Dec/10 02:11;stack;+1 Looks reasonable to me J-D.;;;","14/Dec/10 04:21;jdcryans;Committed to branch and trunk, thanks for looking at it Stack.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't truncate/disable table that has rows in .META. that have empty info:regioninfo column,HBASE-3347,12493120,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,13/Dec/10 21:03,20/Nov/15 12:41,14/Jul/23 06:06,13/Dec/10 22:39,,,,,,,,,,,,0.90.0,,,,,,,0,,,"I somehow manufactured empty info:regioninfo cells in .META. -- still trying to figure how -- but trying to drop the table I get NPE

{code}
ERROR: java.lang.NullPointerException: null
Backtrace: org/apache/hadoop/hbase/util/Writables.java:75:in `getWritable'
           org/apache/hadoop/hbase/util/Writables.java:119:in `getHRegionInfo'
           org/apache/hadoop/hbase/client/HConnectionManager.java:505:in `processRow'
           org/apache/hadoop/hbase/client/MetaScanner.java:190:in `metaScan'
           org/apache/hadoop/hbase/client/MetaScanner.java:95:in `metaScan'
           org/apache/hadoop/hbase/client/MetaScanner.java:73:in `metaScan'
           org/apache/hadoop/hbase/client/HConnectionManager.java:530:in `getHTableDescriptor'
           org/apache/hadoop/hbase/client/HTable.java:320:in `getTableDescriptor'
           /home/stack/hbase/bin/../bin/../src/main/ruby/hbase/admin.rb:205:in `truncate'
           /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands/truncate.rb:33:in `command'
           /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands.rb:64:in `format_simple_command'
           /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands/truncate.rb:31:in `command'
           /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands.rb:31:in `command_safe'
           /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands.rb:70:in `translate_hbase_exceptions'
           /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands.rb:31:in `command_safe'
           /home/stack/hbase/bin/../bin/../src/main/ruby/shell.rb:106:in `command'
           (eval):2:in `truncate'
           (hbase):4:in `irb_binding'
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26796,,,,,Fri Nov 20 12:41:30 UTC 2015,,,,,,,,,,"0|i0hlv3:",100804,,,,,,,,,,,,,,,,,,,,,"13/Dec/10 22:39;stack;Committed to branch and trunk small patch;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master aborts after RPC to server that was shutting down,HBASE-3344,12493117,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,,tlipcon,tlipcon,13/Dec/10 20:27,20/Nov/15 12:42,14/Jul/23 06:06,04/Jan/11 00:12,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"I was doing a rolling restart during a bunch of splits happening, and the master aborted with the following:

2010-12-13 12:24:55,536 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region usertable,user1590589031,1291843166306.dbcbe21b3447c78560802962b87fd34f. (offlining)
2010-12-13 12:24:55,537 FATAL org.apache.hadoop.hbase.master.HMaster: Remote unexpected exception
java.io.IOException: Call to haus03.sf.cloudera.com/172.29.5.34:60020 failed on local exception: java.io.EOFException
        at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:788)
        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:757)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
        at $Proxy6.closeRegion(Unknown Source)
        at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:589)
        at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1085)
        at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1032)
        at org.apache.hadoop.hbase.master.AssignmentManager.balance(AssignmentManager.java:1791)
        at org.apache.hadoop.hbase.master.HMaster.balance(HMaster.java:688)
        at org.apache.hadoop.hbase.master.HMaster$1.chore(HMaster.java:579)
        at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
Caused by: java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:375)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:521)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:459)
2010-12-13 12:24:55,541 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/11 23:44;stack;3344.txt;https://issues.apache.org/jira/secure/attachment/12467378/3344.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26793,Reviewed,,,,Fri Nov 20 12:42:46 UTC 2015,,,,,,,,,,"0|i0hlun:",100802,,,,,,,,,,,,,,,,,,,,,"16/Dec/10 06:54;stack;Closing as duplicate of HBASE-3365.  Reopen Todd please if I got it wrong.;;;","31/Dec/10 02:34;tlipcon;Got this issue again with the same stack trace. 0.90 branch @r1052112;;;","31/Dec/10 04:46;stack;Odd.  For sure you have RC2 loaded?

sendRegionClose is on line #1091 in tip of 0.90 TRUNK (This is not RC2 but I dont think this has changed since RC'ing).

Also, there is this catch clause around the sendRegionClose:

{code}
1115     } catch (EOFException e) {
1116       LOG.info(""Server "" + server + "" returned "" + e.getMessage() + "" for "" +
1117         region.getEncodedName());
1118       // Presume retry or server will expire.
{code}

I wonder why its not triggering.  Maybe its wrapped in a RemoteException?

Good on you Todd.;;;","03/Jan/11 23:44;stack;This patch adds check of EOFE inside a RemoteExcepion. I don't think necessary since my thinking is that the running jar was old w/o fix that was already committed to catch EOFE (See above).

{code}
Index: src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java (revision 1054820)
+++ src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java (working copy)
@@ -1122,6 +1122,10 @@
         // Failed to close, so pass through and reassign
         LOG.debug(""Server "" + server + "" returned "" + ioe + "" for "" +
           region.getEncodedName());
+      } else if (ioe instanceof EOFException) {
+        // Failed to close, so pass through and reassign
+        LOG.debug(""Server "" + server + "" returned "" + ioe + "" for "" +
+          region.getEncodedName());
       } else {
         this.master.abort(""Remote unexpected exception"", ioe);
       }
{code};;;","04/Jan/11 00:06;tlipcon;+1 to the amend. See in the stack trace it says ""Remote unexpected exception"" so it was indeed cast as RemoteException. As discussed on IRC this seems to be a bug in the IPC stack - since it wasn't really a remote exception at all, but rather a transport layer one.;;;","04/Jan/11 00:12;stack;Thanks for review Todd and for fingering for sure the why.  Agree that its wrong that client-side RPC wraps the EOFE with a RemoteException.  Committed branch and trunk.;;;","04/Jan/11 04:02;hudson;Integrated in HBase-TRUNK #1701 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1701/])
    HBASE-3344 Master aborts after RPC to server that was shutting down
;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Server not shutting down after losing log lease,HBASE-3343,12493114,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,tlipcon,tlipcon,13/Dec/10 19:48,20/Nov/15 12:41,14/Jul/23 06:06,21/Dec/10 05:32,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"Ran into this bug testing 0.90rc2. I kill -STOPed a server, and then -CONT it after its logs had been split. It correctly decided it should abort, but got stuck during the shutdown process.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/10 05:32;stack;3343-v2.patch;https://issues.apache.org/jira/secure/attachment/12466681/3343-v2.patch","21/Dec/10 03:22;jdcryans;HBASE-3343.patch;https://issues.apache.org/jira/secure/attachment/12466678/HBASE-3343.patch","13/Dec/10 19:51;tlipcon;shutdown-logs.txt.bz2;https://issues.apache.org/jira/secure/attachment/12466164/shutdown-logs.txt.bz2","13/Dec/10 19:51;tlipcon;stuck-server.txt;https://issues.apache.org/jira/secure/attachment/12466165/stuck-server.txt",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26792,Reviewed,,,,Fri Nov 20 12:41:09 UTC 2015,,,,,,,,,,"0|i0hluf:",100801,,,,,,,,,,,,,,,,,,,,,"13/Dec/10 19:51;tlipcon;Attaching logs and jstack;;;","16/Dec/10 06:58;stack;Bringing into 0.90.  Looking at stacktrace, we're waiting on all our regions to be closed before we'll go down.  I've seen this before where a split would come in mid-close down messing up our shutdown process.  This looks like a spin on that theme.  Hopefully the logs will have what was missed.  Will take a look.;;;","21/Dec/10 01:12;jdcryans;The issue with TestMetaReaderEditor and what I hope is the same here is that we interrupt the OpenRegionHandler but don't really handle it inside that thread. Here's an example:

{noformat}
2010-12-20 16:41:53,327 DEBUG [RS_OPEN_REGION-h38.sfo.stumble.net,63068,1292892076786-2]
 handler.OpenRegionHandler(160): Interrupting thread Thread[PostOpenDeployTasks:3d9e86152e5e75258c19d64d4ddb1a4a,5,main]

2010-12-20 16:41:53,327 DEBUG [RS_OPEN_REGION-h38.sfo.stumble.net,63068,1292892076786-2]
 zookeeper.ZKAssign(660): regionserver:63068-0x12d065f78360002 Attempting to transition node 3d9e86152e5e75258c19d64d4ddb1a4a
 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED

2010-12-20 16:41:53,328 WARN  [PostOpenDeployTasks:3d9e86152e5e75258c19d64d4ddb1a4a]
 handler.OpenRegionHandler$PostOpenDeployTasksThread(195):
 Exception running postOpenDeployTasks; region=3d9e86152e5e75258c19d64d4ddb1a4a
java.nio.channels.ClosedByInterruptException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)
...
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
        at $Proxy12.get(Unknown Source)
        at org.apache.hadoop.hbase.catalog.MetaReader.readLocation(MetaReader.java:286)
        at org.apache.hadoop.hbase.catalog.MetaReader.readMetaLocation(MetaReader.java:262)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:279)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:322)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:362)
        at org.apache.hadoop.hbase.catalog.MetaEditor.updateRegionLocation(MetaEditor.java:146)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1331)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$PostOpenDeployTasksThread.run(OpenRegionHandler.java:192)
{noformat}

So the region stays opened and the region server will wait on it forever.;;;","21/Dec/10 02:04;jdcryans;Testing more I saw also a different error:

{noformat}
2010-12-20 18:01:54,211 ERROR [RS_OPEN_REGION-h38.sfo.stumble.net,50228,1292896899873-0]
 executor.EventHandler(154): Caught throwable while processing event M_RS_OPEN_REGION
java.lang.NullPointerException        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.updateMeta(OpenRegionHandler.java:166)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:98)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{noformat};;;","21/Dec/10 02:06;jdcryans;/me dumb, it's my own debug that NPEs. Move along.;;;","21/Dec/10 03:22;jdcryans;This patch closes 2 holes and fixes one NPE. We were racing between the main OpenRegionHandler thread and PostOpenDeployTasksThread after interrupting it. It now works reliably for me.;;;","21/Dec/10 05:20;stack;@jdcryans +1 on your patch though its addressing other than whats in this log; log is about an aborted server not going down.  Code looks like this in the abort processing:

{code}
....
 629     if (this.killed) {
 630       // Just skip out w/o closing regions.
 631     } else if (abortRequested) {
 632       if (this.fsOk) {
 633         closeAllRegions(abortRequested); // Don't leave any open file handles
 634         closeWAL(false);
 635       }
 636       LOG.info(""aborting server at: "" + this.serverInfo.getServerName());
 637     } else {
 638       closeAllRegions(abortRequested);
 639       closeWAL(true);
 640       closeAllScanners();
 641       LOG.info(""stopping server at: "" + this.serverInfo.getServerName());
 642     }
 643     // Interrupt catalog tracker here in case any regions being opened out in
 644     // handlers are stuck waiting on meta or root.
 645     if (this.catalogTracker != null) this.catalogTracker.stop();
 646     waitOnAllRegionsToClose();
....
{code}

... so if an abort is requested AND the fs is NOT OK, then we won't close regions... we just skip out.  ONLY, we then fall into waitOnAllRegionsToClose on line #646 above which will never complete because we didn't do close on regions.

I think simplest fix is adding this:

{code}
 646     if (this.fsOK) waitOnAllRegionsToClose();
{code}


Let me commit your patch and mine together.





;;;","21/Dec/10 05:27;stack;Oh, I forgot, here are the snippets from Todd's log that show fsok being set to false followed by the abort processing:

{code}
12596 2010-12-13 11:43:46,460 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server serverName=haus03.sf.cloudera.com,60020,1292269332749, load=(requests=894, regions=6, usedHeap=2039, maxHeap=8185): File System not available
12597 java.io.IOException: File system is not available
....
17936 2010-12-13 11:43:46,535 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: aborting server at: haus03.sf.cloudera.com,60020,1292269332749
{code};;;","21/Dec/10 05:32;stack;Here's what I applied to trunk and branch (applied it so it would build up on hudson over night.... we'll see how it is morning).;;;","21/Dec/10 05:32;stack;Committed to branch and trunk. ;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restore HBCK fix of unassignment and dupe assignment for new master,HBASE-3337,12493100,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,streamy,streamy,streamy,13/Dec/10 18:31,20/Nov/15 12:41,14/Jul/23 06:06,14/Dec/10 19:59,,,,,,,,,,,,0.90.0,0.92.0,master,util,,,,0,,,"HBCK fixing of region unassignment and duplicate assignment was broken with the move to the new master.

We've seen it happen doing testing of 0.90 RCs like that over in HBASE-3332.

Rather than the old ""clear everything out approach"" which relied on the BaseScanner, in the new master we should just manipulate unassigned ZK nodes and let the master handle the transition.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-3333,,,,,,,,,,,,,,,,,,"14/Dec/10 19:58;streamy;HBASE-3337-final.patch;https://issues.apache.org/jira/secure/attachment/12466242/HBASE-3337-final.patch","14/Dec/10 01:01;streamy;HBASE-3337-v2.patch;https://issues.apache.org/jira/secure/attachment/12466184/HBASE-3337-v2.patch","14/Dec/10 01:07;streamy;HBASE-3337-v3.patch;https://issues.apache.org/jira/secure/attachment/12466185/HBASE-3337-v3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26791,Reviewed,,,,Fri Nov 20 12:41:17 UTC 2015,,,,,,,,,,"0|i0hltb:",100796,,,,,,,,,,,,,,,,,,,,,"13/Dec/10 23:25;tlipcon;I just ran this on a cluster which had a large disabled table, and it kind of freaked out - made unassigned ZK nodes for every region, even though the region should be offline. Not certain if it's new due to this patch or if it already existed;;;","14/Dec/10 01:01;streamy;This is the patch from over in HBASE-3332 plus an additional fix to modernize disabled table handling in HBCK.  Currently it considers a table disabled if it's in DISABLED state.  It ignores other states like DISABLING (for now), not really sure if we should do something there but perhaps for HBCK, disabled and disabling should both be treated as equivalent (and thus, region should not be deployed).;;;","14/Dec/10 01:07;streamy;I think we should treat DISABLING as disabled because when table is DISABLING we do not expect it to be deployed and in HBCK should treat a region of a DISABLING table that is deployed as incorrect.;;;","14/Dec/10 04:12;stack;Whats this?  +  public static final String HBCK_CODE_NAME = ""HBCKsecret123"";?  Its ugly.  Can you make it uglier and more explanatory?  Something like 'SERVER_NAME_USED_WHEN_HBCK_IS_DOING_MOVE'.  

Your patch does not do the above suggested DISABLING?;;;","14/Dec/10 04:25;stack;You can change the server name on commit.  +1 on commit since it does doe DISABLING.;;;","14/Dec/10 19:58;streamy;Final patch which removes HBASE-3332 and changes hbck server name constant.;;;","14/Dec/10 19:59;streamy;Committed to branch and trunk.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regions stuck in transition after RS failure,HBASE-3332,12492967,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,streamy,tlipcon,tlipcon,10/Dec/10 19:01,20/Nov/15 12:41,14/Jul/23 06:06,14/Dec/10 23:34,0.90.0,,,,,,,,,,,0.90.0,0.92.0,,,,,,0,,,"Testing 0.90rc2 I ran into this issue. The test scenario was to kill -9 the server hosting ROOT and META, and before it had been detected, run ""balancer"" from the shell. After logs were split and regions were reassigned, I ended up with some regions stuck in transition.",,khemani,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/10 01:41;streamy;HBASE-3332-HBCKfix-v1.patch;https://issues.apache.org/jira/secure/attachment/12466110/HBASE-3332-HBCKfix-v1.patch","10/Dec/10 20:27;streamy;HBASE-3332-v1.patch;https://issues.apache.org/jira/secure/attachment/12466017/HBASE-3332-v1.patch","10/Dec/10 19:08;tlipcon;log.txt;https://issues.apache.org/jira/secure/attachment/12466012/log.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26789,Reviewed,,,,Fri Nov 20 12:41:06 UTC 2015,,,,,,,,,,"0|i0hlsf:",100792,,,,,,,,,,,,,,,,,,,,,"10/Dec/10 19:06;streamy;Can you post the master log;;;","10/Dec/10 19:07;streamy;I will try to add this test case into TestMasterFailover once I figure out exactly what happened because it doesn't currently test this triggering of the balancer before detection.;;;","10/Dec/10 19:08;tlipcon;Here's a log excerpt showing one of the regions that got stuck;;;","10/Dec/10 19:19;streamy;I think this is newly broken because we don't do anything with PENDING_CLOSE timeouts anymore.  We used to but in some other jira recently I think stack took it out and removed the unit test for it (I agreed at the time).  Thinking on this case now, it can certainly happen because of the logic in ServerShutdownHandler ignoring this region.

Rather than putting that back, I think there's a better way to deal with PENDING_CLOSE/CLOSING timeouts which should only happen in something like what you saw here.

Let me work on a patch.;;;","10/Dec/10 20:27;streamy;Previously ServerShutdownHandler would not handle any regions on the dead server if they were already in transition.  However, in the TimeoutMonitor, we will never force a region off of a server when it's PENDING_CLOSE or CLOSING because the expectation was we would handle it in server expiration.  But again, over there, we did nothing to all RIT.

Change makes it so we will force reassignment of RIT if PENDING_CLOSE or CLOSING.

Patch is not done yet, I need to make sure master won't screw something up and I want to add a test.  ;;;","10/Dec/10 21:00;tlipcon;Also I think we need to have some tool (even if it's a super advanced one rather than automagic) that could fix this situation, should we have another bug like this. Should I open another JIRA for that?;;;","10/Dec/10 21:09;streamy;I'm working on HBCK fix which will fix this issue (as it happened on the first master and on the backups).;;;","13/Dec/10 01:41;streamy;This patch is completely different from the other.  This is an HBCK fix for the issue.  It brings back the ability to fix unassignment/dupe-assignment (this is unassign obviously).;;;","13/Dec/10 18:21;tlipcon;Didn't look at the new hbck patch code, but just tried it on my broken cluster and it worked to revive it.;;;","14/Dec/10 04:26;stack;+1 Looks right.;;;","14/Dec/10 23:34;streamy;Committed v1 patch.  HBCK fix was committed under HBASE-3337.

Based on testing done with this patch applied, there may be more work to do, but issue found here is fixed.;;;","14/Dec/10 23:34;streamy;Committed on trunk and branch.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add publish of a snapshot to apache repo to our pom,HBASE-3330,12492907,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,10/Dec/10 05:19,20/Nov/15 12:44,14/Jul/23 06:06,04/Feb/11 23:13,,,,,,,,,,,,0.90.1,,,,,,,0,,,See here http://www.apache.org/dev/publishing-maven-artifacts.html#publish-snapshot,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26787,,,,,Fri Nov 20 12:44:06 UTC 2015,,,,,,,,,,"0|i0hlrz:",100790,,,,,,,,,,,,,,,,,,,,,"20/Dec/10 20:08;stack;INFRA-3281

But there is also INFRA-2461.  Maybe we have what we need already?  Let me look....;;;","21/Dec/10 17:10;stack;See http://www.apache.org/dev/publishing-maven-artifacts.html ""...yeah, try to deploy a snapshot, just to make sure everything works...""

INFRA hooked us up.;;;","22/Jan/11 19:41;stack;I've been working on this over last few days.  Its taking me a while.  Our 90minute builds don't help.;;;","28/Jan/11 16:24;stack;I opened https://issues.apache.org/jira/browse/INFRA-3398.  Its as though the 'staging repository' filter is not working and catching the release SNAPSHOT moving it aside to staging.;;;","04/Feb/11 23:13;stack;So, have been able to snapshot and actually release so closing this issue.

I had to RTFM to actually figure it.  The 'trick' for me was answering the questions put to you by the mvn release plugin properly, making sure it was using the actual branch, and finally, before doing the mvn release:perform, VERY IMPORTANT, hand edit the release.properties file that was put under HBASE_HOME by the previous step, release:perform.  You need to edit it to make it point at right locations in SVN.

This is the doc. to follow making a release:  http://www.apache.org/dev/publishing-maven-artifacts.html.

I'll doc. our hbase-particular deviations after I run this process a few more times.;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replication state's znode should be created else it defaults to false,HBASE-3326,12492869,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,09/Dec/10 19:06,20/Nov/15 12:40,14/Jul/23 06:06,14/Dec/10 04:21,,,,,,,,,,,,0.90.0,,,,,,,0,,,"When you start replication from a fresh new 0.90, the replication state isn't written so it defaults to false (eg nothing gets replicated). That's very confusing, should be created by the first thread that sees it.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26785,Reviewed,,,,Fri Nov 20 12:40:42 UTC 2015,,,,,,,,,,"0|i0hlrj:",100788,,,,,,,,,,,,,,,,,,,,,"14/Dec/10 04:21;jdcryans;Fixed in the context of HBASE-3351.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOME in master splitting logs,HBASE-3323,12492801,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,tlipcon,tlipcon,tlipcon,09/Dec/10 08:02,20/Nov/15 12:41,14/Jul/23 06:06,20/Dec/10 20:41,0.90.0,,,,,,,,,,,0.90.0,,master,,,,,0,,,"In testing a RS failure under heavy increment workload I ran into an OOME when the master was splitting the logs.

In this test case, I have exactly 136 bytes per log entry in all the logs, and the logs are all around 66-74MB). With a batch size of 3 logs, this means the master is loading about 500K-600K edits per log file. Each edit ends up creating 3 byte[] objects, the references for which are each 8 bytes of RAM, so we have 160 (136+8*3) bytes per edit used by the byte[]. For each edit we also allocate a bunch of other objects: one HLog$Entry, one WALEdit, one ArrayList, one LinkedList$Entry, one HLogKey, and one KeyValue. Overall this works out to 400 bytes of overhead per edit. So, with the default settings on this fairly average workload, the 1.5M log entries takes about 770MB of RAM. Since I had a few log files that were a bit larger (around 90MB) it exceeded 1GB of RAM and I got an OOME.

For one, the 400 bytes per edit overhead is pretty bad, and we could probably be a lot more efficient. For two, we should actually account this rather than simply having a configurable ""batch size"" in the master.

I think this is a blocker because I'm running with fairly default configs here and just killing one RS made the cluster fall over due to master OOME.",,larsfrancke,qwertymaniac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-3325,HBASE-1364,,,,,,,,,,,,"12/Dec/10 01:40;tlipcon;hbase-3323.4.txt;https://issues.apache.org/jira/secure/attachment/12466089/hbase-3323.4.txt","14/Dec/10 02:57;tlipcon;hbase-3323.5.txt;https://issues.apache.org/jira/secure/attachment/12466188/hbase-3323.5.txt","20/Dec/10 20:40;stack;hbase-3323.6.txt;https://issues.apache.org/jira/secure/attachment/12466657/hbase-3323.6.txt","11/Dec/10 01:32;tlipcon;hbase-3323.txt;https://issues.apache.org/jira/secure/attachment/12466034/hbase-3323.txt","11/Dec/10 00:48;tlipcon;hbase-3323.txt;https://issues.apache.org/jira/secure/attachment/12466032/hbase-3323.txt","10/Dec/10 07:49;tlipcon;hbase-3323.txt;https://issues.apache.org/jira/secure/attachment/12465969/hbase-3323.txt","09/Dec/10 08:33;tlipcon;sizes.png;https://issues.apache.org/jira/secure/attachment/12465886/sizes.png",,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26784,Reviewed,,,,Fri Nov 20 12:41:57 UTC 2015,,,,,,,,,,"0|i0hlrb:",100787,,,,,,,,,,,,,,,,,,,,,"09/Dec/10 19:43;clehene;
Here's the object distribution tlipcon mentioned:

{code}
The values of this map contain the 1.5M+ edits (in Entry objects) tlipcon mentioned

Map<byte[], LinkedList<Entry>> editsByRegion
      |                  |
      |                  |
      |                  |
(encodedRegionName)      |
      .                  |
      .                  |
      .                  |
      .                  |
      .                  |
      .                  | 
      .                  --- WalEdit edit
      .                  |      |
      .                  |      |
      .                  |      |
      .                  |      --- ArrayList<KeyValue> kvs
      .                  |                      |
      .                  |                      |
      .                  |                      |
      .                  |                      --- byte[] bytes
      .                  |                          
      .                  |
      .  ----------------------------------------------------------
      .  |               |                                        |
      .  |               |                                        |
      .  |               --- HLogKey key                          |
      .  |                    |                                   |
      .  |                    |                                   |
      .  |                    |                                   |
      .  |                    |                                   |
      . .| . . . . . . . . . .--- byte[] encodedRegionName        |
         |                    |                                   |
         |                    |                                   |
         |                    |                                   |
         |                    --- byte[] tableName                |
         |                                                        |
         |                                                        |
         | this is useless as we could have this in the map key   |
         ----------------------------------------------------------

{code}

The splitLog workflow loads all the edits in a map indexed by region, and then uses a thread pool to write them to per region directories.


As you can see from this diagram, each edit duplicates the tableName and the encodedRegionName (hence the 2 extra byte[]). 

*One simple, partial solution:*
We can reduce the memory footprint by putting the tableName in the map key with the encodedRegionName (it's free). This would leave us with a LinkedList of WalEdit objects (ArrayList + KeyValue + the actual info: byte[]). Of course this could be further compressed, but it might not be worth it (WalEdit has a replication scope as well IIUC). 
This is a partial solution since we still don't solve the case when we have too much data in the HLogs.


*A second solution/suggestion:*

We can change the split process a bit. Let me explain how HLogs are organized and how we split (please correct me if I'm wrong):

*Context:*
* Eeach region server has one HLog directory in HDFS (under /hbase/.logs)
* In each HRegionServer corresponding directory there's a bunch of HLog files. 
* There's a strict order of the HLog files within a region's dir and edits inside are ordered as well. 
* We read all the files in memory first because we need all the edits for a particular region and to respect the order of the edits. 
* Only after everything is read, we use a thread pool to distribute the log entries per regions. 

*Suggestion:*
We could read the files in parallel, and instead of writing a single file in the HRegion corresponding directory, we write one file for each HLog. This should keep all the edits in strict order. Then HRegionServer could safely load them in the same order and apply edits. 

While we read the files in parallel we don't have to read the entire content in memory: we can just read and write to the corresponding destination file. This should solve the memory footprint problem. 


I haven't spent too much time analyzing the second option; it might have been discussed in the past, so if I'm missing something let me know.


Cosmin
;;;","09/Dec/10 20:19;clehene;Talking with JD, we figured HBASE-1364 might be a solution as well.;;;","10/Dec/10 00:14;tlipcon;Hi Cosmin. I agree that the log split process can be fixed up a bit to use a smaller amount of memory. Fixing the data structure to get rid of all those extra objects is one thing that's pretty straightfowrard like you mentioned. As for how to change the split process itself, I think following a more typical producer/consumer model makes more sense. For example, something like this:

{noformat}
buffers = map<region name, arraylist<edit>>
for each log:
  for each edit:
    buf = map.get(region) [inserting a new arraylist if necessary]
    buf.add(edit)
    if buf length > some number of bytes:
      while workqueue length > some threshold: wait
      workqueue.add(buf)
      buffers.remove(region)
{noformat}

then we have a set of threads which pull chunks of edits off the work queue and write into the appropriate file (where the file handles are kept open).;;;","10/Dec/10 03:33;tlipcon;I'm going to take a leap and assume I still remember how to code and work on this one tonight.;;;","10/Dec/10 06:57;stack;Cosmin, on the 'One simple, partial solution', I played with changing the LinkedList from Entry to LinkedList of WALEdits and changing the Map Key to be a data structure of encodedRegionName plus tablename (I used HLogKey rather than add a new data structure and then made a special Comparator that equated HLogKeys that had same encodedRegionName).  All was going nicely till it came to dump out the per region recovered.edit files.  At this point I need the Entry.  A WALEdit will not do.  So it seems.  We need seqid at least (This could change when KV has seqid -- HBASE-2856 'TestAcidGuarantee broken on trunk').;;;","10/Dec/10 07:06;stack;The second option seems viable to me Cosmin.  HBASE-2727 made it so we could pick up multiple files from recovered.edits directory.  The files we write are named fro the seqid of the first edit so we can pick them up in order.;;;","10/Dec/10 07:49;tlipcon;Here's a patch which basically redoes the way log splitting happens. It needs to be commented up and I want to rename some things, but the basic architecture is this:

- Main thread reads logs in order and writes into a structure called EntrySink (I want to rename this to EntryBuffer or sometihng)
- EntrySink maintains some kind of approximate heap size (I don't think I calculated it quite right, but c'est la vie) and also takes care of managing a RegionEntryBuffer for each region key.
-- The RegionEntryBuffer just has a LinkedList of Entries right now, but it does size accounting, and I think we could change these to a fancier data structure for more efficient memory usage (eg a linked list of 10000-entry arrays)
- If the main thread tries to append into the EntrySink but the heap usage has hit a max threshold, it waits.

Meanwhile, there are N threads called WriterThread-n which do the following in a loop:
- poll the EntrySink to grab a RegionEntryBuffer
-- The EntrySink returns the one with the most outstanding edits (hope is to write larger sequential chunks if possible)
-- The EntrySink also keeps track of which regions already have some thread working on them, so we don't end up with out-of-order appends
- The EntrySink then drains the RegionEntryBuffer into the ""OutputSink"" which maintains the map from region key to WriterAndPath (bug in patch uploaded: this map needs to be synchronizedMap)
- Once the buffer is drained, it notifies the EntrySink that the memory is no longer in use (hence unblocking the producer thread)

In summary, it's a fairly standard producer-consumer pattern with some trickery to make a separate queue per region so as not to reorder edits.

As a non-scientific test I patched this into my cluster which was getting the OOME on master startup, and it not only started up fine, the log splits ran about 50% faster than they did before!

Known bug: the ""log N of M"" always says ""log 1 of M""

Thoughts?;;;","10/Dec/10 07:53;tlipcon;Oh, I should also note that you probably want to turn off DEBUG logs or comment them out before testing this on any real log. It's quite noisy at the moment. Also, it seems like the 128M buffer I hardcoded is plenty fine - the writers stayed completely ahead of the reader in the test I ran.

We should make a standalone benchmark for this algorithm too.;;;","10/Dec/10 23:08;stack;I'm part way through a review but have to leave.  So far it looks like less moving parts and cleaner overall.  Will finish up review tomorrow.   We could set the number of splits to 1 and ship the RC with that but at the moment, going by the other issues that need fixing, its looking like next week before new RC and that might be time to test this redo of splits. ;;;","11/Dec/10 00:48;tlipcon;Here's an updated patch, refactored some stuff a bit.

Also fixed up one of the test cases which was flaky in the old version - it used the presence of region directories to determine whether the split had started running, but those were created by generateHLogs, so it was only winning the race sometimes.;;;","11/Dec/10 00:56;tlipcon;er, there's a deadlock in this version of the patch :) but general structure should be good enough to review at this point;;;","11/Dec/10 01:32;tlipcon;Now certified deadlock free by jcarder;;;","11/Dec/10 18:42;stack;I think I pulled the right patch -- the latest one -- 62820 bytes in size (Todd, add a version to your patches?).  I was trying it and got this on the very end

{code}
2010-12-11 18:39:00,914 INFO org.apache.hadoop.hbase.util.FSUtils: Recovering file hdfs://sv2borg180:10000/hbase/.logs/sv2borg188,60020,1291841481545/sv2borg188%3A60020.1291993339759
2010-12-11 18:39:00,915 ERROR org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Error in log splitting write thread
java.util.ConcurrentModificationException
        at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:761)
        at java.util.LinkedList$ListItr.next(LinkedList.java:696)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:669)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:649)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:621)
{code};;;","11/Dec/10 19:34;stack;So, on restart, the split completed (this is ten servers whose logs were to be split; one split set was of 33 logs... all in a 1G Master heap).

The logging is profuse but I'm grand w/ that.

The below looks like a version of HBASE-2471... is that so?  If so, good stuff.

{code}
2010-12-11 18:35:43,243 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: This region's directory doesn't exist: hdfs://sv2borg180:10000/hbase/TestTable/1d4a17311a6bc9f7b34f121bf121f42b. It is very likely that it was already split so it's safe to discard those edits.
{code}

On the patch, why flip parameter order -- i.e. moving conf from end of list to start?  People want to know!

Whats this about?

{code}
+  public void internTableName(byte []tablename) {
+    assert Bytes.equals(tablename, this.tablename);
+    this.tablename = tablename;
+  }
{code}

We could call this method mutiple times?  And it'd be an error if tablename was different on an invocation?  The method is public w/o doc.  It has to be public because called from different package?

Sorry... still not done w/ review.  Back later.;;;","11/Dec/10 23:33;tlipcon;It seems in my eagerness to fix up the deadlock I ended up undersynchronizing in the latest patch. Oops! I'm going to write some standalone slower tests that actually split larger amounts of data, so we can get proper benchmarks and do a better job of finding these races.

Will address your other comments in the next version as well.;;;","12/Dec/10 01:40;tlipcon;Here's a new version including some new unit tests that set up a mock log reader and mock writers so as to test a lot of edits going through the splitter without actually taking any IO. This reliably reproduced the split bug Stack found - fixed that now.

Didn't get a chance to do a true benchmark yet.;;;","12/Dec/10 20:12;stack;I have a bunch of praise/+1s for changes made in this patch -- the refactoring of where params are passed to HLogSplitter, the new javadoc on the intern'ing methods, the overall less moving parts -- but will say no more on that other than the improvements are great.

Whats this for:

{code}
+  protected final Path rootDir;
+  protected final Path srcDir;
+  protected final Path oldLogDir;
+  protected final FileSystem fs;
+  protected final Configuration conf;
{code}

Is stuff protected rather than private for the subclassers -- the transactional hbasers?


Minor.... the below javadoc is now stale..

{code}
    * Create a new HLogSplitter using the given {@link Configuration} and the
    * <code>hbase.hlog.splitter.impl</code> property to derived the instance
    * class to use.
{code}

Address on commit?

Same for this stuff:

{code}
+   * @param oldLogDir  directory where processed (split) logs will be archived to
+   * @param fs FileSystem
+   * @param conf Configuration
+   * @throws IOException will throw if corrupted hlogs aren't tolerated
...
{code}

Its javadoc of params no longer present on this method.


Mistype '+        ""An HLogSplitter instance may only be used one"");'

Extremely minor comment, the below formatting will be destroyed when rendered by javadoc:

{code}
+   * through the logs to be split. For each log, we:
+   *   - Recover it (take and drop HDFS lease) to ensure no other process can write
+   *   - Read each edit (see {@link #parseHLog}
+   *   - Mark as ""processed"" or ""corrupt"" depending on outcome
{code}

(... but good documentation).

It would be sweeter if this were a percentage of heap rather than hard MB number --> maxHeapUsage .... but no biggie.  Can do in later issue.

So, it looks like we keep the order in which edits were written across the split process as best as I can tell.  We just append to the Entry List in RegionEntryBuffer.  That looks right.

(Reading this patch makes me reconsider asserts)

You iterate logWriters, a synchronized Map, a couple of times.  Is this safe at the time of iteration?

You keep the old format for naming edit files?  Naming them for the sequenceid of their first edit, it seems (you use getRegionSplitEditsPath -- not in the patch).

On the below

{code}
+    if (scopes != null) {
+      ret += ClassSize.TREEMAP;
+      ret += ClassSize.align(scopes.size() * ClassSize.MAP_ENTRY);
+      // TODO this isn't quite right, need help here
+    }
{code}

... maybe Jon can help -- but its fine for the moment I'd say.





;;;","14/Dec/10 02:55;tlipcon;bq. Is stuff protected rather than private for the subclassers - the transactional hbasers?

Yep, I made it protected since inheritors will need to access this stuff. Hopefully we can get rid of the subclasses in favor of log coprocessors some day.

bq. Minor.... the below javadoc is now stale..
bq. ...Its javadoc of params no longer present on this method.
bq. Mistype '+ ""An HLogSplitter instance may only be used one"");'
bq. Extremely minor comment, the below formatting will be destroyed when rendered by javadoc:

Fixed.

bq. So, it looks like we keep the order in which edits were written across the split process as best as I can tell. We just append to the Entry List in RegionEntryBuffer. That looks right.

Added verification to the testThreading test that makes sure the edits come in the right order

bq. You iterate logWriters, a synchronized Map, a couple of times. Is this safe at the time of iteration?

It was safe in the current usage, but I added synchronization on this map for getOutputCounts just in case someone starts to call it in a different context.

bq. You keep the old format for naming edit files? Naming them for the sequenceid of their first edit, it seems (you use getRegionSplitEditsPath - not in the patch).

Yep, left that as-is.
;;;","15/Dec/10 01:00;stack;I'm +1 on committing this patch.  I tried v5 on my little test cluster here killing RSs a few times.  It does the right thing as best as I can tell verified by rowcount of table subsequently to ensure all regions online after the killing.;;;","16/Dec/10 05:58;stack;I did some more testing and came across the following after split was done and HLogSplitter was moving to close all files:

{code}
2010-12-16 01:07:20,394 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Waiting for split writer threads to finish
2010-12-16 01:07:20,640 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Split writers finished
2010-12-16 01:07:20,650 ERROR org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Couldn't close log at hdfs://sv2borg180:10000/hbase/TestTable/02495f8b7cb6404cb7ea0521cc183d56/recovered.edits/0000000000000012854
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/TestTable/02495f8b7cb6404cb7ea0521cc183d56/recovered.edits/0000000000000012854 File does not exist. [Lease.  Holder: DFSClient_hb_m_sv2borg180:         
60000_1292460569453, pendingcreates: 178]
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1418)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1409)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:1464)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:1452)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.complete(NameNode.java:471)
    at sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:961)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:957)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:396)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:955)
    at org.apache.hadoop.ipc.Client.call(Client.java:740)
    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
    at $Proxy5.complete(Unknown Source)
    at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
    at $Proxy5.complete(Unknown Source)
    at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:3457)
    at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3381)
    at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61)
    at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86)
    at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:966)
    at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.close(SequenceFileLogWriter.java:138)
    at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.closeStreams(HLogSplitter.java:756)
    at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWritingAndClose(HLogSplitter.java:741)
    at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:291)
    at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:186)
    at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:194)
    at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:96)
    at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
2010-12-16 01:07:20,685 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://sv2borg180:10000/hbase/TestTable/03dbd921c75876d6bc3f86c10201fa93/recovered.edits/0000000000000014196 (wrote 12 edits in 470ms)
2010-12-16 01:07:20,719 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://sv2borg180:10000/hbase/TestTable/04f81e343d032d43946393636b2b4d2d/recovered.edits/0000000000000012928 (wrote 15 edits in 390ms)
2010-12-16 01:07:20,725 ERROR org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Couldn't close log at hdfs://sv2borg180:10000/hbase/TestTable/06d137bd176e2604761243d396c11b3a/recovered.edits/0000000000000012945
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/TestTable/06d137bd176e2604761243d396c11b3a/recovered.edits/0000000000000012945 File does not exist. [Lease.  Holder: DFSClient_hb_m_sv2borg180:         
60000_1292460569453, pendingcreates: 176]
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1418)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1409)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:1464)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:1452)
...
{code}

It then closes a bunch and fails one or two more until its done with all.

Eventually the split 'completes' and we start assigning out regions:

{code}
2010-12-16 01:07:32,581 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Reassigning 186 region(s) that sv2borg185,60020,1292460570976 was carrying (skipping 1 regions(s) that are already in transition)
{code}

;;;","16/Dec/10 06:03;tlipcon;Strange... can you grep the NN logs for that directory and try to figure it out? Usually that kind of error happens if someone deletes the file while you're writing it.;;;","17/Dec/10 06:32;stack;Ok. I've overwritten the logs from above.  I've tested more since and haven't run into the issue.  Will keep at it.   Will open new issue if I run into it.   I'd say go ahead still w/ commit of this.;;;","20/Dec/10 20:40;stack;Here is what I committed.  Its Todd's patch updated so it'd apply to branch and trunk.;;;","22/Dec/10 00:01;jdcryans;I think the patch is missing this, at least adding it back in HLogSplitter fixes TestHLogSplitting:

{code}
} catch (EOFException eof) {
  // truncated files are expected if a RS crashes (see HBASE-2643)
  LOG.info(""EOF from hlog "" + logPath + "".  continuing"");
  processedLogs.add(logPath);
{code};;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replication.join shouldn't clear the logs znode,HBASE-3321,12492786,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,jdcryans,jdcryans,jdcryans,09/Dec/10 01:29,20/Nov/15 12:43,14/Jul/23 06:06,13/Dec/10 23:54,,,,,,,,,,,,0.90.0,,,,,,,0,,,Replication.join calls zkHelper.deleteOwnRSZNode and it shouldn't since ReplicationSourceManager does it already with a check. Currently you can lose replication data if there where still stuff to replicate.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/10 19:09;jdcryans;HBASE-3321.patch;https://issues.apache.org/jira/secure/attachment/12465930/HBASE-3321.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26782,Reviewed,,,,Fri Nov 20 12:43:19 UTC 2015,,,,,,,,,,"0|i0hlqv:",100785,,,,,,,,,,,,,,,,,,,,,"09/Dec/10 19:09;jdcryans;Simple patch for the issue.;;;","10/Dec/10 06:02;stack;+1

Offline, J-D give me more context on this one-line remove.;;;","13/Dec/10 23:54;jdcryans;Committed to trunk and branch.;;;","23/Dec/10 04:31;hudson;Integrated in HBase-TRUNK #1697 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/])
    ;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split rollback leaves parent with writesEnabled=false,HBASE-3318,12492684,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,jdcryans,jdcryans,jdcryans,07/Dec/10 23:52,20/Nov/15 12:44,14/Jul/23 06:06,10/Dec/10 05:53,,,,,,,,,,,,0.90.0,,,,,,,0,,,"I saw a split rollback today, and it left the region in a state where it was able to take writes, but wasn't able to flush or compact. It's printing this message every few milliseconds:

{noformat}
NOT flushing memstore for region xxx., flushing=false, writesEnabled=false
{noformat}

I see why, writesEnabled is never set back in HRegion.initialize:

{code}
// See if region is meant to run read-only.
if (this.regionInfo.getTableDesc().isReadOnly()) {
  this.writestate.setReadOnly(true);
}
{code}

Instead it needs to pass isReadOnly into the setReadOnly method to work correctly.

I think it should go in 0.90.0 if there's a new RC.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/10 01:08;jdcryans;HBASE-3318.patch;https://issues.apache.org/jira/secure/attachment/12465765/HBASE-3318.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26781,,,,,Fri Nov 20 12:44:01 UTC 2015,,,,,,,,,,"0|i0hlqf:",100783,,,,,,,,,,,,,,,,,,,,,"08/Dec/10 01:08;jdcryans;Patch that fixes the issue. Very minor change.;;;","08/Dec/10 01:15;stack;+1

That looks like an issue we've  had for a long time;;;","10/Dec/10 04:55;stack;Bringing into 0.90.0.  Want to apply J-D?;;;","10/Dec/10 05:53;jdcryans;Looks like I forgot to close the issue when I committed. Since I already did it on trunk and branch and trunk, it's pretty much in 0.90.0 now. I'll edit the CHANGES.txt to reflect that in trunk.;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Javadoc and Throws Declaration for Bytes.incrementBytes() is Wrong,HBASE-3317,12492671,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,ekohlwey,ekohlwey,ekohlwey,07/Dec/10 22:09,20/Nov/15 12:40,14/Jul/23 06:06,07/Dec/10 22:17,,,,,,,,,,,,0.92.0,,,,,,,0,,,"The throws declaration for Bytes.incrementBytes() states that an IOException is thrown by the method, and javadocs suggest that this is expected if the byte array's size is larger than SIZEOF_LONG.

The code actually uses an IllegalArgumentException, which is probably more appropriate anyways. This should be changed to simplify the code that uses this method.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/10 22:11;ekohlwey;HBASE-3317.patch;https://issues.apache.org/jira/secure/attachment/12465743/HBASE-3317.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26780,Reviewed,,,,Fri Nov 20 12:40:41 UTC 2015,,,,,,,,,,"0|i0hlq7:",100782,,,,,,,,,,,,,,,,,,,,,"07/Dec/10 22:17;stack;Committed to TRUNK.  Thank you for the patch Ed.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add debug output for when balancer makes bad balance,HBASE-3315,12492560,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,07/Dec/10 00:00,20/Nov/15 12:43,14/Jul/23 06:06,07/Dec/10 00:59,,,,,,,,,,,,0.90.0,,,,,,,0,,,Balancer had assertions at end of the balanceCluster method.  These assertions trigger on occasion -- just did for me and did previously for j-d -- only there's no data to analyze when it fails.  Add logging data on balancer input and summary.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26779,,,,,Fri Nov 20 12:43:36 UTC 2015,,,,,,,,,,"0|i0hlpr:",100780,,,,,,,,,,,,,,,,,,,,,"07/Dec/10 00:57;stack;Here is what I'm committing..... (after testing on cluster).
{code}
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java      (revision 1042857)
+++ src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java      (working copy)
@@ -293,11 +293,20 @@

     long endTime = System.currentTimeMillis();

-    assert(regionidx == regionsToMove.size()): ""clusterState="" + clusterState +
-      "", regionidx="" + regionidx + "", regionsToMove="" + regionsToMove +
+    if (regionidx != regionsToMove.size() || neededRegions != 0) {
+      // Emit data so can diagnose how balancer went astray.
+      LOG.warn(""regionidx="" + regionidx + "", regionsToMove="" + regionsToMove.size() +
       "", numServers="" + numServers + "", serversOverloaded="" + serversOverloaded +
-      "", serversUnderloaded="" + serversUnderloaded;
-    assert(neededRegions == 0);
+      "", serversUnderloaded="" + serversUnderloaded);
+      StringBuilder sb = new StringBuilder();
+      for (Map.Entry<HServerInfo, List<HRegionInfo>> e: clusterState.entrySet()) {
+        if (sb.length() > 0) sb.append("", "");
+        sb.append(e.getKey().getServerName());
+        sb.append("" "");
+        sb.append(e.getValue().size());
+      }
+      LOG.warn(""Input "" + sb.toString());
+    }

     // All done!
     LOG.info(""Calculated a load balance in "" + (endTime-startTime) + ""ms. "" +
@@ -636,7 +645,7 @@
     public String toString() {
       return ""hri="" + this.hri.getRegionNameAsString() + "", src="" +
         (this.source == null? """": this.source.getServerName()) +
-        "", dest="" + this.dest.getServerName();
+        "", dest="" + (this.dest == null? """": this.dest.getServerName());
     }
   }
 }
{code};;;","07/Dec/10 00:59;stack;Committed logging to branch and trunk.  Will  have to open new issue to fix bad balancer calc when we trip over one.;;;","07/Dec/10 01:00;stack;Here's a bad balance Jon: 
2010-12-07 00:49:26,047 WARN org.apache.hadoop.hbase.master.LoadBalancer: Input sv2borg187,60020,1291682134874 1538, sv2borg183,60020,1291682134478 1392, sv2borg181,60020,1291682134477 1561, sv2borg189,60020,1291682134476 1557, sv2borg182,60020,1291682134474 1535, sv2borg185,60020,1291682134481 1553, sv2borg188,60020,1291682134874 1385, sv2borg186,60020,1291682134480 1542, sv2borg184,60020,1291682134480 1619;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[shell] 'move' is broken,HBASE-3314,12492535,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,06/Dec/10 19:39,20/Nov/15 12:40,14/Jul/23 06:06,06/Dec/10 20:19,,,,,,,,,,,,0.90.0,,,,,,,0,,,"I cannot use 'move' in the shell. throws a casting error. It's a small fix, and I will commit at the same time a change for assign and unassign to make it more like the rest of the code that already exists.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/10 20:19;jdcryans;HBASE-3314.patch;https://issues.apache.org/jira/secure/attachment/12465613/HBASE-3314.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26778,,,,,Fri Nov 20 12:40:58 UTC 2015,,,,,,,,,,"0|i0hlpj:",100779,,,,,,,,,,,,,,,,,,,,,"06/Dec/10 20:19;jdcryans;Patch I committed.;;;","06/Dec/10 20:19;jdcryans;Committed to trunk and branch.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table name isn't checked in isTableEnabled/isTableDisabled,HBASE-3313,12492528,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,yuzhihong@gmail.com,jdcryans,jdcryans,06/Dec/10 18:56,20/Nov/15 12:41,14/Jul/23 06:06,14/Mar/11 15:54,,,,,,,,,,,,0.92.0,,,,,,,0,,,"Currently when we enable or disable a table in the shell, since we don't verify the table name with HTableDescriptor.isLegalTableName in isTableEnabled and isTableDisabled, we get the following exception:

{noformat}
ERROR: java.lang.IllegalArgumentException: Invalid path string ""/jdcryans180/table/TestTable
"" caused by invalid charater @28
{noformat}

This is coming out of ZooKeeper. Instead we should check the table name ourselves in order to give a better feedback.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/11 13:58;yuzhihong@gmail.com;hbase-3313.txt;https://issues.apache.org/jira/secure/attachment/12473507/hbase-3313.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26777,Reviewed,,,,Fri Nov 20 12:41:01 UTC 2015,,,,,,,,,,"0|i0hlpb:",100778,,,,,,,,,,,,noob,,,,,,,,,"13/Mar/11 13:58;yuzhihong@gmail.com;Added HTableDescriptor.isLegalTableName() check for isTableDisabled() and isTableEnabled();;;","14/Mar/11 15:54;stack;Applied to TRUNK.  Thank you for the patch Ted.;;;","17/Mar/11 10:33;hudson;Integrated in HBase-TRUNK #1792 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1792/])
    ;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Addendum patch on HBASE-3298 broke buid,HBASE-3311,12492089,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,05/Dec/10 04:44,20/Nov/15 12:43,14/Jul/23 06:06,05/Dec/10 04:52,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Our addendum patch on HBASE-3298 removed reassign if an unassign failed; it makes no sense doing an assign on failed unassign -- it makes for double-assign (at least, reasoning through the possible failures, i can't find case where we'd want to assign on failed unassign).  Well, TestMasterFailover depended on this behavior.  Its the test that exercises all of this zk stuff.  It tries its best to manufacture messy scenarios.  One scenario adds a PENDING_CLOSE but it doesn't have an associated server.  A null server was returning a 'false' when we tried to do the close rpc which would fall into the since removed reassign code -- which would pick up a server, and the test would go on to succeed.

Removing reassign on unassign broke this.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/10 04:49;stack;3311.txt;https://issues.apache.org/jira/secure/attachment/12465326/3311.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26775,,,,,Fri Nov 20 12:43:53 UTC 2015,,,,,,,,,,"0|i0hlov:",100776,,,,,,,,,,,,,,,,,,,,,"05/Dec/10 04:49;stack;Here's a patch that comments out the PENDING_CLOSE messing in TestMasterFailover.  As is, it makes no sense.  Patch also adds more assertions to TestMasterFailover -- e.g. after wait on RIT, verify RIT is actually empty -- and changes name of the isInitialized data member to initialized.  It will also catch null data passed to handleRegion in AM and I improved comments around the catches in AM#unassign enunciating presumptions around how the recovery of a failed rpc close is expected to happen.

;;;","05/Dec/10 04:52;stack;Ok I applied this.  Let hudson say if its right or not.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failing creating/altering table with compression agrument from the HBase shell,HBASE-3310,12492077,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,iranitov,iranitov,iranitov,04/Dec/10 22:08,20/Nov/15 12:43,14/Jul/23 06:06,06/Dec/10 18:30,,,,,,,,,,,,0.90.0,,shell,,,,,0,,,"HColumnDescriptor setCompressionType takes Compression.Algorithm and not String

hbase(main):007:0> create 't1', { NAME => 'f', COMPRESSION => 'lzo'}

ERROR: cannot convert instance of class org.jruby.RubyString to class org.apache.hadoop.hbase.io.hfile.Compression$Algorithm

",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/10 22:12;iranitov;HBASE-3310.patch;https://issues.apache.org/jira/secure/attachment/12465314/HBASE-3310.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26774,Reviewed,,,,Fri Nov 20 12:43:13 UTC 2015,,,,,,,,,,"0|i0hlon:",100775,,,,,,,,,,,,,,,,,,,,,"04/Dec/10 22:12;iranitov;Here is a patch.;;;","06/Dec/10 18:30;stack;I tried it.  Its broke.  Igor's patch fixed it for me.  Committed branch and trunk.  Thanks for the patch Igor.;;;","06/Dec/10 18:32;stack;I added you as contributor Igor and assigned you this issue.  Thanks.;;;","07/Dec/10 19:44;stack;Looks like I only applied to branch.  Just applied to TRUNK too.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
""" Not running balancer because dead regionserver processing"" is a lie",HBASE-3309,12492050,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,04/Dec/10 00:23,20/Nov/15 12:41,14/Jul/23 06:06,04/Dec/10 00:52,,,,,,,,,,,,0.90.0,,,,,,,0,,,"When running the balancer I see the message:

{noformat}
 Not running balancer because dead regionserver processing
{noformat}

But that's not true, it does run, because the check is wrong:

{noformat}
if (!this.serverManager.areDeadServersInProgress()) {
  LOG.debug(""Not running balancer because dead regionserver processing"");
}
{noformat}

Also it doesn't return false like it should.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/10 00:47;jdcryans;HBASE-3309.patch;https://issues.apache.org/jira/secure/attachment/12465297/HBASE-3309.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26773,Reviewed,,,,Fri Nov 20 12:41:22 UTC 2015,,,,,,,,,,"0|i0hlof:",100774,,,,,,,,,,,,,,,,,,,,,"04/Dec/10 00:24;stack;Thats broke.;;;","04/Dec/10 00:47;jdcryans;Reversed the check and adds a return false.;;;","04/Dec/10 00:47;stack;+1;;;","04/Dec/10 00:52;jdcryans;Committed to branch and trunk.;;;","05/Dec/10 04:25;streamy;My bad.  Thanks JD!;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get spurious master fails during bootup,HBASE-3304,12491951,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,ryanobjc,ryanobjc,ryanobjc,03/Dec/10 01:34,20/Nov/15 12:40,14/Jul/23 06:06,03/Dec/10 05:39,,,,,,,,,,,,0.90.0,,,,,,,0,,,"the log says:


2010-12-01 20:42:21,115 WARN
org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation:
Remove exception connecting to RS
org.apache.hadoop.ipc.RemoteException:
org.apache.hadoop.hbase.ipc.ServerNotRunningException: Server is not
running yet
       at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1035)

       at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:753)
       at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
       at $Proxy6.getProtocolVersion(Unknown Source)
       at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:419)
       at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:393)
       at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:444)
       at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:349)
       at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:953)
       at org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:384)
       at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForRootServerConnection(CatalogTracker.java:210)
       at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRootRegionLocation(CatalogTracker.java:453)
       at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:421)
       at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:379)
       at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:274)
2010-12-01 20:42:21,118 FATAL org.apache.hadoop.hbase.master.HMaster:
Unhandled exception. Starting shutdown.
org.apache.hadoop.hbase.ipc.ServerNotRunningException:
org.apache.hadoop.hbase.ipc.ServerNotRunningException: Server is not
running yet
       at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1035)

       at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
       at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
       at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
       at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
       at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:96)
       at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:959)
       at org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:384)
       at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForRootServerConnection(CatalogTracker.java:210)
       at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRootRegionLocation(CatalogTracker.java:453)
       at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:421)
       at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:379)
       at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:274)
2010-12-01 20:42:21,119 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
2010-12-01 20:42:21,119 DEBUG org.apache.hadoop.hbase.master.HMaster:
Stopping service threads


then the master exits.  the cluster doesn't start.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/10 05:28;stack;3304-v2.txt;https://issues.apache.org/jira/secure/attachment/12465213/3304-v2.txt","03/Dec/10 01:35;ryanobjc;hbase-3304.txt;https://issues.apache.org/jira/secure/attachment/12465199/hbase-3304.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26772,Reviewed,,,,Fri Nov 20 12:40:55 UTC 2015,,,,,,,,,,"0|i0hlnj:",100770,,,,,,,,,,,,,,,,,,,,,"03/Dec/10 01:35;ryanobjc;appears we are initializing these in the wrong order.;;;","03/Dec/10 01:50;jdcryans;I'm not at ease with this patch, what seems to happen is that the CatalogTracker is trying to talk to the old RS but it's IPC server isn't started yet. Then we don't handle the thrown exception. Instead we could just retry... it seems less risky than playing in HBaseServer although I could be totally wrong.;;;","03/Dec/10 04:39;stack;@Ryan Was it easy to repro? And w/ this change it goes away?;;;","03/Dec/10 05:12;stack;Now I think on it, I think it intentional that the ordering was as committed.  Here's where it was changed:

{code}
------------------------------------------------------------------------
r1032812 | rawson | 2010-11-08 18:02:27 -0800 (Mon, 08 Nov 2010) | 3 lines

HBASE-3141  Master RPC server needs to be started before an RS can check in
{code}

Retry seems like the thing to add here?;;;","03/Dec/10 05:28;stack;Exception is when we are verifying the root location at startup to see if we should reassign root.  When we go to do that, we get ServerNotRunningException... so root can't have been assigned.  Therefore, catch this exception and return null for root not assigned... and higher up the assignment will be dealt with.;;;","03/Dec/10 05:29;jdcryans;+1 on latest patch if it passes unit tests.;;;","03/Dec/10 05:39;stack;OK.  I applied my patch to trunk and 0.90.  Lets see if it fixes Ryans's issue.  Ryan, you can bonk me on head if this fix turns out to be way wrong.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Treat java.net.SocketTimeoutException same as ConnectException assigning/unassigning regions.,HBASE-3301,12491851,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,02/Dec/10 05:56,20/Nov/15 12:40,14/Jul/23 06:06,02/Dec/10 19:05,,,,,,,,,,,,0.90.0,,,,,,,0,,,I saw java.net.SocketTimeoutException on my cluster this evening.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/10 05:57;stack;sockettimeout.txt;https://issues.apache.org/jira/secure/attachment/12465105/sockettimeout.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26771,Reviewed,,,,Fri Nov 20 12:40:47 UTC 2015,,,,,,,,,,"0|i0hln3:",100768,,,,,,,,,,,,,,,,,,,,,"02/Dec/10 17:50;jdcryans;+1;;;","02/Dec/10 17:50;streamy;+1;;;","02/Dec/10 19:05;stack;Thanks for reviews lads.  Committed trunk and branch.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If failed open, we don't output the IOE",HBASE-3299,12491831,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,01/Dec/10 22:55,20/Nov/15 12:41,14/Jul/23 06:06,01/Dec/10 23:00,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Jon found this one.... Here's fix:

{code}
Index: src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java   (revision 1040359)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java   (working copy)
@@ -111,8 +111,8 @@
           }
         });
     } catch (IOException e) {
-      LOG.error(""IOException instantiating region for "" + regionInfo +
-        ""; resetting state of transition node from OPENING to OFFLINE"");
+      LOG.error(""Failed open of "" + regionInfo +
+        ""; resetting state of transition node from OPENING to OFFLINE"", e);
       try {
         // TODO: We should rely on the master timing out OPENING instead of this
         // TODO: What if this was a split open?  The RS made the OFFLINE
@@ -216,4 +216,4 @@
     }
     return openingVersion;
   }
-}
\ No newline at end of file
+}
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26770,,,,,Fri Nov 20 12:41:41 UTC 2015,,,,,,,,,,"0|i0hlmv:",100767,,,,,,,,,,,,,,,,,,,,,"01/Dec/10 22:58;streamy;+1;;;","01/Dec/10 23:00;stack;Committed trunk and branch.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regionserver can close during a split causing double assignment,HBASE-3298,12491817,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,ryanobjc,ryanobjc,ryanobjc,01/Dec/10 21:38,20/Nov/15 12:43,14/Jul/23 06:06,03/Dec/10 22:54,,,,,,,,,,,,0.90.0,,,,,,,0,,,"A regionserver got a close message during a split, which seemed to cause the split to fail, and also caused a double assignment and orphaned region.

The log:

2010-11-30 18:29:24,310 INFO org.apache.hadoop.hbase.regionserver.HRegion: completed compaction on region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0
f01. after 40sec
2010-11-30 18:29:24,310 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Starting split of region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd
5f0f01.
2010-11-30 18:29:24,312 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.: disabling compac
tions & flushes
2010-11-30 18:29:24,312 INFO org.apache.hadoop.hbase.regionserver.HRegion: Running close preflush of usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.
2010-11-30 18:29:24,312 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01
., current region memstore size 256.1m
2010-11-30 18:29:24,312 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting, commencing flushing stores
2010-11-30 18:29:24,700 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd
5f0f01., flushing=false, writesEnabled=false
2010-11-30 18:29:27,291 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.
2010-11-30 18:29:27,291 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd
5f0f01., flushing=false, writesEnabled=false
2010-11-30 18:29:27,961 INFO org.apache.hadoop.hbase.regionserver.Store: Renaming flushed file at hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01
/.tmp/1673662210031989562 to hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/1156800948892817282
2010-11-30 18:29:27,966 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/1156800948892
817282, entries=1157300, sequenceid=14819, memsize=256.1m, filesize=59.4m
2010-11-30 18:29:27,966 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f
01. 'IPC Server handler 1 on 60020'
2010-11-30 18:29:27,966 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 0 on 60020'
2010-11-30 18:29:27,967 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 9 on 60020'
2010-11-30 18:29:27,967 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 4 on 60020'
2010-11-30 18:29:27,967 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 8 on 60020'
2010-11-30 18:29:27,967 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 5 on 60020'
2010-11-30 18:29:27,966 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 6 on 60020'
2010-11-30 18:29:27,966 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~256.1m for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. in 3654ms, sequenceid=14819, compaction requested=true
2010-11-30 18:29:27,967 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 7 on 60020'
2010-11-30 18:29:27,967 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 3 on 60020'
2010-11-30 18:29:27,967 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 2 on 60020'
2010-11-30 18:29:28,005 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received close region: usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.
2010-11-30 18:29:28,011 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Processing close of usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.
2010-11-30 18:29:28,012 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x2c765e0da906eb Creating unassigned node for 6e260cb69fda466a97f650debd5f0f01 in a CLOSING state
2010-11-30 18:29:28,074 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates disabled for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.
2010-11-30 18:29:28,074 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01., current region memstore size 4.8m
2010-11-30 18:29:28,074 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting, commencing flushing stores
2010-11-30 18:29:28,079 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.: disabling compactions & flushes
2010-11-30 18:29:28,332 INFO org.apache.hadoop.hbase.regionserver.Store: Renaming flushed file at hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/.tmp/1412284505922212951 to hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/3835216797592840515
2010-11-30 18:29:28,345 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/3835216797592840515, entries=21600, sequenceid=14835, memsize=4.8m, filesize=1.1m
2010-11-30 18:29:28,365 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~4.8m for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. in 291ms, sequenceid=14835, compaction requested=true
2010-11-30 18:29:28,381 DEBUG org.apache.hadoop.hbase.regionserver.Store: closed data
2010-11-30 18:29:28,381 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.
2010-11-30 18:29:28,382 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x2c765e0da906eb Attempting to transition node 6e260cb69fda466a97f650debd5f0f01 from RS_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2010-11-30 18:29:28,466 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x2c765e0da906eb Successfully transitioned node 6e260cb69fda466a97f650debd5f0f01 from RS_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2010-11-30 18:29:28,466 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Closed region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.
2010-11-30 18:29:29,000 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received close region: usertable,user1476676931,1291170472014.829c72aa4c47a31bd71b735da7e33883.
2010-11-30 18:29:29,001 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Processing close of usertable,user1476676931,1291170472014.829c72aa4c47a31bd71b735da7e33883.
2010-11-30 18:29:29,013 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x2c765e0da906eb Creating unassigned node for 829c72aa4c47a31bd71b735da7e33883 in a CLOSING state
2010-11-30 18:29:29,079 INFO org.apache.hadoop.hbase.regionserver.CompactSplitThread: Running rollback of failed split of usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.; org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/splits/019cf676872050ee05c15a6a37655e8a/data/5976204784865149447.6e260cb69fda466a97f650debd5f0f01 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sv4borg226,60020,1291168733786_1291168755622, pendingcreates: 1]
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1378)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1369)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:1424)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:1412)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.complete(NameNode.java:491)
        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:512)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:968)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:964)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:962)

2010-11-30 18:29:29,107 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing usertable,user1476676931,1291170472014.829c72aa4c47a31bd71b735da7e33883.: disabling compactions & flushes
2010-11-30 18:29:29,107 INFO org.apache.hadoop.hbase.regionserver.HRegion: Running close preflush of usertable,user1476676931,1291170472014.829c72aa4c47a31bd71b735da7e33883.
2010-11-30 18:29:29,108 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for usertable,user1476676931,1291170472014.829c72aa4c47a31bd71b735da7e33883., current region memstore size 63.4m
2010-11-30 18:29:29,108 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting, commencing flushing stores
2010-11-30 18:29:29,118 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/1156800948892817282, isReference=false, isBulkLoadResult=false, seqid=14819, majorCompaction=false
2010-11-30 18:29:29,163 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/1485319185163827758, isReference=false, isBulkLoadResult=false, seqid=12873, majorCompaction=false
2010-11-30 18:29:29,179 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/2866498733799966821, isReference=false, isBulkLoadResult=false, seqid=12619, majorCompaction=false
2010-11-30 18:29:29,219 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/3835216797592840515, isReference=false, isBulkLoadResult=false, seqid=14835, majorCompaction=false
2010-11-30 18:29:29,237 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/444462617223715350, isReference=false, isBulkLoadResult=false, seqid=12005, majorCompaction=false
2010-11-30 18:29:29,247 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/5536858845826091476, isReference=false, isBulkLoadResult=false, seqid=13173, majorCompaction=false
2010-11-30 18:29:29,254 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/5976204784865149447, isReference=false, isBulkLoadResult=false, seqid=13427, majorCompaction=false
2010-11-30 18:29:29,275 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/8614929289191582955, isReference=false, isBulkLoadResult=false, seqid=6111, majorCompaction=false
2010-11-30 18:29:29,281 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/9150811729171627958, isReference=false, isBulkLoadResult=false, seqid=12265, majorCompaction=false
2010-11-30 18:29:29,283 INFO org.apache.hadoop.hbase.regionserver.HRegion: Onlined usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.; next sequenceid=14836
2010-11-30 18:29:29,284 INFO org.apache.hadoop.hbase.regionserver.CompactSplitThread: Successful rollback of failed split of usertable,user877196232,1291170014467.6e260cb69f
da466a97f650debd5f0f01.
2010-11-30 18:29:29,284 INFO org.apache.hadoop.hbase.regionserver.HRegion: NOT compacting region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.: compacting=false, writesEnabled=false


",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/10 00:07;stack;3298-addendum.txt;https://issues.apache.org/jira/secure/attachment/12465293/3298-addendum.txt","03/Dec/10 22:49;stack;3298-v5.txt;https://issues.apache.org/jira/secure/attachment/12465283/3298-v5.txt","02/Dec/10 21:39;ryanobjc;HBASE-3298-2.txt;https://issues.apache.org/jira/secure/attachment/12465171/HBASE-3298-2.txt","03/Dec/10 01:12;ryanobjc;HBASE-3298-3.txt;https://issues.apache.org/jira/secure/attachment/12465194/HBASE-3298-3.txt","03/Dec/10 02:52;ryanobjc;HBASE-3298-4.txt;https://issues.apache.org/jira/secure/attachment/12465201/HBASE-3298-4.txt","02/Dec/10 02:59;ryanobjc;HBASE-3298.txt;https://issues.apache.org/jira/secure/attachment/12465100/HBASE-3298.txt","02/Dec/10 05:09;stack;am.txt;https://issues.apache.org/jira/secure/attachment/12465104/am.txt","02/Dec/10 05:01;stack;am.txt;https://issues.apache.org/jira/secure/attachment/12465103/am.txt",,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26769,Reviewed,,,,Fri Nov 20 12:43:07 UTC 2015,,,,,,,,,,"0|i0d4en:",74490,,,,,,,,,,,,,,,,,,,,,"01/Dec/10 22:12;jdcryans;Just hit the same issue, I think the title of this jira would be more descriptive if it said ""Regionserver can close a region while it splits causing double assignment"".;;;","01/Dec/10 22:17;jdcryans;I tried restarting and I can't enable nor disable the table completely... former enabled a couple of regions but then times out, and latter returns immediately without actually disabling. ;;;","02/Dec/10 02:59;ryanobjc;here is a proposed fix, with 2 parts:

- master: remove the assignment plan when we offline regions!
- regionserver: add a new sync block to prevent 2 threads from closing a region _at the same time_.

;;;","02/Dec/10 05:01;stack;So, looking at your patch I noticed that access to this.regionPlans is ""mostly"" synchronized.  This attached file make all accesses synchronized (including the one you add in your patch).

And on your adding an object to lock against doing close, why not just synchronize the close method?

And even so, do we need to do more?  The lock will stop more than one thread running close but it looks like if split going on, the balancer initiated close will run to completion -- it'll just be that the splitter closed the region rather than the CloseRegionHandler.  Is that OK?  I see that zk will be updated if the handler runs to completion.  Is that what we want?  Am I missing something?

;;;","02/Dec/10 05:06;ryanobjc;yeah I might have been too hasty on the regionPlans stuff, that needs a bit more work.

I originally wanted to use synchronized/the 'this' monitor object, but it's being used to halt and signal threads for the resource check.  So I need a new lock, but a simple one.

Looking in the split transaction code, during the close() we will end up with a null list of store files and we'll abort the split transaction at that point.  That seems to be the right thing to do.  I will look a bit more carefully to see what else needs to be done, but without adding a lot of split specific synchronization code this seemed the cleanest patch.  It will also help us in the future if more than 1 thread attempts to close regions for any reason.;;;","02/Dec/10 05:09;stack;So, looking at your patch I noticed that access to this.regionPlans is ""mostly"" synchronized.  This attached file make all accesses synchronized (including the one you add in your patch).

And on your adding an object to lock against doing close, why not just synchronize the close method?

And even so, do we need to do more?  The lock will stop more than one thread running close but it looks like if split going on, the balancer initiated close will run to completion -- it'll just be that the splitter closed the region rather than the CloseRegionHandler.  Is that OK?  I see that zk will be updated if the handler runs to completion.  Is that what we want?  Am I missing something?

;;;","02/Dec/10 05:16;stack;bq. yeah I might have been too hasty on the regionPlans stuff, that needs a bit more work.

I think the am.txt patch should be enough.

bq. Looking in the split transaction code, during the close() we will end up with a null list of store files and we'll abort the split transaction at that point

Abort of a split will cause reopen of the parent region.  You probably don't want that? (Split rollback tries to restore the pre-split condition).;;;","02/Dec/10 21:39;ryanobjc;wip;;;","02/Dec/10 22:47;stack;I just ran into this.

I saw:

{code}
2010-12-02 21:56:57,256 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting compaction on region TestTable,0045408302,1291326972892.218811f604f0b6822aae593939aef871.
...
2010-12-02 21:56:57,398 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction requested for TestTable,0045408302,1291326972892.218811f604f0b6822aae593939aef871. because regionserver60020.cacheFlusher; priority=-1, compaction queue size=28
...
{code}

then the balancer close came in....

{code}
2010-12-02 21:56:58,610 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received close region: TestTable,0045408302,1291326972892.218811f604f0b6822aae593939aef871.
{code}

The above log is just our addition of the close to the eventhandler... not actual running of the close.

The above cited compaction finished and we then moved to split the region.  The split queued a flush.

At this time the CloseRegionHandler started up -- and completed.

Because balancer thinks the close ran successfully... it went and assigned the region elsewhere.... 

Then the split picked up again and failed with:

{code}
2010-12-02 21:57:04,726 INFO org.apache.hadoop.hbase.regionserver.CompactSplitThread: Running rollback of failed split of TestTable,0045408302,1291326972892.218811f604f0b6822aae593939aef871.; org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /     hbase/TestTable/218811f604f0b6822aae593939aef871/splits/2545973d576877f35f55663703a3b134/info/2170653641682610010.218811f604f0b6822aae593939aef871 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sv2borg187,60020,1291321397561_1291321398066, pendingcreates: 1]
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1378)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1369)
...
{code}

The rollback reopened the region.... though it had been assigned elsewhere.






;;;","02/Dec/10 22:52;stack;On v2 of patch:

The below is being done outside of a sync block... use the clearRegionPlan method?

+    // remove the region plan as well just in case.
+    this.regionPlans.remove(regionInfo.getEncodedName());

I don't think the below is right:

+    } catch (ConcurrentCloseSplitFailedException e) {
+      LOG.info(""Split failed due to concurrent close, leaving region "" +
+          parent + "" closed and moving on."");
+      return;

You move stuff around in SplitTransaction.  You sure you are not messing up rollback?  Rollback is very much order dependent... the journal kept in execute must be undone in order in rollback?

Yeah, make sure timeoutmonitor in AssignmentManager does right thing w/ znode that has been set to CLOSING (or whatever the state its meant to be in).;;;","03/Dec/10 01:12;ryanobjc;v3;;;","03/Dec/10 02:52;ryanobjc;an extra lock in updateTimers caused a deadlock, i removed this extra locking, but maybe there is a better solution.;;;","03/Dec/10 06:46;stack;Yeah...Al the synchronizes are getting messy inside in AM.  I'll take a looksee in the morning.  I'd think that with regionPlans it wouldn't be the end of the world if we had a stale view iterating the odd time.  Will look more in morning. ;;;","03/Dec/10 22:49;stack;Here's a v5.  It undoes regionMap being a concurrentskiplist and up in updateTimers instead operates on a copy (expensive but deadlock-safe).  I'm going to commit this and try testing with it.;;;","03/Dec/10 22:55;stack;Committed branch and trunk.;;;","04/Dec/10 00:07;stack;Here is an addendum.  J-D saw deadlock'ing and dbl-assign because of close and concurrent split.  The close got an NSRE because region had been offlined by time close came in.  On receipt of an NSRE, the master will force offline -- though in this case there was nothing in zk so it failed -- and then reassign the region.  Bad.   Trying this patch.;;;","04/Dec/10 00:43;jdcryans;The addendum worked for me.;;;","04/Dec/10 00:50;stack;I applied the addendum.  Will keep testing over the w/e.  I think there are still issues in here.  Will open new issues as I find them (running w/ 2M regions and running balancer continuously seems to turn up ugly stuff).;;;","04/Dec/10 06:28;stack;I had to add this because of the new closeLock object to address broken build:

{code}
Index: src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java     (revision 1041703)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java     (working copy)
@@ -3136,7 +3136,7 @@
       (20 * ClassSize.REFERENCE) + ClassSize.OBJECT + Bytes.SIZEOF_INT);
 
   public static final long DEEP_OVERHEAD = ClassSize.align(FIXED_OVERHEAD +
-      ClassSize.OBJECT + (2 * ClassSize.ATOMIC_BOOLEAN) +
+      (ClassSize.OBJECT * 2) + (2 * ClassSize.ATOMIC_BOOLEAN) +
       ClassSize.ATOMIC_LONG + ClassSize.ATOMIC_INTEGER +
 
       // Using TreeMap for TreeSet
{code}

There is still another failure that the addendum commit has broke... I'm digging into that still.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If rows in .META. with no HRegionInfo cell, then hbck fails read of .META.",HBASE-3297,12491813,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,01/Dec/10 20:55,20/Nov/15 12:42,14/Jul/23 06:06,01/Dec/10 20:57,,,,,,,,,,,,0.90.0,,,,,,,0,,,"hbck was aborting read of .META. because adding empty row to TreeSet, TreeSet complained Result is not Comparable -- which it is not.

Here is a fix:

{code}
@@ -81,7 +83,7 @@ public class HBaseFsck {
   private boolean rerun = false; // if we tried to fix something rerun hbck
   private static boolean summary = false; // if we want to print less output
   // Empty regioninfo qualifiers in .META.
-  private TreeSet<Result> emptyRegionInfoQualifiers = new TreeSet<Result>();
+  private Set<Result> emptyRegionInfoQualifiers = new HashSet<Result>();
{code}
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26768,,,,,Fri Nov 20 12:42:34 UTC 2015,,,,,,,,,,"0|i0hlmn:",100766,,,,,,,,,,,,,,,,,,,,,"01/Dec/10 20:57;stack;Committed small easily verifiable fix.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Newly created table ends up disabled instead of assigned,HBASE-3296,12491806,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,01/Dec/10 19:46,20/Nov/15 12:42,14/Jul/23 06:06,02/Dec/10 20:21,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Something that was seen by someone on the channel yesterday and by me this morning, it's possible to create a table that ends up disabled and the 'create' calls times out. The master log looks like:

{noformat}
2010-12-01 19:32:52,350 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true, flushlogentries=1, optionallogflushinternal=1000ms
2010-12-01 19:32:52,450 INFO org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: Using syncFs -- HDFS-200
2010-12-01 19:32:52,451 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: New hlog /hbase/TestTable/2e2099cd5fce907e670ce8596d9b2368/.logs/hlog.1291231972350
2010-12-01 19:32:52,451 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Using getNumCurrentReplicas--HDFS-826
2010-12-01 19:32:52,452 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Instantiated TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.
2010-12-01 19:32:52,645 INFO org.apache.hadoop.hbase.regionserver.HRegion: Onlined TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.; next sequenceid=1
2010-12-01 19:32:52,784 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Added region TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368. to META
2010-12-01 19:32:52,784 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.: disabling compactions & flushes
2010-12-01 19:32:52,784 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates disabled for region TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.
2010-12-01 19:32:52,784 DEBUG org.apache.hadoop.hbase.regionserver.Store: closed info
2010-12-01 19:32:52,784 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.
2010-12-01 19:32:52,784 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: IPC Server handler 4 on 61000.logSyncer interrupted while waiting for sync requests
2010-12-01 19:32:52,784 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: IPC Server handler 4 on 61000.logSyncer exiting
2010-12-01 19:32:52,784 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: closing hlog writer in hdfs://sv2borg180:9100/hbase/TestTable/2e2099cd5fce907e670ce8596d9b2368/.logs
2010-12-01 19:32:52,908 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: Moved 1 log files to /hbase/TestTable/2e2099cd5fce907e670ce8596d9b2368/.oldlogs
2010-12-01 19:32:52,966 INFO org.apache.hadoop.hbase.master.AssignmentManager: Table TestTable disabled; skipping assign of TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.
2010-12-01 19:32:52,967 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Table being disabled so deleting ZK node
 and removing from regions in transition, skipping assignment of region TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.
2010-12-01 19:32:52,967 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:61000-0x12c84725b4b00b6
 Deleting existing unassigned node for 2e2099cd5fce907e670ce8596d9b2368 that is in expected state RS_ZK_REGION_CLOSED
2010-12-01 19:32:52,968 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Tried to delete closed node for REGION =>
 {NAME => 'TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.', STARTKEY => '', ENDKEY => '', ENCODED =>
 2e2099cd5fce907e670ce8596d9b2368, TABLE => {{NAME => 'TestTable', FAMILIES => [{NAME => 'info', BLOOMFILTER
 => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE =>
 '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}} but it does not exist so just offlining
{noformat}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/10 20:19;jdcryans;HBASE-3296.patch;https://issues.apache.org/jira/secure/attachment/12465162/HBASE-3296.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26767,Reviewed,,,,Fri Nov 20 12:42:50 UTC 2015,,,,,,,,,,"0|i0hlmf:",100765,,,,,,,,,,,,,,,,,,,,,"01/Dec/10 19:48;streamy;Maybe we aren't removing the table from the list of disabled tables in ZK when the table gets dropped?;;;","01/Dec/10 22:57;jdcryans;@Jon, it doesn't seem likely since I can normally truncate. It seems like a more special corner case where the previous state of the cluster was wrong...;;;","02/Dec/10 18:59;jdcryans;So I was able to reproduce the problem, if for any reason a table is disabled and the znode is kept in zookeeper but the user is still able to create it another time, it will be left disabled when it opens because it will pick up the old state. We should cleanup leftovers when creating a table.;;;","02/Dec/10 20:01;jdcryans;This patch just makes sure that the table is enabled before creating it.;;;","02/Dec/10 20:17;stack;Your patch has some conf  and admin.rb cruft.  Be careful not to apply it.  Otherwise +1

;;;","02/Dec/10 20:19;jdcryans;cruft-less patch.;;;","02/Dec/10 20:21;jdcryans;Committed the cruft-less version to branch and trunk.;;;","02/Dec/10 21:44;stack;+1;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dropping a 1k+ regions table likely ends in a client socket timeout and it's very confusing,HBASE-3295,12491803,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,jdcryans,jdcryans,01/Dec/10 19:29,20/Nov/15 12:42,14/Jul/23 06:06,02/Dec/10 18:50,,,,,,,,,,,,0.90.0,,,,,,,0,,,"I tried truncating a 1.6k regions table from the shell and, after the usual disabling timeout, I then got a socket timeout on the second invocation while it was dropping. It looked like this:

{noformat}
ERROR: java.net.SocketTimeoutException: Call to sv2borg180/10.20.20.180:61000 failed on socket timeout exception:
 java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch :
 java.nio.channels.SocketChannel[connected local=/10.20.20.180:59153 remote=sv2borg180/10.20.20.180:61000]
{noformat}

At first I thought that was coming from the master because HDFS was somehow slow, but then understood that it was my socket that timed out meaning that the master was still dropping the table. Calling truncate again, I got:

{noformat}
ERROR: Unknown table TestTable!
{noformat}

Which means that the table would be deleted... I learned later that it wasn't totally deleted after I shut down the cluster. So it leaves me in a situation where I have to manually delete the files on the FS and the remaining .META. entries.

Since I expect a few people will hit this issue rather soon, for 0.90.0, I propose we just set the socket timeout really high in the shell. For 0.90.1, or 0.92, we should do for drop what we do for disabling.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/10 06:33;stack;3295-v2.txt;https://issues.apache.org/jira/secure/attachment/12465107/3295-v2.txt","01/Dec/10 19:39;stack;3295.txt;https://issues.apache.org/jira/secure/attachment/12465062/3295.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26766,Reviewed,,,,Fri Nov 20 12:42:00 UTC 2015,,,,,,,,,,"0|i0hlm7:",100764,,,,,,,,,,,,,,,,,,,,,"01/Dec/10 19:37;streamy;This is basically the same as HBASE-3229 (except there it's only troubling, the operation does seem to succeed).  My opinion is that all operations that hit the master should be async (or fast).  create/enable/disable/drop/etc should be async w/ another method to check the status.  we shouldn't have long running operations holding open rpc requests.;;;","01/Dec/10 19:39;stack;This one-line change should do it.

It makes deleteTable async over on master.

In HBaseAdmin, the current code sends the deleteTable to the master then spins waiting on all entries in .META. to disappear.  The deleteTable was synchronous.  With this change the deleteTable is now async.... so we should be spinning client-side rather than server-side.  Should get rid of the socket timeout.;;;","01/Dec/10 19:43;jdcryans;@Jon, you're right.;;;","02/Dec/10 06:33;stack;HBASE-3229 is actually a little different in that as its currently written all is run from the master.

Also, here's v2 of patch.  There is a tension in the shell in that most of the retries and timesouts are tuned down because its expected that there is a human waiting but these enable/disable/drops can take a long time.  v2 ups amount of time we'll wait on these enable/disable/drop operations (as well as including the v1 change to make drop run async).

If we do time out, then it should be fine just rerunning the operation -- truncate in this case.  If not, then thats a bug... a different bug than this.;;;","02/Dec/10 09:25;larsgeorge;I like the async plus proper communication to user that they have to poll for the status - some sort of queued or async task list. I have found many reasons to press CTRL+c while waiting for a disable or drop just to mess up things. Ideally that could be build into the shell, i.e. it sends the command async and then polls the state while printing out ""."" on the command line to report it is waiting still. Once the async command has succeeded it reports so and exists the loop.;;;","02/Dec/10 10:29;larsgeorge;Sorry Stack, did not see your earlier comment. If that client side spin works then that is what I meant.;;;","02/Dec/10 18:28;jdcryans;The patch doesn't compile because there's a typo:

{noformat}
+    for (int tries = 0; tries < (this.numRetries * this.retryLongerMultiplier); tries++) {
{noformat}

After that it works. +1;;;","02/Dec/10 18:37;stack;@Lars There are new additions to the shell is_enabled and is_disabled... so ctrl-c and it'll interrupt the client-side wait at least of disable, enable and drop.;;;","02/Dec/10 18:50;stack;Thanks for reviews lads.  Committed branch and trunk.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WARN org.apache.hadoop.hbase.regionserver.Store: Not in set (double-remove?) org.apache.hadoop.hbase.regionserver.StoreScanner@76607d3d,HBASE-3294,12491802,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,01/Dec/10 19:19,20/Nov/15 12:42,14/Jul/23 06:06,01/Dec/10 21:08,,,,,,,,,,,,0.90.0,,,,,,,0,,,"We see this ugly message in running hbase.  Its a little disorientating.  I added stack traces around the call to close.  Here are the two I see (The WARN message is the bad one... the INFO is the good case).  Looks like compaction is the one that triggers the WARN.

{code}
2010-12-01 18:20:28,307 WARN org.apache.hadoop.hbase.regionserver.Store: Not in set (double-remove?) org.apache.hadoop.hbase.regionserver.StoreScanner@68faedc7
java.io.IOException: WHY
    at org.apache.hadoop.hbase.regionserver.Store.deleteChangedReaderObserver(Store.java:583)
    at org.apache.hadoop.hbase.regionserver.StoreScanner.close(StoreScanner.java:204)
    at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:242)
    at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:326)
    at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:939)
    at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:748)
    at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:753)
    at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:698)
    at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:81)

2010-12-01 18:21:18,103 INFO org.apache.hadoop.hbase.regionserver.Store: CLOSE org.apache.hadoop.hbase.regionserver.StoreScanner@3dca256e
java.io.IOException: WHY
    at org.apache.hadoop.hbase.regionserver.Store.deleteChangedReaderObserver(Store.java:592)
    at org.apache.hadoop.hbase.regionserver.StoreScanner.close(StoreScanner.java:204)
    at org.apache.hadoop.hbase.regionserver.KeyValueHeap.close(KeyValueHeap.java:192)
    at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.close(HRegion.java:2361)
    at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2945)
    at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2844)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1566)
    at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
    at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1036)
{code}",,khemani,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26765,,,,,Fri Nov 20 12:42:12 UTC 2015,,,,,,,,,,"0|i0hllz:",100763,,,,,,,,,,,,,,,,,,,,,"01/Dec/10 19:38;streamy;I've seen this up on my cluster with some regularity.;;;","01/Dec/10 20:01;ryanobjc;It might be due to this code:


    KeyValue kvReturn = this.current.next();
    KeyValue kvNext = this.current.peek();
    if (kvNext == null) {
      this.current.close();
      ...

If a store scanner comes to the end of it's scan, it will automatically call close() on itself.  This will deregister the changed reader observer.  Then the heap will also call close() which will redo that operation.

Removing the 'this.current.close()' code isn't really possible because this heap is used on both StoreScanner and StoreFileScanner, and we want the close() semantics in there.  The StoreFileScanner doesn't work in the same way as well.  

We should just remove the message, don't even bother checking for existence because it doesn't matter, the important part is we are no longer registered.;;;","01/Dec/10 21:05;stack;OK... I'll apply the below to address this issue:

{code}
stack@sv2borg180:~/trunk$ svn diff
Index: src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/Store.java       (revision 1041161)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/Store.java       (working copy)
@@ -579,9 +579,8 @@
    * @param o Observer no longer interested in changes in set of Readers.
    */
   void deleteChangedReaderObserver(ChangedReadersObserver o) {
-    if (!this.changedReaderObservers.remove(o)) {
-      LOG.warn(""Not in set (double-remove?) "" + o);
-    }
+    // We don't check if observer present; it may not be (legitimately)
+    this.changedReaderObservers.remove(o);
   }
 
   //////////////////////////////////////////////////////////////////////////////
Index: src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java       (revision 1041161)
+++ src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java       (working copy)
@@ -204,8 +204,8 @@
     delete.deleteColumns(HConstants.CATALOG_FAMILY, qualifier);
     catalogTracker.waitForMetaServerConnectionDefault().
       delete(CatalogTracker.META_REGION, delete);
-    LOG.info(""Deleted daughter "" + daughter.getRegionNameAsString() +
-      "" "" + Bytes.toString(qualifier) + "" reference in parent "" +
+    LOG.info(""Deleted daughter reference "" + daughter.getRegionNameAsString() +
+      "", qualifier="" + Bytes.toString(qualifier) + "", from parent "" +
       parent.getRegionNameAsString());
   }
{code}

The latter change is a fix to a log message that confused Ryan... lumping it in here.;;;","01/Dec/10 21:08;stack;Committed trunk and branch.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If split happens while regionserver is going down, we can stick open.",HBASE-3291,12491715,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,01/Dec/10 00:15,20/Nov/15 12:43,14/Jul/23 06:06,02/Dec/10 00:05,,,,,,,,,,,,0.90.0,,,,,,,0,,,"J-D found this one testing.  He found that if a split comes in during shutdown of a regionserver, then the regionserver can stick open... and won't go down.

We fixed a similar problem in the past where if balancer cut in during shutdown and assigned a regionserver an region during shutdown, we'd open it and it'd cause us again to stick open.  We fixed that by introducing the 'closing' state.

Fix for the issue j-d found is to do closing check when onlining daughters.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/10 00:04;stack;3291-v2.txt;https://issues.apache.org/jira/secure/attachment/12465091/3291-v2.txt","01/Dec/10 00:17;stack;closing.txt;https://issues.apache.org/jira/secure/attachment/12465009/closing.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26763,Reviewed,,,,Fri Nov 20 12:43:01 UTC 2015,,,,,,,,,,"0|i0hllj:",100761,,,,,,,,,,,,,,,,,,,,,"01/Dec/10 00:17;stack;J-D also noticed that we are inconsistent in our synchronize of onlineRegions.  This patch addresses this and adds check on split for closing.;;;","01/Dec/10 00:19;jdcryans;Looks good, going to try it on my little cluster and see how it behaves before giving +1.;;;","01/Dec/10 01:15;jdcryans;Well it didn't work for me, and I didn't see the ""Not opening daughter..."" message strangely. Investigating more.;;;","01/Dec/10 22:53;jdcryans;Found two issues:

{code}
+    while (isOnlineRegionsEmpty()) {
{code}

should check if it's not empty, else it loops forever.

{code}
+    HRegion r = null;
+    synchronized (this.onlineRegions) {
+      this.onlineRegions.get(encodedRegionName);
+    }
{code}

is missing the assignment of ""r"", else the web page NPEs.

So with both corrected and when I'm not hitting HBASE-3298, the patch works.;;;","01/Dec/10 23:39;jdcryans;Another issue, when restarting after the fixup happened the parent region is offline and the daughters are never registered leaving a hole in .META. which requires this patch:

{code}

--- a/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
@@ -292,6 +292,7 @@ class SplitTransaction {
       final RegionServerServices services, final HRegion daughter)
   throws IOException, KeeperException {
     if (server.isStopped() || services.isStopping()) {
+      MetaEditor.addDaughter(server.getCatalogTracker(), daughter.getRegionInfo(), null);
       LOG.info(""Not opening daughter "" +
         daughter.getRegionInfo().getRegionNameAsString() +
         "" because stopping="" + services.isStopping() + "", stopped="" +

{code};;;","02/Dec/10 00:04;stack;What I""m committing... original patch along w/ J-D fixes.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master passes IP and not hostname back to region server,HBASE-3286,12491590,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,30/Nov/10 00:05,20/Nov/15 12:42,14/Jul/23 06:06,01/Dec/10 01:34,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Starting my little test cluster on the latest from 0.90, I see:

{noformat}
2010-11-29 23:21:34,131 INFO org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 1024 region(s) across 9 server(s), retainAssignment=true
2010-11-29 23:21:34,134 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 22 region(s) to sv2borg181,61020,1291072886282
2010-11-29 23:21:34,135 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 24 region(s) to sv2borg182,61020,1291072885473
2010-11-29 23:21:34,135 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 37 region(s) to sv2borg183,61020,1291072885646
2010-11-29 23:21:34,135 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 25 region(s) to sv2borg184,61020,1291072886734
2010-11-29 23:21:34,135 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 26 region(s) to sv2borg185,61020,1291072886606
2010-11-29 23:21:34,136 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 70 region(s) to sv2borg186,61020,1291072885486
2010-11-29 23:21:34,136 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 30 region(s) to sv2borg187,61020,1291072886355
2010-11-29 23:21:34,136 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 89 region(s) to sv2borg188,61020,1291072885926
2010-11-29 23:21:34,136 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 701 region(s) to sv2borg189,61020,1291072886739
{noformat}

After another restart:

{noformat}
2010-11-30 00:03:38,100 INFO org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 1024 region(s) across 9 server(s), retainAssignment=true
2010-11-30 00:03:38,103 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 71 region(s) to sv2borg181,61020,1291075409984
2010-11-30 00:03:38,103 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 82 region(s) to sv2borg182,61020,1291075409956
2010-11-30 00:03:38,104 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 61 region(s) to sv2borg183,61020,1291075409952
2010-11-30 00:03:38,104 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 122 region(s) to sv2borg184,61020,1291075409957
2010-11-30 00:03:38,104 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 59 region(s) to sv2borg185,61020,1291075409955
2010-11-30 00:03:38,104 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 71 region(s) to sv2borg186,61020,1291075409963
2010-11-30 00:03:38,105 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 52 region(s) to sv2borg187,61020,1291075411049
2010-11-30 00:03:38,105 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 254 region(s) to sv2borg188,61020,1291075410360
2010-11-30 00:03:38,105 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 252 region(s) to sv2borg189,61020,1291075409959
{noformat}

I also saw one time where everything was assigned to 189.",,khemani,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/10 01:29;jdcryans;HBASE-3286.patch;https://issues.apache.org/jira/secure/attachment/12465017/HBASE-3286.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26762,Reviewed,,,,Fri Nov 20 12:42:23 UTC 2015,,,,,,,,,,"0|i0hlkv:",100758,,,,,,,,,,,,,,,,,,,,,"30/Nov/10 00:40;jdcryans;It seems that the master is only able to recognize a few region servers' HSA, I added some debug and I see that the region servers with a lot of regions are those that are matched and they also get assigned random regions.;;;","30/Nov/10 00:59;jdcryans;So I was able to pin it down, I'm debugging how the HSA are created and how we get them from .META and here's the result:

{noformat}

org.apache.hadoop.hbase.master.LoadBalancer: server list contains 10.20.20.185:61020
...
org.apache.hadoop.hbase.master.LoadBalancer: server list contains 10.20.20.182:61020
org.apache.hadoop.hbase.master.LoadBalancer: Assigning TestTable,,1290643751956.ef7ab3ee0b05de57c5938bb95a462403. 
 randomly because this server doesn't exist sv2borg186:61020
org.apache.hadoop.hbase.master.LoadBalancer: Assigning TestTable,0000049992,1290643755990.124a2b03eed3544d9196876f8660f6dd. to 10.20.20.187:61020
org.apache.hadoop.hbase.master.LoadBalancer: Assigning TestTable,0000100088,1290643795638.e2ddc1eb1a720d89138a49a1622f49be.
 randomly because this server doesn't exist sv2borg184:61020
org.apache.hadoop.hbase.master.LoadBalancer: Assigning TestTable,0000109979,1290643795638.649f2bae753dda53931a7eb10899cdb6. to 10.20.20.187:61020
{noformat}

There's a mix of IPs and hostnames, so it won't match. Same old issue...;;;","30/Nov/10 19:34;jdcryans;Finally got to the bottom of the issue, the master is actually passing an IP back to the region server when it checks in and, although it stores the hostname in ServerManager.onlineServers the first time, when the region servers check back in it overrides the HSA with the hostname with an HSA that only has an IP. Posting patch to review board in a moment.;;;","30/Nov/10 19:48;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1262/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Changes:
 - In HMaster, instead of passing an IP as String we now pass the HSA object completely.
 - In HRegionServer, I cleared a bunch of crufty comments and handle the HSA passed by the master.
 - In HServerInfo, I saw that the hostname wasn't reset when setting the HSA. Fixed.
 - In HServerAddress, I fixed a few places that wasn't explicitly using hostnames and changed the serialization to pass a hostname instead of an IP address.


This addresses bug HBASE-3286.
    http://issues.apache.org/jira/browse/HBASE-3286


Diffs
-----

  /trunk/src/main/java/org/apache/hadoop/hbase/HServerAddress.java 1040669 
  /trunk/src/main/java/org/apache/hadoop/hbase/HServerInfo.java 1040669 
  /trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1040669 
  /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 1040669 

Diff: http://review.cloudera.org/r/1262/diff


Testing
-------

Works on my MBP (I was seeing the same issue but since there's only 1 RS it didn't have any bad effect) and my 10 machines Ubuntu cluster. 


Thanks,

Jean-Daniel


;;;","30/Nov/10 20:28;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1262/#review2013
-----------------------------------------------------------

Ship it!


A little confused by the discrepancy between String host / int port and the Address.  But does seem fine given we don't actually access the string/int values and always use the address object.

Do we need some tests on this stuff?  Seems like we always have issues here but tests don't catch anything.

Looks better than what we have though so I'm +1 regardless.

- Jonathan



;;;","30/Nov/10 20:28;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1262/#review2012
-----------------------------------------------------------

Ship it!


A little confused by the discrepancy between String host / int port and the Address.  But does seem fine given we don't actually access the string/int values and always use the address object.

Do we need some tests on this stuff?  Seems like we always have issues here but tests don't catch anything.

Looks better than what we have though so I'm +1 regardless.


/trunk/src/main/java/org/apache/hadoop/hbase/HServerAddress.java
<http://review.cloudera.org/r/1262/#comment6351>

    Why does stringValue not necessarily equal the host:port we store in those Strings?  Shouldn't they be the same?



/trunk/src/main/java/org/apache/hadoop/hbase/HServerAddress.java
<http://review.cloudera.org/r/1262/#comment6352>

    But on serialization, we use the address hostname not the thing we actually store in hostname/port variables, so after serialized it's different?
    
    Shouldn't we set the hostname/port variables on construction according to address.getAddress/getPort rather than the passed values, if the address values are what we want to use?



/trunk/src/main/java/org/apache/hadoop/hbase/HServerInfo.java
<http://review.cloudera.org/r/1262/#comment6353>

    I guess we never actually use the String host / int port?  Why do we store them in HServerAddress then?



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
<http://review.cloudera.org/r/1262/#comment6354>

    whitespace


- Jonathan



;;;","01/Dec/10 01:25;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>


bq.  On 2010-11-30 12:27:06, Jonathan Gray wrote:
bq.  > A little confused by the discrepancy between String host / int port and the Address.  But does seem fine given we don't actually access the string/int values and always use the address object.
bq.  > 
bq.  > Do we need some tests on this stuff?  Seems like we always have issues here but tests don't catch anything.
bq.  > 
bq.  > Looks better than what we have though so I'm +1 regardless.

Regarding tests, I'm not sure what they would catch... 


bq.  On 2010-11-30 12:27:06, Jonathan Gray wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/HServerAddress.java, line 65
bq.  > <http://review.cloudera.org/r/1262/diff/1/?file=17919#file17919line65>
bq.  >
bq.  >     Why does stringValue not necessarily equal the host:port we store in those Strings?  Shouldn't they be the same?

I'm trying to keep it more consistent with the rest of the code, else when looking at the code you ask yourself the question you just asked me :)


bq.  On 2010-11-30 12:27:06, Jonathan Gray wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/HServerAddress.java, line 177
bq.  > <http://review.cloudera.org/r/1262/diff/1/?file=17919#file17919line177>
bq.  >
bq.  >     But on serialization, we use the address hostname not the thing we actually store in hostname/port variables, so after serialized it's different?
bq.  >     
bq.  >     Shouldn't we set the hostname/port variables on construction according to address.getAddress/getPort rather than the passed values, if the address values are what we want to use?

I'm... not following you. You're saying that we shouldn't store the InetSocketAddress?


bq.  On 2010-11-30 12:27:06, Jonathan Gray wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/HServerInfo.java, line 116
bq.  > <http://review.cloudera.org/r/1262/diff/1/?file=17920#file17920line116>
bq.  >
bq.  >     I guess we never actually use the String host / int port?  Why do we store them in HServerAddress then?

Here I'm just making sure that after updating the address we also update the hostname, since it could have changed.


- Jean-Daniel


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1262/#review2012
-----------------------------------------------------------



;;;","01/Dec/10 01:29;jdcryans;Patch I'm committing.;;;","01/Dec/10 01:34;jdcryans;Committed to branch and trunk.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hlog recovery takes too much time,HBASE-3285,12491584,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,hairong,hairong,hairong,29/Nov/10 21:28,20/Nov/15 12:43,14/Jul/23 06:06,07/Mar/11 18:55,,,,,,,,,,,,0.90.2,,,,,,,0,,,"Currently HBase uses append to trigger the close of HLog during Hlog split. Append is a very expensive operation, which involves not only NameNode operations but creating a writing pipeline. If one of datanodes on the pipeline has a problem, this recovery may takes minutes. I'd like implement a lightweight NameNode operation to trigger lease recovery and make HBase to use this instead.",,amansk,apurtell,khemani,larsfrancke,larsgeorge,tsuna,,,,,,,,,,,,,,,,,,,,,,,,HDFS-1520,,,,,,HBASE-3379,,,,,,,,,,,,,,,,,,,,,"07/Mar/11 19:10;stack;0004-Added-in-the-hdfs-1520-1555-1554-from-tip-of-the-bra.patch;https://issues.apache.org/jira/secure/attachment/12472843/0004-Added-in-the-hdfs-1520-1555-1554-from-tip-of-the-bra.patch","05/Mar/11 00:01;stack;3285-v2.txt;https://issues.apache.org/jira/secure/attachment/12472724/3285-v2.txt","05/Mar/11 07:52;stack;3285-v3.txt;https://issues.apache.org/jira/secure/attachment/12472743/3285-v3.txt","07/Jan/11 22:57;hairong;hbaseRecoverHLog.patch;https://issues.apache.org/jira/secure/attachment/12467773/hbaseRecoverHLog.patch","05/Mar/11 06:26;stack;hdfs-1520,1555,1554-for-cdh3b2.txt;https://issues.apache.org/jira/secure/attachment/12472740/hdfs-1520%2C1555%2C1554-for-cdh3b2.txt",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26761,,,,,Fri Nov 20 12:43:57 UTC 2015,,,,,,,,,,"0|i0b1ov:",62386,"Adds hbase exploitation of  new lease recovery added by hdfs-1520.  New API is available at tip of branch-0.20-append, in advance of the version of hadoop that ships with 0.90.1 hbase (r1057313).  Must patch CDH to add the API.",,,,,,,,,,,,,,,,,,,,"30/Nov/10 00:50;jdcryans;Would this new NN operation still goes through the lease recovery? In my experience, waiting on the leases to expire has been our biggest source of downtime.;;;","01/Dec/10 06:20;hairong;This issue does not speedup the lease recovery process but will reduce the cost of triggering lease recovery. In our environment, we have observed over 10 minutes for append to finish due to a problematic datanode in the pipeline.;;;","07/Jan/11 22:40;hairong;HDFS-1554 provides a new semantics which will speed up lease recovery. (No need to wait for soft lease expiration).;;;","07/Jan/11 22:57;hairong;Here is the patch that uses the new recoverLease API to recover HLog.

I assume that CDH will pull in the HADOOP recoverLease change into their distribution.;;;","23/Feb/11 21:16;stack;@Hairong Are there new exceptions when hdfs-1554 is in place?  This patch seems to just catch and wait and change whats logged, not call some new API (oh, it also adds a new convertion to FNFE).  Thanks.;;;","23/Feb/11 21:22;hairong;RecoverLease does not introduce any new exception. The patch changed to use recoverLease not append to recover lease. HDFS-1554 did not introduce new API, but changed recoverLease semantics.;;;","05/Mar/11 00:01;stack;Made patch work when recoverLease is not available by stealing from Nicolas's HBASE-2312 patch; he uses reflection to test for recoverLease falling back on append if its missing (as opposed to this patch just using recoverLease).  Running tests.;;;","05/Mar/11 04:30;stack;All tests on 0.90 pass w/ this patch in place.  TestReplication failed but when I ran it alone it passed.;;;","05/Mar/11 06:18;stack;Did a bit of server killing on a cluster that was up on the tip of branch-0.20-append -- i.e. had hdfs-1520, 1555, and 1554 -- and recovery of lease seems to run nice and promptly.  Testing now on the branch-0.20-append that hbase 0.90.1 ships with, a branch-0.20-append that was absent 1520, 1555, and 1554.;;;","05/Mar/11 06:26;stack;Patch for cdh3b2 adding recent additions from branch-0.20-append (hdfs-1520,1555,1554) used testing recover hlog works up on cdh too.  Running tests, I see a bunch fail -- TestPread, TestFileCreation, TestDatanodeBlockScanner.  Will try w/o this patch to see if I get same failure set.;;;","05/Mar/11 07:52;stack;Fix a noisey log message in case where recoverlease method is missing.

Tested on cluster w/o the api and seems to work fine.

Will apply this patch to branch and trunk unless objection.  It can make use of the fs.recoverLease API if present purportedly skirting issues seen where we can get into a loop stuck trying to grab lease when trying to open a file for append.;;;","07/Mar/11 18:49;stack;Unit tests are broke on branch-0.20-append.  In particular TestFileAppend4 fails in testRecoverFinalizedBlock.  Its broke before these 1520,1555,1554 patches went in.  TestMultiThreadedSync is also failing.

Before applying patches to CDH3b2, I had this failing TestSyncingWriterInterrupted.  Seems sporadic though because after applying the 1520+ series of patches, only TestAppend4 failed.

Going to apply this patch to hbase.;;;","07/Mar/11 19:10;stack;Updated patch for cdh3b2.;;;","07/Mar/11 21:07;hudson;Integrated in HBase-TRUNK #1771 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1771/])
    HBASE-3285 Hlog recovery takes too much time
;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in AssignmentManager if processing shutdown of RS who doesn't have any regions assigned to it,HBASE-3283,12491580,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,29/Nov/10 20:59,20/Nov/15 12:43,14/Jul/23 06:06,29/Nov/10 21:20,0.90.0,,,,,,,,,,,0.90.0,0.92.0,master,,,,,0,,,"When doing server shutdown, we do the following:

{noformat}
deadRegions = new TreeSet<HRegionInfo>(this.servers.remove(hsi));
{noformat}

This removes the list of regions currently assigned to the specified server from the master in-memory state.  But it's possible that the remove returns null (have seen it in running production cluster).

We should check if this is null and just return if so.",,khemani,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/10 21:00;streamy;HBASE-3283-v1.patch;https://issues.apache.org/jira/secure/attachment/12464899/HBASE-3283-v1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26759,Reviewed,,,,Fri Nov 20 12:43:41 UTC 2015,,,,,,,,,,"0|i0hlkn:",100757,,,,,,,,,,,,,,,,,,,,,"29/Nov/10 21:00;streamy;Adds null check and early out if so.;;;","29/Nov/10 21:11;stack;+1;;;","29/Nov/10 21:20;streamy;Committed to branch and trunk.  Thanks stack.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to retain DeadServers to ensure we don't allow previously expired RS instances to rejoin cluster,HBASE-3282,12491565,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,29/Nov/10 18:11,20/Nov/15 12:42,14/Jul/23 06:06,29/Nov/10 21:03,0.90.0,,,,,,,,,,,0.90.0,0.92.0,master,,,,,0,,,"Currently we clear a server from the deadserver set once we finish processing it's shutdown.  However, certain circumstances (network partitions, race conditions) could lead to the RS not doing a check-in until after the shutdown has been processed.  As-is, this RS will now be let back in to the cluster rather than rejected with YouAreDeadException.

We should hang on to the dead servers so we always reject them.

One concern is that the set will grow indefinitely.  One recommendation by stack is to use SoftReferences.",,khemani,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/10 21:01;streamy;HBASE-3282-v4.patch;https://issues.apache.org/jira/secure/attachment/12464900/HBASE-3282-v4.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26758,Reviewed,,,,Fri Nov 20 12:42:20 UTC 2015,,,,,,,,,,"0|i0hlkf:",100756,,,,,,,,,,,,,,,,,,,,,"29/Nov/10 19:05;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1259/
-----------------------------------------------------------

Review request for hbase and stack.


Summary
-------

We currently let go of dead servers once we finish their shutdown.  We should hang on to them longer to deal with things like network partitions.

I'm not a fan of SoftReferences so I decided on another approach.  DeadServers now has a maximum number of servers to hold on to in the set (default 100).  Once it reaches the max, it evicts the oldest.

More code than I had hoped but nothing too crazy.


This addresses bug HBASE-3282.
    http://issues.apache.org/jira/browse/HBASE-3282


Diffs
-----

  branches/0.90/src/main/java/org/apache/hadoop/hbase/master/DeadServer.java 1040221 
  branches/0.90/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1040221 
  branches/0.90/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java 1040221 
  branches/0.90/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java 1040221 

Diff: http://review.cloudera.org/r/1259/diff


Testing
-------

Running unit tests now.


Thanks,

Jonathan


;;;","29/Nov/10 19:31;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1259/#review2004
-----------------------------------------------------------

Ship it!



branches/0.90/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
<http://review.cloudera.org/r/1259/#comment6320>

    You can make this private now that its no longer referenced by Master?


- stack



;;;","29/Nov/10 19:45;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1259/
-----------------------------------------------------------

(Updated 2010-11-29 11:43:07.682958)


Review request for hbase and stack.


Changes
-------

Makes DeadServers private.  It was still accessed by my TestRollingRestart test so I had to make a small change to how that worked.

Also added some additional testing in TestDeadServers that verifies the new boolean check works as expected and also that the max capacity works as expected.


Summary
-------

We currently let go of dead servers once we finish their shutdown.  We should hang on to them longer to deal with things like network partitions.

I'm not a fan of SoftReferences so I decided on another approach.  DeadServers now has a maximum number of servers to hold on to in the set (default 100).  Once it reaches the max, it evicts the oldest.

More code than I had hoped but nothing too crazy.


This addresses bug HBASE-3282.
    http://issues.apache.org/jira/browse/HBASE-3282


Diffs (updated)
-----

  branches/0.90/src/main/java/org/apache/hadoop/hbase/master/DeadServer.java 1040242 
  branches/0.90/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1040242 
  branches/0.90/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java 1040242 
  branches/0.90/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java 1040242 
  branches/0.90/src/test/java/org/apache/hadoop/hbase/master/TestDeadServer.java 1040242 
  branches/0.90/src/test/java/org/apache/hadoop/hbase/master/TestRollingRestart.java 1040242 

Diff: http://review.cloudera.org/r/1259/diff


Testing
-------

Running unit tests now.


Thanks,

Jonathan


;;;","29/Nov/10 21:01;streamy;Unit tests passed.  Committing this final version of the patch.;;;","29/Nov/10 21:03;streamy;Committed to branch and trunk after verifying unit tests pass.  Thanks for review stack.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YouAreDeadException being swallowed in HRS getMaster(),HBASE-3280,12491551,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,29/Nov/10 15:54,20/Nov/15 12:41,14/Jul/23 06:06,29/Nov/10 18:42,0.90.0,,,,,,,,,,,0.90.0,0.92.0,regionserver,,,,,0,,,"In the HRS, when we lose our connection to the master, we enter into a loop where we keep trying to get the new master location in ZK and attempt to send our heartbeat.  Within tryRegionServerReport() we could get a YouAreDeadException, but we won't let it out.  This leads to the RS continuously heartbeating in to the master although the master keeps telling it to kill itself.",,khemani,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/10 17:43;streamy;HBASE-3280-v1.patch;https://issues.apache.org/jira/secure/attachment/12464885/HBASE-3280-v1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26757,Reviewed,,,,Fri Nov 20 12:41:07 UTC 2015,,,,,,,,,,"0|i0hlk7:",100755,,,,,,,,,,,,,,,,,,,,,"29/Nov/10 16:19;streamy;Somewhat related to this, what happened on a cluster here is that the HRS got stuck in this loop trying to reconnect to master and ignoring the YouAreDeadExceptions.  But then once the master finished shutdown handling, it removes this server from the dead server list.  Then the RS actually successfully heartbeated in to the master and the master thought it was a legit RS (even though it just finished doing a shutdown of it).

Is there a reason we should ever clear things out of the dead server list?  If this RS is in a network partition it may not check back with the master for a long time so we should always remember the dead serverNames (which include start codes)?;;;","29/Nov/10 17:43;streamy;Properly catch and rethrow if we are given a YouAreDeadException doing heartbeat to master.;;;","29/Nov/10 18:02;stack;+1;;;","29/Nov/10 18:07;stack;On remembering forever dead servers (with startcode), yeah, that sounds about right.  Could keep the server in a soft references Map.  Want to make new issue?;;;","29/Nov/10 18:42;streamy;Committed to trunk and branch.  Will be doing cluster testing with this patch soon.

Thanks for review stack.  Opened HBASE-3282 for dead server stuff.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in LoadBalancer,HBASE-3278,12480880,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,jdcryans,jdcryans,24/Nov/10 23:53,20/Nov/15 12:43,14/Jul/23 06:06,07/Dec/10 05:27,,,,,,,,,,,,0.90.0,0.92.0,,,,,,0,,,"While running PE with low splitting configuration, I got this:

{noformat}

2010-11-24 23:23:24,508 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: TestTable,0002485653,...
2010-11-24 23:23:26,129 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: TestTable,0004309306,...
2010-11-24 23:23:26,132 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: TestTable,0001281491...
2010-11-24 23:23:26,162 FATAL org.apache.hadoop.hbase.master.HMaster$1: sv2borg180:61000-BalancerChoreerror
java.lang.AssertionError
	at org.apache.hadoop.hbase.master.LoadBalancer.balanceCluster(LoadBalancer.java:296)
	at org.apache.hadoop.hbase.master.HMaster.balance(HMaster.java:679)
	at org.apache.hadoop.hbase.master.HMaster$1.chore(HMaster.java:578)
	at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
2010-11-24 23:23:26,163 INFO org.apache.hadoop.hbase.master.HMaster$1: sv2borg180:61000-BalancerChore exiting
2010-11-24 23:23:26,236 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: TestTable,0000921369,...
{noformat}

The thread dies but the master survives. There's nothing specific before that in the log, just regions splitting.

The line in LoadBalancer is:

{code}
    assert(regionidx == regionsToMove.size());
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/10 01:48;streamy;HBASE-3278-v1.patch;https://issues.apache.org/jira/secure/attachment/12465649/HBASE-3278-v1.patch","29/Nov/10 20:24;stack;lb.txt;https://issues.apache.org/jira/secure/attachment/12464897/lb.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26756,Reviewed,,,,Fri Nov 20 12:43:41 UTC 2015,,,,,,,,,,"0|i0hljr:",100753,,,,,,,,,,,,,,,,,,,,,"29/Nov/10 20:24;stack;This patch adds logging to the load balancer assertion so we can get a clue as to how the scenario arose.  I'm committing it.;;;","29/Nov/10 20:27;streamy;+1.  You have some HBASE-3243 left in the patch but both good for commit :)

I looked at this and couldn't figure how it happened.  Good on you for debug, stack.;;;","30/Nov/10 00:28;stack;Moving out to 0.90.1 till we get more data;;;","30/Nov/10 00:37;streamy;Seems like HBASE-3284 could cause this.  Need to look more at the locking the balancer does.  At first I thought it wasn't locking enough but now it seems like it locks where it needs to.;;;","07/Dec/10 01:48;streamy;Fixes bug in load balancer and adds test from stack that uncovered bug.;;;","07/Dec/10 04:24;stack;+1 because of the added test;;;","07/Dec/10 05:27;stack;Committed trunk and branch.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase Shell zk_dump command broken,HBASE-3277,12480864,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,svarma,svarma,svarma,24/Nov/10 22:11,20/Nov/15 12:43,14/Jul/23 06:06,24/Nov/10 23:32,0.90.0,,,,,,,,,,,0.90.0,,shell,,,,,0,,,"HBase shell zk_dump command is broken. Typing ""zk_dump"" in the shell gives: 
hbase(main):002:0> zk_dump
ERROR: undefined method `dump' for #<Java::OrgApacheHadoopHbaseZookeeper::ZooKeeperWatcher:0xd6ee28>

Here is some help for this command:
Dump status of HBase cluster as seen by ZooKeeper.
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/10 23:30;stack;3277.txt;https://issues.apache.org/jira/secure/attachment/12460417/3277.txt","25/Nov/10 03:48;svarma;HBASE-3277.patch;https://issues.apache.org/jira/secure/attachment/12460429/HBASE-3277.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26755,,,,,Fri Nov 20 12:43:27 UTC 2015,,,,,,,,,,"0|i0hljj:",100752,,,,,,,,,,,,hbase shell zk_dump,,,,,,,,,"24/Nov/10 23:30;stack;Small fix.  Dump had moved elsewhere.  Patch includes removal of stuff from dump that makes no sense any more.;;;","24/Nov/10 23:32;stack;Committed small fix.;;;","25/Nov/10 03:48;svarma;Used ZKUtil.dump method similar to zk.jsp. Patch file attached.;;;","25/Nov/10 03:50;svarma;Sorry - didn't realize fix was already checked in ... please ignore my patch. ;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[rest] No gzip/deflate content encoding support,HBASE-3275,12480839,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,apurtell,apurtell,apurtell,24/Nov/10 18:04,20/Nov/15 12:43,14/Jul/23 06:06,18/Mar/11 01:58,,,,,,,,,,,,0.90.0,0.92.0,,,,,,0,,,The REST gateway does not support gzip or deflate content encoding. This is a bug because it is a very common performance optimization and Jetty trivially supports it. ,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/10 18:06;apurtell;HBASE-3275.patch;https://issues.apache.org/jira/secure/attachment/12460384/HBASE-3275.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26753,,,,,Fri Nov 20 12:43:28 UTC 2015,,,,,,,,,,"0|i0hljb:",100751,,,,,,,,,,,,,,,,,,,,,"24/Nov/10 18:15;apurtell;Committed trivial patch. All REST tests pass locally.;;;","18/Mar/11 00:50;apurtell;Recent versions of Jetty break this. 

{noformat}
11/03/18 01:32:34 ERROR mortbay.log: /testfish_documents
java.lang.ClassCastException: org.mortbay.jetty.HttpConnection$Output cannot be cast to org.apache.hadoop.hbase.rest.filter.GZIPResponseStream
	at org.apache.hadoop.hbase.rest.filter.GzipFilter.doFilter(GzipFilter.java:54)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{noformat}
;;;","18/Mar/11 01:03;apurtell;Seems to be an issue with error handling. Happens when attempting an illegal GET /tablename (need minimum GET /tablename/row).;;;","18/Mar/11 01:36;stack;So, its just ugly if you have badly formatted URL Andrew?  Thats not so bad.;;;","18/Mar/11 01:58;apurtell;The output stream is not wrapped if there is an error, more type checking needed there.  Added a test case. Committed a fix to trunk and 0.90 branch.;;;","18/Mar/11 06:05;hudson;Integrated in HBase-TRUNK #1795 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1795/])
    HBASE-3275 [rest] No gzip/deflat content encoding support; fix error handling in GzipFilter
;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set the ZK default timeout to 3 minutes,HBASE-3273,12480750,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,23/Nov/10 23:26,20/Nov/15 12:41,14/Jul/23 06:06,24/Nov/10 18:31,,,,,,,,,,,,0.90.0,0.92.0,,,,,,0,,,"Following HBASE-3272, Stack suggested that we up the ZK timeout and proposed that we set it to 3 minutes (he said that last part in person). This should cover most of the big GC pauses out there.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/10 00:47;jdcryans;HBASE-3273.patch;https://issues.apache.org/jira/secure/attachment/12460326/HBASE-3273.patch","24/Nov/10 00:15;stack;doc_of_three_minute.txt;https://issues.apache.org/jira/secure/attachment/12460324/doc_of_three_minute.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26752,Reviewed,,,,Fri Nov 20 12:41:02 UTC 2015,,,,,,,,,,"0|i0hliv:",100749,,,,,,,,,,,,,,,,,,,,,"23/Nov/10 23:47;streamy;At the expense of slower detection of failures.  We should think about adding a second monitor process that lives outside of GC issues so we can lower this timeout rather than continue making it longer.;;;","24/Nov/10 00:15;stack;Here is a patch for the manual adding zookeeper.session.timeout to the list of configurations we suggest you change with explaination for why its set to a long three minute default timeout.;;;","24/Nov/10 00:47;jdcryans;Patch that changes the timeout to 3min and that fixes HQuorumPeer to use the new configuration introduced in ZK 3.3.0;;;","24/Nov/10 00:56;jdcryans;Regarding the documentation:

bq. The default timeout is three minutes

I would add: The default timeout is three minutes (specified in milliseconds)

bq. This means that if a server crash

Shouldn't it be ""crashes""?

Also there's a typo later in ""intriciacies"".;;;","24/Nov/10 03:32;stack;+1 on your patch and on the doc fixes.

Want to make the doc fixes you suggest above when you commit the doc alongside your commit of your patch?;;;","24/Nov/10 18:31;jdcryans;Committed both patches to trunk and 0.90, with my small fixes.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove no longer used options,HBASE-3272,12480745,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,23/Nov/10 22:11,20/Nov/15 12:43,14/Jul/23 06:06,23/Nov/10 23:16,,,,,,,,,,,,0.90.0,,,,,,,0,,,"From Lars George list up on hbase-dev:

{code}
Hi,

I went through the config values as per the defaults XML file (still
going through it again now based on what is actually in the code, i.e.
those not in defaults). Here is what I found:

hbase.master.balancer.period - Only used in hbase-default.xml?

hbase.regions.percheckin, hbase.regions.slop - Some tests still have
it but not used anywhere else

zookeeper.pause, zookeeper.retries - Never used? Only in hbase-defaults.xml


And then there are differences between hardcoded and XML based defaults:

hbase.client.pause - XML: 1000, hardcoded: 2000 (HBaseClient) and 30 *
1000 (HBaseAdmin)

hbase.client.retries.number - XML: 10, hardcoded 5 (HBaseAdmin) and 2 (HMaster)

hbase.hstore.blockingStoreFiles - XML: 7, hardcoded: -1

hbase.hstore.compactionThreshold - XML: 3, hardcoded: 2

hbase.regionserver.global.memstore.lowerLimit - XML: 0.35, hardcoded: 0.25

hbase.regionserver.handler.count - XML: 25, hardcoded: 10

hbase.regionserver.msginterval - XML: 3000, hardcoded: 1000

hbase.rest.port - XML: 8080, hardcoded: 9090

hfile.block.cache.size - XML: 0.2, hardcoded: 0.0


Finally, some keys are already in HConstants, some are in local
classes and others used as literals. There is an issue open to fix
this though. Just saying.

Thoughts?
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/10 22:36;stack;3272.txt;https://issues.apache.org/jira/secure/attachment/12460316/3272.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26751,Reviewed,,,,Fri Nov 20 12:43:06 UTC 2015,,,,,,,,,,"0|i0hlin:",100748,,,,,,,,,,,,,,,,,,,,,"23/Nov/10 22:36;stack;Thanks for making the list Lars.  Here is a patch to address your list of issues.  It doesn't address your last point.  Just removes unused stuff and makes XML match hardcodings.  The former I think is an important purge to do for 0.90.  The rest not so important but did it anyway.;;;","23/Nov/10 22:36;stack;Looking for a +1.;;;","23/Nov/10 22:46;jdcryans;+1

Looking at the patch, it made me remember that we wanted to raise the default for blocking store file to something like 12. Do we still want to do this?;;;","23/Nov/10 23:12;stack;Sure.  

Should we up the default zk timeout while we're at it?;;;","23/Nov/10 23:16;stack;Committed to branch and trunk.;;;","24/Nov/10 10:51;larsgeorge;This is a ""dupe"" to HBASE-2721. Closing it out as well.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HBase table truncate semantics seems broken as ""disable"" table is now async by default.",HBASE-3269,12480714,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,svarma,svarma,23/Nov/10 19:13,18/Sep/16 07:40,14/Jul/23 06:06,24/Nov/10 04:07,0.90.0,,,,,,,,,,,0.90.0,0.92.0,,,,,,0,,,"The new async design for disable table seems to have caused a side effect on the truncate command. (IRC chat with jdcryans)

Apparent Cause: 
""Disable"" is now async by default. When truncate is called, the disable operation returns immediately and when the drop is called, the disable operation is still not completed. This results in HMaster.checkTableModifiable() throwing a TableNotDisabledException.

With earlier versions, disable returned only after Table was disabled.",RHEL5 x86_64,dcswinner001,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26750,,,,,Sun Sep 18 07:40:54 UTC 2016,,,,,,,,,,"0|i0hlif:",100747,,,,,,,,,,,,,,,,,,,,,"23/Nov/10 19:15;jdcryans;Assigning to Stack and marking it against 0.90;;;","24/Nov/10 04:04;stack;This is the fix:

{code}
pynchon-432:clean_trunk stack$ svn diff 
Index: src/main/ruby/hbase/admin.rb
===================================================================
--- src/main/ruby/hbase/admin.rb        (revision 1038464)
+++ src/main/ruby/hbase/admin.rb        (working copy)
@@ -83,7 +83,7 @@
     # Disables a table
     def disable(table_name)
       return unless enabled?(table_name)
-      @admin.disableTableAsync(table_name)
+      @admin.disableTable(table_name)
     end
 
     #----------------------------------------------------------------------------------------------
{code}

I'm just going to commit.;;;","24/Nov/10 04:07;stack;Committed branch and trunk.;;;","24/Nov/10 04:09;stack;Oh, I checked for other mentions of async I found enable is also async in shell so I changed that too.  I committed the change under this issue.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;","18/Sep/16 07:40;dcswinner001;In 0.98.8 versions,I execute:truncate_preserve at 04:00 AM. every day,frequently i it occur the below error:
The error log is:

truncate_preserve 'ns_ztbd:tb_pis_cshopprice'
Truncating 'ns_ztbd:tb_pis_cshopprice' table (it may take a while):
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/bigdata/software/hbase-0.98.8-hadoop2.4/lib/slf4j-log4j12-1.6.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/bigdata/software/hadoop-2.4.0.4/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
 - Disabling table...
 - Truncating table...
 - Dropping table...

ERROR: org.apache.hadoop.hbase.TableNotDisabledException: ns_ztbd:tb_pis_cshopprice
	at org.apache.hadoop.hbase.master.HMaster.checkTableModifiable(HMaster.java:2077)
	at org.apache.hadoop.hbase.master.handler.TableEventHandler.prepare(TableEventHandler.java:83)
	at org.apache.hadoop.hbase.master.HMaster.deleteTable(HMaster.java:1822)
	at org.apache.hadoop.hbase.master.HMaster.deleteTable(HMaster.java:1832)
	at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:41471)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2027)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:108)
	at org.apache.hadoop.hbase.ipc.FifoRpcScheduler$1.run(FifoRpcScheduler.java:74)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Here is some help for this command:
  Disables, drops and recreates the specified table while still maintaing the previous region boundaries.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
close_region shell command breaks region,HBASE-3267,12480652,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,tlipcon,tlipcon,23/Nov/10 08:49,20/Nov/15 12:43,14/Jul/23 06:06,29/Nov/10 19:47,0.90.0,,,,,,,,,,,0.90.0,,master,regionserver,shell,,,0,,,"It used to be that you could use the close_region command from the shell to close a region on one server and have the master reassign it elsewhere. Now if you close a region, you get the following errors in the master log:

2010-11-23 00:46:34,090 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSING for region ffaa7999e909dbd6544688cc8ab303bd from server haus01.sf.cloudera.com,12020,1290501789693 but region was in  the state null and not in expected PENDI
2010-11-23 00:46:34,530 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: master:60000-0x12c537d84e10062 Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/hbase/unassigned/ffaa7999e909dbd6544688cc8ab303bd
2010-11-23 00:46:34,531 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:60000-0x12c537d84e10062 Retrieved 128 byte(s) of data from znode /hbase/unassigned/ffaa7999e909dbd6544688cc8ab303bd and set watcher; region=usertable,user1951957302,1290501969
2010-11-23 00:46:34,531 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_CLOSED, server=haus01.sf.cloudera.com,12020,1290501789693, region=ffaa7999e909dbd6544688cc8ab303bd
2010-11-23 00:46:34,531 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region ffaa7999e909dbd6544688cc8ab303bd from server haus01.sf.cloudera.com,12020,1290501789693 but region was in  the state null and not in expected PENDIN

and the region just gets stuck closed",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26749,Reviewed,,,,Fri Nov 20 12:43:40 UTC 2015,,,,,,,,,,"0|i0hli7:",100746,,,,,,,,,,,,,,,,,,,,,"23/Nov/10 17:46;streamy;The master doesn't expect a region to be properly closed out on an RS w/o being the one to tell it to do so.

Let me dig in to the code and see what the easiest solution would be.

@Todd... I had plans to start working on new features for 0.92, stop finding bugs!  ;);;;","23/Nov/10 20:09;stack;Bringing into 0.90.0.;;;","24/Nov/10 18:43;stack;I'm taking this one.;;;","24/Nov/10 23:11;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1250/
-----------------------------------------------------------

Review request for hbase and Jonathan Gray.


Summary
-------

So, things are different in the new master.  Close region should close region.  Not close and then reopen.  To close and reopen elsewhere, thats an unassign or a move (both of which were missing from shell but which are added in this patch).  I fixed the close so that its a close that does not touch zk... the region is just closed on the regionserver.  No going to zk makes it so the close no longer makes for complaint.  Close is dangerous though in that the region is now permanently offline (I updated the close help to explain this is so).   To address it being permanently offline, I added a new assign to the shell. 

While in here, I removed commands that no longer make senses such as enable_region and disable_region. 

M src/main/java/org/apache/hadoop/hbase/master/HMaster.java
  Change move implementation so can pass an empty host.
  Empty host means move to random location rather than
  explicit server.
  Added assign, unassign
M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  (clearRegionPlan): Added.
M src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
  Improved move javadoc.  Added assign, unassign.
M src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
  Improved javadoc. Added assign and unassign.
M src/main/ruby/hbase/admin.rb
  Added balancer, balance_switch, assign, unassign, removed
  zk, enable_region and disable_region (the latter make no sense
  anymore now disable/enable is done differently).
D src/main/ruby/shell/commands/zk.rb
A src/main/ruby/shell/commands/assign.rb
A src/main/ruby/shell/commands/balance_switch.rb
D src/main/ruby/shell/commands/disable_region.rb
A src/main/ruby/shell/commands/balancer.rb
A src/main/ruby/shell/commands/unassign.rb
D src/main/ruby/shell/commands/enable_region.rb
A src/main/ruby/shell/commands/move.rb
M src/main/ruby/shell/commands/close_region.rb
  Fixed up help
M src/main/ruby/shell.rb
  Added and removed commands.  


This addresses bug hbase-3267.
    http://issues.apache.org/jira/browse/hbase-3267


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java 1038768 
  trunk/src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java 1038768 
  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1038768 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1038768 
  trunk/src/main/ruby/hbase/admin.rb 1038768 
  trunk/src/main/ruby/shell.rb 1038768 
  trunk/src/main/ruby/shell/commands/assign.rb PRE-CREATION 
  trunk/src/main/ruby/shell/commands/balance_switch.rb PRE-CREATION 
  trunk/src/main/ruby/shell/commands/balancer.rb PRE-CREATION 
  trunk/src/main/ruby/shell/commands/close_region.rb 1038768 
  trunk/src/main/ruby/shell/commands/disable_region.rb 1038768 
  trunk/src/main/ruby/shell/commands/enable_region.rb 1038768 
  trunk/src/main/ruby/shell/commands/move.rb PRE-CREATION 
  trunk/src/main/ruby/shell/commands/unassign.rb PRE-CREATION 
  trunk/src/main/ruby/shell/commands/zk.rb 1038768 

Diff: http://review.cloudera.org/r/1250/diff


Testing
-------

I tested shell here on my little cluster.


Thanks,

stack


;;;","24/Nov/10 23:47;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1250/#review1975
-----------------------------------------------------------


This is great.  I like this much better than hacking up the master transition code.

My main concern is around the exact semantics of assign/unassign (and close).  I think we need to do good javadoc on the HBA methods to describe how you would use these or at least a bit about their behavior.  assign() just does an assign, but unassign() actually clears stuff out.  It seems doing a close() behind the masters back, then asking the master to assign that region, should not work... but it does?


trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
<http://review.cloudera.org/r/1250/#comment6230>

    Is there an open_region?  This assign() goes through the master, so what is the opposite of close_region which doesn't go through the master?
    
    Doesn't close_region now put the master in a bad state, so it won't expect an assignment to be done on a region which it thinks is already assigned?  There is a force on unassign() but not on assign().
    
    In the old master, for HBCK, I added a hook in to the master to clear the in-memory state for a region.  To deal with dupe assignment, I did silent close_regions and then cleared the in-memory state.  Then I triggered a new assignment.



trunk/src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
<http://review.cloudera.org/r/1250/#comment6231>

    this is awesome javadoc.  is there somewhere else we can put this rather than in just the move() API?  Maybe in the HBA class comment or something?  Somewhere we can reference in other javadocs about what a regionname is



trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.cloudera.org/r/1250/#comment6232>

    So you're supposed to call move instead of open_region?  Or why the change in move() though this looks good.



trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.cloudera.org/r/1250/#comment6233>

    Why META and not in-memory state?  Once you hit assign() you rely on the in-memory state anyways?



trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.cloudera.org/r/1250/#comment6238>

    on assign we just do the assignment, but below on unassign() we first clear existing plans and clear from RIT.  why the difference.



trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.cloudera.org/r/1250/#comment6234>

    is this necessary?  should the unassign method taking force deal with anything needed to ""force"" it?



trunk/src/main/ruby/hbase/admin.rb
<http://review.cloudera.org/r/1250/#comment6235>

    zk didn't work?  why is this removed?



trunk/src/main/ruby/shell/commands/assign.rb
<http://review.cloudera.org/r/1250/#comment6236>

    whitespace.  and what exactly are the semantics of this?  what if region is already assigned?
    
    we should document somewhere more specifically what the behavior is of these methods if we're going to expose them to the client and the shell.  neither place really describes what this means and i can imagine users will be doing lots of foot shooting with tools like this.
    
    more importantly, though, i'm trying to understand the use cases for these.  if it's to unbreak stuff, it's not clear to me how exactly you would use it given that the master will reject certain operations in the wrong order.



trunk/src/main/ruby/shell/commands/close_region.rb
<http://review.cloudera.org/r/1250/#comment6237>

    Why would you use close and not unassign/assign/move?  It's because close is done silently?  Should say that if that's the distinction.
    
    Is this comment saying you can use unassign or move after you issue close?  or instead of?



trunk/src/main/ruby/shell/commands/unassign.rb
<http://review.cloudera.org/r/1250/#comment6239>

    this doesn't use encoded region name?
    
    is move then different from the other methods?


- Jonathan



;;;","25/Nov/10 00:33;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-11-24 15:45:32, Jonathan Gray wrote:
bq.  > This is great.  I like this much better than hacking up the master transition code.
bq.  > 
bq.  > My main concern is around the exact semantics of assign/unassign (and close).  I think we need to do good javadoc on the HBA methods to describe how you would use these or at least a bit about their behavior.  assign() just does an assign, but unassign() actually clears stuff out.  It seems doing a close() behind the masters back, then asking the master to assign that region, should not work... but it does?

Well, my notion is that user shouldn't be doing these manual messings any more.  Fixup stuff is now for hbck to do.

Yes, close of a region is done w/o master's involvement.  Rare would you do it.  Yes, an assign will assign a region EVEN IF ALREADY assigned.  Messing in here can get you in trouble.  I was able to manufacture some ugly conditions -- a stuck region trying to assign same server over and over -- but then unassign with a force now clears out RIT and does the right thing.... i.e. we have enough tools to hang ourselves on new master but also the tools to undo.


bq.  On 2010-11-24 15:45:32, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java, line 980
bq.  > <http://review.cloudera.org/r/1250/diff/1/?file=17648#file17648line980>
bq.  >
bq.  >     Is there an open_region?  This assign() goes through the master, so what is the opposite of close_region which doesn't go through the master?
bq.  >     
bq.  >     Doesn't close_region now put the master in a bad state, so it won't expect an assignment to be done on a region which it thinks is already assigned?  There is a force on unassign() but not on assign().
bq.  >     
bq.  >     In the old master, for HBCK, I added a hook in to the master to clear the in-memory state for a region.  To deal with dupe assignment, I did silent close_regions and then cleared the in-memory state.  Then I triggered a new assignment.

No open_region.  Someone can add that later if wanted.  Otherwise, use move to place region on specific server.

On close_region, yes, puts master in bad state but you'd only do close_region when doing fix up of some whack condition.  I was tempted to just remove these commands but since we don't know what states new master could put us in, I'll leave them in for now.

I'll add force to assign so same as unassign.


Regards what you did for old master hbck, you could call close_regions then an unassign with a force would clear memory and get the region assigned elsewhere.

But hbck should be doing this.  Not a user manually, not unless things are really hosed.


bq.  On 2010-11-24 15:45:32, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java, line 138
bq.  > <http://review.cloudera.org/r/1250/diff/1/?file=17649#file17649line138>
bq.  >
bq.  >     this is awesome javadoc.  is there somewhere else we can put this rather than in just the move() API?  Maybe in the HBA class comment or something?  Somewhere we can reference in other javadocs about what a regionname is

I moved the interface doc out to HBA as per your suggestion.


bq.  On 2010-11-24 15:45:32, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java, line 709
bq.  > <http://review.cloudera.org/r/1250/diff/1/?file=17651#file17651line709>
bq.  >
bq.  >     So you're supposed to call move instead of open_region?  Or why the change in move() though this looks good.

Just added it as something you might want to do.  unassign does same thing really.  I could back it out.


bq.  On 2010-11-24 15:45:32, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java, line 994
bq.  > <http://review.cloudera.org/r/1250/diff/1/?file=17651#file17651line994>
bq.  >
bq.  >     Why META and not in-memory state?  Once you hit assign() you rely on the in-memory state anyways?

I only have a region server name, not an HRI which is what the inmemory state is keyed by.   I could iterate the Map I suppose but then I'm thinking it may have been cleared from inmemory state.


bq.  On 2010-11-24 15:45:32, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java, line 996
bq.  > <http://review.cloudera.org/r/1250/diff/1/?file=17651#file17651line996>
bq.  >
bq.  >     on assign we just do the assignment, but below on unassign() we first clear existing plans and clear from RIT.  why the difference.

I made it so we only clear state if force is added to the unassign.


bq.  On 2010-11-24 15:45:32, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java, line 1011
bq.  > <http://review.cloudera.org/r/1250/diff/1/?file=17651#file17651line1011>
bq.  >
bq.  >     is this necessary?  should the unassign method taking force deal with anything needed to ""force"" it?

Its needed for case you called assign multiple times. First assign works.  Second one will be stuck in loop where you ask the region open but the regionserver will say its already open and abort the open.  You are stuck in this loop unless RIT gets cleared.


bq.  On 2010-11-24 15:45:32, Jonathan Gray wrote:
bq.  > trunk/src/main/ruby/hbase/admin.rb, line 390
bq.  > <http://review.cloudera.org/r/1250/diff/1/?file=17652#file17652line390>
bq.  >
bq.  >     zk didn't work?  why is this removed?

Because we have ./bin/hbase zkcli now which is better way of doing this zk interaction.


bq.  On 2010-11-24 15:45:32, Jonathan Gray wrote:
bq.  > trunk/src/main/ruby/shell/commands/assign.rb, line 26
bq.  > <http://review.cloudera.org/r/1250/diff/1/?file=17654#file17654line26>
bq.  >
bq.  >     whitespace.  and what exactly are the semantics of this?  what if region is already assigned?
bq.  >     
bq.  >     we should document somewhere more specifically what the behavior is of these methods if we're going to expose them to the client and the shell.  neither place really describes what this means and i can imagine users will be doing lots of foot shooting with tools like this.
bq.  >     
bq.  >     more importantly, though, i'm trying to understand the use cases for these.  if it's to unbreak stuff, it's not clear to me how exactly you would use it given that the master will reject certain operations in the wrong order.

If already assigned it will reassign regardless.  This is a fix up facility for expert use only (I updated the help to say this more explicitly).

Well, using these new commands you can break things and then unbreak them too.


bq.  On 2010-11-24 15:45:32, Jonathan Gray wrote:
bq.  > trunk/src/main/ruby/shell/commands/close_region.rb, line 28
bq.  > <http://review.cloudera.org/r/1250/diff/1/?file=17657#file17657line28>
bq.  >
bq.  >     Why would you use close and not unassign/assign/move?  It's because close is done silently?  Should say that if that's the distinction.
bq.  >     
bq.  >     Is this comment saying you can use unassign or move after you issue close?  or instead of?

I updated the help.   Added 'caution' and for experts only.

Like I say, I wanted to removed these things altogether but my guess is that one day we'll need them -- at least while hbck is lacking and while all failure modes of new master are not yet known.


bq.  On 2010-11-24 15:45:32, Jonathan Gray wrote:
bq.  > trunk/src/main/ruby/shell/commands/unassign.rb, line 30
bq.  > <http://review.cloudera.org/r/1250/diff/1/?file=17661#file17661line30>
bq.  >
bq.  >     this doesn't use encoded region name?
bq.  >     
bq.  >     is move then different from the other methods?

Yes, move is different from the others.  It tries to make this clear in its documentation.   The Map move uses is keyed by encoded region name.  Assign and unassign go get the HRI from .META. by using passed regionname.


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1250/#review1975
-----------------------------------------------------------



;;;","25/Nov/10 00:33;stack;Marking patch available.;;;","25/Nov/10 00:46;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1250/
-----------------------------------------------------------

(Updated 2010-11-24 16:44:36.709870)


Review request for hbase and Jonathan Gray.


Changes
-------

Addresses Jon' commments mostly by way of bulking up help in shell with warnings and pulling into HBA the javadoc that was out on the HMasterInterface.  Also did stuff like make assign and unassign symmetric both taking a force param.


Summary
-------

So, things are different in the new master.  Close region should close region.  Not close and then reopen.  To close and reopen elsewhere, thats an unassign or a move (both of which were missing from shell but which are added in this patch).  I fixed the close so that its a close that does not touch zk... the region is just closed on the regionserver.  No going to zk makes it so the close no longer makes for complaint.  Close is dangerous though in that the region is now permanently offline (I updated the close help to explain this is so).   To address it being permanently offline, I added a new assign to the shell. 

While in here, I removed commands that no longer make senses such as enable_region and disable_region. 

M src/main/java/org/apache/hadoop/hbase/master/HMaster.java
  Change move implementation so can pass an empty host.
  Empty host means move to random location rather than
  explicit server.
  Added assign, unassign
M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  (clearRegionPlan): Added.
M src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
  Improved move javadoc.  Added assign, unassign.
M src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
  Improved javadoc. Added assign and unassign.
M src/main/ruby/hbase/admin.rb
  Added balancer, balance_switch, assign, unassign, removed
  zk, enable_region and disable_region (the latter make no sense
  anymore now disable/enable is done differently).
D src/main/ruby/shell/commands/zk.rb
A src/main/ruby/shell/commands/assign.rb
A src/main/ruby/shell/commands/balance_switch.rb
D src/main/ruby/shell/commands/disable_region.rb
A src/main/ruby/shell/commands/balancer.rb
A src/main/ruby/shell/commands/unassign.rb
D src/main/ruby/shell/commands/enable_region.rb
A src/main/ruby/shell/commands/move.rb
M src/main/ruby/shell/commands/close_region.rb
  Fixed up help
M src/main/ruby/shell.rb
  Added and removed commands.  


This addresses bug hbase-3267.
    http://issues.apache.org/jira/browse/hbase-3267


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java 1038878 
  trunk/src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java 1038878 
  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1038878 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1038878 
  trunk/src/main/ruby/hbase/admin.rb 1038878 
  trunk/src/main/ruby/shell.rb 1038878 
  trunk/src/main/ruby/shell/commands/assign.rb PRE-CREATION 
  trunk/src/main/ruby/shell/commands/balance_switch.rb PRE-CREATION 
  trunk/src/main/ruby/shell/commands/balancer.rb PRE-CREATION 
  trunk/src/main/ruby/shell/commands/close_region.rb 1038878 
  trunk/src/main/ruby/shell/commands/disable_region.rb 1038878 
  trunk/src/main/ruby/shell/commands/enable_region.rb 1038878 
  trunk/src/main/ruby/shell/commands/move.rb PRE-CREATION 
  trunk/src/main/ruby/shell/commands/unassign.rb PRE-CREATION 
  trunk/src/main/ruby/shell/commands/zk.rb 1038878 

Diff: http://review.cloudera.org/r/1250/diff


Testing
-------

I tested shell here on my little cluster.


Thanks,

stack


;;;","25/Nov/10 01:38;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1250/#review1977
-----------------------------------------------------------

Ship it!


Thanks for making changes, this looks great.

I completely understand the need for these, especially for HBCK, I guess I just think of adding things to the shell as stuff people will try to use.  With all the added doc I think it's fine.  We'll for sure make changes to this stuff as we see what happens in the wild with the new master.

+1 for commit

- Jonathan



;;;","29/Nov/10 19:47;stack;Committed 0.90 and TRUNK.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regionservers waiting for ROOT while Master waiting for RegionServers,HBASE-3265,12480650,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,tlipcon,tlipcon,23/Nov/10 08:18,20/Nov/15 12:41,14/Jul/23 06:06,29/Nov/10 22:31,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"After a cluster disastrophe due to a disconnected switch, I ended up in a state where the master was up with no region servers (see HBASE-3263). When I brought the RS back up, because of the aforementioned bug, the master didn't get itself into a happy state (internal datastructure had some null in it). So I killed the master and started it again. Now, the master is in ""Waiting for region servers to check in"" mode, and the region servers are in the following stack:

        - locked <0x00002aaab1bda5d0> (a org.apache.hadoop.hbase.zookeeper.RootRegionTracker)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForRoot(CatalogTracker.java:177)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:537)
        at java.lang.Thread.run(Thread.java:619)

I imagine what happened is that the RS got through ""tryReportForDuty"" with the old master, but the old master was unable to assign anything due to bad state. So, when it crashed, all the RS were stuck in waitForRoot(), and when I brought the new one up, no one was reporting for duty.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/10 21:40;stack;3265.patch;https://issues.apache.org/jira/secure/attachment/12464902/3265.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26747,Reviewed,,,,Fri Nov 20 12:41:20 UTC 2015,,,,,,,,,,"0|i0hlhr:",100744,,,,,,,,,,,,,,,,,,,,,"23/Nov/10 17:41;streamy;The RSs should also be heartbeating in to the master as well.  Can you post full stack dumps from one of the stuck RS and the master?;;;","23/Nov/10 20:11;stack;Bringing in for triage;;;","29/Nov/10 20:31;streamy;I dug into this one and had a hard time understanding what was preventing the RS from heartbeating in.  I'd need a stack dump to see what was up on that RS.  Otherwise not sure how to address this besides a larger overhaul of reconciling our two data points for RS availability (ZK ephemeral nodes and RPC heartbeats).;;;","29/Nov/10 21:40;stack;Here is a suggestion -- don't have HRS hang waiting on ROOT location. ;;;","29/Nov/10 21:54;streamy;+1.  We need to handle root (un)availability whenever we actually use it, so there _should_ be no reason for this waitOnRoot.  I imagine it was there for a reason at some point but hopefully we've removed the need for this check by handling it properly elsewhere.  grin.;;;","29/Nov/10 22:31;stack;Ok. Committed to trunk and branch.  Lets see if removal breaks stuff elsewhere.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unnecessary Guava Dependency,HBASE-3264,12480647,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,nspiegelberg,nspiegelberg,nspiegelberg,23/Nov/10 08:03,20/Nov/15 12:44,14/Jul/23 06:06,23/Nov/10 19:51,,,,,,,,,,,,0.90.0,0.92.0,mapreduce,,,,,0,,,"Currently, TableMapReduceUtil uses Guava for trivial functionality and addDependencyJars() currently adds Guava by default.  However, this jar is only necessary for the ImportTsv MR job.  This is annoying when naively bundling hbase jar with a MR job because you now need a second dependency jar.  Should default bundle with only critical dependencies and have jobs that need fancy Guava functionality explicitly include them.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/10 04:56;nspiegelberg;HBASE-3264-test.patch;https://issues.apache.org/jira/secure/attachment/12464800/HBASE-3264-test.patch","23/Nov/10 08:04;nspiegelberg;HBASE-3264.patch;https://issues.apache.org/jira/secure/attachment/12460247/HBASE-3264.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26746,Reviewed,,,,Fri Nov 20 12:44:01 UTC 2015,,,,,,,,,,"0|i0hlhj:",100743,,,,,,,,,,,,,,,,,,,,,"23/Nov/10 08:08;tlipcon;I'm sort of -0 on this. I think guava is really useful, and in general, we should be willing to add libraries if they make our lives easier.

Perhaps a better solution would be to package some kind of hbase-client-dependencies.jar with all of our dependencies?;;;","23/Nov/10 08:26;nspiegelberg;@Todd: you're not precluded from adding Guava or whatever libraries to this, but I don't think the default action should be to add libraries that you're not using.  Guava is currently the only dependency under addDependencyJars(Job) that is not essential for basic HBase table operations.  Since addDependencyJars(conf, ...) allows concatenation, you can easily append jars that are necessary for your specific config.  We need to use that ourselves to add in compression jars for HFileOutputFormat.  Note that I used this api to change the ImportTsv job to append the Guava jar, since it is the job that requires it right now.;;;","23/Nov/10 18:16;streamy;I'm fine with adding dependencies/libraries when we are using them, but in general think we should also aim to minimize our dependencies.

So I'm +1 for removing an additional dependency from the job if it's trivial to remove it.

I'm also +1 on a complete client-dep jar if we could get maven to do that for us.;;;","23/Nov/10 19:51;stack;Committed.

I agree we should use libs instead of writing the stuff ourselves but also that we move to minimize dependencies.  In this case, I like the bit of Nicolas footwork that changes a little bit of code so we can cut our client dependencies by 25 (or 33?) percent.;;;","28/Nov/10 04:56;nspiegelberg;sorry about the unit test failure.  I added back the Guava test to insure that you can append dependencies after adding the initial set.;;;","29/Nov/10 20:47;streamy;Looks like the unit test fix didn't make it to 90 branch.;;;","29/Nov/10 21:18;stack;I committed Nicolas's last little patch for the test. I also brought fix back to TRUNK.  I just added hbase-0.90 to apache hudson to build our branch.;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stack overflow in AssignmentManager,HBASE-3263,12480646,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,tlipcon,tlipcon,23/Nov/10 07:59,20/Nov/15 12:42,14/Jul/23 06:06,29/Nov/10 23:46,0.90.0,,,,,,,,,,,0.90.0,,master,,,,,0,,,"My test cluster experienced a switch outage earlier this week which threw the master into a really bad state. In the catch clause of AssignmentManager.assign, we recurse, and if all of the region servers are inaccessible, we do so until we get a stack overflow.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/10 00:44;stack;3263-v3.txt;https://issues.apache.org/jira/secure/attachment/12464930/3263-v3.txt","29/Nov/10 22:26;stack;3263.txt;https://issues.apache.org/jira/secure/attachment/12464907/3263.txt","23/Nov/10 08:00;tlipcon;stackoverflow-log.txt;https://issues.apache.org/jira/secure/attachment/12460246/stackoverflow-log.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26745,Reviewed,,,,Fri Nov 20 12:42:27 UTC 2015,,,,,,,,,,"0|i0hlhb:",100742,,,,,,,,,,,,,,,,,,,,,"23/Nov/10 08:00;tlipcon;Here's a log showing the beginning of the runaway recursion. It goes like this until it gets a stack overflow error.;;;","23/Nov/10 08:03;tlipcon;Shortly after the StackOverflowError it also started spitting this exception:

2010-11-19 12:09:50,366 WARN org.apache.hadoop.hbase.master.AssignmentManager: Failed assignment of usertable,,1289960558114.03110b4c3c0b24fa1c920ec7669d03a6. to serverName=haus03.sf.cloudera.com,60020,1289890926773, load=(requests=0, regions=11, usedHeap=5403, maxHeap=8185), trying to assign elsewhere instead
java.lang.NullPointerException
  at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.sendParam(HBaseClient.java:485)
  at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:733)
  at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
  at $Proxy8.openRegion(Unknown Source)
  at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:537)
  at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:830)
;;;","23/Nov/10 08:05;tlipcon;And also thereafter lots of these:

java.lang.NullPointerException
  at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.sendParam(HBaseClient.java:485)
  at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:733)
  at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
  at $Proxy8.getRegionInfo(Unknown Source)
  at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRegionLocation(CatalogTracker.java:416)
  at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:270)
  at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:322)

So somehow we borked a null into one of our maps, it seems;;;","29/Nov/10 22:26;stack;Patch to bound the attempts at reassign recursions. Not pretty but should prevent this runaway from happening.;;;","29/Nov/10 23:40;streamy;+1 for commit.  seems like we could do without an AtomicInteger but minor.;;;","29/Nov/10 23:46;stack;I committed to trunk and branch (Removed atomicinteger -- that was a little silly).  Thanks for the review Jon.;;;","30/Nov/10 00:07;tlipcon;Dare I ask why not just make it into a loop? :);;;","30/Nov/10 00:42;stack;Let me do that.  I hated that recursion thingy.;;;","30/Nov/10 00:44;stack;Changed recursion to loop.  Here is what I applied to trunk and 0.90 branch.;;;","30/Nov/10 00:48;streamy;Wait!  Are you missing the return in the normal flow?  Seems like if successful sendRegionOpen we will loop.;;;","30/Nov/10 00:55;stack;Duh.. thanks Jon.  Check what I just committed.
Added a break.;;;","30/Nov/10 00:58;streamy;Seems right.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestHMasterRPCException uses non-ephemeral port for master,HBASE-3262,12480632,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,23/Nov/10 03:35,20/Nov/15 12:43,14/Jul/23 06:06,23/Nov/10 21:54,0.90.0,,,,,,,,,,,0.90.0,0.92.0,,,,,,0,,,TestHMasterRPCException instantiates an HMaster but doesn't use an ephemeral port which can cause the test to fail if port already in use.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/10 03:36;streamy;HBASE-3262-v1.patch;https://issues.apache.org/jira/secure/attachment/12460229/HBASE-3262-v1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26744,Reviewed,,,,Fri Nov 20 12:43:16 UTC 2015,,,,,,,,,,"0|i0hlh3:",100741,,,,,,,,,,,,,,,,,,,,,"23/Nov/10 03:36;streamy;Uses ephemeral port for master and cleans up unused imports causing warnings.;;;","23/Nov/10 03:36;streamy;Maybe we should push this setting of port 0 as master/rs ports into constructor of HBaseTestingUtility?;;;","23/Nov/10 20:03;stack;This patch is fine -- especially if you want to apply this to 0.90 for second RC -- but yeah, best if HTU does this for all HMaster instantiations.;;;","23/Nov/10 21:54;streamy;Committed to branch and trunk.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE out of HRS.run at startup when clock is out of sync,HBASE-3261,12480621,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,22/Nov/10 23:31,20/Nov/15 12:44,14/Jul/23 06:06,24/Nov/10 18:36,,,,,,,,,,,,0.90.0,0.92.0,,,,,,0,,,"This is what I get when I start a region server that's not properly sync'ed:

{noformat}
Exception in thread ""regionserver60020"" java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:603)
	at java.lang.Thread.run(Thread.java:637)
{noformat}

I this case the line was:
{noformat}
hlogRoller.interruptIfNecessary();
{noformat}

I guess we could add a bunch of other null checks.

The end result is the same, the RS dies, but I think it's misleading.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/10 22:00;jdcryans;HBASE-3261.patch;https://issues.apache.org/jira/secure/attachment/12460314/HBASE-3261.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26743,Reviewed,,,,Fri Nov 20 12:44:03 UTC 2015,,,,,,,,,,"0|i0hlgv:",100740,,,,,,,,,,,,,,,,,,,,,"22/Nov/10 23:52;streamy;Yeah this is something I had to add a lot of checks for in HMaster as well.  +1 on adding null checks before we stop/interrupt stuff.;;;","23/Nov/10 20:21;stack;+1 on adding nullcheck.

The sequence in which stuff is started was undergoing high velocity change up to the end.  Not surprised holes if start sequence aborted midway.;;;","23/Nov/10 22:00;jdcryans;The patch attached just adds a bunch of null checks.;;;","24/Nov/10 03:46;stack;+1;;;","24/Nov/10 18:36;jdcryans;Committed to branch and trunk, thanks for the review Stack.;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't kill the region servers when they wait on the master or the cluster state znode,HBASE-3259,12480603,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,22/Nov/10 18:51,20/Nov/15 12:41,14/Jul/23 06:06,23/Nov/10 23:48,,,,,,,,,,,,0.90.0,0.92.0,,,,,,0,,,"With a situation like HBASE-3258, it's easy to have the region servers stuck on waiting for either the master or the cluster state znode since it has no timeout. You have to kill -9 them to have them shutting down. This is very bad for usability.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/10 20:08;jdcryans;HBASE-3259.patch;https://issues.apache.org/jira/secure/attachment/12460200/HBASE-3259.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26742,Reviewed,,,,Fri Nov 20 12:41:38 UTC 2015,,,,,,,,,,"0|i0hlgf:",100738,,,,,,,,,,,,,,,,,,,,,"22/Nov/10 18:57;streamy;Like you said, maybe this is bad for usability, not sure this is blocking or a bug.

You want to make it so you can just 'kill' without -9?  Or you want to add timeout on RS on startup?

The former seems no different for usability.  The latter might be okay but not sure it's expected behavior.  What would the default timeout be?;;;","22/Nov/10 19:06;jdcryans;bq. Like you said, maybe this is bad for usability, not sure this is blocking or a bug.

I foresee that a majority of our new users will hit this issue if they have any sort of trouble setting up their cluster, so I think this is a blocker.

bq. You want to make it so you can just 'kill' without -9?

Not just kill, but also ""hbase-daemon.sh stop regionserver"" since it also hangs. Imagine a few machines in that state where you have to manually kill -9 every one of them.

bq. Or you want to add timeout on RS on startup?

A timeout to the blocking, but that we retry until either the data is available or the region server is stopped. Like 1 or 2 seconds. 

I'm currently writing the patch.;;;","22/Nov/10 20:08;jdcryans;Small refactoring in HRS to handle the timeout and check to stopped, I thought of doing it down in ZooKeeperNodeTracker but I'm not sure if we want that behavior everywhere.;;;","23/Nov/10 20:01;stack;+1 on application to 0.90 and TRUNK.;;;","23/Nov/10 21:47;jdcryans;Committed to trunk and 0.90, thanks Stack.;;;","23/Nov/10 23:48;stack;Committed.  Closing.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOF when version file is empty,HBASE-3258,12480600,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,22/Nov/10 18:14,20/Nov/15 12:44,14/Jul/23 06:06,23/Nov/10 21:46,,,,,,,,,,,,0.90.0,0.92.0,,,,,,0,,,"I somehow was able to get an empty hbase.version file on a test machine and when I start HBase I see:

{noformat}
starting master, logging to /data/jdcryans/git/hbase/bin/../logs/hbase-jdcryans-master-hbasedev.out
Exception in thread ""master-hbasedev:60000"" java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.HMaster.stopServiceThreads(HMaster.java:559)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:286)
{noformat}

And in the master's log:

{noformat}
2010-11-22 10:08:43,003 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.
java.io.EOFException
        at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
        at java.io.DataInputStream.readUTF(DataInputStream.java:572)
        at org.apache.hadoop.hbase.util.FSUtils.getVersion(FSUtils.java:151)
        at org.apache.hadoop.hbase.util.FSUtils.checkVersion(FSUtils.java:170)
        at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:226)
        at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:104)
        at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:89)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:337)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:273)
2010-11-22 10:08:43,006 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
{noformat}

I thought that that kind of issue was solved a long time ago, but somehow it's there again. I'll fix by handling the EOF and also will look at that ugly NPE.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/10 19:46;jdcryans;HBASE-3258.patch;https://issues.apache.org/jira/secure/attachment/12460196/HBASE-3258.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26741,Reviewed,,,,Fri Nov 20 12:44:01 UTC 2015,,,,,,,,,,"0|i0hlg7:",100737,,,,,,,,,,,,,,,,,,,,,"22/Nov/10 18:47;jdcryans;For the record, the reason is that I was testing 0.90 with 0.20-append and since both are currently incompatible at the data transfer level (ugly ugly), the master is able to create the file but unable to write to. This HDFS-724 situation is bad.;;;","22/Nov/10 19:46;jdcryans;Simple patch that special cases the EOF and prints a message. Also I fixed the NPE.;;;","22/Nov/10 20:06;tlipcon;When we create the .version file, we should create it in a tmp location and then move it into place. It's probably empty in the case that a server crashes between writing and closing.;;;","23/Nov/10 19:58;stack;+1 J-D.

I made HBASE-3270 to accomodate Todd's sensible suggestion above.;;;","23/Nov/10 21:46;jdcryans;Committed to trunk and 0.90, thanks Stack.;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift's missing from all the repositories in pom.xml,HBASE-3253,12480466,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,19/Nov/10 23:25,20/Nov/15 12:43,14/Jul/23 06:06,20/Nov/10 00:11,,,,,,,,,,,,0.90.0,0.92.0,,,,,,0,,,"Follow up to what I said on the mailing list:

{quote}
Currently both trunk and 0.90's pom.xml are incomplete, we were
relying on Ryan's repo have the thrift pom but now that it was changed
to Stack's new comers cannot compile the project since that pom is
missing. Reported by kzk9 on IRC.

So either we had Ryan's repo back in the pom, or Stack copies the
files to his own repo, or we add a FB's repo that has it.
{quote}

I'm going to do it quickly by adding back Ryan's repo, it's pretty harmless.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26740,,,,,Fri Nov 20 12:43:34 UTC 2015,,,,,,,,,,"0|i0hlfb:",100733,,,,,,,,,,,,,,,,,,,,,"20/Nov/10 00:11;jdcryans;Small change, committed.;;;","20/Nov/10 09:24;ryanobjc;my repo is g+w for 'hbase' on people.apache.org, feel free to drop
things in there!
;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestZooKeeperNodeTracker sometimes fails due to a race condition in test notification,HBASE-3252,12480446,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,,ghelmling,ghelmling,19/Nov/10 20:31,20/Nov/15 12:41,14/Jul/23 06:06,19/Nov/10 20:37,0.90.0,,,,,,,,,,,0.90.0,,test,,,,,0,,,"TestZooKeeperNodeTracker sometimes fails with errors like the following:

{noformat}
Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.738 sec <<< FAILURE!
testNodeTracker(org.apache.hadoop.hbase.zookeeper.TestZooKeeperNodeTracker)  Time elapsed: 0.17 sec  <<< FAILURE!
java.lang.AssertionError: 
        at org.junit.Assert.fail(Assert.java:91)
        at org.junit.Assert.assertTrue(Assert.java:43)
        at org.junit.Assert.assertTrue(Assert.java:54)
        at org.apache.hadoop.hbase.zookeeper.TestZooKeeperNodeTracker.testNodeTracker(TestZooKeeperNodeTracker.java:203)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{noformat}

Running the test locally this can happen as much as 25-50% of the time.

It looks like this is due to a basic race condition in the way the test is structured.  The test code uses:

{code}
    // Wait for zk event to be processed
    zkListener.waitForDataChange();
{code}

But, since zkListener is instantiated (and registered with ZooKeeperWatcher) prior to secondTracker (which is always the source of the failure), zkListener will be notified first of the change and there is a race condition between the subsequent test assertions and the secondTracker notification.

Attaching a patch with a simple fix of just instantiating secondTracker prior to zkListener so that it's registered (and notified) first.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/10 20:33;ghelmling;HBASE-3252.patch;https://issues.apache.org/jira/secure/attachment/12460052/HBASE-3252.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26739,Reviewed,,,,Fri Nov 20 12:41:06 UTC 2015,,,,,,,,,,"0|i0hlf3:",100732,,,,,,,,,,,,,,,,,,,,,"19/Nov/10 20:33;ghelmling;Trivial patch to make the test reliably pass.  Just moves secondTracker registration ahead of zkListener.;;;","19/Nov/10 20:36;ghelmling;The problem fixed is only with the test code itself, not the underlying ZK notification, so it's not critical.  But marking for 0.90.0 so that we can get all tests passing on hudson.  Feel free to boot it if you disagree.;;;","19/Nov/10 20:37;apurtell;Committed. Thanks for the patch Gary.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typing 'help shutdown' in the shell shouldn't shutdown the cluster,HBASE-3249,12480394,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,jdcryans,jdcryans,19/Nov/10 00:00,20/Nov/15 12:41,14/Jul/23 06:06,23/Nov/10 21:37,,,,,,,,,,,,0.90.0,,,,,,,0,,,"_hp_ on IRC found out the bad way that typing 'help shutdown' actually gives you the full help... and shuts down the cluster. I don't really understand why we process both commands, putting against 0.90.0 if anyone has an idea.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/10 21:22;stack;shutdown.txt;https://issues.apache.org/jira/secure/attachment/12460306/shutdown.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26737,Reviewed,,,,Fri Nov 20 12:41:31 UTC 2015,,,,,,,,,,"0|i0hlen:",100730,,,,,,,,,,,,,,,,,,,,,"19/Nov/10 00:12;tlipcon;<3 ruby;;;","23/Nov/10 21:22;stack;I took a quick look.  'help shutdown' is being interpreted as two commands, an 'help' followed by a 'shutdown'.  The interpreter is running the 'shutdown' command first and then 'help'.  'help' is a native IRB that we did some hackery to override.  The 'shutdown' is part of our command-set injection.  I'd need to dig in and spend some time figuring how I hacked this up.

For 0.90.0, I propose the following patch where we just remove the shutdown command.  Shutdown from inside the shell seems 'odd' to me.  Meantime I'll open an issue to look into how this help stuff is being interpreted by IRB.  Its kinda ugly.  If you do 'help get' you get first complaint that get has not been passed enough parameters and then the help output followed by the total help output.  Ugly.  We need to fix.  But don't think it blocker on 0.90 RC.;;;","23/Nov/10 21:22;stack;Oh, I'm looking for a +1;;;","23/Nov/10 21:23;jdcryans;+1;;;","23/Nov/10 21:37;stack;Committed.  Thanks for review j-d.;;;","24/Nov/10 10:44;larsgeorge;An after the fact note: I have heard from a few users that they could only stop a cluster using shutdown, i.e. the scripts to do that did not work. So it seemed like a good command to have.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
check to see if we exceeded hbase.regionserver.maxlogs limit is incorrect,HBASE-3241,12480164,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,kannanm,kannanm,kannanm,17/Nov/10 00:18,20/Nov/15 12:41,14/Jul/23 06:06,18/Nov/10 00:55,0.90.0,,,,,,,,,,,0.90.0,0.92.0,,,,,,0,,,"In HLog.java:cleanOldLogs(), the number of logs left after archiving of old logs is computed as:

{code}
    int logCount = this.outputfiles.size() - logsToRemove;
{code}

However, the archival itself already removes the files that were archived from the ""this.outputfiles"" map. 


So shouldn't the above logic simply be the following?

{code}
  int logCount = this.outputfiles.size();
{code}

",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/10 16:07;kannanm;HBASE-3239-and-3241.txt;https://issues.apache.org/jira/secure/attachment/12459801/HBASE-3239-and-3241.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26734,Reviewed,,,,Fri Nov 20 12:41:16 UTC 2015,,,,,,,,,,"0|i0hldj:",100725,,,,,,,,,,,,,,,,,,,,,"17/Nov/10 00:24;ryanobjc;i think kannan is correct here...;;;","17/Nov/10 14:48;streamy;Marking as blocker against 0.90.  We should definitely get this and HBASE-3241 in.;;;","17/Nov/10 16:07;kannanm;Supplied patch addresses both this issue and HBASE-3239.;;;","17/Nov/10 19:29;jdcryans;Would it be possible to add a unit test for HBASE-3239?

Also I'm wondering... when you handle that null, do you start accumulating HLogs or they still get cleaned somehow?;;;","18/Nov/10 00:26;jdcryans;So scratch the unit test, we already have one that got the NPE. +1 on the patch;;;","18/Nov/10 00:55;jdcryans;Committed to trunk and 0.90, thanks Kannan!;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle null regions to flush in HLog.cleanOldLogs,HBASE-3239,12480142,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,kannanm,khemani,khemani,16/Nov/10 20:22,20/Nov/15 12:42,14/Jul/23 06:06,18/Nov/10 00:55,0.90.0,,,,,,,,,,,0.90.0,0.92.0,regionserver,,,,,0,,,"Note from Kannan

findMemstoresWithEditsEqualOrOlderThan() can return NULL it seems like. And we don't check NULL, before ""region.length"".

      regions = findMemstoresWithEditsEqualOrOlderThan(this.outputfiles.firstKey(),
        this.lastSeqWritten);
      StringBuilder sb = new StringBuilder();
      for (int i = 0; i < regions.length; i++) {

===


Stack Trace

2010-11-15 19:19:54,258 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: LRU Stats: total=6.1 GB, free=1.71 GB, max=7.81 GB, blocks=385740, accesses=7020255, hits=6329399, hitRatio=90.15%%, cachingAccesses=6765050, cachingHits=6329399, cachingHitsRatio=93.56%%, evictions=1, evicted=49911, evictedPerRun=49911.0
2010-11-15 19:21:05,204 INFO org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: Using syncFs -- HDFS-200
2010-11-15 19:21:05,211 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Roll /PUMAHBASE001-SNC5-HBASE/.logs/pumahbase042.snc5.facebook.com,60020,1289856892583/10.38.28.57%3A60020.1289877154987, entries=649004, filesize=255069060. New hlog /PUMAHBASE001-SNC5-HBASE/.logs/pumahbase042.snc5.facebook.com,60020,1289856892583/10.38.28.57%3A60020.1289877665062
2010-11-15 19:21:05,222 ERROR org.apache.hadoop.hbase.regionserver.LogRoller: Log rolling failed
java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.wal.HLog.cleanOldLogs(HLog.java:648)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.rollWriter(HLog.java:528)
        at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:94)
2010-11-15 19:21:05,226 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server serverName=pumahbase042.snc5.facebook.com,60020,1289856892583, load=(requests=3476, regions=40, usedHeap=8388, maxHeap=15987): Log rolling failed
java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.wal.HLog.cleanOldLogs(HLog.java:648)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.rollWriter(HLog.java:528)
        at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:94)
2010-11-15 19:21:05,227 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Dump of metrics: request=1264.5834, regions=40, stores=70, storefiles=98, storefileIndexSize=35, memstoreSize=83, compactionQueueSize=0, usedHeap=8370, maxHeap=15987, blockCacheSize=6593768536, blockCacheFree=1788154792, blockCacheCount=388283, blockCacheHitRatio=90, blockCacheHitCachingRatio=93
2010-11-15 19:21:05,227 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: STOPPED: Log rolling failed
2010-11-15 19:21:05,227 INFO org.apache.hadoop.hbase.regionserver.LogRoller: LogRoller exiting.
2010-11-15 19:21:07,255 INFO org.apache.hadoop.ipc.HBaseServer: Stopping server on 60020


===


",,larsfrancke,streamy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26733,Reviewed,,,,Fri Nov 20 12:42:22 UTC 2015,,,,,,,,,,"0|i0hldb:",100724,,,,,,,,,,,,,,,,,,,,,"17/Nov/10 01:45;kannanm;While the fix seems straightforward, i.e. add a safety check before going into the loop, I still haven't been able to explain how this case arises, namely, that we have a lot of log files accumulated, but we can't find a single region/memstore that contains some edits present in the oldest log.;;;","17/Nov/10 14:27;streamy;Making blocker against 0.90;;;","17/Nov/10 14:46;streamy;@Kannan, I think the case you're describing and what is seen here is a fairly ""normal"" behavior, though it may not be seen often in practice if you have good distribution of writes across regions.

Two possible but maybe not common/real world examples:
- All edits in the oldest log are for regions that have moved to other servers
- You have 4 regions, flush size is 64MB, hlog size is 64MB, you hold max 10 logs.  With even distribution of writes, you would expect only the most recent 4 logs (latest 256MB of writes) to have any data which is present in the current memstores (maximum total size of 64*4=256MB).  Eviction of the oldest logs will not contain any edits that are in the memstores of these regions.;;;","17/Nov/10 14:53;streamy;Assigning to Kannan who made a fix for this in our internal branch.;;;","17/Nov/10 16:11;kannanm;The patch uploaded as part of HBASE-3241 also contains a fix for this issue.;;;","17/Nov/10 17:32;jdcryans;I think a possible reason why we haven't seen this before is HBASE-3198 & HBASE-3208, since we were archiving the logs prematurely. 

Also regarding:

bq. All edits in the oldest log are for regions that have moved to other servers

I think it could also happen with splits.;;;","17/Nov/10 20:35;kannanm;Jonathan/JD: Still not sure about the examples. So my understanding is as follows. 

cleanOldLogs() has two stages:

#1. remove any old logs which contain edits older than any outstanding/unflushed edits in a memstore.
#2. if at this point we are still over the threshold, flush regions containing edits from the oldest log.

For the case you mention: ""All edits in the oldest log are for regions that have moved to other servers"" -- wouldn't stage #1 itself have removed the said log? At the end of stage #1, any logs remaining should at least have one memstore that's preventing that log from being reclaimed in stage #1, correct?
 ;;;","17/Nov/10 22:12;kannanm;Note: The cluster this happened on didn't have HBASE-3208 & HBASE-3198 fixes. ;;;","18/Nov/10 00:24;jdcryans;So we confirmed that the cluster only had HBASE-3198 and NOT HBASE-3208, which fixed this particular NPE.

@Kannan, would you mind rescoping the title of this jira to something more around handling the NPE rather than the NPE itself?;;;","18/Nov/10 00:48;jdcryans;About to commit, changing the title.;;;","18/Nov/10 00:55;jdcryans;Committed to trunk and 0.90, thanks Kannan!;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase needs to have the CREATE permission on the parent of its ZooKeeper parent znode,HBASE-3238,12480107,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,posix4e,herberts,herberts,16/Nov/10 12:48,20/Nov/15 12:43,14/Jul/23 06:06,02/Apr/11 14:41,0.90.0,,,,,,,,,,,0.92.0,,,,,,,0,,,"Upon startup, HBase attempts to create its zookeeper.parent.znode in ZooKeeper, it does so using ZKUtil.createAndFailSilent which as its name seems to imply will fail silent if the znode exists. But if HBase does not have the CREATE permission on its zookeeper.parent.znode parent znode then the create attempt will fail with a org.apache.zookeeper.KeeperException$NoAuthException and will terminate the process.

In a production environment where ZooKeeper has a managed namespace it is not possible to give HBase CREATE permission on the parent of its parent znode.

ZKUtil.createAndFailSilent should therefore be modified to check that the znode exists using ZooKeeper.exist prior to attempting to create it.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/11 23:44;posix4e;1;https://issues.apache.org/jira/secure/attachment/12474568/1","25/Mar/11 17:20;posix4e;HBASE-3238-v2.patch;https://issues.apache.org/jira/secure/attachment/12474632/HBASE-3238-v2.patch","28/Mar/11 17:50;posix4e;HBASE-3238-v3.patch;https://issues.apache.org/jira/secure/attachment/12474795/HBASE-3238-v3.patch","25/Mar/11 00:01;posix4e;HBASE-3238.patch;https://issues.apache.org/jira/secure/attachment/12474571/HBASE-3238.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26732,Reviewed,,,,Fri Nov 20 12:43:58 UTC 2015,,,,,,,,,,"0|i0hld3:",100723,,,,,,,,,,,,,,,,,,,,,"24/Mar/11 23:44;posix4e;So I wrote a test to simulate this, although the suggested fix doesn't seem to work unless I am misunderstanding it. I'll upload a patch and maybe you can tell me what I'm doing wrong.

;;;","25/Mar/11 00:02;posix4e;Ok I think I got what he was saying. Two things. Mathias should confirm I fixed what he saw broken. Also someone who is a commiter should decide if this is something we really want, it limits hbase to always write under /hbase;;;","25/Mar/11 04:10;stack;Hey honey.  Is there a create that works even if node already exists?

What is this '+   * @param str String to amend.   -1'

When you say above that it limits to write under /hbase, you don't mean exactly /hbase, you mean whatever is configured as cluster home in zk?;;;","25/Mar/11 17:29;posix4e;> Is there a create that works even if node already exists?
I talked to henry today, we either need to catch it(Which will catch a larger set of acl exceptions), or check exists first (which may have performance implications). Alternatively we can make our API more fine grained and replace all the create /home.dir calls with this new fine grained call. He suggested we catch this exception. Also do we think it should be createOrFailSilent not and fail silent?

> What is this '+ * @param str String to amend. -1'
Me being retarded, it's now removed.

> When you say above that it limits to write under /hbase, you don't mean exactly /hbase, you mean whatever is configured as cluster home in zk?
Correct I clarified the comment.

;;;","25/Mar/11 20:05;posix4e;I just realized what about we just catch the noauth exception, and throw another error if the node doesn't exist. This would save us an exist in the normal case?;;;","25/Mar/11 21:04;herberts;Catching the noauth would do the trick I guess, but what gain compared with the exist call when the znode has already been created?;;;","25/Mar/11 21:38;posix4e;I think the idea is to only call exist if we need to.;;;","28/Mar/11 17:50;posix4e;This one only checks for existence in the exception case.;;;","01/Apr/11 17:02;stack;+1 on patch.  Mathias, what do you think?;;;","02/Apr/11 08:56;herberts;LGTM, maybe the comment '// If we failed to create the file rethrow.' could be changed to '// If we failed to create the file and it does not already exist, rethrow.';;;","02/Apr/11 14:41;stack;Applied to TRUNK.  Thanks for the patch Alex (and for the review Mathias -- I added in your suggested comment change on commit).;;;","06/Apr/11 03:51;hudson;Integrated in HBase-TRUNK #1831 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1831/])
    ;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split request accepted -- BUT CURRENTLY A NOOP,HBASE-3237,12480035,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,tlipcon,tlipcon,16/Nov/10 07:13,20/Nov/15 12:42,14/Jul/23 06:06,19/Nov/10 18:07,0.90.0,,,,,,,,,,,0.90.0,0.92.0,,,,,,0,,,"The ""split"" button from the web UI displays this message and indeed seems to do nothing.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/10 00:11;jdcryans;HBASE-3237.patch;https://issues.apache.org/jira/secure/attachment/12459958/HBASE-3237.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26731,,,,,Fri Nov 20 12:42:49 UTC 2015,,,,,,,,,,"0|i0hlcv:",100722,,,,,,,,,,,,,,,,,,,,,"17/Nov/10 19:42;jdcryans;Assigning to myself, also after talking with Todd he said that we shouldn't offer the regionOrTable methods but splitRegion and splitTable. At the same time, I think some methods in HBA were removed but weren't deprecated so we need to add them back (thinking of the modifyTable with 3 arguments).;;;","19/Nov/10 00:11;jdcryans;Patch that only fixes table.jsp for the moment, the shell was ok.

I tried bringing back some methods from the old API in order to deprecate them, but it was messy. All the other methods like flush and compact changed signature but I'd be surprised if anyone used them directly to act to specific regions since it's hard/weird to do programmatically.;;;","19/Nov/10 18:07;jdcryans;Committed to 0.90 and trunk.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Intermittent incrementColumnValue failure in TestHRegion,HBASE-3235,12479902,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,ghelmling,ghelmling,ghelmling,14/Nov/10 08:30,20/Nov/15 12:40,14/Jul/23 06:06,16/Nov/10 21:23,0.90.0,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"I first saw this in a Hudson build, but can reproduce locally with enough test runs (5-10 times):

{noformat}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hbase.regionserver.TestHRegion
-------------------------------------------------------------------------------
Tests run: 51, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 39.413 sec <<< FAILURE!
testIncrementColumnValue_UpdatingInPlace(org.apache.hadoop.hbase.regionserver.TestHRegion)  Time elapsed: 0.079 sec  <<< FAILURE!
junit.framework.AssertionFailedError: expected:<1> but was:<2>
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.failNotEquals(Assert.java:283)
        at junit.framework.Assert.assertEquals(Assert.java:64)
        at junit.framework.Assert.assertEquals(Assert.java:195)
        at junit.framework.Assert.assertEquals(Assert.java:201)
        at org.apache.hadoop.hbase.regionserver.TestHRegion.testIncrementColumnValue_UpdatingInPlace(TestHRegion.java:1889)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{noformat}

Alternately, the failure can also show up in testIncrementColumnValue_UpdatingInPlace_Negative():
{noformat}
testIncrementColumnValue_UpdatingInPlace_Negative(org.apache.hadoop.hbase.regionserver.TestHRegion)  Time elapsed: 0.03 sec  <<< FAILURE!
junit.framework.AssertionFailedError: expected:<2> but was:<3>
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.failNotEquals(Assert.java:283)
        at junit.framework.Assert.assertEquals(Assert.java:64)
        at junit.framework.Assert.assertEquals(Assert.java:130)
        at junit.framework.Assert.assertEquals(Assert.java:136)
        at
org.apache.hadoop.hbase.regionserver.TestHRegion.assertICV(TestHRegion.java:2081)
        at
org.apache.hadoop.hbase.regionserver.TestHRegion.testIncrementColumnValue_UpdatingInPlace_Negative(TestHRegion.java:1990)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{noformat}",,khemani,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26729,,,,,Fri Nov 20 12:40:32 UTC 2015,,,,,,,,,,"0|i0hlcn:",100721,,,,,,,,,,,,,,,,,,,,,"15/Nov/10 23:52;ghelmling;Some further info on this.  I added logging of the contents of store.memstore.kvset when kvset.size() > 1 in testIncrementColumnValue_UpdatingInPlace(), like so:

{code}
    long value = 1L;
    long amount = 3L;

    Put put = new Put(row);
    put.add(fam1, qual1, Bytes.toBytes(value));
    region.put(put);

    long result = region.incrementColumnValue(row, fam1, qual1, amount, true);

    assertEquals(value+amount, result);

    Store store = region.getStore(fam1);
    // ICV removes any extra values floating around in there.
    if (store.memstore.kvset.size() > 1) {
      for (KeyValue kv : store.memstore.kvset) {
        LOG.debug(""memstore.kvset: row=""+Bytes.toString(kv.getRow()) +
            "" fam=""+Bytes.toString(kv.getFamily())+"" col=""+Bytes.toString(kv.getQualifier())+
            "" val=""+Bytes.toLong(kv.getValue())+"" ts=""+kv.getTimestamp()+"" memstore_ts=""+kv.getMemstoreTS());
      }
    }
    assertEquals(1, store.memstore.kvset.size());
    assertTrue(store.memstore.snapshot.isEmpty());
{code}

With this in place, I get the following output for the failure case:

{noformat}
2010-11-15 15:36:24,242 DEBUG [main] regionserver.TestHRegion(1891): memstore.kvset: row=rowA fam=colfamily1 col=qual1 val=1 ts=1289864184241 memstore_ts=1
2010-11-15 15:36:24,242 DEBUG [main] regionserver.TestHRegion(1891): memstore.kvset: row=rowA fam=colfamily1 col=qual1 val=4 ts=1289864184241 memstore_ts=0
{noformat}

So it seems like it's only happening when the timestamps for the initial put and the ICV are identical.  In this case, the put gets memstoreTS=1 (from RWCC.getWriteNumber()), but the ICV memstoreTS=0 in MemStore.upsert(List).

So, in MemStore.upsert(KeyValue), when we call kvset.tailSet(kv), we're missing the initial put, because KeyValue.KV_COMPARATOR is evaluating it as less than the ICV put, by inverting the memstoreTS comparison.

Given that that's the cause, what is the correct fix?  Should the ICV put get using RWCC.getWriteNumber() as well?  Or should we be obtaining the tailset without regards to the memstoreTS?;;;","16/Nov/10 07:11;hbasereviewboard;Message from: ""Gary Helmling"" <ghelmling@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1224/
-----------------------------------------------------------

Review request for hbase and Ryan Rawson.


Summary
-------

Fix for MemStore.upsert(KeyValue) to start the kvset.tailSet() of potential KVs to remove at the beginning of entries for the row/family/qualifier combination, ignoring timestamp to prevent Puts being skipped based on timestamp alone and masking the ICV.


This addresses bug HBASE-3235.
    http://issues.apache.org/jira/browse/HBASE-3235


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java b7409b0 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java 7640997 

Diff: http://review.cloudera.org/r/1224/diff


Testing
-------

Added a new test: TestHRegion.testIncrementColumnValue_UpdatingInPlace_TimestampClobber() to recreate the existing failure condition: 1) put to a row/family/qualifier, 2) ICV to the same row/family/qualifier with the same timestamp.  This test fails consistently without the patch to MemStore.

With the patch to MemStore, the new test case consistently passes.  I also ran TestHRegion 15+ times and saw no more intermittent failures of testIncrementColumnValue_UpdatingInPlace().  Previously this was failing every 5 or so test runs, so this seems a pretty good indication it's fixed.

I also ran through the full test suite on 0.90 and all passed except for an error in TestHLog...


Thanks,

Gary


;;;","16/Nov/10 08:47;hbasereviewboard;Message from: ""Andrew Purtell"" <apurtell@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1224/#review1937
-----------------------------------------------------------

Ship it!


I believe the TestHLog failure noted was on our Hudson. If so it is unrelated to this change. 

- Andrew



;;;","16/Nov/10 17:07;ghelmling;Bumping up as a blocker and marking for 0.90.0 since it represents possible data loss.;;;","16/Nov/10 21:23;ryanobjc;committed to branch and trunk, thanks for the work Gary!;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"hdfs-724 ""breaks"" TestHBaseTestingUtility multiClusters",HBASE-3234,12479898,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,hairong,stack,stack,14/Nov/10 06:51,20/Nov/15 12:43,14/Jul/23 06:06,30/Nov/10 05:24,,,,,,,,,,,,0.90.0,,,,,,,0,,,"We upgraded our hadoop jar in TRUNK to latest on 0.20-append branch.  TestHBaseTestingUtility started failing reliably.  If I back out hdfs-724, the test passes again.  This issue is about figuring whats up here.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/10 05:22;stack;3234.txt;https://issues.apache.org/jira/secure/attachment/12464937/3234.txt","15/Nov/10 01:49;stack;org.apache.hadoop.hbase.TestHBaseTestingUtility-output.txt;https://issues.apache.org/jira/secure/attachment/12459577/org.apache.hadoop.hbase.TestHBaseTestingUtility-output.txt","14/Nov/10 13:05;stack;org.apache.hadoop.hbase.TestHBaseTestingUtility-output.txt;https://issues.apache.org/jira/secure/attachment/12459554/org.apache.hadoop.hbase.TestHBaseTestingUtility-output.txt","14/Nov/10 13:05;stack;org.apache.hadoop.hbase.TestHBaseTestingUtility.txt;https://issues.apache.org/jira/secure/attachment/12459555/org.apache.hadoop.hbase.TestHBaseTestingUtility.txt",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26728,Reviewed,,,,Fri Nov 20 12:43:02 UTC 2015,,,,,,,,,,"0|i0hlcf:",100720,,,,,,,,,,,,,,,,,,,,,"14/Nov/10 13:03;stack;The new hadoop jar would seem to be responsible for the now failing TestHFileOutputFormat too.  If I put in place an hadoop jar that is minus hdfs-724, it all passes.  Let me attach output of TestHBaseTestingUtility with full DEBUG enabled in case anyone has nought better to do of a sunday but peruse DEBUG spew.;;;","14/Nov/10 13:05;stack;To get the above log, I changed the log4j that tests see as follows:

{code}
Index: src/test/resources/log4j.properties
===================================================================
--- src/test/resources/log4j.properties (revision 1034940)
+++ src/test/resources/log4j.properties (working copy)
@@ -42,6 +42,6 @@
 
 #log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG
 
-log4j.logger.org.apache.hadoop=WARN
+log4j.logger.org.apache.hadoop=DEBUG
 log4j.logger.org.apache.zookeeper=ERROR
 log4j.logger.org.apache.hadoop.hbase=DEBUG
{code}

and then ran this:

{code}
$  mvn clean test -Dtest=TestHBaseTestingUtility
{code};;;","14/Nov/10 23:11;streamy;This is snippet from attached log that seems to be the first failure...

{noformat}
2010-11-14 05:00:08,938 DEBUG [DataStreamer for file /user/stack/.logs/pynchon-432.lan,63324,1289739567228/192.168.1.69%3A63324.1289739568182 block blk_-4366233055961732763_1007] hdfs.DFSClient$DFSOutputStream$DataStreamer(2429): DataStreamer block blk_-4366233055961732763_1007 wrote packet seqno:1 size:795 offsetInBlock:0 lastPacketInBlock:false
2010-11-14 05:00:08,938 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@54cee271] datanode.BlockReceiver(393): Receiving one packet for block blk_-4366233055961732763_1007 of length 774 seqno 1 offsetInBlock 0 lastPacketInBlock false
2010-11-14 05:00:08,938 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@54cee271] datanode.BlockReceiver$PacketResponder(737): PacketResponder 0 adding seqno 1 to ack queue.
2010-11-14 05:00:08,938 DEBUG [PacketResponder 0 for Block blk_-4366233055961732763_1007] datanode.BlockReceiver$PacketResponder(891): PacketResponder 0 for block blk_-4366233055961732763_1007 responded an ack: Replies for seqno 1 are SUCCESS
2010-11-14 05:00:08,938 DEBUG [PacketResponder 0 for Block blk_-4366233055961732763_1007] datanode.BlockReceiver$PacketResponder(789): PacketResponder 0 seqno = -2 for block blk_-4366233055961732763_1007 waiting for local datanode to finish write.
2010-11-14 05:00:08,938 DEBUG [ResponseProcessor for block blk_-4366233055961732763_1007] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2534): DFSClient Replies for seqno 0 are FAILED
2010-11-14 05:00:08,939 WARN  [ResponseProcessor for block blk_-4366233055961732763_1007] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2580): DFSOutputStream ResponseProcessor exception  for block blk_-4366233055961732763_1007java.io.IOException: Bad response 1 for block blk_-4366233055961732763_1007 from datanode 127.0.0.1:63316
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2542)

2010-11-14 05:00:08,939 WARN  [DataStreamer for file /user/stack/.logs/pynchon-432.lan,63324,1289739567228/192.168.1.69%3A63324.1289739568182 block blk_-4366233055961732763_1007] hdfs.DFSClient$DFSOutputStream(2616): Error Recovery for block blk_-4366233055961732763_1007 bad datanode[0] 127.0.0.1:63316
2010-11-14 05:00:08,941 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiver@54cee271] datanode.BlockReceiver(565): Exception in receiveBlock for block blk_-4366233055961732763_1007 java.io.EOFException: while trying to read 795 bytes
2010-11-14 05:00:08,941 INFO  [PacketResponder 0 for Block blk_-4366233055961732763_1007] datanode.BlockReceiver$PacketResponder(844): PacketResponder blk_-4366233055961732763_1007 0 : Thread is interrupted.
2010-11-14 05:00:08,941 INFO  [PacketResponder 0 for Block blk_-4366233055961732763_1007] datanode.BlockReceiver$PacketResponder(907): PacketResponder 0 for block blk_-4366233055961732763_1007 terminating
2010-11-14 05:00:08,941 WARN  [RegionServer:0;pynchon-432.lan,63324,1289739567228.logSyncer] hdfs.DFSClient$DFSOutputStream(3293): Error while syncing
java.io.IOException: All datanodes 127.0.0.1:63316 are bad. Aborting...
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2666)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$1500(DFSClient.java:2157)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2356)
2010-11-14 05:00:08,942 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiver@54cee271] datanode.DataXceiver(377): writeBlock blk_-4366233055961732763_1007 received exception java.io.EOFException: while trying to read 795 bytes
2010-11-14 05:00:08,943 FATAL [RegionServer:0;pynchon-432.lan,63324,1289739567228.logSyncer] wal.HLog(1083): Could not append. Requesting close of hlog
java.io.IOException: Reflection
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.sync(SequenceFileLogWriter.java:147)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.hflush(HLog.java:1059)
        at org.apache.hadoop.hbase.regionserver.wal.HLog$LogSyncer.run(HLog.java:983)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.sync(SequenceFileLogWriter.java:145)
        ... 2 more
Caused by: java.io.IOException: All datanodes 127.0.0.1:63316 are bad. Aborting...
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2666)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$1500(DFSClient.java:2157)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2356)
{noformat};;;","14/Nov/10 23:57;streamy;From a new run, here is all of the activity of the block that causes the Abort...

{noformat}

2010-11-14 15:44:00,042 DEBUG [IPC Server handler 4 on 40196] namenode.FSDirectory(273): DIR* FSDirectory.addFile: /user/jgray/.logs/dev692.sf2p.facebook.com,58498,1289778232413/10.17.83.207%3A58498.1289778233005 with blk_4643386764144918409_1009 block is added to the in-memory file system
2010-11-14 15:44:00,042 INFO  [IPC Server handler 4 on 40196] namenode.FSNamesystem(1482): BLOCK* NameSystem.allocateBlock: /user/jgray/.logs/dev692.sf2p.facebook.com,58498,1289778232413/10.17.83.207%3A58498.1289778233005. blk_4643386764144918409_1009
2010-11-14 15:44:00,044 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.DataXceiver(228): Receiving block blk_4643386764144918409_1009 src: /127.0.0.1:48990 dest: /127.0.0.1:48738
2010-11-14 15:44:00,044 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.FSDataset(1517): b=blk_4643386764144918409_1009, f=null
2010-11-14 15:44:00,044 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.FSDataset(1313): writeTo metafile is /var/users/jgray/hbase/trunk/target/test-data/5173adaa-a370-457d-8293-8f059f70bd31/dfs/data/data1/blocksBeingWritten/blk_4643386764144918409_1009.meta of size 0
2010-11-14 15:44:00,045 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(789): PacketResponder 0 seqno = -2 for block blk_4643386764144918409_1009 waiting for local datanode to finish write.
2010-11-14 15:44:00,045 DEBUG [DataStreamer for file /user/jgray/.logs/dev692.sf2p.facebook.com,58498,1289778232413/10.17.83.207%3A58498.1289778233005 block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$DataStreamer(2429): DataStreamer block blk_4643386764144918409_1009 wrote packet seqno:0 size:339 offsetInBlock:0 lastPacketInBlock:false
2010-11-14 15:44:00,045 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver(393): Receiving one packet for block blk_4643386764144918409_1009 of length 318 seqno 0 offsetInBlock 0 lastPacketInBlock false
2010-11-14 15:44:00,046 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(891): PacketResponder 0 for block blk_4643386764144918409_1009 responded an ack: Replies for seqno 0 are SUCCESS
2010-11-14 15:44:00,046 DEBUG [ResponseProcessor for block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2534): DFSClient Replies for seqno 0 are SUCCESS
2010-11-14 15:44:00,046 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(789): PacketResponder 0 seqno = -2 for block blk_4643386764144918409_1009 waiting for local datanode to finish write.
2010-11-14 15:44:31,551 DEBUG [DataStreamer for file /user/jgray/.logs/dev692.sf2p.facebook.com,58498,1289778232413/10.17.83.207%3A58498.1289778233005 block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$DataStreamer(2429): DataStreamer block blk_4643386764144918409_1009 wrote packet seqno:-1 size:25 offsetInBlock:0 lastPacketInBlock:false
2010-11-14 15:44:31,551 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver(393): Receiving one packet for block blk_4643386764144918409_1009 of length 4 seqno -1 offsetInBlock 0 lastPacketInBlock false
2010-11-14 15:44:31,551 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver(636): Changing block file offset of block blk_4643386764144918409_1009 from 310 to 0 meta file offset to 7
2010-11-14 15:44:31,552 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver(422): Receiving empty packet for block blk_4643386764144918409_1009
2010-11-14 15:44:31,552 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(891): PacketResponder 0 for block blk_4643386764144918409_1009 responded an ack: Replies for seqno -1 are SUCCESS
2010-11-14 15:44:31,552 DEBUG [ResponseProcessor for block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2534): DFSClient Replies for seqno -1 are
2010-11-14 15:44:31,552 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(789): PacketResponder 0 seqno = -2 for block blk_4643386764144918409_1009 waiting for local datanode to finish write.
2010-11-14 15:44:31,867 DEBUG [DataStreamer for file /user/jgray/.logs/dev692.sf2p.facebook.com,58498,1289778232413/10.17.83.207%3A58498.1289778233005 block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$DataStreamer(2429): DataStreamer block blk_4643386764144918409_1009 wrote packet seqno:1 size:804 offsetInBlock:0 lastPacketInBlock:false
2010-11-14 15:44:31,868 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver(393): Receiving one packet for block blk_4643386764144918409_1009 of length 783 seqno 1 offsetInBlock 0 lastPacketInBlock false
2010-11-14 15:44:31,868 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(891): PacketResponder 0 for block blk_4643386764144918409_1009 responded an ack: Replies for seqno 1 are SUCCESS
2010-11-14 15:44:31,868 DEBUG [ResponseProcessor for block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2534): DFSClient Replies for seqno 0 are FAILED
2010-11-14 15:44:31,868 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(789): PacketResponder 0 seqno = -2 for block blk_4643386764144918409_1009 waiting for local datanode to finish write.
2010-11-14 15:44:31,868 WARN  [ResponseProcessor for block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2580): DFSOutputStream ResponseProcessor exception  for block blk_4643386764144918409_1009java.io.IOException: Bad response 1 for block blk_4643386764144918409_1009 from datanode 127.0.0.1:48738
2010-11-14 15:44:31,868 WARN  [DataStreamer for file /user/jgray/.logs/dev692.sf2p.facebook.com,58498,1289778232413/10.17.83.207%3A58498.1289778233005 block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream(2616): Error Recovery for block blk_4643386764144918409_1009 bad datanode[0] 127.0.0.1:48738
2010-11-14 15:44:31,869 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver(565): Exception in receiveBlock for block blk_4643386764144918409_1009 java.io.IOException: Connection reset by peer
2010-11-14 15:44:31,869 INFO  [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(844): PacketResponder blk_4643386764144918409_1009 0 : Thread is interrupted.
2010-11-14 15:44:31,869 INFO  [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(907): PacketResponder 0 for block blk_4643386764144918409_1009 terminating
2010-11-14 15:44:31,869 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.DataXceiver(377): writeBlock blk_4643386764144918409_1009 received exception java.io.IOException: Connection reset by peer
2010-11-14 15:45:32,574 INFO  [IPC Server handler 8 on 40196] namenode.INodeFileUnderConstruction(212): BLOCK* blk_4643386764144918409_1009 recovery started, primary=127.0.0.1:48738
2010-11-14 15:45:34,183 INFO  [org.apache.hadoop.hdfs.server.datanode.DataNode$1@26c94114] datanode.DataNode(1632): NameNode calls recoverBlock(block=blk_4643386764144918409_1009, targets=[127.0.0.1:48738])
2010-11-14 15:45:34,183 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataNode$1@26c94114] datanode.DataNode(1409): block=blk_4643386764144918409_1009
2010-11-14 15:45:34,184 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataNode$1@26c94114] datanode.DataNode(1419): getBlockMetaDataInfo successful block=blk_4643386764144918409_1009 length 771 genstamp 1009
2010-11-14 15:45:34,185 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataNode$1@26c94114] datanode.DataNode(1563): block=blk_4643386764144918409_1009, (length=771), syncList=[block:blk_4643386764144918409_1009 node:127.0.0.1:48738], closeFile=true
2010-11-14 15:45:34,186 INFO  [org.apache.hadoop.hdfs.server.datanode.DataNode$1@26c94114] datanode.DataNode(1450): oldblock=blk_4643386764144918409_1009(length=771), newblock=blk_4643386764144918409_1014(length=771), datanode=127.0.0.1:48738
2010-11-14 15:45:34,188 INFO  [IPC Server handler 1 on 40196] namenode.FSNamesystem(3149): BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:48738 is added to blk_4643386764144918409_1009 size 771
2010-11-14 15:45:34,189 INFO  [IPC Server handler 2 on 40196] namenode.FSNamesystem(1970): commitBlockSynchronization(lastblock=blk_4643386764144918409_1009, newgenerationstamp=1014, newlength=771, newtargets=[127.0.0.1:48738], closeFile=true, deleteBlock=false)
{noformat}

All of the recovery stuff happens after the abort.  The snippet around the exception/abort from this log is this:

{noformat}
2010-11-14 15:44:31,867 DEBUG [PRI IPC Server handler 3 on 58498] hdfs.DFSClient$DFSOutputStream(3154): DFSClient writeChunk allocating new packet seqno=1, src=/user/jgray/.logs/dev692.sf2p.facebook.com
,58498,1289778232413/10.17.83.207%3A58498.1289778233005, packetSize=65557, chunksPerPacket=127, bytesCurBlock=0
2010-11-14 15:44:31,867 DEBUG [RegionServer:0;dev692.sf2p.facebook.com,58498,1289778232413.logSyncer] hdfs.DFSClient$DFSOutputStream(3238): DFSClient flush() : saveOffset 512 bytesCurBlock 771 lastFlush
Offset 310
2010-11-14 15:44:31,867 DEBUG [DataStreamer for file /user/jgray/.logs/dev692.sf2p.facebook.com,58498,1289778232413/10.17.83.207%3A58498.1289778233005 block blk_4643386764144918409_1009] hdfs.DFSClient$
DFSOutputStream$DataStreamer(2429): DataStreamer block blk_4643386764144918409_1009 wrote packet seqno:1 size:804 offsetInBlock:0 lastPacketInBlock:false
2010-11-14 15:44:31,868 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver(393): Receiving one packet for block blk_4643386764144918409_1009 of length 783 seqno 1
 offsetInBlock 0 lastPacketInBlock false
2010-11-14 15:44:31,868 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver$PacketResponder(737): PacketResponder 0 adding seqno 1 to ack queue.
2010-11-14 15:44:31,868 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(891): PacketResponder 0 for block blk_4643386764144918409_1009 responded a
n ack: Replies for seqno 1 are SUCCESS
2010-11-14 15:44:31,868 DEBUG [ResponseProcessor for block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2534): DFSClient Replies for seqno 0 are FAILED
2010-11-14 15:44:31,868 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(789): PacketResponder 0 seqno = -2 for block blk_4643386764144918409_1009 
waiting for local datanode to finish write.
2010-11-14 15:44:31,868 WARN  [ResponseProcessor for block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2580): DFSOutputStream ResponseProcessor exception  for block bl
k_4643386764144918409_1009java.io.IOException: Bad response 1 for block blk_4643386764144918409_1009 from datanode 127.0.0.1:48738
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2542)

2010-11-14 15:44:31,868 WARN  [DataStreamer for file /user/jgray/.logs/dev692.sf2p.facebook.com,58498,1289778232413/10.17.83.207%3A58498.1289778233005 block blk_4643386764144918409_1009] hdfs.DFSClient$
DFSOutputStream(2616): Error Recovery for block blk_4643386764144918409_1009 bad datanode[0] 127.0.0.1:48738
2010-11-14 15:44:31,869 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver(565): Exception in receiveBlock for block blk_4643386764144918409_1009 java.io.IOExcept
ion: Connection reset by peer
2010-11-14 15:44:31,869 INFO  [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(844): PacketResponder blk_4643386764144918409_1009 0 : Thread is interrupt
ed.
2010-11-14 15:44:31,869 INFO  [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(907): PacketResponder 0 for block blk_4643386764144918409_1009 terminating
2010-11-14 15:44:31,869 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.DataXceiver(377): writeBlock blk_4643386764144918409_1009 received exception java.io.IOException: Con
nection reset by peer
2010-11-14 15:44:31,869 WARN  [RegionServer:0;dev692.sf2p.facebook.com,58498,1289778232413.logSyncer] hdfs.DFSClient$DFSOutputStream(3293): Error while syncing
java.io.IOException: All datanodes 127.0.0.1:48738 are bad. Aborting...
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2666)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$1500(DFSClient.java:2157)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2356)
{noformat};;;","15/Nov/10 00:14;tlipcon;Does branch append even pass its own unit tests? It looks like the write pipeline is seriously screwed up, I'd be surprised if it did.;;;","15/Nov/10 01:49;stack;Here is a run against an hadoop that is minus hdfs-724.  One thing I notice is that the good run reports:

{code}
2010-11-15 01:42:02,432 DEBUG [Master:0;pynchon-432.lan:51131] ipc.Client$Connection(469): IPC Client (47) connection to localhost/127.0.0.1:51122 from stack sending #148
{code}

... i.e. user 'stack' whereas the bad run has 

{code}
2010-11-14 23:38:57,672 DEBUG [IPC Server handler 3 on 64745] ipc.HBaseClient$Connection(487): IPC Client (47) connection to /172.16.198.219:64748 from an unknown user sending #148
{code}
.. 'unknown user'.;;;","15/Nov/10 01:49;tlipcon;There seems to be some incompatibility between HDFS-724 and HDFS-895... seeing some similar issues on trunk, working on it.;;;","15/Nov/10 02:19;stack;Good on you Todd.

Ignore the user issue mentioned above.

Lining up a good log and a bad log, here is a good log:

{code}
 8173 2010-11-15 01:42:29,781 DEBUG [PRI IPC Server handler 1 on 51098] hdfs.DFSClient$DFSOutputStream(3099): DFSClient writeChunk allocating new packet seqno=1, src=/user/stack/.lo      gs/pynchon-432.lan,51098,1289785308076/192.168.1.69%3A51098.1289785308941, packetSize=65557, chunksPerPacket=127, bytesCurBlock=0
 8174 2010-11-15 01:42:29,782 DEBUG [RegionServer:0;pynchon-432.lan,51098,1289785308076.logSyncer] hdfs.DFSClient$DFSOutputStream(3170): DFSClient flush() : saveOffset 512 bytesCurB      lock 762 lastFlushOffset 301
 8175 2010-11-15 01:42:29,782 DEBUG [DataStreamer for file /user/stack/.logs/pynchon-432.lan,51098,1289785308076/192.168.1.69%3A51098.1289785308941 block blk_8612924244108136372_100      7] hdfs.DFSClient$DFSOutputStream$DataStreamer(2376): DataStreamer block blk_8612924244108136372_1007 wrote packet seqno:1 size:795 offsetInBlock:0 lastPacketInBlock:false
 8176 2010-11-15 01:42:29,782 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@1a83e35b] datanode.BlockReceiver(393): Receiving one packet for block blk_8612924244108136372      _1007 of length 774 seqno 1 offsetInBlock 0 lastPacketInBlock false
 8177 2010-11-15 01:42:29,782 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@1a83e35b] datanode.BlockReceiver(635): Changing block file offset of block blk_86129242441081      36372_1007 from 301 to 0 meta file offset to 7
 8178 2010-11-15 01:42:29,783 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@1a83e35b] datanode.BlockReceiver$PacketResponder(736): PacketResponder 0 adding seqno 1 to ac      k queue.
 8179 2010-11-15 01:42:29,783 DEBUG [PacketResponder 0 for Block blk_8612924244108136372_1007] datanode.BlockReceiver$PacketResponder(806): PacketResponder 0 for block blk_861292424      4108136372_1007 acking for packet 1
 8180 2010-11-15 01:42:29,783 DEBUG [ResponseProcessor for block blk_8612924244108136372_1007] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2481): DFSClient Replies for seqno 1       are SUCCESS
 8181 2010-11-15 01:42:29,783 DEBUG [PRI IPC Server handler 1 on 51098] ipc.HBaseRPC$Server(574): Served: put queueTime= 0 procesingTime= 3
 8182 2010-11-15 01:42:29,783 DEBUG [PRI IPC Server handler 1 on 51098] ipc.HBaseServer$Responder(718): IPC Server Responder: responding to #157 from 192.168.1.69:51194
 8183 2010-11-15 01:42:29,783 DEBUG [PRI IPC Server handler 1 on 51098] ipc.HBaseServer$Responder(737): IPC Server Responder: responding to #157 from 192.168.1.69:51194 Wrote 8 byte      s.
 8184 2010-11-15 01:42:29,783 DEBUG [IPC Client (47) connection to pynchon-432.lan/192.168.1.69:51098 from an unknown user] ipc.HBaseClient$Connection(524): IPC Client (47) connecti      on to pynchon-432.lan/192.168.1.69:51098 from an unknown user got value #157
 8185 2010-11-15 01:42:29,784 DEBUG [IPC Server handler 4 on 51095] ipc.HBaseRPC$Invoker(261): Call: put 4
 8186 2010-11-15 01:42:29,784 INFO  [IPC Server handler 4 on 51095] catalog.MetaEditor(59): Added region test,,1289785349688.03c89b2db74c8c3215c5fd46429fa2f0. to META
{code}

Here is bad log

{code}
 8118 2010-11-14 23:38:57,616 DEBUG [PRI IPC Server handler 8 on 64748] hdfs.DFSClient$DFSOutputStream(3154): DFSClient writeChunk allocating new packet seqno=1, src=/user/stack/.lo      gs/172.16.198.219,64748,1289777896452/172.16.198.219%3A64748.1289777897337, packetSize=65557, chunksPerPacket=127, bytesCurBlock=0
 8119 2010-11-14 23:38:57,617 DEBUG [RegionServer:0;172.16.198.219,64748,1289777896452.logSyncer] hdfs.DFSClient$DFSOutputStream(3238): DFSClient flush() : saveOffset 512 bytesCurBl      ock 761 lastFlushOffset 300
 8120 2010-11-14 23:38:57,617 DEBUG [DataStreamer for file /user/stack/.logs/172.16.198.219,64748,1289777896452/172.16.198.219%3A64748.1289777897337 block blk_-8256147503844598069_1      007] hdfs.DFSClient$DFSOutputStream$DataStreamer(2429): DataStreamer block blk_-8256147503844598069_1007 wrote packet seqno:1 size:794 offsetInBlock:0 lastPacketInBlock:false
 8121 2010-11-14 23:38:57,617 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@753bc30a] datanode.BlockReceiver(393): Receiving one packet for block blk_-825614750384459806      9_1007 of length 773 seqno 1 offsetInBlock 0 lastPacketInBlock false
 8122 2010-11-14 23:38:57,617 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@753bc30a] datanode.BlockReceiver$PacketResponder(737): PacketResponder 0 adding seqno 1 to ac      k queue.
 8123 2010-11-14 23:38:57,618 DEBUG [PacketResponder 0 for Block blk_-8256147503844598069_1007] datanode.BlockReceiver$PacketResponder(891): PacketResponder 0 for block blk_-8256147      503844598069_1007 responded an ack: Replies for seqno 1 are SUCCESS
 8124 2010-11-14 23:38:57,618 DEBUG [ResponseProcessor for block blk_-8256147503844598069_1007] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2534): DFSClient Replies for seqno 0       are FAILED
 8125 2010-11-14 23:38:57,618 DEBUG [PacketResponder 0 for Block blk_-8256147503844598069_1007] datanode.BlockReceiver$PacketResponder(789): PacketResponder 0 seqno = -2 for block b      lk_-8256147503844598069_1007 waiting for local datanode to finish write.
 8126 2010-11-14 23:38:57,618 WARN  [ResponseProcessor for block blk_-8256147503844598069_1007] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2580): DFSOutputStream ResponseProce      ssor exception  for block blk_-8256147503844598069_1007java.io.IOException: Bad response 1 for block blk_-8256147503844598069_1007 from datanode 127.0.0.1:64740
 8127     at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2542)
 8128 
 8129 2010-11-14 23:38:57,619 WARN  [DataStreamer for file /user/stack/.logs/172.16.198.219,64748,1289777896452/172.16.198.219%3A64748.1289777897337 block blk_-8256147503844598069_1      007] hdfs.DFSClient$DFSOutputStream(2616): Error Recovery for block blk_-8256147503844598069_1007 bad datanode[0] 127.0.0.1:64740
 8130 2010-11-14 23:38:57,619 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiver@753bc30a] datanode.BlockReceiver(565): Exception in receiveBlock for block blk_-8256147503844      598069_1007 java.io.EOFException: while trying to read 794 bytes
{code}


In the bad, its writing 794 bytes but in good its writing 795 bytes.

The good does a ' Changing block file offset of block blk_8612924244108136372_1007 from 301 to 0 meta file offset to 7' before it does   PacketResponder 0 adding seqno 1 to ack queue.'  and then getting an ack to report SUCCESS.   The 'bad' doesn't do this its size seem to be smaller by one byte and after acking seqno 1, it then goes on to look for ack on seqno 0 and reports FAILED.

I don't know what this means (smile).  Seems significant. ;;;","15/Nov/10 03:08;ryanobjc;I got TestHBaseTestingUtility to pass on a variant of branch-20-append w/o 724 and WITH 895. 

It is published here:
https://github.com/ryanobjc/hadoop-common/tree/branch-0.20-append

I also pushed the jars to maven repo @ people.apache.org/~rawson/repo

and I committed changes to pom.xml that depend on these new versions.  

So we'll see what hudson thinks.;;;","15/Nov/10 05:48;ghelmling;@Stack: For ""stack"" vs. ""unknown user"", that's probably not related...

The first is from a Hadoop RPC call (ipc.Client), where the second is from an HBase RPC call (ipc.HBaseClient).  HBase RPC _always_ passes ""null"" for the user in the RPC header -- it's hard coded in HBaseClient.  We've changed this in the secure HBase branch, of course, but it's always been this way in the mainline.  Master and RS should be using the process username for all DFS client interactions.
;;;","15/Nov/10 09:26;stack;OK.  Chatting w/ Todd and Ryan, HDFS-724 is not in CDH.  Removing HDFS-724, all hbase tests pass (Both for me and Ryan).  HDFS-724 looks like critical fix but I'm going to go ahead and RC without it.  There seems to be something up w/ the HDFS-724 that is in the append branch.  While we're figuring it out, our first 0.90.0 RC can get an airing (Anyone can sink the RC if they disagree w/ above).   My guess is that there'll at least be an RC2 and we can get any fix in then.  

I made an hadoop jar off the tip of branch-0.20-append that includes hdfs-895 but removes hdfs-724, signed it and put it up on a hand-built repo up on people.apache.org.  The jar is named 0.20.3-append-r1034938-plusHDFS895-minusHDFS724.

(@Gary -- thanks for the stack vs unknown user tidbit... I kinda picked up on it later but your clarification helped);;;","15/Nov/10 13:41;stack;Oh, just to say that Todd thinks it might be interaction between hdfs-724 and hdfs-895.  He was going to try and look into it.;;;","15/Nov/10 15:46;tlipcon;bq. Oh, just to say that Todd thinks it might be interaction between hdfs-724 and hdfs-895. He was going to try and look into it.

I found a bug on trunk HDFS-895 that was due to an issue interacting with HDFS-724, but according to people working on 20-append, this bug is present even with *just* 724 without 895. So I think it's a bad backport.;;;","15/Nov/10 19:43;jdcryans;So in our 0.90.0 RC email, we say that people can use the 0.20-append branch... but won't it be incompatible since it contains HDFS-724 which bumps the data transfer protocol?;;;","15/Nov/10 20:45;tlipcon;Ah, yes... but do you know of anyone who actually has run a cluster on 20-append? :) Probably not worth rebuilding RC, we can just tell people ""just kidding"" for now ;-);;;","15/Nov/10 20:47;jdcryans;Thanks Todd, just wanted to confirm my understanding of the situation.;;;","15/Nov/10 23:28;hairong;I found out the problem. Looks that HDFS-724 missed a piece of code. PipelineAck#readFields does not serialize the Heartbeat ack correctly. I just checked our internal branch and I did correctly there. That's why our internal testing did not show this error.;;;","15/Nov/10 23:35;stack;@Hairong Excellent.  If you post a patch over in hdfs-724, I can try it for you in hbase.  What about the bump in the transfer protocol version J-D identifies above?  Could the hdfs-724 for 0.20-append branch not bumped the protocol?  What you think?;;;","15/Nov/10 23:36;hairong;I attached a patch that fixes the problem: https://issues.apache.org/jira/secure/attachment/12459664/hbAckReply.patch. Could anybody give it a try? If it fixes the problem, I will commit it to append 0.20.;;;","15/Nov/10 23:59;hairong;For the version # bump problem, HDFS-724 is indeed an incompatible change. For append 0.20, it changes the semantics/syntax of heartbeat packets.;;;","16/Nov/10 00:04;stack;@Hairong OK. I don't think it'll be end of the world asking fellas to restart their clusters going between versions of the hadoop jar pre-724 and post-724.;;;","16/Nov/10 01:10;stack;Tests are still running (its looking good).  Not done yet.  Will report back later.  J-D and Todd, you Ok w/ version number going up when we have hdfs-724 in the hbase hacked hadoop jar again?;;;","16/Nov/10 02:18;jdcryans;My only concern was that if we back out 724, then people won't be able to run on the tip of 0.20-append since the protocol won't be the same.;;;","17/Nov/10 15:11;stack;Add this fix to second 0.90.0 release candidate (by updating bundled hadoop now it has fixed 724 and 895);;;","30/Nov/10 05:22;stack;Here is some pom fixup to point at a new hadoop made from tip of branch-0.20-append today.  Ryan added the signed and md5'd jars to his personal repo.  I tested it.  At least the test that used fail now passes with Hairong's fixup on the hdfs-724 backport.;;;","30/Nov/10 05:24;stack;Applied to branch and trunk.  Closing.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Long Running Stats,HBASE-3233,12479853,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,nspiegelberg,nspiegelberg,nspiegelberg,13/Nov/10 06:25,20/Nov/15 12:42,14/Jul/23 06:06,13/Nov/10 22:47,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"HBASE-3102 has a small bug.  Once long running stats are reset, the reset flag is never cleared.  This is a one-line fix for this issue. Verified on our test clusters.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-3102,,,,,,,,,,,,,"13/Nov/10 06:27;nspiegelberg;HBASE-3233.patch;https://issues.apache.org/jira/secure/attachment/12459520/HBASE-3233.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26727,Reviewed,,,,Fri Nov 20 12:42:05 UTC 2015,,,,,,,,,,"0|i0hlc7:",100719,,,,,,,,,,,,,,,,,,,,,"13/Nov/10 20:07;pranavkhaitan;Looks good;;;","13/Nov/10 22:47;stack;Committed to branch and trunk.  Thanks for the patch Nicoloas.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix KeyOnlyFilter + Add Value Length,HBASE-3232,12479849,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,nspiegelberg,nspiegelberg,nspiegelberg,13/Nov/10 04:41,20/Nov/15 12:41,14/Jul/23 06:06,15/Nov/10 03:15,,,,,,,,,,,,0.90.0,,,,,,,0,,,"HBASE-3211 altered filter code to mutate KeyValues.  What could go wrong?  Well, your scan could mess up because the KVHeap compare functions don't work properly.  If we're going to soft mutate KVs in filter code, we also need to soft copy the KV before filtering.  This was found while adding the ability to have KeyOnlyFilter have the option to return the Value's length.  This is useful for grouping your reduce tasks into equal-sized blocks.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-3211,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26726,,,,,Fri Nov 20 12:41:20 UTC 2015,,,,,,,,,,"0|i0hlbz:",100718,,,,,,,,,,,,,,,,,,,,,"13/Nov/10 04:52;ryanobjc;This feels like something that should be done in co processors
;;;","13/Nov/10 04:54;nspiegelberg;I agree.  This was a lightweight solution necessary for our existing deployment to scale in 0.90.;;;","13/Nov/10 05:18;hbasereviewboard;Message from: ""Nicolas"" <nspiegelberg@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1213/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

HBASE-3211 altered filter code to mutate KeyValues. What could go wrong? Well, your scan could mess up because the KVHeap compare functions don't work properly. If we're going to soft mutate KVs in filter code, we also need to soft copy the KV before filtering. This was found while adding the ability to have KeyOnlyFilter have the option to return the Value's length. This is useful for grouping your reduce tasks into equal-sized blocks.


This addresses bug HBASE-3232.
    http://issues.apache.org/jira/browse/HBASE-3232


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java 1034646 
  trunk/src/main/java/org/apache/hadoop/hbase/filter/KeyOnlyFilter.java 1034646 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1034646 
  trunk/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java 1034646 
  trunk/src/test/java/org/apache/hadoop/hbase/filter/TestFilter.java 1034646 

Diff: http://review.cloudera.org/r/1213/diff


Testing
-------

mvn clean test


Thanks,

Nicolas


;;;","15/Nov/10 02:31;hbasereviewboard;Message from: ""Nicolas"" <nspiegelberg@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1213/
-----------------------------------------------------------

(Updated 2010-11-14 18:28:50.196433)


Review request for hbase.


Changes
-------

Because I didn't implement write/readFields for KeyOnlyFilter when I added the param, client -> server serialization didn't work and the default value of false was always used.  Fixed + added associated unit test


Summary
-------

HBASE-3211 altered filter code to mutate KeyValues. What could go wrong? Well, your scan could mess up because the KVHeap compare functions don't work properly. If we're going to soft mutate KVs in filter code, we also need to soft copy the KV before filtering. This was found while adding the ability to have KeyOnlyFilter have the option to return the Value's length. This is useful for grouping your reduce tasks into equal-sized blocks.


This addresses bug HBASE-3232.
    http://issues.apache.org/jira/browse/HBASE-3232


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java 1034646 
  trunk/src/main/java/org/apache/hadoop/hbase/filter/KeyOnlyFilter.java 1034646 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1034646 
  trunk/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java 1034646 
  trunk/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java 1034646 
  trunk/src/test/java/org/apache/hadoop/hbase/filter/TestFilter.java 1034646 

Diff: http://review.cloudera.org/r/1213/diff


Testing
-------

mvn clean test


Thanks,

Nicolas


;;;","15/Nov/10 03:15;ryanobjc;committed to trunk + 0.90;;;","15/Nov/10 03:16;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1213/#review1922
-----------------------------------------------------------


looks great, i just committed it

- Ryan



;;;","12/Nov/11 02:25;hudson;Integrated in HBase-0.92 #125 (See [https://builds.apache.org/job/HBase-0.92/125/])
    HBASE-3433  Remove the KV copy of every KV in Scan; introduced by HBASE-3232

larsh : 
Files : 
* /hbase/branches/0.92/CHANGES.txt
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/KeyValue.java
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/filter/Filter.java
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/filter/FilterBase.java
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/filter/FilterList.java
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/filter/KeyOnlyFilter.java
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/filter/SkipFilter.java
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/filter/WhileMatchFilter.java
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
* /hbase/branches/0.92/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java
;;;","12/Nov/11 04:12;hudson;Integrated in HBase-TRUNK #2430 (See [https://builds.apache.org/job/HBase-TRUNK/2430/])
    HBASE-3433  Remove the KV copy of every KV in Scan; introduced by HBASE-3232

larsh : 
Files : 
* /hbase/trunk/CHANGES.txt
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/filter/Filter.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/filter/FilterBase.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/filter/FilterList.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/filter/KeyOnlyFilter.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/filter/SkipFilter.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/filter/WhileMatchFilter.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java
;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refresh our hadoop jar and update zookeeper to just-released 3.3.2.,HBASE-3230,12479822,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,12/Nov/10 19:34,20/Nov/15 12:42,14/Jul/23 06:06,20/Nov/10 00:11,,,,,,,,,,,,0.90.0,0.92.0,,,,,,0,,,,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/10 19:37;stack;pom.txt;https://issues.apache.org/jira/secure/attachment/12459479/pom.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26725,,,,,Fri Nov 20 12:42:22 UTC 2015,,,,,,,,,,"0|i0hlbj:",100716,,,,,,,,,,,,,,,,,,,,,"12/Nov/10 19:37;stack;I made an hadoop from head of branch-0.20-append with the 0.20-append patch that Todd just loaded up into hdfs-895.  Its called 0.20.3-append-r1034499-895.  I loaded the built jar up into my own repo under people.apache.org.  I also updated zk pointing at the apache repo.  I left notes in the pom explaining how I did this stuff.;;;","12/Nov/10 20:10;stack;Committed.;;;","12/Nov/10 21:18;stack;I just reverted the move to 3.3.2 zk.  One of our tests is failing.  I made HBASE-3231 to do the zk update.;;;","16/Nov/10 13:06;larsfrancke;You rolled back the change but you kept the repository ""apache-rsync"" (which really shouldn't ever have been added) so if you could remove that as well that would be good.;;;","16/Nov/10 23:46;stack;Reopening.  Fix for second 0.90.0 RC.

Thanks Lars.;;;","19/Nov/10 23:43;jdcryans;With the fix from HBASE-3231 and ZK 3.3.2, I re-ran the unit tests and it's all green, going to commit.

Proof:

{noformat}

Tests run: 605, Failures: 0, Errors: 0, Skipped: 9

[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESSFUL
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 72 minutes 13 seconds
[INFO] Finished at: Fri Nov 19 15:38:45 PST 2010
[INFO] Final Memory: 82M/846M
[INFO] ------------------------------------------------------------------------

{noformat};;;","20/Nov/10 00:11;jdcryans;Committed it back.;;;","03/Dec/10 13:28;larsfrancke;I don't know exactly what happened here but the wrong repository (apache-rsync) still (or again) seems to be in trunk.;;;","03/Dec/10 17:52;stack;I thought I'd removed that @Lars.  I just did it again.  Thanks for reporting.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Table creation, though using ""async"" call to master, can actually run for a while and cause RPC timeout",HBASE-3229,12479744,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,mingma,streamy,streamy,11/Nov/10 19:49,20/Nov/15 12:42,14/Jul/23 06:06,26/Aug/11 19:00,0.90.0,,,,,,,,,,,0.92.0,,Client,master,,,,0,,,"Our create table methods in HBaseAdmin are synchronous from client POV.  However, underneath, we're using an ""async"" create and then looping waiting for table availability.  Because the create is async and we loop instead of block on RPC, we don't expect RPC timeouts.

However, when creating a table with lots of initial regions, the ""async"" create can actually take a long time (more than 30 seconds in this case) which causes the client to timeout and gives impression something failed.

We should make the create truly async so that this can't happen.  And rather than doing one-off, inline assignment as it is today, we should reuse the fancy enable/disable code stack just added to make this faster and more optimal.",,kannanm,larsfrancke,shv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-4010,,,,HBASE-3904,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26724,Reviewed,,,,Fri Nov 20 12:42:14 UTC 2015,,,,,,,,,,"0|i0hlbb:",100715,,,,,,,,,,,,,,,,,,,,,"11/Nov/10 19:51;streamy;I think this is a bug, not a feature, so tied it on 0.90.  But could be convinced on a punt if others don't think this is a big deal.

I'm filing this after a user tripped on it and didn't know exactly what happened.;;;","11/Nov/10 20:06;stack;Moved it out after chatting w/ Jon on IRC.  Not 'critical'.;;;","12/Nov/10 19:21;kannanm;Two additional observations:

1) The HBaseAdmin.java:createTableAsync() (this is from 0.89), the function doesn't actually seem be doing anything async. The master.createTable() appears to be a blocking call.

{code}
public void createTableAsync(HTableDescriptor desc, byte [][] splitKeys)
  throws IOException {
    if (this.master == null) {
      throw new MasterNotRunningException(""master has been shut down"");
    }
    HTableDescriptor.isLegalTableName(desc.getName());
    try {
      this.master.createTable(desc, splitKeys);
    } catch (RemoteException e) {
      throw RemoteExceptionHandler.decodeRemoteException(e);
    }
  }
{code}

2) When creating a table with pre-splits, HBaseAdmin.java:createTable() waits only for the first region/split to be online. 

{code}
 public void createTable(HTableDescriptor desc, byte [][] splitKeys)
  throws IOException {
   
    ...

    createTableAsync(desc, splitKeys);
    for (int tries = 0; tries < numRetries; tries++) {
      try {
        // Wait for new table to come on-line
        connection.locateRegion(desc.getName(), HConstants.EMPTY_START_ROW);

    ...
{code}
;;;","02/Dec/10 06:28;stack;Yeah, createTable is not actually async.   Its waiting over in master.  Needs redo so its like the enable/disable/delete table methods.;;;","24/Jun/11 15:25;yuzhihong@gmail.com;w.r.t. Kannan's comments.
In TRUNK, the following method of HMaster is async - see the third parameter:
{code}
  public void createTable(HTableDescriptor desc, byte [][] splitKeys)
  throws IOException {
    createTable(desc, splitKeys, false);
  }
{code}
It is the only method exposed through HMasterInterface.

patch v5 from HBASE-3904 makes HBaseAdmin.createTable() to wait for all regions to be online.
;;;","25/Aug/11 06:34;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1653/
-----------------------------------------------------------

(Updated 2011-08-25 06:33:04.297176)


Review request for hbase.


Summary
-------

1. Make createTable truly async.
2. Fix up some incorrect comments, logs.


This addresses bug HBASE-3229.
    https://issues.apache.org/jira/browse/HBASE-3229


Diffs (updated)
-----

  http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java 1161360 
  http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java 1161360 
  http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java 1161360 
  http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java 1161360 
  http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1161360 
  http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java PRE-CREATION 
  http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java 1161360 
  http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java 1161360 
  http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java 1161360 

Diff: https://reviews.apache.org/r/1653/diff


Testing
-------

1. add a new test case to verify RPC timeout exception is gone when creating table with lots of regions.
2. hbase shell, unit tests.


Thanks,

Ming

;;;","25/Aug/11 11:53;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1653/#review1638
-----------------------------------------------------------



http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<https://reviews.apache.org/r/1653/#comment3678>

    Should we consider using the existing code if there aren't many entries in newRegions ?


- Ted


On 2011-08-25 06:33:04, Ming Ma wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1653/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-25 06:33:04)
bq.  
bq.  
bq.  Review request for hbase.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  1. Make createTable truly async.
bq.  2. Fix up some incorrect comments, logs.
bq.  
bq.  
bq.  This addresses bug HBASE-3229.
bq.      https://issues.apache.org/jira/browse/HBASE-3229
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java PRE-CREATION 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java 1161360 
bq.  
bq.  Diff: https://reviews.apache.org/r/1653/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  1. add a new test case to verify RPC timeout exception is gone when creating table with lots of regions.
bq.  2. hbase shell, unit tests.
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Ming
bq.  
bq.

;;;","25/Aug/11 17:13;jiraposter@reviews.apache.org;

bq.  On 2011-08-25 11:52:17, Ted Yu wrote:
bq.  > http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java, line 968
bq.  > <https://reviews.apache.org/r/1653/diff/3/?file=35507#file35507line968>
bq.  >
bq.  >     Should we consider using the existing code if there aren't many entries in newRegions ?

Some reasons to do async for all cases.

1. Other table operations like enableTable doesn't have such logics. So it is more consistent.
2. If we do sync for small number of regions, then we have to decide what is considered small. It might end up with another configuration parameter.
3. Less code to manage.

Suggestions?


- Ming


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1653/#review1638
-----------------------------------------------------------


On 2011-08-25 06:33:04, Ming Ma wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1653/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-25 06:33:04)
bq.  
bq.  
bq.  Review request for hbase.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  1. Make createTable truly async.
bq.  2. Fix up some incorrect comments, logs.
bq.  
bq.  
bq.  This addresses bug HBASE-3229.
bq.      https://issues.apache.org/jira/browse/HBASE-3229
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java PRE-CREATION 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java 1161360 
bq.  
bq.  Diff: https://reviews.apache.org/r/1653/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  1. add a new test case to verify RPC timeout exception is gone when creating table with lots of regions.
bq.  2. hbase shell, unit tests.
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Ming
bq.  
bq.

;;;","25/Aug/11 18:52;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1653/#review1649
-----------------------------------------------------------



http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<https://reviews.apache.org/r/1653/#comment3687>

    Jonathan may not like another configuration parameter.
    I think this implementation is fine.
    
    All tests passed.



http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java
<https://reviews.apache.org/r/1653/#comment3688>

    


- Ted


On 2011-08-25 06:33:04, Ming Ma wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1653/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-25 06:33:04)
bq.  
bq.  
bq.  Review request for hbase.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  1. Make createTable truly async.
bq.  2. Fix up some incorrect comments, logs.
bq.  
bq.  
bq.  This addresses bug HBASE-3229.
bq.      https://issues.apache.org/jira/browse/HBASE-3229
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java PRE-CREATION 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java 1161360 
bq.  
bq.  Diff: https://reviews.apache.org/r/1653/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  1. add a new test case to verify RPC timeout exception is gone when creating table with lots of regions.
bq.  2. hbase shell, unit tests.
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Ming
bq.  
bq.

;;;","26/Aug/11 14:07;yuzhihong@gmail.com;+1 on patch.;;;","26/Aug/11 17:59;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1653/#review1661
-----------------------------------------------------------

Ship it!


We could commit this as is but there is one item below I'd like your opinion on first Ming.


http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<https://reviews.apache.org/r/1653/#comment3706>

    I think we need a bit more checking in here before we queue the Excecutor.  We should check if table already exists.  Its no good throwing this exception down in the Executor.  The client won't see it.  Can we check table existence up here before we queue the Executor while we have the client on the line?
    
    I had a quick look at table existence seems to be the only thing to check.



http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java
<https://reviews.apache.org/r/1653/#comment3707>

    Thanks.


- Michael


On 2011-08-25 06:33:04, Ming Ma wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1653/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-25 06:33:04)
bq.  
bq.  
bq.  Review request for hbase.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  1. Make createTable truly async.
bq.  2. Fix up some incorrect comments, logs.
bq.  
bq.  
bq.  This addresses bug HBASE-3229.
bq.      https://issues.apache.org/jira/browse/HBASE-3229
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java PRE-CREATION 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java 1161360 
bq.  
bq.  Diff: https://reviews.apache.org/r/1653/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  1. add a new test case to verify RPC timeout exception is gone when creating table with lots of regions.
bq.  2. hbase shell, unit tests.
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Ming
bq.  
bq.

;;;","26/Aug/11 18:49;jiraposter@reviews.apache.org;

bq.  On 2011-08-26 17:58:10, Michael Stack wrote:
bq.  > http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java, line 934
bq.  > <https://reviews.apache.org/r/1653/diff/3/?file=35507#file35507line934>
bq.  >
bq.  >     I think we need a bit more checking in here before we queue the Excecutor.  We should check if table already exists.  Its no good throwing this exception down in the Executor.  The client won't see it.  Can we check table existence up here before we queue the Executor while we have the client on the line?
bq.  >     
bq.  >     I had a quick look at table existence seems to be the only thing to check.

Stack, CreateTableHandler constructor will throw TableExistsException if the table exists. So CreateTableHandler object won't be created at the first place to be queued. So it should take care of your question, or you meant something else?


- Ming


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1653/#review1661
-----------------------------------------------------------


On 2011-08-25 06:33:04, Ming Ma wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1653/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-25 06:33:04)
bq.  
bq.  
bq.  Review request for hbase.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  1. Make createTable truly async.
bq.  2. Fix up some incorrect comments, logs.
bq.  
bq.  
bq.  This addresses bug HBASE-3229.
bq.      https://issues.apache.org/jira/browse/HBASE-3229
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java PRE-CREATION 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java 1161360 
bq.  
bq.  Diff: https://reviews.apache.org/r/1653/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  1. add a new test case to verify RPC timeout exception is gone when creating table with lots of regions.
bq.  2. hbase shell, unit tests.
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Ming
bq.  
bq.

;;;","26/Aug/11 18:57;jiraposter@reviews.apache.org;

bq.  On 2011-08-26 17:58:10, Michael Stack wrote:
bq.  > http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java, line 934
bq.  > <https://reviews.apache.org/r/1653/diff/3/?file=35507#file35507line934>
bq.  >
bq.  >     I think we need a bit more checking in here before we queue the Excecutor.  We should check if table already exists.  Its no good throwing this exception down in the Executor.  The client won't see it.  Can we check table existence up here before we queue the Executor while we have the client on the line?
bq.  >     
bq.  >     I had a quick look at table existence seems to be the only thing to check.
bq.  
bq.  Ming Ma wrote:
bq.      Stack, CreateTableHandler constructor will throw TableExistsException if the table exists. So CreateTableHandler object won't be created at the first place to be queued. So it should take care of your question, or you meant something else?

I didn't get that part.  I see now that the test is done in the handler constructor.  Nice one Ming.


- Michael


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1653/#review1661
-----------------------------------------------------------


On 2011-08-25 06:33:04, Ming Ma wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1653/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-25 06:33:04)
bq.  
bq.  
bq.  Review request for hbase.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  1. Make createTable truly async.
bq.  2. Fix up some incorrect comments, logs.
bq.  
bq.  
bq.  This addresses bug HBASE-3229.
bq.      https://issues.apache.org/jira/browse/HBASE-3229
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java PRE-CREATION 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java 1161360 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java 1161360 
bq.  
bq.  Diff: https://reviews.apache.org/r/1653/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  1. add a new test case to verify RPC timeout exception is gone when creating table with lots of regions.
bq.  2. hbase shell, unit tests.
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Ming
bq.  
bq.

;;;","26/Aug/11 19:00;stack;Thank you for the patch Ming and review, Ted.  Committed to TRUNK.;;;","26/Aug/11 20:59;hudson;Integrated in HBase-TRUNK #2147 (See [https://builds.apache.org/job/HBase-TRUNK/2147/])
    HBASE-3229 Table creation, though using async call to master, can actually run for a while and cause RPC timeout

stack : 
Files : 
* /hbase/trunk/CHANGES.txt
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java
;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in KeyValue$KVComparator.compare when compacting,HBASE-3224,12479669,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,ryanobjc,jdcryans,jdcryans,11/Nov/10 01:00,20/Nov/15 12:41,14/Jul/23 06:06,12/Nov/10 01:24,,,,,,,,,,,,0.90.0,,,,,,,0,,,"While testing normal insertion via PE, I got this recurrent NPE coming out of KeyValue$KVComparator.compare while it's compacting. So far I saw 2 different stack traces:

{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hbase.KeyValue$KVComparator.compare(KeyValue.java:1356)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap.reseek(KeyValueHeap.java:250)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:385)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:291)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:324)
	at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:926)
	at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:734)
{noformat}

{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hbase.KeyValue$KVComparator.compare(KeyValue.java:1375)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:180)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:156)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:146)
	at java.util.PriorityQueue.siftUpUsingComparator(PriorityQueue.java:594)
	at java.util.PriorityQueue.siftUp(PriorityQueue.java:572)
	at java.util.PriorityQueue.offer(PriorityQueue.java:274)
	at java.util.PriorityQueue.add(PriorityQueue.java:251)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap.reseek(KeyValueHeap.java:258)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:385)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:291)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:324)
	at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:926)

	at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:734)
{noformat}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/10 00:28;ryanobjc;HBASE-3224.txt;https://issues.apache.org/jira/secure/attachment/12459408/HBASE-3224.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26722,Reviewed,,,,Fri Nov 20 12:41:58 UTC 2015,,,,,,,,,,"0|i0hlaf:",100711,,,,,,,,,,,,,,,,,,,,,"11/Nov/10 01:12;jdcryans;IMO this issue is maybe even a blocker, I have two regions in that setup that can't split because the compactions are failing.;;;","11/Nov/10 01:21;jdcryans;I updated the second stack trace with latest code from trunk.;;;","11/Nov/10 08:05;ryanobjc;this is a bug in half hfile reader with reseekTo() which is causing scanners to have 'nulls' in the scanner stack thus kicking out this exception later on.

The nominal fix is, I think like so:
diff --git a/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java b/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java
index d35a28a..711e0d1 100644
--- a/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java
+++ b/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java
@@ -229,6 +229,9 @@ public class HalfStoreFileReader extends StoreFile.Reader {
             return 1;
           }
         }
+        if (!top && atEnd) {
+          return 1;
+        }
         return delegate.reseekTo(key, offset, length);
       }
 

I need to write a unit test, but that will have to wait for tomorrow.;;;","11/Nov/10 16:27;stack;This might be a little cleaner?

{code}
diff --git a/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java b/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java
index d35a28a..343e045 100644
--- a/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java
+++ b/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java
@@ -228,6 +228,7 @@ public class HalfStoreFileReader extends StoreFile.Reader {
             }
             return 1;
           }
+          if (atEnd) return 1;
         }
         return delegate.reseekTo(key, offset, length);
       }
{code}

I'm trying your proposed patch.;;;","11/Nov/10 17:59;stack;Patch seems to have gotten rid of NPEs.;;;","12/Nov/10 00:28;ryanobjc;here is a fix, with an adjustment of stack, and a unit test that fails w/o the fix being applied.;;;","12/Nov/10 00:36;stack;+1

I've been running fix in cluster during today and no more incidence of NPE.;;;","12/Nov/10 00:37;stack;Oh, our friends who contrib'd the new compaction code that brought on this bug don't split... thats why they hadn't run into the NPE.;;;","12/Nov/10 01:07;jdcryans;Tried it, looks much better now. Thanks Ryan!;;;","12/Nov/10 01:24;ryanobjc;committed, thanks for the reviews;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regionserver region listing in UI is no longer ordered.,HBASE-3222,12479667,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,11/Nov/10 00:37,20/Nov/15 12:42,14/Jul/23 06:06,11/Nov/10 00:39,,,,,,,,,,,,0.90.0,,,,,,,0,,,J-D spotted this one.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/10 00:38;stack;sort.txt;https://issues.apache.org/jira/secure/attachment/12459305/sort.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26721,,,,,Fri Nov 20 12:42:21 UTC 2015,,,,,,,,,,"0|i0hl9z:",100709,,,,,,,,,,,,,,,,,,,,,"11/Nov/10 00:38;stack;Added sort to RS UI.;;;","11/Nov/10 00:39;stack;Committing small patch after playing with it on cluster.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race between splitting and disabling,HBASE-3221,12479663,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,jdcryans,jdcryans,jdcryans,10/Nov/10 23:51,20/Nov/15 12:42,14/Jul/23 06:06,11/Nov/10 06:17,,,,,,,,,,,,0.90.0,,,,,,,0,,,"There's a race when you disable a table, if one of the region servers started the split before the call and it reports after the master scanned the .META. regions, then you're not disabling the daughter regions. I see this in the master log:

{noformat}
2010-11-10 15:29:35,990 INFO org.apache.hadoop.hbase.master.handler.DisableTableHandler: Attemping to disable table TestTable
2010-11-10 15:29:35,996 INFO org.apache.hadoop.hbase.master.handler.DisableTableHandler: Offlining 2 regions.
...
2010-11-10 15:29:39,014 INFO org.apache.hadoop.hbase.master.handler.DisableTableHandler: Disabled table is done=true
2010-11-10 15:29:39,105 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: TestTable,,1289431761746.9de9305168be83098273e083f24ea5d8.:
 Daughters; TestTable,,1289431775593.a08b127a61b89e268129aa022fd18ce1.,
 TestTable,0001037720,1289431775593.4a5f831723ffbdb859d45510742d9926. from hbasedev,60020,1289431673756
...
{noformat}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/10 00:51;jdcryans;HBASE-3221.patch;https://issues.apache.org/jira/secure/attachment/12459308/HBASE-3221.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26720,Reviewed,,,,Fri Nov 20 12:42:06 UTC 2015,,,,,,,,,,"0|i0hl9r:",100708,,,,,,,,,,,,,,,,,,,,,"11/Nov/10 00:51;jdcryans;This patch survived an onslaught of disabling/enabling mixed with massive imports. It unassigns the daughter regions when the RS checks in with the split message if the table is disabled/disabling.;;;","11/Nov/10 00:55;jdcryans;For the record, this kind of stuff was impossible in the old master. Really exciting that it's finally fixed!;;;","11/Nov/10 05:59;stack;+1

Small patch.  Go commit.;;;","11/Nov/10 06:17;jdcryans;Thanks Stack, I committed this to trunk.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split parents are reassigned on restart and on disable/enable,HBASE-3219,12479653,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,10/Nov/10 23:02,20/Nov/15 12:44,14/Jul/23 06:06,11/Nov/10 00:20,,,,,,,,,,,,0.90.0,,,,,,,0,,,J-D found this nice bug testing.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/10 23:02;stack;split.txt;https://issues.apache.org/jira/secure/attachment/12459292/split.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26719,Reviewed,,,,Fri Nov 20 12:44:04 UTC 2015,,,,,,,,,,"0|i0hl9j:",100707,,,,,,,,,,,,,,,,,,,,,"10/Nov/10 23:46;jdcryans;+1;;;","11/Nov/10 00:20;stack;Committed.;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestRollingRestart failing on hudson,HBASE-3215,12479554,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,10/Nov/10 02:48,20/Nov/15 12:41,14/Jul/23 06:06,11/Nov/10 01:11,0.90.0,,,,,,,,,,,0.90.0,,test,,,,,0,,,Seems unrelated to HBASE-3214.  I think it must be in the one sleep thing I'm doing in this unit test.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26717,,,,,Fri Nov 20 12:41:40 UTC 2015,,,,,,,,,,"0|i0hl8n:",100703,,,,,,,,,,,,,,,,,,,,,"10/Nov/10 02:50;streamy;Committing below change to see if it fixes hudson:

{noformat}
--- src/test/java/org/apache/hadoop/hbase/master/TestRollingRestart.java	(revision 1033306)
+++ src/test/java/org/apache/hadoop/hbase/master/TestRollingRestart.java	(working copy)
@@ -307,7 +307,7 @@
     // First wait for it to be in dead list
     while (!sm.deadservers.isDeadServer(serverName)) {
       log(""Waiting for ["" + serverName + ""] to be listed as dead in master"");
-      Thread.sleep(100);
+      Thread.sleep(1);
     }
     log(""Server ["" + serverName + ""] marked as dead, waiting for it to "" +
         ""finish dead processing"");
{noformat};;;","11/Nov/10 01:11;streamy;TestRollingRestart passing on hudson now.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestMasterFailover.testMasterFailoverWithMockedRITOnDeadRS is failing,HBASE-3214,12479551,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ghelmling,streamy,streamy,10/Nov/10 02:05,20/Nov/15 12:40,14/Jul/23 06:06,11/Nov/10 02:07,0.90.0,,,,,,,,,,,0.90.0,,test,,,,,0,,,Failing on hudson and locally,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/10 15:31;ghelmling;HBASE-3214.patch;https://issues.apache.org/jira/secure/attachment/12459248/HBASE-3214.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26716,,,,,Fri Nov 20 12:40:30 UTC 2015,,,,,,,,,,"0|i0hl8f:",100702,,,,,,,,,,,,,,,,,,,,,"10/Nov/10 02:11;streamy;This is what I see locally as the failure:

{noformat}
2010-11-09 18:07:17,291 FATAL [Master:0;172.24.154.154:56721] master.HMaster(888): Unhandled exception. Starting shutdown.
java.lang.RuntimeException: Failed exists test on hdfs://localhost:56643/user/jgray/.logs
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLogAfterStartup(MasterFileSystem.java:162)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:374)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:272)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:232)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:623)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:453)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:648)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLogAfterStartup(MasterFileSystem.java:158)
	... 3 more
{noformat}

Somehow DFS is being closed.  No idea why this is happening now but wasn't before.

Shortly before this exception, I see this log line:
{noformat}
2010-11-09 18:07:12,413 INFO  [Shutdown of DFS[DFSClient[clientName=DFSClient_hb_rs_172.24.154.154,56663,1289354815892_1289354817252, ugi=jgray.hfs.1,supergroup]]] hbase.MiniHBaseCluster$SingleFileSystemShutdownThread(248): Hook closing fs=DFS[DFSClient[clientName=DFSClient_hb_rs_172.24.154.154,56663,1289354815892_1289354817252, ugi=jgray.hfs.1,supergroup]]
{noformat}

Looks like an RS exiting is now triggering a complete shutdown of DFS.

If I comment out the below line in MiniHBaseCluster line 189, the test passes.
{noformat}
      this.shutdownThread = new SingleFileSystemShutdownThread(getFileSystem());
{noformat}

What has changed?  In unit tests, if RS is being shut down, should not take the entire FS with it?;;;","10/Nov/10 02:19;streamy;Looks like this was broken with commit of HBASE-3194 (hbase should run on secure + vanilla hadoop);;;","10/Nov/10 02:20;streamy;Also, TestRollingRestart is timing out on hudson but passes locally.  If it times out seems like we can't access the output logs on hudson?;;;","10/Nov/10 02:22;streamy;And I'm seeing loads of this ""Shutdown of DFS"" in my passing TestRollingRestart, so doesn't seem like it always breaks tests but for some reason it's now causing breakage in TestMasterFailover?;;;","10/Nov/10 02:31;streamy;Well, I kind of lied.  Looks like I'm still getting loads of ""Filesystem closed"" exceptions in TestRollingRestart even when it passes but they are doing shutdown handling of RSs and we don't abort anything we just can't do log replay (which is not necessary in this test).

So definitely something fishy with commit of HBASE-3194 and unit tests / aborting RS bringing down DFS underneath running unit test cluster / master.;;;","10/Nov/10 02:53;ghelmling;Looking into it.  Possibly something not accounted for in the changes to MiniHBaseCluster.;;;","10/Nov/10 05:07;ghelmling;Okay, this is the result of an interaction between instance caching in FileSystem.get() based on Configuration contents, and the change to MiniHBaseCluster.MiniHBaseClusterRegionServer to not call HBaseTestingUtility.setDifferentUser() in the constructor.  As a result, the MiniHBaseCluster.conf instance is being modified with the new hadoop.job.ugi entry when each RS is started, and it is this configuration which is picked up when a new master is started at TestMasterFailover, line 801.  Since the contents of the configuration are the same, it picks up the FileSystem instance that was closed by the shutdown hook in the ""dead"" RS.

I'll post a fix as soon as I've run it through the full test suite locally.;;;","10/Nov/10 15:28;ghelmling;Here's a patch that fixes TestMasterFailover for me.  This may also explain the  ""FileSystem closed"" errors showing up in TestRollingRestart, but that's hard to confirm if it's just failing up in hudson.  It does fix the ""FileSystem closed"" error that was aborting the master restart in TestMasterFailover.

With this fix, all tests pass locally for me.  But while testing, I did see intermittent failures of 
{noformat}
org.apache.hadoop.hbase.replication.TestReplication.queueFailover()
{noformat}

It's not clear to me that this is the same issue though, and the failures weren't consistent.  So I don't think we should hold up this fix for it.

;;;","10/Nov/10 15:31;ghelmling;Same attachment as previously, but selecting the correct ""Attachment license"" option this time.  Sorry for the noise.
;;;","10/Nov/10 17:56;streamy;Thanks for the patch, Gary.  I just committed to trunk.  Let's make sure Hudson passes before closing this out.;;;","10/Nov/10 18:02;streamy;btw, tests are passing locally w/ this patch.;;;","10/Nov/10 21:38;ghelmling;Looks like TestMasterFailover.testMasterFailoverWithMockedRITOnDeadRS is still failing up on hudson with this patch.  But now at least it's an assertion failure instead of a timeout:

{noformat}
java.lang.AssertionError: 
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertFalse(Assert.java:68)
	at org.junit.Assert.assertFalse(Assert.java:79)
	at org.apache.hadoop.hbase.master.TestMasterFailover.testMasterFailoverWithMockedRITOnDeadRS(TestMasterFailover.java:855)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.FailOnTimeout$1.run(FailOnTimeout.java:28)
{noformat}

This is down in the check that regions are offline:
{code}
    // Everything that should be offline should not be online
    for (HRegionInfo hri : regionsThatShouldBeOffline) {
      assertFalse(onlineRegions.contains(hri));
    }
{code}

The strange thing is that not only does this pass for me locally, it also passes in our internal hudson build of trunk.;;;","11/Nov/10 02:07;streamy;Passed on latest hudson build.  Will open new jiras if anything new crops up.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
If do abort of backup master will get NPE instead of graceful abort,HBASE-3213,12479550,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,streamy,streamy,streamy,10/Nov/10 01:57,20/Nov/15 12:42,14/Jul/23 06:06,10/Nov/10 05:37,0.90.0,,,,,,,,,,,0.90.0,,master,,,,,0,,,"Doesn't really matter because it's aborting the server anyways, but I've seen it on TestMasterFailover.  Simple fix.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26715,Reviewed,,,,Fri Nov 20 12:42:15 UTC 2015,,,,,,,,,,"0|i0hl87:",100701,,,,,,,,,,,,,,,,,,,,,"10/Nov/10 01:58;streamy;Fix:

{noformat}
--- src/main/java/org/apache/hadoop/hbase/master/HMaster.java	(revision 1033306)
+++ src/main/java/org/apache/hadoop/hbase/master/HMaster.java	(working copy)
@@ -278,7 +278,8 @@
       stopChores();
       // Wait for all the remaining region servers to report in IFF we were
       // running a cluster shutdown AND we were NOT aborting.
-      if (!this.abort && this.serverManager.isClusterShutdown()) {
+      if (!this.abort && this.serverManager != null &&
+          this.serverManager.isClusterShutdown()) {
         this.serverManager.letRegionServersShutdown();
       }
       stopServiceThreads();
{noformat}

Will commit if someone gives me +1;;;","10/Nov/10 05:12;stack;+1;;;","10/Nov/10 05:37;streamy;Committed to trunk, thanks stack.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"More testing of enable/disable uncovered base condition not in place; i.e. that only one enable/disable runs at a time",HBASE-3212,12479541,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,10/Nov/10 00:06,20/Nov/15 12:43,14/Jul/23 06:06,10/Nov/10 00:19,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Testing, uncovered fact that master has 3 handlers currently for enable/disable/delete and modify when hbase-3112 was built on supposition that there was only one.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/10 00:07;stack;3212.txt;https://issues.apache.org/jira/secure/attachment/12459201/3212.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26714,Reviewed,,,,Fri Nov 20 12:43:18 UTC 2015,,,,,,,,,,"0|i0hl7z:",100700,,,,,,,,,,,,,,,,,,,,,"10/Nov/10 00:07;stack;Please review.;;;","10/Nov/10 00:16;streamy;Looks like one of those lines may have gone over 80.

If for correctness we require only one thread in enable/disable executor pool, then how can we make it configurable?  I know it's undocumented config just saying, we may just hard-code as 1 for now since it's only one supported.

Otherwise, +1, commit!;;;","10/Nov/10 00:19;stack;Thanks for review Jon.;;;","10/Nov/10 00:21;stack;On commit, I implemented your suggestion of not having the number of threads for ExecutorType.MASTER_TABLE_OPERATIONS configurable.  Thanks.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HLog.findMemstoresWithEditsOlderThan needs to look for edits that are equal to too,HBASE-3208,12479435,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,08/Nov/10 23:39,20/Nov/15 12:43,14/Jul/23 06:06,09/Nov/10 00:02,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Follow up to HBASE-3198, now that we are doing the right thing it seems that TestWALObserver is throwing NPEs. Digging in more, I figured that HLog.findMemstoresWithEditsOlderThan actually needs to be looking at edits that are equal or older rather than just older since logs with only 1 edit won't get any regions to flush.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-3198,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26713,Reviewed,,,,Fri Nov 20 12:43:08 UTC 2015,,,,,,,,,,"0|i0hl73:",100696,,,,,,,,,,,,,,,,,,,,,"08/Nov/10 23:48;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1188/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

This patch simply adds the ""<="" in findMemstoresWithEditsOlderThan, renames it to findMemstoresWithEditsEqualOrOlderThan, and fixes the unit tests.


This addresses bug HBASE-3208.
    http://issues.apache.org/jira/browse/HBASE-3208


Diffs
-----

  /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java 1032760 
  /trunk/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java 1032760 

Diff: http://review.cloudera.org/r/1188/diff


Testing
-------


Thanks,

Jean-Daniel


;;;","08/Nov/10 23:59;hbasereviewboard;Message from: ""Kannan Muthukkaruppan"" <kannan@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1188/#review1850
-----------------------------------------------------------

Ship it!


Make sense. LGTM.


- Kannan



;;;","08/Nov/10 23:59;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1188/#review1851
-----------------------------------------------------------

Ship it!


looks good to me

- Jonathan



;;;","09/Nov/10 00:01;jdcryans;Committed to trunk, thanks for the reviews guys!;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If we get IOException when closing a region, we should still remove it from online regions and complete the close in ZK",HBASE-3207,12479433,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,08/Nov/10 23:15,20/Nov/15 12:40,14/Jul/23 06:06,09/Nov/10 22:06,0.90.0,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"Ran into issue on cluster where HDFS was taken out from under it.  RS eventually tried to shut itself down.  As regions were being closed, they got IOException ""Filesystem closed"".  In the CloseRegionHandlers, this was causing the close operation to not finish (in ZK and in the online region list in RS).  That, in turn, held up the waitOnAllRegionsToClose() so the RS never shut down.

If we get an IOException during a close, which can happen if fatal error doing flush, this is not recoverable so we should complete the region close in ZK and by removing from map of online regions on that RS.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/10 23:18;streamy;HBASE-3207-v1.patch;https://issues.apache.org/jira/secure/attachment/12459102/HBASE-3207-v1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26712,Reviewed,,,,Fri Nov 20 12:40:41 UTC 2015,,,,,,,,,,"0|i0hl6v:",100695,,,,,,,,,,,,,,,,,,,,,"08/Nov/10 23:18;streamy;Just makes it so we do the ZK transition and remove from online regions, even if IOException.  Adds a little more detail to logging and comments.;;;","09/Nov/10 00:16;stack;+1

(In IRC Jon explained that we don't need deleteClosingState anymore because region is now actually considered closed).;;;","09/Nov/10 22:04;stack;You going to commit or what? JG?;;;","09/Nov/10 22:06;streamy;Committed.  Thanks for review stack.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableRecordReaderImpl.restart NPEs when first next is restarted,HBASE-3205,12479416,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,jdcryans,jdcryans,jdcryans,08/Nov/10 21:16,12/Oct/12 06:17,14/Jul/23 06:06,08/Nov/10 22:42,0.89.20100924,,,,,,,,,,,0.90.0,,,,,,,0,,,"We got this pretty interesting NPE out of TableRecordReaderImpl.restart on a job that was filtering more than 99% of the data from a very huge table with caching set to 10k:

{noformat}
2010-11-08 13:08:22,344 DEBUG org.apache.hadoop.hbase.mapreduce.TableRecordReader:
 recovered from org.apache.hadoop.hbase.client.ScannerTimeoutException:
 61521ms passed since the last invocation, timeout is currently set to 60000
	at org.apache.hadoop.hbase.client.HTable$ClientScanner.next(HTable.java:956)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.nextKeyValue(TableRecordReaderImpl.java:132)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReader.nextKeyValue(TableRecordReader.java:142)
...
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
Caused by: org.apache.hadoop.hbase.UnknownScannerException: org.apache.hadoop.hbase.UnknownScannerException:
 Scanner was closed (timed out?) after we renewed it. Could be caused by a very slow scanner or a lengthy garbage collection
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:2233)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:2260)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1933)
...
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:81)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:37)
	at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getRegionServerWithRetries(HConnectionManager.java:1138)
	at org.apache.hadoop.hbase.client.HTable$ClientScanner.next(HTable.java:942)
	... 8 more

2010-11-08 13:08:22,347 WARN org.apache.hadoop.mapred.TaskTracker: Error running child
java.lang.NullPointerException
	at org.apache.hadoop.hbase.util.Bytes.toStringBinary(Bytes.java:301)
	at org.apache.hadoop.hbase.client.HTable$ClientScanner.<init>(HTable.java:803)
	at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:484)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.restart(TableRecordReaderImpl.java:58)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.nextKeyValue(TableRecordReaderImpl.java:135)
...
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
2010-11-08 13:08:22,349 INFO org.apache.hadoop.mapred.TaskRunner
{noformat}

This is because the last row key we saw is set to null, since we haven't seen any yet :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26711,Reviewed,,,,Mon Nov 08 22:42:22 UTC 2010,,,,,,,,,,"0|i08sf3:",49204,,,,,,,,,,,,,,,,,,,,,"08/Nov/10 21:34;jdcryans;It's a very simple fix, will commit in a few minutes if no objections:

{code}
+      if (lastRow == null) {
+        LOG.warn(""We are restarting the first next() invocation,"" +
+            "" if your mapper's restarted a few other times like this"" +
+            "" then you should consider killing this job and investigate"" +
+            "" why it's taking so long."");
+        lastRow = scan.getStartRow();
+      }
{code};;;","08/Nov/10 21:38;streamy;Seems fine to me.  +1;;;","08/Nov/10 22:42;jdcryans;Committed to trunk, thanks for taking a look Jon.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reenable deferred log flush,HBASE-3204,12479271,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,05/Nov/10 23:19,20/Nov/15 12:42,14/Jul/23 06:06,05/Nov/10 23:38,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Deferred log flush was disabled a few months ago, reenable it and make it false by default.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/10 23:25;jdcryans;HBASE-3204.patch;https://issues.apache.org/jira/secure/attachment/12458968/HBASE-3204.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26710,Reviewed,,,,Fri Nov 20 12:42:06 UTC 2015,,,,,,,,,,"0|i0hl6f:",100693,,,,,,,,,,,,,,,,,,,,,"05/Nov/10 23:25;jdcryans;This patch is only what it takes. It looks like this patch could be refactored but that's really how both append method work at the moment.;;;","05/Nov/10 23:33;streamy;more simple than i could have imagined... +1;;;","05/Nov/10 23:38;jdcryans;Committed to trunk.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We can get an order to open a region while shutting down and it'll hold up regionserver shutdown,HBASE-3203,12479269,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,05/Nov/10 22:45,20/Nov/15 12:41,14/Jul/23 06:06,05/Nov/10 23:01,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Starting and stopping clusters I see that an open directive can come in while we are shutting down all the user space regions on a particular regionserver.  Regionservers will only shut themselves down after all user space regions have closed.  We queue up all the closes at a particular time.  If an open comes in while we are in the closing condition, then the regionserver won't go down.  The region open needs to be blocked.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/10 22:51;stack;stopping-v2.txt;https://issues.apache.org/jira/secure/attachment/12458964/stopping-v2.txt","05/Nov/10 22:59;stack;stopping-v3.txt;https://issues.apache.org/jira/secure/attachment/12458965/stopping-v3.txt","05/Nov/10 22:46;stack;stopping.txt;https://issues.apache.org/jira/secure/attachment/12458962/stopping.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26709,,,,,Fri Nov 20 12:41:16 UTC 2015,,,,,,,,,,"0|i0hl67:",100692,,,,,,,,,,,,,,,,,,,,,"05/Nov/10 22:51;stack;Add check when splitting too.;;;","05/Nov/10 23:01;streamy;looks fine to me.  kind of unfortunate we have new 'stopping' state, but guess we need it because can't just have 'stopped' earlier?  makes sense i suppose.  anything else we want to reject if closing?

good fix on that completely wrong comment :)

+1 to commit if you've done some testing w/ it;;;","05/Nov/10 23:01;stack;Committed small fix (Been testing up on cluster).;;;","05/Nov/10 23:38;stack;Thanks for the review.

We used to do 'stopping'.  Smile.

'Stopping' gets us closer to classic Lifecycle.

I think just needed for this at mo.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Closing a region, if we get a ConnectException, handle it rather than abort",HBASE-3202,12479266,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,05/Nov/10 22:10,20/Nov/15 12:40,14/Jul/23 06:06,05/Nov/10 22:12,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Got this testing:
{code}
2010-11-05 21:58:41,259 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region usertable,user1117790996,1288993795362.0d5d2055ef9d7ee5f11097d58461211f. (offlining)
2010-11-05 21:58:41,261 FATAL org.apache.hadoop.hbase.master.HMaster: Remote unexpected exception
java.net.ConnectException: Connection refused
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
    at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
    at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:404)
    at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:311)
    at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:865)
    at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:732)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
    at $Proxy1.closeRegion(Unknown Source)
    at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:564)
    at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1019)
    at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:966)
    at org.apache.hadoop.hbase.master.handler.DisableTableHandler.handleDisableTable(DisableTableHandler.java:84)
    at org.apache.hadoop.hbase.master.handler.DisableTableHandler.process(DisableTableHandler.java:64)
    at org.apache.hadoop.hbase.master.HMaster.disableTable(HMaster.java:803)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
    at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1025)
2010-11-05 21:58:41,263 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/10 22:12;stack;3202.txt;https://issues.apache.org/jira/secure/attachment/12458961/3202.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26708,,,,,Fri Nov 20 12:40:49 UTC 2015,,,,,,,,,,"0|i0hl5z:",100691,,,,,,,,,,,,,,,,,,,,,"05/Nov/10 22:12;stack;Small patch.;;;","05/Nov/10 22:12;stack;Committed small patch.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add accounting of empty regioninfo_qualifier rows in meta to hbasefsck.,HBASE-3201,12479262,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,05/Nov/10 21:21,20/Nov/15 12:40,14/Jul/23 06:06,05/Nov/10 21:25,,,,,,,,,,,,0.90.0,,,,,,,0,,,Make an accounting of empty HREGION_QUALIFIER rows in .META.  Emit count and details in hbase fsck too.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26707,,,,,Fri Nov 20 12:40:57 UTC 2015,,,,,,,,,,"0|i0hl5r:",100690,,,,,,,,,,,,,,,,,,,,,"05/Nov/10 21:25;stack;Committed small patch that counts empty HRegionInfo qualifiers in .META. and emits as part of the hbasefsck report.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
large response handling: some fixups and cleanups,HBASE-3199,12479245,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,kannanm,kannanm,05/Nov/10 18:35,20/Nov/15 12:43,14/Jul/23 06:06,09/Nov/10 22:58,,,,,,,,,,,,0.90.0,,,,,,,0,,,"This may not be common for many use cases, but it might be good to put a couple of safety nets as well as logging to protect against large responses.

(i) Aravind and I were trying to track down why JVM memory usage was oscillating so much when dealing with very large buffers rather than OOM'ing or hitting some Index out of bound type exception, and this is what we found.

java.io.ByteArrayOutputStream graduates its internal buffers by doubling them. Also, it is supposed to be able to handle ""int"" sized buffers (2G). The code which handles ""write"" (in jdk 1.6) is along the lines of:

{code}
   public synchronized void write(byte b[], int off, int len) {
	if ((off < 0) || (off > b.length) || (len < 0) ||
            ((off + len) > b.length) || ((off + len) < 0)) {
	    throw new IndexOutOfBoundsException();
	} else if (len == 0) {
	    return;
	}
        int newcount = count + len;
        if (newcount > buf.length) {
            buf = Arrays.copyOf(buf, Math.max(buf.length << 1, newcount));
        }
        System.arraycopy(b, off, buf, count, len);
        count = newcount;
    }
{code}

The ""buf.length << 1"" will start producing -ve values when buf.length reaches 1G, and ""newcount"" will instead dictate the size of the buffer allocated. At this point, all attempts to write to the buffer will grow linearly, and the buffer will be resized by only the required amount on each write. Effectively, each write will allocate a new 1G buffer + reqd size buffer, copy the contents, and so on. This will put the process in heavy GC mode (with jvm heap oscillating by several GBs rapidly), and render it practically unusable.

(ii) When serializing a Result, the writeArray method doesn't assert that the resultant size does not overflow an ""int"".

{code}
    int bufLen = 0;
    for(Result result : results) {
      bufLen += Bytes.SIZEOF_INT;
      if(result == null || result.isEmpty()) {
        continue;
      }
      for(KeyValue key : result.raw()) {
        bufLen += key.getLength() + Bytes.SIZEOF_INT;
      }
    }
{code}

We should do the math in ""long"" and assert on bufLen values > Integer.MAX_VALUE.

(iii) In HBaseServer.java on RPC responses, we could add some logging on responses above a certain thresholds.

(iv) Increase buffer size threshold for buffers that are reused by RPC handlers. And make this configurable. Currently, any response buffer about 16k is not reused on next response. (HBaseServer.java).



",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/10 23:48;ryanobjc;HBASE-3199-2.txt;https://issues.apache.org/jira/secure/attachment/12458973/HBASE-3199-2.txt","08/Nov/10 04:26;ryanobjc;HBASE-3199-3.txt;https://issues.apache.org/jira/secure/attachment/12459033/HBASE-3199-3.txt","08/Nov/10 21:56;ryanobjc;HBASE-3199-4.txt;https://issues.apache.org/jira/secure/attachment/12459090/HBASE-3199-4.txt","05/Nov/10 23:00;ryanobjc;HBASE-3199.txt;https://issues.apache.org/jira/secure/attachment/12458966/HBASE-3199.txt","05/Nov/10 21:10;kannanm;HBASE-3199_prelim.txt;https://issues.apache.org/jira/secure/attachment/12458954/HBASE-3199_prelim.txt",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26706,Reviewed,,,,Fri Nov 20 12:43:35 UTC 2015,,,,,,,,,,"0|i0hl5b:",100688,,,,,,,,,,,,,,,,,,,,,"05/Nov/10 18:38;kannanm;For (i), the thought was to subclass ByteArrayOutputStream, and override the write method to something like:

{code}
  public synchronized void write(byte b[], int off, int len) {
    if ((off < 0) || (off > b.length) || (len < 0) ||
        ((off + len) > b.length) || ((off + len) < 0)) {
      throw new IndexOutOfBoundsException();
    } else if (len == 0) {
      return;
    }

    int newcount = count + len;
    if (newcount > buf.length) {
      int newSize = (int)Math.min((((long)buf.length) << 1),                <<<<<<<< proposed change.
                                  (long)(Integer.MAX_VALUE));      
      buf = Arrays.copyOf(buf, Math.max(newSize, newcount));
    }
    System.arraycopy(b, off, buf, count, len);
    count = newcount;
  }
}
{code};;;","05/Nov/10 19:20;hairong;After talking to Konstantin, we decided to create the throttler on the fly for each file transfer. This patch does this.;;;","05/Nov/10 19:22;hairong;Oops, commented on the wrong jira. I wish that I could delete the previous comment.;;;","05/Nov/10 19:22;ryanobjc;I have a fix for all that and more. Ill post it in 2 hours
;;;","05/Nov/10 19:23;hairong;newcount calculation should guard against overflow too.;;;","05/Nov/10 21:10;kannanm;Prelim patch for review/merge with Ryan's work.

Chatted with Ryan in IRC, and he has more in-depth fix to avoid the whole ""double the buffer as you grow"" approach and replace it with a ""precompute size of buffer in one pass and then alloc what you need"". So a good portion of my patch might be superceded by his. Still submitting my  patch so that Ryan  can do the needed merge/union of parts in this patch that are orthogonal to his changes.;;;","05/Nov/10 23:00;ryanobjc;here is my base patch, i'll merge in the other in a moment here;;;","05/Nov/10 23:24;ryanobjc;looks like my patch does nearly everything yours does except handle extremely large size replies.  I'm going to go thru that and use longs and also add some exceptions so that those calls should fail with IOEs instead of choke the regionserver.;;;","05/Nov/10 23:48;ryanobjc;here is a new version which returns exceptions instead of weirdly failing for really large Result buffers.

For super completeness this patch also needs to have WritableWithSize applied to the multi result object as well.  ;;;","06/Nov/10 17:01;kannanm;Ryan,

Looks awesome. Nice work eliminating a bunch of copies. Few minor comments:

1) In the size computation:
{code}
+    long size = kvs.length;                                           <<<< The ""int"" per key is accounted for in the loop below.
+    size *= Bytes.SIZEOF_INT; // 1 int for each kv.   <<< So this seems unnecessaru/
+
+    size += Bytes.SIZEOF_INT;
+
+    for (KeyValue kv : kvs) {
+      size += kv.getLength() + Bytes.SIZEOF_INT;
+    }
{code}

2) checkSizeAndGrow() could probably use some protection against Integer.MAX_VALUE overflow.  (Similar to what my patch does in the HBaseByteArrayOutputStream:write() methods).

3) Will you be adding the ""log responses above a configurable threshold"" changes from my patch?


;;;","06/Nov/10 17:02;kannanm;My first comment above didn't format correctly...

1) in the size computation for Result.write() case, the ""int"" per kv is accounted for in the loop already. So it seems like we are double counting for those.
;;;","08/Nov/10 04:25;ryanobjc;1. you are totally correct here!  I fixed that... I had also fixed it in a subsequent patch in my git.

2. We won't need explicit protection, we get it from ByteBuffer.allocate(). 

3. what is the motivation for this feature?  could add it, but im not seeing the use case personally. ;;;","08/Nov/10 05:12;kannanm;#3 is mainly for debuggability. If there are some bad queries this'll help debug what the RPC and its params were. 

#2. In checkSizeAndGrow(): 

{code}
     ByteBuffer newBuf = ByteBuffer.allocate(buf.capacity() * 2);    <<<<<<<  int overflow possible when we reach 1G range.
{code}         

When buf.capacity() say reaches 1G + something, buf.capacity() * 2 will become negative. It is not clear what ByteBuffer.allocate() will do, but we don't want this case to cause an exception since 1G + something is still within an ""int"". At this point, we should resize to 2G (since we can't go past 2G).

So,how about something like:

{code}
 ByteBuffer newBuf = ByteBuffer.allocate((int)Math.min(  (long)buf.capacity() * 2,
                                                                                                     (long)(Integer.MAX_VALUE)));
{code}



;;;","08/Nov/10 05:13;kannanm;Reassigning to you since it is pretty much going to be your patch.;;;","08/Nov/10 20:10;kannanm;Seeing this in testing:
{code}
2010-11-08 11:42:10,380 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 15 on 62469 caught: java.lang.NullPointerException
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.getWritableSize(HbaseObjectWritable.java:301)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.getWritableSize(HbaseObjectWritable.java:235)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:933)
{code}

Seems like ""instance"" inside HBaseObjectWritable can be null, and the code is currently not safely handling that case.
;;;","08/Nov/10 20:21;kannanm;I think we need something like this at the beginning of HbaseObjectWritable.java:getWritableSize()

{code}
    if (instance == null) {
      return 0L; // no hint is the default
    }
{code};;;","08/Nov/10 20:44;kannanm;Ryan: 

In my testing, we are still going in the resize (buffer doubling code) even for the Result ""write"". I think there is still some off by one error which is causing us to do one extra doubling/copy than necessary.

Looking....;;;","08/Nov/10 21:56;ryanobjc;a patch with fixes and implements all of kannan's suggestions;;;","08/Nov/10 22:53;kannanm;+1 on latest patch. The off by one errors seem gone. Am running the unit tests as well right now.;;;","09/Nov/10 05:00;stack;Marking patch available and bringing into 0.90.0.  Important bug fix.;;;","09/Nov/10 15:21;kannanm;Stack: Please note there were two more int overflow type fixes that Aravind & Hairong pointed out. Have communicated that to Ryan. He'll upload a revised patch.;;;","09/Nov/10 22:58;ryanobjc;resolved, thanks for the reviews and suggestions.  All feedback has been rolled into the committed patch (which was not published to jira).;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log rolling archives files prematurely,HBASE-3198,12479237,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,05/Nov/10 17:40,20/Nov/15 12:41,14/Jul/23 06:06,05/Nov/10 22:50,,,,,,,,,,,,0.90.0,,,,,,,0,,,"From the mailing list, Erdem Agaoglu found a case where when an HLog gets rolled from the periodic log roller and it gets archived even tho the region (ROOT) still has edits in the MemStore. I did an experiment on a local empty machine and it does look broken:

{noformat}
org.apache.hadoop.hbase.regionserver.LogRoller: Hlog roll period 6000ms elapsed
org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: Using syncFs -- HDFS-200
org.apache.hadoop.hbase.regionserver.wal.HLog: Roll /hbase-89-su/.logs/hbasedev,60020,1288977933643/10.10.1.177%3A60020.1288977933829, entries=1,
 filesize=295. New hlog /hbase-89-su/.logs/hbasedev,60020,1288977933643/10.10.1.177%3A60020.1288977943913
org.apache.hadoop.hbase.regionserver.wal.HLog: Found 1 hlogs to remove  out of total 1; oldest outstanding sequenceid is 270055 from region -ROOT-,,0
org.apache.hadoop.hbase.regionserver.wal.HLog: moving old hlog file /hbase-89-su/.logs/hbasedev,60020,1288977933643/10.10.1.177%3A60020.1288977933829
 whose highest sequenceid is 270054 to /hbase-89-su/.oldlogs/10.10.1.177%3A60020.1288977933829
{noformat}

Marking as Blocker and taking a deeper look.",,khemani,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-3208,,,,,,,,,,,,,,,,,"05/Nov/10 22:49;jdcryans;HBASE-3198_for_0.89.patch;https://issues.apache.org/jira/secure/attachment/12458963/HBASE-3198_for_0.89.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26705,Reviewed,,,,Fri Nov 20 12:41:45 UTC 2015,,,,,,,,,,"0|i0hl53:",100687,,,,,,,,,,,,,,,,,,,,,"05/Nov/10 17:48;kannanm;JD: Keep us posted. This looks important. Thanks for looking into this.;;;","05/Nov/10 21:04;jdcryans;So some digging revealed interesting things. When we print the ""whose sequenceid is x"", it's always smaller than the real one by 1 since in the code we do:

{code}
this.outputfiles.put(Long.valueOf(this.logSeqNum.get() - 1), oldFile);
{code}

It may have been right to do this at some point in the past, but now since rolling is async from appending it means that the current logSeqNum is in fact the last one in the log. It's wrong to -1. Then there's this:

{code}
    TreeSet<Long> sequenceNumbers =
    new TreeSet<Long>(this.outputfiles.headMap(
      (Long.valueOf(oldestOutstandingSeqNum.longValue() + 1L))).keySet());
{code}

Here we are getting the log files that we can delete since we know that their oldest edit's sequence number is still smaller than the oldest edit. I don't know why we're doing a +1L, since you don't really  want to delete log files that do contain it. It may be a ""fix"" to my previous finding, but it's still broken since as I showed when creating this jira rolling does remove logs with unflushed edits.

I'm changing the title of this jira to a more broad scope, as any log rolling is at risk of lowering data durability.;;;","05/Nov/10 21:24;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1181/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Removes the +1, -1 madness that I described in the jira and adds a unit test for log cleaning.


This addresses bug HBASE-3198.
    http://issues.apache.org/jira/browse/HBASE-3198


Diffs
-----

  /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java 1031797 
  /trunk/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java 1031797 

Diff: http://review.cloudera.org/r/1181/diff


Testing
-------


Thanks,

Jean-Daniel


;;;","05/Nov/10 21:52;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1181/#review1825
-----------------------------------------------------------

Ship it!


Looks good to me (after chatting more with j-d)


/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
<http://review.cloudera.org/r/1181/#comment5902>

    This is not your comment but I don't understand what its trying to say... can you fix?


- stack



;;;","05/Nov/10 22:49;jdcryans;Here's a patch that applies to 0.89;;;","05/Nov/10 22:50;jdcryans;Committed to trunk, thanks for the review Stack!;;;","08/Nov/10 23:12;kannanm;Nice catch. Good debugging...;;;","09/Nov/10 03:11;kannanm;The patch and what was committed are different for the test:

What's committed for TestHLog.java:

{code}
+    HLog log = new HLog(fs, dir, oldLogDir, conf);
+    HRegionInfo hri = new HRegionInfo(new HTableDescriptor(tableName),
+        HConstants.EMPTY_START_ROW, HConstants.EMPTY_END_ROW);
+    HRegionInfo hri2 = new HRegionInfo(new HTableDescriptor(tableName),      <<<< ### tableName should be tableName2.
+        HConstants.EMPTY_START_ROW, HConstants.EMPTY_END_ROW);
{code}

;;;","09/Nov/10 17:29;jdcryans;@Kannan

Yes it's different, and I committed the change to trunk a few minutes after I discovered my error when doing the 0.89 patch. What I saw is that in trunk we treat that table name differently than in the latest version of 0.89, so my original patch for trunk was working ok but porting it to 0.89 TestHLog was failing.;;;","09/Nov/10 20:43;kannanm;JD: Thanks. Aah-- I only imported the first of the commits. Didn't realize 
there was another related commit. Thanks. Glad to hear it is already 
fixed in trunk. ;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the new TestTransform breakage up on hudson,HBASE-3195,12479063,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,apurtell,stack,stack,04/Nov/10 03:48,20/Nov/15 12:43,14/Jul/23 06:06,06/Nov/10 19:18,,,,,,,,,,,,0.90.0,,,,,,,0,,,"This new test has been failing up on hudson since it was introduce at #1606.  I took a look.  It looks reasonable but its failing in an odd way -- can't find blocks in  hdfs.

I'm moving it aside for now till test gets some loving.  Breakage lasted till at least #1613.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2564,,,,,,,,,,,,,,,,,"06/Nov/10 02:06;apurtell;HBASE-3195.patch;https://issues.apache.org/jira/secure/attachment/12458984/HBASE-3195.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26702,,,,,Fri Nov 20 12:43:59 UTC 2015,,,,,,,,,,"0|i0hl4n:",100685,,,,,,,,,,,,,,,,,,,,,"04/Nov/10 07:08;apurtell;Why would a test for REST using the client API for REST cause missing blocks in HDFS? It's not the test per se. Passes locally. Could be related to HBASE-2564? ;;;","04/Nov/10 13:24;stack;I'm not sure why Andrew.  Its certainly odd and my recent experience wrestling failing unit tests suggests that there is fire where there is smoke.  Lets dig in some time.;;;","04/Nov/10 17:58;apurtell;I have a patch for HBASE-2564 that I'm testing. Will re-enable TestTransform as part of that and close both these issues out if the result checks out up on ASF Hudson. 

Hudson is a picky sort, that's why we like him. ;;;","04/Nov/10 18:05;apurtell@yahoo.com;I have a patch for 

 




----- Original Message ----


      
;;;","05/Nov/10 03:51;apurtell;Waiting for Hudson to chew on the new test suite now.;;;","05/Nov/10 23:26;apurtell;Looks like maybe the configuration from HBaseTestingUtility isn't passed all the way through to the HBase client embedded in the REST server, so it's finding other ZK instances from zombie tests lingering up on Hudson. Investigating.;;;","06/Nov/10 02:06;apurtell;Committed the attached patch to see if Hudson likes this better. Waiting for results.;;;","06/Nov/10 04:45;apurtell;Embedded HBase client in RESTservlet uses the config at first:

{noformat}
2010-11-06 02:31:02,673 DEBUG [26294606@qtp-3974511-0] rest.RowResource(265): PUT http://localhost:59467/TestTransform/testrow1/a:1 as application/octet-stream
2010-11-06 02:31:02,674 DEBUG [26294606@qtp-3974511-0] zookeeper.ZKUtil(95): hconnection opening connection to ZooKeeper with ensemble (localhost:21822)
{noformat}

Then forgets it:

{noformat}
2010-11-06 02:31:02,755 DEBUG [main] client.Client(144): PUT http://localhost:59467/TestTransform/testrow1/b:2 200 OK in 8 ms
2010-11-06 02:31:02,767 DEBUG [main] zookeeper.ZKUtil(95): hconnection opening connection to ZooKeeper with ensemble (localhost:21818)
{noformat}

Checking to see if REST server does something dumb.;;;","06/Nov/10 04:49;apurtell;Oh wait, duh it's the test.

This:

{noformat}
HTable table = new HTable(TABLE);
{noformat}

should be this:

{noformat}
HTable table = new HTable(TEST_UTIL.getConfiguration(), TABLE);
{noformat}

Commited change, waiting for Hudson again.;;;","06/Nov/10 19:18;apurtell;That did it.;;;","25/May/11 23:00;hudson;Integrated in HBase-TRUNK #1939 (See [https://builds.apache.org/hudson/job/HBase-TRUNK/1939/])
    ;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase should run on both secure and vanilla versions of Hadoop 0.20,HBASE-3194,12479053,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,ghelmling,ghelmling,03/Nov/10 23:23,20/Nov/15 12:41,14/Jul/23 06:06,09/Nov/10 21:58,,,,,,,,,,,,0.90.0,,,,,,,0,,,"There have been a couple cases recently of folks trying to run HBase trunk (or 0.89 DRs) on CDH3b3 or secure Hadoop.    While HBase security is in the works, it currently only runs on secure Hadoop versions.  Meanwhile HBase trunk won't compile on secure Hadoop due to backward incompatible changes in org.apache.hadoop.security.UserGroupInformation.

This issue is to work out the minimal set of changes necessary to allow HBase to build and run on both secure and non-secure versions of Hadoop.  Though, with secure Hadoop, I don't even think it's important to target running with HDFS security enabled (and krb authentication).  Just allow HBase to build and run in both versions.

I think mainly this amounts to abstracting usage of UserGroupInformation and UnixUserGroupInformation.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26701,Reviewed,,,,Fri Nov 20 12:41:01 UTC 2015,,,,,,,,,,"0|i0d2af:",74147,,,,,,,,,,,,,,,,,,,,,"03/Nov/10 23:29;ryanobjc;it would also be nice to run on both w/o rebuilding.;;;","04/Nov/10 00:33;apurtell;It should be possible to wrap UGI and UUGI with something that uses reflection to determine what platform variant is below. Anyone forsee a problem with that approach?;;;","04/Nov/10 00:47;ghelmling;Using reflection for isolation should work fine and should allow running against both versions without rebuilding.  I'm working it out now.

The easy part is getting the current UGI.  The harder part is ""setting"" the current UGI (only needed by MiniHBaseCluster and test code at the moment), since secure Hadoop changed this to UGI.doAs() with a PrivilegedAction instance wrapping the actual execution.  I'll sort out an initial attempt at isolating that and we can discuss the general approach.

;;;","06/Nov/10 08:44;hbasereviewboard;Message from: ""Gary Helmling"" <ghelmling@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1184/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Adds a wrapper class, org.apache.hadoop.hbase.security.User, around org.apache.hadoop.security.UserGroupInformation, to accommodate the divergent APIs of both regular Hadoop 0.20, and secure Hadoop 0.20 (which should also mean 0.21 and later, though I haven't specifically tested that).

Common methods in both UserGroupInformation versions are invoked directly.  Version specific methods are invoked using reflection.  Almost all of the usage is around testing code that spins up local clusters, since those are the only parts that exercise UserGroupInformation currently.


This addresses bug HBASE-3194.
    http://issues.apache.org/jira/browse/HBASE-3194


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java 9267679 
  src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java e4c356d 
  src/main/java/org/apache/hadoop/hbase/security/User.java PRE-CREATION 
  src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 83ceac9 
  src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java 8f9c88f 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java d436e2e 
  src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java b9902f1 
  src/test/java/org/apache/hadoop/hbase/security/TestUser.java PRE-CREATION 

Diff: http://review.cloudera.org/r/1184/diff


Testing
-------

Ran full test suite with these changes against both Hadoop 0.20-append and yahoo-0.20.104-append (security+append).  The only failures match what I'm currently seeing in trunk.  Will remerge from trunk and test against as soon as it's back to passing.

Though this is only testing compatibility with both versions with a rebuild.  I still need to test building against 0.20-append then spinning up HBase using security.  Will do that next, just wanted to get some eyes on this to validate the approach.


Thanks,

Gary


;;;","09/Nov/10 01:19;hbasereviewboard;Message from: ""Gary Helmling"" <ghelmling@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1184/
-----------------------------------------------------------

(Updated 2010-11-08 17:18:26.166428)


Review request for hbase.


Changes
-------

Added some notes on additional testing.


Summary
-------

Adds a wrapper class, org.apache.hadoop.hbase.security.User, around org.apache.hadoop.security.UserGroupInformation, to accommodate the divergent APIs of both regular Hadoop 0.20, and secure Hadoop 0.20 (which should also mean 0.21 and later, though I haven't specifically tested that).

Common methods in both UserGroupInformation versions are invoked directly.  Version specific methods are invoked using reflection.  Almost all of the usage is around testing code that spins up local clusters, since those are the only parts that exercise UserGroupInformation currently.


This addresses bug HBASE-3194.
    http://issues.apache.org/jira/browse/HBASE-3194


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java 9267679 
  src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java e4c356d 
  src/main/java/org/apache/hadoop/hbase/security/User.java PRE-CREATION 
  src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 83ceac9 
  src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java 8f9c88f 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java d436e2e 
  src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java b9902f1 
  src/test/java/org/apache/hadoop/hbase/security/TestUser.java PRE-CREATION 

Diff: http://review.cloudera.org/r/1184/diff


Testing (updated)
-------

Ran full test suite with these changes against both Hadoop 0.20-append and yahoo-0.20.104-append (security+append).  The only failures match what I'm currently seeing in trunk.  Will remerge from trunk and test against as soon as it's back to passing.

Though this is only testing compatibility with both versions with a rebuild.  I still need to test building against 0.20-append then spinning up HBase using security.  Will do that next, just wanted to get some eyes on this to validate the approach.

[Update 11/8/2010]: I did some further testing.  Compiled HBase against 0.20-append.  Ran YCSB with the compiled HBase against Hadoop 0.20.2 and yahoo-0.20.104-append (updating just the hadoop jar), to verify the patch will work against both without rebuilding.  YCSB load and varied workloads completed without any errors.


Thanks,

Gary


;;;","09/Nov/10 04:49;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1184/#review1857
-----------------------------------------------------------

Ship it!


Meant to say I'm +1 on this.

- stack



;;;","09/Nov/10 04:50;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1184/#review1856
-----------------------------------------------------------


Gary.  Quality stuff as usual.  Patch looks great.  Is it true we need this to be able to run on CDH3b3?  If so, we need to pull this into 0.90.0.

- stack



;;;","09/Nov/10 04:53;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-11-08 20:48:32, stack wrote:
bq.  > Meant to say I'm +1 on this.

Also, also meant to say that you are getting good at using this Reflection stuff Gary.


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1184/#review1857
-----------------------------------------------------------



;;;","09/Nov/10 05:10;tlipcon;Yea, need something of this sort to run on CDH3b3 - for the HBase we ship, we applied a less fancy reflectionless version of basically the same thing.;;;","09/Nov/10 05:29;ghelmling;Thanks for the review, Stack.

Yeah, what Todd says.  We'll need something like this to work on either CDH3b3 or Y! secure Hadoop 0.20.  This issue came up from two different people simultaneously on IRC last week, so we should definitely get something into 0.90.;;;","09/Nov/10 05:42;stack;We need this to work on CDH3b3 and on Y! secure Hadoop.  Bringing it into 0.90.;;;","09/Nov/10 05:44;stack;Committed.  Thanks for the patch Gary.;;;","09/Nov/10 06:48;tsuna;#1032848 prevents HBase from compiling.  Please fix.

{code}

[INFO] [compiler:compile {execution: default-compile}]
[INFO] Compiling 437 source files to /private/tmp/tt/hbase/target/classes
[INFO] ------------------------------------------------------------------------
[ERROR] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Compilation failure

/private/tmp/tt/hbase/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java:[33,39] package org.apache.hadoop.hbase.security does not exist

/private/tmp/tt/hbase/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java:[176,23] cannot find symbol
symbol  : class User
location: class org.apache.hadoop.hbase.LocalHBaseCluster

/private/tmp/tt/hbase/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java:[203,23] cannot find symbol
symbol  : class User
location: class org.apache.hadoop.hbase.LocalHBaseCluster
{code};;;","09/Nov/10 16:58;stack;Sorry about that.  Just fixed (forgot to add new classes).;;;","09/Nov/10 21:58;stack;Reresolving after I committed tail of the patch.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FilterList with MUST_PASS_ONE and SCVF isn't working,HBASE-3191,12479027,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,,seelmann,seelmann,03/Nov/10 20:40,12/Oct/12 06:17,14/Jul/23 06:06,03/Nov/10 21:17,0.89.20100924,0.90.0,,,,,,,,,,0.90.0,,Filters,,,,,0,,,"In a special case the FilterList with MUST_PASS_ONE operator doesn't work correctly:
- a filter in the list is a SingleColumValueFilter with filterIfMissing=true
- FilterList.filterKeyValue(KeyValue) is called
- SingleColumValueFilter.filterKeyValue(KeyValue) is called
- SingleColumValueFilter.filterKeyValue(KeyValue) returns ReturnCode.INCLUDE if the KeyValue doesn't match a column (to support filterIfMissing)
- FilterList.filterKeyValue(KeyValue) immediately returns ReturnCode.INCLUDE, remaining filters in the list aren't evaluated.

However it is required to evaluate remaining filters, otherwise filterRow() filters out rows in case the filter's filterKeyValue() saves state that is used by filterRow(). (SingleColumValueFilter, SkipFilter, WhileMatchFilter do so)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/10 20:50;seelmann;HBASE-3191.patch;https://issues.apache.org/jira/secure/attachment/12458764/HBASE-3191.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26698,Reviewed,,,,Wed Nov 03 21:17:55 UTC 2010,,,,,,,,,,"0|i08sg7:",49209,,,,,,,,,,,,,,,,,,,,,"03/Nov/10 20:50;seelmann;Patch with Test;;;","03/Nov/10 21:17;stack;Committed to TRUNK.  Thank you for the patch Stefan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stagger Major Compactions,HBASE-3189,12478938,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,nspiegelberg,nspiegelberg,nspiegelberg,02/Nov/10 21:22,20/Nov/15 12:43,14/Jul/23 06:06,02/Nov/10 22:08,,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"For pre-split regions, we can get into a case where the oldest HFile in a Store is pretty large and will not encounter a compaction within the 24hr major compact window.  If that's the case, we don't want multiple multi-GB major compactions being triggered at the same time.  Add ability to stagger the major compaction expiration window.",,kannanm,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-5332,,,"04/Nov/10 21:40;nspiegelberg;HBASE-3189-addendum.patch;https://issues.apache.org/jira/secure/attachment/12458848/HBASE-3189-addendum.patch","02/Nov/10 21:29;nspiegelberg;HBASE-3189.patch;https://issues.apache.org/jira/secure/attachment/12458683/HBASE-3189.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26696,Reviewed,,,,Fri Nov 20 12:43:44 UTC 2015,,,,,,,,,,"0|i0hl3z:",100682,,,,,,,,,,,,,,,,,,,,,"02/Nov/10 21:29;nspiegelberg;Fix for 0.90.  Adds a 'jitter' config option to stagger compactions on a per-store basis.;;;","02/Nov/10 22:08;stack;Committed. Thanks for the patch Nicolas.;;;","04/Nov/10 04:35;kannanm;I think, usability wise, jitter (and it's default) should be specified as fraction (% value) of the major compaction cycle time, instead of absolute terms (like 4 hours)/

Otherwise, you have a backward compat issue with this change for someone who is running a major compaction say every three hours, but has forgotten to set the jitter parameter when they upgrade to 0.90. And they'll be compacting anywhere from 3hrs +/- (2* 4 hours jitter default). This approach will also ensure you don't return -ve values for ""get next compaction time"".
;;;","04/Nov/10 04:35;kannanm;I think, usability wise, jitter (and it's default) should be specified as fraction (% value) of the major compaction cycle time, instead of absolute terms (like 4 hours)/

Otherwise, you have a backward compat issue with this change for someone who is running a major compaction say every three hours, but has forgotten to set the jitter parameter when they upgrade to 0.90. And they'll be compacting anywhere from 3hrs +/- (2* 4 hours jitter default). This approach will also ensure you don't return -ve values for ""get next compaction time"".
;;;","04/Nov/10 21:40;nspiegelberg;addresses Kannan's issue.  this has been accepted/committed internally.;;;","04/Nov/10 23:34;stack;Committed the addendum to TRUNK just now.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
User-triggered compactions are triggering splits!,HBASE-3185,12478831,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,streamy,streamy,streamy,01/Nov/10 19:51,20/Nov/15 12:41,14/Jul/23 06:06,02/Nov/10 02:08,0.90.0,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"Doh... This came in with original commit of master rewrite, not sure why it's in there.

compactRegion is calling region.shouldSplit(true);",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/10 20:10;streamy;HBASE-3185-v1.patch;https://issues.apache.org/jira/secure/attachment/12458577/HBASE-3185-v1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26694,Reviewed,,,,Fri Nov 20 12:41:45 UTC 2015,,,,,,,,,,"0|i0hl3b:",100679,,,,,,,,,,,,,,,,,,,,,"01/Nov/10 19:53;streamy;Also, looking back at 0.20 branch, we've changed behavior and are flushing when we compact.  Is that necessary?  Should we flush before compacting when user triggers a minor or major compaction?;;;","01/Nov/10 19:58;stack;bq. Should we flush before compacting when user triggers a minor or major compaction?

No need.;;;","01/Nov/10 20:10;streamy;Removes set of split and region flush on compact call.;;;","01/Nov/10 20:27;stack;+1;;;","02/Nov/10 02:08;streamy;Committed.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Xmx setting in pom to use for tests/surefire does not appear to work,HBASE-3184,12478825,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,01/Nov/10 18:57,20/Nov/15 12:43,14/Jul/23 06:06,01/Nov/10 19:26,0.90.0,,,,,,,,,,,0.90.0,,build,,,,,0,,,"I was running intense version of TestRollingRestart and changing the Xmx setting in our pom file wasn't changing anything.

What we have now is:

{noformat}
            <argLine>-enableassertions</argLine>
            <argLine>-Xmx1400m</argLine>
{noformat}

But on process listing (and through experimentation with my tests), only -enableassertions is used.

However, changing to below and it worked:

{noformat}
            <argLine>-enableassertions -Xmx1400m</argLine>
{noformat}

Just wanted to open a jira and see if i'm missing something from someone with more maven experience.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/10 18:59;streamy;HBASE-3184-v1.patch;https://issues.apache.org/jira/secure/attachment/12458571/HBASE-3184-v1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26693,Reviewed,,,,Fri Nov 20 12:43:33 UTC 2015,,,,,,,,,,"0|i0hl33:",100678,,,,,,,,,,,,,,,,,,,,,"01/Nov/10 18:59;streamy;trivial patch;;;","01/Nov/10 19:01;stack;There is this issue: http://jira.codehaus.org/browse/SUREFIRE-297   I'm game for putting both into one element.  Might have to quote it.;;;","01/Nov/10 19:04;streamy;You sure that's the same issue?  Seems to be referring to issue with needing quotes?

From what I've seen, it's that having two argLines doesn't work, need to have one.  And the above does work without quotes.;;;","01/Nov/10 19:14;stack;I tried quoting it on linux -- bad.  I tried this patch on linux and good.

Forking command line: /bin/sh -c cd /home/stack/hbase && /usr/lib/jvm/java-6-sun-1.6.0.22/jre/bin/java -enableassertions -Xmx1400m -jar /tmp/surefirebooter9055735266082879429.jar /tmp/surefire849494456353668359tmp /tmp/surefire1318869691288010370tmp

Tried it on mac too....

Forking command line: /bin/sh -c cd /Users/stack/checkouts/trunk && /System/Library/Frameworks/JavaVM.framework/Versions/1.6.0/Home/bin/java -enableassertions -Xmx1400m -jar /var/folders/fm/fmAR6VZ1HTChC80jm2Twmk+++TI/-Tmp-/surefirebooter7770704693040280207.jar /var/folders/fm/fmAR6VZ1HTChC80jm2Twmk+++TI/-Tmp-/surefire1484449681591552372tmp /var/folders/fm/fmAR6VZ1HTChC80jm2Twmk+++TI/-Tmp-/surefire3760098851031822449tmp


+1 on commit.;;;","01/Nov/10 19:26;streamy;Committed.  Thanks for testing stack.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commits of HBASE-3102 and HBASE-3160 broke TestHeapSize,HBASE-3183,12478791,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,01/Nov/10 06:39,20/Nov/15 12:44,14/Jul/23 06:06,01/Nov/10 15:12,0.90.0,,,,,,,,,,,0.90.0,,io,regionserver,,,,0,,,Both JIRAs added class members to Store.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/10 06:44;streamy;HBASE-3183-v1.patch;https://issues.apache.org/jira/secure/attachment/12458537/HBASE-3183-v1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26692,,,,,Fri Nov 20 12:44:07 UTC 2015,,,,,,,,,,"0|i0hl2v:",100677,,,,,,,,,,,,,,,,,,,,,"01/Nov/10 06:45;streamy;I'm going to commit this so we can make sure it passes up on Hudson.  Passes locally for me now.;;;","01/Nov/10 15:12;streamy;Passes locally and passed up on Hudson.  Please reopen if anyone sees a failure in TestHeapSize.;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If server hosting META dies or is stopping while processing another server shutdown, IOE accessing META stop shutdown handler from finishing",HBASE-3182,12478789,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,streamy,streamy,streamy,01/Nov/10 04:04,20/Nov/15 12:41,14/Jul/23 06:06,02/Nov/10 02:06,0.90.0,,,,,,,,,,,0.90.0,,master,,,,,0,,,"In TestRollingRestart, there is a test which kills server hosting ROOT then immediately kills server hosting META.  In a recent run this turned up a small race condition if the server hosting META is closing while we process shutdown of server hosting ROOT.

{noformat}
2010-10-31 20:41:34,621 ERROR [MASTER_META_SERVER_OPERATIONS-dev692.sf2p.facebook.com:54989-0] executor.EventHandler(154): Caught throwable while processing event M_META_SERVER_SHUTDOWN
org.apache.hadoop.ipc.RemoteException: java.io.IOException: Server not running
        at org.apache.hadoop.hbase.regionserver.HRegionServer.checkOpen(HRegionServer.java:2216)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1652)
        at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:561)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1025)

        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:749)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:255)
        at $Proxy8.openScanner(Unknown Source)
        at org.apache.hadoop.hbase.catalog.MetaReader.getServerUserRegions(MetaReader.java:495)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:125)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
{noformat}
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-3181,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26691,,,,,Fri Nov 20 12:41:46 UTC 2015,,,,,,,,,,"0|i0hl2n:",100676,,,,,,,,,,,,,,,,,,,,,"01/Nov/10 04:05;streamy;We need to do something here.  First, I think we should do one of the wait-and-verify META methods.  Even still, we could get an exception in MetaReader (here is another place we touch it).

In this case, we have no choice but to need to retry server shutdown or keep waiting.  We needz meta.;;;","01/Nov/10 20:12;streamy;Tripped over this again but didn't even seem to relate to META.  Just got this uncaught exception that killed my shutdown handler.;;;","01/Nov/10 20:14;streamy;Patch being reviewed for HBASE-3181 includes the fix for this.;;;","02/Nov/10 02:06;streamy;Fix for this was included in commit of HBASE-3181;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Enable ReplicationLogsCleaner only if replication is, and fix its test",HBASE-3179,12478766,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,jdcryans,streamy,streamy,31/Oct/10 18:30,20/Nov/15 12:42,14/Jul/23 06:06,02/Nov/10 00:23,0.90.0,,,,,,,,,,,0.90.0,,master,Replication,,,,0,,,"I'm seeing logs like this on a trunk master:

{noformat}
2010-10-31 11:21:13,793 DEBUG org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner: Didn't find this log in ZK, deleting: 1.1.1.1%3A60020.1288545516340
2010-10-31 11:21:13,793 WARN org.apache.hadoop.hdfs.DFSClient: File /HBASE/.oldlogs/1.1.1.1%3A60020.1288545516340 is beng deleted only through Trash org.apache.hadoop.fs.FsShell.delete because all deletes must go through Trash.
2010-10-31 11:21:13,798 DEBUG org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner: Didn't find this log in ZK, deleting: 1.1.1.1%3A60020.1288545899208
2010-10-31 11:21:13,798 WARN org.apache.hadoop.hdfs.DFSClient: File /HBASE/.oldlogs/1.1.1.1%3A60020.1288545899208 is beng deleted only through Trash org.apache.hadoop.fs.FsShell.delete because all deletes must go through Trash.
{noformat}

There are like 50 of these showing up once a minute.  I'm unsure if this is more valuable when replication is turned on, but should this be running when replication is not on in this cluster?  At the least there should be less logging, like one line per run.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26690,Reviewed,,,,Fri Nov 20 12:42:07 UTC 2015,,,,,,,,,,"0|i0hl1z:",100673,,,,,,,,,,,,,,,,,,,,,"31/Oct/10 18:33;streamy;Assigning to JD so he takes a look;;;","31/Oct/10 18:41;streamy;Also the fact that this shows a DFSClient WARN but should be normal is troubling to see in the logs.;;;","01/Nov/10 17:24;jdcryans;Originally ReplicationLogCleaner wasn't even running if replication wasn't turned on, but that was changed when the log cleaner chaining was reworked for snapshotting. Let me fix that.;;;","02/Nov/10 00:06;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1145/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Patch that disables ReplicationLogCleaner when it's not enabled, and that reenables+forward ports TestLogsCleaner (just figured it was disabled when I wanted to run the unit test).


This addresses bug HBASE-3179.
    http://issues.apache.org/jira/browse/HBASE-3179


Diffs
-----

  /trunk/src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java 1029893 
  /trunk/src/test/java/org/apache/hadoop/hbase/master/TestLogsCleaner.java 1029893 

Diff: http://review.cloudera.org/r/1145/diff


Testing
-------

Unit testing and local instance.


Thanks,

Jean-Daniel


;;;","02/Nov/10 00:12;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1145/#review1759
-----------------------------------------------------------

Ship it!



/trunk/src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java
<http://review.cloudera.org/r/1145/#comment5698>

    Fix this comment on commit


- stack



;;;","02/Nov/10 00:16;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>


bq.  On 2010-11-01 17:11:30, stack wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java, line 56
bq.  > <http://review.cloudera.org/r/1145/diff/1/?file=16373#file16373line56>
bq.  >
bq.  >     Fix this comment on commit

Oh, missing a ""return"" after ""we"". Thanks Stack!


- Jean-Daniel


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1145/#review1759
-----------------------------------------------------------



;;;","02/Nov/10 00:23;jdcryans;Committed to trunk.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commit of HBASE-3160 broke TestPriorityCompactionQueue up on hudson,HBASE-3175,12478710,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,nspiegelberg,streamy,streamy,30/Oct/10 05:00,20/Nov/15 12:41,14/Jul/23 06:06,30/Oct/10 16:02,0.90.0,,,,,,,,,,,0.90.0,,test,,,,,0,,,Not sure what's up but this is failing up on hudson.  Assigning nicolas so he takes a look.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/10 06:16;nspiegelberg;HBASE-3175.patch;https://issues.apache.org/jira/secure/attachment/12458475/HBASE-3175.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26689,Reviewed,,,,Fri Nov 20 12:41:22 UTC 2015,,,,,,,,,,"0|i0hl1j:",100671,,,,,,,,,,,,,,,,,,,,,"30/Oct/10 06:16;nspiegelberg;communication/timing problem.  my diff on Review Board was committed instead of the final one I uploaded to JIRA.  Here's a diff between the two that applies directly onto trunk.;;;","30/Oct/10 16:02;streamy;Committed.  Will reopen if doesn't pass hudson.

Thanks Nicolas!;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase 2984 breaks ability to specify BLOOMFILTER & COMPRESSION via shell,HBASE-3173,12478701,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,kannanm,kannanm,kannanm,29/Oct/10 21:06,20/Nov/15 12:40,14/Jul/23 06:06,07/Dec/10 19:44,,,,,,,,,,,,0.90.0,,,,,,,0,,,HBase 2984 breaks ability to specify BLOOMFILTER & COMPRESSION via shell,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-3631,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/10 19:34;stack;3173-v2.txt;https://issues.apache.org/jira/secure/attachment/12465726/3173-v2.txt","29/Oct/10 21:07;kannanm;HBASE-3173.txt;https://issues.apache.org/jira/secure/attachment/12458450/HBASE-3173.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26688,Reviewed,,,,Fri Nov 20 12:40:41 UTC 2015,,,,,,,,,,"0|i0hl13:",100669,,,,,,,,,,,,,,,,,,,,,"29/Oct/10 21:16;jdcryans;I tried the patch, although it fixes my breakage, I see that this fix breaks something that was possible before:

{noformat}
hbase(main):001:0> create 't', {NAME => 'f', COMPRESSION => 'lzo'}

ERROR: java.lang.IllegalArgumentException: No enum const class org.apache.hadoop.hbase.io.hfile.Compression$Algorithm.lzo
{noformat}

This was possible before. You might just look at what was done, or I can do it since it's my fault you're fixing it :);;;","29/Oct/10 21:50;kannanm;Aah. I have always been using upper case stuff :). So didn't catch it.

We might need to throw in a toUpperCase() in admin.rb. 

JD: Want to make that additional change and commit?

In HColumnDescriptor.java, I see something like:

{code}
  setCompressionType(Compression.Algorithm.
      valueOf(compression.toUpperCase()));
    setBloomFilterType(StoreFile.BloomType.
      valueOf(bloomFilter.toUpperCase()));
{code}
   ;;;","07/Dec/10 19:44;stack;Committed to branch and trunk (after verifying it works).  Modelled fix on Igor's hbase-3310.;;;","21/Jan/11 19:02;stack;This fix was included in 0.90.0.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reverse order of AssignmentManager and MetaNodeTracker in ZooKeeperWatcher,HBASE-3172,12478691,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,streamy,streamy,streamy,29/Oct/10 19:05,20/Nov/15 12:44,14/Jul/23 06:06,29/Oct/10 21:33,0.90.0,,,,,,,,,,,0.90.0,,master,Zookeeper,,,,0,,,"Over in HBASE-3159, I reported a similar double-opened issue but for META.  This is because of the fact that the META unassigned znode has two different consumers {{MetaNodeTracker}} in use by the CatalogTracker and {{AssignmentManager}} which uses it for normal regions-in-transition stuff.

The AssignmentManager needs to go first, otherwise we can get double handling of state transitions.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/10 19:20;streamy;HBASE-3172-v1.patch;https://issues.apache.org/jira/secure/attachment/12458436/HBASE-3172-v1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26687,Reviewed,,,,Fri Nov 20 12:44:09 UTC 2015,,,,,,,,,,"0|i0hl0v:",100668,,,,,,,,,,,,,,,,,,,,,"29/Oct/10 19:20;streamy;Straightforward patch.  Makes the listeners in ZKW into a list rather than set and adds a method which adds a listener to the front of the list.;;;","29/Oct/10 19:21;streamy;Stack, please review when u have a sec.;;;","29/Oct/10 21:26;stack;+1;;;","29/Oct/10 21:33;streamy;Committed.  Thanks for review stack.;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RegionServer confused about empty row keys,HBASE-3170,12478686,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,ddas,tsuna,tsuna,29/Oct/10 18:23,23/Sep/13 18:30,14/Jul/23 06:06,21/Jan/13 20:12,0.89.20100621,0.89.20100924,0.90.0,0.90.1,0.90.2,0.90.3,0.90.4,0.90.5,0.90.6,0.92.0,0.92.1,0.95.0,,regionserver,,,,,1,,,"I'm no longer sure about the expected behavior when using an empty row key (e.g. a 0-byte long byte array).  I assumed that this was a legitimate row key, just like having an empty column qualifier is allowed.  But it seems that the RegionServer considers the empty row key to be whatever the first row key is.
{code}
Version: 0.89.20100830, r0da2890b242584a8a5648d83532742ca7243346b, Sat Sep 18 15:30:09 PDT 2010

hbase(main):001:0> scan 'tsdb-uid', {LIMIT => 1}
ROW                           COLUMN+CELL                                                                          
 \x00                         column=id:metrics, timestamp=1288375187699, value=foo      
 \x00                         column=id:tagk, timestamp=1287522021046, value=bar         
 \x00                         column=id:tagv, timestamp=1288111387685, value=qux      
1 row(s) in 0.4610 seconds

hbase(main):002:0> get 'tsdb-uid', ''
COLUMN                        CELL                                                                                 
 id:metrics                   timestamp=1288375187699, value=foo                         
 id:tagk                      timestamp=1287522021046, value=bar                         
 id:tagv                      timestamp=1288111387685, value=qux                      
3 row(s) in 0.0910 seconds

hbase(main):003:0> get 'tsdb-uid', ""\000""
COLUMN                        CELL                                                                                 
 id:metrics                   timestamp=1288375187699, value=foo                         
 id:tagk                      timestamp=1287522021046, value=bar                         
 id:tagv                      timestamp=1288111387685, value=qux                      
3 row(s) in 0.0550 seconds
{code}

This isn't a parsing problem with the command-line of the shell.  I can reproduce this behavior both with plain Java code and with my asynchbase client.

Since I don't actually have a row with an empty row key, I expected that the first {{get}} would return nothing.",,ddas,digri,enis,hudson,stack,tsuna,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/13 23:34;ddas;3170-0.94.patch;https://issues.apache.org/jira/secure/attachment/12566048/3170-0.94.patch","19/Jan/13 02:33;ddas;3170-1.patch;https://issues.apache.org/jira/secure/attachment/12565622/3170-1.patch","20/Jan/13 18:14;ddas;3170-3.patch;https://issues.apache.org/jira/secure/attachment/12565705/3170-3.patch","20/Jan/13 19:23;ddas;3170-4.patch;https://issues.apache.org/jira/secure/attachment/12565706/3170-4.patch","21/Jan/13 19:19;ddas;3170-5.patch;https://issues.apache.org/jira/secure/attachment/12565830/3170-5.patch","19/Jan/13 06:41;yuzhihong@gmail.com;3170-v2.patch;https://issues.apache.org/jira/secure/attachment/12565629/3170-v2.patch","19/Jan/13 16:09;yuzhihong@gmail.com;3170-v3.patch;https://issues.apache.org/jira/secure/attachment/12565641/3170-v3.patch","19/Jan/13 14:36;yuzhihong@gmail.com;3170-v3.patch;https://issues.apache.org/jira/secure/attachment/12565638/3170-v3.patch","21/Jan/13 17:10;stack;3170v5.txt;https://issues.apache.org/jira/secure/attachment/12565810/3170v5.txt",,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26686,Reviewed,,,,Mon Sep 23 18:30:22 UTC 2013,,,,,,,,,,"0|i02cbz:",11599,"If no row specified by a Get, we no longer return first row in the table.  Now we fail.",,,,,,,,,,,,,,,,,,,,"23/Oct/12 11:00;digri;We are hitting this too, this is a really unexpected behaviour. Why getting empty key should return data of the first row in table? Reproduced in CDH3u4 (0.90.6):

{code}
hbase(main):005:0> create 'emptykey', {NAME=>'data', VERSION=>1}
0 row(s) in 0.2070 seconds

hbase(main):011:0> get 'emptykey', '' 
COLUMN                         CELL                                                                                   
0 row(s) in 0.0120 seconds

hbase(main):006:0> put 'emptykey', 'a', 'data:a', '1234'
0 row(s) in 0.1980 seconds

hbase(main):007:0> put 'emptykey', 'b', 'data:b', '5678'
0 row(s) in 0.0070 seconds

hbase(main):008:0> scan 'emptykey'                      
ROW                            COLUMN+CELL                                                                            
 a                             column=data:a, timestamp=1350989443394, value=1234                                     
 b                             column=data:b, timestamp=1350989450499, value=5678                                     
2 row(s) in 0.0660 seconds

hbase(main):009:0> get 'emptykey', ''
COLUMN                         CELL                                                                                   
 data:a                        timestamp=1350989443394, value=1234                                                    
1 row(s) in 0.0120 seconds
{code}

It works the same way also using thrift.

We can even see, that empty key is supported in fact.
{code}
hbase(main):012:0> put 'emptykey', '', 'data:c', '90'   
0 row(s) in 0.0130 seconds

hbase(main):013:0> get 'emptykey', ''                
COLUMN                         CELL                                                                                   
 data:c                        timestamp=1350989869682, value=90                                                      
1 row(s) in 0.0120 seconds

hbase(main):018:0> scan 'emptykey'   
ROW                            COLUMN+CELL                                                                            
                               column=data:c, timestamp=1350989869682, value=90                                       
 a                             column=data:a, timestamp=1350989933922, value=1234                                     
 b                             column=data:b, timestamp=1350989937820, value=5678                                     
3 row(s) in 0.0180 seconds
{code};;;","23/Oct/12 21:44;stack;Bad bug. Pulling into 0.96 so we fix it soon.;;;","19/Jan/13 02:33;ddas;Attached is a patch. One change is to keep track of the fact that we are doing a 'get' as opposed to a 'scan'. When this was done, it broke the case of a legitimate 'get' with an empty rowkey. So there is a change to handle that case - basically, in the RegionScannerImpl.nextInternal I do special checks for empty start & end keys (in the new method isStopRowConsideringEmptyRows).. 

Thanks to Enis for some useful discussions..;;;","19/Jan/13 02:33;ddas;Let's see what hudson says.;;;","19/Jan/13 02:54;yuzhihong@gmail.com;{code}
+public class TestGetEmptyRowKey {
{code}
The new test would be a medium test, right ? Please add annotation.
{code}
+          //don't stop
+          return false;
{code}
You can save the comment such as the one above if you add javadoc for isStopRowConsideringEmptyRows().;;;","19/Jan/13 03:00;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12565622/3170-1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified tests.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.TestCheckTestClasses
                  org.apache.hadoop.hbase.regionserver.TestScanWithBloomError
                  org.apache.hadoop.hbase.regionserver.TestStoreFile

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/4099//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4099//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4099//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4099//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4099//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4099//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4099//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4099//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4099//console

This message is automatically generated.;;;","19/Jan/13 06:41;yuzhihong@gmail.com;It turns out that we need to keep the original behavior of Scan.isGetScan()

Patch v2 introduced two new fields for RegionScannerImpl and made the logic in isStopRowConsideringEmptyRows() symmetrical.

The following tests passed:

 1430  mt -Dtest=TestScanWithBloomError,TestStoreFile,TestGetEmptyRowKey
 1433  mt -Dtest=TestHRegion,TestColumnSeeking,TestMultiColumnScanner;;;","19/Jan/13 07:47;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12565629/3170-v2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.TestLocalHBaseCluster

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/4101//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4101//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4101//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4101//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4101//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4101//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4101//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4101//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4101//console

This message is automatically generated.;;;","19/Jan/13 14:36;yuzhihong@gmail.com;I realized that the boolean field, RegionScannerImpl.scanCreatedFromGet, is redundant with RegionScannerImpl.stopRowFromScan.;;;","19/Jan/13 15:36;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12565638/3170-v3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.constraint.TestConstraint
                  org.apache.hadoop.hbase.TestLocalHBaseCluster

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/4102//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4102//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4102//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4102//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4102//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4102//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4102//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4102//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4102//console

This message is automatically generated.;;;","19/Jan/13 16:09;yuzhihong@gmail.com;I ran TestConstraint twice locally and it passed.;;;","19/Jan/13 17:10;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12565641/3170-v3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.TestLocalHBaseCluster

     {color:red}-1 core zombie tests{color}.  There are 1 zombie test(s): 

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/4103//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4103//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4103//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4103//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4103//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4103//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4103//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4103//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4103//console

This message is automatically generated.;;;","19/Jan/13 22:31;stack;bq. It turns out that we need to keep the original behavior of Scan.isGetScan()

Why is that?  Seems odd having isGetScan and isScanCreatedFromGet.  I'd think that a method named isGetScan would return true if this was a Scan made to service a Get?

It seems like isGetScan is trying to look at scanner specs to see if a Get scan rather than just at whether or not the Scan constructor that takes a Get was used.

Looking at this, if you pass a Get a null or empty row, should we just throw an exception immediately and not even start the Scan going?   Let a Get with an empty row be illegal?;;;","19/Jan/13 23:11;yuzhihong@gmail.com;bq. Seems odd having isGetScan and isScanCreatedFromGet.
I agree. isGetScan() might have started with what we normally expect. Currently isGetScan() is used to serve other use cases.

bq. should we just throw an exception immediately
That would simplify this issue a lot.

Let'see what [~devaraj] says about the above proposal.;;;","20/Jan/13 18:14;ddas;Okay this patch is a slight rework on my previous patch. It fixes the problem at hand I think and makes the distinction between scan and get clearer...

On the point about disabling Get with empty row, I was also having the same opinion until I started digging into why Get with empty row key is broken at all. The patch does try to distinguish between scan and get and in the process fixes the issue reported (and would prevent future breakages as there is a testcase to protect it). I think this should be considered.

Thoughts?;;;","20/Jan/13 19:05;yuzhihong@gmail.com;The following test failure seems to be related:
{code}
Failed tests:   testMultipleTimestampRanges[0](org.apache.hadoop.hbase.regionserver.TestSeekOptimizations): Expected and actual KV arrays differ at position 0: row1/myCF:qual0/2999/Put/vlen=0/ts=0 (length 3) vs. <out_of_range> (length 0). Bloom=NONE, compr=NONE, Scan: all columns, row=1, maxVersions=1, lazySeek=false
  testMultipleTimestampRanges[1](org.apache.hadoop.hbase.regionserver.TestSeekOptimizations): Expected and actual KV arrays differ at position 0: row1/myCF:qual0/2999/Put/vlen=0/ts=0 (length 3) vs. <out_of_range> (length 0). Bloom=ROW, compr=NONE, Scan: all columns, row=1, maxVersions=1, lazySeek=false
  testMultipleTimestampRanges[2](org.apache.hadoop.hbase.regionserver.TestSeekOptimizations): Expected and actual KV arrays differ at position 0: row1/myCF:qual0/2999/Put/vlen=0/ts=0 (length 3) vs. <out_of_range> (length 0). Bloom=ROWCOL, compr=NONE, Scan: all columns, row=1, maxVersions=1, lazySeek=false
  testMultipleTimestampRanges[3](org.apache.hadoop.hbase.regionserver.TestSeekOptimizations): Expected and actual KV arrays differ at position 0: row1/myCF:qual0/2999/Put/vlen=0/ts=0 (length 3) vs. <out_of_range> (length 0). Bloom=NONE, compr=GZ, Scan: all columns, row=1, maxVersions=1, lazySeek=false
  testMultipleTimestampRanges[4](org.apache.hadoop.hbase.regionserver.TestSeekOptimizations): Expected and actual KV arrays differ at position 0: row1/myCF:qual0/2999/Put/vlen=0/ts=0 (length 3) vs. <out_of_range> (length 0). Bloom=ROW, compr=GZ, Scan: all columns, row=1, maxVersions=1, lazySeek=false
  testMultipleTimestampRanges[5](org.apache.hadoop.hbase.regionserver.TestSeekOptimizations): Expected and actual KV arrays differ at position 0: row1/myCF:qual0/2999/Put/vlen=0/ts=0 (length 3) vs. <out_of_range> (length 0). Bloom=ROWCOL, compr=GZ, Scan: all columns, row=1, maxVersions=1, lazySeek=false
{code};;;","20/Jan/13 19:16;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12565705/3170-3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified tests.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 3 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.TestLocalHBaseCluster
                  org.apache.hadoop.hbase.regionserver.TestSeekOptimizations

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/4105//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4105//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4105//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4105//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4105//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4105//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4105//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4105//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4105//console

This message is automatically generated.;;;","20/Jan/13 19:23;ddas;Ok it seems like some tests construct the Scan object using the default constructor, and then access the setters of Scan to set the fields. The new field I added doesn't get set in those cases. So in this patch, I retain the check in isGetScan to check for those other fields (as in the original code).;;;","20/Jan/13 20:26;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12565706/3170-4.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified tests.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 3 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.TestLocalHBaseCluster

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/4106//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4106//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4106//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4106//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4106//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4106//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4106//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4106//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4106//console

This message is automatically generated.;;;","20/Jan/13 22:53;stack;bq. On the point about disabling Get with empty row, I was also having the same opinion until I started digging into why Get with empty row key is broken at all.

Can you say more?  Would it be hard just throwing IllegalArgument if Get is passed a null or empty row?

v3 patch seems to keep isScanCreatedFromGet beside a method called isGetScan.  I'd think this'll confuse (and yeah, will think that not all Gets use the Get constructor.;;;","21/Jan/13 00:51;ddas;Well I just wanted to fix the problem reported by Benoit if possible :-) 

Please have a look at the 3170-4.patch. That's the last iteration from me and passes all tests.;;;","21/Jan/13 17:10;stack;v5 factors out a bit of common code into a method.  Patch looks good to me.   We could probably just throw an exception if you try and pass a Get a null row too... put the query out of its misery earlier rather than later.  Was wondering about the test.  We spin up a mini cluster instance to do the null key test.  Should this test just be added to another test that has already put up a cluster?

Good stuff lads.;;;","21/Jan/13 17:51;ddas;bq. We could probably just throw an exception if you try and pass a Get a null row too.. Was wondering about the test.
Ok [~stack], will review the patch from that point of view..;;;","21/Jan/13 18:03;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12565810/3170v5.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 3 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.TestLocalHBaseCluster

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/4110//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4110//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4110//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4110//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4110//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4110//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4110//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4110//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4110//console

This message is automatically generated.;;;","21/Jan/13 18:28;yuzhihong@gmail.com;How about adding the new test to:
{code}
public class TestFromClientSide3 {
{code};;;","21/Jan/13 19:19;ddas;This puts in the test in TestFromClientSide3. On the handling of null row key, the RPC handler will throw a NPE immediately since the ProtoBufUtil tries to convert the PB request to a regular request and that will throw NPE. I think handling that should be outside the scope of this jira since there could be other such nulls in requests. [~stack] what do you think?;;;","21/Jan/13 20:12;stack;Committed to trunk.  Thanks lads.;;;","21/Jan/13 20:16;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12565830/3170-5.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 3 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/4114//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4114//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4114//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4114//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4114//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4114//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4114//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4114//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4114//console

This message is automatically generated.;;;","21/Jan/13 23:24;hudson;Integrated in HBase-TRUNK #3775 (See [https://builds.apache.org/job/HBase-TRUNK/3775/])
    HBASE-3170 RegionServer confused about empty row keys (Revision 1436587)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/client/Scan.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide3.java
;;;","22/Jan/13 00:58;hudson;Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #365 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/365/])
    HBASE-3170 RegionServer confused about empty row keys (Revision 1436587)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/client/Scan.java
* /hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide3.java
;;;","22/Jan/13 23:34;ddas;The patch for 0.94.;;;","23/Sep/13 18:30;stack;Marking closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when master joins running cluster if a RIT references a RS no longer present,HBASE-3169,12478685,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,29/Oct/10 18:04,20/Nov/15 12:40,14/Jul/23 06:06,29/Oct/10 18:35,,,,,,,,,,,,0.90.0,,master,,,,,0,,,,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/10 18:05;stack;npe.txt;https://issues.apache.org/jira/secure/attachment/12458429/npe.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26685,Reviewed,,,,Fri Nov 20 12:40:56 UTC 2015,,,,,,,,,,"0|i0hl0n:",100667,,,,,,,,,,,,,,,,,,,,,"29/Oct/10 18:05;stack;Test for null when we check server is online.  If not online, let the RIT timeout and it'll be assigned a new server.  I've been running this up on test cluster and it seems to cure my prob.;;;","29/Oct/10 18:11;streamy;+1.  Looks like you have some logging-level changes in the patch so just be sure to not commit that.;;;","29/Oct/10 18:35;stack;Committed.  Thanks for review Jon.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Handle case where we open META, ROOT has been closed but znode location not deleted yet, and try to update META location in ROOT",HBASE-3164,12478596,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,streamy,streamy,streamy,28/Oct/10 16:24,20/Nov/15 12:43,14/Jul/23 06:06,30/Oct/10 17:04,0.90.0,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"Carrying over from HBASE-3159, I ran into a case with TestRollingRestart with META and ROOT failing concurrently.  This is how it played out:

META is closed on an RS that has been stopped:
{noformat}
2010-10-27 22:47:27,709 DEBUG [RS_CLOSE_META-192.168.0.44,59709,1288244829038-0] handler.CloseRegionHandler(128): Closed region .META.,,1.1028785192
{noformat}

Then ROOT is closed on a different RS that has been stopped:
{noformat}
2010-10-27 22:47:28,863 DEBUG [RS_CLOSE_ROOT-192.168.0.44,59662,1288244792380-0] handler.CloseRegionHandler(128): Closed region -ROOT-,,0.70236052
{noformat}

A running RS is assigned META (the master isn't even aware yet that root has been closed, it is processing shutdown for RS 59709 but not yet received expired node for 59662):
{noformat}
2010-10-27 22:47:29,095 DEBUG [MASTER_META_SERVER_OPERATIONS-192.168.0.44:59629-0] master.AssignmentManager(908): No previous transition plan for .META.,,1.1028785192 so generated a random one; hri=.META.,,1.1028785192, src=, dest=192.168.0.44,59735,1288244843993; 3 (online=3, exclude=null) available servers
2010-10-27 22:47:29,095 DEBUG [MASTER_META_SERVER_OPERATIONS-192.168.0.44:59629-0] master.AssignmentManager(802): Assigning region .META.,,1.1028785192 to 192.168.0.44,59735,1288244843993
...
2010-10-27 22:47:29,123 DEBUG [RS_OPEN_META-192.168.0.44,59735,1288244843993-0] handler.OpenRegionHandler(69): Processing open of .META.,,1.1028785192
{noformat}

After finishing the open of META, the RS goes to update location in ROOT and gets:
{noformat}
2010-10-27 22:47:29,208 ERROR [RS_OPEN_META-192.168.0.44,59735,1288244843993-0] executor.EventHandler(154): Caught throwable while processing event M_RS_OPEN_META
java.lang.NullPointerException: No server for -ROOT-
	at org.apache.hadoop.hbase.catalog.MetaEditor.updateMetaLocation(MetaEditor.java:127)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1271)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:156)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{noformat}

This doesn't actually kill the RS, it's just a caught exception up in the generic EventHandler.  But we get left in a weird state.  Eventually master does the right thing and times-out the OPENING:
{noformat}
2010-10-27 22:48:15,694 INFO  [192.168.0.44:59629.timeoutMonitor] master.AssignmentManager$TimeoutMonitor(1444): Regions in transition timed out:  .META.,,1.1028785192 state=PENDING_OPEN, ts=1288244865700
2010-10-27 22:48:15,694 INFO  [192.168.0.44:59629.timeoutMonitor] master.AssignmentManager$TimeoutMonitor(1454): Region has been PENDING_OPEN for too long, reassigning region=.META.,,1.1028785192
{noformat}

But it chooses to assign it back to the same person because the plan is still there:
{noformat}
2010-10-27 22:48:15,702 DEBUG [192.168.0.44:59629.timeoutMonitor] master.AssignmentManager(916): Using preexisting plan=hri=.META.,,1.1028785192, src=, dest=192.168.0.44,59735,1288244843993
2010-10-27 22:48:15,702 DEBUG [192.168.0.44:59629.timeoutMonitor] master.AssignmentManager(802): Assigning region .META.,,1.1028785192 to 192.168.0.44,59735,1288244843993
{noformat}

But then the RS doesn't open it because it's actually already open on that server.  We fail the ROOT edit but then don't close the region out.
{noformat}
2010-10-27 22:48:15,805 INFO  [IPC Server handler 3 on 59735] regionserver.HRegionServer(1952): Received request to open region: .META.,,1.1028785192
2010-10-27 22:48:15,807 DEBUG [RS_OPEN_META-192.168.0.44,59735,1288244843993-0] handler.OpenRegionHandler(69): Processing open of .META.,,1.1028785192
2010-10-27 22:48:15,807 WARN  [RS_OPEN_META-192.168.0.44,59735,1288244843993-0] handler.OpenRegionHandler(84): Attempting open of .META.,,1.1028785192 but it's already online on this server
{noformat}

This continues indefinitely, once every minute.

1.  Address the race condition when we get the connection to the root server (could exist for meta too).  The blocking call thinks we have a location but then when we get the cached location and don't get one.

2.  If we do get this NPE writing to root (or maybe meta too), we should not just throw the exception all the way up to the EventHandler and log it and continue.  That just stops our META_OPEN in it's tracks.  We complete the open but just not the edit.  We don't roll-back in any way.

3.  If the master is assigning stuff out and a region says, hey, I'm already hosting this region... something must be up.  In this case, it would not have been good for the RS to tell the master that it was already hosting it because it was missing the root edit.  So maybe if this happens, the master asks the RS to close the region in question?  Dunno.

Probably more issues to think about around this :)

This seems to be extremely rare.  I have been running this TestRollingRestart script constantly and this only happens when I do a concurrent kill of the server hosting ROOT and then server hosting META, and then only sometimes, it does work more times than not.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/10 22:14;streamy;HBASE-3164-v1.patch;https://issues.apache.org/jira/secure/attachment/12458456/HBASE-3164-v1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26683,Reviewed,,,,Fri Nov 20 12:43:47 UTC 2015,,,,,,,,,,"0|i0hkzz:",100664,,,,,,,,,,,,,,,,,,,,,"28/Oct/10 18:44;streamy;So, we do actually catch exceptions and roll back if we fail to update root/meta.  Issue is that we throw an NPE here which we don't catch.  Instead we should throw some form of an IOE (we actually throw the NPE it's not an actual NPE).;;;","29/Oct/10 22:14;streamy;Trivial patch that changes exception to an IOException.

This is an interesting JIRA because it showed some race conditions around availability and tracking of catalogs.  Though I think this patch is sufficient for closing this JIRA, we do need to get better at this.  I think the first step to making this stuff easier is to get rid of ROOT over in HBASE-3171.;;;","30/Oct/10 03:56;stack;+1;;;","30/Oct/10 17:04;streamy;Committed to trunk, thanks stack.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If we timeout PENDING_CLOSE and send another closeRegion RPC, need to handle NSRE from RS (comes as a RemoteException)",HBASE-3163,12478544,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,streamy,streamy,streamy,28/Oct/10 06:10,20/Nov/15 12:41,14/Jul/23 06:06,30/Oct/10 17:03,0.90.0,,,,,,,,,,,0.90.0,,master,,,,,0,,,"When we send a closeRegion RPC to an RS, we are catching NSRE but when the RS is the one throwing the NSRE, then it comes back as a RemoteException (then an NSRE) and we aren't unwrapping it properly.

We need to catch this and then deal with it appropriately.

Still tracking how this happened in the first place.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26682,Reviewed,,,,Fri Nov 20 12:41:12 UTC 2015,,,,,,,,,,"0|i0hkzr:",100663,,,,,,,,,,,,,,,,,,,,,"29/Oct/10 22:55;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1130/
-----------------------------------------------------------

Review request for hbase and stack.


Summary
-------

Unwraps RemoteExceptions and we don't abort if it's an NSRE.

This handling is rather different from the handling of opens (we abort in some cases here), so we might consider making them the same and no longer aborting here.  I'm fine with leaving it for a while longer as we do all this testing.


This addresses bug HBASE-3163.
    http://issues.apache.org/jira/browse/HBASE-3163


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1028936 

Diff: http://review.cloudera.org/r/1130/diff


Testing
-------


Thanks,

Jonathan


;;;","30/Oct/10 03:56;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1130/#review1719
-----------------------------------------------------------

Ship it!


+1 for now.  I think yeah that we are going to have to do a sweep of these handlers and the Meta* classes and make them all do the same exception handling catching the same types but letting out (and aborting) for others -- but lets get a bit more experience first.

- stack



;;;","30/Oct/10 17:01;streamy;And I think if we're going to do a sweep up of shit, we should just kill root.  It's going to make life so much easier and will be 90% ripping out code.;;;","30/Oct/10 17:03;streamy;Thanks for review stack.  Committed to trunk.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Double play of OpenedRegionHandler for a single region; fails second time through and aborts Master",HBASE-3159,12478421,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,streamy,stack,stack,27/Oct/10 05:53,20/Nov/15 12:41,14/Jul/23 06:06,28/Oct/10 21:33,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Here is master log with annotations: http://people.apache.org/~stack/master.txt

Region in question is:

b8827a67a9d446f345095d25e1f375f7

The running code is doctored in that I've added in a bit of logging -- zk in particular -- and I've also removed what I thought was a provocation of this condition, reassign inside in an assign if server has gone away when we try the open rpc (Turns out we have the condition even w/o this code in place).

The log starts where the region in question timesout in RIT.

We assign it to 186.

Notice how we see 'Handling transition' for this region TWICE.  This means two OpenedRegionHandlers will be scheduled -- and so the failure to delete a znode already gone.

As best I can tell, the watcher for this region is triggered once only -- which is odd because how then the double scheduling of OpenedRegionHandler but also, why am I not seeing OPENING, OPENING, OPENED and only what I presume is an OPENED?",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/10 21:32;streamy;HBASE-3159-FINAL.patch;https://issues.apache.org/jira/secure/attachment/12458282/HBASE-3159-FINAL.patch","28/Oct/10 00:00;streamy;TestRollingRestart-v4.patch;https://issues.apache.org/jira/secure/attachment/12458208/TestRollingRestart-v4.patch","27/Oct/10 19:40;streamy;hbase-meta-dupe-opened-master-only.txt;https://issues.apache.org/jira/secure/attachment/12458180/hbase-meta-dupe-opened-master-only.txt","27/Oct/10 19:42;streamy;hbase-meta-dupe-opened.txt;https://issues.apache.org/jira/secure/attachment/12458181/hbase-meta-dupe-opened.txt","28/Oct/10 03:45;streamy;master-root-assign-abort.log;https://issues.apache.org/jira/secure/attachment/12458229/master-root-assign-abort.log","28/Oct/10 16:04;streamy;rs_death_on_meta_open_no_root.txt;https://issues.apache.org/jira/secure/attachment/12458259/rs_death_on_meta_open_no_root.txt",,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26681,Reviewed,,,,Fri Nov 20 12:41:06 UTC 2015,,,,,,,,,,"0|i0hkz3:",100660,,,,,,,,,,,,,,,,,,,,,"27/Oct/10 19:32;streamy;Got a double OPENED of meta on the master running my new TestRollingRestart test which does lots of RS killing.

Attaching two logs.  One is raw log from section where problem happened, the second is just the master part (this is unit test so RS+master combined).;;;","27/Oct/10 19:36;streamy;This is fairly easy to ""fix"" in a way that will not make the master abort.  But this does not get to the underlying cause of triggered to OPENED handlers.  There's something going on there that we need to keep digging on (I'm doing so now but with added logging it's not happening anymore).

The fix to prevent abort is to transition the in-memory RIT to the OPEN state when we handleRegion(regionTransitionData).  Exactly like we're already doing in the CLOSED handling in that method, we need to do this:

+          regionState.update(RegionState.State.OPEN, data.getStamp());

When the next attempted handle of the OPENED state comes in, we won't process it because it's not in the expected states of PENDING_OPEN or OPENING, and then the closed handler won't be executed.

But yeah, let's not put that fix in yet.  Seems like some screwy with ZK watches being fired though we don't expect a watch to be set or something.  Digging more...;;;","27/Oct/10 19:40;streamy;Previous files were a bit borked.  These should not be missing lines.;;;","27/Oct/10 23:35;streamy;Stack, when you rerun your tests again, turn off the ZK client logging and ensure all of our ZK logging is set to pickup DEBUG level.  There's always a slight chance we would want the raw ZK client logs, if something really crazy is happening, but there should be enough logging in our ZKW and ZKUtil as long as we pick up debug.

One thing though, change the method at the bottom of ZKUtil to the following:

{noformat}
  private static void logRetrievedMsg(final ZooKeeperWatcher zkw,
      final String znode, final byte [] data, final boolean watcherSet) {
    if (!LOG.isDebugEnabled()) return;
    LOG.debug(zkw.prefix(""Retrieved "" + ((data == null)? 0: data.length) +
      "" byte(s) of data from znode "" + znode +
      (watcherSet? "" and set watcher; "": ""; data="") +
      (data == null? ""null"": (
          znode.startsWith(zkw.assignmentZNode) ?
              RegionTransitionData.fromBytes(data).toString()
              : StringUtils.abbreviate(Bytes.toString(data), 32)))));
  }
{noformat}

The change is that we detect if we're logging an unassigned znode, and if so, we print the region transition data.  This will make debugging this much simpler.;;;","28/Oct/10 00:00;streamy;This is my latest patch doing unit tests of rolling restarts.  Has some logging changes and a couple small fixes.

Also includes the non-fix fix I described earlier but it's commented out so we can still detect this broken state.;;;","28/Oct/10 00:28;streamy;Probably not related but just uncovered a small race condition in AssignmentManager around line 806 in assign(RegionState):

{noformat}
      // Send OPEN RPC. This can fail if the server on other end is is not up.
      serverManager.sendRegionOpen(plan.getDestination(), state.getRegion());
      // Transition RegionState to PENDING_OPEN
      state.update(RegionState.State.PENDING_OPEN);
{noformat}

We need to update the state to PENDING_OPEN before we send the RPC.  Otherwise we could get the OPENING and we'll still be in OFFLINE state locally so we will reject the transition.;;;","28/Oct/10 03:45;streamy;Interesting failure w/ root assignment.  Stack, I think you're better to look at this one.;;;","28/Oct/10 16:04;streamy;Another issue during TestRollingRestart.

When RS is opening META, it goes to update ROOT location.

In the code, it seems like it's supposed to wait indefinitely for a ROOT location before proceeding with the edit for the new meta location.  But in the log it hardly waits at all.  And seemingly returns a null location, which it should not if there's no root location.

In CT.getCachedConnection(HServerAddress) it looks like we will return null in many different instances (and not even log that a connection exception happened).  Somehow we're returning true from the blocking calls waiting on root, but then when we go to verify it doesn't work?;;;","28/Oct/10 19:14;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1108/
-----------------------------------------------------------

Review request for hbase and Jonathan Gray.


Summary
-------

Here is patch I've been testing with up on cluster.  Adds debugging and two fixes -- one setting state to OPEN on receipt of a rs opened event and two, resetting a watcher getting data over in zkutil (as per its documentation claims).


This addresses bug hbase-3159.
    http://issues.apache.org/jira/browse/hbase-3159


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 30e49c8 
  src/main/java/org/apache/hadoop/hbase/master/handler/ClosedRegionHandler.java 1a88700 
  src/main/java/org/apache/hadoop/hbase/master/handler/OpenedRegionHandler.java d4fa82b 
  src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java 81661ef 
  src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java 21a4256 

Diff: http://review.cloudera.org/r/1108/diff


Testing
-------

Running on cluster


Thanks,

stack


;;;","28/Oct/10 21:32;streamy;Patch committed.;;;","28/Oct/10 21:33;streamy;Committed to trunk.  Thanks stack for working with me on this, reviewing, and being awesome at breaking stuff :);;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bloom File Writes Broken if keySize is large,HBASE-3158,12478407,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,nspiegelberg,nspiegelberg,nspiegelberg,26/Oct/10 23:50,20/Nov/15 12:43,14/Jul/23 06:06,27/Oct/10 04:16,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"Yesterday, on our cluster, a region compact() kept crashing at giving this stack trace

2010-10-25 08:48:28,330 ERROR org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction failed for region MailBox_dark_launch_2010_10-04,4b64b600,1286302852538.9183a3b91ebd289bab7724d028cffa69.
java.lang.IllegalArgumentException: maxValue must be > 0
at org.apache.hadoop.hbase.util.ByteBloomFilter.sanityCheck(ByteBloomFilter.java:170)
at org.apache.hadoop.hbase.util.ByteBloomFilter.<init>(ByteBloomFilter.java:156)
at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:707)
at org.apache.hadoop.hbase.regionserver.StoreFile.createWriter(StoreFile.java:566)
at org.apache.hadoop.hbase.regionserver.Store.createWriterInTmp(Store.java:504)
at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:817)
at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:678)
at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:842)
at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:793)
at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:103)

The problem is that we are oveflowing ""int"" for ""bitSize"". The number of keys is about 272M, and we are using about 11 bit per key. So, bitSize ends up being > 2G..",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/10 00:56;nspiegelberg;HBASE-3158.patch;https://issues.apache.org/jira/secure/attachment/12458121/HBASE-3158.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26680,Reviewed,,,,Fri Nov 20 12:43:49 UTC 2015,,,,,,,,,,"0|i0hkyv:",100659,,,,,,,,,,,,,,,,,,,,,"26/Oct/10 23:53;nspiegelberg;Two fixes to make:

1) Catch IllegalArgumentExceptions thrown by the BloomFilter and just continue writing the StoreFile without blooms instead of crashing
2) ByteBloomFilter.<init>.bitSize should be a long and we should be able to handle large bloom sizes
3) We cannot handle byteSize > MAX_INT because java doesn't allow you to allocate arrays that large.  Throw an IAE in that case;;;","27/Oct/10 00:56;nspiegelberg;Also, added ""io.storefile.bloom.max.keys"" to control the max keys allowed in any bloom;;;","27/Oct/10 04:16;stack;Thanks for the fix Nicolas (I like the BBQ exception).;;;","27/Oct/10 04:21;nspiegelberg;lol.  I was trying to debug an assert failure that I had during unit testing.  You can obviously take that conditional out :);;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HFile.appendMetaBlock() uses wrong comparator,HBASE-3155,12478380,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,nspiegelberg,nspiegelberg,nspiegelberg,26/Oct/10 18:46,20/Nov/15 12:42,14/Jul/23 06:06,27/Oct/10 13:45,,,,,,,,,,,,0.90.0,,,,,,,0,,,"We hit this exception last night...

2010-10-26 01:20:20,056 INFO org.apache.hadoop.hbase.regionserver.StoreFile: Bloom added to HFile (...): 18752B, 13012/13601 (96%)
2010-10-26 01:20:20,056 INFO org.apache.hadoop.hbase.regionserver.HRegion: aborted compaction on region 04,04c84c80,1286302852528.77d461b19c7f410041f1d03f4823ef8b. after 20mins, 43sec
2010-10-26 01:20:20,056 ERROR org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction failed for region 04,04c84c80,1286302852528.77d461b19c7f410041f1d03f4823ef8b.
java.lang.ArrayIndexOutOfBoundsException: 17
at org.apache.hadoop.hbase.util.Bytes.compareTo(Bytes.java:860)
at org.apache.hadoop.hbase.KeyValue$KeyComparator.compareRows(KeyValue.java:1888)
at org.apache.hadoop.hbase.KeyValue$KeyComparator.compare(KeyValue.java:1822)
at org.apache.hadoop.hbase.io.hfile.HFile$Writer.appendMetaBlock(HFile.java:476)
at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.close(StoreFile.java:862)
at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:896)
at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:687)
at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:858)
at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:807)

The problem is that appendMetaBlock() is using the wrong comparator. although the variable is called 'rawComparator', it's actually a normal comparator (KeyComparator) that defaults to RawComparator if not specified. All meta sorting needs to be done using the actual Bytes.RAW_COMPARATOR.  This happened because >=2 things were inserted into meta. ",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/10 18:53;nspiegelberg;HBASE-3155.patch;https://issues.apache.org/jira/secure/attachment/12458092/HBASE-3155.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26679,Reviewed,,,,Fri Nov 20 12:42:18 UTC 2015,,,,,,,,,,"0|i0hky7:",100656,,,,,,,,,,,,,,,,,,,,,"26/Oct/10 18:47;nspiegelberg;Code prior to this patch is probably not sorted right, so the fix I give won't be backwards compatible for correctly finding meta data without further effort.  The worst case should be that you request a meta block and it isn't found.  In our case, this is fine because all we store in META is bloom data.  Note that this bug was introduced by the addition of BloomFilters, so only 0.89 + 0.90 users will experience any migration 'oddities', 0.20 users who upgrade should be fine.;;;","27/Oct/10 13:45;stack;Thanks Nicolas. Committed.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when trying to read regioninfo from .META.,HBASE-3151,12478299,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,26/Oct/10 04:49,20/Nov/15 12:43,14/Jul/23 06:06,02/Nov/10 23:50,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"This is an old issue perhaps in a new guise.  From the list, Sebastien Bauer reports:

{code}
> 2010-10-25 08:13:01,690 ERROR
> org.apache.hadoop.hbase.master.CatalogJanitor: Caught exception
> java.lang.NullPointerException
> 2010-10-25 08:13:24,385 INFO
> org.apache.hadoop.hbase.master.ServerManager: regionservers=2,
> averageload=2538


> 2010-10-23 20:16:17,890 DEBUG
>  org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation:
>  Cached location for .META.,,1.1028785192 is
>  db2a.goldenline.pl:60020
>  2010-10-23 20:16:18,432 FATAL org.apache.hadoop.hbase.master.HMaster:
>  Unhandled exception. Starting
>  shutdown.
>
>  java.lang.NullPointerException
>
>        at
>  org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:75)
>
>        at
>  org.apache.hadoop.hbase.util.Writables.getHRegionInfo(Writables.java:119)
>
>        at
>  org.apache.hadoop.hbase.client.MetaScanner$1.processRow(MetaScanner.java:188)
>
>        at
>  org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:157)
>
>        at
>  org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:69)
>
>        at
>  org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:54)
>
>        at
>  org.apache.hadoop.hbase.client.MetaScanner.listAllRegions(MetaScanner.java:195)
>
>       at
>  org.apache.hadoop.hbase.master.AssignmentManager.assignAllUserRegions(AssignmentManager.java:1048)
>
>        at
>  org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:379)
>
>        at
>  org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:265)
>
>  2010-10-23 20:16:18,433 INFO org.apache.hadoop.hbase.master.HMaster:
>  Aborting
>
>  2010-10-23 20:16:18,433 DEBUG org.apache.hadoop.hbase.master.HMaster:
>  Stopping service threads
{code}


I think he has an old master... checking.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/10 23:48;stack;offline.txt;https://issues.apache.org/jira/secure/attachment/12458692/offline.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26676,,,,,Fri Nov 20 12:43:01 UTC 2015,,,,,,,,,,"0|i0hkxj:",100653,,,,,,,,,,,,,,,,,,,,,"26/Oct/10 07:15;anih;Its trunk version, excactly 1026606 revision;;;","02/Nov/10 19:24;stack;I just got this doing big split on big table:

{code}
2010-11-02 19:19:43,127 WARN org.apache.hadoop.hbase.master.CatalogJanitor: REGIONINFO_QUALIFIER is empty in keyvalues={usertable,user1164435829,1287547106288.5782c04a2d7017dcfc74337c7864cf02./info:server/1288722591987/Put/vlen=16, usertable,user1164435829,1287547106288.}
2010-11-02 19:19:43,128 ERROR org.apache.hadoop.hbase.master.CatalogJanitor: Caught exception
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.CatalogJanitor$1.visit(CatalogJanitor.java:101)
        at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:197)
        at org.apache.hadoop.hbase.master.CatalogJanitor.scan(CatalogJanitor.java:107)
        at org.apache.hadoop.hbase.master.CatalogJanitor.chore(CatalogJanitor.java:77)
        at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
{code};;;","02/Nov/10 23:48;stack;I manufactured a .META. with null regioninfos in it by splitting a big table then doing shutdown in middle of splits.  When we startup we are assigning all user regions -- EVEN IF THEY OFFLINED SPLIT PARENTS.  This patch fixes the issue w/ assigning offlined regions and it also fixes all the places where NPE causes shutdown.;;;","02/Nov/10 23:50;stack;Committed this because need it in place to do splits and enable/disable fixes.  Will add to hbck cleanup of empty HRIs.;;;","03/Nov/10 02:03;streamy;No tests for stuff like this?  Seems like a ton of new split code and a lot (unfortunately) in master to deal with splits but almost no testing of the various broken states we can get into?;;;","03/Nov/10 21:26;stack;Well, I think I can add test for case of a .META. row that  has all but HRI.  That'd be good for testing we don't crap out as we were doing.  Let me make a new issue to do that.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Regions stuck in transition after rolling restart, perpetual timeout handling but nothing happens",HBASE-3147,12478198,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,24/Oct/10 22:22,20/Nov/15 12:43,14/Jul/23 06:06,26/Oct/10 16:53,,,,,,,,,,,,0.90.0,,,,,,,0,,,"The rolling restart script is great for bringing on the weird stuff.  On my little loaded cluster if I run it, it horks the cluster and it doesn't recover.  I notice two issues that need fixing:

1. We'll miss noticing that a server was carrying .META. and it never gets assigned -- the shutdown handlers get stuck in perpetual wait on a .META. assign that will never happen.
2. Perpetual cycling of the this sequence per region not succesfully assigned:

{code}
 2010-10-23 21:37:57,404 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  usertable,user510588360,1287547556587.7f2d92497d2d03917afd574ea2aca55b. state=PENDING_OPEN,                       ts=1287869814294  45154 2010-10-23 21:37:57,404 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_OPEN or OPENING for too long, reassigning region=usertable,user510588360,1287547556587.                                     7f2d92497d2d03917afd574ea2aca55b.  45155 2010-10-23 21:37:57,404 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x2bd57d1475046a Attempting to transition node 7f2d92497d2d03917afd574ea2aca55b from RS_ZK_REGION_OPENING to M_ZK_REGION_OFFLINE  45156 2010-10-23 21:37:57,404 WARN org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x2bd57d1475046a Attempt to transition the unassigned node for 7f2d92497d2d03917afd574ea2aca55b from RS_ZK_REGION_OPENING to                 M_ZK_REGION_OFFLINE failed, the node existed but was in the state M_ZK_REGION_OFFLINE  45157 2010-10-23 21:37:57,404 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region transitioned OPENING to OFFLINE so skipping timeout, region=usertable,user510588360,1287547556587.7f2d92497d2d03917afd574ea2aca55b.  
,,,
{code}

Timeout period again elapses an then same sequence.

This is what I've been working on.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/10 16:49;stack;HBASE-3147-v11.patch;https://issues.apache.org/jira/secure/attachment/12458083/HBASE-3147-v11.patch","25/Oct/10 23:58;stack;HBASE-3147-v6.patch;https://issues.apache.org/jira/secure/attachment/12458024/HBASE-3147-v6.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26675,,,,,Fri Nov 20 12:43:12 UTC 2015,,,,,,,,,,"0|i0hkxb:",100652,,,,,,,,,,,,,,,,,,,,,"24/Oct/10 22:41;streamy;Seems that our in-memory RIT state is PENDING_OPEN but it's in OFFLINE in ZK.  Seems like a potentially common case.  Server being assigned to was just not there, never began opening it.

We should probably differentiate between PENDING_OPEN timeout and OPENING timeout.  Let me see what I find in the code.

(Your paste seems to lack line breaks so this jira is a mile wide);;;","25/Oct/10 01:14;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1087/
-----------------------------------------------------------

Review request for hbase and stack.


Summary
-------

Adds new handling of the timeouts for PENDING_OPEN and PENDING_CLOSE in-memory master RIT states.

Adds some new broken RIT states into TestMasterFailover.

Some of these broken states don't seem possible to me but as long as we aren't breaking the existing behaviors and tests I think it's okay if we handle odd cases that can be mocked.  Who knows what will happen in the real world.

The reason TestMasterFailover didn't/doesn't really test for the issue in HBASE-3147 is this new broken condition happens when an RS dies / goes offline rather than a master failover concurrent w/ RS failure.


This addresses bug HBASE-3147.
    http://issues.apache.org/jira/browse/HBASE-3147


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1026911 
  trunk/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java 1026911 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java 1026911 
  trunk/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java 1026911 

Diff: http://review.cloudera.org/r/1087/diff


Testing
-------

TestMasterFailover passes.


Thanks,

Jonathan


;;;","25/Oct/10 16:10;stack;I got this when I tried running patch....

{code}
java.lang.IllegalAccessError: tried to access method org.apache.hadoop.hbase.zookeeper.ZKAssign.getNodeName(Lorg/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher;Ljava/lang/String;)Ljava/lang/String; from class org.apache.hadoop.hbase.master.AssignmentManager$TimeoutMonitor
    at org.apache.hadoop.hbase.master.AssignmentManager$TimeoutMonitor.chore(AssignmentManager.java:1457)
    at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
2010-10-25 16:07:44,354 INFO org.apache.hadoop.hbase.master.AssignmentManager$TimeoutMonitor: sv2borg180:60000.timeoutMonitor exiting
{code}

Let me try fix.;;;","25/Oct/10 16:27;streamy;Hmm... you should have:

  public static String getNodeName(ZooKeeperWatcher zkw, String regionName) {

as part of the diff up on RB;;;","25/Oct/10 23:32;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1087/
-----------------------------------------------------------

(Updated 2010-10-25 16:29:36.379908)


Review request for hbase and stack.


Changes
-------

Added metaservershutdownhandler and rootservershutdownhandler


Summary (updated)
-------

Adds new handling of the timeouts for PENDING_OPEN and PENDING_CLOSE in-memory master RIT states.

Adds some new broken RIT states into TestMasterFailover.

Some of these broken states don't seem possible to me but as long as we aren't breaking the existing behaviors and tests I think it's okay if we handle odd cases that can be mocked.  Who knows what will happen in the real world.

The reason TestMasterFailover didn't/doesn't really test for the issue in HBASE-3147 is this new broken condition happens when an RS dies / goes offline rather than a master failover concurrent w/ RS failure.


v4 of the patch adds to Jons' fixes.  It adds a shutdown server handler for root and another for meta so the processing of servers hosting meta/root do not get frozen out.  I've seen this in my testing.


This addresses bug HBASE-3147.
    http://issues.apache.org/jira/browse/HBASE-3147


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java 1027291 
  trunk/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java 1027291 
  trunk/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java 1027291 
  trunk/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java 1027291 
  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1027291 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1027291 
  trunk/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java 1027291 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/MetaServerShutdownHandler.java PRE-CREATION 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/RootServerShutdownHandler.java PRE-CREATION 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java 1027292 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java 1027291 
  trunk/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java 1027291 

Diff: http://review.cloudera.org/r/1087/diff


Testing
-------

TestMasterFailover passes.


Thanks,

Jonathan


;;;","25/Oct/10 23:40;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1087/#review1662
-----------------------------------------------------------

Ship it!


Looks good.  Not sure if I can +1 my patch but I think we should commit :)


trunk/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
<http://review.cloudera.org/r/1087/#comment5542>

    Should we remove this code from inside of ServerShutdownHandler now?  Not a big deal but being done twice.


- Jonathan



;;;","25/Oct/10 23:58;stack;Here is what I'll commit.  It does as Jon suggests removing check of root or  meta carrying inside in shutdown handler since we're doing the check on the outside now.  This patch also includes missing hookup that testing found.

There is still work to do on this issue.  What seems to be happening is that a watcher is not being triggered.  Need to figure how that is happening.  I'll see a regionserver with all of its opener handlers stuck waiting on notification that meta  has been deployed.... Other servers will have gotten their watcher triggered but not one or two in the cluster....   Master is then stuck timing out this regionservers allocations and then reassigning... calling open on the rpc which adds region to queue but since all openers are stuck waiting on meta, the queues don't get processed.;;;","26/Oct/10 06:27;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1087/
-----------------------------------------------------------

(Updated 2010-10-25 23:25:36.390570)


Review request for hbase and stack.


Changes
-------

So, a few things extra after digging in w/ Jon.

1. A watch was not being called on .META. move because it was not being set; in MetaNodeTracker we were not calling the super inside in nodeDeleted to reset the watch (in rolling restart, only a few servers would actually experience a moved .META. sensation and it was these that were hanging up..  Others when they came up would see .META. in its new location)
2. We were not assigning out .META. if master had trouble reaching meta before it saw server expired.  In the case where we'd trouble contacting meta before we saw its server expire, we'd reset in the catalog tracker its location.  We were using catalog tracker to determine which server was hosting meta.  We use a different technique now.


Summary
-------

Adds new handling of the timeouts for PENDING_OPEN and PENDING_CLOSE in-memory master RIT states.

Adds some new broken RIT states into TestMasterFailover.

Some of these broken states don't seem possible to me but as long as we aren't breaking the existing behaviors and tests I think it's okay if we handle odd cases that can be mocked.  Who knows what will happen in the real world.

The reason TestMasterFailover didn't/doesn't really test for the issue in HBASE-3147 is this new broken condition happens when an RS dies / goes offline rather than a master failover concurrent w/ RS failure.


v4 of the patch adds to Jons' fixes.  It adds a shutdown server handler for root and another for meta so the processing of servers hosting meta/root do not get frozen out.  I've seen this in my testing.


This addresses bug HBASE-3147.
    http://issues.apache.org/jira/browse/HBASE-3147


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/MetaServerShutdownHandler.java PRE-CREATION 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaNodeTracker.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java 1027351 
  trunk/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java 1027351 

Diff: http://review.cloudera.org/r/1087/diff


Testing
-------

TestMasterFailover passes.


Thanks,

Jonathan


;;;","26/Oct/10 06:41;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1087/#review1668
-----------------------------------------------------------

Ship it!


There was a little whitespace but I am +1 to commit this.  I still have some weird failures on TestRollingRestart, need to test w/ this.


trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaNodeTracker.java
<http://review.cloudera.org/r/1087/#comment5544>

    should be after the path check but just an efficiency thing


- Jonathan



;;;","26/Oct/10 07:57;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1087/
-----------------------------------------------------------

(Updated 2010-10-26 00:55:02.299835)


Review request for hbase and stack.


Changes
-------

This patch is almost there.  Its much better.  Fixed testing for .META. server by looking in map of servers to regions; that won't work since its a map of user regions only.  Instead get from catalogtracker.

Locally TestRegionRebalancing failed.  I need to look at that.

On cluster, we turned up an unexpected state as server was opening a region it was also going down. Need to dig in on that too.

Want to also add tests at least for moved .meta.


Summary
-------

Adds new handling of the timeouts for PENDING_OPEN and PENDING_CLOSE in-memory master RIT states.

Adds some new broken RIT states into TestMasterFailover.

Some of these broken states don't seem possible to me but as long as we aren't breaking the existing behaviors and tests I think it's okay if we handle odd cases that can be mocked.  Who knows what will happen in the real world.

The reason TestMasterFailover didn't/doesn't really test for the issue in HBASE-3147 is this new broken condition happens when an RS dies / goes offline rather than a master failover concurrent w/ RS failure.


v4 of the patch adds to Jons' fixes.  It adds a shutdown server handler for root and another for meta so the processing of servers hosting meta/root do not get frozen out.  I've seen this in my testing.


This addresses bug HBASE-3147.
    http://issues.apache.org/jira/browse/HBASE-3147


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/MetaServerShutdownHandler.java PRE-CREATION 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaNodeTracker.java 1027351 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java 1027351 
  trunk/src/test/java/org/apache/hadoop/hbase/catalog/TestCatalogTracker.java 1027351 
  trunk/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java 1027351 

Diff: http://review.cloudera.org/r/1087/diff


Testing
-------

TestMasterFailover passes.


Thanks,

Jonathan


;;;","26/Oct/10 16:49;stack;This is the patch I'm applying.  It fixes the two items raised at the top of this issue but now I'm seeing other, lesser issues for which I'll open new JIRAs.;;;","26/Oct/10 16:53;stack;Committed.  Thanks for writing half of the patch and for reviews Jon.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"document feature ""hbase.regionserver.codecs""",HBASE-3146,12478129,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,ryanobjc,ryanobjc,23/Oct/10 00:37,20/Nov/15 12:40,14/Jul/23 06:06,02/Dec/10 04:14,,,,,,,,,,,,0.90.0,,,,,,,0,,,"a new feature ""hbase.regionserver.codecs"" allows you to poison pill your regionservers if any of the codecs listed do not test operational on server startup.  

the format of the value is ""lzo,gz"" with strings separated by commas.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/10 19:52;stack;hbase.regionserver.codec.html;https://issues.apache.org/jira/secure/attachment/12458486/hbase.regionserver.codec.html",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26674,,,,,Fri Nov 20 12:40:52 UTC 2015,,,,,,,,,,"0|i0hkx3:",100651,,,,,,,,,,,,,,,,,,,,,"23/Oct/10 05:21;tlipcon;Can't we scan META on RS startup and poison if we see a region we can't open? I don't know that adding more configs is the right solution.;;;","23/Oct/10 05:25;streamy;This is kind of nice though.  When a cluster gets started by ops they will know right away if there's a misconfiguration as long as they're setting it up with the config you provided.;;;","23/Oct/10 05:31;ryanobjc;its a pretty good ops tool, as jgray points out, thinking back to the Fun Time With LZO this would have helped a lot.  

While it would be nice not to config, it is entirely optional, and it gives people some ability to have hbase self-sanity check.

PS: this config feature was checked in this afternoon :-);;;","30/Oct/10 19:52;stack;I'm commiting an edit to our book as part of HBASE-2006.  It includes the generation of this documentation of this new feature.;;;","30/Oct/10 19:53;stack;Done as part of hbase-2006 commit.  If anything to add, please make the edit yourself or tell me what to stick in there.;;;","02/Dec/10 04:07;streamy;The document references 'hbase.regionserver.codec' but the correct name is 'hbase.regionserver.codecs'.  I was excited to see the doc on this but then scratching my head why it didn't work :);;;","02/Dec/10 04:14;stack;Addressed Jon's reopen.  Closing again. ;;;","02/Dec/10 10:23;larsgeorge;I did point this out in my (maybe to long?) email about the config properties though:

{quote}
hbase.regionserver.codec
src/docbkx/book.xml:      <link
linkend=""hbase.regionserver.codec"">hbase.regionserver.codec</link>
src/docbkx/book.xml:    <section id=""hbase.regionserver.codec"">
src/docbkx/book.xml:    hbase.regionserver.codec
src/docbkx/book.xml:    hbase.regionserver.codec
src/docbkx/book.xml:    hbase.regionserver.codec
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:
  String [] codecs = conf.getStrings(""hbase.regionserver.codecs"",

Above is a typo in the book.xml?
{quote}

Just saying. ;);;;","02/Dec/10 18:32;stack;Sorry Lars.  I missed your original pointer.  Its fixed now anyways.   Shout louder in future.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hBase importtsv fails when the line contains no data.,HBASE-3145,12478111,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kzk,kzk,kzk,22/Oct/10 19:56,12/Jun/22 00:30,14/Jul/23 06:06,22/Oct/10 20:25,0.89.20100924,,,,,,,,,,,,,util,,,,,0,,,"I've tried to import tsv data by using importtsv tools. But the task failed with the following errors.

10/10/23 02:56:52 INFO mapred.JobClient: Task Id : attempt_201010222300_0036_m_000016_2, Status : FAILED
java.lang.IllegalArgumentException: No columns to insert
        at org.apache.hadoop.hbase.client.HTable.validatePut(HTable.java:682)
        at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:544)
        at org.apache.hadoop.hbase.client.HTable.put(HTable.java:535)
        at org.apache.hadoop.hbase.mapreduce.TableOutputFormat$TableRecordWriter.write(TableOutputFormat.java:104)
        at org.apache.hadoop.hbase.mapreduce.TableOutputFormat$TableRecordWriter.write(TableOutputFormat.java:65)
        at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:523)
        at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
        at org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvImporter.map(ImportTsv.java:241)
        at org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvImporter.map(ImportTsv.java:184)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:639)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:315)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:217)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1063)
        at org.apache.hadoop.mapred.Child.main(Child.java:211)

If the line contains invalid data, the parser should throw BadTsvLineException. But unfortunately, the codepath throws IllegalArgumentException for the empty line, and that wasn't caught in the map() function.
",Linux (CDH3),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/10 20:00;kzk;hbase-fix-importtsv.patch;https://issues.apache.org/jira/secure/attachment/12457866/hbase-fix-importtsv.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26673,Reviewed,,,,Fri Oct 22 20:25:54 UTC 2010,,,,,,,,,,"0|i0d3mv:",74365,,,,,,,,,,,,,,,,,,,,,"22/Oct/10 20:00;kzk;I've fixed this issue by changing the parser to throw BadTsvLineException for invalid line. And also added more robustness by adding row key availability check.

Could you check this patch?;;;","22/Oct/10 20:25;tlipcon;Committed, thanks for the patch!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding the tests' hbase-site.xml to the jar breaks some clients,HBASE-3143,12478021,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,jdcryans,jdcryans,jdcryans,21/Oct/10 19:25,20/Nov/15 12:40,14/Jul/23 06:06,22/Oct/10 23:29,,,,,,,,,,,,0.90.0,,,,,,,0,,,Since our move to maven we started including the src/test/resources/hbase-site.xml to our test jar. This breaks the standalone clients that include it like Hive as depicted in HIVE-1597.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/10 19:47;jdcryans;HBASE-3143.patch;https://issues.apache.org/jira/secure/attachment/12457777/HBASE-3143.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26672,Reviewed,,,,Fri Nov 20 12:40:40 UTC 2015,,,,,,,,,,"0|i0hkwn:",100649,,,,,,,,,,,,,,,,,,,,,"21/Oct/10 19:47;jdcryans;I believe this patch does the right thing, I looked around the jars and tars and everything looks right.;;;","22/Oct/10 23:25;stack;+1;;;","22/Oct/10 23:29;jdcryans;Committed to trunk, thanks for looking at the patch Stack.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master RPC server needs to be started before an RS can check in,HBASE-3141,12478016,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,ryanobjc,streamy,streamy,21/Oct/10 18:50,20/Nov/15 12:43,14/Jul/23 06:06,09/Nov/10 02:03,,,,,,,,,,,,0.90.0,,master,,,,,0,,,"Starting up an RPC server is done in two steps.  In the constructor, we instantiate the RPC server.  Then in startServiceThreads() we start() it.

If someone RPCs in between the instantiation and the start(), it seems that bad things can happen.  We need to make sure this can't happen and there aren't any races here.",,hairong,kannanm,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/10 00:23;ryanobjc;HBASE-3141-2.txt;https://issues.apache.org/jira/secure/attachment/12459118/HBASE-3141-2.txt","09/Nov/10 00:33;ryanobjc;HBASE-3141-3.txt;https://issues.apache.org/jira/secure/attachment/12459119/HBASE-3141-3.txt","01/Nov/10 19:32;streamy;HBASE-3141-v1.patch;https://issues.apache.org/jira/secure/attachment/12458573/HBASE-3141-v1.patch","08/Nov/10 23:43;ryanobjc;HBASE-3141.txt;https://issues.apache.org/jira/secure/attachment/12459110/HBASE-3141.txt",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26670,,,,,Fri Nov 20 12:43:47 UTC 2015,,,,,,,,,,"0|i0hkwf:",100648,,,,,,,,,,,,,,,,,,,,,"21/Oct/10 19:10;stack;This looks like an issue Todd was playing with a while back where RS hosting .META. would connect to itself to get data from .META. before RPC had been put up.  He put up a sketch patch over here http://github.com/toddlipcon/hbase/commit/c1236fca54e25c36561d428b6755a950ca1c208f.  In it, he introduces a flag in rpc server  that is set true when rpc has started.  When false, throws exceptions ""Not Yet Accepting Connections"". 

Lets make a test that this is actually an issue.  Maybe the early rpc is just handled post listener/responder startup?;;;","22/Oct/10 04:40;kannanm;We ran into this today during a shutdown/startup:

In 0.89, things happen in this order in the master code:

{code}
In the constructor:
(i) this.rpcServer = HBaseRPC.getServer(this, a.getBindAddress()... )   // instantiate the server..
(ii) Try to become ""primary"" master, by writing to zookeeper.
In the run loop:
(iii) startServiceThreads() --> this.rpcServer.start()
{code}

Step (ii) blocked indefinitely, as a different master became the primary. At startup, some Region Servers were  trying to report in to this master incorrectly... because the /hbase/master ZK node from previous shutdown hadn't quite expired (?) and it still had this master's info.

What if we simply moved (iii) ahead of (ii) (i.e. start the rpcServer in the constructor itself, before blocking on ZK's /hbase/master node). 

Todd's fix seems more elaborate -- is that extra state of ""accepting calls"" really necessary?

Hairong has also suggested that   we add timeouts on the HBaseRpc.getProxy() calls. See stack below where the RS was stuck indefinitely on the above master.

{code}

""regionserver60020"" prio=10 tid=0x00002aaeb4e5d000 nid=0x1cae in Object.wait() [0x000000004264e000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00002aaab7560fa8> (a org.apache.hadoop.hbase.ipc.HBaseClient$Call)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:732)
        - locked <0x00002aaab7560fa8> (a org.apache.hadoop.hbase.ipc.HBaseClient$Call)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:252)
        at $Proxy0.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:408)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:384)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:431)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:342)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getMaster(HRegionServer.java:1210)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.reportForDuty(HRegionServer.java:1227)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:432)
        at java.lang.Thread.run(Thread.java:619)

{code}


;;;","01/Nov/10 19:32;streamy;Patch moves start of service threads to before we put up cluster up flag.  This kind of deals with this issue.

I think real fix is for RPC server to properly handle/reject any requests when it's in a weird state between construction and starting.;;;","01/Nov/10 22:22;stack;I think you need more.  If construction fails, the server will be left up... need to wrap post server start in try/catch that runs shutdown code if exception.;;;","01/Nov/10 23:03;streamy;@Stack, this change isn't in constructor.  the ordering change is within finishInitialization() which is called in the run() of HMaster.  We're already handling exceptions/stopping appropriately at this stage.;;;","02/Nov/10 21:41;stack;Ryan is going to look at this (Jon is busy the next few days);;;","08/Nov/10 23:20;ryanobjc;In the constructor of HMaster we do this call:


    this.rpcServer = HBaseRPC.getServer(this,
      new Class<?>[]{HMasterInterface.class, HMasterRegionInterface.class},
      a.getBindAddress(), a.getPort(),
      numHandlers,
      0, // we dont use high priority handlers in master
      false, conf,
      0); // this is a DNC w/o high priority handlers

The bind to the service ports happens in here.

In the run() thread we 'stall' until we become the primary master, at which point we call finishInitialization() which then starts the rest of the service threads, such as the listener, etc. 

I think the solution here is to start the service threads ASAP, then have a flag which causes the Handler threads to throw instead of service the calls. Then another call that happens later when the master is ready to start getting calls. I'll post a patch with that in it.;;;","08/Nov/10 23:43;ryanobjc;here is a patch that starts the rpc thread infrastructure right away, but throws exceptions until the master says 'im ready to handle calls'.;;;","08/Nov/10 23:53;streamy;Is the move of startServiceThreads() necessary?  I think it's fine to do it before cluster up flag, but just checking, this should work even without that change because RS will get exceptions and retry?

Also, looks like MasterNotRunningException is imported in HBaseServer but it's ServerNotRunningException that is thrown.;;;","08/Nov/10 23:55;streamy;import stuff aside, this seems fine.  would be nice to have some kind of test that this works.

if you don't want to do a new unit test, as long as existing unit tests pass i'm +1 to commit.;;;","09/Nov/10 00:23;ryanobjc;here is my updated patch with unit test;;;","09/Nov/10 00:33;ryanobjc;unit test etc;;;","09/Nov/10 00:39;streamy;+1, thanks Ryan.  This should fix HBASE-3142 as well.;;;","09/Nov/10 02:03;ryanobjc;committed!;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rest schema modification throw null pointer exception,HBASE-3140,12477984,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,d.worms,d.worms,d.worms,21/Oct/10 12:44,20/Nov/15 12:43,14/Jul/23 06:06,21/Oct/10 22:37,,,,,,,,,,,,0.90.0,,REST,,,,,0,,,"2010-10-21 14:41:38,462 ERROR org.mortbay.log: /node_table_modify/schema
java.lang.NullPointerException
	at org.apache.hadoop.hbase.util.Bytes.toBytes(Bytes.java:400)
	at org.apache.hadoop.hbase.client.HBaseAdmin.addColumn(HBaseAdmin.java:597)
	at org.apache.hadoop.hbase.rest.SchemaResource.update(SchemaResource.java:153)
	at org.apache.hadoop.hbase.rest.SchemaResource.update(SchemaResource.java:177)
	at org.apache.hadoop.hbase.rest.SchemaResource.post(SchemaResource.java:204)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:187)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:70)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:279)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:121)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:136)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:121)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:136)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:86)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:136)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:74)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1357)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1289)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1239)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1229)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:420)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:497)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:684)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:390)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:943)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:843)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",against svn trunk,larsfrancke,,,,,,,,,,,,,,,,,,,,,300,300,,0%,300,300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/10 12:47;d.worms;add_column.patch;https://issues.apache.org/jira/secure/attachment/12457747/add_column.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26669,Reviewed,,,,Fri Nov 20 12:43:55 UTC 2015,,,,,,,,,,"0|i0hkw7:",100647,"Patch provided as attach file, apply against svn trunk",,,,,,,,,,,,,,,,,,,,"21/Oct/10 12:47;d.worms;patch generated through git;;;","21/Oct/10 21:58;stack;Committed to TRUNK.  Thanks for the patch David.;;;","21/Oct/10 21:59;stack;Reopen and moving into 0.20.7 in case we ever release that.;;;","21/Oct/10 22:37;apurtell;Committed to 0.20 branch.;;;","21/Oct/10 22:55;streamy;Added 0.90 back as a fix version now that it's resolved.

Added David Worms as a contributor and assigned to him.  Thanks David!;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Server shutdown processor stuck because meta not online,HBASE-3139,12477962,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,21/Oct/10 08:30,20/Nov/15 12:40,14/Jul/23 06:06,23/Oct/10 06:33,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Playing with rolling restart I see that the server hosting root and meta can go down close to each other.  In below, note how we are processing server hosting -ROOT- and part of its processing involves reading .META. content to see what servers it was carrying.  Well, note that .META. is offline at time (our verification attempt failed because server had just been shutdown and verification got ConnectException).  So we pause the server shutdown processing till .META. comes back online -- only it never does.

{code}
2010-10-21 07:32:23,931 INFO org.apache.hadoop.hbase.catalog.RootLocationEditor: Unsetting ROOT region location in ZooKeeper
2010-10-21 07:32:23,953 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Splitting logs for sv2borg182,60020,1287645693959                                                                                       2010-10-21 07:32:23,994 INFO org.apache.hadoop.hbase.catalog.RootLocationEditor: Unsetting ROOT region location in ZooKeeper
2010-10-21 07:32:24,020 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x12bcd5b344b0115 Creating (or updating) unassigned node for 70236052 with OFFLINE state                                                    2010-10-21 07:32:24,045 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan for -ROOT-,,0.70236052 so generated a random one; hri=-ROOT-,,0.70236052, src=, dest=sv2borg181,60020,1287646329081; 8 (online=8, exclude=null) available servers                                                                                                                                                                                         2010-10-21 07:32:24,045 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region -ROOT-,,0.70236052 to sv2borg181,60020,1287646329081                                                                              2010-10-21 07:32:24,048 INFO org.apache.hadoop.hbase.catalog.CatalogTracker: Failed verification of .META.,,1; java.net.ConnectException: Connection refused
2010-10-21 07:32:24,048 INFO org.apache.hadoop.hbase.catalog.CatalogTracker: Current cached META location is not valid, resetting
2010-10-21 07:32:24,079 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=sv2borg181,60020,1287646329081, region=70236052/-ROOT-                                            2010-10-21 07:32:24,162 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=sv2borg181,60020,1287646329081, region=70236052/-ROOT-
2010-10-21 07:32:24,212 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=sv2borg181,60020,1287646329081, region=70236052/-ROOT-                                             2010-10-21 07:32:24,212 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for 70236052; deleting unassigned node                                                                             2010-10-21 07:32:24,212 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x12bcd5b344b0115 Deleting existing unassigned node for 70236052 that is in expected state RS_ZK_REGION_OPENED                              2010-10-21 07:32:24,238 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Opened region -ROOT-,,0.70236052                                                                                                         2010-10-21 07:32:27,902 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=sv2borg183,60020,1287646347597, regionCount=0, userLoad=false                                                                        2010-10-21 07:32:30,523 INFO org.apache.hadoop.hbase.zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [sv2borg184,60020,1287645693960]                                                    2010-10-21 07:32:30,523 DEBUG org.apache.hadoop.hbase.master.ServerManager: Added=sv2borg184,60020,1287645693960 to dead servers, submitted shutdown handler to be executed                                                        2010-10-21 07:32:36,254 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=sv2borg184,60020,1287646355951, regionCount=0, userLoad=false                                                                        2010-10-21 07:32:39,567 INFO org.apache.hadoop.hbase.zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [sv2borg185,60020,1287645693959]                                                    2010-10-21 07:32:39,567 DEBUG org.apache.hadoop.hbase.master.ServerManager: Added=sv2borg185,60020,1287645693959 to dead servers, submitted shutdown handler to be executed                                                        2010-10-21 07:32:45,614 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=sv2borg185,60020,1287646365304, regionCount=0, userLoad=false                                                                        2010-10-21 07:32:48,652 INFO org.apache.hadoop.hbase.zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [sv2borg186,60020,1287645693962]                                                    2010-10-21 07:32:48,652 DEBUG org.apache.hadoop.hbase.master.ServerManager: Added=sv2borg186,60020,1287645693962 to dead servers, submitted shutdown handler to be executed                                                        2010-10-21 07:32:50,097 INFO org.apache.hadoop.hbase.master.ServerManager: regionservers=8, averageload=93.38, deadservers=[sv2borg185,60020,1287645693959, sv2borg183,60020,1287645693959, sv2borg182,60020,1287645693959,        sv2borg184,60020,1287645693960, sv2borg186,60020,1287645693962]
....
{code}

We're supposed to have a thread of 5 executors to handle server shutdowns.  I see an executor stuck waiting on .META. but I dont see any others running.  Odd.  Trying to figure why executors are 1 only.

{code}
""MASTER_SERVER_OPERATIONS-sv2borg180:60000-1"" daemon prio=10 tid=0x0000000041dc7000 nid=0x50a4 in Object.wait() [0x00007f285d537000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:324)
    - locked <0x00007f286d150ce8> (a java.util.concurrent.atomic.AtomicBoolean)
    at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:359)
    at org.apache.hadoop.hbase.catalog.MetaReader.getServerUserRegions(MetaReader.java:487)
    at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:115)
    at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:150)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/10 23:51;stack;3139-v5.txt;https://issues.apache.org/jira/secure/attachment/12457883/3139-v5.txt","21/Oct/10 21:28;streamy;ExecutorServiceUnitTest.patch;https://issues.apache.org/jira/secure/attachment/12457787/ExecutorServiceUnitTest.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26668,Reviewed,,,,Fri Nov 20 12:40:38 UTC 2015,,,,,,,,,,"0|i0hkvz:",100646,,,,,,,,,,,,,,,,,,,,,"21/Oct/10 21:03;stack;So, I just noticed that doing a startup, there is only one executor fielding open requests when we've configured the executor to run a  max of 10 executors (I confirmed that the right config. is going into executor service so something is up where we are not doing thread pool executor).;;;","21/Oct/10 21:28;streamy;Unit test that shows that our executor services only ever run one at a time.;;;","22/Oct/10 03:40;stack;I tried to run w/ latest over on review board and got this:

{code}
2010-10-22 03:36:56,957 FATAL org.apache.hadoop.hbase.master.HMaster: Failed assignment of regions to serverName=sv2borg181,60020,1287718591505, load=(requests=0, regions=0,        usedHeap=29, maxHeap=987)
org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.util.concurrent.RejectedExecutionException
    at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:1768)
    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
    at org.apache.hadoop.hbase.executor.ExecutorService$Executor.submit(ExecutorService.java:285)
    at org.apache.hadoop.hbase.executor.ExecutorService.submit(ExecutorService.java:216)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.openRegion(HRegionServer.java:1944)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.openRegions(HRegionServer.java:1952)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:561)
    at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1025)

    at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:749)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:255)
    at $Proxy1.openRegions(Unknown Source)
    at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:536)
    at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:687)
    at org.apache.hadoop.hbase.master.AssignmentManager$SingleServerBulkAssigner.run(AssignmentManager.java:1111)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
2010-10-22 03:36:56,959 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
{code];;;","22/Oct/10 03:49;stack;I get this if I set core==maxThreads w/ diff2 applied:

{code}
2010-10-22 03:45:51,915 FATAL org.apache.hadoop.hbase.master.HMaster: Failed assignment of regions to serverName=sv2borg185,60020,1287719078335, load=(requests=0, regions=0,        usedHeap=29, maxHeap=987)
org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.util.concurrent.RejectedExecutionException
    at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:1768)
    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
    at org.apache.hadoop.hbase.executor.ExecutorService$Executor.submit(ExecutorService.java:285)
    at org.apache.hadoop.hbase.executor.ExecutorService.submit(ExecutorService.java:216)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.openRegion(HRegionServer.java:1944)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.openRegions(HRegionServer.java:1952)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:561)
    at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1025)

    at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:749)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:255)
    at $Proxy2.openRegions(Unknown Source)
    at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:536)
    at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:687)
    at org.apache.hadoop.hbase.master.AssignmentManager$SingleServerBulkAssigner.run(AssignmentManager.java:1111)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
{code};;;","22/Oct/10 23:51;stack;This patch sets core == max threads and removes the special blocking queue we had in place, the one that tried to pretend like it was at full capacity when it was not.  Patch also adds test that we don't throw exception when more than maxThread items in queue.  I've tested this patch on cluster and it seems to do right thing now.  I turned down some of the sizes in master and regionserver since they were set when we thought things weren't running fast enough when in actuality we were not getting any parallellism.

Chatting w/ Jon, we don't need a priority queue so I need to fix that.  I tried to use ExecutorService#newFixedThreadPool and newCachedThreadPool but these exhibit same brain-deadedness that we saw where core < maxThreads.

Will put new patch soon.;;;","23/Oct/10 06:21;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1064/
-----------------------------------------------------------

(Updated 2010-10-22 23:20:18.196100)


Review request for hbase and stack.


Summary
-------

See HBASE-3139


This addresses bug hbase-3139.
    http://issues.apache.org/jira/browse/hbase-3139


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java 1026145 
  trunk/src/test/java/org/apache/hadoop/hbase/executor/TestExecutorService.java PRE-CREATION 

Diff: http://review.cloudera.org/r/1064/diff


Testing
-------


Thanks,

Jonathan


;;;","23/Oct/10 06:29;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1064/
-----------------------------------------------------------

(Updated 2010-10-22 23:27:28.712219)


Review request for hbase and stack.


Changes
-------

Remove priority queue -- not needed.  Fix naming format for threads.  Make the thread pool max and corepool size the same so we have a fixed pool size.  Adjust thread pool sizes down.  They were upped when we weren't getting throughput when we thought executorservice was doing parallelism but wasnt'.


Summary
-------

See HBASE-3139


This addresses bug hbase-3139.
    http://issues.apache.org/jira/browse/hbase-3139


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java 1026564 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1026564 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 1026564 
  trunk/src/test/java/org/apache/hadoop/hbase/executor/TestExecutorService.java 1026564 

Diff: http://review.cloudera.org/r/1064/diff


Testing
-------


Thanks,

Jonathan


;;;","23/Oct/10 06:31;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1064/#review1632
-----------------------------------------------------------

Ship it!


Looks good to me.  Nice test :)

- Jonathan



;;;","23/Oct/10 06:33;stack;Committed.  Fix mostly done by jgray -- I just did testing... ;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stale reads from ZK can break the atomic CAS operations we have in ZKAssign,HBASE-3136,12477936,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,streamy,streamy,streamy,20/Oct/10 23:19,21/Jun/20 22:35,14/Jul/23 06:06,24/Oct/10 22:33,0.89.20100621,0.89.20100924,0.90.0,,,,,,,,,0.90.0,,Zookeeper,,,,,0,,,"With ZK based region transitions, we rely on atomic state changes of regions in transition.  For example, an RS needs to atomically switch a node from OFFLINE to OPENING, or the master needs to delete nodes that are in OPENED state, etc...

The way we implement this is by:
- Read existing data (returns byte[] and version in Stat)
- Verify data is in expected state
- Update to the new state, passing the expected version previously read

This doesn't always work as expected because that initial read of the existing data could be a stale read (in ZK, writes are quorum writes but reads are not so you can get stale data).

Can provide a more explicit example if anyone is interested, but a fix is coming.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-3137,,,,,,,HBASE-24603,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26666,Reviewed,,,,Sun Oct 24 22:33:28 UTC 2010,,,,,,,,,,"0|i08sfz:",49208,,,,,,,,,,,,,,,,,,,,,"20/Oct/10 23:22;tlipcon;The ""not work"" will just be that the ""update"" phase fails, and we should then retry, no? Or if the ""verify data is in expected state"" section fails, we should sync() and reread?;;;","20/Oct/10 23:29;streamy;Yes.  There are two ways to make this work.  The easy way is to just sync() before we start.  Or, as you describe, we retry after the update fails or if not in expected state we sync() and re-read.

To get something done today, I'm going to just add sync() calls at the start of the two CAS operations.  We can optimize later by being more optimistic and doing what you describe.;;;","20/Oct/10 23:46;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1056/
-----------------------------------------------------------

Review request for hbase, Todd Lipcon and stack.


Summary
-------

Adds a sync(path) operation into ZKW and three calls into it from the CAS operations in ZKAssign.


This addresses bug HBASE-3136.
    http://issues.apache.org/jira/browse/HBASE-3136


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java 1025790 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java 1025790 

Diff: http://review.cloudera.org/r/1056/diff


Testing
-------

Still need to test more.  I'm not sure it's possible (or feasible in a reasonable amount of time) to make a unit test for this.  We'd probably need to dig into ZK or mock the hell out of stuff.


Thanks,

Jonathan


;;;","20/Oct/10 23:48;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1056/
-----------------------------------------------------------

(Updated 2010-10-20 16:47:05.987539)


Review request for hbase, Todd Lipcon and stack.


Changes
-------

Last patch didn't apply for some reason, made a new one.


Summary
-------

Adds a sync(path) operation into ZKW and three calls into it from the CAS operations in ZKAssign.


This addresses bug HBASE-3136.
    http://issues.apache.org/jira/browse/HBASE-3136


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java 1025790 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java 1025790 

Diff: http://review.cloudera.org/r/1056/diff


Testing
-------

Still need to test more.  I'm not sure it's possible (or feasible in a reasonable amount of time) to make a unit test for this.  We'd probably need to dig into ZK or mock the hell out of stuff.


Thanks,

Jonathan


;;;","20/Oct/10 23:52;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1056/#review1590
-----------------------------------------------------------


seems OK, but we're adding couple extra ms of latency here on all of these calls. Is that going to be expensive for assigning lots of regions?
It seems we should be optimistic, and only really need to sync if we see unexpected state or the checked put fails?

- Todd



;;;","21/Oct/10 00:25;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>


bq.  On 2010-10-20 16:49:52, Todd Lipcon wrote:
bq.  > seems OK, but we're adding couple extra ms of latency here on all of these calls. Is that going to be expensive for assigning lots of regions?
bq.  > It seems we should be optimistic, and only really need to sync if we see unexpected state or the checked put fails?

Yeah, gave that a quick shot.  It's not easy (the code gets messy quick so it needs to be well thought out).

I'd like to commit this and we can open another jira to deal with the optimistic approach.


- Jonathan


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1056/#review1590
-----------------------------------------------------------



;;;","21/Oct/10 00:27;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>


bq.  On 2010-10-20 16:49:52, Todd Lipcon wrote:
bq.  > seems OK, but we're adding couple extra ms of latency here on all of these calls. Is that going to be expensive for assigning lots of regions?
bq.  > It seems we should be optimistic, and only really need to sync if we see unexpected state or the checked put fails?
bq.  
bq.  Jonathan Gray wrote:
bq.      Yeah, gave that a quick shot.  It's not easy (the code gets messy quick so it needs to be well thought out).
bq.      
bq.      I'd like to commit this and we can open another jira to deal with the optimistic approach.

OK, that sounds good by me. We'll fix the perf issue in a follow-up.

Would still like Stack to review, I don't know the master code well enough to know if any other places might be missing, but I agree in concept :)


- Todd


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1056/#review1590
-----------------------------------------------------------



;;;","21/Oct/10 00:32;streamy;Opened HBASE-3137 to optimize this.;;;","21/Oct/10 00:43;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>


bq.  On 2010-10-20 16:49:52, Todd Lipcon wrote:
bq.  > seems OK, but we're adding couple extra ms of latency here on all of these calls. Is that going to be expensive for assigning lots of regions?
bq.  > It seems we should be optimistic, and only really need to sync if we see unexpected state or the checked put fails?
bq.  
bq.  Jonathan Gray wrote:
bq.      Yeah, gave that a quick shot.  It's not easy (the code gets messy quick so it needs to be well thought out).
bq.      
bq.      I'd like to commit this and we can open another jira to deal with the optimistic approach.
bq.  
bq.  Todd Lipcon wrote:
bq.      OK, that sounds good by me. We'll fix the perf issue in a follow-up.
bq.      
bq.      Would still like Stack to review, I don't know the master code well enough to know if any other places might be missing, but I agree in concept :)

Cool.  I'm kicking off a test suite run with this patch and my hbck patch right now as well.

HBASE-3137 filed for perf improvements.


- Jonathan


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1056/#review1590
-----------------------------------------------------------



;;;","22/Oct/10 19:06;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1056/#review1620
-----------------------------------------------------------

Ship it!


+1 on getting it in as is.  I think its going to kill my bulk assign improvements but yesterday discussing w/ Jon and Karthik, there are plenty of options to explore yet... We'll do in another issue.

- stack



;;;","24/Oct/10 22:33;streamy;Committed to trunk.

Thanks to Karthik for finding and debugging this issue!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[replication] ReplicationSource can't recover from session expired on remote clusters,HBASE-3130,12477785,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ctrezzo,jdcryans,jdcryans,19/Oct/10 21:38,20/Nov/15 12:41,14/Jul/23 06:06,29/Sep/11 23:41,0.92.0,,,,,,,,,,,0.92.0,,Replication,,,,,2,,,"Currently ReplicationSource cannot recover when its zookeeper connection to its remote cluster expires. HLogs are still being tracked, but a cluster restart is required to continue replication (or a rolling restart).",,amansk,ctrezzo,larsfrancke,larsh,qwertymaniac,tsuna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/11 01:44;ctrezzo;3130-v2.txt;https://issues.apache.org/jira/secure/attachment/12494735/3130-v2.txt","16/Sep/11 18:50;ctrezzo;3130-v3.txt;https://issues.apache.org/jira/secure/attachment/12494846/3130-v3.txt","28/Sep/11 22:43;ctrezzo;3130-v4.txt;https://issues.apache.org/jira/secure/attachment/12496940/3130-v4.txt","15/Sep/11 20:52;ctrezzo;3130.txt;https://issues.apache.org/jira/secure/attachment/12494691/3130.txt","29/Sep/11 22:35;jdcryans;HBASE-3130-v5.patch;https://issues.apache.org/jira/secure/attachment/12497079/HBASE-3130-v5.patch",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26665,Reviewed,,,,Fri Nov 20 12:41:58 UTC 2015,,,,,,,,,,"0|i0hkvb:",100643,ReplicationPeer is now the abortable for the sink's ZK and can be told to reconnect if the session expires.,,,,,,,,,,,,,,,,,,,,"10/Jun/11 22:45;stack;Moving out of 0.92.0. Pull it back in if you think different.;;;","31/Aug/11 03:27;larsh;We will need this at Salesforce.com. So I am likely going to work on this soon.

What is the exact problem?
If it would as simple as just recreating ReplicationZookeeper or using RecoverableZookeeper J-D would have probably just done it :)
;;;","31/Aug/11 03:31;jdcryans;RecoverableZookeeper only recovers from recoverable exceptions, SessionExpired isn't one of those. Basically you have to catch it on the paths that use ZK, mostly hidden behind ReplicationZookeeper. Treat it as a normal exception, retry it every once in a while.;;;","09/Sep/11 22:52;jdcryans;Looks like it got even worse recently, we got a situation where the SessionExpired was treated like if it was the RS's own and it FATAL'ed.;;;","10/Sep/11 13:55;yuzhihong@gmail.com;@J-D:
Can you post snippet of server log containing the FATAL portion ?;;;","10/Sep/11 16:42;larsh;Yes that would be useful. Somebody at Salesforce is currently looking at this issue anyway.;;;","10/Sep/11 17:05;jdcryans;Here it goes:

{quote}
2011-09-09 19:44:28,224 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server serverName=sv4r17s40,60020,1313587209632, load=(requests=4292, regions=186, usedHeap=11929, maxHeap=24749): connection to cluster: 5-0x130d4937f890066 connection to cluster: 5-0x130d4937f890066 received expired from ZooKeeper, aborting
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:343)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:261)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:530)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:506)

{quote}

As you can see it's pretty generic, I could trace it was the peer connection with the ""connection to cluster"". Moreover the fix will take place around ReplicationPeer which contains a ZKW that requires an Abortable which, at the moment, is the RS itself. Instead we should pass our own, or maybe ReplicationSource should implement it.;;;","11/Sep/11 02:28;ctrezzo;Hi all,

I am the ""somebody at salesforce"" that is currently looking at this issue. I should have a sketch patch for you guys shortly.

Here is the general gist of what I am doing:
1. I am now specifically catching the SESSIONEXPIRED KeeperExceptions that are thrown by methods which use a ZookeeperWatcher from ReplicationPeer.

2. I am adding a public method to ReplicationZookeeper that is responsible for retrying/opening a new connection to the peer zookeeper cluster. This method will take a ReplicationPeer (the old rp from the peer we are trying to reconnect to) and an Abortable (which will be the ReplicationSource). It will return a new ReplicationPeer with a fresh connection.

3. I will modify ReplicationSource so that it implements the Abortable interface.

4. Currently I am going back and forth about two retry strategies:
A. There is a sleepMultiplier and a maxNumberOfRetries. The source will try to reconnect until the maxNumberOfRetries is exceeded, after which it will abort.

B. Same as A, except after exceeding maxNumberOfRetries it will continue retrying indefinitely, but at a very low frequency (i.e every 5 min). With this approach I would implement it in a way such that replication could always be turned off, at which point the retries would stop.

Thoughts/comments/suggestions are always appreciated. I am excited to contribute!

Thanks,
Chris Trezzo;;;","11/Sep/11 02:47;larsh;Hey Chris :)
I'd say 4.B. until an admin explicitly stops replication or removes the peer.
I would also probably handle this in ReplicationSource's main loop.
;;;","13/Sep/11 18:29;larsh;This seems like an important bug fix, can we but this into 0.92 even after we branched it?;;;","13/Sep/11 18:34;stack;@Lars Bugs and doc improvements yes.  Features no.;;;","14/Sep/11 21:03;larsh;@J-D While testing we encountered the problem above (where the RegionServer closes) too.

Should we fix that part in a separate jira, as this one might take a bit longer? The stop gap change is to log the problem and move on, i.e. simply pass an adhoc Abortable a peer's zk watcher.
;;;","15/Sep/11 18:33;jdcryans;@Chris

Sorry for the late answer.

bq. 1. I am now specifically catching the SESSIONEXPIRED KeeperExceptions that are thrown by methods which use a ZookeeperWatcher from ReplicationPeer.

They are all catch at a super low lovel (ZKW) so I don't think you need that.

bq. 2. I am adding a public method to ReplicationZookeeper that is responsible for retrying/opening a new connection to the peer zookeeper cluster.

There might be some synchronization problems with ReplicationSource, watch out.

bq. 3. I will modify ReplicationSource so that it implements the Abortable interface.

I'm -0, I'd prefer if all the handling was kept in ReplicationZookeeper. Ideally for ReplicationSource it would just see that it can't reach the peer for some reason and retry.

bq. 4. Currently I am going back and forth about two retry strategies:

Definitely 4B, that's how the code works at the moment. Wait until the peer comes back, the operator can always turn it off.;;;","15/Sep/11 20:50;ctrezzo;@J-D

Thanks for the comments. I am attaching a first patch that I worked on with Lars. There are a few things that are slightly different from your comments above, but I figured I would post it anyways just to get the ball rolling. Let me know what you think.

Here is an overview of the changes:

1. Added a new reloadZkWatcher method to ReplicationPeer. This method is responsible for reseting the ReplicationPeer's ZookeeperWatcher when the connection for the old watcher dies.

2. The crux of the change is in ReplicationSource.chooseSinks. We catch the KeeperException there, and if it is a ConnectionLoss or SessionExpired we reset that peer's ZookeeperWatcher using the new method from (1). Retries are handled by the existing loops in the callers of chooseSinks (i.e. connectToPeers and shipEdits).

3. ReplicationPeer now implements Abortable and is passed in as the Abortable for the peer's ZookeeperWatcher. The abort method simply logs the fact that it was called and returns.

I ran TestReplication, TestMasterReplication and TestMultiSlaveReplication tests with no failures. I also manually tested the case where the slave cluster fails, and confirmed that the ReplicationSource does recover and resumes replication once the peer cluster comes back.

Thanks,
Chris;;;","15/Sep/11 21:39;jdcryans;Made Chris a contributor, assigning to him.;;;","15/Sep/11 22:32;jdcryans;Some comments on the contribution itself since you're a newcomer:

 - The logging should be done like we do it elsewhere in the code, by using the Log class provided by Commons Logging and then using the LogFactory. ReplicationSource is an example.
 - When logging, try to include relevant information because with all different threads logging it can be hard to follow. For example, this could tell which peer we're talking about:
bq. Log.info(""Refreshing ZookeeperWatcher"");
 - Lines should be max 80 characters long (the one where you're catching SessionExpiredException for example).

More info in this chapter of the book: http://hbase.apache.org/book/developer.html

Now on the content, I still think that the management of the connection should be done inside ReplicationZookeeper. In its current form the patch exposes a feature envy between ReplicationSource and ReplicationPeer, might just be better to contain the functionality in between.

Thanks for working on this Chris!;;;","16/Sep/11 01:43;ctrezzo;@J-D

Thanks for the response! Attaching a new patch with your comments in mind.

Here are the changes:

1. Fixed the logging in ReplicationPeer so that it uses the commons logging LogFactory.

2. Fixed the line with 80+ characters.

3. Moved the connection management logic from ReplicationSource down to ReplicationZookeeper.getSlavesAddresses.

4. I kept the ReplicationPeer.reloadZkWatcher method for a couple reasons. (1) In order to move all the ZookeeperWatcher logic out of ReplicationPeer, we would need to have a setZkw method. This potentially allows for null ZookeeperWatchers within a ReplicationPeer (the reloadZkWatcher method seems like a slightly cleaner choice). (2) It is basically doing the exact same thing that the ReplicationPeer constructor is already doing.

Let me know what you think.

Thanks,
Chris;;;","16/Sep/11 02:21;larsh;+1

I think the patch is nice because it
o Keeps retrying logic in ReplicationSource
o Removes handling KeeperExceptions from ReplicationSource
o Has ReplicationPeer manage its watcher
o Now the zoo keeper failure handling is in ReplicationZookeeper

Nit: You could use Collections.emptyList() instead of new ArrayList<String>(0), and then just fall through to the set- and return getRegionServers.
;;;","16/Sep/11 18:50;ctrezzo;Submitting a new patch that addresses the Collections.emptyList() comment from Lars.;;;","16/Sep/11 18:57;jdcryans;I like where this is going.

Further improvements:

 - In RP, the creation of the ZKW should be refactored
 - The logging in abort() could use some more explanation
 - Would it be possible to have a unit test?;;;","16/Sep/11 18:59;stack;@Chris If you need help writing unit test, ask for it.;;;","16/Sep/11 23:39;jdcryans;@Chris 

Stack was telling me that you were wondering if you need 2 ZK ensembles, I personally don't think so since the session for the master -> slave connection is different from all the others. So you can just expire that one (maybe across all region servers on the master side).;;;","17/Sep/11 02:30;ctrezzo;@J-D

That sounds like a good plan. I'll try writing a test, and if I run into anything weird I'll let you know.;;;","21/Sep/11 04:01;ctrezzo;@J-D

Now that I have looked at the test code a bit, I have a question:

My current understanding is that to kill the master->slave connection, you need to somehow get the session id and session password for the ReplicationPeer's zookeeper session (i.e. you need the ZookeeperWatcher instance). Currently, this is not exposed. Also, this does not seem like something we would want to expose if the only motivation is for testing. Any thoughts?

I could be missing something obvious.

Thanks!
Chris;;;","21/Sep/11 04:58;jdcryans;At the very minimum you could do a TestReplicationPeer that tests the session ""recovery"" code. An integration test might be harder since you have to fiddle with the internals, maybe explore the avenue of having a test that resides in the same package (o.a.h.h.r.replication) and expose the methods only there.;;;","27/Sep/11 20:29;jdcryans;Another thing about the latest patch, this line needs to be removed in RP:

bq. * @param zkw zookeeper connection to the peer;;;","27/Sep/11 23:54;jdcryans;Also I'd add that I was able to test the patch (on 0.90) and it really works, proof:

First it loses the connection:
{quote}
2011-09-27 16:44:54,984 WARN org.apache.hadoop.hbase.replication.ReplicationPeer: connection to cluster: 10.10.30.7:2181:/hbase1-0x132ad0f29d70017 connection to cluster: 10.10.30.7:2181:/hbase1-0x132ad0f29d70017 received expired from ZooKeeper, aborting
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:343)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:261)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:530)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:506)
2011-09-27 16:44:54,984 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down
{quote}

Then later when it tries to replicate it tries to talk to ZK again and it works after a reload:

{quote}
2011-09-27 16:49:03,738 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Since we are unable to replicate, sleeping 1000 times 10
2011-09-27 16:49:13,738 WARN org.apache.hadoop.hbase.replication.ReplicationZookeeper: Lost the ZooKeeper connection for peer 1
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase1/rs
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:118)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1243)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.listChildrenNoWatch(ZKUtil.java:389)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.listChildrenAndGetAsAddresses(ZKUtil.java:355)
	at org.apache.hadoop.hbase.replication.ReplicationZookeeper.fetchSlavesAddresses(ReplicationZookeeper.java:268)
	at org.apache.hadoop.hbase.replication.ReplicationZookeeper.getSlavesAddresses(ReplicationZookeeper.java:239)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.chooseSinks(ReplicationSource.java:205)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:588)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:341)
2011-09-27 16:49:13,772 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=10.10.30.7:2181 sessionTimeout=20000 watcher=connection to cluster: 10.10.30.7:2181:/hbase1
2011-09-27 16:49:13,773 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server /10.10.30.7:2181
2011-09-27 16:49:14,111 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to hbasedev.sfo.stumble.net/10.10.30.7:2181, initiating session
2011-09-27 16:49:14,140 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server hbasedev.sfo.stumble.net/10.10.30.7:2181, sessionid = 0x132ad0f29d70024, negotiated timeout = 20000
{quote};;;","28/Sep/11 02:12;larsh;You sound like you are surprised :)


;;;","28/Sep/11 02:16;jdcryans;More like joyful amusement, it's been broken for so long...  we're pushing this in prod really soon.;;;","28/Sep/11 22:42;ctrezzo;I finally had some time last night to look at the test code. Here is a new patch that addresses the above comments and adds a new test. TestReplicationPeer verifies the refresh ZooKeeperWatcher functionality in ReplicationPeer.

As per J-D's comment above, doing more of an integration test at a higher level seems to be quite tricky and may require a large change.

Let me know what you guys think.

Thanks!
Chris;;;","28/Sep/11 22:51;ctrezzo;I am also glad the patch brings joyful amusement :-);;;","29/Sep/11 22:35;jdcryans;Here's another iteration on the patch, I did the following:

 - Modified the test to not create a full cluster but just ZK, had to add an option in the testing utility to not create an HTable. That also fixed the test.
 - Removed the copyright in RP and fixed some javadoc.
 - Also in RP the constructor now calls reload (that was the refactoring I mentioned earlier).

Let me know if it makes sense to you.;;;","29/Sep/11 23:01;ctrezzo;That looks good to me!;;;","29/Sep/11 23:03;larsh;+1;;;","29/Sep/11 23:41;jdcryans;Committed to 0.92 and trunk, thanks for the good work Chris!;;;","30/Sep/11 13:10;hudson;Integrated in HBase-0.92 #33 (See [https://builds.apache.org/job/HBase-0.92/33/])
    HBASE-3130  [replication] ReplicationSource can't recover from session
               expired on remote clusters (Chris Trezzo via JD)
   HBASE-4499  [replication] Source shouldn't update ZK if it didn't progress
               (Chris Trezzo via JD)

jdcryans : 
Files : 
* /hbase/branches/0.92/CHANGES.txt
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeer.java
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
* /hbase/branches/0.92/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
* /hbase/branches/0.92/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationPeer.java
;;;","30/Sep/11 15:22;hudson;Integrated in HBase-TRUNK #2272 (See [https://builds.apache.org/job/HBase-TRUNK/2272/])
    HBASE-3130  [replication] ReplicationSource can't recover from session
               expired on remote clusters (Chris Trezzo via JD)
   HBASE-4499  [replication] Source shouldn't update ZK if it didn't progress
               (Chris Trezzo via JD)

jdcryans : 
Files : 
* /hbase/trunk/CHANGES.txt
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeer.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationPeer.java
;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"On assign, if ConnectException, reassign another server",HBASE-3128,12477783,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,19/Oct/10 21:24,20/Nov/15 12:42,14/Jul/23 06:06,20/Oct/10 05:13,,,,,,,,,,,,0.90.0,,,,,,,0,,,"{code}

2010-10-19 13:29:56,116 DEBUG [Thread-216-EventThread] master.AssignmentManager(321): Handling transition=M_ZK_REGION_OFFLINE, server=172.24.152.112:51845, region=1028785192/.META.
2010-10-19 13:29:56,116 ERROR [Master:0;172.24.152.112:51845] master.ServerManager(573): Error connecting to region server
org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed setting up proxy interface org.apache.hadoop.hbase.ipc.HRegionInterface to /172.24.152.112:51850 after attempts=1
	at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:351)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:936)
	at org.apache.hadoop.hbase.master.ServerManager.getServerConnection(ServerManager.java:568)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:511)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:726)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:640)
	at org.apache.hadoop.hbase.master.AssignmentManager.assignMeta(AssignmentManager.java:935)
	at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:430)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:377)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:266)
	at java.lang.Thread.run(Thread.java:637)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:404)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:310)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:860)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:728)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:255)
	at $Proxy10.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:412)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:388)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:435)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:345)
	... 10 more
2010-10-19 13:29:56,116 WARN  [Master:0;172.24.152.112:51845] master.AssignmentManager(730): Failed assignment of .META.,,1.1028785192 to serverName=172.24.152.112,51850,1287520189982, load=(requests=0, regions=0, usedHeap=60, maxHeap=123)
java.lang.RuntimeException: Fatal error connection to RS
	at org.apache.hadoop.hbase.master.ServerManager.getServerConnection(ServerManager.java:574)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:511)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:726)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:640)
	at org.apache.hadoop.hbase.master.AssignmentManager.assignMeta(AssignmentManager.java:935)
	at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:430)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:377)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:266)
	at java.lang.Thread.run(Thread.java:637)
Caused by: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed setting up proxy interface org.apache.hadoop.hbase.ipc.HRegionInterface to /172.24.152.112:51850 after attempts=1
	at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:351)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:936)
	at org.apache.hadoop.hbase.master.ServerManager.getServerConnection(ServerManager.java:568)
	... 8 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:404)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:310)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:860)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:728)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:255)
	at $Proxy10.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:412)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:388)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:435)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:345)
	... 10 more

{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/10 05:03;stack;3128.txt;https://issues.apache.org/jira/secure/attachment/12457642/3128.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26664,,,,,Fri Nov 20 12:42:51 UTC 2015,,,,,,,,,,"0|i0hkv3:",100642,,,,,,,,,,,,,,,,,,,,,"20/Oct/10 05:03;stack;This exception is way louder than its bite.  In fact, its harmless -- not the reason for the test failure.

The noise comes of the fact that we had weird catch inside in ServerManager openRegion where we'd print exception, convert IOE to a RuntimeException then up in AssignmentManager assign we'd catch this exception, print it out again, then go reassign but to a different server.  Here's the log after the big spew:

{code}
2010-10-19 13:29:56,117 DEBUG [Master:0;172.24.152.112:51845] master.AssignmentManager(801): No previous transition plan for .META.,,1.1028785192 so generated a random one; hri=.META.,,1.1028785192, src=, dest=172.24.152.112,51852,1287520190000; 3 (online=3, exclude=serverName=172.24.152.112,51850,1287520189982, load=(requests=0, regions=0, usedHeap=60, maxHeap=123)) available servers
{code}

This happens a few times in this log.

Actual issue that broke test might be this:

{code}
2010-10-19 13:29:56,235 INFO  [Master:0;172.24.152.112:51845] master.AssignmentManager(1004): Bulk assigning done
2010-10-19 13:29:56,237 DEBUG [MASTER_OPEN_REGION-172.24.152.112:51845-1] handler.OpenedRegionHandler(87): Handling OPENED event for f98a67e5ccb2d3d04e8c1d20a71c20db; deleting unassigned node
2010-10-19 13:29:56,237 DEBUG [MASTER_OPEN_REGION-172.24.152.112:51845-1] zookeeper.ZKAssign(349): master:51845-0x12bc62e746c0014 Deleting existing unassigned node for f98a67e5ccb2d3d04e8c1d20a71c20db that is in expected state RS_ZK_REGION_OPENED
2010-10-19 13:29:56,238 DEBUG [MASTER_OPEN_REGION-172.24.152.112:51845-1] zookeeper.ZKUtil(1012): master:51845-0x12bc62e746c0014 Retrieved 112 byte(s) of data from znode /hbase/unassigned/f98a67e5ccb2d3d04e8c1d20a71c20db; data=;;;","20/Oct/10 05:13;stack;So, yeah, we were already reassiging a different server on ConnectException... the commit just cleans up the exceptions thrown so its more plain we're doing the right thing.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[rest] Do not perform cache control when returning results,HBASE-3121,12477677,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,apurtell,apurtell,apurtell,18/Oct/10 22:38,20/Nov/15 12:42,14/Jul/23 06:06,18/Oct/10 23:05,,,,,,,,,,,,0.90.0,,,,,,,0,,,"The REST interface currently provides MaxAge hints to HTTP cache servers when returning results, but does not do so in a way that makes much sense. For some other responses such as scanner results or schema, the REST interface provides a NoCache hint. That seems appropriate. Otherwise, especially given the rich configuration languages of caching servers such as Varnish, it is probably not appropriate to manage cache policy in the REST interface. ",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/10 22:38;apurtell;HBASE-3121.patch;https://issues.apache.org/jira/secure/attachment/12457498/HBASE-3121.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26659,Reviewed,,,,Fri Nov 20 12:42:52 UTC 2015,,,,,,,,,,"0|i0hku7:",100638,,,,,,,,,,,,,,,,,,,,,"18/Oct/10 22:41;hbasereviewboard;Message from: ""Andrew Purtell"" <apurtell@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1041/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

The REST interface currently provides MaxAge hints to HTTP cache servers when returning results, but does not do so in a way that makes much sense. For some other responses such as scanner results or schema, the REST interface provides a NoCache hint. That seems appropriate. Otherwise, especially given the rich configuration languages of caching servers such as Varnish, it is probably not appropriate to manage cache policy in the REST interface. 


This addresses bug HBASE-3121.
    http://issues.apache.org/jira/browse/HBASE-3121


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/rest/RESTServlet.java ed92857 
  src/main/java/org/apache/hadoop/hbase/rest/RowResource.java a4cdd1c 
  src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java 0c134aa 

Diff: http://review.cloudera.org/r/1041/diff


Testing
-------


Thanks,

Andrew


;;;","18/Oct/10 22:45;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1041/#review1562
-----------------------------------------------------------

Ship it!


- Ryan



;;;","18/Oct/10 23:05;apurtell;Committed to trunk and 0.20 branch. Thanks for the review Ryan.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test up on hudson are leaking zookeeper ensembles,HBASE-3114,12477425,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,15/Oct/10 06:08,20/Nov/15 12:43,14/Jul/23 06:06,19/Oct/10 05:43,,,,,,,,,,,,0.90.0,,test,,,,,0,,,"Here is from a recent run up on Hudson:

{code}
2010-10-14 23:31:56,482 INFO  [main] zookeeper.MiniZooKeeperCluster(111): Failed binding ZK Server to client port: 21818
2010-10-14 23:31:56,483 INFO  [main] zookeeper.MiniZooKeeperCluster(111): Failed binding ZK Server to client port: 21819
2010-10-14 23:31:56,483 INFO  [main] zookeeper.MiniZooKeeperCluster(111): Failed binding ZK Server to client port: 21820
2010-10-14 23:31:56,522 INFO  [main] zookeeper.MiniZooKeeperCluster(125): Started MiniZK Server on client port: 21821
{code}

See how we start trying to bind to 21818 but we don't get a free port till we get to 21821?

Some test or tests is not cleaning up after itself leaving a running zk cluster or two about.

TestReplication looks to be suspect.  Here is its @AfterClass method:

{code}
  /**
   * @throws java.lang.Exception
   */
  @AfterClass
  public static void tearDownAfterClass() throws Exception {
    /* REENABLE
    utility2.shutdownMiniCluster();
    utility1.shutdownMiniCluster();
    */
  }
{code}

",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/10 05:16;stack;3114-v2.txt;https://issues.apache.org/jira/secure/attachment/12457323/3114-v2.txt","16/Oct/10 00:14;stack;3114.txt;https://issues.apache.org/jira/secure/attachment/12457314/3114.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26657,,,,,Fri Nov 20 12:43:12 UTC 2015,,,,,,,,,,"0|i0hkt3:",100633,,,,,,,,,,,,,,,,,,,,,"16/Oct/10 00:14;stack;This patch adds some fixup to HBaseTestingUtility to make it so can run multiple instances in the one JVM.  It then adds a test to TestHBaseTestingUtility based on the removed TestMultiClusters.  It then reenables the shutdown disabled in TestReplication.  

Needs more testing.;;;","16/Oct/10 05:16;stack;This version of the patch seems to pass on multiple tries on two different OSs.  Will commit.  Here is commit message:

{code}
M src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
  Add a passedZkCluster flag.  If we didn't start the zk cluster,
  don't shut it down on way out.
M src/test/java/org/apache/hadoop/hbase/TestHBaseTestingUtility.java
  Add a test that starts up three clusters in a vm all sharing single
  zk, each to its own chroot location.  Add to tables in each and
  very the add by doing same as the old TestMultiClusters used)
M src/test/java/org/apache/hadoop/hbase/replication/TestReplication.java
  Reenable shutdown of started clusters.  Hopefully this clears
  the leaking of zk ensembles up on hudson.
M src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java
  Testing, it looks like legitimately the callback for session connected
  can be invoked before construction of zookeeperwatcher finishes.  In
  particular the zookeeper data member can be null.  Hang around for a
  second of so before throwing an exception (Make the exception
  indicate the particular zkw by adding to the error message stack
  trace made at zkw construction -- helps with debugging this stuff)
{code}

Will commit to see if this cures the leaking ensembles up on hudson.

Here is where we are currently at in that regard:

{code}
2010-10-15 23:40:37,416 INFO  [main] zookeeper.MiniZooKeeperCluster(111): Failed binding ZK Server to client port: 21818
2010-10-15 23:40:37,418 INFO  [main] zookeeper.MiniZooKeeperCluster(111): Failed binding ZK Server to client port: 21819
2010-10-15 23:40:37,419 INFO  [main] zookeeper.MiniZooKeeperCluster(111): Failed binding ZK Server to client port: 21820
2010-10-15 23:40:37,419 INFO  [main] zookeeper.MiniZooKeeperCluster(111): Failed binding ZK Server to client port: 21821
2010-10-15 23:40:37,480 INFO  [main] zookeeper.MiniZooKeeperCluster(125): Started MiniZK Server on client port: 21822
{code}

This patch will not fix the sporadic fail of TestReplication.  J-D is on that.
{code};;;","16/Oct/10 20:17;stack;Since the patch went in, we seem to be holding steady.  Here is from #1560.;;;","19/Oct/10 05:43;stack;Resolving.  Its holding steady since this patch went in.  Hopefully next restart of hudson will clean up those that are sticking around.  Here is from build #1565:

{code}
2010-10-19 02:24:46,170 INFO  [main] zookeeper.MiniZooKeeperCluster(111): Failed binding ZK Server to client port: 21818
2010-10-19 02:24:46,170 INFO  [main] zookeeper.MiniZooKeeperCluster(111): Failed binding ZK Server to client port: 21819
2010-10-19 02:24:46,170 INFO  [main] zookeeper.MiniZooKeeperCluster(111): Failed binding ZK Server to client port: 21820
2010-10-19 02:24:46,171 INFO  [main] zookeeper.MiniZooKeeperCluster(111): Failed binding ZK Server to client port: 21821
2010-10-19 02:24:46,210 INFO  [main] zookeeper.MiniZooKeeperCluster(125): Started MiniZK Server on client port: 21822
{code};;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't reassign regions if cluster is being shutdown,HBASE-3113,12477400,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,14/Oct/10 21:18,20/Nov/15 12:41,14/Jul/23 06:06,15/Oct/10 20:47,,,,,,,,,,,,0.90.0,,,,,,,0,,,"On stop of a cluster, handling a close message, we'll go ahead and reassign regions as per normal though the cluster up flag is false.  This is what cause the TestRegionRebalancing test to fail up on hudson just now, #1546.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/10 20:41;stack;3113-part2.txt;https://issues.apache.org/jira/secure/attachment/12457293/3113-part2.txt","16/Oct/10 05:06;stack;3113-part3.txt;https://issues.apache.org/jira/secure/attachment/12457322/3113-part3.txt","14/Oct/10 23:10;stack;3113.txt;https://issues.apache.org/jira/secure/attachment/12457206/3113.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26656,,,,,Fri Nov 20 12:41:13 UTC 2015,,,,,,,,,,"0|i0hksv:",100632,,,,,,,,,,,,,,,,,,,,,"14/Oct/10 23:10;stack;Check clusterup flag before going ahead with an assignment.;;;","14/Oct/10 23:11;stack;I applied to TRUNK.  Lets see how it does up on hudson.;;;","15/Oct/10 17:58;stack;This patch needs a part two.  While we don't assign any more if cluster is going down, I saw that #1552 build up on hudson failed because load balancer gave a region to a server that we'd asked to shutdown.  The RS proceeded to open the region and since it had come in under the wire, the RS stayed up holding up test completion.

A RS that has been asked to stop -- by user via HBaseAdmin or in a unit test testing balancing -- should reject requests to open regions.  Master should then catch the rejection exception and reassign the region elsewhere.;;;","15/Oct/10 20:41;stack;Here is a part 2. RS will throw new RegionServerClosedException and assign will catch and reassign bypassing the going-down server.

No unit test.  I thought about it a while and its a hard scenario to manufacture in unit test (an held-up regionserver shutdown with a pointed request to open delivered while its midst shutdown).;;;","15/Oct/10 20:47;stack;Committed.  Resolving.  Will open new issue if problems w/ this implementation.;;;","16/Oct/10 05:06;stack;I missed an important bit of 3113, actual invocation of the functionality part 2 added.  Heres a bit of a commit message to go with the part3 commit:

{code}
M src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
  Bit of javadoc.
M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  If bad server, pass in name of bad server when remaking assignment
  plan -- i missed this important part on commit of part 2 of 3113.
{code};;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable and disable of table needs a bit of loving in new master,HBASE-3112,12477377,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,stack,stack,14/Oct/10 17:17,20/Nov/15 12:41,14/Jul/23 06:06,09/Nov/10 21:33,,,,,,,,,,,,0.90.0,,,,,,,0,,,"The tools are in place to do a more reliable enable/disable of tables.  Some work has been done to hack in a basic enable/disable but its not enough -- see the test avro/thrift tests where a disable/enable/disable switchback can confuse the table state (and has been disabled until this issue addressed).

This issue is about finishing off enable/disable in the new master.   I think we need to add to the table znode an enabling/disabling state rather than have them binary with a watcher that will stop an enable (or disable) starting until the previous completes (Currently we atomically switch the state though the region close/open lags -- some work in enable/disable handlers helps in that they won't complete till all regions have transitioned.. but its not enough).

Need to add tests too.

Marking issue critical bug because loads of the questions we get on lists are about enable/disable probs.",,larsfrancke,streamy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/10 21:29;stack;3112-v14.txt;https://issues.apache.org/jira/secure/attachment/12459179/3112-v14.txt","07/Nov/10 06:08;stack;3112-v2.txt;https://issues.apache.org/jira/secure/attachment/12459015/3112-v2.txt","07/Nov/10 18:35;stack;3112-v3.txt;https://issues.apache.org/jira/secure/attachment/12459021/3112-v3.txt","06/Nov/10 06:05;stack;3112.txt;https://issues.apache.org/jira/secure/attachment/12458987/3112.txt",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26655,,,,,Fri Nov 20 12:41:15 UTC 2015,,,,,,,,,,"0|i0hksn:",100631,,,,,,,,,,,,,,,,,,,,,"22/Oct/10 23:15;jdcryans;While testing if HBASE-2522 was still there with the new master, I found out that we assign the parent regions when we re-enable the table (but disabling seems to work ok).;;;","31/Oct/10 06:00;stack;Testing up on a biggish cluster (3k regions), I see this:

{code}

hbase(main):002:0> disable 'usertable'





10/10/31 05:52:45 INFO client.HBaseAdmin: Disabled usertable
0 row(s) in 113.3990 seconds

hbase(main):003:0>
hbase(main):004:0*
hbase(main):005:0*
hbase(main):006:0*
hbase(main):007:0*
hbase(main):008:0* enable 'usertable'




10/10/31 05:57:45 INFO client.HBaseAdmin: Enabled table usertable
0 row(s) in 273.0060 seconds
{code}

... it seems wrong that the shell is waiting.  I thought enable/disable near instantaneous... I suppose these are shell commands are hung up on the disable/enable handlers which is fine I suppose, for now.  Checking.;;;","04/Nov/10 18:20;stack;Shell waits on enable/disable.  Thats probably fine except I see this on enable of 600 regions that are taking a little while:

{code}

ERROR: java.net.SocketTimeoutException: Call to sv2borg180/10.20.20.180:60000 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.20.20.180:40901 remote=sv2borg180/10.20.20.180:60000]
{code}

... which would be a little disorientating.

St.Ack;;;","05/Nov/10 23:33;stack;So, its possible for the 'enabled' flag to be set in zk but all regions get offlined (e.g. server crashed while disabling some regions).  The way we online is that we get the list of all the regions in a table and regardless, we go assign WHETHER OR NOT THE REGION HAS BEEN OFFLINED OR NOT.  Can ""double"" the regions on your cluster.

Currently also, the client will not go to the server if it sees that the table state is disabled up in zk.  This is not helpful for the case where not all regions were offlined (see above).

So, need to think about it some.  Could be as simple as checking state of region before we enable/disable (assign/unassign) since we got its state from .META. (if not in RIT that is).

Could introduce a disabling/enabling state and disabling a table, you make entry in zk, one for each region.

Need to add to hbck note as to whether table is enabled or disabled.

Could do minimum for now and come back to this later after 0.90.0.;;;","05/Nov/10 23:41;streamy;I don't think we need disabling/enabling at region granularity.  When master acknowledges opens/closes of regions, he already checks if a table is disabled.  I think that stuff is mostly good.

I think adding enabling/disabling to the disabled table node that we've added should be sufficient.  Just need a master to have an internal state about an enabling or disabling table.  If failover, master will rebuild this state for any table in a enabling/disabling state (he'll have to actually ask everyone what they serve) and will issue assign/unassign as necessary.

Can you describe the case you've seen a little better?  I'd like to add a test of it.  It's when enabling a table?  Not quite sure I understand exactly at which point who is dying?;;;","06/Nov/10 04:32;stack;On way home was thinking of how I need a disabling/enabling state for a table.  I need at least for case where a split is happening when a region is being disabled.  When splits go to open on the regionserver, they should check zk if table is disabled or disabling and pass on the open (splits don't do state up in zk).

@Jon Disable and enable are run from executor.  In each case, we do a get of all regions in a table and then per region we run unassign inline inside in the executor.  Any failure of a server while this process is going on makes for a table that is partially disabled/enabled.

;;;","06/Nov/10 05:02;stack;Currently enable/disable runs synchronously and serially in that the enable/disable handler runs each table regions unassign one after the other.  This is at a minimum slow when it doesn't have to be.

What if we changed it so it was async.

Here is how it might work.

 * Client invokes disable table on master
 ** Disable invocation returns immediately
 ** We add a new is_disabled/is_enabled call to client so client can check on state of enable/disable.
 * Master queues a DisableTableHandler.
 ** DTH checks if table already disabled, if so returns
 ** Otherwise, sets disabling flag up in zk then loops on
 *** Getting all regions in a table from meta
 *** Per region, checks if in RIT or not already offline, and if not queues close region executor
 *** Waits around till all table regions clear RIT
 ** On exit from the loop, it sets up in zk that table is disabled.

Same for ETH

If master dies midway, new master will start up a DTH or ETH per table that has disabling or enabling up in zk.

DTH/ETH must be made idempotent because I think there are situations even still in which they might fail. If they fail, user might reschedule the disable/enable which would start up a new DTH (if already a DTH queued and running, they won't clash since only one thread for this exectuor and the second will just early out because when it checks flag in zk it'll be disabled so it'll have no work to do).;;;","06/Nov/10 06:05;stack;Just a start.

Cleans out code that used set the offline of a region into .META.  We don't do 'offline' disabling.. that state is in zk.  Offline is only for parents of splits not yet cleared.

Started in on a bulk assigned for enable/disable of table so can have an executor to run assign/unassigns in bulk.

DTH is a mess.

TODO: Add disabling/enabling of table.. try to use same znode to hold all disabling/enable/disabled states.;;;","07/Nov/10 06:08;stack;Here is complete implementation for disable.  I have enable to do yet.

Adds new base bulk assign class used on start up doing bulk assign and for enable/disable.  Adds new enabling/disabling table states up in zk.

Not tested but outline of how things will work should be plain.;;;","07/Nov/10 18:35;stack;Add enable of a table.  Still need to test.;;;","08/Nov/10 19:40;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1187/
-----------------------------------------------------------

Review request for hbase and Jonathan Gray.


Summary
-------

Renamed ZKTableDisable as ZKTable, making it a generic zk util for managing 'tables'.
Added enabing/disabling states to table the current set of enabled/disabled only.

M src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
  (createSetData): Added.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
  Removed offlining region utility methods no longer used.
  (We do it now over in MetaEditor)
M src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
  Javadoc.
M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  Add a base abstract class to do 'bulk assignments'.  Redo
  assignAllUserRegions to use subclass of new bulk assigner class.
  Added isTableEnabled, disablingTable, enablingTable.
M src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java
  Redid to use new bulk assigner class.
M src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java
A Added TestZKTable


This addresses bug hbase-3112.
    http://issues.apache.org/jira/browse/hbase-3112


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTableDisable.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java 1032652 
  trunk/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java 1032652 
  trunk/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKTable.java PRE-CREATION 

Diff: http://review.cloudera.org/r/1187/diff


Testing
-------


Thanks,

stack


;;;","08/Nov/10 19:49;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1187/
-----------------------------------------------------------

(Updated 2010-11-08 11:47:12.895726)


Review request for hbase and Jonathan Gray.


Changes
-------

Patch was missing the new files.


Summary
-------

Renamed ZKTableDisable as ZKTable, making it a generic zk util for managing 'tables'.
Added enabing/disabling states to table the current set of enabled/disabled only.

M src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
  (createSetData): Added.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
  Removed offlining region utility methods no longer used.
  (We do it now over in MetaEditor)
M src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
  Javadoc.
M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  Add a base abstract class to do 'bulk assignments'.  Redo
  assignAllUserRegions to use subclass of new bulk assigner class.
  Added isTableEnabled, disablingTable, enablingTable.
M src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java
  Redid to use new bulk assigner class.
M src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java
A Added TestZKTable


This addresses bug hbase-3112.
    http://issues.apache.org/jira/browse/hbase-3112


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java PRE-CREATION 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTableDisable.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java 1032652 
  trunk/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java 1032652 
  trunk/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKTable.java PRE-CREATION 

Diff: http://review.cloudera.org/r/1187/diff


Testing
-------


Thanks,

stack


;;;","08/Nov/10 19:52;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1187/#review1840
-----------------------------------------------------------


Going to continue review on v2 patch


trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
<http://review.cloudera.org/r/1187/#comment5962>

    whitespace (and we don't have IOException in javadoc but that's not your fault)



trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
<http://review.cloudera.org/r/1187/#comment5964>

    What does this mean?  done vs. not done?  I think we should be more descriptive in the logging (if done, then we've completed assignment of regions on cluster startup).  But if not done, on startup, what does this mean?  There's comment later that RIT timeouts should fix it up, so should be in log message here?  Or on startup case of bulk assign, should we fail startup here if this doesn't pass?



trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
<http://review.cloudera.org/r/1187/#comment5961>

    Can we move all this stuff into a separate class?  AssignmentManager is getting huge.  Maybe BulkAssign could be class that contains all these other class definitions?
    
    Also gives good opportunity in class comment to describe in general how this stuff works.


- Jonathan



;;;","08/Nov/10 20:03;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-11-08 11:50:55, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java, line 1143
bq.  > <http://review.cloudera.org/r/1187/diff/1/?file=16872#file16872line1143>
bq.  >
bq.  >     What does this mean?  done vs. not done?  I think we should be more descriptive in the logging (if done, then we've completed assignment of regions on cluster startup).  But if not done, on startup, what does this mean?  There's comment later that RIT timeouts should fix it up, so should be in log message here?  Or on startup case of bulk assign, should we fail startup here if this doesn't pass?

Removed it.  It confuses (see above for exhibit A).

If problem doing bulk assign, we'll crash out master.  Otherwise, timeout of RIT should fix bulk assign stragglers.


bq.  On 2010-11-08 11:50:55, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java, line 1149
bq.  > <http://review.cloudera.org/r/1187/diff/1/?file=16872#file16872line1149>
bq.  >
bq.  >     Can we move all this stuff into a separate class?  AssignmentManager is getting huge.  Maybe BulkAssign could be class that contains all these other class definitions?
bq.  >     
bq.  >     Also gives good opportunity in class comment to describe in general how this stuff works.

BulkAssigner class comment says -- perhaps a little curtly -- what it does?  I'll move it out.  The implementations though I'll leave beside where they are used -- in class.


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1187/#review1840
-----------------------------------------------------------



;;;","08/Nov/10 20:14;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1187/#review1841
-----------------------------------------------------------


Looks pretty good.  Should be much improved.

What about client-side semantics and checks?  I thought there was going to be some change there?  An async trigger and then a fast way to see if it's done or not?  Should be clear on javadoc since enable/disable is always thing we get questions on.

I think most of my issues with patch is that it adds huge amount of new code into already big AssignmentManager class.  Should move these classes out and not sure if we should have these methods which just touch ZK.  I think having all logic about the state transitions, usage of ZK, etc more explicitly controlled in the handlers themselves might be more clear to follow how this works.  Maybe not but was a little hard to follow (that, for example, we don't have enabling/disabling states in memory... where we were talking about in-memory state vs zk states, etc).

Also, ZKTable can be simplified (and made more future proof) with an enum.

Now, for failover, I know we said we would punt to 0.92 on handling of this case, but we should at least make it so we don't get into broken state if we have a master failover.  What will happen if we are in disabling up in zk but not all closes have been done?

Lastly... I think we definitely need some unit tests on this stuff.  TestMasterFailover does a little but not really relevant to these changes (only deals with regions already RIT).  TestRollingRestart does an enable/disable of table.  But none of this stuff takes into account failure of stuff in handlers, failure of RS, etc... Don't need to hold up this patch on unit tests if it's working in cluster testing on large tables, but it's kind of thing that would be good to test at some point.


trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
<http://review.cloudera.org/r/1187/#comment5966>

    I thought we had this method somewhere already?



trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
<http://review.cloudera.org/r/1187/#comment5967>

    nevermind, you just moved it :)



trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
<http://review.cloudera.org/r/1187/#comment5968>

    Whitespace, and should make note that this is checking ZK not in-memory state?



trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
<http://review.cloudera.org/r/1187/#comment5969>

    Are these methods necessary?  Seems like unnecessary stuff in AssignmentManager.  The Handlers can just use the ZK methods directly?  (keep the AM methods as check/set of it's internal state?)



trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java
<http://review.cloudera.org/r/1187/#comment5970>

    This is for repeated runs of enable?  Should we log if this actually removes regions (table has X total regions, already Y online, assigning Z still offline)?
    
    It's okay that this operation is not done under any locks?  Couldn't regions be coming online/offline concurrent with this operation?



trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java
<http://review.cloudera.org/r/1187/#comment5972>

    whitespace



trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java
<http://review.cloudera.org/r/1187/#comment5973>

    enum?



trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java
<http://review.cloudera.org/r/1187/#comment5975>

    yeah, instead of all these permutations on isEnabledOrDisabling and what not, should just use an enum and pass that in as argument to method which checks node state.



trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
<http://review.cloudera.org/r/1187/#comment5976>

    What is this method's policy on watches?  Please note it in javadoc.  This method is not at all strict and is multiple operations so is not safe to use on nodes where we rely on watches / monitoring of state transitions, so let's make some kind of note.


- Jonathan



;;;","09/Nov/10 00:07;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-11-08 12:13:11, Jonathan Gray wrote:
bq.  > Looks pretty good.  Should be much improved.
bq.  > 
bq.  > What about client-side semantics and checks?  I thought there was going to be some change there?  An async trigger and then a fast way to see if it's done or not?  Should be clear on javadoc since enable/disable is always thing we get questions on.
bq.  > 
bq.  > I think most of my issues with patch is that it adds huge amount of new code into already big AssignmentManager class.  Should move these classes out and not sure if we should have these methods which just touch ZK.  I think having all logic about the state transitions, usage of ZK, etc more explicitly controlled in the handlers themselves might be more clear to follow how this works.  Maybe not but was a little hard to follow (that, for example, we don't have enabling/disabling states in memory... where we were talking about in-memory state vs zk states, etc).
bq.  > 
bq.  > Also, ZKTable can be simplified (and made more future proof) with an enum.
bq.  > 
bq.  > Now, for failover, I know we said we would punt to 0.92 on handling of this case, but we should at least make it so we don't get into broken state if we have a master failover.  What will happen if we are in disabling up in zk but not all closes have been done?
bq.  > 
bq.  > Lastly... I think we definitely need some unit tests on this stuff.  TestMasterFailover does a little but not really relevant to these changes (only deals with regions already RIT).  TestRollingRestart does an enable/disable of table.  But none of this stuff takes into account failure of stuff in handlers, failure of RS, etc... Don't need to hold up this patch on unit tests if it's working in cluster testing on large tables, but it's kind of thing that would be good to test at some point.

Added to shell is_enabled and is_disabled.  Added javadoc to enableTable and disableTable explaining how these methods return immediately now but that process can take a while to complete.

On methods that just touch zk, they are of a class of which one member -- isTableDisabled -- does checks of AM data structures... so I can't really move them out.

Change ZKTable to use enums.

If disabling and master failover, when new master comes online, will be broke. But enable/disable are idempotent and a rerun of enable/disable will start up the process again and it should finish off properly.  Thats the theory anyways.

Regards unit tests, enable/disable is used all over unit test code base.  If you are looking for a test that simulates big table enable/disable, that's hard to do in a unit test.


bq.  On 2010-11-08 12:13:11, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java, line 1302
bq.  > <http://review.cloudera.org/r/1187/diff/2/?file=16884#file16884line1302>
bq.  >
bq.  >     I thought we had this method somewhere already?

there is a similarly named one for zk that you added -- is that what you are referring to?


bq.  On 2010-11-08 12:13:11, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java, line 1573
bq.  > <http://review.cloudera.org/r/1187/diff/2/?file=16884#file16884line1573>
bq.  >
bq.  >     Are these methods necessary?  Seems like unnecessary stuff in AssignmentManager.  The Handlers can just use the ZK methods directly?  (keep the AM methods as check/set of it's internal state?)

See note above where argument is that these are of a set and that since one of the set members does AM machinations, then all belong in AM.  For now I think its fine.


bq.  On 2010-11-08 12:13:11, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java, line 97
bq.  > <http://review.cloudera.org/r/1187/diff/2/?file=16888#file16888line97>
bq.  >
bq.  >     This is for repeated runs of enable?  Should we log if this actually removes regions (table has X total regions, already Y online, assigning Z still offline)?
bq.  >     
bq.  >     It's okay that this operation is not done under any locks?  Couldn't regions be coming online/offline concurrent with this operation?

Added logging to enable/disable.

Regards 'locks', these are my own local copies of these Lists.  Also notion that regions in table count can change is sort of allowed..  Each time through loop we'll recheck .META. for enabling and we'll re-look at the list of table regions in AssignmentManager....


bq.  On 2010-11-08 12:13:11, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java, line 41
bq.  > <http://review.cloudera.org/r/1187/diff/2/?file=16890#file16890line41>
bq.  >
bq.  >     enum?

done


bq.  On 2010-11-08 12:13:11, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java, line 716
bq.  > <http://review.cloudera.org/r/1187/diff/2/?file=16892#file16892line716>
bq.  >
bq.  >     What is this method's policy on watches?  Please note it in javadoc.  This method is not at all strict and is multiple operations so is not safe to use on nodes where we rely on watches / monitoring of state transitions, so let's make some kind of note.

K. This method is for non-watched znode.  Will add this to javadoc.


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1187/#review1841
-----------------------------------------------------------



;;;","09/Nov/10 00:11;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1187/
-----------------------------------------------------------

(Updated 2010-11-08 16:09:44.239145)


Review request for hbase and Jonathan Gray.


Changes
-------

Added shell is_enabled, is_disabled. Did some fixup because of issues revealed testing.  Implemented jgray suggestions.


Summary
-------

Renamed ZKTableDisable as ZKTable, making it a generic zk util for managing 'tables'.
Added enabing/disabling states to table the current set of enabled/disabled only.

M src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
  (createSetData): Added.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
  Removed offlining region utility methods no longer used.
  (We do it now over in MetaEditor)
M src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
  Javadoc.
M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  Add a base abstract class to do 'bulk assignments'.  Redo
  assignAllUserRegions to use subclass of new bulk assigner class.
  Added isTableEnabled, disablingTable, enablingTable.
M src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java
  Redid to use new bulk assigner class.
M src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java
A Added TestZKTable


This addresses bug hbase-3112.
    http://issues.apache.org/jira/browse/hbase-3112


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/master/BulkAssigner.java PRE-CREATION 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java PRE-CREATION 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTableDisable.java 1032652 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java 1032652 
  trunk/src/main/ruby/hbase/admin.rb 1032778 
  trunk/src/main/ruby/shell.rb 1032778 
  trunk/src/main/ruby/shell/commands/disable.rb 1032778 
  trunk/src/main/ruby/shell/commands/enable.rb 1032778 
  trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java 1032652 
  trunk/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java 1032652 
  trunk/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKTable.java PRE-CREATION 

Diff: http://review.cloudera.org/r/1187/diff


Testing
-------


Thanks,

stack


;;;","09/Nov/10 01:05;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1187/#review1853
-----------------------------------------------------------

Ship it!


I can live without the state-checking methods in ZKTable but it's fine, no big deal as is.

Only issue is that we're hitting ZK on each assign().

Otherwise, if tests are passing and it's working for you up on cluster, I'm +1 on this.

If you add async/sync support, let's be sure to nail the javadoc.


trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
<http://review.cloudera.org/r/1187/#comment6017>

    why describe 'enabling' state in our client API?  do we let out 'enabling' at all anywhere?
    
    and do we need to make mention that if it seems to never finish, this method should be retried?



trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
<http://review.cloudera.org/r/1187/#comment6019>

    same.  i think the javadoc is descriptive enough with this explanation of states.  just not sure if we need any javadoc about retrying?



trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
<http://review.cloudera.org/r/1187/#comment6022>

    isTableDisabling is actually a ZK read rather than checking in-memory state.  Should be move enabling/disabling to in-memory state as well?  I'm not sure it's a good idea to have to read from ZK on every assign() call, especially for something as rare as disabling.


- Jonathan



;;;","09/Nov/10 05:18;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-11-08 17:04:25, Jonathan Gray wrote:
bq.  > I can live without the state-checking methods in ZKTable but it's fine, no big deal as is.
bq.  > 
bq.  > Only issue is that we're hitting ZK on each assign().
bq.  > 
bq.  > Otherwise, if tests are passing and it's working for you up on cluster, I'm +1 on this.
bq.  > 
bq.  > If you add async/sync support, let's be sure to nail the javadoc.

I want the state-checking methods to flag bad state transitions.

On hitting zk on each assign, pardon me, I didn't grok what you were on about when you talked this up earlier.  I get it now.  Will fix.

Tests are failing because I changed sematics; disable/enable are now async where before they were sync.  Will do your suggestion of creating a sync version if only to make tests pass (should only be for use of tests).


bq.  On 2010-11-08 17:04:25, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java, line 447
bq.  > <http://review.cloudera.org/r/1187/diff/3/?file=16953#file16953line447>
bq.  >
bq.  >     why describe 'enabling' state in our client API?  do we let out 'enabling' at all anywhere?
bq.  >     
bq.  >     and do we need to make mention that if it seems to never finish, this method should be retried?

ok.  removed mention of states.


bq.  On 2010-11-08 17:04:25, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java, line 659
bq.  > <http://review.cloudera.org/r/1187/diff/3/?file=16955#file16955line659>
bq.  >
bq.  >     isTableDisabling is actually a ZK read rather than checking in-memory state.  Should be move enabling/disabling to in-memory state as well?  I'm not sure it's a good idea to have to read from ZK on every assign() call, especially for something as rare as disabling.

Will fix in next patch.


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1187/#review1853
-----------------------------------------------------------



;;;","09/Nov/10 06:36;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1187/
-----------------------------------------------------------

(Updated 2010-11-08 22:35:19.111130)


Review request for hbase and Jonathan Gray.


Changes
-------

Addressed Jon comments.   Still to do is finish up synchronizations on cache passed into ZKTable, cache of table states to save on trips to zk ensemble.   Also, need to run tests an put in place calls to synchronized enable/disable table methods that this patch adds so tests start passing again.  Will do in morning.


Summary
-------

Renamed ZKTableDisable as ZKTable, making it a generic zk util for managing 'tables'.
Added enabing/disabling states to table the current set of enabled/disabled only.

M src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
  (createSetData): Added.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
  Removed offlining region utility methods no longer used.
  (We do it now over in MetaEditor)
M src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
  Javadoc.
M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  Add a base abstract class to do 'bulk assignments'.  Redo
  assignAllUserRegions to use subclass of new bulk assigner class.
  Added isTableEnabled, disablingTable, enablingTable.
M src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java
  Redid to use new bulk assigner class.
M src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java
A Added TestZKTable


This addresses bug hbase-3112.
    http://issues.apache.org/jira/browse/hbase-3112


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/master/BulkAssigner.java PRE-CREATION 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java PRE-CREATION 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTableDisable.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java 1032841 
  trunk/src/main/ruby/hbase/admin.rb 1032841 
  trunk/src/main/ruby/shell.rb 1032841 
  trunk/src/main/ruby/shell/commands/disable.rb 1032841 
  trunk/src/main/ruby/shell/commands/enable.rb 1032841 
  trunk/src/main/ruby/shell/commands/is_disabled.rb PRE-CREATION 
  trunk/src/main/ruby/shell/commands/is_enabled.rb PRE-CREATION 
  trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java 1032841 
  trunk/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java 1032841 
  trunk/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKTable.java PRE-CREATION 

Diff: http://review.cloudera.org/r/1187/diff


Testing
-------


Thanks,

stack


;;;","09/Nov/10 17:53;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1187/
-----------------------------------------------------------

(Updated 2010-11-09 09:50:33.321899)


Review request for hbase and Jonathan Gray.


Changes
-------

Ok. All table state is now managed by ZKTable, moved out of AM.  It also keeps cache of states to save on zk ensemble roundtrips.  I think I've addressed all of your review comments now Jon.


Summary
-------

Renamed ZKTableDisable as ZKTable, making it a generic zk util for managing 'tables'.
Added enabing/disabling states to table the current set of enabled/disabled only.

M src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
  (createSetData): Added.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
  Removed offlining region utility methods no longer used.
  (We do it now over in MetaEditor)
M src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
  Javadoc.
M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  Add a base abstract class to do 'bulk assignments'.  Redo
  assignAllUserRegions to use subclass of new bulk assigner class.
  Added isTableEnabled, disablingTable, enablingTable.
M src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java
  Redid to use new bulk assigner class.
M src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java
A Added TestZKTable


This addresses bug hbase-3112.
    http://issues.apache.org/jira/browse/hbase-3112


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/master/BulkAssigner.java PRE-CREATION 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/ClosedRegionHandler.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/OpenedRegionHandler.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/TableEventHandler.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java PRE-CREATION 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTableDisable.java 1032841 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java 1032841 
  trunk/src/main/ruby/hbase/admin.rb 1032841 
  trunk/src/main/ruby/shell.rb 1032841 
  trunk/src/main/ruby/shell/commands/disable.rb 1032841 
  trunk/src/main/ruby/shell/commands/enable.rb 1032841 
  trunk/src/main/ruby/shell/commands/is_disabled.rb PRE-CREATION 
  trunk/src/main/ruby/shell/commands/is_enabled.rb PRE-CREATION 
  trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java 1032841 
  trunk/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java 1032841 
  trunk/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKTable.java PRE-CREATION 

Diff: http://review.cloudera.org/r/1187/diff


Testing
-------


Thanks,

stack


;;;","09/Nov/10 19:06;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1187/#review1866
-----------------------------------------------------------



trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
<http://review.cloudera.org/r/1187/#comment6054>

    I gave you my comments in person. Short version, I think that those methods' method shouldn't change and that we should have methods clearly marked as ""async"", and then do a job of educating people towards using them.



trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java
<http://review.cloudera.org/r/1187/#comment6052>

    Looks an awful lot like BulkDisabler



trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java
<http://review.cloudera.org/r/1187/#comment6053>

    A few methods need javadoc


- Jean-Daniel



;;;","09/Nov/10 20:24;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>


bq.  On 2010-11-09 11:05:46, Jean-Daniel Cryans wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java, line 444
bq.  > <http://review.cloudera.org/r/1187/diff/5/?file=17034#file17034line444>
bq.  >
bq.  >     I gave you my comments in person. Short version, I think that those methods' method shouldn't change and that we should have methods clearly marked as ""async"", and then do a job of educating people towards using them.

I meant method's behavior


- Jean-Daniel


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1187/#review1866
-----------------------------------------------------------



;;;","09/Nov/10 20:58;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-11-09 11:05:46, Jean-Daniel Cryans wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java, line 444
bq.  > <http://review.cloudera.org/r/1187/diff/5/?file=17034#file17034line444>
bq.  >
bq.  >     I gave you my comments in person. Short version, I think that those methods' method shouldn't change and that we should have methods clearly marked as ""async"", and then do a job of educating people towards using them.
bq.  
bq.  Jean-Daniel Cryans wrote:
bq.      I meant method's behavior

Yeah, I agree with you after chatting.  Will fix (And you spotted prob. w/ way async was running anyways).


bq.  On 2010-11-09 11:05:46, Jean-Daniel Cryans wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java, line 135
bq.  > <http://review.cloudera.org/r/1187/diff/5/?file=17043#file17043line135>
bq.  >
bq.  >     Looks an awful lot like BulkDisabler

I disagree.  The overrides each differ substantially (They look similar if you don't look close -- smile).


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1187/#review1866
-----------------------------------------------------------



;;;","09/Nov/10 21:29;stack;Here is what I committed.  Thanks for reviews Jon and J-D.;;;","09/Nov/10 21:33;stack;Resolving after commit.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestTableMapReduce broken up on hudson,HBASE-3111,12477313,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,14/Oct/10 05:57,20/Nov/15 12:42,14/Jul/23 06:06,15/Oct/10 20:11,,,,,,,,,,,,0.90.0,,,,,,,0,,,"So, test has failed for various reasons since fixed over last week or so.  It is currently failing when the reducer starts up.  Its failing because its not picking up the zk servers location; its using stale config.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/10 05:32;stack;3111-2nd-attempt.txt;https://issues.apache.org/jira/secure/attachment/12457235/3111-2nd-attempt.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26654,,,,,Fri Nov 20 12:42:38 UTC 2015,,,,,,,,,,"0|i0hksf:",100630,,,,,,,,,,,,,,,,,,,,,"14/Oct/10 05:59;stack;I'm going to try this change over night:

{code}
Index: src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java     (revision 1022380)
+++ src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java     (working copy)
@@ -180,7 +180,6 @@
     job.setOutputKeyClass(ImmutableBytesWritable.class);
     job.setOutputValueClass(Writable.class);
     if (partitioner == HRegionPartitioner.class) {
-      HBaseConfiguration.addHbaseResources(conf);
       job.setPartitionerClass(HRegionPartitioner.class);
       HTable outputTable = new HTable(conf, table);
       int regions = outputTable.getRegionsInfo().size();
{code}

My supposition is that its overwriting the config. we've passed in which has in it the amended zk location.

This is the last failing test on hudson.  If this passes, I'll make up a better patch... will need documentation.;;;","14/Oct/10 16:58;stack;This fix looks to have taken care of the failing.  Lets run hudson a few more times before closing.;;;","15/Oct/10 04:17;stack;So, the above 'fix' didn't work (looking closer, of course it didn't work).  We're still picking up wrong ensemble out in TableOutputFormat.  I can replicate the issue now by changing the hardcoded MiniZooKeeperCluster clientport underneath the deprecated HBaseClusterTestCase that is used by TestTableMapReduce.  Digging.;;;","15/Oct/10 05:32;stack;Problem looks to have been a little more insidious.  TableInputFormat implements Configurable.  TableOutputFormat does not and hbase configs were hoisted up into TOF inside in the RecordWriter constructor by calling HBaseConfiguration.addHbaseResources(context);   This doesn't looks like it would ever pick up config. set by the job.  Let me try this patch against hudson.  Committing it now.  Here's a commit message.

{code}
M src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
  Changed mentions of 'quorum' to 'ensemble'.
M src/main/java/org/apache/hadoop/hbase/mapreduce/IdentityTableReducer.java
M src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormat.java
  Minor formatting.
M src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java
  Removed unused imports and minor formatting.
M src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
  Documented what the quorumAddress parameter is for.
  Removed an unnecessary looking HBaseConfiguration.addHbaseResources(conf);
  (and adjusted documentation of job to say no hbase config set by the
  reduce setup method).
M src/main/java/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.java
  Made this class implment Configurable.  Moved creation of table from
  RecordWrite constructor based off passed TaskAttemptContext instead
  into the new setConf method.  Added table and conf data members.
{code};;;","15/Oct/10 20:11;stack;We don't see this failure in the last few hudson runs.  Resolving.;;;","01/Feb/11 22:18;jdcryans;This commit broke the CopyTable job because TOF.setConf is called when the job gets created, thus replaces the ZK addresses from the source cluster with the target cluster. AK found the issue.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestReplicationSink failing in TRUNK up on Hudson,HBASE-3110,12477309,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,14/Oct/10 04:02,20/Nov/15 12:43,14/Jul/23 06:06,16/Oct/10 05:47,,,,,,,,,,,,0.90.0,,,,,,,0,,,,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/10 04:12;stack;3110.txt;https://issues.apache.org/jira/secure/attachment/12457122/3110.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26653,,,,,Fri Nov 20 12:43:42 UTC 2015,,,,,,,,,,"0|i0hks7:",100629,,,,,,,,,,,,,,,,,,,,,"14/Oct/10 04:12;stack;So, this test is failing because it decides of a sudden to connect to a different zk cluster, not the one started by this test.  When it does this it gets bad master -- a stale master address -- and so fails. 

It of a sudden connects to a stale ensemble because it resorts to using default location for zk ensemble.  Its using a connection that hasn't been updated with the actual location of the zk ensemble.  Because we're leaking a zk cluster in the tests somewhere  -- some test is starting it up but not shutting it down -- all subsequent tests fail to start an ensemble on the default and resort to unorthodox ports.

I'm looking for the culprit but haven't figured it yet.;;;","14/Oct/10 04:13;stack;I applied the patch to see if it fixes hudson.;;;","16/Oct/10 05:47;stack;Resovling.  This issue seems fixed now up on hudson.   Reopen if we see this again.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scan.toString fails to escape binary start/stop rows,HBASE-3105,12477171,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,,davelatham,davelatham,12/Oct/10 18:44,12/Jun/22 00:12,14/Jul/23 06:06,12/Oct/10 20:21,0.20.6,,,,,,,,,,,,,Client,,,,,0,,,"Scan.toString calls Bytes.toString(this.startRow)  - same for stopRow.
Prints out ugly binary stuff in the logs.  Replace it with Bytes.toStringBinary()",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/10 18:49;davelatham;3105-0.20.patch;https://issues.apache.org/jira/secure/attachment/12456993/3105-0.20.patch","12/Oct/10 18:49;davelatham;3105.patch;https://issues.apache.org/jira/secure/attachment/12456992/3105.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26652,Reviewed,,,,Tue Oct 12 20:21:14 UTC 2010,,,,,,,,,,"0|i0hkrb:",100625,,,,,,,,,,,,,,,,,,,,,"12/Oct/10 18:49;davelatham;Simple patches to use Bytes.toStringBinary in Scan.toString;;;","12/Oct/10 20:21;stack;Thanks for the patch Dave.   Removing 0.90 from 'Fix Version/s"".   Moving into 0.20.7.  Will commit if we do a 0.20.7.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Disable TestMultiClusters; it doesn't really test anything and HBaseTestingUtility needs more work for this to pass anyways",HBASE-3104,12477160,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,12/Oct/10 17:54,20/Nov/15 12:42,14/Jul/23 06:06,12/Oct/10 17:55,,,,,,,,,,,,0.90.0,,,,,,,0,,,,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26651,,,,,Fri Nov 20 12:42:01 UTC 2015,,,,,,,,,,"0|i0hkr3:",100624,,,,,,,,,,,,,,,,,,,,,"12/Oct/10 17:55;stack;Committed.

{code}
Index: src/test/java/org/apache/hadoop/hbase/TestMultiClusters.java
===================================================================
--- src/test/java/org/apache/hadoop/hbase/TestMultiClusters.java        (revision 1021800)
+++ src/test/java/org/apache/hadoop/hbase/TestMultiClusters.java        (working copy)
@@ -25,6 +25,7 @@
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.Ignore;
 import org.junit.Test;
 
 import static org.junit.Assert.assertEquals;
@@ -46,7 +47,10 @@
    * what we insert in one place doesn't end up in the other.
    * @throws Exception
    */
-  @Test (timeout=100000)
+  // Ignore this test.  HTU needs work so can have two clusters running in
+  // the one test.  Each HTU minicluster needs to run as a different user so
+  // the shutdown will run cleanly.  St.Ack 20101012
+  @Ignore @Test (timeout=100000)
   public void twoClusters() throws Exception{
     Configuration conf1 = HBaseConfiguration.create();
     // Different path for different clusters
{code};;;","15/Oct/10 20:53;stack;I just removed this class altogether.;;;","16/Oct/10 05:02;stack;Over in 3114, we readd the guts of what this test used to do only we add it into the TestHBaseTestingUtility as a test to prove the testing suite can put up multiple clusters in the one jvm.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin assembly doesn't include -tests or -source jars,HBASE-3101,12476938,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,tlipcon,tlipcon,tlipcon,10/Oct/10 00:35,20/Nov/15 12:41,14/Jul/23 06:06,18/Oct/10 18:31,0.90.0,,,,,,,,,,,0.90.0,,build,,,,,0,,,"Currently the bin assembly tries to include the hbase-VERSION-tests.jar but there's a typo ""test"" instead of ""tests"" in the assembly descriptor, so it doesn't do so. Also, I think we should probably ship the -sources jar even in the -bin assembly - it's useful for people to point their IDE at it to get API javadocs, etc.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/10 00:36;tlipcon;pom-fix.txt;https://issues.apache.org/jira/secure/attachment/12456784/pom-fix.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26650,Reviewed,,,,Fri Nov 20 12:41:29 UTC 2015,,,,,,,,,,"0|i0hkqn:",100622,,,,,,,,,,,,,,,,,,,,,"10/Oct/10 06:47;stack;Ok on sources jar but why include the test jar?  It adds nothing.  I'd say remove it (It used to be mildly useful because tests had PE but now we're all YCSBers, its of no value including it I believe).  If you agree +1 on including src but REMOVING the bad tests jar line from assembly.;;;","10/Oct/10 18:19;tlipcon;The reason for the test jar is that downstream projects sometimes want to use MiniHBaseCluster or HBaseTestingUtility. Sqoop, for example, does this:
http://github.com/cloudera/sqoop/blob/master/src/test/com/cloudera/sqoop/hbase/HBaseTestCase.java

Granted, it should ideally pull from Maven, but some people are non-mavenized.;;;","16/Oct/10 20:27;stack;ok  +1;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestMergeTable failing in TRUNK,HBASE-3100,12476932,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,09/Oct/10 19:13,20/Nov/15 12:40,14/Jul/23 06:06,15/Oct/10 20:49,,,,,,,,,,,,0.90.0,,,,,,,0,,,"This is an old crufty test that is failing in TRUNK.  We create regions -- user, root, and meta regions offline -- but then come along and disable them after starting up minicluster.  Confuses new master.  This test also does not emit logging up in hudson because its written against old-style testing... even has a subclass of the HBaseTestCase to do this crazy manual creation.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/10 17:04;stack;3100-part3.txt;https://issues.apache.org/jira/secure/attachment/12457083/3100-part3.txt","13/Oct/10 05:41;stack;3100-xtra-debug.txt;https://issues.apache.org/jira/secure/attachment/12457045/3100-xtra-debug.txt","09/Oct/10 19:22;stack;3100.txt;https://issues.apache.org/jira/secure/attachment/12456779/3100.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26649,,,,,Fri Nov 20 12:40:46 UTC 2015,,,,,,,,,,"0|i0hkqf:",100621,,,,,,,,,,,,,,,,,,,,,"09/Oct/10 19:22;stack;Here's a patch.  I rewrote TestMergeTable so it should now be intelligible.  Also added assertions that its actually doing a merge.  Patch also does some cleanup of old unused super class and a test that did nothing.

{code}
M src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
  Some fixup to support stepped start up of mini cluster; allow
  starting dfs cluster, then later put an hbase mini cluster on top.
  (startMiniHBaseCluster, createRootDir): Added.
D src/test/java/org/apache/hadoop/hbase/AbstractMergeTestBase.java
  Removed messy subclass of HBaseClusterTestCase used building
  up some specific loaded regions.  Replaced with utility added
  to HBaseTestingUtility and by methods added to specific test.
D src/test/java/org/apache/hadoop/hbase/util/TestMergeMeta.java
  Deleted test that did nothing -- test merging of .META. -- but
  the superclass was making user regions, not multiple instances
  of .META. -- which we don't support anyways currently.
M src/test/java/org/apache/hadoop/hbase/util/TestMergeTable.java
  Rewritten to use HBaseTestingUtility.  Also added assertions that
  it actually did successful merge (Were none previous).
M src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
  Added a new constructor.  Are the others redundant given I just
  added implementation of Abortable to HConnection interface (the
  implmementation of HConnection used implement it -- I've just
  moved it up into the Interface itself).
M src/main/java/org/apache/hadoop/hbase/util/HMerge.java
  Bit of minor cleanup refactoring.
M src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
  The HConnection Interface now implements Abortable.
M src/main/java/org/apache/hadoop/hbase/client/HConnection.java
  Extend Abortable (The implementation was implementing Abortable
  anyways).
{code};;;","09/Oct/10 19:24;stack;Committed patch.  Lets see if it works up on hudson.;;;","13/Oct/10 05:25;stack;So, this is the last failing test up on hudson.  I see a few failure types.  One is that the fat region splits which is not expected in the test.   Another failure I'm seeing is that Master runs OPENED region handler twice.  Have a patch for the first to stop it happening in future.  Will add extra debug to figure how the second comes about. ;;;","13/Oct/10 05:41;stack;Committed this fix for one hudson failure mode.  Also adds debug for another failure mode seen up on hudson (3100 not done yet).;;;","13/Oct/10 17:04;stack;Upping compaction threshold did not prevent splits.  A split went ahead because there were 'References'.  Trying another config.  Also, set an upper bound timer of 5 minutes on this test.  Turns out this test failed on hudson and it cause TestTableMapReduce to fail (hudson #1541) because it left a zk running.   TTMR put up its own zk on a different port but last task in the MR job suddenly finds this tests ZK instead of the TTMR one (looking into that too but committing this in meantime).;;;","15/Oct/10 20:49;stack;This test is now passing up on hudson.  Resolving.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"TestMetaReaderEditor is broken in TRUNK; hangs",HBASE-3098,12476911,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,stack,stack,09/Oct/10 03:44,20/Nov/15 12:41,14/Jul/23 06:06,16/Oct/10 05:38,,,,,,,,,,,,0.90.0,,,,,,,0,,,It gets stuck.  Lars F reported it today.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/10 06:04;stack;3098-part2-v2.txt;https://issues.apache.org/jira/secure/attachment/12456795/3098-part2-v2.txt","09/Oct/10 21:35;stack;3098-part2.txt;https://issues.apache.org/jira/secure/attachment/12456781/3098-part2.txt","09/Oct/10 03:46;stack;3098.txt;https://issues.apache.org/jira/secure/attachment/12456762/3098.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26647,,,,,Fri Nov 20 12:41:04 UTC 2015,,,,,,,,,,"0|i0hkq7:",100620,,,,,,,,,,,,,,,,,,,,,"09/Oct/10 03:46;stack;Here is a patch that seems to fix the hang.  We were failing update of meta because location we had for it was bad -- catch the particular IOE type and then wait on update of meta location before proceeding w/ edit.;;;","09/Oct/10 03:47;stack;I committed attached patch to see if it helps up on hudson.;;;","09/Oct/10 20:07;stack;Now this test and the TestTableMapReduce are hanging here:

{code}
""RS_OPEN_REGION-sv2borg180,45108,1286653922397-1"" daemon prio=10 tid=0x00000000421fb000 nid=0x7062 in Object.wait() [0x00007f04591d0000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00007f048e52a828> (a java.util.concurrent.atomic.AtomicBoolean)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:294)
        - locked <0x00007f048e52a828> (a java.util.concurrent.atomic.AtomicBoolean)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:329)
        at org.apache.hadoop.hbase.catalog.MetaEditor.updateRegionLocation(MetaEditor.java:146)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1250)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:146)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:150)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
{code}

CatalogTracker needs to be interruptible.;;;","09/Oct/10 21:35;stack;Patch to add stop to node trackers.  Testing it.  Not sure it works just yet.

{code}
M src/test/java/org/apache/hadoop/hbase/zookeeper/TestZooKeeperNodeTracker.java
  Test that we can interrupt a wait by calling stop
M src/test/java/org/apache/hadoop/hbase/catalog/TestCatalogTracker.java
  Test that stop will interrupt infinite waits.
M src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperNodeTracker.java
  (stop): Added
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
M src/main/java/org/apache/hadoop/hbase/master/HMaster.java
  Call stop of the catalog manager.
M src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
  Improve logging by adding name of region we're closing to all messages.
M src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
  (stop): Added.
M src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
M src/main/java/org/apache/hadoop/hbase/client/HConnection.java
  (getConfiguration): Added.
M src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
  Cleanup all uses of CatalogTracker when done with it.
{code};;;","10/Oct/10 06:04;stack;Here is a better patch.  I had the cleanup of CatalogTracker in wrong location. Also does better job over in OpenRegionHandler of cleanup removing from online regions on its way out.  Will commit this to see if it fixes the hang on TestMetaReader;;;","16/Oct/10 05:38;stack;This test is now passing up on TRUNK.  Closing.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Client needs to reconnect if it expires its zk session,HBASE-3095,12476892,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,stack,stack,08/Oct/10 20:02,20/Nov/15 12:44,14/Jul/23 06:06,04/Nov/10 19:39,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Clients use an HConnection down in their guts to connect to the hbase cluster.  Master-is-running and root-region-location are up in zk.   Setup of a new HConnection sets up a connection to ZooKeeper.  If the session with ZK expires for whatever reason -- in tests they would expire because zk ensemble was restarted across tests or we might expire because of a long GC, well, it'll be frustrating to users if we do not just try and resetup the zk connection.",,khemani,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26645,Reviewed,,,,Fri Nov 20 12:44:04 UTC 2015,,,,,,,,,,"0|i0hkpj:",100617,,,,,,,,,,,,,,,,,,,,,"08/Oct/10 20:03;stack;I'm putting this in 0.90 for now.  Its marked a major bug rather than critical or blocker so could get punted.  My sense though is that this is something that could become an irritant.  Lets see.  I'll let the issue stew some before going about fixing.;;;","13/Oct/10 15:34;stack;It looks like Benoit fixed this for the old-school ZooKeeperWrapper.  His fix should work for the new stuff.  Its not there currently.  Bring it in.  See HBASE-2849.;;;","04/Nov/10 18:50;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1167/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Changes ZKW to send the SessionExpiredException in the abort call so that clients can intercept it and reconnect to ZooKeeper by first clearing up their state and establishing a new connection.


This addresses bug HBASE-3095.
    http://issues.apache.org/jira/browse/HBASE-3095


Diffs
-----

  /trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java 1030759 
  /trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java 1030759 
  /trunk/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java 1030759 

Diff: http://review.cloudera.org/r/1167/diff


Testing
-------

Adapted the client session expiration unit test.


Thanks,

Jean-Daniel


;;;","04/Nov/10 19:32;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1167/#review1804
-----------------------------------------------------------

Ship it!


Excellent.  Nice and clean.


/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java
<http://review.cloudera.org/r/1167/#comment5868>

    Why a new exception?  Why not pass the one that got us here? (Or, maybe there isn't one? Or you can't get at it)


- stack



;;;","04/Nov/10 19:36;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>


bq.  On 2010-11-04 12:30:24, stack wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java, line 326
bq.  > <http://review.cloudera.org/r/1167/diff/1/?file=16666#file16666line326>
bq.  >
bq.  >     Why a new exception?  Why not pass the one that got us here? (Or, maybe there isn't one? Or you can't get at it)

Yeah there's no exception leading us there, it's the ZK event. Will add more documentation tho.


- Jean-Daniel


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1167/#review1804
-----------------------------------------------------------



;;;","04/Nov/10 19:39;jdcryans;Committed to trunk the patch I posted on RB plus those comments on top of ZKW.connectionEvent:

{noformat}
* If Disconnected or Expired, this should shutdown the cluster. But, since
* we send a KeeperException.SessionExpiredException along with the abort
* call, it's possible for the Abortable to catch it and try to create a new
* session with ZooKeeper. This is what the client does in HCM.
{noformat};;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixes for miscellaneous broken tests,HBASE-3094,12476891,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,08/Oct/10 19:37,20/Nov/15 12:41,14/Jul/23 06:06,10/Oct/10 21:00,,,,,,,,,,,,0.90.0,,,,,,,0,,,"So, tests have been failing so long with in particular, some tests running w/o finishing hiding behind them tests that have been broke for ages.  Broken tests has let a raft of brokenness to creep in.  This issue is a grab back of fixes for all those failing up on hudson.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/10 19:43;stack;3094.txt;https://issues.apache.org/jira/secure/attachment/12456721/3094.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26644,,,,,Fri Nov 20 12:41:09 UTC 2015,,,,,,,,,,"0|i0hkpb:",100616,,,,,,,,,,,,,,,,,,,,,"08/Oct/10 19:43;stack;Here is a patch

{code}
M src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
  Added passing a Configuration to createTable.
M src/test/java/org/apache/hadoop/hbase/HBaseClusterTestCase.java
  Make a new configuration each time we create a cluster else this
  old testing utility fails when we do restart in middle of a test
  suite using this old stuff.
M src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
  Fixed imports.
M src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
  Added catch for other than IOE... shutdown if anything thrown.
M src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
  This test was for sure horked.  Fixed it so we were testing
  the javadoc description of how its supposed to work.
M src/test/java/org/apache/hadoop/hbase/master/TestDeadServer.java
  Another test that just wasn't working -- couldn't have worked on
  commit.
M src/test/java/org/apache/hadoop/hbase/master/TestLoadBalancer.java
  Must have resolvable host (How did this test ever work)?
M src/test/java/org/apache/hadoop/hbase/client/TestGetRowVersions.java
   Fixup to make this test pass.  New Configuration after cluster restart.
M src/test/java/org/apache/hadoop/hbase/client/TestScannerTimeout.java
  Fixup to make this test pass.  The low scanner timeout was stopping
  this test from even starting up.
M src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
  Use new method from HTU.
M src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
  If passed server is null, we are in test mode.
M src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
  Catch other dropped connection exception types when trying to prove
  we have a good root/meta address.
{code}
;;;","08/Oct/10 19:45;stack;Committed this patch.  Lets see how it does up on hudson.;;;","10/Oct/10 21:00;stack;These tests are now passing... resolving.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix TestKillingServersFromMaster in TRUNK; it just hangs since new master went in",HBASE-3091,12476829,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,stack,stack,08/Oct/10 05:48,20/Nov/15 12:42,14/Jul/23 06:06,28/Oct/10 03:52,,,,,,,,,,,,0.90.0,,,,,,,0,,,"This test depends on mechanisms much changed after new master went in so it just hangs; its not getting the confirmations it used expect.  This issue is about recasting the test -- because what it tests is useful (just reading the test I found bug in new master) -- so need to get it going again.  Marking critiical for 0.90.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/10 03:51;stack;3091.txt;https://issues.apache.org/jira/secure/attachment/12458230/3091.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26642,,,,,Fri Nov 20 12:42:51 UTC 2015,,,,,,,,,,"0|i0hkov:",100614,,,,,,,,,,,,,,,,,,,,,"08/Oct/10 05:49;stack;For now I renamed src/test/java/org/apache/hadoop/hbase/master/TestKillingServersFromMaster.java as src/test/java/org/apache/hadoop/hbase/master/BROKE_FIX_TestKillingServersFromMaster.java;;;","28/Oct/10 03:51;stack;Reenable of test.  1 of the 3 didn't make sense any more.  Reenabling found a few issues around shutdown.... small stuff but still....

{code}
A src/test/java/org/apache/hadoop/hbase/master/TestKillingServersFromMaster.java
  Rename of tests file we'd put aside.  Also removed @Ignore from two of the
  three tests this class used contain. I left @Ignore on 3rd test because
  behavior has changed regards what happens when an 'unknown server'.  I added
  note to the test explaining why its left @Ignored
D src/test/java/org/apache/hadoop/hbase/master/BROKE_FIX_TestKillingServersFromMaster.java
  Rename ... removed BROKE_FIX_ prefix.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
  I saw us go into wait on root after server hosting root went down.  Added
  a timeout waiting on root.  Check stop if we timeout w/o finding root.
  Also, we weren't doing right thing if we got a YouAreDeadException.  Fix.
M src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
  Remove unused imports.
M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  Don't assign if server is stopped.
M src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java
  Don't process server shutdown if stopped.
M src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
  Add new isRootAvailable.
{code};;;","28/Oct/10 03:52;stack;Committed the test rename and reenable.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
REST tests are broken locally and up in hudson,HBASE-3089,12476809,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,07/Oct/10 22:19,20/Nov/15 12:41,14/Jul/23 06:06,10/Oct/10 20:54,,,,,,,,,,,,0.90.0,,,,,,,0,,,The RESTServlet is a singleton.  In our rest tests the singleton is carried over between tests that start a new mini cluster each time.  A while back we added cleanup of zk when we let go of a connection... Whats happen is that the singleton is using a cleaned up zookeeperwatcher,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/10 22:55;stack;3089.txt;https://issues.apache.org/jira/secure/attachment/12456645/3089.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26641,,,,,Fri Nov 20 12:41:04 UTC 2015,,,,,,,,,,"0|i0hkof:",100612,,,,,,,,,,,,,,,,,,,,,"07/Oct/10 22:55;stack;I applied this patch that does a bit of cleanup on RESTServlet setting members private and final and then also adding a 'stop' that removes the singleton instance (we'll just make another next time through).  Rest tests pass now.  Lets see how it does up on hudson.;;;","10/Oct/10 20:54;stack;REST tests  haven't failed since this patch went in up on hudson.  Resolving.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestAvroServer and TestThriftServer broken because use same table in all tests and tests enable/disable/delete,HBASE-3088,12476790,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,07/Oct/10 18:10,20/Nov/15 12:41,14/Jul/23 06:06,16/Oct/10 05:55,,,,,,,,,,,,0.90.0,,,,,,,0,,,"There is dross left up in zk when you disable a table so if you go create same table name in new test, it'll fail to enable because zk says table disabled.  Clean up any table mentions in memory and up in zk on delete of a table (as well as all the other stuff we do on table delete).",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/10 18:11;stack;3088.txt;https://issues.apache.org/jira/secure/attachment/12456615/3088.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26640,,,,,Fri Nov 20 12:41:06 UTC 2015,,,,,,,,,,"0|i0hko7:",100611,,,,,,,,,,,,,,,,,,,,,"07/Oct/10 18:11;stack;Here is something to try against hudson.

It adds to the DeleteTableHandler a clean up of table references in zk and in assingmentmanager.;;;","16/Oct/10 05:55;stack;These tests are now passing up on hudson.  Resolving.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestSchemaResource broken on TRUNK up on HUDSON,HBASE-3085,12475908,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,06/Oct/10 00:01,20/Nov/15 12:40,14/Jul/23 06:06,10/Oct/10 20:55,,,,,,,,,,,,0.90.0,,,,,,,0,,,,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/10 00:04;stack;3085.txt;https://issues.apache.org/jira/secure/attachment/12456454/3085.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26637,,,,,Fri Nov 20 12:40:50 UTC 2015,,,,,,,,,,"0|i0hknr:",100609,,,,,,,,,,,,,,,,,,,,,"06/Oct/10 00:04;stack;Here's what I applied to see if it fixes this broke test.

I added to AssignmentManager and to EnableTableHandler/DisableTableHandler checks to see if table already in state we are tranistioning too.  This is hack to get tests to work.  Already have an issue to go about doing this stuff properly (e.g. have a state in zk that is disabling for a table and then when last region closes, move state to close).

Also added to HLog main facility to pass a file that is up in hdfs while in here.;;;","10/Oct/10 20:55;stack;This test is now passing up on hudson.  Resolving.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log Splitting & Replay: Distinguish between Network IOE and Parsing IOE,HBASE-3081,12475878,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,nspiegelberg,nspiegelberg,nspiegelberg,05/Oct/10 18:54,12/Oct/12 06:17,14/Jul/23 06:06,16/Oct/10 05:36,0.89.20100924,0.90.0,,,,,,,,,,0.89.20100924,0.90.0,io,master,regionserver,Replication,,0,,,"Originally, if HBase got an IOE from HDFS while splitting or opening a region, it would abort the operation. The assumption being that this is a network failure that will likely disappear at a later time or different partition of the network. However, if HBase gets parsing exceptions, we want to log the problem and continue opening/splitting the region anyways, because parsing is an idempotent problem and retries won't fix this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2933,,,,,,,HADOOP-6986,,,,,,"05/Oct/10 21:03;nspiegelberg;HBASE-3081_89.patch;https://issues.apache.org/jira/secure/attachment/12456437/HBASE-3081_89.patch","05/Oct/10 21:03;nspiegelberg;HBASE-3081_90.patch;https://issues.apache.org/jira/secure/attachment/12456438/HBASE-3081_90.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26635,Reviewed,,,,Sat Oct 16 05:36:23 UTC 2010,,,,,,,,,,"0|i08sb3:",49186,,,,,,,,,,,,,,,,,,,,,"05/Oct/10 18:56;nspiegelberg;this jira fixes all the edge cases that HBase-2933 could not address without an HDFS patch;;;","08/Oct/10 22:58;stack;Sorry Nicolas, I seem to have let this patch rot.  It won't apply to HRegion and code seems well different where your snippet would go in.  Mind taking a look?;;;","08/Oct/10 23:17;nspiegelberg;@Stack: have you applied HBASE-2933 first?  This issue is dependent on that jira being checked in.;;;","16/Oct/10 05:36;stack;Thanks for the patch Nicolas.  Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestAdmin hanging on hudson,HBASE-3080,12475836,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,05/Oct/10 04:59,20/Nov/15 12:41,14/Jul/23 06:06,10/Oct/10 20:56,,,,,,,,,,,,0.90.0,,,,,,,0,,,TestAdmin is hanging in the enable/disable exercising code.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/10 05:03;stack;3080.txt;https://issues.apache.org/jira/secure/attachment/12456362/3080.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26634,,,,,Fri Nov 20 12:41:52 UTC 2015,,,,,,,,,,"0|i0hkn3:",100606,,,,,,,,,,,,,,,,,,,,,"05/Oct/10 05:03;stack;Patch that after enable or disable waits on all regions from the table to clear regions-in-transition (no need of timer since regions-in-transition already has a timer) and no need to protect enable from running on top of disable since they are only run by the one handler.;;;","05/Oct/10 05:03;stack;Committed to see if passes up on hudson.  Test passes locally with this patch.;;;","10/Oct/10 20:56;stack;Resolving.  Passes up on hudson now.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shell displaying uninformative exceptions,HBASE-3079,12475789,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,streamy,streamy,04/Oct/10 18:40,12/Oct/12 06:17,14/Jul/23 06:06,11/Nov/10 16:16,0.89.20100924,,,,,,,,,,,0.90.0,,shell,,,,,0,,,"The shell seems to hang longer than the normal client and then display uninformative messages when doing wrong things.

For example, inserting into a non-existing family or reading from a disabled table.  I believe in both these cases, HTable will throw informative exceptions like InvalidFamilyException and TableDisabledException (need to confirm exactly what it does).

But in the shell, I get things like:

Inserting to a family that does not exist:
{noformat}
ERROR: org.apache.hadoop.hbase.client.RetriesExhaustedException: Still had 1 puts left after retrying 7 times.
{noformat}

Reading from a disabled table (this takes a long time before anything is displayed):
{noformat}

ERROR: org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 10.0.0.4:62505 for region sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a., row '', but failed after 7 attempts.
Exceptions:
org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2221)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1812)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:557)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1007)

org.apache.hadoop.hbase.client.RegionOfflineException: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a.
org.apache.hadoop.hbase.client.RegionOfflineException: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a.
org.apache.hadoop.hbase.client.RegionOfflineException: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a.
org.apache.hadoop.hbase.client.RegionOfflineException: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a.
org.apache.hadoop.hbase.client.RegionOfflineException: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a.
org.apache.hadoop.hbase.client.RegionOfflineException: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a.
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26633,Reviewed,,,,Thu Nov 11 16:16:03 UTC 2010,,,,,,,,,,"0|i08sfb:",49205,,,,,,,,,,,,,,,,,,,,,"31/Oct/10 06:32;stack;This is way wrong (I'm trying to add a row on a column family that doesn't exist):

{code}
hbase(main):002:0> describe 'a'
DESCRIPTION                                                                                                                                                                ENABLED                                                                                       
 {NAME => 'a', FAMILIES => [{NAME => 'b', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '6553 true                                                                                          
 6', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}                                                                                                                                                                                                                       
1 row(s) in 0.3880 seconds

hbase(main):003:0> put 'a', 'a', 'c:c', 'd'




ERROR: java.io.IOException: java.io.IOException: java.io.IOException: java.lang.AssertionError
        at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:1425)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1366)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:2415)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:561)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1025)
{code}

;;;","02/Nov/10 05:59;stack;When I go to count a disabled table I get:

{code}
hbase(main):012:0> count 'usertable'

ERROR: java.lang.reflect.UndeclaredThrowableException: null

Here is some help for this command:
          Count the number of rows in a table. This operation may take a LONG
          time (Run '$HADOOP_HOME/bin/hadoop jar hbase.jar rowcount' to run a
          counting mapreduce job). Current count is shown every 1000 rows by
          default. Count interval may be optionally specified. Scan caching
          is enabled on count scans by default. Default cache size is 10 rows.
          If your rows are small in size, you may want to increase this
          parameter. Examples:

          hbase> count 't1'
          hbase> count 't1', INTERVAL => 100000
          hbase> count 't1', CACHE => 1000
          hbase> count 't1', INTERVAL => 10, CACHE => 1000
{code};;;","10/Nov/10 02:03;ryanobjc;the changes in HBASE-2898 will substantially improve these error messages.  Here is an example from a unit test:


2010-11-09 17:59:56,093 DEBUG [main] client.TestMultiParallel(162): org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: NoSuchColumnFamilyException: 1 time, servers with issues: localhost:50070, 

We could beef up the exception text a bit, but I think this is a good step forward.

The UndeclaredThrowableException is kind of annoying, we can probably unwrap it a bit in jruby.;;;","10/Nov/10 02:05;streamy;I'm not following the connection of this jira to HBASE-2898.  This jira is just about the shell not properly displaying the exceptions we already have?;;;","10/Nov/10 02:17;ryanobjc;this jira is also about lame exceptions like:


ERROR: org.apache.hadoop.hbase.client.RetriesExhaustedException: Still had 1 puts left after retrying 7 times.


When the REAL problem was a no such family exception.

There is no amount of shell hackery to fix a broken exception. HBASE-2898 fixes those exceptions, which fixes the output, which helps address this issue (in part).;;;","10/Nov/10 02:26;streamy;makes sense, thanks.  there's still an issue in the shell though, right?;;;","10/Nov/10 03:25;ryanobjc;things seem to be much better in the latest, although we don't say 'the table is offline' when we run into problems:

hbase(main):006:0> put 'test', 'row', 'bad:jojo', 'value'

ERROR: org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: NoSuchColumnFamilyException: 1 time, servers with issues: sv4borg234:60020, 

hbase(main):007:0> disable 'test'
0 row(s) in 0.0330 seconds

hbase(main):008:0> put 'test', 'row', 'bad:jojo', 'value'

ERROR: org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: NotServingRegionException: 1 time, servers with issues: sv4borg234:60020, 

hbase(main):010:0> count 'test'

ERROR: org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server sv4borg234:60020 for region test,,1289359289116.7798c2aa758a7f8b855819cacafd0180., row '', but failed after 7 attempts.
Exceptions:
org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: test,,1289359289116.7798c2aa758a7f8b855819cacafd0180.
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2190)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1670)
        at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1036)

org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: test,,1289359289116.7798c2aa758a7f8b855819cacafd0180.
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2190)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1670)
        at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1036)

org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: test,,1289359289116.7798c2aa758a7f8b855819cacafd0180.
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2190)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1670)
        at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1036)

org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: test,,1289359289116.7798c2aa758a7f8b855819cacafd0180.
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2190)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1670)
        at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1036)

org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: test,,1289359289116.7798c2aa758a7f8b855819cacafd0180.
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2190)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1670)
        at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1036)

org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: test,,1289359289116.7798c2aa758a7f8b855819cacafd0180.
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2190)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1670)
        at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1036)

org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: test,,1289359289116.7798c2aa758a7f8b855819cacafd0180.
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2190)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1670)
        at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1036)



We could probably wrap some exception reporting and check to see if the table is online or not.  Seems like a common use case people run into;;;","10/Nov/10 21:56;stack;I tried this on cluster.  For disabled table it does right thing.  For put to non-existent family, I ran into NPE... after hacking around that I go this:

{code}
10/11/10 21:54:05 DEBUG client.MetaScanner: Scanning .META. starting at row=usertable,user992934061,00000000000000 for max=10 rows
10/11/10 21:54:05 INFO client.HConnectionManager$HConnectionImplementation: Removed usertable,user992934061,1288992609496.686d4b2520f56c32877d51bcc08b355a. for tableName=usertable from cache because of x

ERROR: org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: servers with issues: sv2borg187:60020,
Backtrace: org/apache/hadoop/hbase/client/HConnectionManager.java:1218:in `processBatch'
           org/apache/hadoop/hbase/client/HConnectionManager.java:1232:in `processBatchOfPuts'
           org/apache/hadoop/hbase/client/HTable.java:819:in `flushCommits'
           org/apache/hadoop/hbase/client/HTable.java:675:in `doPut'
           org/apache/hadoop/hbase/client/HTable.java:660:in `put'
           sun/reflect/NativeMethodAccessorImpl.java:-2:in `invoke0'
           sun/reflect/NativeMethodAccessorImpl.java:39:in `invoke'
           sun/reflect/DelegatingMethodAccessorImpl.java:25:in `invoke'
...
{code}

Over on the regionserver I see this:

{code}
2010-11-10 21:52:35,977 WARN org.apache.hadoop.hbase.regionserver.HRegion: No such column family in batch put
org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column family x does not exist in region usertable,user992934061,1288992609496.686d4b2520f56c32877d51bcc08b355a. in table {NAME => 'usertable', FAMILIES => [{NAME => 'values', BLOOMFILTER => 'NONE',        REPLICATION_SCOPE => '0', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}
    at org.apache.hadoop.hbase.regionserver.HRegion.checkFamily(HRegion.java:3126)
    at org.apache.hadoop.hbase.regionserver.HRegion.checkFamilies(HRegion.java:1705)
    at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:1400)
    at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1365)
...

{code}

Ryan is on it!;;;","11/Nov/10 06:11;stack;@Ryan.... so doesn't look like it was a deploy issue on my part.  Let me explain.

Here is the change I made just now (after we last talkd):

{code}
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
index 1cd7f62..5a17df2 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
@@ -2451,10 +2451,13 @@ public class HRegionServer implements HRegionInterface, HBaseRPCErrorHandler,
 
           this.requestCount.addAndGet(puts.size());
 
+            LOG.info(""CODE BEFORE lENGTH "");
           OperationStatusCode[] codes =
               region.put(putsWithLocks.toArray(new Pair[]{}));
 
+            LOG.info(""CODE lENGTH "" + codes.length);
           for( int i = 0 ; i < codes.length ; i++) {
+            LOG.info(""CODE "" + codes[i]);
             OperationStatusCode code = codes[i];
 
             Action theAction = puts.get(i);
{code}

The important addition is ""+            LOG.info(""CODE BEFORE lENGTH "");""

Now if I look in logs I see:

{code}
2010-11-11 06:00:59,160 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: CODE BEFORE lENGTH
2010-11-11 06:00:59,160 WARN org.apache.hadoop.hbase.regionserver.HRegion: No such column family in batch put
org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column family x does not exist in region usertable,user992934061,1288992609496.686d4b2520f56c32877d51bcc08b355a. in table {NAME => 'usertable', FAMILIES => [{NA}
        at org.apache.hadoop.hbase.regionserver.HRegion.checkFamily(HRegion.java:3126)
        at org.apache.hadoop.hbase.regionserver.HRegion.checkFamilies(HRegion.java:1705)
...
{code}

Error is coming up out of 

{code}
2454             LOG.info(""CODE BEFORE lENGTH "");
2455           OperationStatusCode[] codes =
2456               region.put(putsWithLocks.toArray(new Pair[]{}));
{code}

Looking down in minibatch...  why doesn't this assert go off? (We're only writing one row...

{code}
1423       // We've now grabbed as many puts off the list as we can
1424       assert numReadyToWrite > 0;
{code}


I'm running w/ asserts enabled... export HBASE_OPTS=""-ea -X...;;;","11/Nov/10 15:39;stack;Here from Ryan:

{code}
Turning on RPC level debug we see in the logs now:

2010-11-10 23:47:32,507 DEBUG org.apache.hadoop.ipc.HBaseServer: IPC
Server handler 7 on 60020, call
multi(org.apache.hadoop.hbase.client.MultiAction@20ec6bb1) from
10.10.20.235:55930: error: java.io.IOException:
java.lang.AssertionError
java.io.IOException: java.lang.AssertionError
       at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:1424)
       at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1365)
       at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:2454)
       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
       at java.lang.reflect.Method.invoke(Method.java:597)
       at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
       at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1036)
2010-11-10 23:47:32,508 DEBUG org.apache.hadoop.ipc.HBaseServer: IPC
Server Responder: responding to #50 from 10.10.20.235:55930
2010-11-10 23:47:32,508 DEBUG org.apache.hadoop.ipc.HBaseServer: IPC
Server Responder: responding to #50 from 10.10.20.235:55930 Wrote 759
bytes.

we could do this so we it works with -ea:

diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index d76bf63..9d8d85d 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -1420,7 +1420,10 @@ public class HRegion implements HeapSize { // , Writable{
        lastIndexExclusive++;
        numReadyToWrite++;
      }
+      if (numReadyToWrite == 0)
+          return 0L; // no size change.
      // We've now grabbed as many puts off the list as we can
+
      assert numReadyToWrite > 0;

      // ------------------------------------

{code};;;","11/Nov/10 16:09;stack;I just tried the above and it works for me now:

{code}
hbase(main):004:0> put 'usertable', 'x', 'x:x', 'x'

ERROR: org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: NoSuchColumnFamilyException: 1 time, servers with issues: sv2borg183:60020, 
{code}

I'm going to commit the above change (I removed the assert checking for 0 since it won't ever be after this change goes in).;;;","11/Nov/10 16:16;stack;Resolving.  Lets open new issues for other shell probs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Zookeeper test failing on hudson,HBASE-3074,12475696,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,02/Oct/10 23:45,20/Nov/15 12:43,14/Jul/23 06:06,10/Oct/10 20:52,,,,,,,,,,,,0.90.0,,,,,,,0,,,"We're failing here because .META. moved:

{code}
retrying after sleep of 5000 because: Connection refused
2010-10-02 00:50:49,728 DEBUG [main] client.HConnectionManager$HConnectionImplementation(717): locateRegionInMeta attempt 2 of 4 failed; retrying after sleep of 5000 because: Connection refused
2010-10-02 00:50:59,730 DEBUG [main] client.HConnectionManager$HConnectionImplementation(717): locateRegionInMeta attempt 0 of 4 failed; retrying after sleep of 5000 because: Connection refused
2010-10-02 00:51:04,731 DEBUG [main] client.HConnectionManager$HConnectionImplementation(717): locateRegionInMeta attempt 1 of 4 failed; retrying after sleep of 5000 because: Connection refused
2010-10-02 00:51:09,732 DEBUG [main] client.HConnectionManager$HConnectionImplementation(717): locateRegionInMeta attempt 2 of 4 failed; retrying after sleep of 5000 because: Connection refused
2010-10-02 00:51:14,734 WARN  [main] client.HConnectionManager$HConnectionImplementation(597): Encounted problems when prefetch META table: 
org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server vesta.apache.org:54172 for region .META.,,1, row 'test1285980613475,,99999999999999', but failed after 4 attempts.
Exceptions:
java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused

	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.java:946)
	at org.apache.hadoop.hbase.client.HTable.getRowOrBefore(HTable.java:500)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:104)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:594)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:645)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:539)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:507)
	at org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:287)
	at org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:207)
	at org.apache.hadoop.hbase.TestZooKeeper.testSanity(TestZooKeeper.java:140)
...

{code}

I'm not sure why we're not picking up new locations.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/10 23:47;stack;3074-xtra-debug.txt;https://issues.apache.org/jira/secure/attachment/12456217/3074-xtra-debug.txt","08/Oct/10 01:11;stack;3074.txt;https://issues.apache.org/jira/secure/attachment/12456653/3074.txt","08/Oct/10 06:02;stack;hbase-3074-part2.txt;https://issues.apache.org/jira/secure/attachment/12456671/hbase-3074-part2.txt","11/Oct/10 19:24;larsfrancke;lars-stacktrace.1.txt;https://issues.apache.org/jira/secure/attachment/12456884/lars-stacktrace.1.txt",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26629,,,,,Fri Nov 20 12:43:17 UTC 2015,,,,,,,,,,"0|i0hkmn:",100604,,,,,,,,,,,,,,,,,,,,,"02/Oct/10 23:47;stack;Committing this... adds detail to logging messages so can tell more about whats happening.;;;","08/Oct/10 01:11;stack;Tests were sharing an HCM.  First test was expiring it.  HCMs were proceeding though ZKW had expired; was just serving stale data.  On expiration, the HCM actually gets an abort but we do nothing but log currently.  Now we'll set an actually unused 'closed' flag.

The first TZ test was also doing nothing.  Changed it to assert connection closed.  Also corralled this session expiring test making sure the connection used herein will not be used elsewhere (by making a new Configuration instance).

I'm going to commit.  Hopefully fixes ZK up on hudson.  MIght break something else though, but probably something we want to break because its reliant on flakey stale data to run.;;;","08/Oct/10 01:14;stack;committed patch.  lets see how hudson likes it.;;;","08/Oct/10 06:02;stack;Here is part 2 of this fix where I generalize the fix I made for zk to see if it'll fix other failing tests.  In particular TestClusterRestart.

{code}
HConnections go stale if miniCluster is shutdown on them.  Make it
so we make new HConnections on other side of a new cluster spin up.
Was cause for failing of last few tests.

M src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
  So if you shutdown a cluster and you have HConnections that were
  made using the HTUs Configuration, they'll all be stale --
  session expired -- and won't be of any use connecting to new cluster
  that comes up... now that HConnection is first class dependent on
  its own ZooKeeperWatcher instance.
  So, I went through this class and fixed it up so we use new
  Configurations when could be called either side of a mini cluster
  restart.  Added note to getConfiguration to be careful what you
  do with it because Configuration makes for an HConnection.
M src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java
  Don't use same Configuration everywhere.
M src/test/java/org/apache/hadoop/hbase/master/TestRestartCluster.java
  Make new Configuration when we make new Cluster so we get fresh
  HConnection -- not one w/ stale data, disconnected.
M src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
  Check that new server that comes in on a regionServerReport
  doesn't have same port and host as one we have already registered
  as we do in reportForDuty call (This is preemtive bug fix).
M src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
  Save of an identifier for HCM and use it in log messages, particularly
  when we close -- helps debugging.
M src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
  getConfiguration added.
{code};;;","10/Oct/10 20:52;stack;The two patches here would seem to have resovled a few of the failures, TestZooKeeper in particular.  I have not seen it fail in a good while now up on hudson.  Resolving.;;;","11/Oct/10 19:24;larsfrancke;I have no idea if this has anything to do with the other issue from this jira but the test testClientSessionExpired fails for me about 1/3 of the time. I've attached a log output. The version that ran on was cosmetically modified (line numbers won't match up) but I just tried on a trunk build and had it fail as well.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"New APIs for Result, faster implementation for some calls",HBASE-3073,12475677,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,01/Oct/10 23:54,12/Oct/12 06:17,14/Jul/23 06:06,06/Oct/10 00:44,0.89.20100924,,,,,,,,,,,0.90.0,,,,,,,0,,,"Our existing API for Result hasn't been given much love in the last year.  In the mean time, inefficiencies in the existing implementation have come to light, causing issues with benchmarks.  Furthermore, some people are finding the API both difficult to use as well as not useful enough (See: HBASE-1937).

I propose the following new APIs:
public List<KeyValue> getColumn(byte [] family, byte [] qualifier);
public KeyValue getColumnLatest(byte [] family, byte [] qualifier);

The implementation of these use a binary search on the underlying kvs array (which is sorted).  I also have new implementations for
public boolean containsColumn(byte [] family, byte [] qualifier);
public byte [] getValue(byte [] family, byte [] qualifier);

Which in the small case run faster, but in the big case seem to run a bit slower.  That is if you call getValue() 10 times for a Result it will be faster with the new implementation, but if you call getValue() 100 times for the same Result it is faster using the old implementation.  My tests indicated about 10% slower on 'getValue' 100x with an overall 1000x iteration on 1000 different Result objects.  Considering most people use getValue() to retrieve named columns and iteration when the qualifier list is unknown I think this is a reasonable trade off.

Along with the new API, there is a recommendation to use raw() to get the list of KeyValue objects for iteration.  This increases the visibility of KeyValue, and also is much faster to iterate (4.9 times on my mini benchmark, 100 columns per Result, redone 1000 times on different Result objects).

Given my recent major speed boost by changing YCSB to use the raw() interface, I think that this is a must have for 0.90.  ",,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/10 00:18;ryanobjc;HBASE-3073.txt;https://issues.apache.org/jira/secure/attachment/12456171/HBASE-3073.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26628,,,,,Wed Oct 06 01:02:07 UTC 2010,,,,,,,,,,"0|i08sfj:",49206,,,,,,,,,,,,,,,,,,,,,"02/Oct/10 00:06;apurtell;+1 based on the YCSB improvement. ;;;","02/Oct/10 00:18;ryanobjc;updated patch removing some WIP stuff that doesnt make sense;;;","02/Oct/10 00:24;stack;On this patch, if it fixed HBASE-1937 and HBASE-2753 it'd help your case that the fix for this issue is 'vital'.


Whats this?

{code}
+  //private transient int size = 0; // size of underlying data (kv.getLength all added together)
{code}

Remove it?

For this change....

{code}
   /**
-   * Return the unsorted array of KeyValues backing this Result instance.
-   * @return unsorted array of KeyValues
+   * Return the array of KeyValues backing this Result instance.
+   * @return array of KeyValues
    */
{code]

... is the result actually sorted?  If so, javadoc should say so?

Why we sorting (see HBASE-2753)?

{code}
   public KeyValue[] sorted() {
-    if (isEmpty()) { // used for side effect!
-      return null;
-    }
+    raw(); // side effect of loading this.kvs
     if (!sorted) {
{code}
... 


Below .....
{code}
+   * @param family
+   * @param qualifier
+   * @return
+   */
{code}

fix the javadoc... fix the return at least.  It looks like if no such column you don't get null but an empty List.   Should note that.

Need to fix javadoc in these highly visible methods elsewhere in the patch too.

Do these new APIs replace others?  Are there equivs in the API already for these?  If so, deprecate the old and link in javadoc else new APIs to do same thing confuses.

Fix these names:


valueEx

and 

containsColumnEx

;;;","02/Oct/10 00:35;ryanobjc;this does indeed fix HBASE-1937, and HBASE-2753 is in progress under it's own commit.  At that time we can make raw/sorted do the same thing (just return 'kvs').

This fixes HBASE-1937 by introducing the getColumnLatest() which returns a KeyValue which the user can call getTimestamp() on.  Instead of creating Result API calls for every conceivable thing a user might want to do, let's just expose the KeyValue which has a rich API for doing all sorts of things, such as getting various fields (timestamp, qualifier, field, row, value) and comparison.

I need to do a bit more perf testing and then I will clean up those javadocs.  ;;;","05/Oct/10 22:36;ryanobjc;i beefed up the javadoc a lot.  I removed the 'ex' and just replaced the older implementations since my testing shows that the new code is faster or as fast as the old in all situations.;;;","05/Oct/10 22:51;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/963/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

New APIs for Result, faster implementation for some calls


This addresses bug HBASE-3073.
    http://issues.apache.org/jira/browse/HBASE-3073


Diffs
-----

  trunk/CHANGES.txt 1003710 
  trunk/src/main/java/org/apache/hadoop/hbase/client/Result.java 1003710 
  trunk/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java 1003709 
  trunk/src/test/java/org/apache/hadoop/hbase/client/TestResult.java PRE-CREATION 

Diff: http://review.cloudera.org/r/963/diff


Testing
-------


Thanks,

Ryan


;;;","05/Oct/10 23:04;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/963/#review1426
-----------------------------------------------------------



trunk/src/main/java/org/apache/hadoop/hbase/client/Result.java
<http://review.cloudera.org/r/963/#comment4980>

    i removed this in my svn client, it was remnant of my perf testing


- Ryan



;;;","05/Oct/10 23:32;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/963/#review1427
-----------------------------------------------------------


Just javadoc cleanup, otherwise this looks great.  As noted in comments, I think we should deprecate sorted(), recommend and @link to raw(), and ensure we aren't using sorted() internally (especially since it actually is less efficient w/ the assert).


trunk/src/main/java/org/apache/hadoop/hbase/client/Result.java
<http://review.cloudera.org/r/963/#comment4982>

    You might {@link to KV.Comparator so javadoc for that can show up on hover



trunk/src/main/java/org/apache/hadoop/hbase/client/Result.java
<http://review.cloudera.org/r/963/#comment4981>

    > 80



trunk/src/main/java/org/apache/hadoop/hbase/client/Result.java
<http://review.cloudera.org/r/963/#comment4983>

    Add @deprecated and then recommend using raw instead w/ an @link to it 



trunk/src/main/java/org/apache/hadoop/hbase/client/Result.java
<http://review.cloudera.org/r/963/#comment4984>

    Should be ""Returns the"" or ""Return the"" rather than 'me'.  Also >80 chars



trunk/src/main/java/org/apache/hadoop/hbase/client/Result.java
<http://review.cloudera.org/r/963/#comment4985>

    @link



trunk/src/main/java/org/apache/hadoop/hbase/client/Result.java
<http://review.cloudera.org/r/963/#comment4986>

    Should use raw()?



trunk/src/main/java/org/apache/hadoop/hbase/client/Result.java
<http://review.cloudera.org/r/963/#comment4987>

    >80



trunk/src/main/java/org/apache/hadoop/hbase/client/Result.java
<http://review.cloudera.org/r/963/#comment4988>

    Maybe ""Returns (or Gets) the KeyValue of the most recent version of the specified column""... and >80


- Jonathan



;;;","05/Oct/10 23:55;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/963/#review1428
-----------------------------------------------------------



trunk/src/main/java/org/apache/hadoop/hbase/client/Result.java
<http://review.cloudera.org/r/963/#comment4989>

    after https://issues.apache.org/jira/browse/HBASE-2753 is resolved, raw()===sort() and there wont be a difference.


- Ryan



;;;","05/Oct/10 23:59;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>


bq.  On 2010-10-05 16:25:58, Ryan Rawson wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/client/Result.java, line 202
bq.  > <http://review.cloudera.org/r/963/diff/1/?file=14002#file14002line202>
bq.  >
bq.  >     after https://issues.apache.org/jira/browse/HBASE-2753 is resolved, raw()===sort() and there wont be a difference.

Okay.  Just saying, weird to use a deprecated method internally but agree that it doesn't matter if you're planning to remove the assert.


- Jonathan


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/963/#review1428
-----------------------------------------------------------



;;;","06/Oct/10 00:44;ryanobjc;i committed this. thanks to all the reviewers!;;;","06/Oct/10 01:02;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>


bq.  On 2010-10-05 16:25:58, Ryan Rawson wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/client/Result.java, line 202
bq.  > <http://review.cloudera.org/r/963/diff/1/?file=14002#file14002line202>
bq.  >
bq.  >     after https://issues.apache.org/jira/browse/HBASE-2753 is resolved, raw()===sort() and there wont be a difference.
bq.  
bq.  Jonathan Gray wrote:
bq.      Okay.  Just saying, weird to use a deprecated method internally but agree that it doesn't matter if you're planning to remove the assert.

the code calls raw() now, hopefully our asserts dont trigger


- Ryan


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/963/#review1428
-----------------------------------------------------------



;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"IllegalStateException when new server comes online, is given 200 regions to open and 200th region gets timed out of regions in transition",HBASE-3068,12475663,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,01/Oct/10 20:51,20/Nov/15 12:41,14/Jul/23 06:06,01/Oct/10 23:02,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Yesterday we committed a change that makes it so the master will crash is a zk transition that is unexpected.   Its extreme but good for highlighting bad state changes (we also started marking these as illegalstateexceptions yesterday too).

So, testing new master I brought up a new server.  Balancer tried to give new server 256 regions.

{code}
2010-10-01 16:01:42,972 INFO org.apache.hadoop.hbase.master.LoadBalancer: Calculated a load balance in 0ms. Moving 256 regions off of 7 overloaded servers onto 1 less loaded servers
{code}

Turns out we failed complete open of all 256 servers within the regions-in-transition timeout period so we tried to reassign.  The master aborted because region was in the PENDING_OPEN state when we went about assigning.

{code}
2010-10-01 16:02:28,809 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  usertable,user1128734802,1285701924906.006696a9bf346f8593df66728e18e029. state=PENDING_OPEN, ts=1285948921051
2010-10-01 16:02:28,809 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_OPEN or OPENING for too long, reassigning region=usertable,user1128734802,1285701924906.006696a9bf346f8593df66728e18e029.
2010-10-01 16:02:28,811 FATAL org.apache.hadoop.hbase.master.HMaster: Unexpected state trying to OFFLINE; usertable,user1128734802,1285701924906.006696a9bf346f8593df66728e18e029. state=PENDING_OPEN, ts=1285948921051
java.lang.IllegalStateException
    at org.apache.hadoop.hbase.master.AssignmentManager.setOfflineInZooKeeper(AssignmentManager.java:662)
    at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:632)
    at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:560)
    at org.apache.hadoop.hbase.master.AssignmentManager$TimeoutMonitor.chore(AssignmentManager.java:1102)
    at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
{code}
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26626,,,,,Fri Nov 20 12:41:12 UTC 2015,,,,,,,,,,"0|i0hklz:",100601,,,,,,,,,,,,,,,,,,,,,"01/Oct/10 21:27;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/930/
-----------------------------------------------------------

(Updated 2010-10-01 14:05:45.726776)


Review request for hbase and Jonathan Gray.


Changes
-------

Update to javadoc and comments


Summary
-------

Fix is two-fold.

First, added new facility where on successful open, we go and update the timers on all regions in transition that were on the same server.

Secondly, in the timeout monitor, we'll do necessary cleanup and state transitions so that when we go into re-assign, we have the proper state

M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  Changed regionPlans to be a concurrentskiplist.  Makes life easier
  and in no place do we need lock on regionPlans to span other than
  regionPlans changes.
  Added to the processing of successful region open, the cleanup
  of its regionPlan and a run of updateTimers.
  Put setOffline in place of some code that duplicated what it did.


This addresses bug hbase-3068.
    http://issues.apache.org/jira/browse/hbase-3068


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1003330 

Diff: http://review.cloudera.org/r/930/diff


Testing
-------

Basic unit tests seem to be passing.  Testing now up on cluster.


Thanks,

stack


;;;","01/Oct/10 21:27;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/930/
-----------------------------------------------------------

Review request for hbase and Jonathan Gray.


Summary
-------

Fix is two-fold.

First, added new facility where on successful open, we go and update the timers on all regions in transition that were on the same server.

Secondly, in the timeout monitor, we'll do necessary cleanup and state transitions so that when we go into re-assign, we have the proper state

M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  Changed regionPlans to be a concurrentskiplist.  Makes life easier
  and in no place do we need lock on regionPlans to span other than
  regionPlans changes.
  Added to the processing of successful region open, the cleanup
  of its regionPlan and a run of updateTimers.
  Put setOffline in place of some code that duplicated what it did.


This addresses bug hbase-3068.
    http://issues.apache.org/jira/browse/hbase-3068


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1003330 

Diff: http://review.cloudera.org/r/930/diff


Testing
-------

Basic unit tests seem to be passing.  Testing now up on cluster.


Thanks,

stack


;;;","01/Oct/10 23:02;stack;Committing.  Tested it a couple of ways up on loaded cluster and no longer see the illegalstateexception nor does master crash;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We don't put the port for hregionserver up into znode since new master,HBASE-3066,12475655,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,01/Oct/10 19:21,20/Nov/15 12:43,14/Jul/23 06:06,01/Oct/10 20:44,,,,,,,,,,,,0.90.0,,,,,,,0,,,Found by jd,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/10 19:22;stack;hrs.txt;https://issues.apache.org/jira/secure/attachment/12456154/hrs.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26625,Reviewed,,,,Fri Nov 20 12:43:25 UTC 2015,,,,,,,,,,"0|i0hklr:",100600,,,,,,,,,,,,,,,,,,,,,"01/Oct/10 19:22;stack;Patch that seems to pass unit tests.  Review please.;;;","01/Oct/10 20:28;jdcryans;+1, fixes the issue I saw.;;;","01/Oct/10 20:44;stack;Committed.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Retry all 'retryable' zk operations; e.g. connection loss",HBASE-3065,12475630,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,liyin,stack,stack,01/Oct/10 15:51,20/Nov/15 12:43,14/Jul/23 06:06,03/Aug/11 02:06,,,,,,,,,,,,0.92.0,,,,,,,0,,,"The 'new' master refactored our zk code tidying up all zk accesses and coralling them behind nice zk utility classes.  One improvement was letting out all KeeperExceptions letting the client deal.  Thats good generally because in old days, we'd suppress important state zk changes in state.  But there is at least one case the new zk utility could handle for the application and thats the class of retryable KeeperExceptions.  The one that comes to mind is conection loss.  On connection loss we should retry the just-failed operation.  Usually the retry will just work.  At worse, on reconnect, we'll pick up the expired session event. 

Adding in this change shouldn't be too bad given the refactor of zk corralled all zk access into one or two classes only.

One thing to consider though is how much we should retry.  We could retry on a timer or we could retry for ever as long as the Stoppable interface is passed so if another thread has stopped or aborted the hosting service, we'll notice and give up trying.  Doing the latter is probably better than some kinda timeout.

HBASE-3062 adds a timed retry on the first zk operation.  This issue is about generalizing what is over there across all zk access.",,ghelmling,kannanm,khemani,larsfrancke,liyin,mahadev,qwertymaniac,srivas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-5281,,,,,,,,,,,,,"30/Apr/11 20:55;stack;3065-v3.txt;https://issues.apache.org/jira/secure/attachment/12477884/3065-v3.txt","28/Jul/11 06:25;stack;3065-v4.txt;https://issues.apache.org/jira/secure/attachment/12488068/3065-v4.txt","29/Jul/11 12:36;ram_krish;HBASE-3065-addendum.patch;https://issues.apache.org/jira/secure/attachment/12488200/HBASE-3065-addendum.patch","04/Apr/11 05:21;liyin;HBase-3065[r1088475]_1.patch;https://issues.apache.org/jira/secure/attachment/12475338/HBase-3065%5Br1088475%5D_1.patch","28/Apr/11 03:29;liyin;hbase3065_2.patch;https://issues.apache.org/jira/secure/attachment/12477605/hbase3065_2.patch",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26624,Reviewed,,,,Fri Nov 20 12:43:14 UTC 2015,,,,,,,,,,"0|i0hklj:",100599,Adds recovery of 'recoverable' zk operations.,,,,,,,,,,,,,,,,,,,,"01/Dec/10 00:52;streamy;I'd like to get to this for 0.92 at the latest.;;;","15/Feb/11 19:43;clehene;We started getting tons of {code}org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase{code} both in our MR jobs and in our ""unit"" tests that use the MiniHBaseCluster after upgrading from 0.89 to 0.90.



;;;","15/Feb/11 19:49;ryanobjc;can you check your ZK cluster health?  There is a link at the top of the master page called 'zk dump'.

We had a situation where 2/5 of our quorum members were not part of it, and you get error messages like that a lot. We changed the logging so it might be illustrating a deployment issue on your end.;;;","31/Mar/11 04:29;liyin;Most of retry are simple, except 2: create and setData. 
I got some basic idea of retry 'create' from http://wiki.apache.org/hadoop/ZooKeeper/ErrorHandling

But how to do the setData? 
The problem is the 1st setData may success and got a connectionloss exception after that.
Then it retries and got the badversion exception. 
How to make know that this badversion is caused by the result of previous correctly setData?
;;;","31/Mar/11 06:18;mahadev;liyin, 
  This is a real problem. The only way you can make sure is to have some metadata in your setdata to make sure that it was your setdata which succeeded. Connectionloss error handling has been a widely argued abt issue with ZK and I believe ZOOKEEPER-22 is gaining more and more importance because of issues like these. But ZOOKEEPER-22 will take some time, so you'd have to get arnd this issue with some metadata information for now.

;;;","31/Mar/11 07:13;liyin;Thanks Mahadev
I think adding hostname and processID to the set data works. 
And every time when calling the getData, it just remove that metadata.

;;;","31/Mar/11 15:14;stack;Sounds good to me Liyin.;;;","01/Apr/11 23:59;liyin;Changing the zk data format has a compatible problem.
Right now, I put some MAGIC number at the beginning of the new format zk data. 
So it can handle the old format zk data easily.
 
But the origin code cannot handle the new format of zk data. 
When deploying, it needs a fresh restart and deploy all the new jar files together.
;;;","02/Apr/11 01:22;stack;Requiring a restart is ok Liyin.  Going from 0.90.x to 0.92 will require a restart currently anyways (The RCP version changed because of the addition of coprocessors among other additions and removals).;;;","02/Apr/11 01:22;stack;Oh, so, do you even need the MAGIC in that case? (Restart usually clears zk state);;;","04/Apr/11 05:21;liyin;RecoverableZooKeeper is a wrapper of real Zookeeper instance. 
It will retry operations for a configurable times if there is a connection loss exception. 
The only limit is that it will not throw out NoNode exception for delete operation or throw out NodeExist exception for create operation.They are the only 2 false negative cases.
But application won't get any false positive exception.;;;","16/Apr/11 19:31;stack;@Liyin Pardon me.  I missed your new patch.  It looks great.  Should I remove the retry on around create of baseZNode in ZooKeeperWatcher constructor?  

bq. The only limit is that it will not throw out NoNode exception for delete operation or throw out NodeExist exception for create operation.They are the only 2 false negative cases.

So, to be clear (and after looking at code), if node already exists when we create, we just return as though the create was successful.  Similar for delete; if node already gone, we'll return as though successful delete.

Let me know on above.  I can do fixes on commit (Patch looks good to me).;;;","26/Apr/11 18:26;stack;Ping Liyin!;;;","26/Apr/11 18:36;liyin;Hi Stack
I am so sorry for the delay:) I will fix this and submit a new patch~~
Thanks for the review:);;;","28/Apr/11 03:29;liyin;1) remove the unnecessary retry in the zookeeper watcher.
2) create still will throw out node exist exception and delete also will throw out no node exception. It only hide the exceptions caused by the retry logic. ;;;","30/Apr/11 20:37;stack;Thanks Liyin.  I think this will fix the intermittent fails of rolling restart test over on apache jenkins.;;;","30/Apr/11 20:44;stack;Applied to TRUNK.  Patch is actually a reversed patch so I did a new reverse on application.  I added licenses to classes that were missing them.  Thanks for the sweet patch Liyin.;;;","30/Apr/11 20:53;stack;Reopening. Most tests are failing.;;;","30/Apr/11 20:55;stack;Here is a non-reversed patch with a fix for compile error.

Would you mind taking a looksee Liyin to see why tests are failing?  Here is failure from first test in the test suite (mvn clean test):

{code}
 t/s/org.apache.hadoop.hbase.master.TestHMasterRPCException.txt                                                                                                                                                                               
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hbase.master.TestHMasterRPCException
-------------------------------------------------------------------------------
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.348 sec <<< FAILURE!
testRPCException(org.apache.hadoop.hbase.master.TestHMasterRPCException)  Time elapsed: 0.312 sec  <<< ERROR!
org.apache.hadoop.hbase.ZooKeeperConnectionException: master:57938-0x12fa82ed2230000 Unexpected KeeperException creating base node
    at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:160)
    at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:236)
    at org.apache.hadoop.hbase.master.TestHMasterRPCException.testRPCException(TestHMasterRPCException.java:46)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:62)
    at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:140)
    at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:165)
    at org.apache.maven.surefire.Surefire.run(Surefire.java:107)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:289)
    at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1005)
Caused by: org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/unassigned
    at org.apache.zookeeper.KeeperException.create(KeeperException.java:118)
    at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
    at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:809)
    at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:837)
    at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:197)
    at org.apache.hadoop.hbase.zookeeper.ZKUtil.createAndFailSilent(ZKUtil.java:807)
    at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:155)
    ... 28 more
{code};;;","10/May/11 17:17;liyin;Thanks stack, I will take a close look:) 
I don't know why I never received emails update for this jira. 
So sorry for the late response. 
;;;","19/May/11 22:10;stack;Thanks Liyin.  I just tried the patch again to see if I could figure it but it doesn't apply now (ugh).  This patch I think important.  Tests used to fail sporadically up on apache jenkins from time to time because of a lack of retry.  You've done the work, would be nice to bring it home.  Let me know if I can help out.;;;","19/May/11 22:14;liyin;Thanks stack. If you cannot patch it, I can submit a new patch later. Since the 89 has a different way to use zk, I need more more time to debug the failure of the unit tests:)

Thanks Liyin;;;","10/Jun/11 22:45;stack;Moving out of 0.92.0. Pull it back in if you think different.;;;","10/Jun/11 23:52;stack;Bringing back in.  Try and hack this patch into our TRUNK.;;;","28/Jul/11 06:25;stack;Update to trunk, fix failing TestHMasterRPC and some cleanup on Liyin's v3 patch.;;;","28/Jul/11 06:45;stack;Committed to TRUNK Liyin.  This may have broken distributed split test.  Will look into that this morning (it has other issues).;;;","28/Jul/11 07:34;hudson;Integrated in HBase-TRUNK #2059 (See [https://builds.apache.org/job/HBase-TRUNK/2059/])
    HBASE-3065 Retry all 'retryable' zk operations; e.g. connection loss

stack : 
Files : 
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitLogWorker.java
* /hbase/trunk/CHANGES.txt
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/util/Bytes.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/util/RetryCounterFactory.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/util/RetryCounter.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java
;;;","28/Jul/11 22:58;hudson;Integrated in HBase-TRUNK #2060 (See [https://builds.apache.org/job/HBase-TRUNK/2060/])
    HBASE-3065  Retry all 'retryable' zk operations; e.g. connection loss; addendum... split log zk code url encodes which interacts w/ new naming of znodes that this patch introduces... this commit fixes testslitlogworker

stack : 
Files : 
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitLogWorker.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKSplitLog.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java
;;;","29/Jul/11 10:36;ram_krish;Reason for TestSplitLogManager failures
=======================================
Pls find the analysis
-> The Recoverable zookeeper encodes the node name while creating.(This is already pointed out by Stack).
-> The RecoverableZookeeper while writing the data adds some metadata to it.
{noformat}
byte[] newData = appendMetaData(data);
{noformat}
While we executing some of the testcases SplitLogManager.getDataSetWatchSuccess() gets invoked.
Here we do some state comparions like
{noformat}
    if (TaskState.TASK_UNASSIGNED.equals(data)) {
      LOG.debug(""task not yet acquired "" + path + "" ver = "" + version);
      handleUnassignedTask(path);
    } else if (TaskState.TASK_OWNED.equals(data)) {
      registerHeartbeat(path, version,
          TaskState.TASK_OWNED.getWriterName(data));
    } else if (TaskState.TASK_RESIGNED.equals(data)) {
      LOG.info(""task "" + path + "" entered state "" + new String(data));
      resubmit(path, true);
    }
{noformat}

Here the data variable is with metadata appended while writing the data whereas the  TaskState is without metadata.
So any comparison that we make fails.

Also one more observation is 'testOrphanTaskAcquisition()' testcase needs some wait mechanism before proceeding.
Because the GetDataAsyncCallback call is asynchronous.

In RecoverableZooKeeper the javadoc itself says creating a node should be handled carefully.
I have still not completely covered all the failures but this is the basic reason.  Even the test case 'testTaskDone'()' hanging is due to the same problem I feel.  
Am not fully aware of the splitlog feature with zookeeper will try to provide an addendum to this.;;;","29/Jul/11 10:42;ram_krish;The above analysis is for the testcase 'testOrphanTaskAcquisition()';;;","29/Jul/11 11:32;ram_krish;I will upload the patch.  I got the exact fix;;;","29/Jul/11 12:46;ram_krish;I have uploaded the addendum.  I think there is a better way also
Currently in RecoverableZookeeper.getData() api already does the removeAppend step.  But those api doesnot take the
AsyncCallback as parameter but the one in Zookeeper does.  

Here the problem is Zookeeper.getData() that takes AsyncCallback  doesnot return the byte[] instead internally it inovkes the AsyncCallback.processResult().  that is the reason we dont have the corresponding similar api in RecoverableZookeeper.  Pls let me know if the patch is ok.  Also correct me if my analysis is wrong. 
;;;","29/Jul/11 13:32;yuzhihong@gmail.com;With the addendum, TestSplitLogManager passed and I got one fewer test failure from TestDistributedLogSplitting:
{code}
Failed tests: 
  testThreeRSAbort(org.apache.hadoop.hbase.master.TestDistributedLogSplitting)
  testWorkerAbort(org.apache.hadoop.hbase.master.TestDistributedLogSplitting)
{code}
Applied addendum to TRUNK.

Thanks for the analysis Ramkrishna.;;;","29/Jul/11 13:55;ram_krish;@Ted
The testcase testWorkerAbort() is working fine.  
testThreeRSAbort I am getting 
'java.lang.AssertionError: expected:<4000> but was:<3400>.'
Is this the error that we get?;;;","29/Jul/11 13:59;yuzhihong@gmail.com;There is still more to fix.;;;","29/Jul/11 14:01;yuzhihong@gmail.com;On my MacBook, I got:
{code}
testThreeRSAbort(org.apache.hadoop.hbase.master.TestDistributedLogSplitting)  Time elapsed: 30.298 sec  <<< FAILURE!
java.lang.AssertionError: expected:<4000> but was:<2900>
...
testWorkerAbort(org.apache.hadoop.hbase.master.TestDistributedLogSplitting)  Time elapsed: 48.424 sec  <<< FAILURE!
java.lang.AssertionError: expected:<1> but was:<0>
{code};;;","29/Jul/11 16:05;hudson;Integrated in HBase-TRUNK #2063 (See [https://builds.apache.org/job/HBase-TRUNK/2063/])
    HBASE-3065 Addendum that removes metadata in getDataSetWatchSuccess() (Ramkrishna)

tedyu : 
Files : 
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
;;;","29/Jul/11 16:10;stack;Excellent Ram.  I'd figured the first part of your patch last night -- the metdata prefix -- but not the second.  Thank you.  Let me build on your patch for the rest of the fix.

So, yes, distributed log splitting does heavy interactions with zk.  It went in after the original 3065 patch was done.  The url encoding that distributed splitting zk'ing does is clashing with suffixes that this patch adds to file names -- we need to find the places where we want to use znode name and make sure we url decode.  The second issue is the one you note above where this patch adds metadata at front of data and we need to strip it reading in a few places.

Let me see how far I get today (I'm gone for a week starting this evening...);;;","29/Jul/11 16:40;yuzhihong@gmail.com;I guess Prakash and Liyin would be able to accommodate this JIRA for distributed log splitting since they work together.;;;","29/Jul/11 18:02;stack;@Ram We could add methods to RecoverableZookeeper than wrapped the callbacks to strip the metadata prefix; maybe we should do that sometime.  The addendum will do for now.;;;","02/Aug/11 04:51;yuzhihong@gmail.com;In RecoverableZooKeeper, should we make the handling of zero length data in appendMetaData() and removeMetaData() symmetrical ?
I mean this change:
{code}
   private byte[] appendMetaData(byte[] data) {
-    if(data == null){
+    if(data == null || data.length == 0){
       return data;
     }
{code};;;","02/Aug/11 22:38;jdcryans;Log splitting was broken since this jira was committed because a removeMetaData was missing in SplitLogWorker, here's what I just committed:

{code}
Index: src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java	(revision 1153269)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java	(working copy)
@@ -537,6 +537,7 @@
         getDataSetWatchFailure(path);
         return;
       }
+      data = watcher.getRecoverableZooKeeper().removeMetaData(data);
       getDataSetWatchSuccess(path, data);
       return;
     }
{code};;;","03/Aug/11 01:12;hudson;Integrated in HBase-TRUNK #2072 (See [https://builds.apache.org/job/HBase-TRUNK/2072/])
    HBASE-3065 don't prepend MAGIC if data is empty
HBASE-3065 adding a missing removeMetaData in SplitLogWorker.java

tedyu : 
Files : 
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java

jdcryans : 
Files : 
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java
;;;","03/Aug/11 02:06;yuzhihong@gmail.com;In TRUNK build 2072, there were only two test failures which are not related to this JIRA.

Resolving.;;;","03/Aug/11 04:45;ram_krish;@Ted, I think there is some randomness here.  Because if we try to debug i was not getting any error yesterday.  and even the failures were random in nature. ;;;","15/Dec/11 10:42;qwertymaniac;As a result of this, can the blocks of code of the following manner be considered as resolved?

{code}
      if (rc != 0) {
        // Thisis resultcode.  If non-zero, need to resubmit.
        LOG.warn(""rc != 0 for "" + path + "" -- retryable connectionloss -- "" +
          ""FIX see http://wiki.apache.org/hadoop/ZooKeeper/FAQ#A2"");
        return;
      }
{code}

This is from AssignmentManager, lines 1306 and 1336 (two instances).

Or are those callbacks still not retrying in nature?;;;","15/Dec/11 10:43;qwertymaniac;Actually, the more critical one is from the CreateUnassigned one:

{code}
      if (rc != 0) {
        // Thisis resultcode.  If non-zero, need to resubmit.
        LOG.warn(""rc != 0 for "" + path + "" -- retryable connectionloss -- "" +
          ""FIX see http://wiki.apache.org/hadoop/ZooKeeper/FAQ#A2"");
        this.zkw.abort(""Connectionloss writing unassigned at "" + path +
          "", rc="" + rc, null);
        return;
      }
{code}

(Line 1306, AssignmentManager). As you see there, it aborts the whole thing instead of retrying.;;;","15/Dec/11 16:09;stack;We should make a test to prove these blocks not needed Harsh?;;;","15/Dec/11 17:25;qwertymaniac;Good point.

/me bangs his head on the wall for not trying first :)

I'll spend some time in the weekend to try out 0.92 and force this callback to fail.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Long sleeping in HConnectionManager after thread is interrupted,HBASE-3064,12475612,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,bruno,bruno,bruno,01/Oct/10 13:32,20/Nov/15 12:41,14/Jul/23 06:06,01/Oct/10 23:48,,,,,,,,,,,,0.90.0,,Client,IPC/RPC,,,,0,,,"We run sometimes into the problem that when a thread running HBase client code is interrupted, it hangs. The problem is it is sleeping in HConnectionManager, in the methods locateRegionInMeta and getRegionServerWithRetries, where there is code like this:

{code}
try{
  Thread.sleep(getPauseTime(tries));
} catch (InterruptedException e) {
  // continue
}
{code}

which is located in a for-loop, so it will keep retrying even when someone requested the thread to stop its work.

The attached patch proposes as fix to re-assert the interrupted status of the thread and to throw an IOException. Some other cases of InterruptedException-handling in the same class do a similar thing, though sometimes returning null or breaking. I found returning null causes NPE's in other locations so I think it is better to throw an informative exception.

Side thought: I would not be against propagating the InterruptedException all the way up to the client APIs (HTable/HBaseAdmin), so that users who want to support interruptable threads do not have to check the interrupted flag. I'd need to check some more but I have the impression that now sometimes methods like HTable.get() simply return null when a thread is interrupted.

Some background on good ways of handling InterruptedExceptions can be found here:
http://www.ibm.com/developerworks/java/library/j-jtp05236.html",,bruno,evertot,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/10 13:33;bruno;connectionmgr-interruptedexc-patch.txt;https://issues.apache.org/jira/secure/attachment/12456114/connectionmgr-interruptedexc-patch.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26623,Reviewed,,,,Fri Nov 20 12:41:51 UTC 2015,,,,,,,,,,"0|i0hklb:",100598,,,,,,,,,,,,,,,,,,,,,"01/Oct/10 13:33;bruno;Patch against r1003508;;;","01/Oct/10 15:16;stack;Over in the new master, we let out all InterruptedExceptions now.  As you suggest, we should do same in client.  The change will be disruptive though in that it'll change APIs since we'll have to add IE as thrown to all methods.  So, I think we should apply this patch for 0.90 and then make a new issue to add IE to client APIs for 0.92?  Hows that sound?  If agreeable I'll apply this patch (it looks good).;;;","01/Oct/10 15:22;bruno;Sounds perfect. I did not expect the API to be changed overnight, just launching the idea.

Thanks for the quick follow up.;;;","01/Oct/10 23:48;stack;Committed.  Thanks for the patch Bruno.  I added HBASE-3072 to make the client generally interruptible.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestThriftServer failing in TRUNK,HBASE-3063,12475578,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,01/Oct/10 06:07,20/Nov/15 12:40,14/Jul/23 06:06,04/Oct/10 18:17,,,,,,,,,,,,0.90.0,,,,,,,0,,,Delete of a table is removing regions from the filesystem while they are trying to close.  Not sure yet how to fix.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/10 05:05;stack;3063.txt;https://issues.apache.org/jira/secure/attachment/12456229/3063.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26622,,,,,Fri Nov 20 12:40:37 UTC 2015,,,,,,,,,,"0|i0hkl3:",100597,,,,,,,,,,,,,,,,,,,,,"03/Oct/10 05:05;stack;This patch disables the switch back where TestThriftServer goes disable, enable, disable, delete and replaces it with a disable, delete -- for now.  This stuff needs more study but inside in TestThriftServer is not the place to be doing it.  I'll open a new issue for it.  This patch also adds stack trace inside in CloseRegionHandler if IOE; I couldn't figure out what was broke w/o it.  It also adds to DeleteTableHandler a bit of friction where we'll wait on table regions to clear region in transition before going forward with delete.;;;","04/Oct/10 18:17;stack;Resolving. This test  has been passing on last few hudson runs.  I opened HBASE-3077 to dig into the real issue.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ZooKeeper KeeperException$ConnectionLossException is a ""recoverable"" exception; we should retry a while on server startup at least.",HBASE-3062,12475576,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,01/Oct/10 04:59,20/Nov/15 12:43,14/Jul/23 06:06,09/Oct/10 17:55,,,,,,,,,,,,0.90.0,,,,,,,0,,,"On startup of daemons we'll sometimes fail (new master) with something like the following:

{code}
2010-09-30 23:38:36,979 INFO org.apache.hadoop.hbase.zookeeper.ZKUtil: regionserver60020 opening connection to ZooKeeper with quorum (sv2borg182:20001,sv2borg181:20001,sv2borg180:20001)
2010-09-30 23:38:36,979 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=sv2borg182:20001,sv2borg181:20001,sv2borg180:20001 sessionTimeout=60000 watcher=regionserver60020
2010-09-30 23:38:36,980 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server sv2borg182/10.20.20.182:20001
2010-09-30 23:38:36,980 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to sv2borg182/10.20.20.182:20001, initiating session
2010-09-30 23:38:36,981 INFO org.apache.zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect
2010-09-30 23:38:37,083 ERROR org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: regionserver60020 Unexpected KeeperException creating base node
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase
    at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
    at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
    at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:637)
    at org.apache.hadoop.hbase.zookeeper.ZKUtil.createAndFailSilent(ZKUtil.java:807)
    at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:107)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.initializeZooKeeper(HRegionServer.java:438)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.initialize(HRegionServer.java:420)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:305)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2436)
    at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:60)
    at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:75)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
    at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2460)
{code}

I'll see it over on master too the odd time.

Currently we fail out (though in above case, because of order in which we do startup the regionserver actually hung up because RPC was started before zk)

We should retry this error some at least on startup because its 'recoverable' (See http://wiki.apache.org/hadoop/ZooKeeper/ErrorHandling).  In fact, we should probably retry always until we get a session expired.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/10 17:56;stack;3062-v3.txt;https://issues.apache.org/jira/secure/attachment/12456778/3062-v3.txt","01/Oct/10 05:08;stack;3063-v2.txt;https://issues.apache.org/jira/secure/attachment/12456085/3063-v2.txt","01/Oct/10 05:01;stack;3063.txt;https://issues.apache.org/jira/secure/attachment/12456083/3063.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26621,Reviewed,,,,Fri Nov 20 12:43:03 UTC 2015,,,,,,,,,,"0|i0hkkv:",100596,,,,,,,,,,,,,,,,,,,,,"01/Oct/10 05:01;stack;This patch retries connection loss a configurable amount of time.  It also changes startup order in hrs so we have zk before we startup up rpc.

Need to test this patch still.;;;","01/Oct/10 05:08;stack;This one will let let cluster start (No NPE when go to get port in zk).;;;","01/Oct/10 15:40;streamy;+1;;;","09/Oct/10 17:53;stack;Patch seems to be doing the right thing. See here (zk quorum was down at the time):

{code}
2010-10-09 10:52:33,731 INFO  [main] zookeeper.ZKUtil(94): master:51876 opening connection to ZooKeeper with quorum (localhost:21810)
2010-10-09 10:52:33,877 DEBUG [main] zookeeper.ZooKeeperWatcher(122): Retrying zk create for another 9889ms; set 'hbase.zookeeper.recoverable.waittime' to change wait time); KeeperErrorCode = ConnectionLoss for /hbase
2010-10-09 10:52:34,010 DEBUG [main] zookeeper.ZooKeeperWatcher(122): Retrying zk create for another 9756ms; set 'hbase.zookeeper.recoverable.waittime' to change wait time); KeeperErrorCode = ConnectionLoss for /hbase
2010-10-09 10:52:34,615 DEBUG [main] zookeeper.ZooKeeperWatcher(122): Retrying zk create for another 9151ms; set 'hbase.zookeeper.recoverable.waittime' to change wait time); KeeperErrorCode = ConnectionLoss for /hbase
2010-10-09 10:52:36,052 DEBUG [main] zookeeper.ZooKeeperWatcher(122): Retrying zk create for another 7714ms; set 'hbase.zookeeper.recoverable.waittime' to change wait time); KeeperErrorCode = ConnectionLoss for /hbase
2010-10-09 10:52:36,715 DEBUG [main] zookeeper.ZooKeeperWatcher(122): Retrying zk create for another 7051ms; set 'hbase.zookeeper.recoverable.waittime' to change wait time); KeeperErrorCode = ConnectionLoss for /hbase
2010-10-09 10:52:37,509 DEBUG [main] zookeeper.ZooKeeperWatcher(122): Retrying zk create for another 6257ms; set 'hbase.zookeeper.recoverable.waittime' to change wait time); KeeperErrorCode = ConnectionLoss for /hbase
2010-10-09 10:52:39,069 DEBUG [main] zookeeper.ZooKeeperWatcher(122): Retrying zk create for another 4697ms; set 'hbase.zookeeper.recoverable.waittime' to change wait time); KeeperErrorCode = ConnectionLoss for /hbase
2010-10-09 10:52:39,618 DEBUG [main] zookeeper.ZooKeeperWatcher(122): Retrying zk create for another 4148ms; set 'hbase.zookeeper.recoverable.waittime' to change wait time); KeeperErrorCode = ConnectionLoss for /hbase
2010-10-09 10:52:39,982 DEBUG [main] zookeeper.ZooKeeperWatcher(122): Retrying zk create for another 3784ms; set 'hbase.zookeeper.recoverable.waittime' to change wait time); KeeperErrorCode = ConnectionLoss for /hbase
2010-10-09 10:52:41,886 DEBUG [main] zookeeper.ZooKeeperWatcher(122): Retrying zk create for another 1880ms; set 'hbase.zookeeper.recoverable.waittime' to change wait time); KeeperErrorCode = ConnectionLoss for /hbase
2010-10-09 10:52:42,577 DEBUG [main] zookeeper.ZooKeeperWatcher(122): Retrying zk create for another 1189ms; set 'hbase.zookeeper.recoverable.waittime' to change wait time); KeeperErrorCode = ConnectionLoss for /hbase
2010-10-09 10:52:43,377 DEBUG [main] zookeeper.ZooKeeperWatcher(122): Retrying zk create for another 389ms; set 'hbase.zookeeper.recoverable.waittime' to change wait time); KeeperErrorCode = ConnectionLoss for /hbase
2010-10-09 10:52:45,335 ERROR [main] zookeeper.ZooKeeperWatcher(135): master:51876 Unexpected KeeperException creating base node
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:637)
...
{code};;;","09/Oct/10 17:55;stack;Committed.;;;","09/Oct/10 17:56;stack;Here is what I committed.  Slightly different from what Jon reviewed in that log messages format was changed and prefixed with an isDebugEnabled check.;;;","09/Oct/10 18:18;streamy;We should open a JIRA to do this across all zk operations.  Not sure what the cleanest way for us to get this retry logic across ZKUtil would be.;;;","09/Oct/10 18:47;stack;Already did.  HBASE-3065.  Sorry.  Should have made mention of it in this issue.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[replication] Reenable replication on trunk with unit tests,HBASE-3060,12475555,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,streamy,streamy,30/Sep/10 21:47,20/Nov/15 12:41,14/Jul/23 06:06,11/Oct/10 18:05,0.90.0,,,,,,,,,,,0.90.0,,Replication,test,,,,0,,,"Replication currently doesn't work on trunk because it needs to be ported on the new ZK utils. Tests that weren't passing were marked as DISABLED or @ignore. 

Do the port and reenable everything.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/10 17:39;jdcryans;HBASE-3060.patch;https://issues.apache.org/jira/secure/attachment/12456877/HBASE-3060.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26619,Reviewed,,,,Fri Nov 20 12:41:10 UTC 2015,,,,,,,,,,"0|i0hkkf:",100594,,,,,,,,,,,,,,,,,,,,,"30/Sep/10 21:48;streamy;Renamed it to DISABLEDTestReplicationSourceManager for now.;;;","30/Sep/10 21:54;jdcryans;I'm currently working on getting replication working on trunk (now that it's in prod here on 0.89).;;;","01/Oct/10 23:38;jdcryans;Gratuitously changing the scope of this jira to what it really should be, reenabling replication. Also it's a blocker.;;;","11/Oct/10 17:39;jdcryans;Patch that ports replication to the new master and reenables the unit tests, which are now passing for me.;;;","11/Oct/10 17:47;stack;+1 if tests pass.;;;","11/Oct/10 18:05;jdcryans;Committed, thanks for looking at it Stack!;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestReadWriteConsistencyControl occasionally hangs,HBASE-3059,12475550,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,hairong,hairong,hairong,30/Sep/10 21:32,20/Nov/15 12:43,14/Jul/23 06:06,06/Oct/10 01:45,,,,,,,,,,,,0.90.0,,,,,,,0,,,"The test hung when I ran mvn test today. The jstack shows that a Writer thread hung at

""Thread-1"" prio=10 tid=0x00002aaad81d2800 nid=0x6ce9 in Object.wait() [0x0000000040f37000]
   java.lang.Thread.State: WAITING (on object monitor)  at java.lang.Object.wait(Native Method)
  at org.apache.hadoop.hbase.regionserver.ReadWriteConsistencyControl.completeMemstoreInsert(ReadWriteConsistencyControl.java:130)
  -- locked <0x00002aaac9fa0f50> (a java.lang.Object)                
  at org.apache.hadoop.hbase.regionserver.TestReadWriteConsistencyControl$Writer.run(TestReadWriteConsistencyControl.java:56)  at java.lang.Thread.run(Thread.java:619)

It seems to be caused by a race condition in ReadWriteConsistencyControl#completeMemStoreInsert. Accesses/updates of the value of memStoreRead should be done while holding the readWaiters lock.",,kannanm,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/10 21:35;hairong;hbase_trunk_consistency.patch;https://issues.apache.org/jira/secure/attachment/12456064/hbase_trunk_consistency.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26618,,,,,Fri Nov 20 12:43:28 UTC 2015,,,,,,,,,,"0|i0hkk7:",100593,,,,,,,,,,,,,,,,,,,,,"30/Sep/10 21:42;ryanobjc;what jvm version were you using?;;;","30/Sep/10 21:45;ryanobjc;a few questions:

- What jvm are you using?  You must be using a 64 bit jvm to run hbase.  32 bit jvms dont offer atomic updates to longs, which is required for this code.
- how and why does this fix the hang?  The variable is volatile, so adding a synchronized block should not improve the characteristics according the the JMM.
- How much does this slow down the code?  Readwaiters is a fairly sensitive lock and holding it for less time would be better.
;;;","30/Sep/10 22:40;hairong;I ran with 64 bit jvm.

But this bug can not be fixed by atomic update. The problem is that it is possible that after a writer W1 checked
memstoreRead < e.getWriterNumber() to be true, another Writer W2 sets memestoreRead = nextReadValue then does a notifyAll(). Then W1 does readWaiters.wait(0). Since it misses the singal from W2, W1 waits there forever.;;;","04/Oct/10 21:22;kannanm;+1 on the patch.;;;","04/Oct/10 21:29;ryanobjc;thanks, the patch looks good, i'll commit it;;;","06/Oct/10 01:45;ryanobjc;committed, thanks for that.

Strange that we did not notice it sooner...;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix REST tests on trunk,HBASE-3058,12475540,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,streamy,streamy,streamy,30/Sep/10 20:06,20/Nov/15 12:44,14/Jul/23 06:06,30/Sep/10 20:58,0.90.0,,,,,,,,,,,0.90.0,,REST,,,,,0,,,"Most of the REST tests do not pass on trunk.  Most likely because configuration is being generated internally within REST classes rather than being passed in, so when tests override configs they are not getting picked up.

There was a similar issue already fixed with thrift and avro.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/10 20:55;streamy;HBASE-3058-v3.patch;https://issues.apache.org/jira/secure/attachment/12456053/HBASE-3058-v3.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26617,Reviewed,,,,Fri Nov 20 12:44:02 UTC 2015,,,,,,,,,,"0|i0hkjz:",100592,,,,,,,,,,,,,,,,,,,,,"30/Sep/10 20:33;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/927/
-----------------------------------------------------------

Review request for hbase, stack and Andrew Purtell.


Summary
-------

This method adds Configuration as a parameter to almost all the rest classes.  This is because they all extend a base class, ResourceBase, which just holds a reference to a single RESTServlet that gets instantiated as a singleton.  It's this RESTServlet which needs to take a Configuration.  Since ResourceBase is instantiated all over the place, everything needs Configuration.

This doesn't work though.  Most of these classes get instantiated through REST/JAX automagic, so it's getting null Configuration.

Could use some help.


This addresses bug HBASE-3058.
    http://issues.apache.org/jira/browse/HBASE-3058


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/rest/ExistsResource.java 1003206 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/Main.java 1003206 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/RESTServlet.java 1003206 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java 1003206 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/ResourceBase.java 1003206 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/ResultGenerator.java 1003206 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/RootResource.java 1003206 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java 1003206 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/RowResultGenerator.java 1003206 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java 1003206 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java 1003206 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/ScannerResultGenerator.java 1003206 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java 1003206 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/StorageClusterStatusResource.java 1003206 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/StorageClusterVersionResource.java 1003206 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/TableResource.java 1003206 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/VersionResource.java 1003206 

Diff: http://review.cloudera.org/r/927/diff


Testing
-------

REST tests failing now with NPE because everything gets null Configuration objects.  Without patch they fail because they try to contact the wrong master.


Thanks,

Jonathan


;;;","30/Sep/10 20:55;streamy;Simpler fix.  REST tests passing.;;;","30/Sep/10 20:58;streamy;Committed v3 patch.  Thanks for help and review Andrew!;;;","30/Sep/10 21:07;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/927/
-----------------------------------------------------------

(Updated 2010-09-30 13:50:57.922028)


Review request for hbase, stack and Andrew Purtell.


Changes
-------

Much simpler patch.  REST tests passing.


Summary
-------

This method adds Configuration as a parameter to almost all the rest classes.  This is because they all extend a base class, ResourceBase, which just holds a reference to a single RESTServlet that gets instantiated as a singleton.  It's this RESTServlet which needs to take a Configuration.  Since ResourceBase is instantiated all over the place, everything needs Configuration.

This doesn't work though.  Most of these classes get instantiated through REST/JAX automagic, so it's getting null Configuration.

Could use some help.


This addresses bug HBASE-3058.
    http://issues.apache.org/jira/browse/HBASE-3058


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/rest/RESTServlet.java 1003250 
  trunk/src/test/java/org/apache/hadoop/hbase/rest/HBaseRESTClusterTestBase.java 1003206 

Diff: http://review.cloudera.org/r/927/diff


Testing
-------

REST tests failing now with NPE because everything gets null Configuration objects.  Without patch they fail because they try to contact the wrong master.


Thanks,

Jonathan


;;;","30/Sep/10 21:18;hbasereviewboard;Message from: ""Andrew Purtell"" <apurtell@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/927/#review1369
-----------------------------------------------------------

Ship it!


- Andrew



;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition when closing regions that causes flakiness in TestRestartCluster,HBASE-3057,12475531,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,30/Sep/10 18:44,20/Nov/15 12:43,14/Jul/23 06:06,30/Sep/10 18:54,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"In {{TestRestartCluster.testClusterRestart()}} we spin up cluster, create three tables, shut it down, start it back up, and ensure we still have three regions.

A subtle race condition during the first shutdown makes it so the flush of META doesn't finish so when we start back up there are no user regions.

I'm not sure if there are reasons the ordering is as such, but the is the section of code in CloseRegionHandler around line 118:

{noformat}
      this.rsServices.removeFromOnlineRegions(regionInfo.getEncodedName());
      region.close(abort);
{noformat}

We remove from the online map of regions before actually closing.  But what the main run() loop in the RS is waiting on to determine when it can shut down is that the online region map is empty.

{noformat}
  private void waitOnAllRegionsToClose() {
    // Wait till all regions are closed before going out.
    int lastCount = -1;
    while (!this.onlineRegions.isEmpty()) {
{noformat}

Any reason not to swap these two and do the close before removing from online regions?",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/10 18:53;streamy;HBASE-3057-v1.patch;https://issues.apache.org/jira/secure/attachment/12456040/HBASE-3057-v1.patch","11/Oct/10 21:25;larsfrancke;lars-stacktrace.1.txt;https://issues.apache.org/jira/secure/attachment/12456894/lars-stacktrace.1.txt","11/Oct/10 21:25;larsfrancke;lars-stacktrace.2.txt;https://issues.apache.org/jira/secure/attachment/12456895/lars-stacktrace.2.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26616,Reviewed,,,,Fri Nov 20 12:43:07 UTC 2015,,,,,,,,,,"0|i0hkjr:",100591,,,,,,,,,,,,,,,,,,,,,"30/Sep/10 18:49;stack;+1  Looks like mistake on my part (I love unit tests).;;;","30/Sep/10 18:54;streamy;Committed to trunk;;;","11/Oct/10 21:25;larsfrancke;I'm not sure if this is the same problem. If not I'll open another issue.

testClusterRestart fails for me most of the time. I've attached two logs. The first is unaltered trunk, the second again slightly modified (so line numbers etc. won't match up) but a slightly different error.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ordering in ZKWatcher constructor to prevent weird race condition,HBASE-3056,12475526,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,streamy,streamy,30/Sep/10 17:58,20/Nov/15 12:43,14/Jul/23 06:06,30/Sep/10 18:22,0.90.0,,,,,,,,,,,0.90.0,,Zookeeper,,,,,0,,,"A small race condition in ZKWatcher leads to an NPE:

{noformat}
2010-09-30 10:56:36,028 INFO  [Thread-217] zookeeper.ZKUtil(93): hconnection opening connection to ZooKeeper with quorum (localhost:21815)
2010-09-30 10:56:36,036 DEBUG [Thread-217-EventThread] zookeeper.ZooKeeperWatcher(184): hconnection Received ZooKeeper Event, type=None, state=SyncConnected, path=null
2010-09-30 10:56:36,036 ERROR [Thread-217-EventThread] zookeeper.ClientCnxn$EventThread(490): Error while calling watcher 
java.lang.NullPointerException
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:243)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:193)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:488)
{noformat}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/10 18:09;streamy;HBASE-3056-v1.patch;https://issues.apache.org/jira/secure/attachment/12456036/HBASE-3056-v1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26615,Reviewed,,,,Fri Nov 20 12:43:31 UTC 2015,,,,,,,,,,"0|i0hkjj:",100590,,,,,,,,,,,,,,,,,,,,,"30/Sep/10 18:09;streamy;Easy fix.  Quick review anyone?;;;","30/Sep/10 18:12;stack;+1;;;","30/Sep/10 18:22;streamy;Committed.  Thanks for review stack.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remore TestEmptyMetaInfo; it doesn't make sense any more.",HBASE-3054,12475473,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,30/Sep/10 03:42,20/Nov/15 12:41,14/Jul/23 06:06,30/Sep/10 03:43,,,,,,,,,,,,0.90.0,,,,,,,0,,,"At the head of the TestEmptyMetaInfo it says ""TODO: Does this test make sense any more?""

Its a test that checks that the basescanner when it runs cleans up weirdly incomplete rows in .META.

We don't really have a scanner any more and the thing we have, does not do this kinda cleaning anyways so this test is testing something removed.

i'm removing the test.  Its a mess anyways based on TestEmptyMetaInfo.  It just started failing too.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26614,,,,,Fri Nov 20 12:41:37 UTC 2015,,,,,,,,,,"0|i0hkj3:",100588,,,,,,,,,,,,,,,,,,,,,"30/Sep/10 03:43;stack;Done. Committed.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If new master crashes, restart is messy",HBASE-3047,12475350,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,28/Sep/10 21:30,20/Nov/15 12:42,14/Jul/23 06:06,30/Sep/10 03:31,,,,,,,,,,,,0.90.0,,,,,,,0,,,"If master crashes, the cluster-is-up flag is left stuck on.

On restart of cluster, regionservers may come up before the master.  They'll have registered themselves in zk by time the master assumes its role and master will think its joining an up and running cluster when in fact this is a fresh startup.  Other probs. are that there'll be a root region that is bad up in zk.  Same for meta and at moment we're not handling bad root and meta very well.

Here's sample of kinda of issues we're running into:

{code}
2010-09-25 23:53:13,938 FATAL org.apache.hadoop.hbase.master.HMaster:
Unhandled exception. Starting shutdown.
java.io.IOException: Call to /10.20.20.188:60020 failed on local
exception: java.io.IOException: Connection reset by peer
   at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:781)
   at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:750)
   at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:255)
   at $Proxy1.getProtocolVersion(Unknown Source)
   at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:412)
   at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:388)
   at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:435)
   at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:345)
   at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:889)
   at org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:350)
   at org.apache.hadoop.hbase.catalog.CatalogTracker.getRootServerConnection(CatalogTracker.java:209)
   at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:241)
   at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:286)
   at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:326)
   at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:157)
   at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:140)
   at org.apache.hadoop.hbase.master.AssignmentManager.rebuildUserRegions(AssignmentManager.java:753)
   at org.apache.hadoop.hbase.master.AssignmentManager.processFailover(AssignmentManager.java:174)
   at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:314)
Caused by: java.io.IOException: Connection reset by peer
   at sun.nio.ch.FileDispatcher.read0(Native Method)
   at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:21)
   at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:233)
   at sun.nio.ch.IOUtil.read(IOUtil.java:206)
{code}

Notice, we think its a case of processFailover so we think we can just scan meta to fixup our inmemory picture of the running cluster, only the scan of meta fails because the meta isn not assigned.

",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/10 04:07;stack;3047-final.txt;https://issues.apache.org/jira/secure/attachment/12455971/3047-final.txt","28/Sep/10 21:42;stack;3047.txt;https://issues.apache.org/jira/secure/attachment/12455877/3047.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26612,Reviewed,,,,Fri Nov 20 12:42:36 UTC 2015,,,,,,,,,,"0|i0hkhj:",100581,,,,,,,,,,,,,,,,,,,,,"28/Sep/10 21:42;stack;{code}
M src/test/java/org/apache/hadoop/hbase/catalog/TestCatalogTracker.java
  Add test of case where HRegionInterface connection throws a
  ConnectionException. Also tests two new verify root and meta 
  locations added to CatalogTracker.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
  Change order in which we start up trackers in ZK.  Also add blocking
  until master is up to make it less likely we'll start before master
  comes up, especially around the cluster start up situation.
M src/main/java/org/apache/hadoop/hbase/master/HMaster.java
  Introduce new state on startup, the case where the cluster is
  NOT a fresh startup and its NOT a cluster where all is fully
  assigned.  The repair the master needs run to fixup this new
  state is not yet done; we throw a NotImplementedException for
  now.  TODO.  Added new isRunningCluster checker used figuring
  what the cluster condition is when master is joining.  Not
  comprehensive but good enough for now.
M src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
  Javadoc.
  Added new verifyRootRegionLocation and verifyMetaRegionLocation.
  Needed to verify whats in zk is actually locations of catalog
  regions.
M src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
  Add fact that the verifying method, getRegionInfo, can throw
  ConnectException
{code};;;","28/Sep/10 23:15;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/915/
-----------------------------------------------------------

Review request for hbase, stack and Jonathan Gray.


Summary
-------

This is patch from Stack, just putting up on rb.

M src/test/java/org/apache/hadoop/hbase/catalog/TestCatalogTracker.java
  Add test of case where HRegionInterface connection throws a
  ConnectionException. Also tests two new verify root and meta 
  locations added to CatalogTracker.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
  Change order in which we start up trackers in ZK.  Also add blocking
  until master is up to make it less likely we'll start before master
  comes up, especially around the cluster start up situation.
M src/main/java/org/apache/hadoop/hbase/master/HMaster.java
  Introduce new state on startup, the case where the cluster is
  NOT a fresh startup and its NOT a cluster where all is fully
  assigned.  The repair the master needs run to fixup this new
  state is not yet done; we throw a NotImplementedException for
  now.  TODO.  Added new isRunningCluster checker used figuring
  what the cluster condition is when master is joining.  Not
  comprehensive but good enough for now.
M src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
  Javadoc.
  Added new verifyRootRegionLocation and verifyMetaRegionLocation.
  Needed to verify whats in zk is actually locations of catalog
  regions.
M src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
  Add fact that the verifying method, getRegionInfo, can throw
  ConnectException


This addresses bug HBASE-3047.
    http://issues.apache.org/jira/browse/HBASE-3047


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java 1002359 
  trunk/src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java 1002359 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1002359 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 1002359 
  trunk/src/test/java/org/apache/hadoop/hbase/catalog/TestCatalogTracker.java 1002359 

Diff: http://review.cloudera.org/r/915/diff


Testing
-------


Thanks,

Jonathan


;;;","29/Sep/10 01:11;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/915/#review1349
-----------------------------------------------------------


Overall this looks like a good improvement over what we had.  I'm still a little confused about isRunningCluster (or isProperRunningCluster per comments).

Repeat from inline comments, but, is there ever a time a single region is deployed and we don't want to trigger the failover codepath?

Isn't the case we're really protecting against here that the cluster was not shutdown properly so the cluster status flag is up when it shouldn't be?

And does this handle case that cluster is killed quickly and then restarted again so the master ephemeral node is actually still there?  Then the RS will have master node and cluster up node and startup but potentially without a real master?


trunk/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
<http://review.cloudera.org/r/915/#comment4482>

    Why is this an ""implementation""?  Doesn't the HRI represent the actual connection object?  I get that it's an implementation of HRI but normally that would be used in class names implementing?  No biggie, should just be consistent and seems a weird name to me (I think I was referring to this stuff as ""connection"" elsewhere in the class in method names/variable names)



trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.cloudera.org/r/915/#comment4481>

    Is this really the exception we want to throw (commons.lang)?  Or this is just short-term temporary?



trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.cloudera.org/r/915/#comment4483>

    yay thanks



trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.cloudera.org/r/915/#comment4484>

    So case that we are adding for here (but just throwing exception for now) is master came up, did not think it was fresh cluster (because cluster status flag in zk up? maybe note in comments above?), but we determine the cluster was not running because ROOT and META are not assigned.
    
    What about case where other regions are assigned?  Should this check actually be whether _any_ regions are assigned?  I think we discussed this, and I think looking for root/meta covers most cases, but maybe add a TODO?
    
    Though, even in failover case, we'll need to handle ROOT/META not being properly assigned, so if _any_ regions are assigned we would trigger failover, if no regions assigned we would assume it actually is a cluster startup and go into the branch of code which currently throws the exception.



trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.cloudera.org/r/915/#comment4485>

    javadoc about what this method does to determine if it's running cluster



trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.cloudera.org/r/915/#comment4486>

    So this method would be ""proper running cluster""?
    
    Isn't it the case that if a single region is deployed anywhere we are not in startup, we are failover?



trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
<http://review.cloudera.org/r/915/#comment4487>

    looks good


- Jonathan



;;;","29/Sep/10 04:01;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/915/#review1353
-----------------------------------------------------------


Here's a few comments on yours.

Actually, testing this patch on cluster brought up some issues.  I think I should recast.  I have some ideas on how.  v2 coming.  Will incorporate your belows.


trunk/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
<http://review.cloudera.org/r/915/#comment4495>

    I can change it (you get my intent but it still confused so I should change it).



trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.cloudera.org/r/915/#comment4496>

    Yeah, what you say.  Let me fix up comments.



trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.cloudera.org/r/915/#comment4497>

    will do


- stack



;;;","29/Sep/10 06:40;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/915/
-----------------------------------------------------------

(Updated 2010-09-28 23:31:22.975377)


Review request for hbase, stack and Jonathan Gray.


Changes
-------

Here, this should be more robust.  Your comments should be addressed also.  For sure, AM#processFailover has holes -- e.g. what if a regionserver crashed while new master was coming up -- but lets address that in another issue.  Below are notes on changes made since v1 of the patch.

M src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperNodeTracker.java
  Change here was because saw a case where we hung for ever (my guess is that remaining became equal to NO_TIMEOUT).  Redid the logic here.
M src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java
  Set this thread to be daemon.  Have seen it hold up RS shutdowns.
M src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
  Renamed the initialize method as createInitialFileSystemLayout, made it private it and called it from constructor.  Its idempotent, cheap, and no need others should be concerned with these mechanics; encapsulate it.
M src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
  Removed freshClusterStartup flag.  Now, let any 'unknown' server in and register it UNLESS its a dead server (fixed up expiration so we add to dead servers BEFORE we remove from online servers).  Have waitForRegionServers return count of regions out on cluster.  This will be 0 if servers are coming in with clean regionServerStartup but if they came in and were registered on a regionServerReport, then they'll have a filled out HServerLoad with a count of regions.  Use count of regions as way to tell if regions out on cluster or not.
M src/main/java/org/apache/hadoop/hbase/master/HMaster.java
  Removed freshClusterStartup.  Added logging of state of cluster-up flag, and # of regionservers out on cluster.  Use count of regions out on cluster to figure if we are to do assign of all user regions or if instead we are to do process failover.  Added splitting of WALs always and check and reassign of root and meta whether fresh start up or failover.
M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  Added notes on holes in processFailover.
M src/main/resources/hbase-default.xml
  Set checkin down from 5 to 3 seconds again.


Summary
-------

This is patch from Stack, just putting up on rb.

M src/test/java/org/apache/hadoop/hbase/catalog/TestCatalogTracker.java
  Add test of case where HRegionInterface connection throws a
  ConnectionException. Also tests two new verify root and meta 
  locations added to CatalogTracker.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
  Change order in which we start up trackers in ZK.  Also add blocking
  until master is up to make it less likely we'll start before master
  comes up, especially around the cluster start up situation.
M src/main/java/org/apache/hadoop/hbase/master/HMaster.java
  Introduce new state on startup, the case where the cluster is
  NOT a fresh startup and its NOT a cluster where all is fully
  assigned.  The repair the master needs run to fixup this new
  state is not yet done; we throw a NotImplementedException for
  now.  TODO.  Added new isRunningCluster checker used figuring
  what the cluster condition is when master is joining.  Not
  comprehensive but good enough for now.
M src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
  Javadoc.
  Added new verifyRootRegionLocation and verifyMetaRegionLocation.
  Needed to verify whats in zk is actually locations of catalog
  regions.
M src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
  Add fact that the verifying method, getRegionInfo, can throw
  ConnectException


This addresses bug HBASE-3047.
    http://issues.apache.org/jira/browse/HBASE-3047


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperNodeTracker.java 1001981 
  trunk/src/main/resources/hbase-default.xml 1001981 
  trunk/src/test/java/org/apache/hadoop/hbase/catalog/TestCatalogTracker.java 1001981 

Diff: http://review.cloudera.org/r/915/diff


Testing
-------


Thanks,

Jonathan


;;;","29/Sep/10 17:31;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/915/
-----------------------------------------------------------

(Updated 2010-09-29 10:23:49.845051)


Review request for hbase, stack and Jonathan Gray.


Changes
-------

More cleanup


Summary
-------

This is patch from Stack, just putting up on rb.

M src/test/java/org/apache/hadoop/hbase/catalog/TestCatalogTracker.java
  Add test of case where HRegionInterface connection throws a
  ConnectionException. Also tests two new verify root and meta 
  locations added to CatalogTracker.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
  Change order in which we start up trackers in ZK.  Also add blocking
  until master is up to make it less likely we'll start before master
  comes up, especially around the cluster start up situation.
M src/main/java/org/apache/hadoop/hbase/master/HMaster.java
  Introduce new state on startup, the case where the cluster is
  NOT a fresh startup and its NOT a cluster where all is fully
  assigned.  The repair the master needs run to fixup this new
  state is not yet done; we throw a NotImplementedException for
  now.  TODO.  Added new isRunningCluster checker used figuring
  what the cluster condition is when master is joining.  Not
  comprehensive but good enough for now.
M src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
  Javadoc.
  Added new verifyRootRegionLocation and verifyMetaRegionLocation.
  Needed to verify whats in zk is actually locations of catalog
  regions.
M src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
  Add fact that the verifying method, getRegionInfo, can throw
  ConnectException


This addresses bug HBASE-3047.
    http://issues.apache.org/jira/browse/HBASE-3047


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/RemoteExceptionHandler.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java 1001981 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperNodeTracker.java 1001981 
  trunk/src/main/resources/hbase-default.xml 1001981 
  trunk/src/test/java/org/apache/hadoop/hbase/catalog/TestCatalogTracker.java 1001981 

Diff: http://review.cloudera.org/r/915/diff


Testing
-------


Thanks,

Jonathan


;;;","29/Sep/10 22:35;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/915/
-----------------------------------------------------------

(Updated 2010-09-29 15:12:30.449220)


Review request for hbase, stack and Jonathan Gray.


Changes
-------

New version.  Comes of back and forth w/ Jon


Summary
-------

This is patch from Stack, just putting up on rb.

M src/test/java/org/apache/hadoop/hbase/catalog/TestCatalogTracker.java
  Add test of case where HRegionInterface connection throws a
  ConnectionException. Also tests two new verify root and meta 
  locations added to CatalogTracker.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
  Change order in which we start up trackers in ZK.  Also add blocking
  until master is up to make it less likely we'll start before master
  comes up, especially around the cluster start up situation.
M src/main/java/org/apache/hadoop/hbase/master/HMaster.java
  Introduce new state on startup, the case where the cluster is
  NOT a fresh startup and its NOT a cluster where all is fully
  assigned.  The repair the master needs run to fixup this new
  state is not yet done; we throw a NotImplementedException for
  now.  TODO.  Added new isRunningCluster checker used figuring
  what the cluster condition is when master is joining.  Not
  comprehensive but good enough for now.
M src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
  Javadoc.
  Added new verifyRootRegionLocation and verifyMetaRegionLocation.
  Needed to verify whats in zk is actually locations of catalog
  regions.
M src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
  Add fact that the verifying method, getRegionInfo, can throw
  ConnectException


This addresses bug HBASE-3047.
    http://issues.apache.org/jira/browse/HBASE-3047


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/RemoteExceptionHandler.java 1002359 
  trunk/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java 1002359 
  trunk/src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java 1002359 
  trunk/src/main/java/org/apache/hadoop/hbase/mapreduce/package-info.java 1002359 
  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1002359 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1002359 
  trunk/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java 1002359 
  trunk/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java 1002359 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 1002359 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java 1002359 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperNodeTracker.java 1002359 
  trunk/src/main/resources/hbase-default.xml 1002359 
  trunk/src/test/java/org/apache/hadoop/hbase/catalog/TestCatalogTracker.java 1002359 

Diff: http://review.cloudera.org/r/915/diff


Testing
-------


Thanks,

Jonathan


;;;","29/Sep/10 22:55;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/915/#review1360
-----------------------------------------------------------

Ship it!


Commit!  Just fix the missing insertion into deadServers map on commit as discussed.

- Jonathan



;;;","30/Sep/10 03:31;stack;Thanks for the review Jon.  The not-putting-servername-into-deadservers though big comment about how important doing so at that point was a good catch.  Committed earlier to day.;;;","30/Sep/10 04:07;stack;What I committed -- last diff up on review board plus jon suggstion.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[replication] ReplicationSource won't cleanup logs if there's nothing to replicate,HBASE-3044,12475275,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,28/Sep/10 03:52,12/Oct/12 06:17,14/Jul/23 06:06,15/Oct/10 00:01,0.89.20100924,,,,,,,,,,,0.90.0,,,,,,,0,,,"ReplicationSource only calls ReplicationSourceManager.logPositionAndCleanOldLogs if it replicated something, meaning that a region server that doesn't host a region with replicable edits will read through the logs but never report the position in zookeeper and won't clean the processed hlogs. This happened on one cluster here and shutting down a region server took a few minutes just to delete all the znodes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/10 23:36;jdcryans;HBASE-3044.patch;https://issues.apache.org/jira/secure/attachment/12457208/HBASE-3044.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26610,Reviewed,,,,Fri Oct 15 00:01:45 UTC 2010,,,,,,,,,,"0|i08shb:",49214,,,,,,,,,,,,,,,,,,,,,"14/Oct/10 23:36;jdcryans;Fixes the issue and adds a safeguard later on in the code.;;;","14/Oct/10 23:46;stack;+1;;;","15/Oct/10 00:01;jdcryans;Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use LO4J in SequenceFileLogReader,HBASE-3042,12475242,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,nspiegelberg,nspiegelberg,nspiegelberg,27/Sep/10 21:13,20/Nov/15 12:42,14/Jul/23 06:06,27/Sep/10 22:10,0.89.20100621,0.90.0,,,,,,,,,,0.90.0,,,,,,,0,,,Trivial change.  The SequenceFileLogReader class accidentally uses the Mortbay logging class instead of the LOG4J.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-3038,,"27/Sep/10 21:14;nspiegelberg;HBASE-3042.patch;https://issues.apache.org/jira/secure/attachment/12455753/HBASE-3042.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26609,Reviewed,,,,Fri Nov 20 12:42:05 UTC 2015,,,,,,,,,,"0|i0hkgv:",100578,,,,,,,,,,,,,,,,,,,,,"27/Sep/10 22:10;stack;Committed. Thanks for the patch Nicolas.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[replication] ReplicationSink shouldn't kill the whole RS when it fails to replicate,HBASE-3041,12475215,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,27/Sep/10 17:15,12/Oct/12 06:17,14/Jul/23 06:06,15/Oct/10 00:21,0.89.20100924,,,,,,,,,,,0.90.0,,,,,,,0,,,"This is kind of a funny bug, as long as you don't run into it. I thought I'd be a good idea to kill the region servers that act as sinks when they can't replicate edits on their own cluster (this is often something we do in face of fatal errors throughout the code), but not so much.

So, last friday while I was using CopyTable to replicate data from a master to a slave cluster while the new data was being replicated, one table got really slow and took too long to split which tripped RetriesExhaustedException coming out of HTable in ReplicationSink. This killed a first region server, which was itself hosting regions. Splitting the logs took a bit longer since the cluster was under high insert load, so this triggered other exceptions in the other region servers, to a point where they were all down. I restarted the cluster, the master splits all the logs that were remaining and begins assigning regions. Some of them took too long to open because each region server had a few regions to recover each and the last ones in the queue were minutes from being opened. Since the master cluster was already pushing edits to the slave, the region servers all got RetriesExhausted and all went down again. I changed the client pause from 1 to 3 and restarted, same happened. I changed it to 5, and finally was able to keep the cluster up. Fortunately, the master cluster was queueing up the HLogs so we didn't lose any data and the backlog was replicated in a few minutes.

So, instead of killing the region server, any exception coming out of HTable should just be treated as a failure to apply and the source cluster should retry later.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/10 23:42;jdcryans;HBASE-3041.patch;https://issues.apache.org/jira/secure/attachment/12457209/HBASE-3041.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26608,Reviewed,,,,Fri Oct 15 00:21:07 UTC 2010,,,,,,,,,,"0|i08sgv:",49212,,,,,,,,,,,,,,,,,,,,,"14/Oct/10 23:42;jdcryans;Patch that removes all the weird exception handling and the killing of the region server.;;;","14/Oct/10 23:47;stack;+1;;;","15/Oct/10 00:21;jdcryans;Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stuck in regionsInTransition because rebalance came in at same time as a split,HBASE-3039,12475116,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,25/Sep/10 21:46,20/Nov/15 12:41,14/Jul/23 06:06,27/Sep/10 21:02,,,,,,,,,,,,0.90.0,,master,,,,,0,,,"Saw this doing cluster tests:

{code}
2010-09-25 21:31:48,212 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because regions in transition: {73781e505e452221c9cd0e03585eb5d1=usertable,user800184056, 
128...
{code}

Here's the problem:

{code}
2010-09-25 08:16:48,186 INFO org.apache.hadoop.hbase.master.HMaster: balance hri=usertable,user800184056,1285397376525.73781e505e452221c9cd0e03585eb5d1., src=su184,60020,      
1285371621579, dest=sv2borg189,60020,1285371621577

2010-09-25 08:16:48,186 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region usertable,user800184056,1285397376525.                               
73781e505e452221c9cd0e03585eb5d1. (offlining)

2010-09-25 08:16:52,656 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: usertable,user800184056,1285397376525.73781e505e452221c9cd0e03585eb5d1.:           
Daughters; usertable,user800184056,1285402609029.c05825561e7ea3cc6507c70bfb21541a., usertable,user804024623,1285402609029.28f64903a7875bdafc1e7ee344b225b0.
2010-09-25 08:17:11,414 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  usertable,user800184056,1285397376525.                              
73781e505e452221c9cd0e03585eb5d1. state=PENDING_CLOSE, ts=1285402608186
{code}


....just as we were doing a balance, the region split.

Over on RS, I see the split starting up and then in comes the balance 'close' message.  By the time the close handler runs on regionserver the split is well underway and close handler actually doesn't find an online region to split.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/10 22:00;stack;3039.txt;https://issues.apache.org/jira/secure/attachment/12455578/3039.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26607,,,,,Fri Nov 20 12:41:38 UTC 2015,,,,,,,,,,"0|i0hkgf:",100576,,,,,,,,,,,,,,,,,,,,,"25/Sep/10 22:00;stack;Here is fix... remove stuff from regionsintransition on receipt of split message.  This will do for now but I think there are likely other holes in state transition probably around split since this is the one action the master does not control.  Plugging the holes is easier in new master.  Just have to find them.

Here is what patch does.  I'm testing it now.

{code}

M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
  Add region name to warning log message (w/o it message is no good).
M src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
  Add src of split message else need to deduce where it came from by looking
  elsewhere.
M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  Updated log messages to include region and where appropritate source
  server name; debug hard w/o
  Changed regionOnline and regionOffline to check for unexpected
  states and log warnings rather than proceed regardless.
  Added in fix for concurrent balance+split; split message now
  updates regionsintransition where previous it did not.
  Remove checkRegion method.  Its a reimplementation of
  what regionOnline and regionOffline do only less comprehensive
  regards what gets updated (this.regions + this.servers rather
  than this.regions, this.servers and regionsInTransition)
  That they were less comprehensive is root of this bug.
M src/main/java/org/apache/hadoop/hbase/master/HMaster.java
  Make the message about why we are not running balancer richer
  (print out how many reigons in transition and more of the
  regionsintrnasition list).
M src/main/java/org/apache/hadoop/hbase/executor/RegionTransitionData.java
  Javadoc and minor formatting.
{code};;;","27/Sep/10 21:02;stack;Committed after testing up on cluster.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WALReaderFSDataInputStream.getPos() fails if Filesize > MAX_INT,HBASE-3038,12475095,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,nspiegelberg,nspiegelberg,nspiegelberg,25/Sep/10 01:58,20/Nov/15 12:42,14/Jul/23 06:06,28/Sep/10 05:56,0.89.20100621,0.90.0,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"WALReaderFSDataInputStream.getPos() uses  this.in.available() to determine the actual length of the file.  Except that available() returns an int instead of a long.  Therefore, our current logic is broke when trying to read a split log > 2GB.",,eli,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-3042,"28/Sep/10 20:37;stack;3038-addendum.txt;https://issues.apache.org/jira/secure/attachment/12455872/3038-addendum.txt","28/Sep/10 00:35;nspiegelberg;HBASE-3038.patch;https://issues.apache.org/jira/secure/attachment/12455787/HBASE-3038.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26606,Reviewed,,,,Fri Nov 20 12:42:50 UTC 2015,,,,,,,,,,"0|i0hkg7:",100575,,,,,,,,,,,,,,,,,,,,,"25/Sep/10 02:00;nspiegelberg;Beautiful note in DFSClient.java
{code}
    /**
     * WARNING: This method does not work with files larger than 2GB.
     * Use getFileLength() - getPos() instead.
     */
    @Override
    public synchronized int available() throws IOException {
{code}
I plan to heed that warning and use introspection to fix this problem.;;;","25/Sep/10 17:46;kannanm;Excellent catch Nicolas!

All: just as an fyi, this is the exception/stack you'll run into because of this issue on large files in recovered.edits:

{code}
2010-09-22 16:05:43,939 INFO org.apache.hadoop.hbase.regionserver.HRegion: Replaying edits from hdfs://<xyz>:9000/HBASE/test_table/ce0cd6e5793564a4b1a75de83232701b/recovered.edits/0000000000020477687; minSeqId=20484537
2010-09-22 16:06:02,475 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: Error opening test_table,5dddddd8,1283714332727.ce0cd6e5793564a4b1a75de83232701b.
java.io.EOFException
at java.io.DataInputStream.readInt(DataInputStream.java:375)
at org.apache.hadoop.io.SequenceFile$Reader.readRecordLength(SequenceFile.java:1953)
at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1983)
at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1888)
at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1934)
at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:121)
at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:113)
at org.apache.hadoop.hbase.regionserver.HRegion.replayRecoveredEdits(HRegion.java:1982)
at org.apache.hadoop.hbase.regionserver.HRegion.replayRecoveredEdits(HRegion.java:1957)
at org.apache.hadoop.hbase.regionserver.HRegion.replayRecoveredEditsIfAny(HRegion.java:1915)
at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:344)
at org.apache.hadoop.hbase.regionserver.HRegionServer.instantiateRegion(HRegionServer.java:1479)
at org.apache.hadoop.hbase.regionserver.HRegionServer.openRegion(HRegionServer.java:1426)
at org.apache.hadoop.hbase.regionserver.HRegionServer$Worker.run(HRegionServer.java:1334)
at java.lang.Thread.run(Thread.java:619)
{code}
;;;","28/Sep/10 05:56;stack;Committed after running tests.  Looks good Nicolas.  Thanks for the patch.;;;","28/Sep/10 19:14;jdcryans;Nicolas, you are way ahead of us! We just figured we have been having the same issue for a while here, thanks a lot for the patch.;;;","28/Sep/10 20:37;stack;Had to add this to make it work on other hadoops (smile)... via J-D and Nicolas.;;;","07/Jan/11 18:33;stack;Nicolas:

Could we not do this reflection once only in constructor rather than per getPos lookup?

{code}
          try {
            Field fIn = FilterInputStream.class.getDeclaredField(""in"");
            fIn.setAccessible(true);
            Object realIn = fIn.get(this.in);
            Method getFileLength = realIn.getClass().
              getMethod(""getFileLength"", new Class<?> []{});
            getFileLength.setAccessible(true);
            long realLength = ((Long)getFileLength.
              invoke(realIn, new Object []{})).longValue();
            assert(realLength >= this.length);
            adjust = realLength - this.length;
          } catch(Exception e) {
{code};;;","07/Jan/11 21:23;nspiegelberg;this is only being called on the first invocation, which is the SequenceFile.Reader constructor.

{code}
         if (this.firstGetPosInvocation) {
           this.firstGetPosInvocation = false;
           ...
{code};;;","07/Jan/11 21:27;stack;@Nicolas Thanks for looking.  I should have looked closer.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When new master joins running cluster does ""Received report from unknown server -- telling it to STOP_REGIONSERVER...""",HBASE-3037,12475086,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,24/Sep/10 23:03,20/Nov/15 12:43,14/Jul/23 06:06,25/Sep/10 06:25,,,,,,,,,,,,0.90.0,,,,,,,0,,,Working on it.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/10 05:55;stack;3037-v2.txt;https://issues.apache.org/jira/secure/attachment/12455550/3037-v2.txt","24/Sep/10 23:59;stack;3037.txt;https://issues.apache.org/jira/secure/attachment/12455535/3037.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26605,,,,,Fri Nov 20 12:43:17 UTC 2015,,,,,,,,,,"0|i0hkfz:",100574,,,,,,,,,,,,,,,,,,,,,"24/Sep/10 23:59;stack;This patch seems to basically work.  Testing it more.;;;","25/Sep/10 05:55;stack;This stuff seems to be basically work up on the cluster.  I'm going to go ahead and commit.  Master should be roughly equivalent now to the old (but for all the extras new master brings). 

{code}
M src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
 On regionServerReport, if we get a report from an 'unknown' regionserver,
 we used to tell it stop itself.  Now, if 'unknown' server AND
 this master did not start the cluster, its joining the cluster, then
 treat the report as a regionServerStart and register the incoming
 server rather than tell it shutdown.
M src/main/java/org/apache/hadoop/hbase/master/HMaster.java
 Pass the freshClusterStartup flag to ServerManager.
 Add more executors for opening and closing.  On cluster startup a
 master shouldn't be bottleneck clearning the server opens.
 Expose the run-balancer method so can make it available in
 ServeAdmin.
M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
 Minor formatting and javadoc
M src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPCProtocolVersion.java
 Upped rpc version number because of new balancer addition (and because
 we didn't do it when we put in new master).
M src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
 Added balancer method.
M src/main/resources/hbase-default.xml
 Change how ofter we check in from every 3 seconds to every 5 seconds.
{code};;;","25/Sep/10 06:25;stack;Committed.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
avro tests failing up on hudson (pass locally),HBASE-3036,12475078,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,24/Sep/10 20:39,20/Nov/15 12:43,14/Jul/23 06:06,27/Sep/10 22:33,,,,,,,,,,,,0.90.0,,,,,,,0,,,,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26604,,,,,Fri Nov 20 12:43:36 UTC 2015,,,,,,,,,,"0|i0hkfr:",100573,,,,,,,,,,,,,,,,,,,,,"24/Sep/10 20:41;stack;Trying the passing of the TEST_UTIL.getConfiguration to the HBaseUtil.;;;","24/Sep/10 23:02;stack;This change may have worked.  Avro tests just passed on Hudson.  Lets let it run a few more times.;;;","27/Sep/10 22:33;stack;None of the recent hudson builds have had avro failures.  Closing.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CopyTable MR job named ""Copy Table"" in Driver",HBASE-3031,12474854,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,stack,jdcryans,jdcryans,22/Sep/10 19:02,20/Nov/15 12:42,14/Jul/23 06:06,19/Oct/10 17:37,,,,,,,,,,,,0.90.0,,,,,,,0,,,"The CopyTable MR job needs to change name, currently it requires passing quotes around it since it's ""Copy Table"". Also all the other names are lower case and without and white spaces.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/10 17:36;stack;3031.txt;https://issues.apache.org/jira/secure/attachment/12457577/3031.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26602,,,,,Fri Nov 20 12:42:28 UTC 2015,,,,,,,,,,"0|i0hken:",100568,,,,,,,,,,,,,,,,,,,,,"19/Oct/10 17:36;stack;Simple patch.  Verified it works:

{code}
stack:trunk Stack$ ./bin/hbase org.apache.hadoop.hbase.mapreduce.Driver
An example program must be given as the first argument.
Valid program names are:
  completebulkload: Complete a bulk data load.
  copytable: Export a table from local cluster to peer cluster
  export: Write table data to HDFS.
  import: Import data written by Export.
  importtsv: Import data in TSV format.
  rowcounter: Count rows in HBase table
stack:trunk Stack$ ./bin/hbase org.apache.hadoop.hbase.mapreduce.Driver copytable
Usage: CopyTable [--rs.class=CLASS] [--rs.impl=IMPL] [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] <tablename>

Options:
 rs.class     hbase.regionserver.class of the peer cluster
....
{code};;;","19/Oct/10 17:37;stack;Committed small patch.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The return code of many filesystem operations are not checked,HBASE-3030,12474849,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,dhruba,dhruba,dhruba,22/Sep/10 18:07,20/Nov/15 12:41,14/Jul/23 06:06,27/Sep/10 23:03,,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"The region server makes call delete/rename/mkdir calls to the FileSystem. These calls return true or false depending on whether the operation was performed successfully or not. Region server should check these return values, and either throw an exception or log it.",,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/10 18:19;dhruba;fsErrorCodeCheck.txt;https://issues.apache.org/jira/secure/attachment/12455284/fsErrorCodeCheck.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26601,Reviewed,,,,Fri Nov 20 12:41:04 UTC 2015,,,,,,,,,,"0|i0hkef:",100567,,,,,,,,,,,,,,,,,,,,,"22/Sep/10 18:08;dhruba;The handling of these errors become all the more important when the Hadoop Namenode fails over from primary to the standby.;;;","22/Sep/10 18:19;dhruba;Check the return status from the FileSystem.delete, rename and mkdir operations. In some cases log it and in other cases throw an Exception.

Can somebody pl review this patch and comment on whether my selection of code paths that throw exceptions are indeed valid?;;;","27/Sep/10 23:03;stack;Committed.  Thanks for the nice patch Dhruba.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"No basescanner means no GC'ing of split, offlined parent regions",HBASE-3028,12474793,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,22/Sep/10 06:41,20/Nov/15 12:41,14/Jul/23 06:06,24/Sep/10 05:47,,,,,,,,,,,,0.90.0,,master,,,,,0,,,I need to add back cleanup of split parents probably as a Chore in master.  Can use new MetaReader code.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/10 05:59;stack;3028-v2.txt;https://issues.apache.org/jira/secure/attachment/12455354/3028-v2.txt","24/Sep/10 05:23;stack;3028-v4.txt;https://issues.apache.org/jira/secure/attachment/12455464/3028-v4.txt","22/Sep/10 23:56;stack;3028.txt;https://issues.apache.org/jira/secure/attachment/12455330/3028.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26599,,,,,Fri Nov 20 12:41:59 UTC 2015,,,,,,,,,,"0|i0hke7:",100566,,,,,,,,,,,,,,,,,,,,,"22/Sep/10 06:42;stack;Or, look into having the regionserver do it after its completed a compaction (sloppy scanner process might be better way to go);;;","22/Sep/10 23:56;stack;Adding back a basescanner that looks for parents to clean.  This attached patch is not yet complete.  Its mostly rejiggering the 0.20 scanners to work in new context.;;;","23/Sep/10 05:59;stack;Adding unit tests.  Need to add a few more then cluster test.;;;","24/Sep/10 05:23;stack;Here is a complete patch:

{code}
M src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
  Test of new basescanner (now called catalogjanitor).  Mocks up
  a 'Server' and a 'MasterServies'.  Tests the check for references
  and gc'ing of let-go of offlined parent regions.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
  (removeRegionFromMeta): Removed.  Users should be going via
  MetaEditor instead.
M src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
  (isSplitParent): Added.
M src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
  Minor formatting
M src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java
  New chore that runs every 5 minutes.  It scans .META. and if
  it finds a parent row that is offlined and marked as product of
  a split, it will check daughters to see if they have references.
  If none, the row is removed and its content in FS is cleaned up.
  This is the old 0.20 BaseScanner brought forward into TRUNK
  only now it does this one check only (The fixup of dropped
  splits is done as part of server shutdown handling in TRUNK).
M src/main/java/org/apache/hadoop/hbase/master/HMaster.java
  Added in start and stop of new CatalogJanitor Chore.  Also
  made a stopServiceThreads to match startServiceThreads.  I
  moved into this new method from 'run' the stopping of all threads
  started in startServiceThreads.
M src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java
  Refactored fullScan so it now works by taking a Visitor.  Default
  fullScan does as it used to only via a Visitor now.  Adding in
  this facility means I can use the fullScan code when doing the
  catalog janitorial chore.
M src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java
  (deleteDaughterReferenceInparent): Added.
M src/main/java/org/apache/hadoop/hbase/util/Threads.java
  Added a setDaemonThreadRunning override that gets the name to use
  from the passed thread.
{code};;;","24/Sep/10 05:42;stack;Did some cluster testing.  When it first ran, it did a bunch of this:

{code}
2010-09-24 05:29:33,011 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughter usertable,user992373275,1285090830082.d821675081aebe369b1fb9c11d48b012. reference splitA from usertable,user992373275,1285019557053.4ae8a9836d6ba0e97e6017547bac9e6c. .META.
2010-09-24 05:29:33,014 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughter usertable,user996181626,1285090830082.77acffdf0fbb0128ef6a4afd800906ab. reference splitB from usertable,user992373275,1285019557053.4ae8a9836d6ba0e97e6017547bac9e6c. .META.
2010-09-24 05:29:33,014 INFO org.apache.hadoop.hbase.master.CatalogJanitor: Deleting region usertable,user992373275,1285019557053.4ae8a9836d6ba0e97e6017547bac9e6c. because daughter splits no longer hold references
2010-09-24 05:29:33,014 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: DELETING region hdfs://sv2borg180:20000/hbase/usertable/4ae8a9836d6ba0e97e6017547bac9e6c
2010-09-24 05:29:33,033 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted region usertable,user992373275,1285019557053.4ae8a9836d6ba0e97e6017547bac9e6c. from META
2010-09-24 05:29:33,033 INFO org.apache.hadoop.hbase.master.CatalogJanitor: Scanned 1089 catalog row(s) and gc'd 375 unreferenced parent region(s)
{code}

I let it run a while.  Checked table is wholesome still and it passes.

Going to commit.;;;","24/Sep/10 05:47;stack;Committed on TRUNK;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fixup of ""missing"" daughters on split is too aggressive",HBASE-3026,12474791,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,22/Sep/10 06:28,20/Nov/15 12:41,14/Jul/23 06:06,22/Sep/10 18:44,,,,,,,,,,,,0.90.0,,master,,,,,0,,,"There is a bug in how we check for whether the daughters mentioned in parent region are present in .META.  The check is done when we are processing a server shutdown.  We're making the mistake of checking for presence of the daughter in the list of regions that used to live on the crashed server BUT fact of the matter is is that the daughter could just as well be rebalanced to another server.

The upshot is that we are inserting into .META. and trying to assign regions that are already assigned.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/10 06:39;stack;3026.txt;https://issues.apache.org/jira/secure/attachment/12455240/3026.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26597,,,,,Fri Nov 20 12:41:05 UTC 2015,,,,,,,,,,"0|i0hkdr:",100564,,,,,,,,,,,,,,,,,,,,,"22/Sep/10 06:39;stack;Here''s a patch that goes back to the META to check if daughters are present.

Testing it now.;;;","22/Sep/10 18:44;stack;Committed.  Ran on cluster and cleaned it cleaned up the mess being made by incorrect daughter insertion/reassign.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE processing server crash in MetaEditor.addDaughter,HBASE-3024,12474762,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,21/Sep/10 22:21,20/Nov/15 12:41,14/Jul/23 06:06,21/Sep/10 22:24,,,,,,,,,,,,0.90.0,,,,,,,0,,,"{code}
2010-09-21 22:11:01,812 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Reassigning the 116 region(s) that sv2borg183,60020,1285106817665 was carrying.
2010-09-21 22:11:01,813 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Fixup; missing daughter [B@4b0bc3c9
2010-09-21 22:11:01,818 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN
java.lang.NullPointerException
        at org.apache.hadoop.hbase.catalog.MetaEditor.addDaughter(MetaEditor.java:102)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.fixupDaughter(ServerShutdownHandler.java:156)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.fixupDaughters(ServerShutdownHandler.java:137)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:120)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:150)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/10 22:23;stack;3024.txt;https://issues.apache.org/jira/secure/attachment/12455197/3024.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26596,,,,,Fri Nov 20 12:41:20 UTC 2015,,,,,,,,,,"0|i0hkdj:",100563,,,,,,,,,,,,,,,,,,,,,"21/Sep/10 22:23;stack;ServerInfo doing fixup on a daughter can legitimately be null; we even check for it only later, when we go to log what we've just done, we've lost the serverinfo null check.  This patch adds it back.;;;","21/Sep/10 22:24;stack;Committed small patch.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE processing server crash in MetaReader. getServerUserRegions,HBASE-3023,12474760,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,21/Sep/10 21:58,20/Nov/15 12:43,14/Jul/23 06:06,25/Sep/10 06:54,,,,,,,,,,,,0.90.0,,,,,,,0,,,"{code}
2010-09-21 18:28:30,856 ERROR
org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while
processing event M_SERVER_SHUTDOWN
java.lang.NullPointerException
       at org.apache.hadoop.hbase.catalog.MetaReader.getServerUserRegions(MetaReader.java:413)
       at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:106)
       at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:150)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)
{code}

Problem is that there was no server set in .META. yet for a particular row but we went ahead tried to use the server as though it non-null.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/10 21:59;stack;3023.txt;https://issues.apache.org/jira/secure/attachment/12455194/3023.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26595,,,,,Fri Nov 20 12:43:40 UTC 2015,,,,,,,,,,"0|i0hkdb:",100562,,,,,,,,,,,,,,,,,,,,,"21/Sep/10 21:59;stack;Patch that allows that the return from metaRowToRegionPair may contain a null address (updated javadoc to match).;;;","25/Sep/10 06:54;stack;Was applied a while back.  Closing.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More log pruning,HBASE-3017,12474622,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,20/Sep/10 18:49,20/Nov/15 12:44,14/Jul/23 06:06,20/Sep/10 19:05,,,,,,,,,,,,0.90.0,,,,,,,0,,,"This issue covers some tightening up of log messages; as is all of the zk noise tends to overwhelm.

For example, zkwatcher logs a generic ""This event happened in zk with path X and event type Y"" but just after, there will be a log from the handler of this zk event with this subsequent log more descriptive.  This change would make zkwatcher log at INFO by default rather than DEBUG cutting down on logging content (re-enabling DEBUG is easy to do if needed).",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26592,,,,,Fri Nov 20 12:44:10 UTC 2015,,,,,,,,,,"0|i0hkc7:",100557,,,,,,,,,,,,,,,,,,,,,"20/Sep/10 19:05;stack;Committed

{code}
M conf/log4j.properties
  Make ZKW log at INFO-level
M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  Log message cleanup.
M src/main/java/org/apache/hadoop/hbase/master/HMaster.java
  Remove excessive hostname+port qualifier on master for zk messages
M src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
  Log message cleanup
M src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
  Format the ServerMonitor message.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
  Remove excessive hostname on zk message id; just add port
M src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java
  Cleanup of messages.
{code};;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"recovered.edits files not deleted if it only contain edits that have already been flushed; hurts perf for all future opens of the region",HBASE-3015,12474463,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,kannanm,kannanm,18/Sep/10 02:46,20/Nov/15 12:42,14/Jul/23 06:06,20/Sep/10 18:43,,,,,,,,,,,,0.90.0,,,,,,,0,,,"On RS crash, master processes the RS's logs, splits them into per region log files, and puts them in recovered.edits sub-directory of the region. 

It may be the case the some of these files contain only old edits that have already been flushed, and don't need to be reapplied again. However, in this case the file is not deleted, and stays in recovered.edits for ever. This will slow down every future ""open"" of this region, as the region will unnecessarily spend time processing this file.

In  HRegion.java:replaceRecoveredEditsIfAny(), the code below checks if the file we just processed contain any edits that were applied, and in that case flushes the memstore into which things were being recovered. 

{code}
  if (seqid > minSeqId) {
      // Then we added some edits to memory. Flush and cleanup split edit files.
      internalFlushcache(null, seqid);
      for (Path file: files) {
        if (!this.fs.delete(file, false)) {
          LOG.error(""Failed delete of "" + file);
        } else {
          LOG.debug(""Deleted recovered.edits file="" + file);
        }
      }
    }
{code}

But it is not clear why the 'for' loop to clean up the recovered.edits file is inside the if check.


",,dhruba,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/10 18:23;stack;3015.txt;https://issues.apache.org/jira/secure/attachment/12454949/3015.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26591,Reviewed,,,,Fri Nov 20 12:42:23 UTC 2015,,,,,,,,,,"0|i0hkbr:",100555,,,,,,,,,,,,,,,,,,,,,"18/Sep/10 18:23;stack;It looks like a silly mistake (on my part I believe).  This patch look ok to you Kannan?;;;","20/Sep/10 17:39;kannanm;Stack: Patch looks good.;;;","20/Sep/10 18:43;stack;Committed (Thanks for review Kannan).;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TOF doesn't take zk client port for remote clusters,HBASE-3012,12474410,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,jdcryans,jdcryans,jdcryans,17/Sep/10 17:21,20/Nov/15 12:42,14/Jul/23 06:06,27/Oct/10 23:18,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Currently we are only able to specify the ZK ensemble and root znode for the remote cluster in TOF, we should also be able to give the client port (like in replication). This will require a change in CopyTable's command line arguments too.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26590,Reviewed,,,,Fri Nov 20 12:42:29 UTC 2015,,,,,,,,,,"0|i0hkbb:",100553,,,,,,,,,,,,,,,,,,,,,"23/Oct/10 01:05;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1077/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Does exactly what this jira is about, and refactors all the cluster key processing into ZKUtil.


This addresses bug HBASE-3012.
    http://issues.apache.org/jira/browse/HBASE-3012


Diffs
-----

  /trunk/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java 1026525 
  /trunk/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java 1026525 
  /trunk/src/main/java/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.java 1026525 
  /trunk/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java 1026525 
  /trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java 1026525 

Diff: http://review.cloudera.org/r/1077/diff


Testing
-------


Thanks,

Jean-Daniel


;;;","23/Oct/10 06:41;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1077/#review1633
-----------------------------------------------------------


Looks great J-D but no +1 until you've added a unit test that proves your parse stuff in ZKUtil does the right thing when single ensemble member, many ensemble members, empty port spec., etc. (No need to post the patch with unit test -- just commit)


/trunk/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
<http://review.cloudera.org/r/1077/#comment5492>

    Minor spacing issue here?
    


- stack



;;;","27/Oct/10 23:18;jdcryans;Committed to trunk with a new test in TestZooKeeper.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't start/stop/start... cluster using new master,HBASE-3010,12474362,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,stack,stack,17/Sep/10 08:06,20/Nov/15 12:41,14/Jul/23 06:06,18/Sep/10 00:51,,,,,,,,,,,,0.90.0,,master,,,,,0,,,"Currently you might start a small cluster the first time on TRUNK -- i.e. new master -- but second time you do the startup you run into a couple of interesting issues:

+ The old root-region-location is still in place. It gets cleaned later but for a while on startup it does not have the 'right' address.
+ Regionserver (or a client) on startup creates a catalogtracker, a class that notices changes in meta tables keeping up catalog table locations.  Starting the catalogtracker results in a check for current catalog locations.  As part of this process, since root-region-location ""exists"", catalogtracker tries to verify root's location by doing a noop against root host, only, to do this it needs to do the initial rpc proxy setup.  It can so happen that the old root address was that of the current regionserver trying to initialize so we'll be trying to connect to ourself to verify root location ONLY, we're doing this before we've setup the rpcserver and handlers -- so we block, and as it happens there is no timeout on proxy setup (Todd ran into this yesterday, I ran into it today -- its easy to manufacture).
+ So regionserver can't progress.  Meantime the master can't progress because there are no regionservers checking in.  And you can't shut it down because we're not looking at the right 'stop' flag",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26588,Reviewed,,,,Fri Nov 20 12:41:07 UTC 2015,,,,,,,,,,"0|i0hkb3:",100552,,,,,,,,,,,,,,,,,,,,,"17/Sep/10 08:43;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/873/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Patch changes catalogtracker to not connect to root on start.  Instead, wait on kick from zk before going after root or meta.  This change doesn't address the case where a RS can get stuck on itself trying to connect to an RPC that is not yet running.  Rather it sidesteps (We should come back and do something about non-timeout when setting up proxy since its possible. I'll file an issue on it).  Patch includes other fixups, not all cosmetic.

M src/test/java/org/apache/hadoop/hbase/master/TestActiveMasterManager.java
  Add a test for case where master comes up and up in zk the
  master znode contains our address
D src/test/java/org/apache/hadoop/hbase/master/TestMinimumServerCount.java
  Removed test of something we no longer do; wait on an explicit number
  of regions to come in before we'll go ahead w/ master startup.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
  Swapped order in which we do some of the startup (Cosmetic)
M src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
  Javadoc
M src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
  Removed minimumServerCount.  Seems bad predicating master startup
  on N RS's coming in.
  Renamed method numServers as countOfRegionServers and made it protected.
  Removed other unused methods.
  Redid waitForMinServers as waitForRegionServers... where we just
  hang around until count of regionservers stabilizes.  TODO: improve
M src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java
  Handle case where the current master znode has our address; in this
  case we can hurry up the expiration by deleting the znode.
M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  Minor formatting
M src/main/java/org/apache/hadoop/hbase/master/HMaster.java
  Renamed clusterStarter as freshClusterStartup.  Predicate this boolean
  off the count of regionservers.  If 0, then fresh cluster start.  Else
  do special handling (TODO).
  Edit on HMaster constructor comments.
  Moved some code out of Master constructor into stackIfBackupMaster method
  If aborting set stop flag.
M src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java
  Removed unused imports.
M src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
  Make catalogtracker lazy about getting metalocation....don't do it
  on start.


This addresses bug hbase-3010.
    http://issues.apache.org/jira/browse/hbase-3010


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java 2bcd5d0 
  src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java 87fe9cd 
  src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 690f78c 
  src/main/java/org/apache/hadoop/hbase/master/HMaster.java c1b80eb 
  src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java c675db9 
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java 498650f 
  src/main/java/org/apache/hadoop/hbase/master/ServerManager.java 528bb9d 
  src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 1ec7f4e 
  src/test/java/org/apache/hadoop/hbase/master/TestActiveMasterManager.java 030bc12 
  src/test/java/org/apache/hadoop/hbase/master/TestMinimumServerCount.java d6f2c02 

Diff: http://review.cloudera.org/r/873/diff


Testing
-------

Can now start/stop cluster repeatedly.


Thanks,

stack


;;;","17/Sep/10 23:47;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/873/#review1267
-----------------------------------------------------------

Ship it!



src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java
<http://review.cloudera.org/r/873/#comment4312>

    hrm, I guess that's a good idea, but something seems a little strange about this :)



src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.cloudera.org/r/873/#comment4313>

    this should probably move down until after we're the active master


- Todd



;;;","18/Sep/10 00:51;stack;Thanks for review Todd.  Committed.;;;","18/Sep/10 01:06;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-09-17 16:25:15, Todd Lipcon wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java, line 142
bq.  > <http://review.cloudera.org/r/873/diff/1/?file=11929#file11929line142>
bq.  >
bq.  >     hrm, I guess that's a good idea, but something seems a little strange about this :)

Yeah, this is a little 'bold' but trying to think around it, i couldn't see issue w/ it, whereas not doing it is going to frustrate as restart will have this minute or so stall while we waiting on the znode expire.  I'd say its good for now and I suppose we'll see later if it becomes a prop.


bq.  On 2010-09-17 16:25:15, Todd Lipcon wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/master/HMaster.java, line 222
bq.  > <http://review.cloudera.org/r/873/diff/1/?file=11931#file11931line222>
bq.  >
bq.  >     this should probably move down until after we're the active master

sure... will do on commit.


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/873/#review1267
-----------------------------------------------------------



;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memstore.updateColumnValue passes wrong flag to heapSizeChange,HBASE-3008,12474336,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,16/Sep/10 22:51,20/Nov/15 12:41,14/Jul/23 06:06,06/Oct/10 21:36,,,,,,,,,,,,0.89.20100924,0.90.0,,,,,,0,,,"Memstore.updateColumnValue passes the wrong flag to heapSizeChange, making it so that the size keeps growing while the actual size is probably the same. For example, in our production environment we see tables that only take ICVs doing flushes of a few KBs when it thinks there's 64MB in:

{noformat}
Started memstore flush for region somecountingtable,,1248719998664.121282795. Current region memstore size 64.0
Added hdfs://borg9:9000/hbase/somecountingtable/121282795/counter/3564459650504019443, entries=905, sequenceid=72504291507, 
memsize=183.3k, filesize=18.5k to somecountingtable,,1248719998664.121282795
{noformat}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/10 22:53;jdcryans;HBASE-3008.patch;https://issues.apache.org/jira/secure/attachment/12454812/HBASE-3008.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26586,,,,,Fri Nov 20 12:41:05 UTC 2015,,,,,,,,,,"0|i0hkav:",100551,,,,,,,,,,,,,,,,,,,,,"16/Sep/10 22:53;jdcryans;This is the patch that Ryan wrote and that fixes the issue.;;;","18/Sep/10 15:55;stack;+1;;;","18/Sep/10 18:12;tlipcon;Unit test? This seems pretty easy to test.;;;","24/Sep/10 18:02;jdcryans;I'm putting this in the next 0.89 without a test, will write test later for 0.90;;;","06/Oct/10 01:41;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/965/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

a problem we observed with HBASE-3008 + HBASE-2997, that memstore sizes can go negative preventing us from flushing them even if there is data within.  This is a data loss bug.


This addresses bug HBASE-3008.
    http://issues.apache.org/jira/browse/HBASE-3008


Diffs
-----

  trunk/CHANGES.txt 1004867 
  trunk/src/main/java/org/apache/hadoop/hbase/KeyValue.java 1004867 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java 1004867 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java 1004867 

Diff: http://review.cloudera.org/r/965/diff


Testing
-------


Thanks,

Ryan


;;;","06/Oct/10 21:36;ryanobjc;this was commited as part of r1000276 as well as r1005261

;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading compressed HFile blocks causes way too many DFS RPC calls severly impacting performance,HBASE-3006,12474298,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,kannanm,kannanm,kannanm,16/Sep/10 16:40,20/Nov/15 12:42,14/Jul/23 06:06,17/Sep/10 03:57,0.89.20100621,,,,,,,,,,,0.89.20100924,0.90.0,,,,,,0,,,"On some read perf tests, we noticed several perf outliers (10 second plus range). The rows were large (spanning multiple blocks, but still the numbers didn't add up). We had compression turned on.

We enabled DN clienttrace logging,
log4j.logger.org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace=DEBUG

and noticed lots of 516 byte reads at the DN level, several of them at the same offset in the block.

{code}
2010-09-16 09:28:32,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:38713, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 203000
2010-09-16 09:28:32,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:40547, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 119000
2010-09-16 09:28:32,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:40650, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 149000
2010-09-16 09:28:32,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:40861, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 135000
2010-09-16 09:28:32,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:41129, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 117000
2010-09-16 09:28:32,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:41691, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 148000
2010-09-16 09:28:32,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:42881, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 114000
2010-09-16 09:28:32,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:49511, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 153000
2010-09-16 09:28:32,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:51158, bytes: 3096, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.\
30.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 139000
{code}

This was strange coz our block size was 64k, and on disk block size after compression should generally have been around 6k.

Some print debugging at the HFile and BoundedRangeFileInputStream (which is wrapped by createDecompressionStream) revealed the following:

We are trying to read 20k from DFS @ HFile layer. The BounderRangeFileInputStream instead reads several header bytes 1 byte at a time, and then reads a 11k chunk and later a 9k chunk.

{code}
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 34386760 compressedSize = 20711 decompressedSize = 92324
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386760; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386761; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386762; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386763; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386764; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386765; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386766; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386767; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386768; bytes: 11005
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397773; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397774; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397775; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397776; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397777; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397778; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397779; bytes: 1
2010-09-16 09:21:27,913 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397780; bytes: 1
2010-09-16 09:21:27,913 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397781; bytes: 9690
{code}

Seems like it should be an easy fix to prefetch the compressed size... rather than incremental fetches.
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/10 22:12;kannanm;HBASE-3006-2.txt;https://issues.apache.org/jira/secure/attachment/12454810/HBASE-3006-2.txt","16/Sep/10 19:01;kannanm;HBASE-3006.txt;https://issues.apache.org/jira/secure/attachment/12454790/HBASE-3006.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26584,Reviewed,,,,Fri Nov 20 12:42:11 UTC 2015,,,,,,,,,,"0|i0hkaf:",100549,,,,,,,,,,,,,,,,,,,,,"16/Sep/10 17:01;stack;@Kannan You are my new hero.  I love the debug trace above.  You think though this will add up to the 10 seconds?;;;","16/Sep/10 17:23;kannanm;Stack: This was just a snippet of the trace. Some rows span a lot more blocks.

I am now looking at this in hfile/Compression.java:

{code}
public InputStream createDecompressionStream(
        InputStream downStream, Decompressor decompressor,
        int downStreamBufferSize) throws IOException {
      CompressionCodec codec = getCodec();
      // Set the internal buffer size to read from down stream.
      if (downStreamBufferSize > 0) {
        Configurable c = (Configurable) codec;
        c.getConf().setInt(""io.file.buffer.size"", downStreamBufferSize);
      }
      CompressionInputStream cis =
          codec.createInputStream(downStream, decompressor);
      BufferedInputStream bis2 = new BufferedInputStream(cis, DATA_IBUF_SIZE);
      return bis2;

    }
{code}

Off-hand don't understand all the params to the various Stream abstractions :) I tried passing the
compressed size of the block to above function instead of the current value 0 (for downStreamBufferSize) but 
that didn't do it. Still digging, but if you know off-hand what setting to change, let me know.;;;","16/Sep/10 17:33;kannanm;stack: To clarify regarding your: 

<<I love the debug trace above. You think though this will add up to the 10 seconds? >>

The snippet was for a much smaller row. The outliers we saw were on larger blocks. But this should just help even the small row case. Basically the cost of missing any block in the block cache when compression is turned on was quite steep right now (will do about 5x-10x more DFS RPCs than needed).;;;","16/Sep/10 18:33;kannanm;Wrapping a BufferedInputStream around did it.

{code}
2010-09-16 11:30:40,842 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 32060739 compressedSize = 6197 decompressedSize = 65800
2010-09-16 11:30:40,842 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### reading @ 32060739; bytes: 6197
2010-09-16 11:30:40,843 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 32066936 compressedSize = 6083 decompressedSize = 65658
2010-09-16 11:30:40,843 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### reading @ 32066936; bytes: 6083
2010-09-16 11:30:40,844 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 32073019 compressedSize = 5003 decompressedSize = 65708
2010-09-16 11:30:40,844 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### reading @ 32073019; bytes: 5003
2010-09-16 11:30:40,844 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 32078022 compressedSize = 4834 decompressedSize = 65700
2010-09-16 11:30:40,844 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### reading @ 32078022; bytes: 4834
2010-09-16 11:30:40,845 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 32082856 compressedSize = 6137 decompressedSize = 65566
2010-09-16 11:30:40,845 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### reading @ 32082856; bytes: 6137
2010-09-16 11:30:40,846 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 32088993 compressedSize = 4727 decompressedSize = 65766
2010-09-16 11:30:40,846 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### reading @ 32088993; bytes: 4727
2010-09-16 11:30:40,876 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 32093720 compressedSize = 5025 decompressedSize = 65760
2010-09-16 11:30:40,876 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### reading @ 32093720; bytes: 5025
{code}

and perf is screaming back again.

We can debate about whether the buffer size for the BufferInputStream should be the compressedSize (read everything) or some uniform sized chunks like 16k to play well with JVM. But basically the fix seems to work really well.

Will post a patch shortly.



;;;","16/Sep/10 19:01;kannanm;Patch submitted. Fixes perf problems we were seeing.

Averages dropped from 1-2 seconds to 100ms for my test runs. No outliers > 10 seconds. Previously we had many, and some which took 40-50 seconds.

Yet to run unit tests (will report back on that once it completes).

Thoughts on what BufferedInputStream's buffer setting would be? The disadvantage of using compressedSize (as i n provided patch) might lots of varying length allocations. Does JVM prefer allocations to all be of a uniform size? If we set a different value, say 16k, it'll incur more DFS RPCs (but at least not silly 1 byte RPCs).;;;","16/Sep/10 19:59;stack;Patch looks great to me (This code was robbed from tfile a long time back IIRC).

My sense is that though this a short-lived allocation, allocating compressedSize is overdoing it and as you suggest, it can vary, possibly widely.  If a cell was large, then we could have an hfile much larger than 64k.  A buffer of 8 or 16k would just as well save against the 1 byte rpc reads.  If you agree, I can add this in on commit (allocate a 16k buffer by default)?;;;","16/Sep/10 20:46;tlipcon;Rather than adding buffering, can we just avoid using the random-access API to DFSInputStream? If you use the other APIs, you will take advantage of the internal buffering of DFSInputStream instead of double buffering, and it should be faster.;;;","16/Sep/10 20:54;pranavkhaitan;Looks good to me.. nice catch Kannan.. This will significantly improve read durations;;;","16/Sep/10 21:11;kannanm;Caught up with Todd in IRC. Avoiding double buffering sounds good. But not clear what the alternate DFS api is.

I am planning to try with min(16k, compressedSize);;;;","16/Sep/10 22:01;ryanobjc;if the objects are short lived, it doesnt matter the object allocation size.  If they are used only for the lifecycle of reading a single block, i'd say size it to the compressed Size.

;;;","16/Sep/10 22:10;kannanm;For our test case, min(16k, compressedSize) will be a no-op since our 
compressed sizes were almost always smaller than 16k. In our internal
branch, we're planning to go with 64k. Thought, for large objects, 16k 
batching might be too small. Will upload the new patch.
;;;","17/Sep/10 03:57;stack;Committed.  Thanks for nice fix Kannan (I left it at 64k -- use the HFile.DEFAULT_BLOCKSIZE define instead -- thinking that less rpc'ing is a better saving than a bit of memory in local heap).  ;;;","24/Sep/10 18:23;jdcryans;Adding to the latest 0.89;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassSize constants dont use 'final' ,HBASE-3003,12474231,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,15/Sep/10 23:45,20/Nov/15 12:42,14/Jul/23 06:06,22/Sep/10 23:53,,,,,,,,,,,,0.90.0,,,,,,,0,,,constants should really be final.  things would be confusing if they changed later.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/10 23:46;ryanobjc;HBASE-3003.txt;https://issues.apache.org/jira/secure/attachment/12454719/HBASE-3003.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26582,,,,,Fri Nov 20 12:42:58 UTC 2015,,,,,,,,,,"0|i0hk9r:",100546,,,,,,,,,,,,,,,,,,,,,"22/Sep/10 23:53;ryanobjc;committed this small patch;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
rolling-restart.sh shouldn't rely on zoo.cfg,HBASE-2998,12474127,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,jdcryans,jdcryans,14/Sep/10 21:53,20/Nov/15 12:42,14/Jul/23 06:06,22/Oct/10 19:56,,,,,,,,,,,,0.90.0,,,,,,,0,,,"I tried the rolling-restart script on our dev environment, which is configured with zoo.cfg for zookeeper, and it worked pretty well. Then I tried it on our MR cluster, which doesn't have a zoo.cfg, and we suffered some downtime (no biggie tho, nothing critical was running). When the script calls this line:

{code}
bin/hbase zkcli stat $zmaster
{code}

It directly runs a ZooKeeperMain which isn't modified to read from the HBase configuration files. What happens next if ZK isn't running on the master node is that it receives a ConnectionRefused, ignores it, procedes to restart the master (which waits on the znode), and the starts restarting the region servers. They can't shutdown properly under 60 seconds, since they need a master, so they get killed. What follows is pretty ugly and pretty much requires a whole restart.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/10 15:56;stack;2998.txt;https://issues.apache.org/jira/secure/attachment/12457672/2998.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26581,Reviewed,,,,Fri Nov 20 12:42:59 UTC 2015,,,,,,,,,,"0|i0hk8n:",100541,,,,,,,,,,,,,,,,,,,,,"05/Oct/10 22:36;stack;This has to work.  Making it critical.;;;","20/Oct/10 05:36;stack;So, ZKMain takes a -server argument which is host+port.  I was thinking of doing something like the following in hbase so when passed zkcli we actually passed in the -server argument with the host+port read from hbase Configuration (or from zoo.cfg if present):

{code}
Index: bin/hbase
===================================================================
--- bin/hbase   (revision 1024523)
+++ bin/hbase   (working copy)
@@ -262,7 +262,8 @@
     HBASE_OPTS=""$HBASE_OPTS $HBASE_ZOOKEEPER_OPTS""
   fi
 elif [ ""$COMMAND"" = ""zkcli"" ] ; then
-  CLASS='org.apache.zookeeper.ZooKeeperMain'
+  SERVERPORT=`""$bin""/hbase org.apache.hadoop.hbase.zookeeper.ZKServerTool -hostport | grep '^ZK hostport:' | sed 's,^ZK hostport:,,'` 
+  CLASS='org.apache.zookeeper.ZooKeeperMain -server ${SERVERPORT}'
 elif [ ""$COMMAND"" = ""classpath"" ] ; then
   echo $CLASSPATH
   exit 0
{code}

I need to mangle the ZKServerTool some to output what I want.;;;","20/Oct/10 15:56;stack;Not finished yet....;;;","21/Oct/10 00:55;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1057/#review1594
-----------------------------------------------------------


Looking good!


trunk/src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java
<http://review.cloudera.org/r/1057/#comment5394>

    


- Jonathan



;;;","21/Oct/10 08:56;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1057/
-----------------------------------------------------------

(Updated 2010-10-21 01:54:34.658192)


Review request for hbase, Jean-Daniel Cryans and Jonathan Gray.


Changes
-------

New patch includes faster assign of regions on startup (Uses async create/exists-set-watcher).  Getting this working helps w/ rolling restart tests.  Assign and watcher set for 2k regions runs fast now... used to be 90 seconds for 2k regions over 10 servers ... now its a matter of seconds for total bulk assign of all regions in just over a minute.

This patch is not yet ready.  I need to test more.


Summary
-------

Fix 'hbase zkcli' so it reads zk ensemble location from hbase config/zoo.cfg.  This fixes rolling restart.  Patch also includes fix so rolling restarts work on new master.

A src/test/java/org/apache/hadoop/hbase/zookeeper/TestZooKeeperMainServerArg.java
  Test for new TZMSA class.
M src/main/java/org/apache/hadoop/hbase/zookeeper/ZKServerTool.java
  Minor edit of javadoc.
A src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperMainServerArg.java
  Tool to emit what ZooKeeperMain wants for a server argument.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
  (isAbort): Added.
M src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java
  Shutdown hook now needs to startup region shutdowns since  new
  master changed how shutdown sequence runs.
M src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
  Don't do opens if server is stopped.
M src/main/java/org/apache/hadoop/hbase/master/handler/OpenedRegionHandler.java
  Minor formatting.
M bin/hbase
  Run new ZKMSA tool to figure '-server host:port' to pass ZKM
M bin/hbase-daemon.sh
  Make default wait be longer.


This addresses bug hbase-2998.
    http://issues.apache.org/jira/browse/hbase-2998


Diffs (updated)
-----

  trunk/bin/hbase 1025815 
  trunk/bin/hbase-daemon.sh 1025815 
  trunk/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java 1025815 
  trunk/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java 1025815 
  trunk/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java 1025815 
  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1025815 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/OpenedRegionHandler.java 1025815 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java 1025815 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java 1025815 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKServerTool.java 1025815 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java 1025815 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java 1025815 

Diff: http://review.cloudera.org/r/1057/diff


Testing
-------


Thanks,

stack


;;;","21/Oct/10 08:58;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-10-20 17:54:04, Jonathan Gray wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java, line 103
bq.  > <http://review.cloudera.org/r/1057/diff/1/?file=15040#file15040line103>
bq.  >
bq.  >
bq.  
bq.  Jonathan Gray wrote:
bq.      Not sure where my comment went :)  Just wondering if in the rolling restart case, we aren't aborting, right?
bq.      
bq.      Is the stop() now a hard stop and that's why we need to close regions first?

Looking into this more --prompted by your comment -- this extra callout to shut down user regions in shutdown hook shouldn't be needed.  Something else is going on.  I'm digging in.

Right, we are not aborting when rolling restart.  We are intercepting the jvm kill signal.


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1057/#review1594
-----------------------------------------------------------



;;;","22/Oct/10 19:00;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1057/
-----------------------------------------------------------

(Updated 2010-10-22 11:59:16.222128)


Review request for hbase, Jean-Daniel Cryans and Jonathan Gray.


Changes
-------

Here is updated patch. I've been testing it up on cluster.  It fixes a bunch of things that rolling restart unearths...but there is still work to do.  Meantime, this patch is growing beyond scope of the JIRA so would like to get it in in its current state.

Includes consideration of Jon's last review -- shutdown handler didn't need
amending afterall.

Includes refactor of master run because failure during processing of failover
was having the master exit but not go down -- rpc server was left up.


Summary
-------

Fix 'hbase zkcli' so it reads zk ensemble location from hbase config/zoo.cfg.  This fixes rolling restart.  Patch also includes fix so rolling restarts work on new master.

A src/test/java/org/apache/hadoop/hbase/zookeeper/TestZooKeeperMainServerArg.java
  Test for new TZMSA class.
M src/main/java/org/apache/hadoop/hbase/zookeeper/ZKServerTool.java
  Minor edit of javadoc.
A src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperMainServerArg.java
  Tool to emit what ZooKeeperMain wants for a server argument.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
  (isAbort): Added.
M src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java
  Shutdown hook now needs to startup region shutdowns since  new
  master changed how shutdown sequence runs.
M src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
  Don't do opens if server is stopped.
M src/main/java/org/apache/hadoop/hbase/master/handler/OpenedRegionHandler.java
  Minor formatting.
M bin/hbase
  Run new ZKMSA tool to figure '-server host:port' to pass ZKM
M bin/hbase-daemon.sh
  Make default wait be longer.


This addresses bug hbase-2998.
    http://issues.apache.org/jira/browse/hbase-2998


Diffs (updated)
-----

  trunk/bin/hbase 1026448 
  trunk/bin/hbase-daemon.sh 1026448 
  trunk/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java 1026448 
  trunk/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java 1026448 
  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1026448 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1026448 
  trunk/src/main/java/org/apache/hadoop/hbase/master/handler/OpenedRegionHandler.java 1026448 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java 1026448 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java 1026448 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKServerTool.java 1026448 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java 1026448 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperMainServerArg.java PRE-CREATION 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java 1026448 
  trunk/src/test/java/org/apache/hadoop/hbase/zookeeper/TestZooKeeperMainServerArg.java PRE-CREATION 

Diff: http://review.cloudera.org/r/1057/diff


Testing
-------


Thanks,

stack


;;;","22/Oct/10 19:48;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1057/#review1624
-----------------------------------------------------------

Ship it!


+1 after doing changes we discussed on IRC.  Namely to make sure that the shutdown methods are idempotent and will work for stopping a backup master and that TestMasterFailover passes.  Also some minor logging/comment changes around deleting root location.

- Jonathan



;;;","22/Oct/10 19:56;stack;Thanks for the review Jon.  I did as you suggested (and that test passes).  I just tried it too up on cluster w/ 5 node ensemble.  Committing.

;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance fixes - profiler driven,HBASE-2997,12474125,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,ryanobjc,ryanobjc,ryanobjc,14/Sep/10 21:27,20/Nov/15 12:41,14/Jul/23 06:06,15/Sep/10 21:48,0.89.20100621,,,,,,,,,,,0.89.20100924,0.90.0,,,,,,0,,,while profiling hbase I found a number of slow pieces.  Here are fixes for them.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-3103,,,,,,,,,,,,,"14/Sep/10 21:57;ryanobjc;HBASE-2997-1.txt;https://issues.apache.org/jira/secure/attachment/12454599/HBASE-2997-1.txt","14/Sep/10 21:56;ryanobjc;HBASE-2997-2.txt;https://issues.apache.org/jira/secure/attachment/12454598/HBASE-2997-2.txt","14/Sep/10 21:56;ryanobjc;HBASE-2997-3.txt;https://issues.apache.org/jira/secure/attachment/12454597/HBASE-2997-3.txt","14/Sep/10 21:56;ryanobjc;HBASE-2997-4.txt;https://issues.apache.org/jira/secure/attachment/12454596/HBASE-2997-4.txt","14/Sep/10 21:55;ryanobjc;HBASE-2997-5.txt;https://issues.apache.org/jira/secure/attachment/12454595/HBASE-2997-5.txt",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26580,,,,,Fri Nov 20 12:41:19 UTC 2015,,,,,,,,,,"0|i0hk8f:",100540,,,,,,,,,,,,,,,,,,,,,"14/Sep/10 22:13;ryanobjc;there are 5 small fixes that improve performance:

- HCD has a sync block causing contention to get a _FINAL_ value that never changes (!!!!!)
- HeapByteBuffer.getInt() is surprisingly slow
- Cache calculated values in KeyValue for cpu time improvements
- do an earlier out in loops of Memstore.updateColumn
- pre-create metrics and remove a sync block in HBaseRpcMetrics

all of these show demonstrated improvement in my profiler.;;;","14/Sep/10 22:52;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/842/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

HBASE-2997 performance improvements


This addresses bug HBASE-2997.
    http://issues.apache.org/jira/browse/HBASE-2997


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java 3966108 

Diff: http://review.cloudera.org/r/842/diff


Testing
-------


Thanks,

Ryan


;;;","14/Sep/10 22:54;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/840/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

HBASE-2997 performance improvements


This addresses bug HBASE-2997.
    http://issues.apache.org/jira/browse/HBASE-2997


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java 8e3bd53 

Diff: http://review.cloudera.org/r/840/diff


Testing
-------


Thanks,

Ryan


;;;","14/Sep/10 23:00;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/843/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

HBASE-2997 performance improvements


This addresses bug HBASE-2997.
    http://issues.apache.org/jira/browse/HBASE-2997


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/KeyValue.java abd5a6b 

Diff: http://review.cloudera.org/r/843/diff


Testing
-------


Thanks,

Ryan


;;;","14/Sep/10 23:01;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/845/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

HBASE-2997 performance improvements


This addresses bug HBASE-2997.
    http://issues.apache.org/jira/browse/HBASE-2997


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java 134288b 
  src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcMetrics.java 92468e3 
  src/main/java/org/apache/hadoop/hbase/master/HMaster.java bc21a1e 
  src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java bba7b67 

Diff: http://review.cloudera.org/r/845/diff


Testing
-------


Thanks,

Ryan


;;;","14/Sep/10 23:09;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/844/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

HBASE-2997 performance improvements


This addresses bug HBASE-2997.
    http://issues.apache.org/jira/browse/HBASE-2997


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java d52ea65 

Diff: http://review.cloudera.org/r/844/diff


Testing
-------


Thanks,

Ryan


;;;","14/Sep/10 23:11;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/842/#review1207
-----------------------------------------------------------

Ship it!



src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
<http://review.cloudera.org/r/842/#comment4117>

    is this a bug fix or a perf thing?



src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
<http://review.cloudera.org/r/842/#comment4119>

    remove commented-out code?



src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
<http://review.cloudera.org/r/842/#comment4120>

    same


- Todd



;;;","14/Sep/10 23:21;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/845/#review1204
-----------------------------------------------------------

Ship it!


Looks like a fine refactor, but how does this change performance?


src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcMetrics.java
<http://review.cloudera.org/r/845/#comment4115>

    do we need these anymore?



src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcMetrics.java
<http://review.cloudera.org/r/845/#comment4114>

    can you just call through to initMethods() here?



src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
<http://review.cloudera.org/r/845/#comment4112>

    random junk


- Todd



;;;","14/Sep/10 23:28;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/842/#review1210
-----------------------------------------------------------


yes this is actually faster. isn't that nuts?


src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
<http://review.cloudera.org/r/842/#comment4121>

    perf thing, we do 2 less int decodes


- Ryan



;;;","14/Sep/10 23:36;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/840/#review1209
-----------------------------------------------------------

Ship it!


- Todd



;;;","15/Sep/10 03:51;hbasereviewboard;Message from: ""Andrew Purtell"" <apurtell@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/840/#review1218
-----------------------------------------------------------

Ship it!


- Andrew



;;;","15/Sep/10 21:48;ryanobjc;committed to trunk;;;","15/Sep/10 21:48;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/843/#review1229
-----------------------------------------------------------



src/main/java/org/apache/hadoop/hbase/KeyValue.java
<http://review.cloudera.org/r/843/#comment4186>

    Should this be volatile?



src/main/java/org/apache/hadoop/hbase/KeyValue.java
<http://review.cloudera.org/r/843/#comment4187>

    Is the trade of cpu for memory space worth it here?



src/main/java/org/apache/hadoop/hbase/KeyValue.java
<http://review.cloudera.org/r/843/#comment4188>

    Volatile or is it possible to read partial long?


- stack



;;;","15/Sep/10 22:17;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/844/#review1232
-----------------------------------------------------------

Ship it!


- stack



;;;","15/Sep/10 22:35;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/843/#review1230
-----------------------------------------------------------



src/main/java/org/apache/hadoop/hbase/KeyValue.java
<http://review.cloudera.org/r/843/#comment4189>

    key values are only used by 1 thread at a time, so no.
    
    and even if they were shared, we'd just result in the current situation - use more cpu than is theoretically necessary.



src/main/java/org/apache/hadoop/hbase/KeyValue.java
<http://review.cloudera.org/r/843/#comment4190>

    it actually is worth it... also key values tend to be short lived, but recopying this array out half a dozen times during a scan is pricy according to the profiler.  6% cpu.



src/main/java/org/apache/hadoop/hbase/KeyValue.java
<http://review.cloudera.org/r/843/#comment4191>

    in a 64 bit jvm you cannot read a partial long, the updates are atomic.
    
    this is part of the spec.  also single threaded comment above.


- Ryan



;;;","24/Sep/10 18:48;jdcryans;Bringing this in the latest 0.89;;;","31/Dec/10 01:34;jk-public@troove.net;HBaseRpcMetrics is now logging a WARN message every time it encounters an unregistered RPC method.

In my case I now get huge log files filled with these warnings because the hbase-trx transactional extension of HBase uses a subclass of HRegionServer that adds new interface methods.

It's easy enough to tell log4j to ignore HBaseRpcMetrics output.

However, it would be nice if the Server/HRegionServer HBaseRpcMetrics mechanism was more extensible so I could pass down new interfaces or grab the HBaseRpcMetrics object to add interfaces from up top...;;;","31/Dec/10 05:18;ghelmling;James, I moved the last comment to HBASE-3405.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect dependency on Log class from Jetty,HBASE-2995,12474112,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,larsfrancke,larsfrancke,14/Sep/10 19:17,20/Nov/15 12:43,14/Jul/23 06:06,27/Sep/10 22:18,,,,,,,,,,,,0.90.0,,,,,,,0,,,"There are a few dependencies on the class {{org.mortbay.log.Log}} which is a bug:

21:05 < dj_ryan> we should not depend on that logging framework
21:05 < dj_ryan> at all
21:05 < dj_ryan> ever

I could find it in use in these classes:
* {{org.apache.hadoop.hbase.client.ScannerCallable}}
* {{org.apache.hadoop.hbase.client.TestHCM}}
* {{org.apache.hadoop.hbase.mapreduce.Export}}
* {{org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader}}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/10 22:14;stack;2995.txt;https://issues.apache.org/jira/secure/attachment/12455766/2995.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26579,,,,,Fri Nov 20 12:43:46 UTC 2015,,,,,,,,,,"0|i0hk7z:",100538,,,,,,,,,,,,,,,,,,,,,"27/Sep/10 22:14;stack;HBASE-3042 did one instance.  Here is a patch to do the others.;;;","27/Sep/10 22:18;stack;Committed change.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[replication] MalformedObjectNameException in ReplicationMetrics,HBASE-2992,12474021,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,jdcryans,jdcryans,jdcryans,14/Sep/10 00:42,20/Nov/15 12:42,14/Jul/23 06:06,24/Sep/10 18:35,,,,,,,,,,,,0.89.20100924,0.90.0,,,,,,0,,,"It's possible to get an exception that looks like this when creating a new ReplicationSource:

{noformat}
javax.management.MalformedObjectNameException: Invalid character ',' in key part of property
        at javax.management.ObjectName.construct(ObjectName.java:535)
        at javax.management.ObjectName.<init>(ObjectName.java:1403)
        at org.apache.hadoop.metrics.util.MBeanUtil.getMBeanName(MBeanUtil.java:80)
        at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:51)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationStatistics.<init>(ReplicationStatistics.java:43)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceMetrics.<init>(ReplicationSourceMetrics.java:77)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.init(ReplicationSource.java:176)

...
{noformat}

Need to make sure the MBean's name is valid.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/10 18:13;jdcryans;HBASE-2992.patch;https://issues.apache.org/jira/secure/attachment/12454562/HBASE-2992.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26577,,,,,Fri Nov 20 12:42:40 UTC 2015,,,,,,,,,,"0|i0hk7j:",100536,,,,,,,,,,,,,,,,,,,,,"14/Sep/10 18:13;jdcryans;I'm tempted to do like in the patch I just attached, simply URL encoding the name of the RS like we do for the hlog names.;;;","24/Sep/10 18:35;jdcryans;We've been running with this in production for some time and it's a really small fix, committed to trunk and 0.89;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[replication] RSM won't cleanup after locking if 0 peers,HBASE-2989,12473987,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,jdcryans,jdcryans,jdcryans,13/Sep/10 17:28,20/Nov/15 12:42,14/Jul/23 06:06,24/Sep/10 18:35,,,,,,,,,,,,0.89.20100924,0.90.0,,,,,,0,,,"Small bug in ReplicationSourceManager, it won't cleanup after locking another's RS znode if it didn't contain any queue at all. It happens in transferQueues():

{code}
LOG.info(""Moving "" + rsZnode + ""'s hlogs to my queue"");
    SortedMap<String, SortedSet<String>> newQueues =
        this.zkHelper.copyQueuesFromRS(rsZnode);
    if (newQueues == null || newQueues.size() == 0) {
      return;
    }
    this.zkHelper.deleteRsQueues(rsZnode);
{code}

That last line should be before the if, so that it deletes the lock znode and the RS znode. Currently a lot of cruft piles up in ZK after a few restarts with replication enabled and no queues, or in slave RSs.

",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26575,,,,,Fri Nov 20 12:42:41 UTC 2015,,,,,,,,,,"0|i0hk73:",100534,,,,,,,,,,,,,,,,,,,,,"24/Sep/10 18:33;jdcryans;Minor change we've been running in production with for some time. Committed to latest 0.89 and 0.90;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
multi writable can npe causing client hang,HBASE-2986,12473875,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,ryanobjc,ryanobjc,ryanobjc,10/Sep/10 23:14,20/Nov/15 12:40,14/Jul/23 06:06,15/Sep/10 14:37,0.89.20100621,,,,,,,,,,,0.90.0,,,,,,,0,,,"I have this backtrace:

} org.apache.hadoop.hbase.NotServingRegionException: usertable,,1284159178472.010c503fa9c9f12bd0fd9551ede360ec. is closed
        at org.apache.hadoop.hbase.regionserver.HRegion.startRegionOperation(HRegion.java:3068)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1248)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1709)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:2412)
        at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:557)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1007)

2010-09-10 16:00:56,808 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 15 on 60020 caught: java.lang.NullPointerException
        at org.apache.hadoop.hbase.client.MultiResponse.write(MultiResponse.java:92)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.writeObject(HbaseObjectWritable.java:376)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.write(HbaseObjectWritable.java:242)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1029)

After this happened my client just sat there and didnt go anywhere.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/10 21:54;ryanobjc;HBASE-2986.txt;https://issues.apache.org/jira/secure/attachment/12454594/HBASE-2986.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26574,Reviewed,,,,Fri Nov 20 12:40:36 UTC 2015,,,,,,,,,,"0|i0hk6f:",100531,,,,,,,,,,,,,,,,,,,,,"10/Sep/10 23:15;ryanobjc;ps: this was due to a table split;;;","15/Sep/10 05:12;stack;+1 on patch.

Was stuck trying to fix the testmultiparallel test.  The multi call had completed on the server but over in rpcclient it was stuck waiting on Call to complete.  Ryan suggested I try this patch and now the issue is gone.;;;","15/Sep/10 05:15;stack;I took a look at the patch.  These comments disconcert:

{code}
+          out.writeInt(-1); // cant have index -1
+        } else {
+          out.writeInt(r.getFirst()); // this can npe!?!
{code}

On commit can you make them less distressing (smile).;;;","15/Sep/10 05:16;stack;Oh, yeah, an NSRE was the prelude to my hang too.;;;","15/Sep/10 14:24;stack;OK, I see the NPE if I enable INFO-level logging for ipc (I added 'log4j.logger.org.apache.hadoop.ipc=DEBUG' line... to see it in tests I added this line to src/test/resouce/log4j.properties).  It looks like this:

{code}
2010-09-15 07:16:52,522 INFO  [IPC Server handler 3 on 59328] regionserver.HRegionServer(2249): MULTI REMOVE RETURN
2010-09-15 07:16:52,523 DEBUG [IPC Server handler 3 on 59328] ipc.HBaseRPC$Server(581): Served: multi queueTime= 0 procesingTime= 863
2010-09-15 07:16:52,523 INFO  [IPC Server handler 3 on 59328] ipc.HBaseServer$Handler(964): IPC Server handler 3 on 59328 caught: java.lang.NullPointerException
    at org.apache.hadoop.hbase.client.MultiResponse.write(MultiResponse.java:92)
    at org.apache.hadoop.hbase.io.HbaseObjectWritable.writeObject(HbaseObjectWritable.java:394)
    at org.apache.hadoop.hbase.io.HbaseObjectWritable.write(HbaseObjectWritable.java:257)
    at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:941)
...
{code};;;","15/Sep/10 14:24;stack;This is a bad bug.  I'm going to commit to TRUNK.;;;","15/Sep/10 14:37;stack;Committed.  I fixed up comments so less alarming.  I added the below also so that these kinda of probs are no longer 'silent':

{code}
Index: src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java  (revision 997346)
+++ src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java  (working copy)
@@ -961,7 +961,7 @@
             throw e;
           }
         } catch (Exception e) {
-          LOG.info(getName() + "" caught: "" +
+          LOG.warn(getName() + "" caught: "" +
                    StringUtils.stringifyException(e));
         }
       }
{code};;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HRegionServer.multi() no longer calls HRegion.put(List) when possible,HBASE-2985,12473874,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,10/Sep/10 23:12,20/Nov/15 12:40,14/Jul/23 06:06,19/Oct/10 01:02,0.89.20100621,,,,,,,,,,,0.90.0,,,,,,,0,,,This should result in a reduce performance of puts in batched mode,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26573,,,,,Fri Nov 20 12:40:40 UTC 2015,,,,,,,,,,"0|i0hk67:",100530,,,,,,,,,,,,,,,,,,,,,"18/Oct/10 23:27;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>


bq.  On 2010-10-18 16:15:13, stack wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java, line 2357
bq.  > <http://review.cloudera.org/r/1038/diff/1/?file=14871#file14871line2357>
bq.  >
bq.  >     Remove this message on commit
bq.  >

i think it's a legit comment, people might need that printf debugging in prod :-)


- Ryan


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1038/#review1566
-----------------------------------------------------------



;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[shell] Altering a family shouldn't reset to default unchanged attributes,HBASE-2984,12473871,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,10/Sep/10 22:48,20/Nov/15 12:42,14/Jul/23 06:06,22/Oct/10 23:29,,,,,,,,,,,,0.90.0,,,,,,,0,,,"I changed the replication on a family that was also VERSIONS => 1 and COMPRESSION => LZO. I forgot that you have to respecify everything everytime you alter a family, so both were reset to 3 and NONE. Then the regions were compacted... and it has been splitting for about 20 minutes now. Fortunately this is our MR environment so our web site isn't affected, but it's still a major pain. Oh and also the table cannot be disabled to be re-altered since split parents are always present (I hope it'll stop splitting before midnight).

The shell should use the old values for attributes that aren't changed.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2960,,,,,,,,,,,,,,,,,,,,"11/Sep/10 00:01;jdcryans;2984-meta-offline-fixer.patch;https://issues.apache.org/jira/secure/attachment/12454350/2984-meta-offline-fixer.patch","22/Oct/10 21:57;jdcryans;HBASE-2984.patch;https://issues.apache.org/jira/secure/attachment/12457877/HBASE-2984.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26572,Reviewed,,,,Fri Nov 20 12:42:45 UTC 2015,,,,,,,,,,"0|i0hk5z:",100529,,,,,,,,,,,,,,,,,,,,,"11/Sep/10 00:01;jdcryans;If it ever happens to anyone else (I didn't want to wait for eternity for all the regions to split), here's a path that makes MetaUtils runnable and that resets the schema like I wanted it for the specified table. Tailor it to your own need. (yes it's super hacky, kids don't try this at home!);;;","22/Oct/10 21:57;jdcryans;Simple patch that fixes the issue.;;;","22/Oct/10 23:26;stack;+1  Excellent.;;;","22/Oct/10 23:29;jdcryans;Committed to trunk, thanks for looking at the patch Stack.;;;","29/Oct/10 19:48;kannanm;JD:

Was trying this patch, and ran into an error with Compression/BloomFilter settings, because  family.setBloomFilterType()  & family.setCompressionType() expect enum instead of string.

{code}
hbase(main):006:0> alter 't1', {NAME => 'f1', VERSIONS=> 2, COMPRESSION => 'LZO'}
alter 't1', {NAME => 'f1', VERSIONS=> 2, COMPRESSION => 'LZO'}

ERROR: for method HColumnDescriptor.setCompressionType expected [class org.apache.hadoop.hbase.io.hfile.Compression$Algorithm]; got: [java.lang.String]; error: argument type mismatch
{code}

So the patch currently breaks 'alter' & 'create'  from HBase shell if compression or bloom needs to be specified.





;;;","29/Oct/10 20:29;kannanm;I suppose something like this: 
family.setBloomFilterType(StoreFile.BloomType.valueOf(arg[HColumnDescriptor::BLOOMFILTER]))  
would do the trick. But I am getting some errors trying to import the required enum into Jruby. JD: you know how to?
;;;","29/Oct/10 20:41;kannanm;Ok, looks like in jruby the enum should be referred using the ""::"" syntax ( StoreFile::BloomType.valueOf), and with the appropriate import for  StoreFile thrown in it should work.;;;","29/Oct/10 20:57;jdcryans;@Kannan, looks like I broke something, can you open a new jira?;;;","29/Oct/10 21:04;kannanm;sure.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestHLog unit test is mis-comparing in an assertion,HBASE-2983,12473865,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,posix4e,posix4e,posix4e,10/Sep/10 21:57,20/Nov/15 12:43,14/Jul/23 06:06,10/Sep/10 22:09,,,,,,,,,,,,0.90.0,,,,,,,0,,,The TestHLog unit test seems to be failing,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/10 21:59;posix4e;HBASE-2983.patch;https://issues.apache.org/jira/secure/attachment/12454342/HBASE-2983.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26571,Reviewed,,,,Fri Nov 20 12:43:52 UTC 2015,,,,,,,,,,"0|i0hk5r:",100528,,,,,,,,,,,,,,,,,,,,,"10/Sep/10 22:07;tlipcon;+1;;;","10/Sep/10 22:16;hbasereviewboard;Message from: ""Alex Newman"" <newalex@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/815/
-----------------------------------------------------------

(Updated 2010-09-10 15:00:12.397297)


Review request for hbase and Jean-Daniel Cryans.


Changes
-------

assigning to jd


Summary
-------

These unit tests fail on my machine. Here's a patch


This addresses bug hbase-2983.
    http://issues.apache.org/jira/browse/hbase-2983


Diffs
-----

  src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java 185aff2 

Diff: http://review.cloudera.org/r/815/diff


Testing
-------

 mvn test -Dtest=org.apache.hadoop.hbase.regionserver.wal.TestHLog,org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit,org.apache.hadoop.hbase.regionserver.wal.TestLogRolling


Thanks,

Alex


;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maven 3 doesn't like duplicate dependencies in hbase/pom.xml,HBASE-2982,12473839,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,larsfrancke,jk-public@troove.net,jk-public@troove.net,10/Sep/10 18:36,20/Nov/15 12:44,14/Jul/23 06:06,04/Oct/10 05:44,,,,,,,,,,,,0.90.0,,build,,,,,0,,,"An install of the latest maven plugin resolves dependencies with Maven 3.
Maven 3 does not like the fact that several dependencies in the main pom.xml are duplicated.

Below is the error message which can be fixed by removing:

* The 2nd com.google.guava dependency from the ""Test dependencies"" section
* both org.slf4j dependencies from the ""Avro dependencies"" section 


9/10/10 11:15:50 AM PDT: Build errors for hbase; org.apache.maven.project.ProjectBuildingException: Some problems were encountered while processing the POMs:
[WARNING] 'version' contains an expression but should be a constant. @ org.apache.hbase:hbase:${hbase.version}, /opt/eclipse/troove/hbase/pom.xml
[ERROR] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.slf4j:slf4j-log4j12:jar -> duplicate declaration of version ${slf4j.version} @ org.apache.hbase:hbase:${hbase.version}, /opt/eclipse/troove/hbase/pom.xml
[ERROR] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.slf4j:slf4j-api:jar -> duplicate declaration of version ${slf4j.version} @ org.apache.hbase:hbase:${hbase.version}, /opt/eclipse/troove/hbase/pom.xml
[ERROR] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: com.google.guava:guava:jar -> duplicate declaration of version ${guava.version} @ org.apache.hbase:hbase:${hbase.version}, /opt/eclipse/troove/hbase/pom.xml
[WARNING] 'build.plugins.plugin.version' is missing for org.codehaus.mojo:build-helper-maven-plugin @ org.apache.hbase:hbase:0.89-SNAPSHOT-withHLogSplit, /opt/eclipse/troove/hbase/pom.xml
",Using eclipse maven plugin that forces usage of embedded Maven 3 for dependency resolution.,larsfrancke,,,,,,,,,,,,,,,,,,,,,1200,1200,,0%,1200,1200,,,,,,,,,,,,,,,,,HBASE-2996,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26570,,,,,Fri Nov 20 12:44:03 UTC 2015,,,,,,,,,,"0|i0hk5j:",100527,Fixed as part of HBASE-2996,,,,,,,,,,,maven,,,,,,,,,"10/Sep/10 20:52;larsfrancke;I'm going to/have fix all of those in another ticket (at least I think there's already another one for that).;;;","11/Sep/10 19:17;stack;@Lars OK If I remove what James suggests above?  I just tried it and in non-extensive testing stuff seems fine w/o them?

{code}
stack@face:~/hbase$ git diff pom.xml 
diff --git a/pom.xml b/pom.xml
index 267ab49..8485b6b 100644
--- a/pom.xml
+++ b/pom.xml
@@ -739,11 +739,6 @@
 
     <!-- Avro dependencies -->
     <dependency>
-      <groupId>org.slf4j</groupId>
-      <artifactId>slf4j-log4j12</artifactId>
-      <version>${slf4j.version}</version>
-    </dependency>
-    <dependency>
       <groupId>org.apache.hadoop</groupId>
       <artifactId>avro</artifactId>
       <version>1.3.2</version>
@@ -783,11 +778,6 @@
       <groupId>org.apache.hadoop</groupId>
       <artifactId>hadoop-test</artifactId>
     </dependency>
-    <dependency>
-      <groupId>com.google.guava</groupId>
-      <artifactId>guava</artifactId>
-      <version>${guava.version}</version>
-   </dependency>
   </dependencies>
 
   <!--
{code};;;","14/Sep/10 20:45;larsfrancke;As per discussion on IRC I've created an issue to capture all the changes to the pom for the 0.90 release: HBASE-2996. This change is part of the patch I've just uploaded to that issue.;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix failing TestMultParrallel in hudson build,HBASE-2979,12473768,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,10/Sep/10 06:03,20/Nov/15 12:41,14/Jul/23 06:06,15/Sep/10 18:14,,,,,,,,,,,,0.90.0,,,,,,,0,,,Its failing w/ a while now.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/10 23:53;stack;2979-v2.txt;https://issues.apache.org/jira/secure/attachment/12454349/2979-v2.txt","15/Sep/10 18:13;stack;2979-v3.txt;https://issues.apache.org/jira/secure/attachment/12454678/2979-v3.txt","10/Sep/10 06:05;stack;2979.txt;https://issues.apache.org/jira/secure/attachment/12454270/2979.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26569,,,,,Fri Nov 20 12:41:31 UTC 2015,,,,,,,,,,"0|i0hk53:",100525,,,,,,,,,,,,,,,,,,,,,"10/Sep/10 06:05;stack;Not done yet.  Made it junit4 and HBaseTestingUtility while I was at it because its long running test -- was starting a cluster between each test.  Also moved it down into client package where it belongs.;;;","10/Sep/10 23:53;stack;This test is failing because new master setup is not handling abort of server carrying .META. properly.  On opening of a bunch of regions, .META. carrier goes down in the middle causing the OPENED processing to fail because it can't update meta w/ info on just opened region.  So, need to make the RS wait on the meta to come back up.

But tools to do this are not yet in place.  Am writing a test for CatalogTracker first before I do anything else and adding in a few methods; e.g. we need to pass a Progressable to the waitForMeta so we can tickle zk letting it know that opening is still going on (Jon's idea).;;;","15/Sep/10 18:13;stack;Here is what I just committed.  Also adds new CatalogTracker test and a few bug fixes for CatalogTracker+AssignmentManager

{code}

M src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
  Make KEYS available for tests that want to mess with multiregion
  table row keys.
  (ensureSomeRegionServersAreAvailable): Added a boolean return which
  is true if we actually started a server.
D src/test/java/org/apache/hadoop/hbase/TestMultiParallel.java
  Moved into client package.
A src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
  Moved from o.a.h.h package.  Also converted to use juni4 & HTU.
M src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
  Catch regular NPE rather than let it spew over log on exit.
A src/test/java/org/apache/hadoop/hbase/catalog/TestCatalogTracker.java
  Tests for CatalogTracker
M src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java
  Removed commented out code.
M src/main/java/org/apache/hadoop/hbase/zookeeper/RootRegionTracker.java
  Javadoc.
M src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperNodeTracker.java
  (getNode) Return node that is being tracked.
M src/main/java/org/apache/hadoop/hbase/zookeeper/MetaNodeTracker.java
  Made this be a ZKNodeTracker; thats what it is.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
M src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
M src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
M src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
  Formatting.
M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  Added better handling of zk master events (more to do).
  Fixed interesting bug where just presence of a server in assignments,
  though it was empty of regions and a crashed region, was making the
  load balancer give it regions to open (post-crash)
M src/main/java/org/apache/hadoop/hbase/master/HMaster.java
  Don't run balancer if deadservers.  Also log balance plans (for now)
M src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
  Add toString to Plan. Helps debugging.
M src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
  Javadoc.
  Fixup of waitForMeta and waitForRoot. In particular handle
  ConnectException when doing verify of region location.
  Made waitForRoot(timeout) protected (rather than public)
M src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java
  Javadoc and shutdown access to a few methods.
M src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
M src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
  Formatting.
M src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
  Better exception  message.
M src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java
  Added in LOG.
M bin/rolling-restart.sh
  Fix double entry (patch applied twice)
{code};;;","15/Sep/10 18:14;stack;Committed fix for broke test.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LoadBalancer IndexOutOfBoundsException,HBASE-2978,12473767,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,10/Sep/10 05:51,20/Nov/15 12:41,14/Jul/23 06:06,10/Sep/10 06:01,,,,,,,,,,,,0.90.0,,,,,,,0,,,"{code}
2010-09-09 22:49:21,500 ERROR [192.168.1.157:51901-balancerChore] hbase.Chore(69): Caught exception
java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.hadoop.hbase.master.LoadBalancer.balanceCluster(LoadBalancer.java:250)
	at org.apache.hadoop.hbase.master.HMaster.balance(HMaster.java:568)
	at org.apache.hadoop.hbase.master.HMaster$1.chore(HMaster.java:480)
	at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26568,,,,,Fri Nov 20 12:41:44 UTC 2015,,,,,,,,,,"0|i0hk4v:",100524,,,,,,,,,,,,,,,,,,,,,"10/Sep/10 06:00;stack;I added this at line 249:

{code}
+        if (idx >= server.getValue().size()) break;
{code};;;","10/Sep/10 06:01;stack;Committed.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running HFile tool passing fully-qualified filename I get 'IllegalArgumentException: Wrong FS',HBASE-2976,12473699,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,09/Sep/10 16:28,20/Nov/15 12:43,14/Jul/23 06:06,09/Sep/10 16:41,,,,,,,,,,,,0.90.0,,,,,,,0,,,"This fixes it:

{code}
diff --git a/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java b/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
index 65cbb9d..3966108 100644
--- a/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
+++ b/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
@@ -1826,6 +1826,8 @@ public class HFile {
       Configuration conf = HBaseConfiguration.create();
       conf.set(""fs.defaultFS"",
         conf.get(org.apache.hadoop.hbase.HConstants.HBASE_DIR));
+      conf.set(""fs.default.name"",
+        conf.get(org.apache.hadoop.hbase.HConstants.HBASE_DIR));
       FileSystem fs = FileSystem.get(conf);
       ArrayList<Path> files = new ArrayList<Path>();
       if (cmd.hasOption(""f"")) {
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26567,,,,,Fri Nov 20 12:43:48 UTC 2015,,,,,,,,,,"0|i0hk4f:",100522,,,,,,,,,,,,,,,,,,,,,"09/Sep/10 16:41;stack;Committed small patch.  Added a bit of doc to the hbase book too on the hfile tool.  Resolving.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DFSClient names in master and RS should be unique,HBASE-2975,12473651,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,tlipcon,tlipcon,tlipcon,09/Sep/10 05:55,20/Nov/15 12:42,14/Jul/23 06:06,09/Sep/10 17:13,0.90.0,,,,,,,,,,,0.90.0,,master,regionserver,,,,0,,,"In the post-newmaster trunk, there's some code which gives the regionserver and masters fancy names based on hostname and port. This breaks log recovery, though, if the master starts recovering a log (ie has an append lease on a log file), then crashes and comes back on the same port. The NN doesn't see this as a new client, since the client name is the same, so it thinks it still holds a lease on the file. The new master, though, can't call append() because the NN thinks it's appending, so it loops forever.",,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/10 05:56;tlipcon;hbase-2975.txt;https://issues.apache.org/jira/secure/attachment/12454187/hbase-2975.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26566,Reviewed,,,,Fri Nov 20 12:42:11 UTC 2015,,,,,,,,,,"0|i0hk47:",100521,,,,,,,,,,,,,,,,,,,,,"09/Sep/10 06:06;stack;+1;;;","09/Sep/10 17:13;tlipcon;Committed to 0.89.20100830 branch, plus trunk. Thanks for reviewing stack.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LoadBalancer ArithmeticException: / by zero,HBASE-2974,12473650,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,09/Sep/10 05:42,20/Nov/15 12:41,14/Jul/23 06:06,09/Sep/10 05:54,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Having an issue so regionservers are not checking in.  Balancer meantime went to run and threw ArithmeticException: / by zero

Committing this:

{code}
Index: src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java      (revision 995307)
+++ src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java      (working copy)
@@ -140,6 +140,10 @@
       new TreeMap<HServerInfo,List<HRegionInfo>>(
           new HServerInfo.LoadComparator());
     int numServers = clusterState.size();
+    if (numServers == 0) {
+      LOG.debug(""numServers=0 so nothing to balance"");
+      return null;
+    }
     int numRegions = 0;
     // Iterate so we can count regions as we build the map
     for(Map.Entry<HServerInfo, List<HRegionInfo>> server :
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26565,,,,,Fri Nov 20 12:41:59 UTC 2015,,,,,,,,,,"0|i0hk3z:",100520,,,,,,,,,,,,,,,,,,,,,"09/Sep/10 05:54;stack;Committed small fix to TRUNK;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in LogCleaner,HBASE-2973,12473635,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,08/Sep/10 23:32,20/Nov/15 12:43,14/Jul/23 06:06,08/Sep/10 23:32,,,,,,,,,,,,0.90.0,,,,,,,0,,,"From TRUNK:

{code}
2010-09-08 15:52:42,221 DEBUG org.apache.hadoop.hbase.master.LogCleaner: Add log cleaner in chain: org.apache.hadoop.hbase.master.TimeToLiveLogCleaner
2010-09-08 15:52:42,221 DEBUG org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner: Didn't find this log in ZK, deleting: null
2010-09-08 15:52:42,221 DEBUG org.apache.hadoop.hbase.master.LogCleaner: Add log cleaner in chain: org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner
2010-09-08 15:52:42,234 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: <master-sv2borg185:60000> Set watcher on existing znode /hbase/root-region-server
2010-09-08 15:52:42,235 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: <master-sv2borg185:60000> Retrieved 18 bytes of data from znode /hbase/root-region-server and set a watcher
2010-09-08 15:53:42,220 INFO org.apache.hadoop.hbase.master.ServerManager: 0 region servers, 0 dead, average load NaN[]
2010-09-08 15:53:42,227 ERROR org.apache.hadoop.hbase.master.LogCleaner: Caught exception
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.LogCleaner.chore(LogCleaner.java:128)
        at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
2010-09-08 15:54:42,221 INFO org.apache.hadoop.hbase.master.ServerManager: 0 region servers, 0 dead, average load NaN[]
2010-09-08 15:54:42,225 ERROR org.apache.hadoop.hbase.master.LogCleaner: Caught exception
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.LogCleaner.chore(LogCleaner.java:128)
        at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26564,,,,,Fri Nov 20 12:43:37 UTC 2015,,,,,,,,,,"0|i0hk3r:",100519,,,,,,,,,,,,,,,,,,,,,"08/Sep/10 23:32;stack;Committed this:

{code}
Index: src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java        (revision 995264)
+++ src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java        (working copy)
@@ -123,7 +123,8 @@
   @Override
   protected void chore() {
     try {
-      FileStatus[] files = this.fs.listStatus(this.oldLogDir);
+      FileStatus [] files = this.fs.listStatus(this.oldLogDir);
+      if (files == null) return;
       int nbDeletedLog = 0;
       FILE: for (FileStatus file : files) {
         Path filePath = file.getPath();
{code};;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
missing sync in HTablePool.getTable(),HBASE-2969,12473607,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,guiga,guiga,guiga,08/Sep/10 17:58,20/Nov/15 12:43,14/Jul/23 06:06,08/Sep/10 22:42,0.89.20100621,,,,,,,,,,,0.90.0,,Client,,,,,0,,,"Considering that the method _getTable(String)_ in _org.apache.hadoop.hbase.client.HTablePool_ is invoked by multiple threads, it may happen that while _'queue == null'_ is true, it is possible to have a queue mapped to that name into the tables map when _'tables.put(tableName, queue)'_ is executed. 

However, I don't think it will cause any harm because the overwritten queue will eventually be garbage-collected. :-)",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/10 18:04;guiga;HBASE-2969.patch;https://issues.apache.org/jira/secure/attachment/12454129/HBASE-2969.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26562,Reviewed,,,,Fri Nov 20 12:43:50 UTC 2015,,,,,,,,,,"0|i0hk33:",100516,,,,,,,,,,,,,,,,,,,,,"08/Sep/10 18:02;guiga;Now using ConcurrentMap.putIfAbsent;;;","08/Sep/10 18:06;guiga;Sorry. I've used the wrong action.;;;","08/Sep/10 22:42;stack;Thanks for the patch Guilherme.  Committed to trunk;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed split: IOE 'File is Corrupt!' -- sync length not being written out to SequenceFile,HBASE-2967,12473532,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,stack,stack,08/Sep/10 06:25,20/Nov/15 12:42,14/Jul/23 06:06,08/Sep/10 21:23,,,,,,,,,,,,0.90.0,,,,,,,0,,,"We saw this on one of our clusters:

{code}
2010-09-07 18:07:16,229 WARN org.apache.hadoop.hbase.master.RegionServerOperationQueue: Failed processing: ProcessServerShutdown of sv4borg18,60020,1283516293515; putting onto delayed todo queue
java.io.IOException: File is corrupt!
        at org.apache.hadoop.io.SequenceFile$Reader.readRecordLength(SequenceFile.java:1907)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1932)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1837)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1883)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:121)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:113)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.parseHLog(HLog.java:1493)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1256)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1143)
        at org.apache.hadoop.hbase.master.ProcessServerShutdown.process(ProcessServerShutdown.java:299)
        at org.apache.hadoop.hbase.master.RegionServerOperationQueue.process(RegionServerOperationQueue.java:147)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:532)
{code}

Because it was an IOE, it got requeued.  Each time around we failed on it again.

A few things:

+ This exception needs to add filename and the position in file at which problem found.
+ Need to commit little patch over in HBASE-2889 that outputs position and ordinal of wal edit because it helps diagnose these kinds of issues.
+ We should be able to skip the bad edit; just postion ourselves at byte past the bad sync and start reading again
+ There must be something about our setup that makes it so we fail write of the sync 16 random bytes that make up the SF 'sync' marker though oddly for one of the files, the sync failure happens at 1/3rd of the way into a 64MB wal, edit #2000 out of 130k odd edits.



",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26561,,,,,Fri Nov 20 12:42:31 UTC 2015,,,,,,,,,,"0|i0hk2n:",100514,,,,,,,,,,,,,,,,,,,,,"08/Sep/10 21:13;stack;I got the lowdown from Todd. 

Problem would seem to be here in SequenceFile:

{code}
    /** create a sync point */
    public void sync() throws IOException {
      if (sync != null && lastSyncPos != out.getPos()) {
        out.writeInt(SYNC_ESCAPE);                // mark the start of the sync
        out.write(sync);                          // write sync
        lastSyncPos = out.getPos();               // update lastSyncPos
      }
    }
{code}

Notice how the method is unsynchronized and how it does two distinct writes to out.

If you browse down in SequenceFile to the append methods, you'll see that these methods ARE synchronized.

In concurrent times, you could imagine the above write of SYNC_ESCAPE followed by the actual 16 bytes of the sync array being interpolated by an append.  On read, we'd fail the sync check.

So, not calling the sync would seem to be the way to go (This is what CDH does according to Todd).  But says you, what if we ever want to make use of these sync sequences to skip records corrupted by cosmic rays up in HDFS?  Well, turns out SF calls checkAndWriteSync internal to the append calls and will write the sync out at every SYNC_INTERVAL... so when we want to come back and handle these 'File is Corrupt!' messages later, we can.

Here's the change I'll make:

{code}
Index: src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java   (revision 995156)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java   (working copy)
@@ -120,7 +120,6 @@
 
   @Override
   public void sync() throws IOException {
-    this.writer.sync();
     if (this.syncFs != null) {
       try {
        this.syncFs.invoke(this.writer, HLog.NO_ARGS);
{code}

Regards other items in the above, over in HBASE-2889, we add logging of filename+position to exception and did some fixup on HLog main to help diagnose these kinds of issues
 ;;;","08/Sep/10 21:23;stack;Committed branch and trunk.;;;","08/Sep/10 23:17;stack;So, looking at a snapshot of log files on our production, about 1/2 had this issue.  After rolling out the above change, subsequent log files are again parseable.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock when RS tries to RPC to itself inside SplitTransaction,HBASE-2964,12473415,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,tlipcon,tlipcon,tlipcon,07/Sep/10 01:16,20/Nov/15 12:42,14/Jul/23 06:06,08/Sep/10 16:58,0.90.0,,,,,,,,,,,0.90.0,,IPC/RPC,regionserver,,,,1,,,"In testing the 0.89.20100830 rc, I ran into a deadlock with the following situation:

- All of the IPC Handler threads are blocked on the region lock, which is held by CompactSplitThread.
- CompactSplitThread is in the process of trying to edit META to create the offline parent. META happens to be on the same server as is executing the split.

Therefore, the CompactSplitThread is trying to connect back to itself, but all of the handler threads are blocked, so the IPC never happens. Thus, the entire RS gets deadlocked.",,larsfrancke,tsuna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2782,,,,,,,,,,,,,"07/Sep/10 07:15;tlipcon;hbase-2964.txt;https://issues.apache.org/jira/secure/attachment/12453992/hbase-2964.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26559,Reviewed,,,,Fri Nov 20 12:42:29 UTC 2015,,,,,,,,,,"0|i0hk2f:",100513,,,,,,,,,,,,,,,,,,,,,"07/Sep/10 01:18;tlipcon;Fixing this is a little tricky. We could short-circuit the IPC path when detecting that a region is hosted in the same process, and thus avoid going through handlers (this is what the datanode does in the block recovery code). However, you still can have a situation where two regionservers are trying to talk to each other and end up in a deadlock.

Another option is to add a timeout to these RPCs, abort the split and try again later if it fails.

Another thing that might help is to have the start of the split transaction flag the table as ""going offline"", and before taking the readlock, other accessors of the table can check for this case and immediately throw NSRE rather than blocking once the split is in progress.;;;","07/Sep/10 01:22;tlipcon;HBASE-2782 is another solution - if we provide QoS for the META and ROOT tables so they always have a few reserved handlers in a separate thread pool, we can avoid this issue.;;;","07/Sep/10 04:20;stack;I agree this a blocker on 0.90.x;;;","07/Sep/10 05:14;tlipcon;As noted on the list, this seems to be due to HBASE-2461.

Prior to 2461, when we split, we would close the region before doing any of the writes to META, and didn't hold any locks while doing the META updates. Now we keep the write lock all the way through, even after closing the region.

I think simply moving the writeLock().unlock() up after the this.parent.close(false) in SplitTransaction should fix this issue. I'm testing that change on my test cluster now.;;;","07/Sep/10 07:15;tlipcon;I also had to move the ""new HTable"" call outside of the lock, since the HTable constructor does an RPC.

This patch seems to fix the issue for me. Running an overnight load test - if it's still going in the morning I'd say we're good :);;;","07/Sep/10 14:35;tlipcon;Overnight test completed OK with that patch. I think we should rebuild the rc with this if Stack thinks it looks good.;;;","07/Sep/10 18:00;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/798/
-----------------------------------------------------------

Review request for hbase and stack.


Summary
-------

Moves all RPCs outside of the region writeLock - the writeLock is now only used long enough to set the 'closing' flag. When we drop the lock any waiters will see 'closing' upon acquiring the lock, and thus throw NSRE.

In the case that we abort the split, it will reopen the region as before. Accessors will have gotten NSRE but will just come back to the same region eventually.


This addresses bug HBASE-2964.
    http://issues.apache.org/jira/browse/HBASE-2964


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java 3507c0d 

Diff: http://review.cloudera.org/r/798/diff


Testing
-------

YCSB testing on my cluster - it used to deadlock due to this bug within an hour. I ran a 5 hour load test overnight and it worked OK.


Thanks,

Todd


;;;","07/Sep/10 18:30;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/798/#review1110
-----------------------------------------------------------



src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
<http://review.cloudera.org/r/798/#comment3770>

    Let me make a version of this patch that takes care of rollback -- currently rollback expects the lock to be held on entrance; this will not be the case post close if above applied.


- stack



;;;","07/Sep/10 19:31;stack;Hmmm... now I'm thinking instead that we punt locking up here in splittransaction completely.  The core issue comes of an incorrect mapping of old splitLock on to new region 'lock'.  Looking at what was done under the old splitLock, it all looks safe in the face of concurrency.  Down in the region close, its already taking out the region write lock.  Let me make a different kinda patch.;;;","07/Sep/10 20:48;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/798/
-----------------------------------------------------------

(Updated 2010-09-07 13:38:39.968517)


Review request for hbase and stack.


Changes
-------

This version removes from SplitTransaction the setting of the this.parent.lock completely.  Its not needed.  Down in the parent close, it takes out the write lock.

In the past, we had a split lock and a close lock (splitLock and splitsAndClosesLock).  The split lock was held across the split while daughter regions were calculated and during close, actual split and update of .META.  As part of lock pruning, an error made in hbase-2641, was using splitsAndClosesLock where splitLock was used previously -- and even expanding the scope of what splitLock used cover).

Looking, splitLock looks like it could have served some purpose preventing two threads contending over splitting (splits make objects in filesystem and move stuff around), but we don't really need this in current HBase since only CompactSplitThread runs splits -- even in new master regime where client can call a splitRegion. Later when we want to run multiple concurrent split transactions, we'll need to reexamine.


Summary
-------

Moves all RPCs outside of the region writeLock - the writeLock is now only used long enough to set the 'closing' flag. When we drop the lock any waiters will see 'closing' upon acquiring the lock, and thus throw NSRE.

In the case that we abort the split, it will reopen the region as before. Accessors will have gotten NSRE but will just come back to the same region eventually.


This addresses bug HBASE-2964.
    http://issues.apache.org/jira/browse/HBASE-2964


Diffs (updated)
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java a692125 
  src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java 3507c0d 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java a245d97 

Diff: http://review.cloudera.org/r/798/diff


Testing
-------

YCSB testing on my cluster - it used to deadlock due to this bug within an hour. I ran a 5 hour load test overnight and it worked OK.


Thanks,

Todd


;;;","08/Sep/10 01:39;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/798/#review1122
-----------------------------------------------------------


Seems to make sense. Let me try it on a cluster before I +1 it


src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
<http://review.cloudera.org/r/798/#comment3823>

    maybe now we can do an:
    
    assert !this.parent.lock.writeLock().isHeldByCurrentThread() : ""Unsafe to hold write lock while performing RPCs"";


- Todd



;;;","08/Sep/10 16:32;tlipcon;+1 to stack's patch from reviewboard. Imported about 550G over night, worked OK.;;;","08/Sep/10 16:48;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-09-07 18:33:16, Todd Lipcon wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java, line 207
bq.  > <http://review.cloudera.org/r/798/diff/2/?file=11132#file11132line207>
bq.  >
bq.  >     maybe now we can do an:
bq.  >     
bq.  >     assert !this.parent.lock.writeLock().isHeldByCurrentThread() : ""Unsafe to hold write lock while performing RPCs"";

I'll add in this assert


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/798/#review1122
-----------------------------------------------------------



;;;","08/Sep/10 16:58;stack;Thanks for review and for testing Todd (applied to TRUNK and to 0.89.20100830 branch.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add missing methods to HTableInterface (and HTable),HBASE-2962,12473316,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,larsfrancke,larsfrancke,04/Sep/10 17:24,20/Nov/15 12:42,14/Jul/23 06:06,07/Sep/10 22:21,,,,,,,,,,,,0.90.0,,Client,,,,,0,,,"HBASE-1845 added two new methods in HTable (batch). Those need to be in HTableInterface as well.

And in HTable we have:
* put(Put)
* put(List<Put>)
* delete(Delete)
* delete(List<Delete>)
* get(Get)

Shouldn't we add a get(List<Get>) as well for consistency?

Others that are missing:
* getRegionLocation
* getScannerCaching / setgetScannerCaching
* getStartKeys / getEndKeys / getStartEndKeys
* getRegionsInfo
* setAutoFlush
* getWriteBufferSize / setWriteBufferSize
* getWriteBuffer
* prewarmRegionCache
* serializeRegionInfo / deserializeRegionInfo

For some of those it might not make sense to add them. I'm just listing them all.

The patch is trivial once we've decided which to add, I'll prepare one for batch & get.",,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/10 17:52;larsfrancke;HBASE-2962.1.diff;https://issues.apache.org/jira/secure/attachment/12453871/HBASE-2962.1.diff",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26558,Reviewed,,,,Fri Nov 20 12:42:09 UTC 2015,,,,,,,,,,"0|i0hk1z:",100511,,,,,,,,,,,,,,,,,,,,,"04/Sep/10 17:53;larsfrancke;Another thing: Javadoc automatically copies comments from parent to child methods so we'd only need those in HTableInterface and could remove them from HTable to avoid duplicate work. On the other hand this and other issues show how easy it is to forget HTableInterface....;;;","04/Sep/10 22:37;stack;+1 on removing from HTable, replacing the javadoc w/ @Override, and putting the javadoc into HTableInterface.  If @Override, eclipse at least will complain if nothing overridden and will underline the fact that there is an HTableInterface to consider.

Thanks for doing this Lars.;;;","07/Sep/10 22:21;stack;Committed.  Thanks for the polish Lars.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Close zookeeper when done with it (HCM, Master, and RS)",HBASE-2961,12473292,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,03/Sep/10 23:30,20/Nov/15 12:42,14/Jul/23 06:06,10/Sep/10 00:07,,,,,,,,,,,,0.90.0,,,,,,,0,,,"We're not closing down zk properly, mostly in HCM.  Makes for spew in zk logs and it also causes shutdown to run longer.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2889,,,,,,,,,,,,,,,,,"08/Sep/10 23:50;stack;2961-v2.txt;https://issues.apache.org/jira/secure/attachment/12454171/2961-v2.txt","10/Sep/10 00:02;stack;2961-v3.txt;https://issues.apache.org/jira/secure/attachment/12454257/2961-v3.txt","07/Sep/10 23:52;stack;2961.txt;https://issues.apache.org/jira/secure/attachment/12454058/2961.txt","03/Sep/10 23:32;stack;debug.txt;https://issues.apache.org/jira/secure/attachment/12453842/debug.txt",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26557,,,,,Fri Nov 20 12:42:27 UTC 2015,,,,,,,,,,"0|i0hk1r:",100510,,,,,,,,,,,,,,,,,,,,,"03/Sep/10 23:32;stack;Here is a mix of debug and patch... Still trying to figure why zk decides to bang its head and try and reconnect to ensemble though the sessionid thats complaining seems to have been 'closed' properly.;;;","07/Sep/10 23:52;stack;This gets rid of noise from zk in standalone mode (shutdown runs faster).  It breaks tests though because fix is making a Configuration per server..... and so, new HConnections need to each get root location (Previous, they all shared single connection and the single root location).   Need to make the HCM use new RootRegionTracker.  Cleaner.;;;","08/Sep/10 23:50;stack;This patch moves HCM to use RootRegionTracker rather than do that weird setRootLocation thing we used do -- there is no need for it when HCM has a ZooKeeperWatcher inside in it.

(This patch also removes ServerConnection, and ServerConnectionManager! interfaces and replaces them with HConnection... renames TableServers as HConnectionImplementation... hopefully now noobies won't have to spend a month of sundays trying to disentangle the flow in an hbase client).

Not all tests pass yet.;;;","10/Sep/10 00:02;stack;{code}
Make it so zk shutdown is clean.  Did it by making each server have
its own Configuration and hence its own HConnection instance, then
in each server (and client) added HCM.deleteConnection on way out. This
runs clean up of the HConnection.. Added to HConnection#close the shutdown
of zk.

Did other refactor while in here; removed crazy ServerConnection and
ServerConnectionManager stuff substituting new RootRegionTracker in its
place.

Tried to fix outstanding tests.  One still failing.

M src/test/java/org/apache/hadoop/hbase/TestMultiParallel.java
  Up retries for now to make tests pass.  Needs rewrite.
M src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java
  Make a new Configuration so the setting of less tries has an effect
  and we don't retry for ever and fail the test.
M src/test/java/org/apache/hadoop/hbase/catalog/TestMetaReaderEditor.java
  Minor formatting.
M src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
  Reuse predefined variable.
M src/test/java/org/apache/hadoop/hbase/client/TestHCM.java
  Renamed of TableServers to HCI
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
  Removed ServerConnection.  Use HConnection.
  Use RootRegionTracker to keep tabs on root location.
  Add logging of zk sessionid to help w/ debug
M src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
  Minor formatting.
M src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java
  Need to preserve EOF if thats what we got.
M src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
  Minor reformatting.
M src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
M src/main/java/org/apache/hadoop/hbase/master/HMaster.java
M src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
  ServerConnection is gone.  Use HConnection instead.
M src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
  Renamed TableServers as HConnectionImplementation
  Use RootRegionTracker for figuring root location. Made it so could get
  rid of some locateRegion code.
  Added clearRegionCache(tableName) override.
M src/main/java/org/apache/hadoop/hbase/client/HConnection.java
  Added clearRegionCache(tableName)
M src/main/java/org/apache/hadoop/hbase/client/ServerConnection.java
M src/main/java/org/apache/hadoop/hbase/client/ServerConnectionManager.java
  Deleted crazy stuff.
M src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
  On delete of table, don't delete connection; connection is more than 
  just a connection to an individual table.
{code};;;","10/Sep/10 00:07;stack;Committed  w/o review though a fairly substantial patch; lots touched but its mostly just fixing failing tests and remove of SC and SCM.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix broken build caused by hbase-2692 commit,HBASE-2954,12473085,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,01/Sep/10 21:39,20/Nov/15 12:43,14/Jul/23 06:06,01/Sep/10 21:41,,,,,,,,,,,,0.90.0,,,,,,,0,,,TestReplication is broke because the replication stuff needs to be carried forward to sit on top of new zk pattern.   The TestSplitLog was a silly oversight on my part.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/10 21:40;stack;2954.txt;https://issues.apache.org/jira/secure/attachment/12453629/2954.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26553,,,,,Fri Nov 20 12:43:18 UTC 2015,,,,,,,,,,"0|i0hk0v:",100506,,,,,,,,,,,,,,,,,,,,,"01/Sep/10 21:41;stack;Committed small patch.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin/hbase  shell broken,HBASE-2948,12473002,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,anih,anih,01/Sep/10 05:37,20/Nov/15 12:41,14/Jul/23 06:06,01/Sep/10 23:16,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"hbase shell is broken after master rewrite merge:

hbase(main):001:0> status

ERROR: undefined method `getZooKeeperWrapper' for #<#<Class:01x17eda64e>:0x73415727>

Here is some help for this command:
          Show cluster status. Can be 'summary', 'simple', or 'detailed'. The
          default is 'summary'. Examples:

            hbase> status
            hbase> status 'simple'
            hbase> status 'summary'
            hbase> status 'detailed'


hbase(main):001:0> list
TABLE                                                                                                                                                                           

ERROR: undefined method `getZooKeeperWrapper' for #<#<Class:01x63220fd1>:0x513c952f>

Here is some help for this command:
          List all tables in hbase. Optional regular expression parameter could
          be used to filter the output. Examples:

            hbase> list
            hbase> list 'abc.*'
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/10 07:13;anih;admin.rb.patch;https://issues.apache.org/jira/secure/attachment/12453577/admin.rb.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26552,Reviewed,,,,Fri Nov 20 12:41:57 UTC 2015,,,,,,,,,,"0|i0hjzj:",100500,,,,,,,,,,,,,,,,,,,,,"01/Sep/10 07:15;anih;patch available;;;","01/Sep/10 14:40;stack;Committed. Thanks for the fixup Sebastian.;;;","01/Sep/10 14:58;anih;you didn't commit admin.rb ;);;;","01/Sep/10 16:11;stack;@Sebastian Thanks for flagging my mess up.... done now.;;;","01/Sep/10 23:16;stack;I committed it for sure on Sebastian's prompting.  Reresolving.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cannot alter bloomfilter setting for a column family from hbase shell,HBASE-2944,12472985,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,kannanm,kannanm,kannanm,01/Sep/10 00:23,20/Nov/15 12:40,14/Jul/23 06:06,01/Sep/10 00:33,,,,,,,,,,,,0.90.0,,,,,,,0,,,"{code}
hbase(main):002:0> create 't1', 'cf'
create 't1', 'cf'
0 row(s) in 1.1320 seconds

hbase(main):003:0> disable 't1'
disable 't1'
0 row(s) in 1.0810 seconds

hbase(main):004:0> alter 't1', {NAME => 'cf', BLOOMFILTER => 'ROW'}
alter 't1', {NAME => 'cf', BLOOMFILTER => 'ROW'}

ERROR: no constructor with arguments matching [class org.jruby.java.proxies.ArrayJavaProxy, class org.jruby.RubyFixnum, class org.jruby.RubyString, class org.jruby.RubyBoolean, class org.jruby.RubyBoolean, class org.jruby.RubyFixnum, class org.jruby.RubyFixnum, class org.jruby.RubyBoolean, class org.jruby.RubyFixnum] on object #<Java::OrgApacheHadoopHbase::HColumnDescriptor:0x1e4218cb>
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/10 00:27;kannanm;HBASE-2944.txt;https://issues.apache.org/jira/secure/attachment/12453563/HBASE-2944.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26550,,,,,Fri Nov 20 12:40:46 UTC 2015,,,,,,,,,,"0|i0hjyv:",100497,,,,,,,,,,,,,,,,,,,,,"01/Sep/10 00:25;kannanm;Minor fix needed to admin.rb script. When BLOOMFILTER type
was changed from boolean to string, looks like code path was missed.
Will upload patch shortly.
;;;","01/Sep/10 00:33;streamy;Committed to trunk.  Thanks Kannan!;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
major_compact (and other admin commands) broken for .META.,HBASE-2943,12472979,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,streamy,streamy,31/Aug/10 22:48,20/Nov/15 12:41,14/Jul/23 06:06,02/Sep/10 20:41,,,,,,,,,,,,0.90.0,,,,,,,0,,,Table admin commands seem to be broken against META.  Implementation is new in master rewrite branch so should wait until that goes in to see if this bug still exists.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/10 20:02;stack;2943-v2.txt;https://issues.apache.org/jira/secure/attachment/12453722/2943-v2.txt","02/Sep/10 00:12;stack;2943.txt;https://issues.apache.org/jira/secure/attachment/12453643/2943.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26549,Reviewed,,,,Fri Nov 20 12:41:30 UTC 2015,,,,,,,,,,"0|i0hjyn:",100496,,,,,,,,,,,,,,,,,,,,,"31/Aug/10 22:51;jdcryans;HBASE-2788 ?;;;","31/Aug/10 23:25;streamy;Mostly.  The issue isn't in the shell, afaik, it's in a method called getTableRegions where we are not looking at ROOT when doing something to META (we look in META for META), doh.;;;","02/Sep/10 00:12;stack;Start in on a fix.  Also adds unit test for MetaReader.  Not done yet.;;;","02/Sep/10 20:02;stack;{code}
Bug fix was making MetaReader deal when passed .META. and -ROOT-; presumption
was for most methods that table referred to was always user table.  This
made calls to compact, etc., break.

M src/test/java/org/apache/hadoop/hbase/catalog/TestMetaReaderEditor.java
  Added a bunch of tests for MetaReader mostly.
M src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java
M src/main/java/org/apache/hadoop/hbase/avro/AvroServer.java
  Convert the IE that compact can now throw.
M src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java
  Renamed method to better explain what it does.
M src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
  Added override on constructor.
M src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java
  Revamped methods so can handle being passed .META. and -ROOT-.
  Renamed getServerRegions as getServerUserRegions.
M src/main/java/org/apache/hadoop/hbase/client/HConnection.java
  Javadoc spelling correction
M src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
  Let out IE.
{code};;;","02/Sep/10 20:38;jdcryans;+1, fix the following on commit if you think it's good:

I think it's more related to the master rewrite, but it would be nice to see the message on the region server when it arrives (before we saw it because it was coming from the master).

In HBA you don't give an explanation for the IE's javadoc, although the IO gets that treatment. For consistency, I would add a few words on how you can get that exception and what to do with it.;;;","02/Sep/10 20:41;stack;Committed.  Thanks for review J-D.  Got your first comment -- added logging of user initiated flush -- but didn't get second comment.  Will address later in a general review of how we now let out IEs.  Thanks.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom filters should not require registration in HbaseObjectWritable,HBASE-2942,12472965,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,ghelmling,ghelmling,ghelmling,31/Aug/10 20:21,20/Nov/15 12:42,14/Jul/23 06:06,08/Sep/10 20:28,,,,,,,,,,,,0.90.0,,Filters,,,,,0,,,"Some of the filter RPC serialization still requires that code -> class mappings be added to HbaseObjectWritable.  FilterList in particular requires this for it's child filters, since it calls HbaseObjectWritable.writeObject() on each.  This makes developing custom filters a big pain, as HbaseObjectWritable must be modified and the hbase core jar re-staged to the cluster.

We should fix this so that all filters can be written as a class name + data if no code exists.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2666,,,,,,,,,,,,,,,,,,,,,"08/Sep/10 17:04;ghelmling;HBASE_2942.patch;https://issues.apache.org/jira/secure/attachment/12454127/HBASE_2942.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26548,Reviewed,,,,Fri Nov 20 12:42:17 UTC 2015,,,,,,,,,,"0|i0hjyf:",100495,,,,,,,,,,,,,,,,,,,,,"08/Sep/10 17:04;ghelmling;The attached patch uses the Writable class code for Writable implementations if no other code is found, and adds some unit tests around problem cases.  I believe this should fix any remaining problems that would have required added class codes to HbaseObjectWritable.;;;","08/Sep/10 17:07;ghelmling;The fix here is pretty trivial, so didn't think it needed to go to reviewboard.  The attached patch just uses the class code for Writable for Writable implementations when no other class code is found.  There is existing code in writeObject() for serializing the class name for custom writables, but we were bailing early due to the missing class code.;;;","08/Sep/10 20:28;apurtell;Committed to trunk and branch.;;;","08/Sep/10 22:20;stack;Good on Gary.  Thanks.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
port HADOOP-6713 - threading scalability for RPC reads - to HBase,HBASE-2941,12472888,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,30/Aug/10 22:31,20/Nov/15 12:42,14/Jul/23 06:06,16/Sep/10 21:39,0.20.6,0.89.20100621,,,,,,,,,,0.89.20100924,0.90.0,,,,,,0,,,"HADOOP-6713 has  patch to fix the read scalability of hadoop rpc.  Right now a single thread accepts() then receives the RPC payload for every single RPC in hbase. Including object creation, writable deserialization, etc.

Apply the patch from that issue to our own forked HBaseRPC code.",,anty,larsfrancke,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/10 22:06;ryanobjc;HBASE-2941.patch;https://issues.apache.org/jira/secure/attachment/12453553/HBASE-2941.patch","14/Sep/10 21:20;ryanobjc;HBASE-2941.txt;https://issues.apache.org/jira/secure/attachment/12454592/HBASE-2941.txt","31/Aug/10 21:46;ryanobjc;HBASE-2941.xlsx;https://issues.apache.org/jira/secure/attachment/12453547/HBASE-2941.xlsx","02/Sep/10 18:52;ted_yu;HBaseServer.java;https://issues.apache.org/jira/secure/attachment/12453713/HBaseServer.java","31/Aug/10 22:39;tlipcon;Rplot001.png;https://issues.apache.org/jira/secure/attachment/12453558/Rplot001.png",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26547,,,,,Fri Nov 20 12:42:09 UTC 2015,,,,,,,,,,"0|i0hjxz:",100493,,,,,,,,,,,,,,,,,,,,,"31/Aug/10 21:45;ryanobjc;Doing some testing on this, using ycsb as a read load generator (gets), using 50 and 200 threads client side, with the patch from HBASE-2939, I am showing all sorts of great improvements:

- the latency curve slides inwards
- the average request time goes down from .4-.8 ms

Overall it's a huge huge win.  I'll attach the XLSX that has graphs and data from my runs - 8 sheets in total.;;;","31/Aug/10 22:21;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/752/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

HBASE-2941 - multithreaded read for server rpc, port of HADOOP-6713


This addresses bug HBASE-2941.
    http://issues.apache.org/jira/browse/HBASE-2941


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java 991357 

Diff: http://review.cloudera.org/r/752/diff


Testing
-------

see jira for performance spreadsheet


Thanks,

Ryan


;;;","31/Aug/10 22:39;tlipcon;I did a plot that overlays the results from Ryan's 100-thread tests with 1 server thread vs 10 server threads. Attaching here for those whose excel skills are weak like mine :);;;","01/Sep/10 00:15;hbasereviewboard;Message from: ""Ted Yu"" <ted_yu@yahoo.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/752/#review1067
-----------------------------------------------------------



trunk/src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
<http://review.cloudera.org/r/752/#comment3391>

    Should we come out of the while loop ?



trunk/src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
<http://review.cloudera.org/r/752/#comment3392>

    Should the default value be higher ?


- Ted



;;;","01/Sep/10 00:36;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/752/#review1068
-----------------------------------------------------------



trunk/src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
<http://review.cloudera.org/r/752/#comment3393>

    the while (Running) condition will catch this and exit once the server wants to shut down.



trunk/src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
<http://review.cloudera.org/r/752/#comment3394>

    yes it should, i have been testing @ 1 and 10 threads, probably 10 makes sense considering how heavily we use hbase rpc.


- Ryan



;;;","02/Sep/10 18:52;ted_yu;Here is HBaseServer.java from 0.20.6 with Ryan's patch;;;","14/Sep/10 21:20;ryanobjc;updated version i also put to review board;;;","14/Sep/10 21:31;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/752/
-----------------------------------------------------------

(Updated 2010-09-14 14:19:53.985785)


Review request for hbase.


Changes
-------

use shutdownNow() to interrupt the reader threads so the server will cleanly shut down.


Summary
-------

HBASE-2941 - multithreaded read for server rpc, port of HADOOP-6713


This addresses bug HBASE-2941.
    http://issues.apache.org/jira/browse/HBASE-2941


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java 997098 

Diff: http://review.cloudera.org/r/752/diff


Testing
-------

see jira for performance spreadsheet


Thanks,

Ryan


;;;","14/Sep/10 23:03;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/846/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

HBASE-2941 reader threads for hbase rpc


This addresses bug HBASE-2941.
    http://issues.apache.org/jira/browse/HBASE-2941


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java d3c6c21 

Diff: http://review.cloudera.org/r/846/diff


Testing
-------


Thanks,

Ryan


;;;","15/Sep/10 00:25;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/846/#review1213
-----------------------------------------------------------


Looks good.  Do the RPC tests pass?  There are a few small things below.


src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
<http://review.cloudera.org/r/846/#comment4124>

    What happens now if key is not acceptable?



src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
<http://review.cloudera.org/r/846/#comment4125>

    Now we let the IE out?



src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
<http://review.cloudera.org/r/846/#comment4126>

    No need of the limit of ten anymore because pool is bounded?


- stack



;;;","15/Sep/10 01:11;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/846/#review1216
-----------------------------------------------------------

Ship it!



src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
<http://review.cloudera.org/r/846/#comment4128>

    do you not have to wait here until indication that the selector thread has woken up and is in the ""wait"" loop? I'm not 100% sure of the state preconditions for the NIO stuff


- Todd



;;;","15/Sep/10 03:44;hbasereviewboard;Message from: ""Andrew Purtell"" <apurtell@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/846/#review1219
-----------------------------------------------------------



src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
<http://review.cloudera.org/r/846/#comment4132>

    This won't happen now?


- Andrew



;;;","15/Sep/10 03:44;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/846/#review1217
-----------------------------------------------------------



src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
<http://review.cloudera.org/r/846/#comment4131>

    this seems to be some kind of coordinate hand off so that the reader gets notified immediately of new things to read.  
    
    this is mostly a straightforward port of the hadoop bug so i cant answer to the suitability of the design really... but it does work... reliably.



src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
<http://review.cloudera.org/r/846/#comment4129>

    Not really sure, i dont think it should be possible, since this thread will only have 'accept' bits set on the sockets... but this is just a straightforward port of the HADOOP bug.



src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
<http://review.cloudera.org/r/846/#comment4130>

    they used to accept up to 10 connections, then read from 1, then go back into the select loop.  
    
    now the listener thread only accepts 1, then passes that to another thread to do the reads, so it doesnt have to attempt to keep socket accept() delays down by accepting batches at a time.


- Ryan



;;;","15/Sep/10 03:56;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/846/#review1220
-----------------------------------------------------------



src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
<http://review.cloudera.org/r/846/#comment4134>

    The IE doesnt get thrown anymore, it was being thrown inside doRead() which we no longer call...


- Ryan



;;;","15/Sep/10 04:15;hbasereviewboard;Message from: ""Andrew Purtell"" <apurtell@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/846/#review1221
-----------------------------------------------------------

Ship it!


- Andrew



;;;","15/Sep/10 04:31;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/846/#review1222
-----------------------------------------------------------

Ship it!


+1 from me too

- Todd



;;;","16/Sep/10 21:39;ryanobjc;committed to trunk;;;","24/Sep/10 18:16;jdcryans;Adding to latest 0.89;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Refactor ""Corrupt Data"" Tests in TestHLogSplit",HBASE-2935,12472511,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,posix4e,nspiegelberg,nspiegelberg,25/Aug/10 18:08,20/Nov/15 12:43,14/Jul/23 06:06,04/Nov/10 23:40,0.89.20100621,,,,,,,,,,,0.90.0,,test,,,,,0,,,"While fixing HBASE-2643, I noticed that a couple of the HLogSplit tests from HBASE-2437 were now failing.  3 tests are trying to detect proper handling of garbage data: testCorruptedFileGetsArchivedIfSkipErrors, testTrailingGarbageCorruptionLogFileSkipErrorsFalseThrows, testCorruptedLogFilesSkipErrorsFalseDoesNotTouchLogs.  However, these tests are corrupting data at the HBase level.  Data corruption should be tested at the HDFS level, because the filesystem is responsible for data validation.  These tests need to inject corrupt data at the HDFS level & then verify that ChecksumExceptions are thrown.",,larsfrancke,posix4e,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2643,HBASE-2437,,,,,,,,,,,,"29/Oct/10 22:48;posix4e;0001-HBASE-2935.-fix-testCorruptedFileGetsArchivedIfSkipE.patch;https://issues.apache.org/jira/secure/attachment/12458457/0001-HBASE-2935.-fix-testCorruptedFileGetsArchivedIfSkipE.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26546,Reviewed,,,,Fri Nov 20 12:43:10 UTC 2015,,,,,,,,,,"0|i0hjwv:",100488,https://review.cloudera.org/r/1115/,,,,,,,,,,,,,,,,,,,,"10/Oct/10 22:01;posix4e;I am not exactly sure what we are testing with this jira.

- Are we verifying that hdfs throws the ChecksumException, shouldn't this be an hdfs test?
- Is it enough to just force cause a checksumException and to make sure the logsplitter handles it correctly?
;;;","11/Oct/10 03:21;nspiegelberg;3 tests were removed, so this jira is about evaluating whether we should add them back in and to what extent.  My suggestion is to run a system-level test, where we corrupt at the HDFS level and ensure that everything percolate up and gets handled properly.  Just throwing a ChecksumException is fine to simply test the splitLog() logic but defeats the previous idea that Cosmin had of actually corrupting the underlying file data.;;;","11/Oct/10 13:50;posix4e;At first I believed the same thing. On the other hand we do need to test the checksum exception as it is a code path. How about I do the checksum exception one first and create a new jira for corrupting hdfs. Also, it's my personal opinion that making sure HDFS causes a checksum exception is really a hdfs thing, not a hbase thing.;;;","11/Oct/10 13:51;posix4e;I should mention I am more than happy to take this one on, I just wanted some clarification first.;;;","29/Oct/10 00:04;posix4e;https://review.cloudera.org/r/1115/;;;","29/Oct/10 00:04;posix4e;https://review.cloudera.org/r/1115/;;;","29/Oct/10 00:05;posix4e;If y'all want I can mangle some underlying blocks in a test as well?;;;","29/Oct/10 20:26;hbasereviewboard;Message from: ""Alex Newman"" <newalex@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1115/
-----------------------------------------------------------

(Updated 2010-10-29 13:24:54.726844)


Review request for hbase and Nicolas.


Summary
-------

So I took the approach taken with the instrumented sequence file rider. Not sure if exposing those methods in the WALReader is cool so I am down with other suggestions.

- Add a FaultySequenceFileLogReader
- Un-commented the 2935 tests


This addresses bug hbase-2935.
    http://issues.apache.org/jira/browse/hbase-2935


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java c8a10af 
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java 0b7dc78 
  src/test/java/org/apache/hadoop/hbase/regionserver/wal/FaultySequenceFileLogReader.java PRE-CREATION 
  src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java 473c359 

Diff: http://review.cloudera.org/r/1115/diff


Testing
-------

Running on our internal hudson as I type this. Also I ran the TestHLogSplit test locally. I'll update the review if hudson is green.


Thanks,

Alex


;;;","29/Oct/10 22:45;hbasereviewboard;Message from: ""Alex Newman"" <newalex@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1115/#review1714
-----------------------------------------------------------



src/test/java/org/apache/hadoop/hbase/regionserver/wal/FaultySequenceFileLogReader.java
<http://review.cloudera.org/r/1115/#comment5641>

    woops pretend this says 
        return FailureType.valueOf(conf.get(""faultysequencefilelogreader.failuretype"", FailureType.NONE.name()));
    


- Alex



;;;","29/Oct/10 22:48;posix4e;v2 based on stack's and ryan's input;;;","04/Nov/10 23:40;stack;Committed.  Thanks for the patch Alex.  Good one.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skip EOF Errors during Log Recovery,HBASE-2933,12472139,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,nspiegelberg,nspiegelberg,nspiegelberg,21/Aug/10 01:07,20/Nov/15 12:42,14/Jul/23 06:06,16/Oct/10 05:29,,,,,,,,,,,,0.90.0,,,,,,,0,,,"While testing a cluster, we hit upon the following assert during region assigment.  We were killing the master during a long run of splits.  We think what happened is that the HMaster was killed while splitting, woke up & split again.  If this happens, we will have 2 files: 1 partially written and 1 complete one.  Since encountering partial log splits upon Master failure is considered normal behavior, we should continue at the RS level if we encounter an EOFException & not an filesystem-level exception, even with skip.errors == false.

2010-08-20 16:59:07,718 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: Error opening MailBox_dsanduleac,57db45276ece7ce03ef7e8d9969eb189:99900000000008@facebook.com,1280960828959.7c542d24d4496e273b739231b01885e6.
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:375)
        at org.apache.hadoop.io.SequenceFile$Reader.readRecordLength(SequenceFile.java:1902)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1932)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1837)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1883)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:121)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:113)
        at org.apache.hadoop.hbase.regionserver.HRegion.replayRecoveredEdits(HRegion.java:1981)
        at org.apache.hadoop.hbase.regionserver.HRegion.replayRecoveredEdits(HRegion.java:1956)
        at org.apache.hadoop.hbase.regionserver.HRegion.replayRecoveredEditsIfAny(HRegion.java:1915)
        at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:344)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.instantiateRegion(HRegionServer.java:1490)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openRegion(HRegionServer.java:1437)
        at org.apache.hadoop.hbase.regionserver.HRegionServer$Worker.run(HRegionServer.java:1345)
        at java.lang.Thread.run(Thread.java:619)
2010-08-20 16:59:07,719 ERROR org.apache.hadoop.hbase.regionserver.RSZookeeperUpdater: Aborting open of region 7c542d24d4496e273b739231b01885e6",,dhruba,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-3081,,,,HBASE-2337,HBASE-2643,,,,,,,,,,,HADOOP-6986,"24/Sep/10 18:04;nspiegelberg;HBASE-2933.patch;https://issues.apache.org/jira/secure/attachment/12455505/HBASE-2933.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26545,Reviewed,,,,Fri Nov 20 12:42:34 UTC 2015,,,,,,,,,,"0|i0hjwn:",100487,,,,,,,,,,,,,,,,,,,,,"21/Aug/10 01:54;tlipcon;Hrm, isn't this basically the same as that other ticket about log splitting? ie if the master gets killed during splitting, the next master should completely resplit and the half-done log splits should be removed, right?;;;","21/Aug/10 03:36;nspiegelberg;Linking related issues that I found.  @todd: I don't recall seeing one about deleting the old logs at Master startup.  I remember addressing it in HBASE-2337, so may it didn't get properly ported to 0.90.  I agree that we should delete old split logs on master startup + split.  Should we also be making sure that any EOF exceptions don't force the region offline, especially if they are still archived?;;;","25/Aug/10 02:19;tlipcon;I can't remember the particular JIRA either, but it seems to me that the regionserver shouldn't even get to the point of doing recovery if the logs haven't been completely recovered. ie the phases should be:

1) Original RS is writing logs and dies
2) Master A notices failure and starts splitting logs. It gets halfway through writing region_1/oldlog
3) Master A dies
4) Master B takes over, and knows from ZK that RS's recovery is incomplete.
5) Master B should remove the half-written log split done by Master A, and try again from the start.

ie no region server should attempt to open region 1 until the logs have been properly split. Thus, the RS should never see an EOFException on log recovery, since it indicates that log splitting is incomplete.;;;","25/Aug/10 05:22;streamy;I believe this happened when a cluster was being forcibly shutdown in the middle of log splitting on the master.  The new master came up acting like a cluster startup not failover.  I'm not even sure if this makes a difference or not in the current master but it might on the new master.  I guess in both cases we should be doing the same log splitting checks?;;;","25/Aug/10 20:16;tlipcon;Yep - even with good working master failover, you might lose power on the cluster, in which case all the masters would die. We'd have a ""clean startup"" but to not lose data we have to split *all* the logs (woo!);;;","08/Sep/10 04:53;stack;Bringing into 0.90.  Need to write a test to ensure that new master removes old partially split logs if old master died mid-split; also make it so we don't die if RS gets EOF -- though this should never happen as Todd says if proper split -- but also we should keep going if we get something like IOE """"File is corrupt!"" (See below)

{code}
    private synchronized int readRecordLength() throws IOException {
      if (in.getPos() >= end) {
        return -1;
      }
      int length = in.readInt();
      if (version > 1 && sync != null &&
          length == SYNC_ESCAPE) {              // process a sync entry
        in.readFully(syncCheck);                // read syncCheck
        if (!Arrays.equals(sync, syncCheck))    // check it
          throw new IOException(""File is corrupt!"");
        syncSeen = true;
        if (in.getPos() >= end) {
          return -1;
        }
        length = in.readInt();                  // re-read length
      } else {
        syncSeen = false;
      }

      return length;
    }
{code};;;","08/Sep/10 05:12;stack;Looking in logs I see this kinda thing:

{code}
2010-09-07 18:10:27,965 WARN org.apache.hadoop.hbase.regionserver.wal.HLog: Found existing old edits file. It could be the result of a previous failed split attempt. Deleting hdfs://sv4borg9:9000/hbase/api_access_token_stats_day/1845102219/recovered.edits/0000000068762427569, length=264167
{code}

.. so we're some cleanup of old split attempts.;;;","24/Sep/10 18:04;nspiegelberg;Handles EOFE and the IOE that was mentioned by stack.  The SequenceFile.Reader has a few more IOEs, so this isn't 100% fail-proof.  The general problem we seem to have is that we need to differentiate between a Network IOE and a File Format IOE.  A File Format IOE is idempotent, where a Network IOE may not be.

Network = we need to fail and let another server try to take over
FileFormat = our file was written or parsed incorrectly. retrying won't fix anything. We need to just open what we have and store the original file away for later analysis.;;;","01/Oct/10 23:41;stack;Marking as patch available;;;","04/Oct/10 22:04;nspiegelberg;Note that I created HADOOP-6986 to fix all edge-cases of ParseExceptions in the SequenceFile.Reader.  I will create a new jira to address the edge-cases once this fix is in 0.20-append.;;;","05/Oct/10 20:36;nspiegelberg;@Stack : This jira should be good to commit, right?  We can address the edge cases with HBASE-3081;;;","16/Oct/10 05:29;stack;@Nicolas Thanks for clarification.  Committed.  Thanks for the patch.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Do not throw RuntimeExceptions in RPC/HbaseObjectWritable code, ensure we log and rethrow as IOE",HBASE-2931,12472127,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,karthik.ranga,streamy,streamy,20/Aug/10 21:10,20/Nov/15 12:42,14/Jul/23 06:06,22/Aug/10 22:21,,,,,,,,,,,,0.90.0,,,,,,,0,,,"When there are issues with RPC and HbaseObjectWritable, primarily when server and client have different jars, the only thing that happens is the client will receive an EOF exception.  The server does not log what happened at all and the client does not receive a server trace, rather the server seems to close the connection and the client gets an EOF because it tries to read off of a closed stream.

We need to ensure that we catch, log, and rethrow as IOE any exceptions that may occur because of an issue with RPC or HbaseObjectWritable.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/10 16:56;karthik.ranga;HBASE-2931.patch;https://issues.apache.org/jira/secure/attachment/12452746/HBASE-2931.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26544,Reviewed,,,,Fri Nov 20 12:42:38 UTC 2015,,,,,,,,,,"0|i0hjw7:",100485,,,,,,,,,,,,,,,,,,,,,"22/Aug/10 16:56;karthik.ranga;Simple patch - posting directly instead of review board.

Moves the newly added class down to the end of the object writeable opcode list so that all subsequent op-codes do not change. Also added some logging.;;;","22/Aug/10 22:21;stack;Applied to TRUNK.  Thanks for the patch Karthik.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fault in logic in BinaryPrefixComparator leads to ArrayIndexOutOfBoundsException.,HBASE-2928,12471944,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,pranavkhaitan,pranavkhaitan,pranavkhaitan,18/Aug/10 21:19,20/Nov/15 12:43,14/Jul/23 06:06,18/Aug/10 22:29,,,,,,,,,,,,0.90.0,,Filters,,,,,0,,,"Following statement makes an incorrect assumption that value.length >= this.value.length.

return Bytes.compareTo(this.value, 0, this.value.length, value, 0,
      this.value.length);",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26542,,,,,Fri Nov 20 12:43:09 UTC 2015,,,,,,,,,,"0|i0hjvr:",100483,,,,,,,,,,,,,,,,,,,,,"18/Aug/10 21:58;hbasereviewboard;Message from: ""Pranav Khaitan"" <pranavkhaitan@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/686/
-----------------------------------------------------------

Review request for hbase and Jonathan Gray.


Summary
-------

Following statement makes an incorrect assumption that value.length >= this.value.length.

return Bytes.compareTo(this.value, 0, this.value.length, value, 0,
this.value.length);


(Also, removed a couple of unused imports)


This addresses bug HBASE-2928.
    http://issues.apache.org/jira/browse/HBASE-2928


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/filter/BinaryPrefixComparator.java 986945 
  trunk/src/main/java/org/apache/hadoop/hbase/filter/ColumnPrefixFilter.java 986945 

Diff: http://review.cloudera.org/r/686/diff


Testing
-------


Thanks,

Pranav


;;;","18/Aug/10 22:00;hbasereviewboard;Message from: ""Pranav Khaitan"" <pranavkhaitan@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/686/
-----------------------------------------------------------

(Updated 2010-08-18 14:46:42.108879)


Review request for hbase and Jonathan Gray.


Summary
-------

Following statement makes an incorrect assumption that value.length >= this.value.length.

return Bytes.compareTo(this.value, 0, this.value.length, value, 0,
this.value.length);


(Also, removed a couple of unused imports)


This addresses bug HBASE-2928.
    http://issues.apache.org/jira/browse/HBASE-2928


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/filter/BinaryPrefixComparator.java 986945 
  trunk/src/main/java/org/apache/hadoop/hbase/filter/ColumnPrefixFilter.java 986945 

Diff: http://review.cloudera.org/r/686/diff


Testing (updated)
-------

All tests based on PrefixFilter run successfully.


Thanks,

Pranav


;;;","18/Aug/10 22:29;streamy;Gave +1 on rb.  Committed to trunk.  Thanks pranav.;;;","18/Aug/10 22:35;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/686/#review949
-----------------------------------------------------------

Ship it!


Looks good to me.

- Jonathan



;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BaseScanner gets stale HRegionInfo in some race cases,HBASE-2927,12471937,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,jdcryans,jdcryans,jdcryans,18/Aug/10 19:31,20/Nov/15 12:43,14/Jul/23 06:06,18/Aug/10 19:42,0.20.6,0.89.20100621,,,,,,,,,,0.90.0,,,,,,,0,,,"This issue is a branch of HBASE-2812, as it is a specific fix for disabling tables in some situations. The issue is that the BaseScanner can get a stale HRI by the time it checks if the region is OFFLINE, and even in trunk where we do a double checkAssigned there is no way for us to tell if the region was offlined in the mean time.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/10 19:33;jdcryans;HBASE-2927.patch;https://issues.apache.org/jira/secure/attachment/12452440/HBASE-2927.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26541,Reviewed,,,,Fri Nov 20 12:43:49 UTC 2015,,,,,,,,,,"0|i0hjvj:",100482,,,,,,,,,,,,,,,,,,,,,"18/Aug/10 19:33;jdcryans;Same patch that was posted in HBASE-2812, going to commit as it was tested/reviewed already.;;;","18/Aug/10 19:42;jdcryans;Committed to branch and the part about the HRI to trunk.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LRU of HConnectionManager.HBASE_INSTANCES breaks if HBaseConfiguration is changed,HBASE-2925,12471856,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,rmahfoud,rmahfoud,17/Aug/10 21:27,20/Nov/15 12:42,14/Jul/23 06:06,06/Sep/10 15:52,0.20.3,0.90.0,,,,,,,,,,0.90.0,,,,,,,0,,,"{{HConnectionManager.getConnection(config)}} caches the created {{TableServer}} in {{HBASE_INSTANCES}} (a {{LinkedHashMap}} ) which is keyed by the configuration instance itself.
Given the current implementation of {{hashCode()}} (and {{equals()}}) of {{HBaseConfiguration}}, the hash code of the configuration is changed if any of its properties are changed, which will cause the keys of {{HBASE_INSTANCES}} to be inconsistent with the hashtable that contains them, making some entries unreachable.
In this case, when the map's LRU strategy needs to remove the oldest entry, it tries to remove it based on the oldest key, which no longer gives the original hash code, therefore the lookup in {{HBASE_INSTANCES.remove(oldest)}} doesn't actually remove anything.

This has been observed to lead to OOM errors in long running clients.
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2027,,,,,,HBASE-1251,HBASE-1976,HBASE-3773,HBASE-3777,,,,,,,,,,,,,,"03/Sep/10 20:15;stack;2925-v2.txt;https://issues.apache.org/jira/secure/attachment/12453820/2925-v2.txt","03/Sep/10 04:54;stack;2925.txt;https://issues.apache.org/jira/secure/attachment/12453747/2925.txt","17/Aug/10 21:51;rmahfoud;SimpleHConnectionManagerLeakReplicator.java;https://issues.apache.org/jira/secure/attachment/12452332/SimpleHConnectionManagerLeakReplicator.java",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26540,Reviewed,,,,Fri Nov 20 12:42:13 UTC 2015,,,,,,,,,,"0|i0hjv3:",100480,,,,,,,,,,,,noob,,,,,,,,,"17/Aug/10 21:51;rmahfoud;This is a class that I put together that illustrates this problem and replicate it. Could be used as a basis for the unit test for a patch.;;;","17/Aug/10 22:07;rmahfoud;The only fix I could think of is to clone the configuration before inserting in the new {{TableServer}} in {{HBASE_INSTANCES}}. This way, LRU will work since the key cannot be changed since no other object holds a reference to it.
This will add the overhead of creating a new {{HBaseConfiguration}} instance with every connection, which is minimal if the cache works as it should.
;;;","03/Sep/10 02:59;stack;@Robert What if we just removed the dump hash and equals and used object equality instead?  I think it a bit much trying to equate Configuration objects; i.e. they are the same if they have ""same"" config where ""same"" is every properties's hash equates.  If we were to go the route of trying to equate Configurations by the properties they carry, we should equate the values 'true', 'True', 'TRUE', and if a boolean !0 (anything but zero).

Thanks for filing this issue.;;;","03/Sep/10 03:08;stack;Hmm... Looks like I thought removing hashcode from HBC a good idea a while back too (See comment in HBASE-1976).;;;","03/Sep/10 04:54;stack;Here's a start.  Its not done yet but shows direction: i.e. removing hashcode and equals from HBC.  TODO, is test that prove that HBASE-1251 is still fixed (I think thing to do here is just read configs down in TableServers out of the conf each time rather than once up front so if number of retries is changed mid-use, subsequent invocations will pick up new config. -- let me see).

{code}
Removes hashcode and equals from HBC.  Use Object hashcode instead
of try and have Configurations with same exact content somehow
equate.

M a/src/main/java/org/apache/hadoop/hbase/HBaseConfiguration.java
  Removed hashCode and equals.
M a/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
M a/src/main/java/org/apache/hadoop/hbase/util/HMerge.java
M a/src/test/java/org/apache/hadoop/hbase/HBaseClusterTestCase.java
M a/src/test/java/org/apache/hadoop/hbase/util/DisabledTestMetaUtils.java
  Renamed deleteConnectionInfo as deleteConnection.
M a/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
  Fixed up some comments and javadoc.  Moved a few methods around.
  Use Configuration instance itself as key for TableServer instances
  rather than the Configuation hash code int.
  Renamed deleteConnectionInfo as deleteConnection.
M a/src/main/java/org/apache/hadoop/hbase/client/HTable.java
  Fixed up javadoc trying to explain implication of not passing
  Configuration constructing an HTable instance.
M a/src/main/java/org/apache/hadoop/hbase/client/HTableFactory.java
  White space removal.
M a/src/test/java/org/apache/hadoop/hbase/client/TestHCM.java
  Added testNewConnectionsDoesNotOOME.  It calls slightly
  refactored Robert Mahfoud illustrative code.  Added assertions.
  Made it not run for ever.
{code};;;","03/Sep/10 04:55;stack;Oh Robert, I just noticed that you did not grant Apache license on your test code.  If intentional, np, I'll back your test out of my patch.  Just say so.;;;","03/Sep/10 17:51;rmahfoud;@stack, thank you for picking up this issue. Feel free to reuse the code provided in SimpleHConnectionManagerLeakReplicator.java in any way you want. (Let me know if this is not enough I could submit another file with Apache license added.)

Back to the discussion, I agree that removing the {{hasCode()}} and {{equals()}} methods does resolve this issue. However, I wonder if it defeats the purpose of having a connection cache from the first place. You also alluded to the problem of the configuration settings changing after the connection is open, which, if we rely solely on Object equality, could lead to subtle error or unexpected behavior.

Hence my suggestion to pin down the configuration once the TableServers instance is created. The disadvantage there is that small/unrelated changes in the configuration will invalidate the cache entry as well.

If that is deemed a real problem given the common use cases, I was thinking maybe we define ""equality"" of the configuration for the purposes of caching as the equality of all properties that start with {{hbase.}} or {{zk.}} or any other prefix whose properties could be used as possible connection parameters. Not sure if the comparison becomes too slow in that case, but what is more important is whether correctness is compromised if we make the assumption that only properties starting with those prefixes affect the connection.;;;","03/Sep/10 18:26;stack;@Robert ...I wonder if it defeats the purpose of having a connection cache from the first place.

As I see it, the main benefit to the cache is saving on region lookups, setup of zk connection, and master proxy setup.  Having the likes of the following config cached is secondary: e.g.

{code}
    private final long pause;
    private final int numRetries;
    private final int maxRPCAttempts;
    private final long rpcTimeout;
    private final int prefetchRegionLimit;
{code}

In fact, I'm now thinking that if a user changes any of the above in a Configuration that is being used as a key in HCM#HBASE_INSTANCES, that its ok if we don't notice the change as long as we doc the fact that the Configuration is read on instantiation of the HTable and then no more.  I don't tthink this should 'surprise' the user too much and they can just go create new HTable with the new Configuration if they really want their new config. to take hold (Making things work this way is similar to what you suggest only you would copy the Configuration before adding it as a key).

As to your last suggestion, I think we should move away from trying to equate Configurations at all; there be daemons that way.

Let me know what you think.  If you are agreeable, I'll work up the patch some more mostly adding doc clarifying what we've agreed here.




;;;","03/Sep/10 19:27;rmahfoud;{quote}
Making things work this way is similar to what you suggest only you would copy the Configuration before adding it as a key ...
{quote}
This is what I had in mind... Sorry if I didn't make it clear in previous comments.

{quote}
... its ok if we don't notice the change as long as we doc the fact that the Configuration is read on instantiation of the HTable and then no more ...
{quote}
I agree that these semantics are good for all the use cases I could think of. So feel free to proceed with your patch.
;;;","03/Sep/10 20:15;stack;This version of the patch just adds javadoc to HTable explaining the advantage of shared Configuration -- the sharing of zookeeper connection, cache of region locations, etc.

If you have a minute, give it a gander Robert and if its good w/ you, I'll go ahead and commit.;;;","06/Sep/10 04:18;rmahfoud;You may have left out a couple of print statements in TestHCM. Not sure if that was intentional.    

{code}
      System.out.println(""Hash Code: "" + configuration.hashCode());
      ...
          System.out.println(""!! Got same connection for once !!"");
{code}


Otherwise it's good to go.
;;;","06/Sep/10 15:52;stack;@Robert Yeah I removed some, left others but also added asserts so tests should fail is regression.  Thanks for doing up the test.  I just committed v2.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestLogRolling doesn't use the right HLog half the time,HBASE-2924,12471850,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,17/Aug/10 21:05,20/Nov/15 12:42,14/Jul/23 06:06,19/Aug/10 20:58,,,,,,,,,,,,0.90.0,,,,,,,1,,,"Since HBASE-2868, TestLogRolling uses 2 region servers instead of 1. The rest of the un-refactored code isn't expecting that, and only used the log from the first RS. This is why we get very inconsistent results. Fix by either coming back to 1 RS or at least use the right HLog.",,larsfrancke,posix4e,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/10 22:26;jdcryans;HBASE-2924.patch;https://issues.apache.org/jira/secure/attachment/12452334/HBASE-2924.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26539,Reviewed,,,,Fri Nov 20 12:42:35 UTC 2015,,,,,,,,,,"0|i0hjuv:",100479,,,,,,,,,,,,,,,,,,,,,"17/Aug/10 22:26;jdcryans;Patch that adds a new utility method to HBaseTestingUtility to get the region server that holds the first region in a table, which is used to select the right server in TestLogRolling. Test now passes 100% of the time (on 8 runs).;;;","17/Aug/10 22:49;larsfrancke;Applied the patch, ran the test five times. Everything looks fine now. Thanks for finding and fixing this!

+1;;;","18/Aug/10 07:32;stack;+1 though I do not like your addition to HBaseTestingUtiliy.  My guess is that this test is only test that will use the added method.  We shoudl be a little judicious about what we add HTU.  We don't want it to be turn into a hard to maintain dumping ground.;;;","19/Aug/10 20:58;jdcryans;I refactored TestScannerTimeout to use the new tool too, which is now more commented. Committed to trunk.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock between HRegion.internalFlushCache and close,HBASE-2923,12471830,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,17/Aug/10 18:30,20/Nov/15 12:43,14/Jul/23 06:06,17/Aug/10 18:45,,,,,,,,,,,,0.90.0,,,,,,,0,,,"HBASE-2461 added a synchronize on close(), but it's deadlocking with internalFlushCache. We should just check after getting the write locks if the region is already closed.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/10 18:39;jdcryans;HBASE-2923.patch;https://issues.apache.org/jira/secure/attachment/12452302/HBASE-2923.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26538,Reviewed,,,,Fri Nov 20 12:43:52 UTC 2015,,,,,,,,,,"0|i0hjun:",100478,,,,,,,,,,,,,,,,,,,,,"17/Aug/10 18:39;jdcryans;Removes synchronize, adds a check is the region is closed and returns null if so. TestScanner now passes.;;;","17/Aug/10 18:41;stack;+1;;;","17/Aug/10 18:45;jdcryans;Committed to trunk, thanks for the review Stack!;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTable.checkAndPut/Delete doesn't handle null values,HBASE-2920,12471721,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,jdcryans,jdcryans,jdcryans,16/Aug/10 17:27,20/Nov/15 12:42,14/Jul/23 06:06,25/Aug/10 19:00,0.89.20100621,,,,,,,,,,,0.90.0,,,,,,,1,,,"From John Beatty on the ML:

{quote}
Thanks Ryan, but I seem to be missing something then. It NPEs for me.
When running against 0.89.20100726 and providing a null expected value
I get the below stack trace (and works like a champ when I provide a
byte[0]. I also don't see the transformation you're referring to in
HTable.

(for reference,
http://svn.apache.org/viewvc/hbase/branches/0.89.20100726/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?view=markup)

java.io.IOException: java.io.IOException: java.lang.NullPointerException
       at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:845)
       at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:835)
       at org.apache.hadoop.hbase.regionserver.HRegionServer.checkAndMutate(HRegionServer.java:1754)
       at org.apache.hadoop.hbase.regionserver.HRegionServer.checkAndPut(HRegionServer.java:1773)
       at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
       at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:576)
       at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:919)
Caused by: java.lang.NullPointerException
       at org.apache.hadoop.hbase.regionserver.HRegion.checkAndMutate(HRegion.java:1616)
       at org.apache.hadoop.hbase.regionserver.HRegionServer.checkAndMutate(HRegionServer.java:1751)
       ... 6 more
{quote}

Looking in the code, I'm not sure either where the null conversion is done, even worse is that we don't even have unit tests! It should be put intoTestFromClientSide.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/10 16:51;jdcryans;HBASE-2920.patch;https://issues.apache.org/jira/secure/attachment/12453053/HBASE-2920.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26537,Reviewed,,,,Fri Nov 20 12:42:40 UTC 2015,,,,,,,,,,"0|i0hju7:",100476,,,,,,,,,,,,,,,,,,,,,"25/Aug/10 16:51;jdcryans;Simple patch that checks if the value is null before checking emptiness, along with unit test that fails without this patch.;;;","25/Aug/10 18:00;stack;+1;;;","25/Aug/10 19:00;jdcryans;Committed to trunk, thanks for looking at it Stack!;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
initTableReducerJob: Unused method parameter.,HBASE-2919,12471693,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,korusef,korusef,korusef,16/Aug/10 13:57,12/Jun/22 00:09,14/Jul/23 06:06,16/Aug/10 17:24,0.89.20100621,,,,,,,,,,,,,mapreduce,,,,,0,,,"In method org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(String, Class<? extends TableReducer>, Job, Class) the partitioner parameter was not passed to called org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(String, Class<? extends TableReducer>, Job, Class) method.",,,,,,,,,,,,,,,,,,,,,,,60,60,,0%,60,60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/10 14:04;korusef;hbase-2919.patch;https://issues.apache.org/jira/secure/attachment/12452172/hbase-2919.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26536,Reviewed,,,,Mon Aug 16 17:24:54 UTC 2010,,,,,,,,,,"0|i0hjtz:",100475,HBASE-2919 initTableReducerJob: method parameter not passed to work horse function.,,,,,,,,,,,,,,,,,,,,"16/Aug/10 14:04;korusef;Fixes the bug by passing the parameter.;;;","16/Aug/10 17:24;stack;Committed.  Thanks for the patch Libor (I added you as contributor and assigned you this issue and 2908).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SequenceFileLogWriter doesnt make it clear if there is no append by config or by missing lib/feature,HBASE-2918,12471595,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,,ryanobjc,ryanobjc,14/Aug/10 00:20,20/Nov/15 12:42,14/Jul/23 06:06,01/Sep/10 22:33,0.89.20100621,,,,,,,,,,,0.90.0,,,,,,,0,,,"This code doesnt make the situation clear:


    Method m = null;
    if (conf.getBoolean(""dfs.support.append"", false)) {
      try {
        // function pointer to writer.syncFs()
        m = this.writer.getClass().getMethod(""syncFs"", new Class<?> []{});
      } catch (SecurityException e) {
        throw new IOException(""Failed test for syncfs"", e);
      } catch (NoSuchMethodException e) {
        // Not available
      }
    }
    this.syncFs = m;
    LOG.info((this.syncFs != null)?
      ""Using syncFs -- HDFS-200"": ""syncFs -- HDFS-200 -- not available"");


we dont know if the test failed, or if the config is turned off.  it would make debuggers life easier if the message was clearer.",,larsfrancke,umamaheswararao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26535,,,,,Fri Nov 20 12:42:01 UTC 2015,,,,,,,,,,"0|i0hjtr:",100474,,,,,,,,,,,,,,,,,,,,,"01/Sep/10 22:33;stack;Committed the below which prints out state of the dfs.append config when logging append not available:

{code}
Index: src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java   (revision 991708)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java   (working copy)
@@ -91,7 +91,8 @@
     // Now do dirty work to see if syncFs is available.
     // Test if syncfs is available.
     Method m = null;
-    if (conf.getBoolean(""dfs.support.append"", false)) {
+    boolean append = conf.getBoolean(""dfs.support.append"", false);
+    if (append) {
       try {
         // function pointer to writer.syncFs()
         m = this.writer.getClass().getMethod(""syncFs"", new Class<?> []{});
@@ -103,7 +104,8 @@
     }
     this.syncFs = m;
     LOG.info((this.syncFs != null)?
-      ""Using syncFs -- HDFS-200"": ""syncFs -- HDFS-200 -- not available"");
+      ""Using syncFs -- HDFS-200"":
+      (""syncFs -- HDFS-200 -- not available, dfs.support.append="" + append));
   }
 
   @Override
{code}
;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock between HRegion.ICV and HRegion.close,HBASE-2915,12471578,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,13/Aug/10 19:20,20/Nov/15 12:40,14/Jul/23 06:06,23/Aug/10 22:49,,,,,,,,,,,,0.90.0,,,,,,,1,,,"HRegion.ICV gets a row lock then gets a newScanner lock.

HRegion.close gets a newScanner lock, slitCloseLock and finally waits for all row locks to finish.

If the ICV got the row lock and then close got the newScannerLock, both end up waiting on the other. This was introduced when Get became a Scan.

Stack thinks we can get rid of the newScannerLock in close since we setClosing to true.",,larsfrancke,tsuna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/10 22:43;jdcryans;HBASE-2915-v4.patch;https://issues.apache.org/jira/secure/attachment/12452874/HBASE-2915-v4.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26534,Reviewed,,,,Fri Nov 20 12:40:34 UTC 2015,,,,,,,,,,"0|i0hjt3:",100471,,,,,,,,,,,,,,,,,,,,,"19/Aug/10 17:21;jdcryans;There is another deadlock that needs fixing in the scope of this jira. Since the split code was redone, there's a deadlock when SplitTransaction acquires the splitAndCloses writelock while a flush is running for it. It looks like:

{noformat}

""regionserver60021.compactor"" daemon prio=10 tid=0x00007fc31845b800 nid=0x5f62 in Object.wait() [0x00007fc31e9e7000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:493)
	- locked <0x00007fc336877998> (a org.apache.hadoop.hbase.regionserver.HRegion$WriteState)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.execute(SplitTransaction.java:213)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.execute(SplitTransaction.java:186)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread.split(CompactSplitThread.java:157)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:87)

""regionserver60021.cacheFlusher"" daemon prio=10 tid=0x00007fc31845a000 nid=0x5f61 waiting on condition [0x00007fc31eae8000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007fc336561750> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:877)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1197)
	at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(ReentrantReadWriteLock.java:594)
	at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:793)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:249)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:223)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:146)
{noformat};;;","19/Aug/10 22:13;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/691/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

This patch removes newScannerLock and renames splitAndClose lock to just ""lock"". Every operation is now required to obtain the read lock on ""lock"" before doing anything (including getting a row lock). This is done by calling openRegionTransaction inside a try statement and by calling closeRegionTransaction in finally.

flushcache got refactored some more in order to do the locking in the proper order; first get the read lock, then do the writestate handling.

Finally, it removes the need to have a writeLock when flushing when subclassers give atomic work do to via internalPreFlushcacheCommit. This means that this patch breaks external contribs. This is required to keep our whole locking mechanism simpler.


This addresses bug HBASE-2915.
    http://issues.apache.org/jira/browse/HBASE-2915


Diffs
-----

  /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 987300 
  /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java 987300 
  /trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java 987300 

Diff: http://review.cloudera.org/r/691/diff


Testing
-------

5 concurrent ICV threads + randomWrite 3 + scans on a single RS. I'm also in the process of deploying it on a cluster.


Thanks,

Jean-Daniel


;;;","19/Aug/10 22:52;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/691/#review966
-----------------------------------------------------------



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3144>

    oh wow i cant believe this was ever here



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3145>

    i thought we agreed that the closing flag had to be set BEFORE the write lock was acquired to prevent race conditions?



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3146>

    lets just excise this and break compile time compatibility.  Also remove internalPreFlushcacheCommit too



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3147>

    ditto remove this whole try/finally bit



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3148>

    maybe we shouldnt call this 'transaction' might confuse people into thinking we support real transactions... not sure what to call it at this moment tho


- Ryan



;;;","19/Aug/10 22:59;hbasereviewboard;Message from: ""Ted Yu"" <ted_yu@yahoo.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/691/#review967
-----------------------------------------------------------



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3151>

    How about naming this method openRegionProlog ?



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3149>

    Please add:
    It has to be called inside the corresponding finally block



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3150>

    How about naming this method closeRegionEpilog ?


- Ted



;;;","19/Aug/10 23:33;hbasereviewboard;Message from: ""Ted Yu"" <ted_yu@yahoo.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/691/#review969
-----------------------------------------------------------



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3155>

    Or regionOperationProlog()



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3156>

    and regionOperationEpilog()


- Ted



;;;","20/Aug/10 22:22;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/691/#review973
-----------------------------------------------------------


Further testing shows that I completely forgot to properly lock incrementColumnValue, thus missing the point of the title of the jira :S Uploading a new patch soon that addresses that, along with comments from the reviews.

- Jean-Daniel



;;;","20/Aug/10 23:19;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/691/#review975
-----------------------------------------------------------


oh, one other thing, in discussions, we talked of no longer needing to wait on row locks to expire... I don't see this being excised from the close method.  Should that be in here?

- stack



;;;","20/Aug/10 23:24;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/691/#review974
-----------------------------------------------------------

Ship it!


+1  Nice fix.  A few comments below.


/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3163>

    I suppose this order is ok if the first thing we do on entrance to HRegion is get the read lock before check of closing.



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3164>

    Seems like you could use your opentransaction/closetransaction methods here and in flush too to be consistent?



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3165>

    Yeah, just remove.



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3166>

    Aren't these lines unnecessary?  openRegionTransaction does it?



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3167>

    So, this javadoc is good but do you think we need some more doc?  Does there need to be more detail on new locking regime? Maybe there is no more to be said that what is here in this paragraph.  You've done all the work unravelling our lock mess.  With time your nice unravelling will rot unless its clear what the pattern is.    I'm just trying to think of ways of preventing that happening.


- stack



;;;","20/Aug/10 23:32;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>


bq.  On 2010-08-20 15:57:34, stack wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 507
bq.  > <http://review.cloudera.org/r/691/diff/1/?file=7612#file7612line507>
bq.  >
bq.  >     I suppose this order is ok if the first thing we do on entrance to HRegion is get the read lock before check of closing.

So I just redid that part. setClosing is first taken so that when the client threads arrive they can fast fail by looking at closing.get before trying to get the readLock.


bq.  On 2010-08-20 15:57:34, stack wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 712
bq.  > <http://review.cloudera.org/r/691/diff/1/?file=7612#file7612line712>
bq.  >
bq.  >     Seems like you could use your opentransaction/closetransaction methods here and in flush too to be consistent?

Yeah the issue with compact and flush is that the callers don't expect to see NSRE, the want null values.


bq.  On 2010-08-20 15:57:34, stack wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 1115
bq.  > <http://review.cloudera.org/r/691/diff/1/?file=7612#file7612line1115>
bq.  >
bq.  >     Aren't these lines unnecessary?  openRegionTransaction does it?

Good catch.


bq.  On 2010-08-20 15:57:34, stack wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 3142
bq.  > <http://review.cloudera.org/r/691/diff/1/?file=7612#file7612line3142>
bq.  >
bq.  >     So, this javadoc is good but do you think we need some more doc?  Does there need to be more detail on new locking regime? Maybe there is no more to be said that what is here in this paragraph.  You've done all the work unravelling our lock mess.  With time your nice unravelling will rot unless its clear what the pattern is.    I'm just trying to think of ways of preventing that happening.

Yeah I'll included some more javadoc, maybe with code examples?


- Jean-Daniel


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/691/#review974
-----------------------------------------------------------



;;;","20/Aug/10 23:35;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>


bq.  On 2010-08-20 15:58:23, stack wrote:
bq.  > oh, one other thing, in discussions, we talked of no longer needing to wait on row locks to expire... I don't see this being excised from the close method.  Should that be in here?

Yep, it needs to go.


- Jean-Daniel


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/691/#review975
-----------------------------------------------------------



;;;","20/Aug/10 23:40;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-08-20 15:57:34, stack wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 507
bq.  > <http://review.cloudera.org/r/691/diff/1/?file=7612#file7612line507>
bq.  >
bq.  >     I suppose this order is ok if the first thing we do on entrance to HRegion is get the read lock before check of closing.
bq.  
bq.  Jean-Daniel Cryans wrote:
bq.      So I just redid that part. setClosing is first taken so that when the client threads arrive they can fast fail by looking at closing.get before trying to get the readLock.

Don't you have to check again the setClosing after you get the read lock?


bq.  On 2010-08-20 15:57:34, stack wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 712
bq.  > <http://review.cloudera.org/r/691/diff/1/?file=7612#file7612line712>
bq.  >
bq.  >     Seems like you could use your opentransaction/closetransaction methods here and in flush too to be consistent?
bq.  
bq.  Jean-Daniel Cryans wrote:
bq.      Yeah the issue with compact and flush is that the callers don't expect to see NSRE, the want null values.

OK. Not important. This is deep internal stuff or make a version that takes a flag on whether to throw exception (default throws exception .. might get messy though... not important).


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/691/#review974
-----------------------------------------------------------



;;;","20/Aug/10 23:44;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-08-20 15:57:34, stack wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 507
bq.  > <http://review.cloudera.org/r/691/diff/1/?file=7612#file7612line507>
bq.  >
bq.  >     I suppose this order is ok if the first thing we do on entrance to HRegion is get the read lock before check of closing.
bq.  
bq.  Jean-Daniel Cryans wrote:
bq.      So I just redid that part. setClosing is first taken so that when the client threads arrive they can fast fail by looking at closing.get before trying to get the readLock.
bq.  
bq.  stack wrote:
bq.      Don't you have to check again the setClosing after you get the read lock?
bq.  
bq.  Jean-Daniel Cryans wrote:
bq.      I'm about to post a new patch, but it looks like this and it has to be called just before ""try"" instead of inside:
bq.           if (this.closing.get()) {
bq.            throw new NotServingRegionException(regionInfo.getRegionNameAsString() +
bq.                "" is closing"");
bq.          }
bq.          lock.readLock().lock();
bq.          if (this.closed.get()) {
bq.            throw new NotServingRegionException(regionInfo.getRegionNameAsString() +
bq.                "" is closed"");
bq.          }

That looks right.


bq.  On 2010-08-20 15:57:34, stack wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 712
bq.  > <http://review.cloudera.org/r/691/diff/1/?file=7612#file7612line712>
bq.  >
bq.  >     Seems like you could use your opentransaction/closetransaction methods here and in flush too to be consistent?
bq.  
bq.  Jean-Daniel Cryans wrote:
bq.      Yeah the issue with compact and flush is that the callers don't expect to see NSRE, the want null values.
bq.  
bq.  stack wrote:
bq.      OK. Not important. This is deep internal stuff or make a version that takes a flag on whether to throw exception (default throws exception .. might get messy though... not important).
bq.  
bq.  Jean-Daniel Cryans wrote:
bq.      I'm afraid those little methods could be clogged fast.

Yeah. Not important.


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/691/#review974
-----------------------------------------------------------



;;;","20/Aug/10 23:50;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>


bq.  On 2010-08-20 15:57:34, stack wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 507
bq.  > <http://review.cloudera.org/r/691/diff/1/?file=7612#file7612line507>
bq.  >
bq.  >     I suppose this order is ok if the first thing we do on entrance to HRegion is get the read lock before check of closing.
bq.  
bq.  Jean-Daniel Cryans wrote:
bq.      So I just redid that part. setClosing is first taken so that when the client threads arrive they can fast fail by looking at closing.get before trying to get the readLock.
bq.  
bq.  stack wrote:
bq.      Don't you have to check again the setClosing after you get the read lock?

I'm about to post a new patch, but it looks like this and it has to be called just before ""try"" instead of inside:
     if (this.closing.get()) {
      throw new NotServingRegionException(regionInfo.getRegionNameAsString() +
          "" is closing"");
    }
    lock.readLock().lock();
    if (this.closed.get()) {
      throw new NotServingRegionException(regionInfo.getRegionNameAsString() +
          "" is closed"");
    }


bq.  On 2010-08-20 15:57:34, stack wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 712
bq.  > <http://review.cloudera.org/r/691/diff/1/?file=7612#file7612line712>
bq.  >
bq.  >     Seems like you could use your opentransaction/closetransaction methods here and in flush too to be consistent?
bq.  
bq.  Jean-Daniel Cryans wrote:
bq.      Yeah the issue with compact and flush is that the callers don't expect to see NSRE, the want null values.
bq.  
bq.  stack wrote:
bq.      OK. Not important. This is deep internal stuff or make a version that takes a flag on whether to throw exception (default throws exception .. might get messy though... not important).

I'm afraid those little methods could be clogged fast.


- Jean-Daniel


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/691/#review974
-----------------------------------------------------------



;;;","20/Aug/10 23:53;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>


bq.  On 2010-08-20 15:57:34, stack wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 712
bq.  > <http://review.cloudera.org/r/691/diff/1/?file=7612#file7612line712>
bq.  >
bq.  >     Seems like you could use your opentransaction/closetransaction methods here and in flush too to be consistent?
bq.  
bq.  Jean-Daniel Cryans wrote:
bq.      Yeah the issue with compact and flush is that the callers don't expect to see NSRE, the want null values.
bq.  
bq.  stack wrote:
bq.      OK. Not important. This is deep internal stuff or make a version that takes a flag on whether to throw exception (default throws exception .. might get messy though... not important).
bq.  
bq.  Jean-Daniel Cryans wrote:
bq.      I'm afraid those little methods could be clogged fast.
bq.  
bq.  stack wrote:
bq.      Yeah. Not important.

Moreover, and this should be added to the documentation, none of the user-facing methods now directly use the lock, it has to go through those new methods. Means that flush, compact, split and close are the only ones with direct access.


- Jean-Daniel


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/691/#review974
-----------------------------------------------------------



;;;","21/Aug/10 00:59;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/691/
-----------------------------------------------------------

(Updated 2010-08-20 17:43:52.668799)


Review request for hbase.


Changes
-------

Updated patch with ""closing"" handling and ICV locking, still needs more javadoc.


Summary
-------

This patch removes newScannerLock and renames splitAndClose lock to just ""lock"". Every operation is now required to obtain the read lock on ""lock"" before doing anything (including getting a row lock). This is done by calling openRegionTransaction inside a try statement and by calling closeRegionTransaction in finally.

flushcache got refactored some more in order to do the locking in the proper order; first get the read lock, then do the writestate handling.

Finally, it removes the need to have a writeLock when flushing when subclassers give atomic work do to via internalPreFlushcacheCommit. This means that this patch breaks external contribs. This is required to keep our whole locking mechanism simpler.


This addresses bug HBASE-2915.
    http://issues.apache.org/jira/browse/HBASE-2915


Diffs (updated)
-----

  /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 987355 
  /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java 987355 
  /trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java 987355 

Diff: http://review.cloudera.org/r/691/diff


Testing
-------

5 concurrent ICV threads + randomWrite 3 + scans on a single RS. I'm also in the process of deploying it on a cluster.


Thanks,

Jean-Daniel


;;;","21/Aug/10 04:10;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/691/#review988
-----------------------------------------------------------

Ship it!


+1

if below are issues address on commit.


/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3194>

    Any reason this does not follow the pattern used elsewhere?  You are not testing closing before getting the read lock?



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3195>

    ditto



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/691/#comment3196>

    Is this supposed to be inside the try?


- stack



;;;","23/Aug/10 17:00;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>


bq.  On 2010-08-20 20:49:15, stack wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 716
bq.  > <http://review.cloudera.org/r/691/diff/2/?file=7680#file7680line716>
bq.  >
bq.  >     Any reason this does not follow the pattern used elsewhere?  You are not testing closing before getting the read lock?

mmm didn't want to copy the same chunk of code over there, but yeah it will also give us a fail-fast behavior which will speed up flushing.


bq.  On 2010-08-20 20:49:15, stack wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 785
bq.  > <http://review.cloudera.org/r/691/diff/2/?file=7680#file7680line785>
bq.  >
bq.  >     ditto

ditto


bq.  On 2010-08-20 20:49:15, stack wrote:
bq.  > /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 2960
bq.  > <http://review.cloudera.org/r/691/diff/2/?file=7680#file7680line2960>
bq.  >
bq.  >     Is this supposed to be inside the try?

Per that method's javadoc, no. The reason is that if it's all in a try block, and that closing is true, then you won't hold a readLock and you can't do a isHeldByCurrent thread on that lock. So I thought we could catch IllegalStateException in closeRegionOperation, but that would be really ugly. This leaves us to the current situation where if an exception is thrown when we are on this line:
     if (this.closed.get()) {
that it would leave the readLock locked by the thread, although in this case the region is closing so we're getting rid of it anyways. Although, thinking about it, I should probably do this instead:

    if (this.closed.get()) {
      lock.readLock().unlock();
      throw new NotServingRegionException(regionInfo.getRegionNameAsString() +
          "" is closed"");
    }


- Jean-Daniel


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/691/#review988
-----------------------------------------------------------



;;;","23/Aug/10 22:43;jdcryans;This patch is the same as the last one I posted on rb, with the addition of the unlocking in case the region is closed but the thread was able to get a readLock. I tested it on a 13 nodes cluster with 200 YCSB threads hitting it in different ways, didn't see any deadlock. Going to commit.;;;","23/Aug/10 22:49;jdcryans;Committed to trunk, thanks for the review Stack!;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SoftValueSortedMap is broken, can generate NPEs",HBASE-2909,12471350,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,11/Aug/10 21:07,20/Nov/15 12:42,14/Jul/23 06:06,13/Aug/10 22:11,0.20.6,0.89.20100621,,,,,,,,,,0.90.0,,Client,,,,,0,,,"The way SoftValueSortedMap is using SoftValues, it looks like that it's able to get it's keys garbage collected along with the values themselves. We got this issue in production but I was also able to randomly generate it using YCSB with 300 threads. Here's an example on 0.20 with jdk 1.6u14:

{noformat}

java.lang.NullPointerException
        at org.apache.hadoop.hbase.util.Bytes.compareTo(Bytes.java:1036)
        at org.apache.hadoop.hbase.util.Bytes$ByteArrayComparator.compare(Bytes.java:104)
        at org.apache.hadoop.hbase.util.Bytes$ByteArrayComparator.compare(Bytes.java:96)
        at java.util.TreeMap.cmp(TreeMap.java:1911)
        at java.util.TreeMap.get(TreeMap.java:1835)
        at org.apache.hadoop.hbase.util.SoftValueSortedMap.get(SoftValueSortedMap.java:91)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getCachedLocation(HConnectionManager.java:788)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:651)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:634)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:601)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:128)
        at org.apache.hadoop.hbase.thrift.ThriftServer$HBaseHandler.getTable(ThriftServer.java:262)
        at org.apache.hadoop.hbase.thrift.ThriftServer$HBaseHandler.mutateRowTs(ThriftServer.java:585)
        at org.apache.hadoop.hbase.thrift.ThriftServer$HBaseHandler.mutateRow(ThriftServer.java:578)
        at org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRow.process(Hbase.java:2345)
        at org.apache.hadoop.hbase.thrift.generated.Hbase$Processor.process(Hbase.java:1988)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:259)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
{noformat}

In this specific case, the null cannot be the passed key because it's coming from HTable which uses HConstants.EMPTY_START_ROW. It cannot be a null key that was inserted previously because we would have got the NPE at insert time. This can only mean that some key *became* null.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-4785,,,"11/Aug/10 22:16;jdcryans;hbase-2909.patch;https://issues.apache.org/jira/secure/attachment/12451834/hbase-2909.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26530,Reviewed,,,,Fri Nov 20 12:42:17 UTC 2015,,,,,,,,,,"0|i0hjsf:",100468,,,,,,,,,,,,,,,,,,,,,"11/Aug/10 22:16;jdcryans;Here's a more ""normal"" way of doing a soft references structure, with the SoftValue now inside SoftValueSortedMap. I also got rid of the implementation of Map.Entry (which was suspicious) and disabled entrySet because 1) it wasn't used and 2) it used the Map.Entry which wasn't really one. Ran a few tests and it works, need more at-scale testing.;;;","11/Aug/10 22:19;ryanobjc;+1 (and not just because it is modelled after my SimpleBlockCache)

;;;","12/Aug/10 17:18;stack;+1;;;","13/Aug/10 22:11;jdcryans;Committed to branch and trunk, thanks for checking it out guys.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong order of null-check,HBASE-2908,12471280,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,korusef,korusef,korusef,11/Aug/10 12:52,20/Nov/15 12:40,14/Jul/23 06:06,11/Aug/10 14:44,0.89.20100621,,,,,,,,,,,0.90.0,,mapreduce,,,,,0,,,"In method org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(JobContext)
this.table is used before null-throw check.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/10 13:13;korusef;hbase-2908-fix.patch;https://issues.apache.org/jira/secure/attachment/12451774/hbase-2908-fix.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26529,Reviewed,,,,Fri Nov 20 12:40:50 UTC 2015,,,,,,,,,,"0|i0hjs7:",100467,,,,,,,,,,,,,,,,,,,,,"11/Aug/10 13:14;korusef;hbase-2908-fix.patch should fix the mentioned issue.;;;","11/Aug/10 14:44;stack;Thanks for the patch Libor.  Applied to TRUNK.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[rest/stargate] URI decoding in RowResource,HBASE-2906,12471245,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,apurtell,kbriggs,kbriggs,11/Aug/10 01:49,20/Nov/15 12:41,14/Jul/23 06:06,06/Oct/10 17:46,0.20.3,0.20.4,0.20.5,0.20.6,0.89.20100621,,,,,,,0.90.0,,REST,,,,,0,,,"Currently the RowResource constructor URI-decodes the rowspec string before passing it to the RowSpec constructor, which breaks rowspecs whose row, column etc identifiers contain slashes.

When addressing a row and/or column whose identifier contains a slash, the client must URI-encode the values, so for example a row whose identifier is 'http://hbase.apache.org/' would be addressed as:
  /tablename/http%3a%2f%2fhbase.apache.org%2f/column:qualifier

Currently RowResource() decodes this before passing to RowSpec(), so RowSpec() recieves:
  /tablename/http://hbase.apache.org//column:qualifier
which cannot be correctly parsed because of the extra slashes.

RowResource() should pass the string on to RowSpec() undecoded, and RowSpec() should decode the components individually after piecing apart the path.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/10 01:30;apurtell;HBASE-2906.patch;https://issues.apache.org/jira/secure/attachment/12456460/HBASE-2906.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26528,Reviewed,,,,Fri Nov 20 12:41:16 UTC 2015,,,,,,,,,,"0|i0hjrr:",100465,,,,,,,,,,,,,,,,,,,,,"06/Oct/10 02:53;apurtell;I'm going to commit this bugfix patch tomorrow unless there is an objection.;;;","06/Oct/10 04:24;stack;+1 Patch looks good to me especially test of the change.;;;","06/Oct/10 17:46;apurtell;Committed to trunk and branch. Thanks for the review Stack.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nullpointer Exception is throwed when insert mass data via rest interface,HBASE-2905,12471103,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,sandy_yin,sandy_yin,sandy_yin,09/Aug/10 10:36,20/Nov/15 12:44,14/Jul/23 06:06,09/Aug/10 15:56,0.89.20100621,,,,,,,,,,,0.90.0,,REST,,,,,0,,,"Nullpointer Exception is throwed when insert mass data via rest interface.

{code}
java.lang.NullPointerException
	at org.mortbay.io.ByteArrayBuffer.wrap(ByteArrayBuffer.java:361)
	at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:588)
	at com.sun.jersey.spi.container.servlet.WebComponent$Writer.write(WebComponent.java:233)
	at com.sun.jersey.spi.container.ContainerResponse$CommittingOutputStream.write(ContainerResponse.java:108)
	at org.apache.hadoop.hbase.rest.provider.producer.ProtobufMessageBodyProducer.writeTo(ProtobufMessageBodyProducer.java:78)
	at com.sun.jersey.spi.container.ContainerResponse.write(ContainerResponse.java:254)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:744)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:667)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:658)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:318)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:425)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:604)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:389)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:879)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:747)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:520)
{code}

The issue is caused by using WeakHashMap as buffer. When get the object from the buffer, the JVM gc possible has removed the object.","CentOS 5.2 x86_64, HBase 0.89",larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/10 10:46;sandy_yin;HBase-2905-89.patch;https://issues.apache.org/jira/secure/attachment/12451576/HBase-2905-89.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26527,Reviewed,,,,Fri Nov 20 12:44:00 UTC 2015,,,,,,,,,,"0|i0hjrj:",100464,Fix Nullpointer exception when insert mass data via rest interface ,,,,,,,,,,,,,,,,,,,,"09/Aug/10 10:52;sandy_yin;Use thread local variable instead of  WeakHashmap to prevent the Nullpointer Exception.;;;","09/Aug/10 14:05;stack;+1

What you think Andrew?;;;","09/Aug/10 14:47;apurtell;Looks good, +1, I'll commit. ;;;","09/Aug/10 15:56;apurtell;Committed to trunk.;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBASE-2461 broke build,HBASE-2901,12470864,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,05/Aug/10 03:58,20/Nov/15 12:41,14/Jul/23 06:06,25/Aug/10 01:13,,,,,,,,,,,,0.90.0,,,,,,,0,,,I broke the build.  Fix coming.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/10 04:00;stack;fix.txt;https://issues.apache.org/jira/secure/attachment/12451303/fix.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26526,,,,,Fri Nov 20 12:41:34 UTC 2015,,,,,,,,,,"0|i0hjqv:",100461,,,,,,,,,,,,,,,,,,,,,"05/Aug/10 04:00;stack;My check that splitkey was within the region didn't account for the region whose end key was the empty byte array.   This patch removes all my checks and instead uses HRI.containsRow.  Tests pass now.;;;","25/Aug/10 01:13;jdcryans;Stack committed this a while ago.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hfile.min.blocksize.size ignored/documentation wrong,HBASE-2899,12470772,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,stack,larsfrancke,larsfrancke,03/Aug/10 23:01,12/Jun/22 00:01,14/Jul/23 06:06,15/Sep/10 23:58,,,,,,,,,,,,,,,,,,,0,,,"There is a property in hbase-default.xml called {{hfile.min.blocksize.size}} set to {{65536}}.

The description says: Minimum store file block size.  The smaller you make this, the  bigger your index and the less you fetch on a random-access.  Set size down  if you have small cells and want faster random-access of individual cells.

This property is only used in the HFileOutputFormat and nowhere else. So we should at least change the description to something more meaningful.

The other option I see would be: HFile now has a DEFAULT_BLOCKSIZE field which could be moved to HConstants and HFile could somehow read the {{hfile.min.blocksize.size}} from the Configuration or use HConstansts.DEFAULT_BLOCKSIZE if it's not defined. I believe this is what's happening to the other config variables?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/10 06:38;stack;2899.txt;https://issues.apache.org/jira/secure/attachment/12454525/2899.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26524,,,,,Sat May 07 00:08:32 UTC 2011,,,,,,,,,,"0|i0hjqn:",100460,,,,,,,,,,,,,,,,,,,,,"09/Sep/10 19:51;karthick;On top of this, there's also a HColumnDescriptor#BLOCKSIZE property, which is specific to a column family. IMO, the effective file block size should be the value of  HColumnDescriptor#BLOCKSIZE, HBaseConfiguration.get(""hfile.min.blocksize.size"") or HFile.DEFAULT_BLOCKSIZE, in that order of precedence.;;;","09/Sep/10 21:43;streamy;+1 on ordering suggested by karthick;;;","09/Sep/10 22:23;ryanobjc;that would be fine


;;;","10/Sep/10 16:31;stack;This sounds right lads (Lars+Karthick);;;","14/Sep/10 06:38;stack;Small patch to address this issue (Not on review board because I didn't make patch w/ git so having issues uploading)

Renames hfile.min.blocksize.size to be hbase.mapreduce.hfileoutputformat.blocksize
Blocksize is normally a column family attribute set using the BLOCKSIZE key on
an HColumnDescriptor.  This above configuration is for the mapreduce outputfile
context where there is not table schema available.

I did not move HFile.DEFAULT_BLOCKSIZE to HConstants.  HFile is trying to 
minimize its dependency on the backing hbase so doesn't want to depend on
Configuration or HConstants (though I notice the main has Configuration and
HConstants). It wants configuration passed in on construction (which is
how we set hfile blocksize, by passing whats set in HColumnDescritor
into the hfile constructor).

The default BLOCKSIZE is set to HFile.DEFAULT_BLOCKSIZE.

So, I think regards Karthik's comment, the blocksize is HColumnDescriptor#BLOCKSIZE,
never HBaseConfiguration.get(""hfile.min.blocksize.size (as of this patch), and
yes, the default is whats in File.DEFAULT_BLOCKSIZE;;;","14/Sep/10 14:27;streamy;But shouldn't there be a way to change the default block size, outside HFOF context?  Or you have to override each family separately?;;;","14/Sep/10 15:22;stack;@Jon Thats something else.  Thats a means of changing defaults in HColumnDescriptor -- default versions, ttls, whether bloom is on or off, etc. -- and in HTableDescriptor.  We need that too but lets make a new issue for that.  Lets do it as part of move all schema up to zk (Can have a template schema up in zk that all new schema's seed from).  While we are at it move Configuration to zk so on change, all running processes are kicked and update their config.;;;","15/Sep/10 23:58;stack;Committing this for now.  Lets open new issue for the fancyness Jon wants (of if this patch doesn't nail whats described here).;;;","07/May/11 00:08;hudson;Integrated in HBase-TRUNK #1909 (See [https://builds.apache.org/hudson/job/HBase-TRUNK/1909/])
    HBASE-3864 Rename of hfile.min.blocksize.size in HBASE-2899 reverted in HBASE-1861
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultiPut makes proper error handling impossible and leads to corrupted data,HBASE-2898,12470753,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,ryanobjc,tsuna,tsuna,03/Aug/10 19:00,20/Nov/15 12:41,14/Jul/23 06:06,10/Nov/10 01:55,0.89.20100621,,,,,,,,,,,0.90.0,,Client,regionserver,,,,0,,,"tl;dr version: I think the {{MultiPut}} RPC needs to be completely rewritten.  The current code makes it totally impossible for an HBase client to do proper error handling.  When an edit fails, the client has no clue as to what the problem was (certain error cases can be retried, others cannot e.g. when using a non-existent family) and the client doesn't even know which of the edits have been applied successfully.  So the client often has to retry edits without knowing whether they've been applied or not, which leads to extra unwanted versions for the {{KeyValue}} that were successfully applied (for those who care about versions, this is essentially equivalent to data corruption).  In addition, there's no way for a client to properly handle {{NotServingRegionException}}, the client has to unnecessarily invalidate cached locations of some regions and retry *all* edits.

h2. Life of a failed multi-put

Let's see why step by step what happens when a single edit in a multi-put fails.

# An HBase user calls any of the {{put}} methods on an {{HTable}} instance.
# Eventually, {{HTable#flushCommits}} is invoked to actually send the edits to the RegionServer(s).
# This takes us to {{HConnectionManager#processBatchOfPuts}} where all edits are sorted into one or more {{MultiPut}}.  Each {{MultiPut}} is aggregating all the edits that are going to a particular RegionServer.
# A thread pool is used to send all the {{MultiPut}} in parallel to their respective RegionServer.  Let's follow what happens for a single {{MultiPut}}.
# The {{MultiPut}} travels through the IPC code on the client and then through the network and then through the IPC code on the RegionServer.
# We're now in {{HRegionServer#multiPut}} where a new {{MultiPutResponse}} is created.
# Still in {{HRegionServer#multiPut}}.  Since a {{MultiPut}} is essentially a map from region name to a list of {{Put}} for that region, there's a {{for}} loop that executes each list of {{Put}} for each region sequentially.  Let's follow what happens for a single list of {{Put}} for a particular region.
# We're now in {{HRegionServer#put(byte[], List<Put>)}}.  Each {{Put}} is associated with the row lock that was specified by the client (if any).  Then the pairs of {{(Put, lock id)}} are handed to the right {{HRegion}}.
# Now we're in {{HRegion#put(Pair<Put, Integer>[])}}, which immediately takes us to {{HRegion#doMiniBatchPut}}.
# At this point, let's assume that we're doing just 2 edits.  So the {{BatchOperationInProgress}} that {{doMiniBatchPut}} contains just 2 {{Put}}.
# The {{while}} loop in {{doMiniBatchPut}} that's going to execute each {{Put}} starts.
# The first {{Put}} fails because an exception is thrown when appending the edit to the {{WAL}}.  Its {{batchOp.retCodes}} is marked as {{OperationStatusCode.FAILURE}}.
# Because there was an exception, we're back to {{HRegion#put(Pair<Put, Integer>[])}} where the {{while}} loop will test that {{batchOp.isDone}} is {{false}} and do another iteration.
# {{doMiniBatchPut}} is called again and handles the remaining {{Put}}.
# The second {{Put}} succeeds normally, so its {{batchOp.retCodes}} is marked as {{OperationStatusCode.SUCCESS}}.
# {{doMiniBatchPut}} is done and returns to {{HRegion#put(Pair<Put, Integer>[])}}, which returns to {{HRegionServer#put(byte[], List<Put>)}}.
# At this point, {{HRegionServer#put(byte[], List<Put>)}} does a {{for}} loop and extracts the index of the *first* {{Put}} that failed out of the {{OperationStatusCode[]}}.  In our case, it'll return 0 since the first {{Put}} failed.
# This index in the list of {{Put}} of the first that failed (0 in this case) is returned to {{HRegionServer#multiPut}}, which records in the {{MultiPutResponse}} - the client knows that the first {{Put}} failed but has no idea about the other one.

So the client has no reliable way of knowing which {{Put}} failed (if any) past the first failure.  All it knows is that for a particular region, they succeeded up to a particular {{Put}}, at which point there was a failure, and then the remaining may or may not have succeeded.  Its best bet is to retry all the {{Put}} past the index of the first failure for this region.  But this has an unintended consequence.  The {{Put}} that were successful during the first run will be *re-applied*.  This will unexpectedly create extra versions.  Now I realize most people don't really care about versions, so they won't notice.  But whoever relies on the versions for whatever reason will rightfully consider this to be data corruption.

As it is now, {{MultiPut}} makes proper error handling impossible.  Since this RPC cannot guarantee any atomicity other than at the individual {{Put}} level, it should return to the client specific information about which {{Put}} failed in case of a failure, so that the client can do proper error handling.

This requires us to change the {{MultiPutResponse}} so that it can indicate which {{Put}} specifically failed.  We could do this for instance by giving the index of the {{Put}} along with its {{OperationStatusCode}}.  So in the scenario above, the {{MultiPutResponse}} would essentially return something like: ""for that particular region, put #0 failed, put #1 succeeded"".  If we want to save a bit of space, we may want to omit the successes from the response and only mention the failures - so a response that doesn't mention any failure means that everything was successful.  Not sure whether that's a good idea though.

Since doing this require an incompatible RPC change, I propose that we take the opportunity to rewrite the {{MultiPut}} RPC too.  Right now it's very inefficient, it's just a hack on top of {{Put}}.  When {{MultiPut}} is written to the wire, a lot of unnecessary duplicate data is sent out.  The timestamp, the row key and the family are sent out to the wire N+1 times, where N is the number of edits for a particular row, instead of just once (!).

Alternatively, if we don't want to change the RPCs, we can fix this issue in a backward compatible way by making the {{while}} loop in {{HRegion#put(Pair<Put, Integer>[])}} stop as soon as a failure is encountered.

h2. Inability to properly handle {{NotServingRegionException}}

Since the code in {{HRegionServer#multiPut}} invokes {{HRegionServer#put(byte[], List<Put>)}} for each region for which there are edits, it's possible that edits for a first region all get successfully applied, then when moving on the 2nd region, a {{NotServingRegionException}} is thrown, which fails the RPC entirely and leaves the client with absolutely no clue as to which edits were successfully applied or not and which region caused the {{NotServingRegionException}}.  Currently the code in {{HConnectionManager}} will simply retry everything when that happens, and it'll invalidate the cached location of all the regions involved in the multi-put (even though it could well be that just a single region has to be invalidated).

I'm not sure how to best solve this problem because the Hadoop RPC protocol doesn't give us an easy way of passing data around in an exception.  The only two things I can think of are both really ugly:
# The message of the exception could contain the name of the region that caused the exception so that the client can parse the name out of the message and invalidate the right entry in its cache.
# As a special case, instead of throwing a {{NotServingRegionException}}, we'd change {{MultiPutResponse}} to also contain a list of regions no longer served by this region server, so the client could invalidate the right entries in its cache and retry just the edits that need to be retried.

I think 2. is better but it also requires an incompatible RPC change.

h2. Repro code

Simple test case to highlight the bug (against HBase trunk).  The HBase client retries hopelessly until it reaches the maximum number of attempts before bailing out.
{code}

hbase(main):001:0> describe 'mytable'
DESCRIPTION                                                              ENABLED                                
 {NAME => 'mytable', FAMILIES => [{NAME => 'myfam', BLOOMFILTER => 'NONE true                                   
 ', REPLICATION_SCOPE => '0', COMPRESSION => 'NONE', VERSIONS => '1', TT                                        
 L => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCAC                                        
 HE => 'true'}]}                                                                                                
1 row(s) in 0.7760 seconds
{code}
{code:java}

import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Put;

final class put {
  public static void main(String[]a) throws Exception {
    final HTable t = new HTable(HBaseConfiguration.create(), ""mytable"");
    final Put p = new Put(""somerow"".getBytes());
    p.add(""badfam"".getBytes(), ""qual2"".getBytes(), ""badvalue"".getBytes());
    t.put(p);
    t.flushCommits();
  }
}
{code}
Excerpt of the log produced when running {{put}}:
{noformat}
HConnectionManager$TableServers: Found ROOT at 127.0.0.1:54754
HConnectionManager$TableServers: Cached location for .META.,,1.1028785192 is localhost.:54754
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 1000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 1000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 1000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 2000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 2000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 4000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 4000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 8000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 16000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 32000 ms!
Exception in thread ""main"" org.apache.hadoop.hbase.client.RetriesExhaustedException: Still had 1 puts left after retrying 10 times.
	at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.processBatchOfPuts(HConnectionManager.java:1534)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:664)
	at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:549)
	at org.apache.hadoop.hbase.client.HTable.put(HTable.java:535)
	at put.main(put.java:10)
{noformat}",,fern,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-1845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26523,Incompatible change,Reviewed,,,Fri Nov 20 12:41:45 UTC 2015,,,,,,,,,,"0|i0hjqf:",100459,,,,,,,,,,,,data corruption,,,,,,,,,"03/Aug/10 21:59;stack;Bringing into 0.90.  Agree its a blocker.;;;","03/Aug/10 22:09;ryanobjc;At step 16, the return code of HRS#put(...) is the offset of which Put failed.  In ye olde days, this worked well because the first Put that failed in any way aborted and the return code would let the client know where to continue.

The way it used to work is the first bad put we came across, we returned the index at which we failed.  If it was a WrongRegionException or a bad column family, either way we'd just stop and the caller would know which put failed (but not the reason why).  

If all you get is WRE, then the new code behaves much like the old code.  The first Put that runs into WRE will in step #16 cause a return code (ie: list offset) to return to the client.

But when you run into bad column families.... what seems to be happening is the those puts are marked as 'bad', then we process the rest of the puts.  In the HRS wrapper (ie: step 16) we return a failure even though perhaps all but 1 succeeded.

Previously there was no way to know which put failed and why, which is why the implementation of multiput made sense, but now with the detailed error codes we can do something more.;;;","04/Aug/10 02:23;tsuna;I adjusted the description of the issue as it was inaccurate at some point, as Ryan pointed out.  The issue remains valid though.;;;","14/Aug/10 00:53;ryanobjc;I am interested in a new multi-put for 0.90.  There are also cases for multi-get and multi just about everything. See HBASE-1845.

Improving serialization would be nice, but we'd have to wreck our KeyValue serialization mechanism and have something custom on the wire.  There is a similar situation in the Result serialization as well, it's just KeyValues all the way on down.;;;","14/Aug/10 01:05;tsuna;I edited the description of the issue to further explain how it's impossible for clients to reasonably handle {{NotServingRegionException}} when using multi-put.;;;","09/Nov/10 00:53;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1176/#review1852
-----------------------------------------------------------


Patch looks great.  Needs the test filled out before +1 and do all unit tests pass?  It changes a bunch of critical code so wouldn't be surprised if unexpected side effects.


trunk/pom.xml
<http://review.cloudera.org/r/1176/#comment6013>

    You didn't mean this, right?  We can't have direct cloudera dependency.



trunk/src/main/java/org/apache/hadoop/hbase/client/HConnection.java
<http://review.cloudera.org/r/1176/#comment6014>

    No one depends on these?



trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
<http://review.cloudera.org/r/1176/#comment6015>

    review board is showing a bunch of white space on these lines?  Tabs?



trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
<http://review.cloudera.org/r/1176/#comment6016>

    You changing public API here?
    
    I suppose we are but only in HCM which is rarely used by other than internals... disregard this comment since I see you convert it in HTable below to an IOE.



trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
<http://review.cloudera.org/r/1176/#comment6018>

    Hmmm... would make life easier if we just let out the IE.



trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
<http://review.cloudera.org/r/1176/#comment6020>

    Long line.



trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
<http://review.cloudera.org/r/1176/#comment6023>

    Yeah, cleaner if IE is let out (I suppose).



trunk/src/main/java/org/apache/hadoop/hbase/client/HTable.java
<http://review.cloudera.org/r/1176/#comment6024>

    This is kinda ugly.  For sure exceptions have been purged here?



trunk/src/main/java/org/apache/hadoop/hbase/client/HTable.java
<http://review.cloudera.org/r/1176/#comment6025>

    Adding an IE here is ok?  Is this a new method in 0.90?



trunk/src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java
<http://review.cloudera.org/r/1176/#comment6026>

    Oh, ok... then IE is fine.



trunk/src/main/java/org/apache/hadoop/hbase/client/MultiResponse.java
<http://review.cloudera.org/r/1176/#comment6027>

    Could there be exception here or its always Result?  If always Result why change type to Object?



trunk/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java
<http://review.cloudera.org/r/1176/#comment6028>

    Missing class comment on what this doohickey does



trunk/src/test/java/org/apache/hadoop/hbase/client/MultiResponseTest.java
<http://review.cloudera.org/r/1176/#comment6029>

    Hey man, great test!  Can you add something in here?  Smile.


- stack



;;;","09/Nov/10 01:01;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1176/#review1855
-----------------------------------------------------------



trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
<http://review.cloudera.org/r/1176/#comment6033>

    people can selective not catch IE and then let their regular abort do it's job.



trunk/src/main/java/org/apache/hadoop/hbase/client/HTable.java
<http://review.cloudera.org/r/1176/#comment6034>

    yes, absolutely, because at the end we check every single entry in the results[] and if there are any failures (null, Throwable instances) we throw an exception. So one can be sure that everything is a Result during success, and if you get an exception you have the Throwable handy dandy in both the results array and in thrown exception. 



trunk/src/main/java/org/apache/hadoop/hbase/client/HTable.java
<http://review.cloudera.org/r/1176/#comment6035>

    this is a brand new method, never was in any previous 0.89 DR even.



trunk/src/main/java/org/apache/hadoop/hbase/client/MultiResponse.java
<http://review.cloudera.org/r/1176/#comment6036>

    it could be either a Throwable or a Result. batch() shouldn't throw many exceptions, they should be identified and lined up with each index entry.



trunk/src/test/java/org/apache/hadoop/hbase/client/MultiResponseTest.java
<http://review.cloudera.org/r/1176/#comment6037>

    I blame SVN!  It was including my scratch work :-)


- Ryan



;;;","09/Nov/10 22:55;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1176/#review1875
-----------------------------------------------------------



trunk/src/main/java/org/apache/hadoop/hbase/client/HConnection.java
<http://review.cloudera.org/r/1176/#comment6075>

    this was called by HTable, but those have shifted to use processBatch() instead.


- Ryan



;;;","10/Nov/10 01:55;ryanobjc;committed a fix to this. We now detail which puts/gets/deletes failed and the client code takes appropriate action to retry appropriately.
;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[stargate] RowResultGenerator should handle NoSuchColumnFamilyException,HBASE-2897,12470700,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,apurtell,apurtell,apurtell,03/Aug/10 08:36,20/Nov/15 12:43,14/Jul/23 06:06,25/Aug/10 16:27,,,,,,,,,,,,0.90.0,,REST,,,,,0,,,"From Sasha Maksimenko up on user@hbase:

{quote}
hi!
thanks for answer. I use very simple code

{code}
org.apache.hadoop.hbase.stargate.client.Client client = new
  org.apache.hadoop.hbase.stargate.client.Client();
Response put = client.post(""http://hostname:port/task/2/value"",
""application/octet-stream"", ""1"".getBytes());
System.out.println(put.getCode()+new String(put.getBody()));
client.shutdown();
{code}

In the first invocation I use  correct column name ""value"" and everything is OK. After that I use wrong column name""valueS"" and get exception

503 javax.ws.rs.WebApplicationException:
org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException:
[...]

Next time I change column back but problem still exist. When I re-start server problem is dissappear
{quote}

RowResultGenerator should gracefully handle NoSuchColumnFamilyException.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/10 08:37;apurtell;HBASE-2897-0.20.patch;https://issues.apache.org/jira/secure/attachment/12451110/HBASE-2897-0.20.patch","03/Aug/10 08:37;apurtell;HBASE-2897-trunk.patch;https://issues.apache.org/jira/secure/attachment/12451109/HBASE-2897-trunk.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26522,Reviewed,,,,Fri Nov 20 12:43:01 UTC 2015,,,,,,,,,,"0|i0hjq7:",100458,,,,,,,,,,,,,,,,,,,,,"03/Aug/10 08:48;hbasereviewboard;Message from: ""Andrew Purtell"" <apurtell@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/482/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

RowResultGenerator should handle NoSuchColumnFamilyException. Otherwise the client gets an ugly exception and Jersey is left in an incorrect state.


This addresses bug HBASE-2897.
    http://issues.apache.org/jira/browse/HBASE-2897


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/rest/RowResultGenerator.java b742ded 
  src/main/java/org/apache/hadoop/hbase/rest/client/Client.java 45cf2db 
  src/test/java/org/apache/hadoop/hbase/rest/TestRowResource.java 392505e 

Diff: http://review.cloudera.org/r/482/diff


Testing
-------

Added unit test. Confirmed bug as reported. Fixed bug. New unit test now passes. No regressions observed running the unit test suite locally.


Thanks,

Andrew


;;;","03/Aug/10 14:49;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/482/#review629
-----------------------------------------------------------

Ship it!


+1 Looks good to me.

- stack



;;;","24/Aug/10 22:54;jdcryans;I committed a very small fix to get that test finally working, but it seems that this issue was targeted for 0.20.7 and wasn't committed to branch. Should that commit be done, or should we remove that target?;;;","25/Aug/10 00:04;apurtell;Commit to branch I'd say. Patches on trunk should just apply after path fixup. 

There are a few pending Stargate issues. I'm back next week for 10 days, can clear them all. When do you want to do a 0.20.7?

;;;","25/Aug/10 00:07;jdcryans;Not sure if there will ever be a 0.20.7, but putting it there doesn't cost much.;;;","25/Aug/10 16:27;jdcryans;Committed to branch.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replication metrics aren't updated,HBASE-2892,12470554,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,30/Jul/10 23:14,20/Nov/15 12:40,14/Jul/23 06:06,30/Jul/10 23:42,,,,,,,,,,,,0.90.0,,,,,,,0,,,"When I committed HBASE-2838, I changed the way the metrics were managed in my last patch but I forgot to put the doUpdates in place.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/10 23:34;jdcryans;HBASE-2892.patch;https://issues.apache.org/jira/secure/attachment/12450929/HBASE-2892.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26520,Reviewed,,,,Fri Nov 20 12:40:35 UTC 2015,,,,,,,,,,"0|i0hjpj:",100455,,,,,,,,,,,,,,,,,,,,,"30/Jul/10 23:34;jdcryans;Adds the metricsRecord updating and cleans the jvm registration (already done in RS's metrics).;;;","30/Jul/10 23:37;stack;+1;;;","30/Jul/10 23:42;jdcryans;Committed to trunk and 0.89.20100726, thanks for the review Stack.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Initialize RPC JMX metrics on startup,HBASE-2890,12470482,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,ghelmling,ghelmling,ghelmling,30/Jul/10 00:28,20/Nov/15 12:43,14/Jul/23 06:06,30/Jul/10 03:28,,,,,,,,,,,,0.90.0,,IPC/RPC,,,,,0,,,"Currently RPC call metrics are created dynamically based on cluster activity.  So when monitoring via JMX, not all RPC statistics may be present in the exported MBean, depending on what the past cluster activity has been.

HBASE-2146 has previously added code to initialize the MBean attributes on startup for all RPC methods, but in a way that depended on the defunct code -> method name mappings.

This issue is to initialize the exported MBean attributes in a cleaner way, by introspecting the RPC protocol interfaces (HMasterInterface, HMasterRegionInterface, HRegionInterface).",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/10 03:28;stack;HBASE-2890.patch;https://issues.apache.org/jira/secure/attachment/12450884/HBASE-2890.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26519,Reviewed,,,,Fri Nov 20 12:43:49 UTC 2015,,,,,,,,,,"0|i0hjpb:",100454,,,,,,,,,,,,,,,,,,,,,"30/Jul/10 00:44;hbasereviewboard;Message from: ""Gary Helmling"" <ghelmling@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/442/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Simple change to initialize RPC metrics based on methods declared in RPC protocol interfaces: HMasterInterface, HMasterRegionInterface, HRegionInterface.


This addresses bug HBASE-2890.
    http://issues.apache.org/jira/browse/HBASE-2890


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcMetrics.java d88c12d 

Diff: http://review.cloudera.org/r/442/diff


Testing
-------


Thanks,

Gary


;;;","30/Jul/10 03:22;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/442/#review600
-----------------------------------------------------------

Ship it!


Will commit in a minute.

- stack



;;;","30/Jul/10 03:28;stack;Patch I committed to TRUNK;;;","30/Jul/10 03:28;stack;Committed to 0.89.20100726 branch and to TRUNK.  Thanks for the patch Gary.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestHFileOutputFormat intermittent diff: LoadIncrementalHFiles should put expected data in table expected:<2048> but was:<1024>,HBASE-2884,12470289,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,tlipcon,kannanm,kannanm,27/Jul/10 19:27,20/Nov/15 12:43,14/Jul/23 06:06,30/Jul/10 00:48,,,,,,,,,,,,0.90.0,,,,,,,0,,,"{code}
Tests run: 5, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 160.582 sec <<< FAILURE!
testMRIncrementalLoad(org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat)  Time elapsed: 85.676 sec  <<< FAILURE!
java.lang.AssertionError: LoadIncrementalHFiles should put expected data in table expected:<2048> but was:<1024>
        at org.junit.Assert.fail(Assert.java:91)
        at org.junit.Assert.failNotEquals(Assert.java:645)
        at org.junit.Assert.assertEquals(Assert.java:126)
        at org.junit.Assert.assertEquals(Assert.java:470)
        at org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.doIncrementalLoadTest(TestHFileOutputFormat.java:300)
        at org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.testMRIncrementalLoad(TestHFileOutputFormat.java:248)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code}

Will upload full test output shortly.
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/10 19:33;kannanm;TestHFileOutputFormat-output.txt;https://issues.apache.org/jira/secure/attachment/12450619/TestHFileOutputFormat-output.txt","27/Jul/10 19:33;kannanm;TestHFileOutputFormat.txt;https://issues.apache.org/jira/secure/attachment/12450618/TestHFileOutputFormat.txt","28/Jul/10 23:49;tlipcon;hbase-2884.txt;https://issues.apache.org/jira/secure/attachment/12450773/hbase-2884.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26516,Reviewed,,,,Fri Nov 20 12:43:41 UTC 2015,,,,,,,,,,"0|i0hjov:",100452,,,,,,,,,,,,,,,,,,,,,"27/Jul/10 19:33;kannanm;Uploaded test output files.;;;","28/Jul/10 17:44;tlipcon;Looked at this a bit -- the thing is that it's expecting 2048 entries, which would indicate mapred.map.tasks was set to 2. Each mapper is expected to generate 1024 rows. But, we seed Random() in RandomKVGeneratingMapper with System.currentTimeMillis - perhaps in this flaky case both mappers started in the same millisecond and thus generated identical data. When we go and scan, we only get 1024 rows because we just double-inserted everything?;;;","28/Jul/10 23:49;tlipcon;Attached patch ensures that different mappers generate different data, even if they have the same random number generator. Tested this by changing it the old code to 'new Random(5)' and seeing that it failed with the same message Kannan saw. Then added the workaround to set the last byte of the key to the task ID, and verified that the test passed. ;;;","29/Jul/10 02:35;kannanm;+1. Patch looks good.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase hbck: false positive error reported for parent regions that are in offline state in meta after a split,HBASE-2876,12470074,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,kannanm,kannanm,24/Jul/10 04:33,20/Nov/15 12:43,14/Jul/23 06:06,25/Jul/10 03:46,,,,,,,,,,,,0.90.0,,,,,,,0,,,"HBase Checker will sometimes report something like the following:

{code}
ERROR: Region test1,9922400000,1279934604048.8cb65b1882960f230abb97860dd13c53. is not served by any region server but is listed in META to be on server null
{code}

The region in question is a parent region that has been offlined following a split. META still contains for the above region only because there are daughter regions which still have references to the parent region. Once the daughter regions undergo compaction, these references will be gone; and the parent region's entry will be removed from META. But ""hbck"" should detect entries in this condition and not complain.

{code}
 test1,9922400000,12799346 column=info:regioninfo, timestamp=1279938675016, value=REGION => {NAME =>
 04048.8cb65b1882960f230ab  'test1,9922400000,1279934604048.8cb65b1882960f230abb97860dd13c53.', STAR
 b97860dd13c53.            TKEY => '9922400000', ENDKEY => '', ENCODED => 8cb65b1882960f230abb97860d
                           d13c53, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'test1', FAMIL
                           IES => [{NAME => 'actions', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '
                           0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZ
                           E => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}
 test1,9922400000,12799346 column=info:server, timestamp=1279938675016, value=
 04048.8cb65b1882960f230ab
 b97860dd13c53.
 test1,9922400000,12799346 column=info:serverstartcode, timestamp=1279938675016, value=
 04048.8cb65b1882960f230ab
 b97860dd13c53.
 test1,9922400000,12799346 column=info:splitA, timestamp=1279938675016, value=\x00\x0A9961500000\x00
 04048.8cb65b1882960f230ab \x00\x00\x01*\x02J9'@test1,9922400000,1279938672935.94425ba581acd336d1cbd
 b97860dd13c53.            11181ee2785.\x00\x0A9922400000\x00\x00\x00\x05\x05test1\x00\x00\x00\x00\x
                           00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x00\x00\x00\x07IS_META
                           \x00\x00\x00\x05false\x00\x00\x00\x01\x08\x07actions\x00\x00\x00\x08\x00\
                           x00\x00\x0BBLOOMFILTER\x00\x00\x00\x04NONE\x00\x00\x00\x11REPLICATION_SCO
                           PE\x00\x00\x00\x010\x00\x00\x00\x0BCOMPRESSION\x00\x00\x00\x04NONE\x00\x0
                           0\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TTL\x00\x00\x00\x0A2147
                           483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00\x00\x09IN_ME
                           MORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04true\x
                           B7\x04q\x18
 test1,9922400000,12799346 column=info:splitB, timestamp=1279938675016, value=\x00\x00\x00\x00\x00\x
 04048.8cb65b1882960f230ab 01*\x02J9'@test1,9961500000,1279938672935.bb521c9d8c51fd8133f145dc3c75013
 b97860dd13c53.            6.\x00\x0A9961500000\x00\x00\x00\x05\x05test1\x00\x00\x00\x00\x00\x02\x00
                           \x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x00\x00\x00\x07IS_META\x00\x00\x
                           00\x05false\x00\x00\x00\x01\x08\x07actions\x00\x00\x00\x08\x00\x00\x00\x0
                           BBLOOMFILTER\x00\x00\x00\x04NONE\x00\x00\x00\x11REPLICATION_SCOPE\x00\x00
                           \x00\x010\x00\x00\x00\x0BCOMPRESSION\x00\x00\x00\x04NONE\x00\x00\x00\x08V
                           ERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TTL\x00\x00\x00\x0A2147483647\x00
                           \x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00\x00\x09IN_MEMORY\x00\x
                           00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04true""\xE8XD
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/10 05:28;stack;2876.txt;https://issues.apache.org/jira/secure/attachment/12450388/2876.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26511,Reviewed,,,,Fri Nov 20 12:43:15 UTC 2015,,,,,,,,,,"0|i0hjnj:",100446,,,,,,,,,,,,,,,,,,,,,"24/Jul/10 05:37;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/383/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Small patch for hbck tool; don't complain about offlined regions not being served.


This addresses bug HBASE-2876.
    http://issues.apache.org/jira/browse/HBASE-2876


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/client/HBaseFsck.java 4e1bb59 

Diff: http://review.hbase.org/r/383/diff


Testing
-------

None


Thanks,

stack


;;;","24/Jul/10 06:36;kannanm;will try out patch on my test cluster.;;;","24/Jul/10 07:22;hbasereviewboard;Message from: ""Kannan Muthukkaruppan"" <kannan@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/383/#review480
-----------------------------------------------------------

Ship it!


LGTM. Ran into on my test cluster before/after the fix... and the false positives are gone with the fix now. Thanks for the fix Stack.

- Kannan



;;;","25/Jul/10 03:46;stack;Committed.  Thanks for the review and test Kannan.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unnecessary double-synchronization in ZooKeeperWrapper,HBASE-2874,12470069,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,tsuna,tsuna,tsuna,24/Jul/10 03:07,20/Nov/15 12:42,14/Jul/23 06:06,24/Jul/10 05:11,,,,,,,,,,,,0.90.0,,Zookeeper,,,,,0,,,The {{listeners}} attribute is a synchronized collection but it's only accessed from 3 methods that are already marked as {{synchronized}}.  The double-synchronization is unnecessary and can be eliminated.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/10 03:19;tsuna;trunk-HBASE-2874-Unnecessary-double-synchronization-in-Zoo.patch;https://issues.apache.org/jira/secure/attachment/12450381/trunk-HBASE-2874-Unnecessary-double-synchronization-in-Zoo.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26509,Reviewed,,,,Fri Nov 20 12:42:48 UTC 2015,,,,,,,,,,"0|i0hjnb:",100445,,,,,,,,,,,,,,,,,,,,,"24/Jul/10 03:19;tsuna;1-line change that fixes the issue (modulo the trailing whitespaces being killed).;;;","24/Jul/10 05:11;stack;Committed.  Thanks for the patch Benôit.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Regularize how we log sequenceids -- sometimes its myseqid, other times its sequence id, etc.",HBASE-2869,12469967,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,22/Jul/10 23:48,20/Nov/15 12:42,14/Jul/23 06:06,22/Jul/10 23:52,,,,,,,,,,,,0.90.0,,,,,,,0,,,"I'm trying to trace sequenceids over time to make sure all is working properly over crashes, etc., in an HRS and its way too painful.  Regularize how we log so whenever a sequence id is mentioned in logs its named sequenceid.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/10 23:51;stack;2869.txt;https://issues.apache.org/jira/secure/attachment/12450242/2869.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26507,,,,,Fri Nov 20 12:42:00 UTC 2015,,,,,,,,,,"0|i0hjm7:",100440,,,,,,,,,,,,,,,,,,,,,"22/Jul/10 23:51;stack;Just changes log messages.;;;","22/Jul/10 23:52;stack;Committing. Just log changes.;;;","22/Jul/10 23:53;jdcryans;Take credit stack!;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Region permanently offlined ,HBASE-2866,12469949,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,karthik.ranga,kannanm,kannanm,22/Jul/10 20:20,20/Nov/15 12:42,14/Jul/23 06:06,23/Jul/10 22:45,,,,,,,,,,,,0.90.0,,,,,,,0,,,"After split, master attempts to reassign a region to a region server. Occasionally, such a region can get permanently offlined.


Master:
---------
{code}
2010-07-22 01:26:00,914 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_SPLIT_INCLUDES_DAUGHTERS: test1,6512200000,1279784117114.6466481aa931f8c1fa87622735487a72.: Daughters; test1,6512200000,1279787158624.6ead25ae677116cc88fc5420bb39d52e., test1,6531790000,1279787\
158624.8d5490bfc166c687657cb09203bd7d44. from test024.test.xyz.com,60020,1279780567744; 1 of 1                                                                                                                                                                                                     
2010-07-22 01:26:00,935 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Creating UNASSIGNED region 8d5490bfc166c687657cb09203bd7d44 in state = M2ZK_REGION_OFFLINE

2010-07-22 01:26:00,935 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Creating UNASSIGNED region 8d5490bfc166c687657cb09203bd7d44 in state = M2ZK_REGION_OFFLINE

2010-07-22 01:26:00,945 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region test1,6531790000,1279787158624.8d5490bfc166c687657cb09203bd7d44. to test024.test.xyz.com,60020,1279780567744

2010-07-22 01:26:00,949 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: While updating UNASSIGNED region 8d5490bfc166c687657cb09203bd7d44 exists, state = M2ZK_REGION_OFFLINE

2010-07-22 01:26:00,954 DEBUG org.apache.hadoop.hbase.master.RegionManager: Created UNASSIGNED zNode test1,6531790000,1279787158624.8d5490bfc166c687657cb09203bd7d44. in state M2ZK_REGION_OFFLINE
{code}

-------------------

Region Server:

{code}
2010-07-22 01:26:00,947 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_OPEN: test1,6531790000,1279787158624.8d5490bfc166c687657cb09203bd7d44.
2010-07-22 01:26:00,947 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_OPEN: test1,6512200000,1279787158624.6ead25ae677116cc88fc5420bb39d52e.
2010-07-22 01:26:00,947 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_OPEN: test1,6531790000,1279787158624.8d5490bfc166c687657cb09203bd7d44.
2010-07-22 01:26:00,948 DEBUG org.apache.hadoop.hbase.regionserver.RSZookeeperUpdater: Updating ZNode /hbase/UNASSIGNED/8d5490bfc166c687657cb09203bd7d44 with [RS2ZK_REGION_OPENING] expected version = 0
2010-07-22 01:26:00,952 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Got ZooKeeper event, state: SyncConnected, type: NodeDataChanged, path: /hbase/UNASSIGNED/8d5490bfc166c687657cb09203bd7d44
2010-07-22 01:26:00,974 WARN org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: <msgstorectrl001.test.xyz.com,msgstorectrl021.test.xyz.com,msgstorectrl041.test.xyz.com,msgstorectrl061.test.xyz.com,msgstorectrl081.ash2.facebook\
.com:/hbase,test024.test.xyz.com,60020,1279780567744>Failed to write data to ZooKeeper
org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /hbase/UNASSIGNED/8d5490bfc166c687657cb09203bd7d44
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:106)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
        at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1038)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.writeZNode(ZooKeeperWrapper.java:1062)
        at org.apache.hadoop.hbase.regionserver.RSZookeeperUpdater.updateZKWithEventData(RSZookeeperUpdater.java:161)
        at org.apache.hadoop.hbase.regionserver.RSZookeeperUpdater.startRegionOpenEvent(RSZookeeperUpdater.java:115)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openRegion(HRegionServer.java:1428)
        at org.apache.hadoop.hbase.regionserver.HRegionServer$Worker.run(HRegionServer.java:1337)
        at java.lang.Thread.run(Thread.java:619)
2010-07-22 01:26:00,975 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: Error opening test1,6531790000,1279787158624.8d5490bfc166c687657cb09203bd7d44.
java.io.IOException: org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /hbase/UNASSIGNED/8d5490bfc166c687657cb09203bd7d44
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.writeZNode(ZooKeeperWrapper.java:1072)
{code}

Meta:
-----

Relevant section of META.

Note that these are the only two entries for the problem region. The first one is the parent region (and this problem
region is its splitB).  For the next one, note that there is no ""info:server"" and ""info:serverstartcode"" columns.

{code}
 test1,6512200000,12797841 column=info:splitB, timestamp=1279787160693, value=\x00\x0A6551820000\x00
 17114.6466481aa931f8c1fa8 \x00\x00\x01)\xF9BL`@test1,6531790000,1279787158624.8d5490bfc166c687657cb
 7622735487a72.            09203bd7d44.\x00\x0A6531790000\x00\x00\x00\x05\x05test1\x00\x00\x00\x00\x
                           00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x00\x00\x00\x07IS_META
                           \x00\x00\x00\x05false\x00\x00\x00\x01\x08\x07actions\x00\x00\x00\x08\x00\
                           x00\x00\x0BBLOOMFILTER\x00\x00\x00\x04NONE\x00\x00\x00\x11REPLICATION_SCO
                           PE\x00\x00\x00\x010\x00\x00\x00\x0BCOMPRESSION\x00\x00\x00\x04NONE\x00\x0
                           0\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TTL\x00\x00\x00\x0A2147
                           483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00\x00\x09IN_ME
                           MORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04true\x
                           FE\xA0\xFD\xC5

 ..

 test1,6531790000,12797871 column=info:regioninfo, timestamp=1279787160782, value=REGION => {NAME =>
 58624.8d5490bfc166c687657  'test1,6531790000,1279787158624.8d5490bfc166c687657cb09203bd7d44.', STAR
 cb09203bd7d44.            TKEY => '6531790000', ENDKEY => '6551820000', ENCODED => 8d5490bfc166c687
                           657cb09203bd7d44, TABLE => {{NAME => 'test1', FAMILIES => [{NAME => 'acti
                           ons', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', C
                           OMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMOR
                           Y => 'false', BLOCKCACHE => 'true'}]}}
{code}


I think Karthik has a handle on the first part (i.e. why the RS ran into the version mismatch, and aborted opening the region). He'll add details to the JIRA. But what we aren't clear about at this stage is why the base scanner didn't kick in and try to reassign the region.

BTW, HBase ""hbck"" reported this as well (which was good!):

{code}
Number of Tables: 5
Number of live region servers:92
Number of dead region servers:0
.........
ERROR: Region test1,6512200000,1279784117114.6466481aa931f8c1fa87622735487a72. is not served by any region server  but is listed in META to be on server null
ERROR: Region test1,6531790000,1279787158624.8d5490bfc166c687657cb09203bd7d44. is not served by any region server  but is listed in META to be on server null
{code}


",,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/10 02:05;kannanm;master.log;https://issues.apache.org/jira/secure/attachment/12450252/master.log",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26506,Reviewed,,,,Fri Nov 20 12:42:57 UTC 2015,,,,,,,,,,"0|i0hjlj:",100437,,,,,,,,,,,,,,,,,,,,,"22/Jul/10 22:53;jdcryans;This issue looks like the one in TestAdmin that fails every now and then on Hudson like: http://hudson.zones.apache.org/hudson/job/HBase-TRUNK/1397/;;;","23/Jul/10 02:05;kannanm;@Stack: Attached the master's log.;;;","23/Jul/10 06:04;stack;@Kannan Thanks.  Looking at master and at code, my thought is that the fixup code didn't run because that region is stuck in transition.   Here is where we'd skip out starting at about #562 in BaseScanner:

{code}
    synchronized (this.master.getRegionManager()) {
      /* We don't assign regions that are offline, in transition or were on
       * a dead server. Regions that were on a dead server will get reassigned
       * by ProcessServerShutdown
       */
      if (info.isOffline() ||
        this.master.getRegionManager().regionIsInTransition(info.getRegionNameAsString()) ||
         // St.Ack ^^^^^^^^^^ My guess is we are in here^^^^^^^^^
          (serverName != null && this.master.getServerManager().isDead(serverName))) {
        return;
      }
{code}

I think 'status' in shell:

{code}
hbase(main):003:0> status 'detailed'
version 0.89.0-SNAPSHOT
0 regionsInTransition
1 live servers
    192.168.1.157:49248 1279864501042
        requests=0, regions=3, usedHeap=32, maxHeap=994
        .META.,,1
            stores=2, storefiles=0, storefileSizeMB=0, memstoreSizeMB=0, storefileIndexSizeMB=0
        x,,1279864569260.65c4857477eb31bff0fafae4797a90d8.
            stores=1, storefiles=0, storefileSizeMB=0, memstoreSizeMB=0, storefileIndexSizeMB=0
        -ROOT-,,0
            stores=1, storefiles=1, storefileSizeMB=0, memstoreSizeMB=0, storefileIndexSizeMB=0
0 dead servers
{code}

@Karthik Give me a clue as to what you are thinking and I'll have a go at fixing this one if you don't have the time boss.;;;","23/Jul/10 16:04;karthik.ranga;Hey Stack,

Have a fix ready - testing now, will put it up in a bit.

Fix is simple: we get into this situation because we update the same region in transition in ZK again and again, which bumps up the revision number of the ZNode. This causes the update to fail. So if the ZNode is already in the target state, do not update it again.

The above explanation is super-cryptic :), so will sync up with you on the issue and the fix.;;;","23/Jul/10 17:54;kannanm;Stack:

{code}
hbase(main):002:0> status 'detailed'
status 'detailed'
version 0.21.0-SNAPSHOT
1 regionsInTransition
    name=test1,6531790000,1279787158624.8d5490bfc166c687657cb09203bd7d44., state=PENDING_OPEN
92 live servers
...
{code}

So we find one region in PENDING_OPEN state. HBase ""hbck"" though complained about 2 regions. (So need to look into what was happening with the other region).

And, surprisingly though, ZK UNASSIGNED node shows 4 entries.

{code}
hbase(main):006:0> zk ""ls /hbase/UNASSIGNED""
zk ""ls /hbase/UNASSIGNED""
[9d675cd0b61c44c6605e752490a36eaf, 2e8c1def6dfe3ee1b8fc965629a93041, 9697bd45dc3d62d7e6b519a13d668062, 8d5490bfc166c687657cb09203bd7d44]
{code}

;;;","23/Jul/10 18:02;kannanm;Wondering how this case is handled...

Master asks a RS to open a region, and I guess adds 
it to regions in transition. If RS dies before even starting
to work on the region, is there some timeout mechanism
that kicks in and master realized it needs to give this
region to someone else?

---------

For now, what's the manual steps to be taken to get master 
to reassign the region? Restarting the master would work
I suppose for now. Any other ideas?




;;;","23/Jul/10 18:33;karthik.ranga;One thing we can try is to change the state of the region to ""CLOSED"" in UNASSIGNED in zk...

Alternatively, is it possible to edit META somehow to set the region unassigned? ;;;","23/Jul/10 20:02;stack;@Kannan

bq. ...So need to look into what was happening with the other region...

Yes. Would help improve hbck tool.

bq. ...And, surprisingly though, ZK UNASSIGNED node shows 4 entries....

Whats up w/ that?

So, jgray and karthik, the current state of code in master is that its in transition still.. we have not yet hit the end point -- that there is still a bunch of change coming.  Right?  Can we have a fixup for this issue for now?  I'd like to roll a new 0.89.x but w/ a fix for this (sounds like you have it Karthik).

@Kannan Restart master is how its addressed currently.  Want to add a little tool to UI?

@Karthik A region is unassigned in .META. if it does not have a server and startcode as this one does.;;;","23/Jul/10 21:21;karthik.ranga;
Stack - just uploaded a review at http://review.hbase.org/r/380/;;;","23/Jul/10 21:27;hbasereviewboard;Message from: ""Karthik Ranganathan"" <karthik.ranga@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/380/
-----------------------------------------------------------

(Updated 2010-07-23 14:26:01.718168)


Review request for hbase, stack and Kannan Muthukkaruppan.


Changes
-------

Adding hbase group


Summary
-------

Region permanently offlined - if the ZNode is already in the target state, do not update it again.


This addresses bug HBASE-2866.
    http://issues.apache.org/jira/browse/HBASE-2866


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/master/ZKUnassignedWatcher.java 967128 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java 967128 

Diff: http://review.hbase.org/r/380/diff


Testing
-------

Ran unit tests, went through fine (except TestRowAtomicity, which is known to be failing).


Thanks,

Karthik


;;;","23/Jul/10 21:53;hbasereviewboard;Message from: ""Kannan Muthukkaruppan"" <kannan@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/380/#review470
-----------------------------------------------------------



trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java
<http://review.hbase.org/r/380/#comment1960>

    should this be:
    
     update = true;
    
    


- Kannan



;;;","23/Jul/10 22:02;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/380/#review471
-----------------------------------------------------------


Some nits, I'm also trying it out (with the update=true fix)


trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java
<http://review.hbase.org/r/380/#comment1962>

    Remove all the trailing white spaces



trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java
<http://review.hbase.org/r/380/#comment1963>

    Reuse curState and newState instead



trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java
<http://review.hbase.org/r/380/#comment1961>

    Simply return when you figure that you should, then you can get rid of ""update""


- Jean-Daniel



;;;","23/Jul/10 22:06;hbasereviewboard;Message from: ""Karthik Ranganathan"" <karthik.ranga@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/380/
-----------------------------------------------------------

(Updated 2010-07-23 14:53:37.123965)


Review request for hbase, stack and Kannan Muthukkaruppan.


Changes
-------

Good catch Kannan... updated the diff with comments this time.


Summary
-------

Region permanently offlined - if the ZNode is already in the target state, do not update it again.


This addresses bug HBASE-2866.
    http://issues.apache.org/jira/browse/HBASE-2866


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/master/ZKUnassignedWatcher.java 967128 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java 967128 

Diff: http://review.hbase.org/r/380/diff


Testing
-------

Ran unit tests, went through fine (except TestRowAtomicity, which is known to be failing).


Thanks,

Karthik


;;;","23/Jul/10 22:13;hbasereviewboard;Message from: ""Karthik Ranganathan"" <karthik.ranga@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/380/
-----------------------------------------------------------

(Updated 2010-07-23 15:08:16.286786)


Review request for hbase, stack and Kannan Muthukkaruppan.


Changes
-------

Addressed some comments (re-use variables, removed white spaces)


Summary
-------

Region permanently offlined - if the ZNode is already in the target state, do not update it again.


This addresses bug HBASE-2866.
    http://issues.apache.org/jira/browse/HBASE-2866


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/master/ZKUnassignedWatcher.java 967128 
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java 967128 

Diff: http://review.hbase.org/r/380/diff


Testing
-------

Ran unit tests, went through fine (except TestRowAtomicity, which is known to be failing).


Thanks,

Karthik


;;;","23/Jul/10 22:35;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/380/#review474
-----------------------------------------------------------

Ship it!


Just one comment


trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java
<http://review.hbase.org/r/380/#comment1965>

    What is the watcher being triggered if data is not changing?


- stack



;;;","23/Jul/10 22:39;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/380/#review473
-----------------------------------------------------------

Ship it!


+1 LGTM and TestAdmin passes on my machine without flinching

- Jean-Daniel



;;;","23/Jul/10 22:45;jdcryans;Committed to trunk, thanks Karthik!;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBASE-2553 removed an important edge case ,HBASE-2863,12469888,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,22/Jul/10 06:11,20/Nov/15 12:41,14/Jul/23 06:06,27/Jul/10 21:30,,,,,,,,,,,,0.90.0,,,,,,,0,,,"in HBASE-2553 an important edge case whereby a KV with the same TS in snapshot was lost, tests have been failing (but flakly so) indicating it as well.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26504,Reviewed,,,,Fri Nov 20 12:41:33 UTC 2015,,,,,,,,,,"0|i0hjkv:",100434,,,,,,,,,,,,,,,,,,,,,"22/Jul/10 08:13;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/354/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

There are tricky edge cases that were removed by HBASE-2553 (oopsy!)... flaky tests have been illustrating them. This patch fixes those flaky tests to be not flaky (using the EnvironmentEdgeManager thing) and also fixes them, and introduces tests that cover the particular use cases slightly better as well. Oh yes and and fixes the actual bug.

Without these fixes we would end up with KVs with different values with the same Timestamp which causes problems.  This can happen when we get more than 1 increment/millisecond and especially during a snapshot.


This addresses bug HBASE-2863.
    http://issues.apache.org/jira/browse/HBASE-2863


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/KeyValue.java e32d683 
  src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java 44fa0c3 
  src/main/java/org/apache/hadoop/hbase/regionserver/Store.java c1ff9f2 
  src/main/java/org/apache/hadoop/hbase/util/ManualEnvironmentEdge.java PRE-CREATION 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java 4ead02d 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java a32eed6 

Diff: http://review.hbase.org/r/354/diff


Testing
-------


Thanks,

Ryan


;;;","22/Jul/10 14:09;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/354/#review450
-----------------------------------------------------------


At least missing license needs fixing.  There's at least one question in the below too.  Good stuff.


src/main/java/org/apache/hadoop/hbase/KeyValue.java
<http://review.hbase.org/r/354/#comment1878>

    Nice.



src/main/java/org/apache/hadoop/hbase/KeyValue.java
<http://review.hbase.org/r/354/#comment1879>

    Nice here too



src/main/java/org/apache/hadoop/hbase/KeyValue.java
<http://review.hbase.org/r/354/#comment1880>

    Good stuff.



src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java
<http://review.hbase.org/r/354/#comment1881>

    Could this first kv on the row in memstore have a ts in advance of 'now'?  I suppose it can't -- least it shouldn't be possible, right?
    
    If  thousands of updates a second, could this be a prob?  This logic?



src/main/java/org/apache/hadoop/hbase/util/ManualEnvironmentEdge.java
<http://review.hbase.org/r/354/#comment1882>

    Missing license



src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java
<http://review.hbase.org/r/354/#comment1883>

    Nice test


- stack



;;;","22/Jul/10 19:05;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>


bq.  On 2010-07-22 06:57:10, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java, line 384
bq.  > <http://review.hbase.org/r/354/diff/1/?file=3014#file3014line384>
bq.  >
bq.  >     Could this first kv on the row in memstore have a ts in advance of 'now'?  I suppose it can't -- least it shouldn't be possible, right?
bq.  >     
bq.  >     If  thousands of updates a second, could this be a prob?  This logic?

so what was happening previously:
- snapshot happens
- we do an increment which found there was no 'existing' TS in memstore to 'update' so we make a new one with 'now'.
- we end up with two KVs, one in snapshot one in memstore both with the same timestamp.

Another issue (which the unit test perfectly tests for) is so:
- time=1 snapshot occurs with a KV ts=1 into snapshot
- time=1 ICV happens, but now the KV in memstore _must_ have a TS=2 (or else we get duplicate TS - bad!)
- time=1 ICV happens, but the KV in memstore is TS=2, but now=1, so we need to keep the max(now,TS in memstore) or else we get potential duplicates

In both these cases if we have a Put with ts=Now+X we have problems:
- in HRegion we do a Get and we see Value=100 ts=Now+X
- in Store we do updateColumnValue with Value=101, TS=now
- If we dont clear that ts=Now+X we will put a shadowed KV, and the _next_ ICV will see value '100' not value '101' and we will never actually 'increment' until we get past Now+X.  The compromise is to clear out all KVs in memstore period.

We can end up with a situation where the KV in memstore leads 'real time' by a millisecond during snapshots.  If we had a way of comprehensively deduping equivalent TSs during post-hoc reads perhaps we wouldnt need this.


bq.  On 2010-07-22 06:57:10, stack wrote:
bq.  > src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java, line 405
bq.  > <http://review.hbase.org/r/354/diff/1/?file=3018#file3018line405>
bq.  >
bq.  >     Nice test

thanks, the EnvironmentEdge stuff is paying off already.


- Ryan


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/354/#review450
-----------------------------------------------------------



;;;","22/Jul/10 21:16;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/354/#review458
-----------------------------------------------------------

Ship it!


+1 I buy your rationale.  Please add missing license on commit (Oh, +1 predicated on all tests passing).

- stack



;;;","27/Jul/10 21:30;stack;Ryan committed this a while ago.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
regionserver's logsyncer thread hangs on DFSClient$DFSOutputStream.waitForAckedSeqno,HBASE-2861,12469845,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,hairong,kannanm,kannanm,21/Jul/10 18:49,20/Nov/15 12:43,14/Jul/23 06:06,01/Nov/10 06:23,,,,,,,,,,,,0.90.0,,,,,,,0,,,"During loads into HBase, we are noticing that a RS is sometimes getting stuck.

The logSyncer thread:

{code}
      at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.waitForAckedSeqno(DFSClient.java:3367)
        - locked <0x00002aaac7fef748> (a java.util.LinkedList)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3301)
        at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
        at org.apache.hadoop.io.SequenceFile$Writer.syncFs(SequenceFile.java:944)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.sync(SequenceFileLogWriter.java:124)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.hflush(HLog.java:949)
{code}

A lot of other threads are stuck on:

{code}
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at org.apache.hadoop.hbase.regionserver.wal.HLog$LogSyncer.addToSyncQueue(HLog.java:916)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:936)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.append(HLog.java:828)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1657)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1425)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1393)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1665)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multiPut(HRegionServer.java:2326)
{code}

Subsequently, trying to disable the table, which in turn attempts to close the region(s), caused internalFlushCache() also to get stuck here:

{code}
      at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
        at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:807)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:974)
        at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:511)
        - locked <0x00002aaab76af670> (a java.lang.Object)
        at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:463)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:1468)
        at org.apache.hadoop.hbase.regionserver.HRegionServer$Worker.run(HRegionServer.java:1329)
{code}

I'll attach the full jstack trace soon.


",,hairong,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HDFS-724,,,,,,,,,,,,,,,,,"21/Jul/10 18:53;kannanm;jstack.txt;https://issues.apache.org/jira/secure/attachment/12450077/jstack.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26503,,,,,Fri Nov 20 12:43:59 UTC 2015,,,,,,,,,,"0|i0eon3:",83754,,,,,,,,,,,,,,,,,,,,,"21/Jul/10 18:54;kannanm;Uploaded jstack trace (jstack.txt).

Wondering if this is related to HDFS-895.
;;;","22/Jul/10 00:00;hairong;It looks that org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync did not succeed. Need to take a look at the DataNode log to figure out what happened on the write pipeline. Also why the client did not time out?;;;","22/Jul/10 04:34;stack;Bringing into 0.90.0 (Hurray! Hairong is here!);;;","26/Jul/10 04:47;tlipcon;Strange -- ResponseProcessor here is waiting to read an ack from the DN but not getting anything.

Have you tweaked the DFS client timeout settings at all, by any chance?;;;","26/Jul/10 20:32;nspiegelberg;No custom timeout settings.  We were similarly puzzled that it was stuck here.  The only thing I can think is that SocketIOWithTimeout has a bug.  While investigating this issue, we noticed in DataStreamer.run():

{code}              while (!hasError && ackQueue.size() != 0 && clientRunning) {
                try {
                  ackQueue.wait();   // wait for acks to arrive from datanodes
                } catch (InterruptedException  e) {
                }
              }{code}

ackQueue.wait() has no timeout, but was inside a while() loop that included hasError, a volatile variable.  So I think that needs to be fixed.;;;","26/Jul/10 23:00;tlipcon;Hrm, I don't think the wait timeout should be strictly necessary, since we always notify on ackQueue after changing hasError, right? But I guess it's good practice.;;;","24/Aug/10 23:50;jdcryans;Kannan, did you ever see this issue again? Do you have more info from the DN's log or jstack?;;;","24/Aug/10 23:55;kannanm;No, didn't see this again. And don't have any logs handy any more either. Should we close this out?;;;","13/Sep/10 18:45;kannanm;We hit this issue again. Reopening. Looking at logs... will keep you posted.;;;","16/Sep/10 00:13;hairong;What happened was that the first datanode in the pipeline got stuck at receiving the hflushed data although the client has already sent the packet over. Netstat shows that the packet is already at the datanode side, where port DD is the datanode and CC is the client. It happened that both the client and first datanode were on the same machine XX.
# netstat -v |grep CC
tcp      551      0 machineXX:DD machineXX:CC ESTABLISHED 
tcp        0      0 machineXX:CC machineXX:DD ESTABLISHED;;;","21/Sep/10 23:01;hairong;Hit this problem again. It seems to me that this is either a JVM bug or an OS kernel problem.

Although the root cause of the problem is not confirmed yet, pull in https://issues.apache.org/jira/browse/HDFS-724 to the Hadoop 0.20 branch should fix the client stuck problem.;;;","05/Oct/10 21:23;stack;Changed from blocker to critical.  Shouldn't hold up 0.90.  Will document this as known issue w/ hdfs-724 suggested workaround.;;;","05/Oct/10 21:31;hairong;I've already pulled in HDFS-724 to our internal HDFS 0.20 branch. Will work on getting the fix to Apache HDFS append branch.;;;","08/Oct/10 23:10;stack;@Hairong That'd be cool.;;;","01/Nov/10 06:23;hairong;Closing this since HDFS-724 is committed to append20 branch.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestReplication.queueFailover fails half the time,HBASE-2858,12469768,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,21/Jul/10 00:23,20/Nov/15 12:44,14/Jul/23 06:06,22/Jul/10 00:17,,,,,,,,,,,,0.90.0,,,,,,,0,,,"TestReplication.queueFailover fails 50% of the time, it's because ZooKeeperWrapper.listZnodes (introduced in HBASE-2694 and missed by HBASE-2735) doesn't use the Watcher it's passed so sometimes ReplicationSource misses hlogs to replicate for the region server we kill. Also it uncovered an issue (while I was fixing the first one) that RepSource ignores log files too quickly when the master is a bit too slow to split logs.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26501,Reviewed,,,,Fri Nov 20 12:44:07 UTC 2015,,,,,,,,,,"0|i0hjjz:",100430,,,,,,,,,,,,,,,,,,,,,"21/Jul/10 00:54;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/349/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

This patch fixes the ZKW.listZNodes issue and clears up a bit the path situation in ReplicationSource by removing a lock and adding wits to figure where the log is moved. The test now passes 100% of the time for me (up from 50%).

There's one open issues as outlined by the two TODOS, what happens if a log is missing from HDFS? When the queue is recovered, it could mean that HDFS was cleared but not ZK, but during normal operations it would point to a bug? Report and continue?


This addresses bug HBASE-2858.
    http://issues.apache.org/jira/browse/HBASE-2858


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java e6b365e 
  src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java a037aae 
  src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java 6b9dcb5 
  src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java 2e13a0a 
  src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java e8dd268 
  src/test/java/org/apache/hadoop/hbase/replication/TestReplication.java 163671f 
  src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java bb09bc3 

Diff: http://review.hbase.org/r/349/diff


Testing
-------

Unit testing.


Thanks,

Jean-Daniel


;;;","21/Jul/10 03:47;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/349/#review435
-----------------------------------------------------------


Looks good.  I'm not clear on some of below.  Can you clarify before I +1 it?


src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
<http://review.hbase.org/r/349/#comment1834>

    Just remove this method altogether?



src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
<http://review.hbase.org/r/349/#comment1835>

    Is this right?  You are getting a reader here.  If file is not in expected location, we fall into FNFE area.  These just seem to be testing existance, not getting a reader on the new location if it exists.



src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java
<http://review.hbase.org/r/349/#comment1836>

    See above comment on this same method.


- stack



;;;","21/Jul/10 18:38;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>


bq.  On 2010-07-20 20:34:23, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java, line 221
bq.  > <http://review.hbase.org/r/349/diff/1/?file=2968#file2968line221>
bq.  >
bq.  >     Just remove this method altogether?

I was thinking of keeping it around in the interface for other uses... but currently there's none. Deleting.


bq.  On 2010-07-20 20:34:23, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java, line 437
bq.  > <http://review.hbase.org/r/349/diff/1/?file=2968#file2968line437>
bq.  >
bq.  >     Is this right?  You are getting a reader here.  If file is not in expected location, we fall into FNFE area.  These just seem to be testing existance, not getting a reader on the new location if it exists.

If the queue was recovered, we're waiting for the master to finish splitting it to start reading from it. Then it sleeps and retries.

If it's a normal queue, the location is updated. Then it sleeps and retries with the new location. I guess it could get the reader right away.


- Jean-Daniel


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/349/#review435
-----------------------------------------------------------



;;;","21/Jul/10 20:39;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/349/
-----------------------------------------------------------

(Updated 2010-07-21 13:33:51.105842)


Review request for hbase.


Changes
-------

Updated diff, removed the logArchived method and, when a log is archived, we start reading from the new location right away.


Summary
-------

This patch fixes the ZKW.listZNodes issue and clears up a bit the path situation in ReplicationSource by removing a lock and adding wits to figure where the log is moved. The test now passes 100% of the time for me (up from 50%).

There's one open issues as outlined by the two TODOS, what happens if a log is missing from HDFS? When the queue is recovered, it could mean that HDFS was cleared but not ZK, but during normal operations it would point to a bug? Report and continue?


This addresses bug HBASE-2858.
    http://issues.apache.org/jira/browse/HBASE-2858


Diffs (updated)
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java e6b365e 
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java e1b4077 
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/LogActionsListener.java 9b18992 
  src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java a037aae 
  src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java 6b9dcb5 
  src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceInterface.java 1d7ae7e 
  src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java 2e13a0a 
  src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java e8dd268 
  src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogActionsListener.java d03809c 
  src/test/java/org/apache/hadoop/hbase/replication/ReplicationSourceDummy.java ad9e8fb 
  src/test/java/org/apache/hadoop/hbase/replication/TestReplication.java 163671f 
  src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java bb09bc3 

Diff: http://review.hbase.org/r/349/diff


Testing
-------

Unit testing.


Thanks,

Jean-Daniel


;;;","21/Jul/10 23:56;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/349/#review444
-----------------------------------------------------------

Ship it!


+1

- stack



;;;","22/Jul/10 00:17;jdcryans;Committed to trunk, thanks for the review Stack!;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestAcidGuarantee broken on trunk ,HBASE-2856,12469746,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,amitanand,ryanobjc,ryanobjc,20/Jul/10 19:28,12/Oct/12 05:35,14/Jul/23 06:06,18/Nov/11 02:21,0.89.20100621,,,,,,,,,,,0.94.0,,,,,,,0,,,"TestAcidGuarantee has a test whereby it attempts to read a number of columns from a row, and every so often the first column of N is different, when it should be the same.  This is a bug deep inside the scanner whereby the first peek() of a row is done at time T then the rest of the read is done at T+1 after a flush, thus the memstoreTS data is lost, and previously 'uncommitted' data becomes committed and flushed to disk.

One possible solution is to introduce the memstoreTS (or similarly equivalent value) to the HFile thus allowing us to preserve read consistency past flushes.  Another solution involves fixing the scanners so that peek() is not destructive (and thus might return different things at different times alas).

",,amitanand,anty,bruno,davelatham,dhruba,esteban,evertot,gqchen,hammer,jboyd963,jmhsieh,khemani,larsgeorge,larsh,liyin,nkeywal,nspiegelberg,qwertymaniac,schubertzhang,srivas,wjiangwen,,,,,,,,,,,HBASE-3149,,,,,,,,HBASE-4570,HBASE-5121,,,HBASE-3498,HBASE-3543,HBASE-5569,,HBASE-3290,HBASE-4838,HBASE-3404,,,,,,,"20/Nov/11 06:57;larsh;2856-0.92.txt;https://issues.apache.org/jira/secure/attachment/12504421/2856-0.92.txt","10/Dec/10 00:07;stack;2856-v2.txt;https://issues.apache.org/jira/secure/attachment/12465957/2856-v2.txt","13/Jan/11 00:02;stack;2856-v3.txt;https://issues.apache.org/jira/secure/attachment/12468190/2856-v3.txt","18/Jan/11 19:47;stack;2856-v4.txt;https://issues.apache.org/jira/secure/attachment/12468675/2856-v4.txt","31/Jan/11 09:08;stack;2856-v5.txt;https://issues.apache.org/jira/secure/attachment/12469807/2856-v5.txt","17/Nov/11 23:18;amitanand;2856-v6.txt;https://issues.apache.org/jira/secure/attachment/12504131/2856-v6.txt","17/Nov/11 23:18;amitanand;2856-v7.txt;https://issues.apache.org/jira/secure/attachment/12504132/2856-v7.txt","18/Nov/11 01:24;amitanand;2856-v8.txt;https://issues.apache.org/jira/secure/attachment/12504157/2856-v8.txt","18/Nov/11 02:00;amitanand;2856-v9-all-inclusive.txt;https://issues.apache.org/jira/secure/attachment/12504160/2856-v9-all-inclusive.txt","08/Dec/10 00:57;stack;acid.txt;https://issues.apache.org/jira/secure/attachment/12465764/acid.txt",,,,,,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26500,,,,,Wed Nov 23 01:00:41 UTC 2011,,,,,,,,,,"0|i08qpj:",48927,,,,,,,,,,,,,,,,,,,,,"27/Jul/10 15:28;stack;Making this a blocker.  Also disabling in TRUNK for now so tests pass for the second 0.89 release.;;;","16/Aug/10 22:01;ryanobjc;This issue is pernicious and difficult.  I have attempted many times to nail this but there continue to be issues.

Right now there are at least 2 issues:
- When a memstore is flushed to HFile the 'memstoreTS' generation stamp is dropped.  With the way the scanners work right now, this causes some minor issue.  A row is really composed on a sequence of KeyValues, with different memstoreTS values, so you can imagine right before the flush the scanner looks sort of like so:

                           scanner points here
(new gen here) |
K K K K K K K K K K K K K K K K K K K K K  (keys for 1 row)
                                    (new and old gen)

The new gen have newer memstoreTS value and were ignored during the next().  So the scanner is pointing to a Key part of the way through a row.   When the memstore is pushed to a HFile, we mistakenly ignore all the skipped new gen keys, and we end up with the FIRST KeyValue from the older gen, and the subsequent columns from the newer gen, ending up with a mixed generation result, thus looking like non-atomic put.

One solution to this problem is to take the top Key during a scanner reset (done after a switch between memstore -> hfile) and transform it into KeyValue.createFirstOnRow(). There has to be exception code for partial row fetches where the scanner gets partial row results each time (Because the row is too big).

- The second issue is that of scanner updates and multi-column families.  Right now we do code sort of like this:
for each store (aka: family) in region:
   switch memstore -> hfile

For each Store/family the switch between memstore->hfile is done atomically with respect to scanners, that is for a scanner they either access the old snapshot and old files or the new null snapshot and the new files.  But this atomic snapshot is done at a Store at-a-time level, meaning that concurrent scanners might see some data in one store from memstore (complete with generation timestamps) and some data in another store all from hfile (with no timestamps), thus again ending up with mixed generation row data that violates row atomicity.

One possible solution to this problem is to move scanner updates to the HRegion/RegionScanner level.  This is not as easy as it sounds from a code structure point of view.  Furthermore by using larger update blocks we may be introducing performance issues surrounding scanner updates. 


-----------

The ultimate problem here is these two problems are just the most recently identified bugs in a long list of bugs.  We can fix them in the ways described above, but what about the next series of bugs?  We are plugging leaks in a dam.  A better approach would be to build a new, better, dam, only with the knowledge we have about what previously went wrong.

------------

The proposal is thusly:
- Add a new generation timestamp (or perhaps we could call it transaction id) to every KeyValue.  This generation timestamp is also serialized and stored inside HFile.  
- The generation timestamp starts at 1 for brand new tables.  Every write operation increments this by 1, much like ReadWriteConsistencyControl
- KeyValues with ts=0 belong to no generation and are always included in every read operation
- KeyValues with ts>0 belong to that specific generation and the reader should include them only if their own read point is larger. The reader obtains the read point at scanner start time, and may possibly update it between rows if the implementation can do so. (As per the current RWCC algorithms)
- Bulk imported hfiles have all their TS=0 (the default value)
- Modifications set their ts=X as per the current RWCC algorithms.
- During hfile flush the largest TS is written to the HFile metadata.
- During HRegion/Store init, the largest TS is obtained from HFile metadata, and the RWCC is set to this value.

Some predicted questions:
- What about the extra space? 8 bytes is a lot.  
-- With vint encoding and that the values start at 1, we should be able to shave a lot of space off.  We will also have to store a length field with 1 extra byte. 2 bytes minimum for '0'.
- What about vint decode speed?
-- It may not be an issue, we need to profile.  If it is, we can trade off RAM space in KeyValue to cache this.
- What about file migration? I dont want to take a long time to upgrade my cluster!
-- By creating a new metadata key, call it KEYVALUE_VERSION, the Store layer on top of HFile can parse the older KeyValues from HFile (which wont have this metadata), substituting '0' for the missing timestamp. 
-- Newer HFiles will use the newer KeyValue parse code to obtain the generation timestamp.
- This is a lot of cost just to fix something that seems we should know in memory
-- Yes it isn't cheap, but this problem has proven to be difficult to solve. By addressing the issue directly we can have confidence in our design and the existing test frameworks which are failing should validate it for us.
- What about splits?
-- Each split region will 'take off' from the maximal generation timestamp and evolve independently.  If they were to be merged together that'd be ok, each region would have to close, halting all writes, then take the largest generation timestamp and continue evolving from there.  The timestamp exists to differentiate committed and non-committed data, and during a close by definition all values in the HFile would be committed.
- Ok so I'm paying at least 2 bytes per KeyValue, can we get better return on investment?
-- Yes!  By using the generation timestamp we can detect the situation where a previously inserted Delete is masking new Puts.  We can switch between the existing behaviour and newer behaviour at runtime and ignore the rogue Delete marker.
-- There may be other benefits as well. Providing a partial total order to all the KeyValue inserts and grouped by transaction would provide useful debugging info, for both HBase and client bugs.
- What about read isolation?
-- With this we can definitely implement at-scanner-open read isolation.  Due to the nature of the scanner design advancing the generation timestamp read point is more difficult, but it may be possible.


;;;","17/Aug/10 19:40;ryanobjc;one addendum to this proposal, the hadoop vint format already encodes the length in an ""easy to read"" way.  For small values < 120 or so the vint is encoded in 1 byte, else it may take up to 9 bytes.;;;","20/Aug/10 00:27;pranavkhaitan;I guess we could fix this by not updating the scanners after a flush. Currently, after every flush we are notifying the scanners (called as observers) so that they update their heap. If we do not notify them about the flush, the scanner wouldn't encounter any inconsistencies. This should solve the specific problem you discussed above where flushing results in inconsistency. This seems like an easy change and maintains correctness. The only drawback is that we are holding some memstore keys for a little longer which doesn't seem too big of a problem.;;;","20/Aug/10 02:04;ryanobjc;That sounds possible... the extra memory held could be up to 64mb *
block-size * # of families. Ie: a few hundred megs or even gigs.

https://issues.apache.org/jira/browse/HBASE-2856?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12900535#action_12900535]
Currently, after every flush we are notifying the scanners (called as
observers) so that they update their heap. If we do not notify them about
the flush, the scanner wouldn't encounter any inconsistencies. This should
solve the specific problem you discussed above where flushing results in
inconsistency. This seems like an easy change and maintains correctness. The
only drawback is that we are holding some memstore keys for a little longer
which doesn't seem too big of a problem.
columns from a row, and every so often the first column of N is different,
when it should be the same. This is a bug deep inside the scanner whereby
the first peek() of a row is done at time T then the rest of the read is
done at T+1 after a flush, thus the memstoreTS data is lost, and previously
'uncommitted' data becomes committed and flushed to disk.
equivalent value) to the HFile thus allowing us to preserve read consistency
past flushes. Another solution involves fixing the scanners so that peek()
is not destructive (and thus might return different things at different
times alas).
;;;","20/Aug/10 23:48;stack;@Ryan Thanks for writing this up.   Useful.  My thinking is that the extra ts/version is too much to do just now as we're coming up fast on a 0.90.x.  Its a 0.92.x project I'm thinking especially as I think it'll take a bit of work to implement it so migration is done at runtime.  Also, following on from the Pranav suggestion, for now, can we not just let the scanner exhaust the current row only against whatever was in memstore rather than let the scanner run to the end of the region and then when it reaches the end of the row, then let go of memstore reference?  In other words, only at row junctures update its memstore/hfile scanner set?;;;","20/Aug/10 23:55;pranavkhaitan;@Stack, in my understanding what you are saying is happening now. The scanner keeps hold of the snapshot till it is reading the current row and only lets go of it before it starts reading the next row.;;;","21/Aug/10 00:09;stack;@Pranav: Now I seem to recall Ryan remarks where the row boundary is unknowable; of how the only way you learn of a row boundary is AFTER you've read the first on the next row.  At this stage you have already read into the next row so can't make the memstore/hfile switch.;;;","21/Aug/10 06:15;ryanobjc;You guys have iet right, that you can't know the next row until you
are already past the first key.

The problem is that we would have to hold on to the snapshot and kvset
until the end of the scanner when it is closed.  This will up our
chance of preventable OOME, but it would depend on the scanner use
patterns. It seems risky to me to take this approach.

I'm not sure we want to change the entire nature of how scanner
updates work, it would require redoing the heap a bit to bubble update
scanner events down.  I'm not sure how complex that patch would be,
there is no way to easily know until the work is done.  But without
either of those two approaches, this JIRA may have to be a 0.92 issue.




;;;","21/Aug/10 18:47;stack;So, rather than return inconsistent data around a storefile switch, since we get rows at a time (we do -- right?), what if we aborted the current row when we have to swap memstore for new file?  What if we threw an exception?  Let the client fix up the scanner and make it come back in on the row we were about to serve? (We do something like this when a NSRE anyways)?;;;","01/Oct/10 22:25;ryanobjc;to fix it once and for all requires hfile/serialization changes.;;;","02/Dec/10 23:32;nspiegelberg;When this issue is fixed, see if we can bring sorting back into the compaction algorithm to handle odd skew due to bulk migrations.;;;","06/Dec/10 23:25;stack;Ryan, Kannan, and myself chatted on this issue a while:

+ We'll add a new insertion sequence id into the KeyValue Key.  It will be inserted after the current Timestamp and before KeyValue Type.  This new insertion sequence id will be used resolving 'order' when all else matches in two KeyValue keys.
+ We'd use the last two bits of the KeyValue Type for 'version'.  We'll set bit 128 to denote KeyValue version '2' (what we currently have is KeyValue version '1').
+ We'd adjust our comparators so they ignored these upper bits in Type so we can compare Type 1 and Type 2 KeyValues.
+ A total cluster restart will be needed.  We'll up the RPC version to be sure.
+ Post-restart, writes will be version 2 KeyValues.
+ Reads may be a mix of version 2 and version 1 KeyValues while regions are made of a mix of old and new style HFiles.  This should be fine as long as comparators work properly witha mix of version 1 and version 2 KVs.
+ We're thinking the new insertion sequence id will be fixed-length long.  A vint/vlong will make for an awkward parse.
;;;","06/Dec/10 23:25;stack;Please add to the above if I missed anything Ryan/Kannan;;;","07/Dec/10 00:22;ryanobjc;I think we can use HLog sequenceID as the ""insertion sequence ID"".  

This is how it will work:

- HLog append happens, resulting in sequence ID  ""X""
- Start memstoreInsert, but instead of using what is provided, use X instead.
- Do the usual, stamping KVs with X
- Commit and push read point forward to X when done.

Successive ""X"" values will be increasing in value.  One thing we will need to do is initialize ""RWCC"" read point to the largest HLog sequenceID on Region open (before region can take any edits).  

;;;","07/Dec/10 00:32;ryanobjc;question was: would we overflow a long?  

At 2^63, we have about 9.22 x 10^18 values available.  
At 50,000 operations/second, we will take about 5,849,424 years to roll over.

Stack points out, what about bulk commit, this would then wrap up every bulk commit as 1 memstore/RWCC transaction, which might be the logically wrong thing to do, and also the extra time it takes to insert that much stuff into memstore could also be an issues with RWCC.  It is very important that RWCC transactions stay short or else we pile up too many handlers.
;;;","08/Dec/10 00:57;stack;Making a start.  Adding version to KV.;;;","10/Dec/10 00:07;stack;This patch adds sequence number to KeyValue.  Also adds handling of version (version of KV is an internal detail, not let out of KV).  All current TestKeyValue tests pass but everything else is broke currently.

The new KV particle is called sequence number.  Its looking like this sequence number could be the HRS sequence id.  I've purposely not made connection between the two.  We'll start out with HRS sequence id == KV sequence number but this may change at some later time.

Will next make RWCC use the KV sequence number instead of the KV data member it used keep up.;;;","14/Dec/10 08:32;ryanobjc;I'd prefer bitbashing to be in hex, not decimal, eg:


  static final byte VERSION_BITS = (byte)0x40;

instead of:

  static final byte VERSION_BITS = (byte)64;

it makes it clearer I think.  Do you really need to have a constant for the bit compliment of a different constant?  The compiler will do the optimization properly and inline constants if you do ~(some other thing thats a constant).;;;","14/Dec/10 15:06;stack;I'll use Hex.  On the define, I think intent is clearer than doing ~SOMETHING.;;;","07/Jan/11 22:52;stack;@Ryan Regards using seqid for the 'memstoreTS', what about this special case of memstoreTS == 0?  How we handle that if we move seqid?;;;","07/Jan/11 22:55;pranavkhaitan;Thanks for your message. I am currently traveling without email access
and will be able to read your email by 8th Jan.

Regards,
Pranav
;;;","07/Jan/11 22:57;ryanobjc;i don't think seqid ever is 0. if so, can't we just start it at 1
instead and be ok?
;;;","10/Jan/11 22:13;ryanobjc;one thought I had, if we are doing replication do we use seqid or do
we generate a new one locally at each cluster?

If we allow multimaster we will probably have to generate a new one at
each cluster replication target. Else we might end up with a situation
where new incoming edits are not visible yet, due to being < current
seqid.
;;;","10/Jan/11 22:15;jdcryans;The edit is applied as a Put through HTable.;;;","13/Jan/11 00:02;stack;Latest.  Starts moving readPoint up into HRegionScanner and out of MemStore.  Adds tests.  Adds setting sequence number into KVs whether they go via WAL or not, etc.   Not done by a long shot and probably not in a state for review.  Just adding work to date.;;;","13/Jan/11 00:46;stack;Just had good conversation with Ryan.  We conclude that using the HLog sequence number is NOT a good idea, mostly for performance reasons.  Too many updates will be stuck waiting on the completion of edits that may have started before our update but that have yet to complete (we do not want to return to the client until all transaction started before ours -- but that are slower than ours to run --  have completed else there is the danger of not being able to see what you have written).  Instead, we need to keep a running sequence number that is per HRegion rather than per HRegionServer as HLog sequence number is.  This new HRegion sequence number  is very much like HLog sequence number in that on open of HRegion we read in the largest and then increment from there.

Let me try and explain how we arrived at this notion.

We do ACID - - prevent readers reading part of an update --  by only letting clients (scanners and gets) read stuff that has been fully committed.  Currently we do this by moving forward a monotonically increasing  'read point'.  Each update is given a write point.  The read point is moved forward to encompass all completed write points or  'transactions'.  Transactions complete willy-nilly but the read point will not move beyond the incomplete.

Here are the coarse steps involved in a 'transaction':

{code}
(0) row lock (Put, Increment, etc.)
(1) Go to WAL
(2) get new sequence id
(3) actually write WAL
(4) update memstore
(5) wait for our edit to be visible
(6) commit/move forward the read point 
(7) undo rowlock
{code}

Up to this, the way we did 'ACID' was around memstore only.  The readpoint is kept up inside in an instance of RWCC.  A RWCC instance is Region scoped (one is created on creation of a HRegion).  A new writepoint is created when we go to write the memstore in step (4) above and then the readpoint is moved forward to match the writepoint just before we do step (7) in the above.  Currently our RWCC transaction spans step (4) to (7) roughly.

""Wait to be visible"" in the above means wait until all transactions that have an id that is less than mine complete before I proceed to update the read point and return to the client. A transaction that started before us may not complete until after ours because of thread scheduling, hiccups, etc.  We do not want to move the read point forward until all updates previous to ours have completed else we'll be letting clients read the incomplete earlier transactions.

Of note in the above, how long the WAL takes is not part of a RWCC transaction.

IF we move to using HLog sequence numbers, now the transaction starts at step (1) when we go to the WAL.  We'll need to update in RWCC the writepoint at step (1).  The HLog sequence number is for all of the region server, its not just HRegion scoped.   The 'wait for our edit to be visible' will be dependent now on the completion on edits against unrelated HRegions whose character may be completely different (e.g. the schema on HRegion A may be for increments whereas the schema on HRegion B may be for fat batches of cells.  If both are on the same regionserver, the 'wait for our edit to be visible' may have the increments waiting on the completion of a fat batch of updates).

So, the thought is instead to have a per region sequence number with the write point updated only after we emerge from the WAL append.  We keep the current 'transaction' scope where scope is between steps (4) and (7) in the above.

I'm going to go implement the per region edit number unless an alternative suggested.;;;","13/Jan/11 01:02;nspiegelberg;@stack: we briefly talked about this issue internally the other week.  I think you want a per-CF sequence ID.  We could split flushes to happen on a per-CF basis and a lot of our filtering needs to be done on a per-file basis (and consequently, per-CF).;;;","13/Jan/11 01:12;ryanobjc;if you use a per-CF sequence ID you can only have atomic properties on
at most a column family.  We use the transaction id to know which ones
were committed and which ones were not, and if it wasn't across all
the CFs, we would not have atomicity across all CFs which is what we
DO want.

even if we split flushes to happen on per-Store/CF basis, it would not
affect the acid guarantees we are trying to achieve with this patch.
;;;","13/Jan/11 01:47;nspiegelberg;@ryan.  sorry, I looked at this description a little too quickly.  We were talking about slightly different scenario with HMaster log split pruning.  You are correct, the region level seems to be the correct location.;;;","13/Jan/11 05:12;stack;Thinking on the way home and after chatting more with Ryan, I'm thinking that we do not want two sets of sequence numbers -- one at regionserver level (HLog sequence id) and then another HRegion-scoped one.  All edits are bottlenecked via the WAL -- give or take the skip WAL write flag and deferred flush -- and so arbitrarily breaking the transaction into two phases, the WAL write then the commit to memstore, seems like it gains us little; edits will stack up behind the WAL chicane anyways.

Paranoia regards performance issues come from long waits in 'wait to be visible' phase when the Transaction spanned WAL.

I'm thinking now that for this issue, we do it first 'correct' -- e.g. use HLog sequence number throughout and start the 'transaction' pre-WAL -- and then in a new issue figure the smarts that will get us over the slowness we've seen before taking this tack.;;;","13/Jan/11 06:02;ryanobjc;Unfortunately the paranoia re: performance is borne out by direct
experience.  It will be an issue, and it will be a blocker and we
should deal with it right now. Since fixing it might require
architectural level changes in how we manage these things internally.
including up to and including using a different ID stream for atomic
consistency.

Backing up a bit, the basic issue is that a handler thread cannot
complete and return to the client until the row-transaction it was
working on is visible to other clients. To do otherwise risks data
loss for ICV and inconsistent read-your-own-write scenarios for
clients.  But while waiting we are tying up a handler thread, and have
to wait on the longest pole HLog append (which can take seconds at
their worst!).  You end up with a RS level stall which is pretty ugly.

I dont want 2 sets of sequence numbers, but I am concerned that we
might need it.  Perhaps we can find a more elegant mechanism of
cheaply keeping track of which seqids are 'committed' and visible and
which are not.  Right now we use a simple 'read point' which acts like
a line in the sand.  Previous proposals called for a bitmask of the
last N numbers.  The problem with this is that deferred flushing
combined with non-deferred flushing would cause major problems, as the
last N we need to keep track of keeps on expanding.

Perhaps a reverse bitmask where we keep track of the PREVIOUS N tx
that are NOT committed might make more sense.  Implementing it
efficiently is another question.
;;;","13/Jan/11 06:29;stack;How do the arrays of outstanding txs work?  How do you correlate a particular bit to a particular transaction?

I don't get how this would be different than read point since you can't return to client till all transactions before yours have completed, right?  Or does the bit array somehow expand our vocabulary beyond binary read point?;;;","13/Jan/11 06:34;ryanobjc;yes precisely, the bit array allows us to not just have a binary read
point threshold, hence we dont have to 'wait' anymore.

at the cost of space and complexity though. how to do it in a
manageable way is outstanding.
;;;","14/Jan/11 00:27;nspiegelberg;@ryan : I'm trying to understand why you think we might need 2 seqids.  If you move the seqid from the RS level to the region level, I see a lot of benefits beyond this jira.  Log recovery, for example, wouldn't need to balance between the dead RS's seqid numbering and the new RS's numbering. ;;;","18/Jan/11 19:47;stack;Added in some work on runtime-migration of old-style KVs. 

Patch not yet ready for review.;;;","31/Jan/11 09:08;stack;More self-migration fixes and tests.;;;","01/Feb/11 20:13;stack;Chatting w/ Benoît last night, he had a good suggestion.  He suggested we NOT add extra long to the KV, that instead we just use the already existing in-memory only memstorets that is in each KV.  He suggested that on open of a storefile, that we just pick up its oldest sequence number or assign a sequence number on opening and keep that too in memory associated with the storefile.  Chatting, whenever we read in a KV from a store file, we could insert into the memstorets field the storefiles sequence number; all edits in a storefile would have the same sequence number.  When we flush, and we bring online the new storefile, it would have the flush sequence number.  We would still need to move to using sequence number instead of the incrementing number we have in RWCC and we'd need to have the transaction, for correctness, span the WAL where it doesn't now, but this suggestion would save our upping the Key part of KV when persisted.

Trying the above on Ryan.;;;","01/Feb/11 21:46;ryanobjc;Don't we already have this? The comparator uses the max_seq_id to break ties between KVs...

The primary issue is that we need to know which KVs are 'committed' and which are still being created in progress. Right now we have a problem whereby the scanner stack gets a little wonky about how it handles partial next()s. By moving the memstoreTS pruning up to the HRegion scanner level, and working on entire rows at a time, this might mitigate most of the problem actually. This might get ugly with family only flushes, since in theory you might end up with a row that is not completely written but is in memstore & hfile at the same time. Given that the scope of a RWCC ""transaction"" is only memstore insert, I'm not sure how that would happen. It's possible we could prevent it from becoming a problem with judicious use of the updateLock in HRegion though.

For example, by grabbing the updateLock.writeLock().lock() during the switch over, or the flush, we could ensure that all the pending writes are now complete, then do the switch out, then we'd never have a situation where a half committed write is in memstore & hfile at the same time.;;;","02/Feb/11 01:19;stack;Chatting with Ryan:

+ To be solved is how read/write rwcc points are respected on hfile flush; how do we not pull from hfile, items that are in the future as far as current rwcc read point is concerned (especially when cf7 of a ten cf row flushes mid-read).
+ Soln is that we'll move the read point forward up in region scanner on each next invocation; i.e on entrance into a new row.  We'll also only swap in new hfiles on next up in the region scanner (rather than down in store scanner as is currently done).  So, if row of 100 cfs and 1M columns, as we're reading, we'll hold the rwcc read point.  cf 48 and cf 59 might flush but we'll not swap in their new store files until we get to the end of the row (we'll be holding on to the snapshots for a little longer than we do now).
+ On next up in region scanner, we also need to reseek each row even though this could be a perf killer.  Our current notion of end-of-row marker is the kv that does not have a row that matches that of the row we are currently in.  Lets call this next row kv kvnext.  We park here in between next invocations.  Well, what if in between region next invocations, there is a big pause and a bunch of puts come in only the puts have same row as kvnext AND they happen to sort before the kvnext at which we are currently parked.  We have to reseek (It could be worse, a new row could have been inserted between next invocations in between kvbefore and kvnext...... if parked at kvnext we're not going to see it, not unless we do hbase-3498).
+ We do not think we need to add a sequence number to KV, one that is persisted out to HFile.
+ It looks like we do not need to use hlog's sequence number all over; we can keep up RWCCs little incrementing value.  We can also keep its memstore scope -- as opposed to what was being discussed above where we were going to broaden the scope to cover WAL writing.;;;","02/Feb/11 16:01;nspiegelberg;http://rhaas.blogspot.com/2011/02/mysql-vs-postgresql-part-2-vacuum-vs.html
Interesting article comparing how InnoDB and PostgreSQL handle RWCC for roughly this same issue.;;;","02/Feb/11 16:10;stack;Thanks for pointer N.;;;","02/Feb/11 18:13;stack;That article just talks about mvcc, which is kinda what we're doing already.  Our difficulty though is managing versions across row boundaries and flushes.;;;","03/Feb/11 00:43;stack;Regards HBASE-2673, which is about consistent view when intra-row scanning, if we do NOT add the transactionid/sequenceid to the hfile, then we must punt on it; i.e. intra-row scanning, you will not get a consistent view.;;;","03/Feb/11 19:46;stack;The other dimension we need to consider is bulk load -- especially the new multifamily bulk load.  The bulk addition needs to come in at a row boundary so we keep up a consistent row view.;;;","03/Feb/11 19:50;ryanobjc;i dont think bulk load is an issue, since we only call update readers between rows (once we move the update readers to the HRegion.Scanner level), then it will be an atomic 'appearance' of data. Does that sound right?;;;","21/Jun/11 19:43;stack;Moving out of 0.92.  The work won't be done in time.;;;","07/Sep/11 18:58;amitanand;I have created a few sub-tasks and submitted a diff for the same. Could y'll 
have a look at it, and let me know what you think? 

Thanks,
-Amit;;;","05/Oct/11 19:20;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/
-----------------------------------------------------------

Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.


Summary
-------

address the 2856 issues by writing the memstoreTS to the disk.

version v11 of the patch.

uploading it here for easier review process.


This addresses bug HBASE-2856.
    https://issues.apache.org/jira/browse/HBASE-2856


Diffs
-----

  /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1174515 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1174515 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1174515 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1175027 
  /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1174515 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1174515 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1174515 
  /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1174515 
  /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1174515 
  /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1174515 
  /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1175027 
  /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1174515 
  /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1174515 
  /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1174515 

Diff: https://reviews.apache.org/r/2224/diff


Testing
-------

mvn test


Thanks,

Amitanand

;;;","05/Oct/11 22:08;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/#review2369
-----------------------------------------------------------


Find inline some partial review comments (joint review with Kannan)

- Karthik


On 2011-10-05 19:18:51, Amitanand Aiyer wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/2224/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-10-05 19:18:51)
bq.  
bq.  
bq.  Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  address the 2856 issues by writing the memstoreTS to the disk.
bq.  
bq.  version v11 of the patch.
bq.  
bq.  uploading it here for easier review process.
bq.  
bq.  
bq.  This addresses bug HBASE-2856.
bq.      https://issues.apache.org/jira/browse/HBASE-2856
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1175027 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1174515 
bq.    /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1175027 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1174515 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1174515 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1174515 
bq.  
bq.  Diff: https://reviews.apache.org/r/2224/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  mvn test
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Amitanand
bq.  
bq.

;;;","05/Oct/11 22:08;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/#review2366
-----------------------------------------------------------



/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
<https://reviews.apache.org/r/2224/#comment5451>

    Rename to shouldIncludeMemstoreTS



/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<https://reviews.apache.org/r/2224/#comment5434>

    Whitespace



/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<https://reviews.apache.org/r/2224/#comment5435>

    Whitespace



/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<https://reviews.apache.org/r/2224/#comment5436>

    whitespace



/src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java
<https://reviews.apache.org/r/2224/#comment5437>

    Is this constructor used anywhere? We could remove both this and the default constructor as this.memstoreRead and this.memstoreWrite should be 0 by default.



/src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java
<https://reviews.apache.org/r/2224/#comment5450>

    Rename to useRWCC



/src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java
<https://reviews.apache.org/r/2224/#comment5448>

    if () {
      ...
    }



/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
<https://reviews.apache.org/r/2224/#comment5442>

    Comment update - sequence id should be timestamp



/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
<https://reviews.apache.org/r/2224/#comment5449>

    Pass hint to StoreScanner about whether rwcc is to be used or not (non-compaction versus compaction)


- Karthik


On 2011-10-05 19:18:51, Amitanand Aiyer wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/2224/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-10-05 19:18:51)
bq.  
bq.  
bq.  Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  address the 2856 issues by writing the memstoreTS to the disk.
bq.  
bq.  version v11 of the patch.
bq.  
bq.  uploading it here for easier review process.
bq.  
bq.  
bq.  This addresses bug HBASE-2856.
bq.      https://issues.apache.org/jira/browse/HBASE-2856
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1175027 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1174515 
bq.    /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1175027 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1174515 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1174515 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1174515 
bq.  
bq.  Diff: https://reviews.apache.org/r/2224/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  mvn test
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Amitanand
bq.  
bq.

;;;","05/Oct/11 23:22;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/#review2365
-----------------------------------------------------------


this is some good stuff.  nice work amit.


/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
<https://reviews.apache.org/r/2224/#comment5429>

    whitespace added in this file



/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
<https://reviews.apache.org/r/2224/#comment5439>

    maybe add one line of javadoc to describe why we need this method (this is a method that gets used pretty widely in the code to support this change)



/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
<https://reviews.apache.org/r/2224/#comment5430>

    should we be pulling this stuff from KeyValue constants instead of Bytes directly?



/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
<https://reviews.apache.org/r/2224/#comment5431>

    debug?



/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
<https://reviews.apache.org/r/2224/#comment5433>

    maybe move this Bytes.SIZEOF_LONG into some other constant that reflects what this is for?  KeyValue.MEMSTORE_TS_SIZE or some such thing



/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java
<https://reviews.apache.org/r/2224/#comment5440>

    comment here



/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java
<https://reviews.apache.org/r/2224/#comment5441>

    accidental new line w/ whitespace



/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<https://reviews.apache.org/r/2224/#comment5443>

    introducing whitespace here and throughout file



/src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java
<https://reviews.apache.org/r/2224/#comment5444>

    since this is a public constructor, add a comment that this initializes to 0



/src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java
<https://reviews.apache.org/r/2224/#comment5445>

    add javadoc to the public method and maybe turn it into a setter.  i actually have a chance i'd like to get in eventually that requires disabling rwcc for a specified read



/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
<https://reviews.apache.org/r/2224/#comment5446>

    this javadoc is stale now



/src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java
<https://reviews.apache.org/r/2224/#comment5447>

    just remove


- Jonathan


On 2011-10-05 19:18:51, Amitanand Aiyer wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/2224/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-10-05 19:18:51)
bq.  
bq.  
bq.  Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  address the 2856 issues by writing the memstoreTS to the disk.
bq.  
bq.  version v11 of the patch.
bq.  
bq.  uploading it here for easier review process.
bq.  
bq.  
bq.  This addresses bug HBASE-2856.
bq.      https://issues.apache.org/jira/browse/HBASE-2856
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1175027 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1174515 
bq.    /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1175027 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1174515 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1174515 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1174515 
bq.  
bq.  Diff: https://reviews.apache.org/r/2224/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  mvn test
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Amitanand
bq.  
bq.

;;;","05/Oct/11 23:24;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/#review2374
-----------------------------------------------------------

Ship it!


LGTM (Caveat the feedback by the boys above).  Minors in the below to consider if making new patch.  Oh, lots of whitespace as the lads say that you might purge though no biggie.


/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java
<https://reviews.apache.org/r/2224/#comment5461>

    If new version, fix this ... add parens or put it all on one line.. hard to read as is.



/src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java
<https://reviews.apache.org/r/2224/#comment5462>

    Add parens if you are making new patch here.



/src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java
<https://reviews.apache.org/r/2224/#comment5463>

    ditto



/src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java
<https://reviews.apache.org/r/2224/#comment5464>

    Parens



/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
<https://reviews.apache.org/r/2224/#comment5465>

    Is this a good name?  Its come back in from the storefile metadata.  There is no memstore in this context.  No biggie.  Just saying.


- Michael


On 2011-10-05 19:18:51, Amitanand Aiyer wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/2224/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-10-05 19:18:51)
bq.  
bq.  
bq.  Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  address the 2856 issues by writing the memstoreTS to the disk.
bq.  
bq.  version v11 of the patch.
bq.  
bq.  uploading it here for easier review process.
bq.  
bq.  
bq.  This addresses bug HBASE-2856.
bq.      https://issues.apache.org/jira/browse/HBASE-2856
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1175027 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1174515 
bq.    /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1175027 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1174515 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1174515 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1174515 
bq.  
bq.  Diff: https://reviews.apache.org/r/2224/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  mvn test
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Amitanand
bq.  
bq.

;;;","05/Oct/11 23:54;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/#review2376
-----------------------------------------------------------


Rest of the diff reviewed, and comments inline. (Joint review with Kannan).


/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
<https://reviews.apache.org/r/2224/#comment5472>

    Rename to: hasMemstoreTS



/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
<https://reviews.apache.org/r/2224/#comment5469>

    if () {
    }



/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
<https://reviews.apache.org/r/2224/#comment5470>

    Remove comment



/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
<https://reviews.apache.org/r/2224/#comment5471>

    Should check against the constant:
    Bytes.toInt(keyValueFormatVersion) == KEY_VALUE_VER_WITH_MEMSTORE



/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java
<https://reviews.apache.org/r/2224/#comment5467>

    if () {
    }
    
    1. Discussed with Amitanand, he is planning to move this to the end of the KV (to play nice with delta encoding). 
    2. Also planning to store this in varying-length format
    3. Also, if (kv.memstoreTS < current read point across all scanners) then we can just write a 0. This would be the case for most of the KV's except the last few written. 



/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java
<https://reviews.apache.org/r/2224/#comment5468>

    Need a:
    if (this.includeMemstoreTS) {
    }


- Karthik


On 2011-10-05 19:18:51, Amitanand Aiyer wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/2224/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-10-05 19:18:51)
bq.  
bq.  
bq.  Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  address the 2856 issues by writing the memstoreTS to the disk.
bq.  
bq.  version v11 of the patch.
bq.  
bq.  uploading it here for easier review process.
bq.  
bq.  
bq.  This addresses bug HBASE-2856.
bq.      https://issues.apache.org/jira/browse/HBASE-2856
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1175027 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1174515 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1174515 
bq.    /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1175027 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1174515 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1174515 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1174515 
bq.  
bq.  Diff: https://reviews.apache.org/r/2224/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  mvn test
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Amitanand
bq.  
bq.

;;;","06/Oct/11 01:51;yuzhihong@gmail.com;FYI patch v11 from HBASE-4344 didn't contain 4485-v4.txt for HBASE-4485.;;;","10/Oct/11 18:28;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/
-----------------------------------------------------------

(Updated 2011-10-10 18:27:58.815672)


Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.


Changes
-------

use variable length encoding to store the memstoreTS. 


Summary
-------

address the 2856 issues by writing the memstoreTS to the disk.

version v11 of the patch.

uploading it here for easier review process.


This addresses bug HBASE-2856.
    https://issues.apache.org/jira/browse/HBASE-2856


Diffs (updated)
-----

  /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1179910 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1179910 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1179910 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1179910 
  /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1179910 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1179910 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1179910 
  /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1179910 
  /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1179910 
  /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1179910 
  /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1179910 
  /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1179910 
  /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1179910 
  /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1179910 

Diff: https://reviews.apache.org/r/2224/diff


Testing
-------

mvn test


Thanks,

Amitanand

;;;","10/Oct/11 23:14;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/
-----------------------------------------------------------

(Updated 2011-10-10 23:14:05.211809)


Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.


Changes
-------

address some of the comments given.


Summary
-------

address the 2856 issues by writing the memstoreTS to the disk.

version v11 of the patch.

uploading it here for easier review process.


This addresses bug HBASE-2856.
    https://issues.apache.org/jira/browse/HBASE-2856


Diffs (updated)
-----

  /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1181113 
  /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1181113 
  /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1181113 
  /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1181113 

Diff: https://reviews.apache.org/r/2224/diff


Testing
-------

mvn test


Thanks,

Amitanand

;;;","10/Oct/11 23:18;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/#review2497
-----------------------------------------------------------



/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
<https://reviews.apache.org/r/2224/#comment5655>

    convert tab to spaces.



/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java
<https://reviews.apache.org/r/2224/#comment5656>

    Would MAX_MEMSTORE_TS_KEY be a better name ?


- Ted


On 2011-10-10 23:14:05, Amitanand Aiyer wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/2224/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-10-10 23:14:05)
bq.  
bq.  
bq.  Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  address the 2856 issues by writing the memstoreTS to the disk.
bq.  
bq.  version v11 of the patch.
bq.  
bq.  uploading it here for easier review process.
bq.  
bq.  
bq.  This addresses bug HBASE-2856.
bq.      https://issues.apache.org/jira/browse/HBASE-2856
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1181113 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1181113 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1181113 
bq.    /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1181113 
bq.  
bq.  Diff: https://reviews.apache.org/r/2224/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  mvn test
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Amitanand
bq.  
bq.

;;;","10/Oct/11 23:30;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/
-----------------------------------------------------------

(Updated 2011-10-10 23:30:10.189214)


Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.


Changes
-------

fix whitespace issues.


Summary
-------

address the 2856 issues by writing the memstoreTS to the disk.

version v11 of the patch.

uploading it here for easier review process.


This addresses bug HBASE-2856.
    https://issues.apache.org/jira/browse/HBASE-2856


Diffs (updated)
-----

  /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1181113 
  /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1181113 
  /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1181113 
  /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1181113 

Diff: https://reviews.apache.org/r/2224/diff


Testing
-------

mvn test


Thanks,

Amitanand

;;;","10/Oct/11 23:36;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/
-----------------------------------------------------------

(Updated 2011-10-10 23:35:00.552025)


Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.


Changes
-------

rename MAX_MEMSTORE_KEY to MAX_MEMSTORE_TS_KEY


Summary
-------

address the 2856 issues by writing the memstoreTS to the disk.

version v11 of the patch.

uploading it here for easier review process.


This addresses bug HBASE-2856.
    https://issues.apache.org/jira/browse/HBASE-2856


Diffs (updated)
-----

  /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1181113 
  /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1181113 
  /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1181113 
  /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1181113 
  /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1181113 

Diff: https://reviews.apache.org/r/2224/diff


Testing
-------

mvn test


Thanks,

Amitanand

;;;","10/Oct/11 23:40;jiraposter@reviews.apache.org;

bq.  On 2011-10-05 23:23:04, Michael Stack wrote:
bq.  > /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java, line 556
bq.  > <https://reviews.apache.org/r/2224/diff/1/?file=48330#file48330line556>
bq.  >
bq.  >     Is this a good name?  Its come back in from the storefile metadata.  There is no memstore in this context.  No biggie.  Just saying.

 any suggestions?


- Amitanand


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/#review2374
-----------------------------------------------------------


On 2011-10-10 23:35:00, Amitanand Aiyer wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/2224/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-10-10 23:35:00)
bq.  
bq.  
bq.  Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  address the 2856 issues by writing the memstoreTS to the disk.
bq.  
bq.  version v11 of the patch.
bq.  
bq.  uploading it here for easier review process.
bq.  
bq.  
bq.  This addresses bug HBASE-2856.
bq.      https://issues.apache.org/jira/browse/HBASE-2856
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1181113 
bq.    /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1181113 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1181113 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1181113 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1181113 
bq.  
bq.  Diff: https://reviews.apache.org/r/2224/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  mvn test
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Amitanand
bq.  
bq.

;;;","11/Oct/11 01:04;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/#review2499
-----------------------------------------------------------


Amit: 

This is looking really good.

+1 on the changes. You mentioned the TestAcidGuarantees still has some issues-- so +1 pending a resolution of that.

There are still some whitespace issues. Also, please find some inlined comments.

regards,
Kannan


/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
<https://reviews.apache.org/r/2224/#comment5658>

    1) Unfortunate that we have to create these two objects (ByteArrayInputStream & DataInputStream) for every KV. I suppose if this becomes an issue we could add overloads in WriteableUtils.readVLong that directly work on byte arrays instead of requiring a DataInput stream. [Probably not a big deal-- but just something to remember.]
    
    2) Could you combine lines 582 & 583 by using this overload of the constructor:
    
    public ByteArrayInputStream(byte[] buf,
                                int offset,
                                int length)
    
    


- Kannan


On 2011-10-10 23:35:00, Amitanand Aiyer wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/2224/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-10-10 23:35:00)
bq.  
bq.  
bq.  Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  address the 2856 issues by writing the memstoreTS to the disk.
bq.  
bq.  version v11 of the patch.
bq.  
bq.  uploading it here for easier review process.
bq.  
bq.  
bq.  This addresses bug HBASE-2856.
bq.      https://issues.apache.org/jira/browse/HBASE-2856
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1181113 
bq.    /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1181113 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1181113 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1181113 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1181113 
bq.  
bq.  Diff: https://reviews.apache.org/r/2224/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  mvn test
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Amitanand
bq.  
bq.

;;;","11/Oct/11 01:10;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/
-----------------------------------------------------------

(Updated 2011-10-11 01:09:52.483911)


Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.


Changes
-------

fix the TestAcidGuarantees test.

Had changed a condition from > 0 to == HFileWriterV2.KEY_VALUE_VER_WITH_MEMSTORE

make sure it is == 


Summary
-------

address the 2856 issues by writing the memstoreTS to the disk.

version v11 of the patch.

uploading it here for easier review process.


This addresses bug HBASE-2856.
    https://issues.apache.org/jira/browse/HBASE-2856


Diffs (updated)
-----

  /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1181113 
  /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1181113 
  /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1181113 
  /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1181113 
  /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1181113 
  /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1181113 

Diff: https://reviews.apache.org/r/2224/diff


Testing
-------

mvn test


Thanks,

Amitanand

;;;","11/Oct/11 01:24;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/#review2500
-----------------------------------------------------------



/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
<https://reviews.apache.org/r/2224/#comment5659>

    :) new revision looks good.


- Kannan


On 2011-10-11 01:09:52, Amitanand Aiyer wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/2224/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-10-11 01:09:52)
bq.  
bq.  
bq.  Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  address the 2856 issues by writing the memstoreTS to the disk.
bq.  
bq.  version v11 of the patch.
bq.  
bq.  uploading it here for easier review process.
bq.  
bq.  
bq.  This addresses bug HBASE-2856.
bq.      https://issues.apache.org/jira/browse/HBASE-2856
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1181113 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1181113 
bq.    /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1181113 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1181113 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1181113 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1181113 
bq.  
bq.  Diff: https://reviews.apache.org/r/2224/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  mvn test
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Amitanand
bq.  
bq.

;;;","11/Oct/11 01:32;yuzhihong@gmail.com;I think we should tackle TestAcidGuarantees failure(s) next.;;;","15/Oct/11 04:10;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/
-----------------------------------------------------------

(Updated 2011-10-15 04:08:41.977544)


Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.


Changes
-------

Fix 2 more issues that could potentially cause ACID violations

(a) We only used to write maxVersions number of KV's to disk during 
flush. 

  Not all KVs should be counted during this calculation. We shall
ignore all KV's newer than the oldest read point. So the oldest
Scanner can also get enough versions.

(b) move the ignoring newer KV's logic to the StoreFileScanner. That
way, this only returns KV's that are guaranteed to be included in the
search.

There was a condition where if two KVs were written to the same file. Both
identical, but only differ in memstoreTS, then we would skip the duplicate.

It was possible that the first one would be ignored because it has a newer
memstoreTS, and we would never get to see the second one, which might be
the KV we want.


Summary
-------

address the 2856 issues by writing the memstoreTS to the disk.

version v11 of the patch.

uploading it here for easier review process.


This addresses bug HBASE-2856.
    https://issues.apache.org/jira/browse/HBASE-2856


Diffs (updated)
-----

  /pom.xml 1183581 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1183581 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1183581 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1183581 
  /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1183581 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ColumnCount.java 1183581 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java 1183581 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java 1183581 
  /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1183581 
  /src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java 1183581 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1183581 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1183581 
  /src/main/java/org/apache/hadoop/hbase/regionserver/ScanWildcardColumnTracker.java 1183581 
  /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1183581 
  /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1183581 
  /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java 1183581 
  /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1183581 
  /src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 1183581 
  /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1183581 
  /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1183581 
  /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1183581 
  /src/test/java/org/apache/hadoop/hbase/regionserver/TestExplicitColumnTracker.java 1183581 
  /src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java 1183581 
  /src/test/java/org/apache/hadoop/hbase/regionserver/TestScanWildcardColumnTracker.java 1183581 
  /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1183581 

Diff: https://reviews.apache.org/r/2224/diff


Testing
-------

mvn test


Thanks,

Amitanand

;;;","17/Oct/11 04:41;yuzhihong@gmail.com;For patch v7, boolean ignoreCount is added to checkColumn(). I think javadoc for this new parameter should be added to ColumnTracker.java
Javadoc for long readPointToUse of ScanQueryMatcher ctor should be added.
Javadoc for boolean useRWCC of StoreFileScanner ctor and getScannersForStoreFiles() should be added.
There is duplicate code in StoreFileScanner.next(): lines 164 to 172.;;;","17/Oct/11 06:08;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/#review2614
-----------------------------------------------------------


Amit: Did you rebase before uploading the new patch. That, unfortunately, is making it hard to isolate the changes between r6 and r7. Will review tomorrow morning.

But I did read your description about the issues you mentioned. 

Regarding (b)-- we had already discussed in person. That makes sense.

And really nice catch on (a) too!! That is indeed subtle and tricky. Super!!!


- Kannan


On 2011-10-15 04:08:41, Amitanand Aiyer wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/2224/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-10-15 04:08:41)
bq.  
bq.  
bq.  Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  address the 2856 issues by writing the memstoreTS to the disk.
bq.  
bq.  version v11 of the patch.
bq.  
bq.  uploading it here for easier review process.
bq.  
bq.  
bq.  This addresses bug HBASE-2856.
bq.      https://issues.apache.org/jira/browse/HBASE-2856
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    /pom.xml 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ColumnCount.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ScanWildcardColumnTracker.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestExplicitColumnTracker.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestScanWildcardColumnTracker.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1183581 
bq.  
bq.  Diff: https://reviews.apache.org/r/2224/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  mvn test
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Amitanand
bq.  
bq.

;;;","18/Oct/11 20:07;jiraposter@reviews.apache.org;

bq.  On 2011-10-17 06:06:23, Kannan Muthukkaruppan wrote:
bq.  > Amit: Did you rebase before uploading the new patch. That, unfortunately, is making it hard to isolate the changes between r6 and r7. Will review tomorrow morning.
bq.  > 
bq.  > But I did read your description about the issues you mentioned. 
bq.  > 
bq.  > Regarding (b)-- we had already discussed in person. That makes sense.
bq.  > 
bq.  > And really nice catch on (a) too!! That is indeed subtle and tricky. Super!!!
bq.  >

Looks like a lot has changed since the original revision that I based my first patch off.

Please disregard v7.

Let me submit these modifications as a separate diff. I have a sub-jira created for each part.


- Amitanand


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/#review2614
-----------------------------------------------------------


On 2011-10-15 04:08:41, Amitanand Aiyer wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/2224/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-10-15 04:08:41)
bq.  
bq.  
bq.  Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  address the 2856 issues by writing the memstoreTS to the disk.
bq.  
bq.  version v11 of the patch.
bq.  
bq.  uploading it here for easier review process.
bq.  
bq.  
bq.  This addresses bug HBASE-2856.
bq.      https://issues.apache.org/jira/browse/HBASE-2856
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    /pom.xml 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ColumnCount.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ScanWildcardColumnTracker.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestExplicitColumnTracker.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestScanWildcardColumnTracker.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1183581 
bq.  
bq.  Diff: https://reviews.apache.org/r/2224/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  mvn test
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Amitanand
bq.  
bq.

;;;","20/Oct/11 21:18;yuzhihong@gmail.com;Maybe https://reviews.apache.org/r/2483 belongs to this JIRA.
Two new parameters are added to ScanQueryMatcher ctor: readPointToUse and ignoreDuplicates.
HBASE-4536 is going to be integrated soon where we have:
{code}
+  public ScanQueryMatcher(Scan scan, Store.ScanInfo scanInfo,
+      NavigableSet<byte[]> columns, StoreScanner.ScanType scanType,
+      long earliestPutTs) {
{code}
I think any new parameters should be reviewed before deciding where they belong.;;;","27/Oct/11 05:43;jiraposter@reviews.apache.org;

bq.  On 2011-10-17 06:06:23, Kannan Muthukkaruppan wrote:
bq.  > Amit: Did you rebase before uploading the new patch. That, unfortunately, is making it hard to isolate the changes between r6 and r7. Will review tomorrow morning.
bq.  > 
bq.  > But I did read your description about the issues you mentioned. 
bq.  > 
bq.  > Regarding (b)-- we had already discussed in person. That makes sense.
bq.  > 
bq.  > And really nice catch on (a) too!! That is indeed subtle and tricky. Super!!!
bq.  >
bq.  
bq.  Amitanand Aiyer wrote:
bq.      Looks like a lot has changed since the original revision that I based my first patch off.
bq.      
bq.      Please disregard v7.
bq.      
bq.      Let me submit these modifications as a separate diff. I have a sub-jira created for each part.
bq.      
bq.

Seems like we are all basically +1 on this patch upto to some point - could we commit till there? We can add the other changes as separate tasks under the same umbrella task. 

The other diffs are based on this diff... and they are separate issues from what this is addressing (persisting memstoreTS). Its getting confusing if we keep adding to this diff.


- Karthik


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/2224/#review2614
-----------------------------------------------------------


On 2011-10-15 04:08:41, Amitanand Aiyer wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/2224/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-10-15 04:08:41)
bq.  
bq.  
bq.  Review request for Ted Yu, Michael Stack, Kannan Muthukkaruppan, and Karthik Ranganathan.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  address the 2856 issues by writing the memstoreTS to the disk.
bq.  
bq.  version v11 of the patch.
bq.  
bq.  uploading it here for easier review process.
bq.  
bq.  
bq.  This addresses bug HBASE-2856.
bq.      https://issues.apache.org/jira/browse/HBASE-2856
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    /pom.xml 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ColumnCount.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/ScanWildcardColumnTracker.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java 1183581 
bq.    /src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestExplicitColumnTracker.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestScanWildcardColumnTracker.java 1183581 
bq.    /src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 1183581 
bq.  
bq.  Diff: https://reviews.apache.org/r/2224/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  mvn test
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Amitanand
bq.  
bq.

;;;","27/Oct/11 06:46;stack;bq. Seems like we are all basically +1 on this patch upto to some point - could we commit till there?

Sounds good to me.  Can you do this Amit?  (i.e. cut out the bits that have the +1s?)  Good stuff.;;;","10/Nov/11 04:49;dhruba;hi amit, what needs to be done to get this patch to the next step? Thanks.;;;","11/Nov/11 02:56;hudson;Integrated in HBase-TRUNK #2427 (See [https://builds.apache.org/job/HBase-TRUNK/2427/])
    HBASE-3690 Option to Exclude Bulk Import Files from Minor Compaction

Summary:
We ran an incremental scrape with HFileOutputFormat and
encountered major compaction storms. This is caused by the bug in
HBASE-3404. The permanent fix is a little tricky without HBASE-2856. We
realized that a quicker solution for avoiding these compaction storms is
to simply exclude bulk import files from minor compactions and let them
only be handled by time-based major compactions. Add with functionality
along with a config option to enable it.

Rewrote this feature to be done on a per-bulkload basis.

Test Plan:
 - mvn test -Dtest=TestHFileOutputFormat

DiffCamp Revision:

Reviewers: stack, Kannan, JIRA, dhruba

Reviewed By: stack

CC: dhruba, lhofhansl, nspiegelberg, stack

Differential Revision: 357

nspiegelberg : 
Files : 
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat.java
;;;","18/Nov/11 00:08;nspiegelberg;Thanks for the patch Amit! Kannan & I internally reviewed this.  It looks like external reviewers are fine as well. Going to commit.;;;","18/Nov/11 00:19;larsh;v7 still has the change to surefire plugin version 2.8.;;;","18/Nov/11 00:22;yuzhihong@gmail.com;2856-v7.txt didn't apply cleanly on TRUNK.

It would be nice to see HadoopQA run TestAcidGuarantees to completion.;;;","18/Nov/11 00:37;nspiegelberg;@Lars & @Ted: I will commit.  Working directly with Amit on all these issues.  The proper file to use, rebasing, the commit order, making sure unit tests pass, everything.  Hang tight :);;;","18/Nov/11 01:51;amitanand;2856-v8.txt is essentially 2856-v6.txt, after rebasing to the latest HEAD.
;;;","18/Nov/11 02:00;amitanand;2856-v9-all-inclusive includes all the 4 patches for the sub tasks
(i) persist memstorTS to disk
(ii) 4485
(iii) track versions corrrectly
(iv) rename RWCC to MVCC.

This is rebased to the latest trunk HEAD. trunk@1203428

Ran the unit tests on mr. and they seem to pass except TestShell (which seems to fail even without these patches).;;;","18/Nov/11 02:26;larsh;Yeah baby!;;;","18/Nov/11 07:12;hudson;Integrated in HBase-TRUNK #2454 (See [https://builds.apache.org/job/HBase-TRUNK/2454/])
    HBASE-2856 Cross Column Family Read Atomicity

nspiegelberg : 
Files : 
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java
;;;","18/Nov/11 19:34;stack;Whoopdee!  (Good on you Amit).;;;","18/Nov/11 19:44;stack;So remaining acid issues are linked from this one?  hbase-3543, etc.?;;;","19/Nov/11 00:50;jmhsieh;I've been looping TestAcidGuarantee's fro about 6 hours now and it is still chugging along and has not  failed.  I'm going to let it go overnight.  (I believe it used to fail within an hour)  

What are thoughts on backporting this onto the 0.92 branch?   (as a separate issue..);;;","19/Nov/11 05:02;stack;If someone did it in next day or so, I'd be up for having it committed to 0.92 in time for first RC -- there's one issue outstanding at mo (Thats good news this patch is not yet failing -- its pretty amazing really).   If its not done in next day or so, it'll miss the first RC.  Could commit it to the RC that follows (Somehow I don't think the first RC is going to make it out).;;;","19/Nov/11 05:08;larsh;+1 on backporting to 0.92 (I can't volunteer, though).;;;","19/Nov/11 05:38;stack;I took a look.  It seems a little tricky getting this in.  There is more than just this patch involved.  I see RWCC's begin/ends that are in trunk but not in 0.92.  So we'd need more than just this patch.;;;","20/Nov/11 04:07;larsh;Since this mainly clashes with my change from HBASE-4536, I'm probably best qualified to adapt the patch to 0.92. I'm working on a 0.92 patch now.;;;","20/Nov/11 04:10;larsh;I extracted the trunk patch this way: svn diff -r r1203428:r1203468;;;","20/Nov/11 05:53;larsh;Turns out this relies on API introduced in HBASE-4219;;;","20/Nov/11 06:57;larsh;Here's a patch against 0.92. I pulled in the necessary API changes from HBASE-4219 (but not the rest of the functionality).

I could use some help verifying the patch and testing this!

TestAcidGuarantees passed (but only ran it once).;;;","20/Nov/11 12:39;jmhsieh;On trunk, TestAcidGuarantees ran for a solid day and a half (33+ hours) without failing.  

larsh@ I'll loop the 0.92 version and let it run through today and report how it fared around midday monday.;;;","20/Nov/11 12:39;jmhsieh;On trunk, TestAcidGuarantees ran for a solid day and a half (33+ hours) without failing.  

larsh@ I'll loop the 0.92 version and let it run through today and report how it fared around midday monday.;;;","20/Nov/11 19:15;larsh;Thanks Jon!


;;;","20/Nov/11 19:19;larsh;@Nicolas and @Amit, could you review the 0.92 patch? It turned out to be much more manual than I had wished or expected, so it is very possible that I missed something.

(I tried to upload the 0.92 patch to review board for easier verification, but apparently that does not work for branches other than trunk.);;;","21/Nov/11 00:48;jmhsieh;@larsh I posted it for you here.  https://reviews.apache.org/r/2893/

I applied the patch, committed it and generated a git-patch via 'git format-patch HEAD^' which has enough info to find the right branch.;;;","21/Nov/11 04:40;stack;You fellas want this in 0.92?   I want to cut a 0.92 RC.  I have 0.92 tests passing up on jenkins a few times in a row now and all criticals and blockers are in.  Should we wait?  Or should we cut the RC and get this into the second RC (I""m sure there'll be one).;;;","21/Nov/11 04:52;stack;Do all tests pass w/ 0.92 version of this patch in place?;;;","21/Nov/11 05:59;larsh;Re: 0.92, I was going by your comment above
bq. If someone did it in next day or so, I'd be up for having it committed to 0.92 in time for first RC.

It's not entirely clean, yet:

{noformat}
Results :

Failed tests:   testClosing(org.apache.hadoop.hbase.client.TestHCM)
  testFilterAcrossMultipleRegions(org.apache.hadoop.hbase.client.TestFromClientSide): expected:<17576> but was:<28064>
  testForceSplit(org.apache.hadoop.hbase.client.TestAdmin): Scanned more than expected (6000)
  testForceSplitMultiFamily(org.apache.hadoop.hbase.client.TestAdmin): Scanned more than expected (6000)
  testSplitWhileBulkLoadPhase(org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFilesSplitRecovery)
  testGroupOrSplitPresplit(org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFilesSplitRecovery)
  testWholesomeSplit(org.apache.hadoop.hbase.regionserver.TestSplitTransaction)
  testRollback(org.apache.hadoop.hbase.regionserver.TestSplitTransaction)
  testBasicSplit(org.apache.hadoop.hbase.regionserver.TestHRegion)

Tests in error: 
  testShutdownFixupWhenDaughterHasSplit(org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster): test timed out after 300000 milliseconds

Tests run: 1065, Failures: 9, Errors: 1, Skipped: 7
{noformat}

I have no time to look at these tonight, though. But that probably points to another RC.

Would sure be nice if the acid guarantees that HBase claims would be met in 0.92 :)
;;;","21/Nov/11 06:26;larsh;@Jon: Thanks for uploading to RB, btw.;;;","21/Nov/11 15:39;jmhsieh;@lars the 0.92 version or TestAcidGuarantees ran for about 12 hours without problems. 
;;;","21/Nov/11 15:51;nspiegelberg;Something to keep in mind: we have a version of this for our prod branch running on some smaller test clusters, but not yet on our actual prod clusters (since we committed it at the same time you did).  Also, note that between HFileV2 & this, there is no easy downgrade strategy after moving from 90 to 92.  I think that putting this in a 92 RC definitely means a extra testing effort.  However, it's been the last massive outstanding caveat for ACID semantics so it makes sense for 92 inclusion.  I'm sure that other companies consider this a critical issue for their customers, so they would be up for accelerating this testing effort ahead of our schedule. :);;;","21/Nov/11 17:17;stack;Lets get it in.

@Lars TestHCM failed recently for me in 0.92 building locally.   Maybe its not related to this.

@Jon Thanks for running 12 our proofing.;;;","21/Nov/11 18:12;larsh;testClosing is something I added as part of: HBASE-4805, I'll take a look.
Some of the other failing tests in there scare me. :);;;","21/Nov/11 19:32;larsh;This one looks bad:

{noformat}
testFilterAcrossMultipleRegions(org.apache.hadoop.hbase.client.TestFromClientSid
e)  Time elapsed: 12.233 sec  <<< FAILURE!
java.lang.AssertionError: expected:<17576> but was:<28064>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.junit.Assert.assertEquals(Assert.java:456)
	at org.apache.hadoop.hbase.client.TestFromClientSide.assertRowCount(Test
FromClientSide.java:528)
	at org.apache.hadoop.hbase.client.TestFromClientSide.testFilterAcrossMul
tipleRegions(TestFromClientSide.java:436)
{noformat}

Happens only with the 0.92 patch applied. It seems the scanner now finds too many cells.;;;","21/Nov/11 19:47;larsh;I looked through the entire patch again manually, but I can't figure out what would cause this failure.;;;","21/Nov/11 20:24;jmhsieh;On the bulkload operation, the error has something to do with the split point -- in the test I force a split and the resulting error has something to do with the point where the start of the second daughter.

@Lars -- since the original issue is resolved, and since this seems non-trival, maybe this should get move into a new issue?;;;","21/Nov/11 20:55;larsh;Created HBASE-4838;;;","23/Nov/11 01:00;larsh;Please see new updates in HBASE-4838.;;;"
broken tests on trunk,HBASE-2854,12469685,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,ryanobjc,ryanobjc,20/Jul/10 06:58,20/Nov/15 12:41,14/Jul/23 06:06,20/Jul/10 07:00,0.89.20100621,,,,,,,,,,,0.90.0,,,,,,,0,,,"the following tests are broken:


>>><<< org.apache.hadoop.hbase.regionserver.TestStore.testIncrementColumnValue_ICVDuringFlush

All of the TestQueryMatcher tests.
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/10 17:03;pranavkhaitan;TimerangeAlltime.patch;https://issues.apache.org/jira/secure/attachment/12449946/TimerangeAlltime.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26499,,,,,Fri Nov 20 12:41:03 UTC 2015,,,,,,,,,,"0|i0hjjb:",100427,,,,,,,,,,,,,,,,,,,,,"20/Jul/10 07:00;ryanobjc;committed;;;","20/Jul/10 17:03;pranavkhaitan;The tests were broken because the Timestamp was not being set and it used to take the default value which is Long.Max_Value. The default timerange is 0 to Long.Max_Value (exclusive) so the default timestamp will never exist within default timerange. I created a patch which uses the existing extra variable to handle this case. I don't think this change is required but some tests in future which do not set the timestamp may fail again if this is not used. 

Note: Even after this patch, there is no way we can create a finite timerange which includes Long.Max_Value .;;;","20/Jul/10 18:04;stack;@Pranav Maybe its ok that LATEST_TIMESTAMP breaks things?  That way we find the places where its coming in unexpected?;;;","20/Jul/10 19:09;kannanm;@stack: Agreed. I think the code fix isn't necessary.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestLoadIncrementalHFiles fails on TRUNK,HBASE-2853,12469682,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,20/Jul/10 05:30,20/Nov/15 12:41,14/Jul/23 06:06,20/Jul/10 05:32,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Its writing KVs w/ LATEST_TIMESTAMP. Here's fix:

{code}
pynchon-56:hbase stack$ git diff
diff --git a/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java b/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java
index d5374d2..e02d11a 100644
--- a/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java
+++ b/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java
@@ -175,10 +175,11 @@ public class TestLoadIncrementalHFiles {
   {
     HFile.Writer writer = new HFile.Writer(fs, path, BLOCKSIZE, COMPRESSION,
         KeyValue.KEY_COMPARATOR);
+    long now = System.currentTimeMillis();
     try {
       // subtract 2 since iterateOnSplits doesn't include boundary keys
       for (byte[] key : Bytes.iterateOnSplits(startKey, endKey, numRows-2)) {
-        KeyValue kv = new KeyValue(key, family, qualifier, key);
+        KeyValue kv = new KeyValue(key, family, qualifier, now, key);
         writer.append(kv);
       }
     } finally {
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26498,,,,,Fri Nov 20 12:41:48 UTC 2015,,,,,,,,,,"0|i0hjj3:",100426,,,,,,,,,,,,,,,,,,,,,"20/Jul/10 05:32;stack;Committed.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bloom filter NPE,HBASE-2852,12469674,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,pranavkhaitan,pranavkhaitan,pranavkhaitan,20/Jul/10 01:02,20/Nov/15 12:41,14/Jul/23 06:06,27/Jul/10 01:32,,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"When a rowcol Bloom filter is being used and the user submits a query for all columns, a null pointer exception is thrown. This is because there is no checking if columns have been specified or not.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/10 01:29;streamy;HBASE-2852-BloomNPEV3.diff;https://issues.apache.org/jira/secure/attachment/12450545/HBASE-2852-BloomNPEV3.diff",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26497,,,,,Fri Nov 20 12:41:49 UTC 2015,,,,,,,,,,"0|i0hjiv:",100425,,,,,,,,,,,,,,,,,,,,,"20/Jul/10 01:13;hbasereviewboard;Message from: ""Pranav Khaitan"" <pranavkhaitan@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/336/
-----------------------------------------------------------

Review request for hbase, Nicolas, Karthik Ranganathan, and Kannan Muthukkaruppan.


Summary
-------

Fixed bloom filter code in StoreFile


This addresses bug HBASE-2852.
    http://issues.apache.org/jira/browse/HBASE-2852


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 965615 
  trunk/src/test/java/org/apache/hadoop/hbase/client/TestBloomFilter.java PRE-CREATION 

Diff: http://review.hbase.org/r/336/diff


Testing
-------

JUnit tests added


Thanks,

Pranav


;;;","20/Jul/10 01:47;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/336/#review425
-----------------------------------------------------------



trunk/src/test/java/org/apache/hadoop/hbase/client/TestBloomFilter.java
<http://review.hbase.org/r/336/#comment1794>

    can you rework this test not to use TEST_UTIL and starting up a mini-hbase cluster?  
    
    For alternate options, try doing what HRegion does, which is to instantiate a HRegion then use the HRegion API calls instead. 


- Ryan



;;;","20/Jul/10 18:24;hbasereviewboard;Message from: ""Pranav Khaitan"" <pranavkhaitan@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/336/
-----------------------------------------------------------

(Updated 2010-07-20 11:19:26.615795)


Review request for hbase, Nicolas, Karthik Ranganathan, and Kannan Muthukkaruppan.


Changes
-------

Moved the test to TestHRegion


Summary
-------

Fixed bloom filter code in StoreFile


This addresses bug HBASE-2852.
    http://issues.apache.org/jira/browse/HBASE-2852


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 965615 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java 965615 

Diff: http://review.hbase.org/r/336/diff


Testing
-------

JUnit tests added


Thanks,

Pranav


;;;","22/Jul/10 05:35;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/336/#review445
-----------------------------------------------------------



trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
<http://review.hbase.org/r/336/#comment1872>

    you should use initHRegion in this class instead of repeating the boilerplate.



trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
<http://review.hbase.org/r/336/#comment1871>

    this might not be necessary, since flushcache() returns after the file has been loaded into HRegion/Stores.


- Ryan



;;;","22/Jul/10 06:13;hbasereviewboard;Message from: ""Pranav Khaitan"" <pranavkhaitan@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/336/
-----------------------------------------------------------

(Updated 2010-07-21 23:04:10.126963)


Review request for hbase, Nicolas, Karthik Ranganathan, and Kannan Muthukkaruppan.


Summary
-------

Fixed bloom filter code in StoreFile


This addresses bug HBASE-2852.
    http://issues.apache.org/jira/browse/HBASE-2852


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 965615 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java 965615 

Diff: http://review.hbase.org/r/336/diff


Testing
-------

JUnit tests added


Thanks,

Pranav


;;;","25/Jul/10 07:51;pranavkhaitan;Thanks for going through the patch. I had updated the patch based on the
latest suggestions. Please let me know if any further changes are required.

Regards,
Pranav





;;;","27/Jul/10 01:29;streamy;Final patch from pranav over on reviewboard to be committed;;;","27/Jul/10 01:30;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/336/#review492
-----------------------------------------------------------

Ship it!


Just a simple null check and added test.  LGTM

- Jonathan



;;;","27/Jul/10 01:32;streamy;Committed to trunk.  Thanks pranav.;;;","27/Jul/10 01:38;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>


bq.  On 2010-07-26 18:25:37, Jonathan Gray wrote:
bq.  > Just a simple null check and added test.  LGTM

and all of ryan's comments have been addressed.  thanks pranav.


- Jonathan


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/336/#review492
-----------------------------------------------------------



;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove testDynamicBloom() unit test,HBASE-2851,12469668,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,nspiegelberg,nspiegelberg,nspiegelberg,19/Jul/10 22:40,20/Nov/15 12:40,14/Jul/23 06:06,19/Jul/10 23:34,,,,,,,,,,,,0.90.0,,,,,,,0,,,"HBASE-2843 added TestByteBloomFilter back, however the reason for removing it was spotty test results for testDynamicBloom(), which tests a class that we don't use anyways.  Need to remove the offending test.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/10 22:42;nspiegelberg;HBASE-2851.patch;https://issues.apache.org/jira/secure/attachment/12449897/HBASE-2851.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26496,Reviewed,,,,Fri Nov 20 12:40:33 UTC 2015,,,,,,,,,,"0|i0hjin:",100424,,,,,,,,,,,,,,,,,,,,,"19/Jul/10 23:34;stack;Thanks for the patch  Nicolas;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase clients cannot recover when their ZooKeeper session becomes invalid,HBASE-2849,12469634,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,tsuna,stack,stack,19/Jul/10 16:52,20/Nov/15 12:40,14/Jul/23 06:06,24/Jul/10 05:19,0.89.20100621,,,,,,,,,,,0.90.0,,Client,,,,,1,,,"Someone made mention of this loop last week but I don't think I filed an issue.  Here is another instance, again from a secret hbase admirer:

""It seems that when Zookeeper dies and restarts, all client applications need to be restarted too. I just restarted HBase in non-distributed mode (which includes a ZK) and now my application can't reconnect to ZK unless I restart it too.  I'm stuck in this loop:

{code}
2010-07-19 00:13:05,725 INFO org.apache.zookeeper.server.NIOServerCnxn:
  Closed socket connection for client /127.0.0.1:55153 (no session established for client)
2010-07-19 00:13:07,052 INFO org.apache.zookeeper.server.NIOServerCnxn:
  Accepted socket connection from /127.0.0.1:55154
2010-07-19 00:13:07,053 INFO org.apache.zookeeper.server.NIOServerCnxn:
  Refusing session request for client /127.0.0.1:55154 as it has seen zxid 0xf5 our last zxid is 0xd7
  client must try another server
{code}
""",,larsfrancke,tsuna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/10 03:58;tsuna;0001-HBASE-2849-Have-HBase-clients-recover-from-ZooKeeper.patch;https://issues.apache.org/jira/secure/attachment/12450384/0001-HBASE-2849-Have-HBase-clients-recover-from-ZooKeeper.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26495,Reviewed,,,,Fri Nov 20 12:40:34 UTC 2015,,,,,,,,,,"0|i0hji7:",100422,,,,,,,,,,,,"reliability, zookeeper",,,,,,,,,"19/Jul/10 17:02;tsuna;Reformatting a little bit.;;;","19/Jul/10 17:32;streamy;Client+ZK interaction just got a makeover in the master rewrite branch.  It now handles master failovers properly and re-initializes zk connections but I'm not sure it will sustain a zk restart.  Any chance of a reproducing unit test?;;;","23/Jul/10 23:01;tsuna;http://hadoop.apache.org/zookeeper/docs/r3.3.1/api/org/apache/zookeeper/ZooKeeper.html
bq. If for some reason, the client fails to send heart beats to the server for a prolonged period of time (exceeding the sessionTimeout value, for instance), the server will expire the session, and the session ID will become invalid. The client object will no longer be usable. To make ZooKeeper API calls, the application must create a new client object.

So apparently, a new {{ZooKeeper}} object must be created when the session becomes invalid.  This sounds like a bad API, not sure why they did it this way.  In HBase's source code, it seems that the only thing that creates a {{ZooKeeper}} instance is in {{ZooKeeperWrapper#reconnectToZk}}.  This method, although it's public, is only called from 3 other methods in that class: the constructor, {{exists}} and {{deleteUnassignedRegion}}.  The latter, {{deleteUnassignedRegion}}, is only used by the master.  The former, {{exists}}, is only called from the following locations:
* {{ZKUnassignedWatcher}}'s constructor.  This is only used in the master.
* {{RSZookeeperUpdater#startRegionCloseEvent}}.  This is only used in the region server.
* {{ZooKeeperWrapper#createOrUpdateUnassignedRegion}}.  This is only used by the master's {{RegionManager}}.
* {{ZooKeeperWrapper#createUnassignedRegion}} and {{ZooKeeperWrapper#updateUnassignedRegion}}.  Those two methods, even though they're public, are only called from {{ZooKeeperWrapper#createOrUpdateUnassignedRegion}}, which itself is only used by the master's {{RegionManager}}.

In other words, for someone writing an HBase application, only a single {{ZooKeeper}} instance gets created when the {{ZooKeeperWrapper}} is instantiated.  Any failure that causes the client's session to become invalid will is unrecoverable with the current code and the client has to be killed and restarted.

Jonathan, is the work being done for the master rewrite branch going to address this issue?  Bear in mind that here I'm concerned about HBase *client* applications.;;;","23/Jul/10 23:06;streamy;It would be possible for the client to try to reconnect after being expired.  It's not built-in to the way I have it now but it's possible to add it.;;;","24/Jul/10 03:58;tsuna;Patch that fixes the issue.  Actually there was some logic I didn't notice earlier in {{HConnectionManager}} to attempt to deal with ZK failures and reconnect when needed, but the code wasn't doing the right thing and didn't work when there was a disconnection between the HBase client and the ZK quorum.  So the patch is rather simple and consists in fixing the existing logic in {{HConnectionManager.ClientZKWatcher}}.

I tested this by starting a long running HBase application, killing the whole ZooKeeper ensemble and restarting it.  The application experiences a hiccup while ZK is unavailable and is able to recover automatically soon after the ZK quorum is back online.  Someone else is more than welcome to write a unit test that simulates this scenario if they feel like it.;;;","24/Jul/10 05:19;stack;Committed.  Thats for the 'duh' patch Benôit.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make rest server be same as thrift and avro servers,HBASE-2846,12469553,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,18/Jul/10 05:18,20/Nov/15 12:43,14/Jul/23 06:06,18/Jul/10 05:25,,,,,,,,,,,,0.90.0,,,,,,,0,,,,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/10 05:23;stack;rest.txt;https://issues.apache.org/jira/secure/attachment/12449782/rest.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26492,,,,,Fri Nov 20 12:43:04 UTC 2015,,,,,,,,,,"0|i0hjhz:",100421,,,,,,,,,,,,,,,,,,,,,"18/Jul/10 05:23;stack;Add 'rest' to the listing when you type 'bin/hbase'.  Make its usage output same as avro and thrift.  Make it take start/stop like other servers.;;;","18/Jul/10 05:25;stack;Committed.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Re-add bloomfilter test over-zealously removed by HBASE-2625 ,HBASE-2843,12469511,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,17/Jul/10 00:34,20/Nov/15 12:40,14/Jul/23 06:06,17/Jul/10 00:46,,,,,,,,,,,,0.90.0,,,,,,,0,,,I removed TestByteBloomFilter when I shouldn't have when I removed all related to unused dynamic bloomfilters.  Readd.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26491,,,,,Fri Nov 20 12:40:59 UTC 2015,,,,,,,,,,"0|i0hjhb:",100418,,,,,,,,,,,,,,,,,,,,,"17/Jul/10 00:46;stack;Committed (Sorry for removing it in first place Nicolas).;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the final remnants of the old Get code - the query matchers and other helper classes,HBASE-2840,12469506,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,ryanobjc,ryanobjc,16/Jul/10 23:12,20/Nov/15 12:41,14/Jul/23 06:06,16/Jul/10 23:32,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"even though we no longer call the old Get code, there are QueryMatchers and other accessory classes which used to be necessary but are no longer.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26490,,,,,Fri Nov 20 12:41:23 UTC 2015,,,,,,,,,,"0|i0hjgv:",100416,,,,,,,,,,,,,,,,,,,,,"16/Jul/10 23:23;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/332/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Remove the final remnants of the old Get code - the query matchers and other helper classes


This addresses bug HBASE-2840.
    http://issues.apache.org/jira/browse/HBASE-2840


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java 35b03a1 
  src/main/java/org/apache/hadoop/hbase/regionserver/DeleteCompare.java 11da530 
  src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java 37bbee0 
  src/main/java/org/apache/hadoop/hbase/regionserver/GetClosestRowBeforeTracker.java 52fbbd3 
  src/main/java/org/apache/hadoop/hbase/regionserver/GetDeleteTracker.java 8e68d65 
  src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java 8f61df3 
  src/main/java/org/apache/hadoop/hbase/regionserver/QueryMatcher.java a8dcfab 
  src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java c805bfa 
  src/main/java/org/apache/hadoop/hbase/regionserver/ScanWildcardColumnTracker.java a32d6e3 
  src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileGetScan.java 4d2e742 
  src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java e49a0f0 
  src/main/java/org/apache/hadoop/hbase/regionserver/WildcardColumnTracker.java 0e673c4 
  src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 8fc0766 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestDeleteCompare.java 6ea4a1c 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestExplicitColumnTracker.java 8c38a42 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestGetDeleteTracker.java c8d7680 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java 96b51e7 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestQueryMatcher.java 3670450 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestScanWildcardColumnTracker.java 2ff5a25 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestWildcardColumnTracker.java 195744a 

Diff: http://review.hbase.org/r/332/diff


Testing
-------


Thanks,

Ryan


;;;","16/Jul/10 23:29;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/332/#review421
-----------------------------------------------------------

Ship it!


+1

- stack



;;;","16/Jul/10 23:32;ryanobjc;committed!  Get code, we hardly knew ye.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBASE-2613 broke rolling upgrades,HBASE-2837,12469363,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,15/Jul/10 17:46,12/Oct/12 06:16,14/Jul/23 06:06,15/Jul/10 18:32,0.20.5,,,,,,,,,,,0.20.6,,,,,,,0,,,"HBASE-2613 broke rolling upgrades because it removed MSG_CALL_SERVER_STARTUP from the HMsg.Type enum. Even if it won't be used anywhere, we should put it back so that people can do rolling upgrades.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26489,,,,,Thu Jul 15 18:32:29 UTC 2010,,,,,,,,,,"0|i08sc7:",49191,,,,,,,,,,,,,,,,,,,,,"15/Jul/10 18:32;jdcryans;Committed this to branch in HMsg:

{code}
+
+    /** UNUSED, it's kept in 0.20 only for rolling upgrades */
+    MSG_CALL_SERVER_STARTUP,
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix '$bin' path duplication in setup scripts,HBASE-2831,12469133,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,nspiegelberg,nspiegelberg,nspiegelberg,12/Jul/10 23:35,20/Nov/15 12:40,14/Jul/23 06:06,13/Jul/10 00:25,0.89.20100621,,,,,,,,,,,0.90.0,,scripts,,,,,0,,,"I have my bash environment setup to echo absolute pathnames when a relative one is specified in 'cd'. This caused problems with all the Hadoop bash scripts because the script accidentally sets the $bin variable twice in this setup. (e.g. would set $bin=""/path/bin/hbase\n/path/bin/hbase"")

This jira is for HBase scripts. I filed a separate jira for HDFS scripts, which share the same pattern.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HDFS-1281,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/10 23:38;nspiegelberg;HBASE-2831.patch;https://issues.apache.org/jira/secure/attachment/12449305/HBASE-2831.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26487,Reviewed,,,,Fri Nov 20 12:40:59 UTC 2015,,,,,,,,,,"0|i0hjfb:",100409,,,,,,,,,,,,,,,,,,,,,"13/Jul/10 00:25;stack;Committed to TRUNK.  Not applying to 0.20 branch.  I don't think you care Nicolas and its not an important fix.  ;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTable unnecessarily coupled with HMaster,HBASE-2828,12468963,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,nspiegelberg,nspiegelberg,nspiegelberg,09/Jul/10 23:52,20/Nov/15 12:43,14/Jul/23 06:06,03/Nov/10 21:22,0.90.0,,,,,,,,,,,0.90.0,,Client,,,,,0,,,"HTable constructor calls ""getCurrentNrHRS()"" to get the region server count for thread pool creation.  This code calls HBaseAdmin.getClusterStatus() [aka: the HMaster] to get the server count.  This information can be scraped from counting the ZooKeeper /hbase/rs/--- ZNodes.  Need to remove unnecessary master queries when ZooKeeper can do the same job.",,khemani,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/10 19:40;streamy;HBASE-2828-0.90.patch;https://issues.apache.org/jira/secure/attachment/12458758/HBASE-2828-0.90.patch","13/Jul/10 05:15;nspiegelberg;HBASE-2828.a.patch;https://issues.apache.org/jira/secure/attachment/12449320/HBASE-2828.a.patch","13/Jul/10 01:23;nspiegelberg;HBASE-2828.patch;https://issues.apache.org/jira/secure/attachment/12449312/HBASE-2828.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26486,Reviewed,,,,Fri Nov 20 12:43:03 UTC 2015,,,,,,,,,,"0|i0hjen:",100406,,,,,,,,,,,,,,,,,,,,,"13/Jul/10 01:23;nspiegelberg;I just kept this simple and used existing API calls to grab the zkWrapper, get all RS nodes, and call size().  It would be nice if ZK provided a getChildrenCount(), so it didn't have to list all the paths, but I assumed this shouldn't be a significant performance concern, even for larger clusters.

Note that I also used this opportunity to clean up the class header documentation for HTable & HBaseAdmin.  Feel free to ignore those changes if you have your own doc update.;;;","13/Jul/10 05:15;nspiegelberg;Found the API I needed.   zk.exists() returns a Stat class, which contains the children count.;;;","13/Jul/10 19:22;stack;Committed. Thank you for the patch Nicolas. Applied TRUNK.;;;","03/Nov/10 19:38;streamy;This is in some 0.89 branches but was left out from 0.90 (I guess during master rewrite commit).;;;","03/Nov/10 19:40;streamy;Looks like the documentation changes made it, just not the HTable.getCurrentNrHRS().  Patch changes implementation of that method (same principle as original patch but slightly different0.;;;","03/Nov/10 21:22;stack;I committed your patch Jon.;;;","03/Nov/10 21:28;nspiegelberg;can we add some explicit comment in there about purposefully not going to Master for the RS count.  I wouldn't want a 3rd occurrence of this...;;;","03/Nov/10 22:48;stack;done;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scans respect row locks,HBASE-2825,12468934,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,ryanobjc,bernardos,bernardos,09/Jul/10 15:48,20/Nov/15 12:41,14/Jul/23 06:06,01/Oct/10 22:46,,,,,,,,,,,,0.90.0,,documentation,,,,,0,,,"In the javadoc Package org.apache.hadoop.hbase.client Description it states that ""Scans (currently) operate without respect for row locks."" I think that since 0.20.4 and HBASE-2248 this is no longer the case.",,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26484,,,,,Fri Nov 20 12:41:00 UTC 2015,,,,,,,,,,"0|i0hjef:",100405,,,,,,,,,,,,,,,,,,,,,"13/Jul/10 20:30;stack;Bringing into 0.90.0;;;","01/Sep/10 22:22;stack;Can you make a patch for the javadoc so we can close this Ryan? You know how to say it best!;;;","01/Sep/10 22:35;streamy;Wait, scans respect row locks?;;;","01/Sep/10 22:35;streamy;(i don't see that as being the same as providing isolation to concurrent writes);;;","01/Sep/10 22:49;ryanobjc;of course not, no read should 'respect' a row lock.  Huge performance hit.

This is just fixing the javadoc, removing that terrible 'respect row lock' language;;;","01/Sep/10 22:57;streamy;So even single-row gets do not use the row lock now?  This is a change in semantics that should be documented if so.;;;","01/Oct/10 22:46;ryanobjc;tweaked javadoc;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Entire Row Deletes not stored in Row+Col Bloom,HBASE-2823,12468848,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,espr1t,nspiegelberg,nspiegelberg,08/Jul/10 20:25,20/Nov/15 12:41,14/Jul/23 06:06,05/Aug/10 05:17,0.90.0,,,,,,,,,,,0.90.0,,Filters,regionserver,,,,0,,,"If the user issues a Row Delete on an family with Row+Col blooms, that information is not currently detected by shouldSeek().  Possible known solutions are:

1. adding Row as Bloom Filter Key on Row Delete, shouldSeek() should do both a Row & Row+Col query for Row+Col filters.
2. keep delete information in a separate storage element.

#1 seems like the best solution, but need to investigate further and fix this problem.",,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/10 05:15;stack;HBASE-2823.patch;https://issues.apache.org/jira/secure/attachment/12451305/HBASE-2823.patch","03/Aug/10 20:05;espr1t;HBASE-2823.patch;https://issues.apache.org/jira/secure/attachment/12451150/HBASE-2823.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26483,Reviewed,,,,Fri Nov 20 12:41:49 UTC 2015,,,,,,,,,,"0|i0hjdz:",100403,,,,,,,,,,,,hbase,,,,,,,,,"08/Jul/10 20:38;ryanobjc;Deleting an entire row is just entering a delete family in to every
family, so it would make sense that these KeyValues would appear in
both the row and row+col blooms if they exist.


On Thu, Jul 8, 2010 at 1:26 PM, Nicolas Spiegelberg (JIRA)

;;;","08/Jul/10 20:40;streamy;I think I like #1.  Seems like the most minimally invasive way to do this.  There's virtually no impact on memory consumption, hfile size/complexity, etc... The cost will be doing two bloom checks, however bloom operations are constant time so I suspect the impact to be negligible.;;;","08/Jul/10 20:43;streamy;This issue applies to DeleteFamily instances.  (like ryan says, we have no delete row)

The issue is that a DeleteFamily applies to all columns.  In a row+col bloom, you may not include a file which includes a DeleteFamily because that file doesn't contain any instances of the column you are looking for.  To ensure that file would be included, we should include a key into the row+col bloom of just the row, and when checking the blooms, we should check each for row+col and for row.  Note, this only applies to row+col, and in this case it is _only_ the DeleteFamilys that should add an additional key to the bloom.;;;","09/Jul/10 22:57;streamy;Added Alexander as a contributor and assigning to him.;;;","29/Jul/10 18:18;hbasereviewboard;Message from: ""Alexander Georgiev"" <espr1t.net@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/426/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

When a Delete Row is issued on a row with row+col bloom filter, some of the columns might not be deleted. Since a Delete Row is just Delete Family applied to all columns, if a file doesn't contain the column we are searching for it might end up unaffected. In order to ensure the file will be included, the row together with row+col are added in the bloom. Then shouldSeek() checks both row and row+col if the bloom is row+col (BloomType.ROWCOL). That adds additional false positives, which are taken into account with dividing the error rate the user requires by two.


This addresses bug HBASE-2823.
    http://issues.apache.org/jira/browse/HBASE-2823


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 979864 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java 979864 

Diff: http://review.cloudera.org/r/426/diff


Testing
-------

Added new test that checks this in TestHRegion.java.
Dumped the contents of the StoreFile in order to ensure that the bloom filter has row as a value when using ROWCOL blooms.


Thanks,

Alexander


;;;","29/Jul/10 18:31;hbasereviewboard;Message from: ""Nicolas"" <nspiegelberg@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/426/#review577
-----------------------------------------------------------

Ship it!


looks good


trunk/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
<http://review.cloudera.org/r/426/#comment2220>

    inverted logic not necessary here. it might be more straightforward to say: 
    
    if(type == ROWCOL) {
     // comments + special stuff
    } else {
     // normal stuff.
    }



trunk/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
<http://review.cloudera.org/r/426/#comment2219>

    small typo: s/and/an/


- Nicolas



;;;","29/Jul/10 18:54;espr1t;When a Delete Row is issued on a row with row+col bloom filter, some of the columns might not be deleted. Since a Delete Row is just Delete Family applied to all columns, if a file doesn't contain the column we are searching for it might end up unaffected. In order to ensure the file will be included, the row together with row+col are added in the bloom. Then shouldSeek() checks both row and row+col if the bloom is row+col (BloomType.ROWCOL). That adds additional false positives, which are taken into account with dividing the error rate the user requires by two.

Added new test that checks this in TestHRegion.java.
Dumped the contents of the StoreFile in order to ensure that the bloom filter has row as a value when using ROWCOL blooms.;;;","03/Aug/10 20:15;hbasereviewboard;Message from: ""Alexander Georgiev"" <espr1t.net@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/426/
-----------------------------------------------------------

(Updated 2010-08-03 13:04:02.245493)


Review request for hbase.


Changes
-------

Applied corrections suggested by Nicolas.


Summary
-------

When a Delete Row is issued on a row with row+col bloom filter, some of the columns might not be deleted. Since a Delete Row is just Delete Family applied to all columns, if a file doesn't contain the column we are searching for it might end up unaffected. In order to ensure the file will be included, the row together with row+col are added in the bloom. Then shouldSeek() checks both row and row+col if the bloom is row+col (BloomType.ROWCOL). That adds additional false positives, which are taken into account with dividing the error rate the user requires by two.


This addresses bug HBASE-2823.
    http://issues.apache.org/jira/browse/HBASE-2823


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java 979864 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java 979864 

Diff: http://review.cloudera.org/r/426/diff


Testing
-------

Added new test that checks this in TestHRegion.java.
Dumped the contents of the StoreFile in order to ensure that the bloom filter has row as a value when using ROWCOL blooms.


Thanks,

Alexander


;;;","05/Aug/10 05:15;stack;Uploading v2 patch from review.hbase.org, the patch I'm about to apply;;;","05/Aug/10 05:17;stack;Committed.  Thanks for the patch Alexander Georgiev.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hbck throws an error if hbase root dir isn't on default FS,HBASE-2820,12468689,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,tlipcon,tlipcon,tlipcon,07/Jul/10 04:58,20/Nov/15 12:43,14/Jul/23 06:06,28/Jul/10 23:15,0.90.0,,,,,,,,,,,0.90.0,,scripts,,,,,0,,,"""Wrong FS"" exception gets thrown since we construct the default FS instead of the one from rootdir",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/10 21:17;tlipcon;hbase-2820.txt;https://issues.apache.org/jira/secure/attachment/12450751/hbase-2820.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26482,Reviewed,,,,Fri Nov 20 12:43:31 UTC 2015,,,,,,,,,,"0|i0hjdj:",100401,,,,,,,,,,,,,,,,,,,,,"28/Jul/10 21:17;tlipcon;Trivial patch. We need unit tests throughout for hbck, but that's a separate problem.;;;","28/Jul/10 21:33;stack;+1;;;","28/Jul/10 23:15;tlipcon;Committed to trunk.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
not able to run the test suite in background because TestShell gets suspended on tty output,HBASE-2815,12468659,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kovyrin,kannanm,kannanm,06/Jul/10 18:13,20/Nov/15 12:42,14/Jul/23 06:06,25/Jul/10 03:50,,,,,,,,,,,,0.90.0,,shell,,,,,0,,,"Can't run the test suite in background. Problem seems to be due to TestShell.

This works fine:
{code}
% mvn test -Dtest=TestShell -Dtest.output=true
{code}

But:
{code}
% mvn test -Dtest=TestShell -Dtest.output=true & 
or,
% mvn test -Dtest=TestShell -Dtest.output=true >& test.log &
{code}

causes test to hang, and eventually timeout after 3600 seconds.

The process is reported as being suspended on tty output.

[3]  + Suspended (tty output)        mvn test -Dtest=TestShell -Dtest.output=true
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/10 07:05;kovyrin;HBASE-2815.patch;https://issues.apache.org/jira/secure/attachment/12450137/HBASE-2815.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26480,Reviewed,,,,Fri Nov 20 12:42:30 UTC 2015,,,,,,,,,,"0|i0hjd3:",100399,Fixed. Please review and apply the patch.,,,,,,,,,,,,,,,,,,,,"07/Jul/10 12:52;stack;Any chance of your taking a look at this one?;;;","07/Jul/10 20:23;kovyrin;Yes, I'll take a look at the issue this week.;;;","19/Jul/10 02:49;kannanm;Alexey: did you get a chance to look at this? ;;;","22/Jul/10 03:36;kovyrin;I've got some serious back problems and was pretty much away from the computer for a few weeks. Now I'm back and building the trunk to figure out this problem.;;;","22/Jul/10 03:52;stack;@Alexey: Look after your back first!   We'll survive meantime.;;;","22/Jul/10 07:05;kovyrin;Fixed. The problem was in the call we used to determine terminal width.;;;","22/Jul/10 07:33;kannanm;Nice! Thanks Alexey. And take are of your back :);;;","22/Jul/10 13:46;streamy;@Kannan, can you confirm this fixes your problem?  Will commit after you give +1.  Otherwise patch looks good, thanks Alexey!;;;","24/Jul/10 07:27;kannanm;@Jonathan: +1. It fixes the issue. Please go ahead and commit.;;;","25/Jul/10 03:50;stack;Committed.  Thanks for the fix Alexey.  Thanks for reviewing and testing Kannan.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable 'table' fails to complete frustrating my ability to test easily,HBASE-2812,12468502,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,spullara,spullara,03/Jul/10 22:23,11/Jun/22 23:55,14/Jul/23 06:06,19/Jul/14 01:03,0.20.6,0.89.20100621,,,,,,,,,,,,master,,,,,3,,,"I see this in the client after it gives up:

hbase(main):006:0> disable 'test_schema'

ERROR: org.apache.hadoop.hbase.RegionException: Retries exhausted, it took too long to wait for the table test_schema to be disabled.

Here is some help for this command:
          Disable the named table: e.g. ""hbase> disable 't1'""

and this in the server log, a set of about 5 reports it is closing per disable call:

2010-07-03 15:19:47,554 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing unserved regions
2010-07-03 15:19:47,554 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing regions currently being served
2010-07-03 15:19:47,555 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081. to setClosing list
2010-07-03 15:19:47,576 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:47,576 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:48,567 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing unserved regions
2010-07-03 15:19:48,567 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing regions currently being served
2010-07-03 15:19:48,568 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081. to setClosing list
2010-07-03 15:19:48,577 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:48,578 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:49,580 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing unserved regions
2010-07-03 15:19:49,580 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing regions currently being served
2010-07-03 15:19:49,581 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081. to setClosing list
2010-07-03 15:19:50,580 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:50,581 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:50,592 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing unserved regions
2010-07-03 15:19:50,592 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing regions currently being served
2010-07-03 15:19:50,593 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081. to setClosing list
2010-07-03 15:19:51,581 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:51,581 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:52,605 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing unserved regions
2010-07-03 15:19:52,605 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing regions currently being served
2010-07-03 15:19:52,606 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081. to setClosing list
2010-07-03 15:19:52,703 INFO org.apache.hadoop.hbase.master.ServerManager: 1 region servers, 0 dead, average load 3.0
2010-07-03 15:19:52,863 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scanning meta region {server: 192.168.2.1:54389, regionname: -ROOT-,,0.70236052, startKey: <>}
2010-07-03 15:19:52,867 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scan of 1 row(s) of meta region {server: 192.168.2.1:54389, regionname: -ROOT-,,0.70236052, startKey: <>} complete
2010-07-03 15:19:53,585 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:53,585 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:53,671 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Cache Stats: Sizes: Total=1.6292953MB (1708440), Free=196.70822MB (206263512), Max=198.33751MB (207971952), Counts: Blocks=2, Access=52, Hit=50, Miss=2, Evictions=0, Evicted=0, Ratios: Hit Ratio=96.15384340286255%, Miss Ratio=3.8461539894342422%, Evicted/Run=NaN
2010-07-03 15:19:54,617 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing unserved regions
2010-07-03 15:19:54,617 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing regions currently being served
2010-07-03 15:19:54,618 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081. to setClosing list
2010-07-03 15:19:54,821 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scanning meta region {server: 192.168.2.1:54389, regionname: .META.,,1.1028785192, startKey: <>}
2010-07-03 15:19:54,825 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scan of 2 row(s) of meta region {server: 192.168.2.1:54389, regionname: .META.,,1.1028785192, startKey: <>} complete
2010-07-03 15:19:54,825 INFO org.apache.hadoop.hbase.master.BaseScanner: All 1 .META. region(s) scanned
2010-07-03 15:19:55,589 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:55,589 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:58,629 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing unserved regions
2010-07-03 15:19:58,629 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing regions currently being served
2010-07-03 15:19:58,630 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081. to setClosing list
2010-07-03 15:19:59,595 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:59,595 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:20:52,706 INFO org.apache.hadoop.hbase.master.ServerManager: 1 region servers, 0 dead, average load 3.0
2010-07-03 15:20:52,866 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scanning meta region {server: 192.168.2.1:54389, regionname: -ROOT-,,0.70236052, startKey: <>}
2010-07-03 15:20:52,873 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scan of 1 row(s) of meta region {server: 192.168.2.1:54389, regionname: -ROOT-,,0.70236052, startKey: <>} complete
2010-07-03 15:20:53,671 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Cache Stats: Sizes: Total=1.6292953MB (1708440), Free=196.70822MB (206263512), Max=198.33751MB (207971952), Counts: Blocks=2, Access=54, Hit=52, Miss=2, Evictions=0, Evicted=0, Ratios: Hit Ratio=96.29629850387573%, Miss Ratio=3.7037037312984467%, Evicted/Run=NaN
2010-07-03 15:20:54,823 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scanning meta region {server: 192.168.2.1:54389, regionname: .META.,,1.1028785192, startKey: <>}
2010-07-03 15:20:54,828 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scan of 2 row(s) of meta region {server: 192.168.2.1:54389, regionname: .META.,,1.1028785192, startKey: <>} complete
2010-07-03 15:20:54,828 INFO org.apache.hadoop.hbase.master.BaseScanner: All 1 .META. region(s) scanned
2010-07-03 15:21:52,709 INFO org.apache.hadoop.hbase.master.ServerManager: 1 region servers, 0 dead, average load 3.0
2010-07-03 15:21:52,869 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scanning meta region {server: 192.168.2.1:54389, regionname: -ROOT-,,0.70236052, startKey: <>}
2010-07-03 15:21:52,876 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scan of 1 row(s) of meta region {server: 192.168.2.1:54389, regionname: -ROOT-,,0.70236052, startKey: <>} complete
2010-07-03 15:21:53,672 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Cache Stats: Sizes: Total=1.6292953MB (1708440), Free=196.70822MB (206263512), Max=198.33751MB (207971952), Counts: Blocks=2, Access=56, Hit=54, Miss=2, Evictions=0, Evicted=0, Ratios: Hit Ratio=96.42857313156128%, Miss Ratio=3.57142873108387%, Evicted/Run=NaN
2010-07-03 15:21:54,826 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scanning meta region {server: 192.168.2.1:54389, regionname: .META.,,1.1028785192, startKey: <>}
2010-07-03 15:21:54,830 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scan of 2 row(s) of meta region {server: 192.168.2.1:54389, regionname: .META.,,1.1028785192, startKey: <>} complete
2010-07-03 15:21:54,830 INFO org.apache.hadoop.hbase.master.BaseScanner: All 1 .META. region(s) scanned
","0.89, non-distributed mode",bowlinearl,davelatham,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26479,,,,,Sat Sep 25 06:34:31 UTC 2010,,,,,,,,,,"0|i02c13:",11550,,,,,,,,,,,,,,,,,,,,,"13/Jul/10 19:50;jdcryans;What I see a master that thinks that some region is hosted on a region server, that that region server receives the message but does nothing about it (in the scope of that log). Bigger log snippet and thread dump would really be helpful, or even a basic unit test that shows the issue.

Also the fix version is for 0.20.6 but you wrote that you used 0.89... so can we move it only to trunk in order to release 0.20.6? Thanks Sam.;;;","15/Jul/10 18:46;jdcryans;Didn't see any objections for 2 days, moving to 0.90;;;","05/Aug/10 16:38;davelatham;I have also run into this, using 0.20.6.  I have a test environment that runs a set of automated tests with our latest code every hour.  Some of these tests are set to disable, drop, and recreate a table in order to make an empty one.  After succeeding for 5 to 10 executions, it seems to get into this state.  I'm not able to disable and get rid of the test table until hbase is restarted, so it is definitely hampering our ability to do automated testing with hbase.

The logs I have look similar, though the master and region server are separate processes with separate logs in my environment.

Does anyone have any ideas as to the cause of this?  Is there any more information that would help in diagnosing?;;;","05/Aug/10 17:42;jdcryans;Dave sent me the logs, and it really looks like a case of HBASE-2755:

{noformat}
2010-08-04 19:26:58,291 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region test_ReferralQueue,,1280975190155 to setClosing list
2010-08-04 19:26:58,565 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scanning meta region {server: 192.168.21.66:60020, regionname: .META.,,1, startKey: <>}
2010-08-04 19:26:58,654 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_CLOSE: test_ReferralQueue,,1280975190155 from hslave3,60020,1280951333279; 1 of 1
2010-08-04 19:26:58,654 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessRegionClose of test_ReferralQueue,,1280975190155, true, reassign: false
2010-08-04 19:26:58,656 INFO org.apache.hadoop.hbase.master.ProcessRegionClose$1: region closed: test_ReferralQueue,,1280975190155
2010-08-04 19:26:58,686 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET on test_ReferralQueue,,1280975190155 got different startcode than SCAN: sc=0, serverAddress=1280951333279
2010-08-04 19:26:58,686 DEBUG org.apache.hadoop.hbase.master.BaseScanner: Current assignment of test_ReferralQueue,,1280975190155 is not valid;  serverAddress=, startCode=0 unknown.
{noformat}

Here the base scanner saw that that region was half unassigned (atomicity problem?) since we don't see the message that ""sa"" is different.   It means that the base scanner reassigns the region because it believes that:

 - the region isn't assigned to anyone
 - that it's not online
 - that it's not in transition

This means that we should also verify our assumption that the HRI.isOffline() is correct when we read it. I think we can augment HBASE-2755's patch to do that, since doing the GET we already read the new HRI. Making a patch.;;;","05/Aug/10 18:01;jdcryans;Patch generated from 0.20.6 (so that Dave can test) that incorporates HBASE-2755 and adds the checking of the newly obtained HRI.;;;","09/Aug/10 15:40;davelatham;The automated tests have now run for close to 100 executions against 0.20.6 + this patch without any problems this time, so it appears to fix the issue.  Is anyone up to review and commit it to the 0.20 branch?;;;","09/Aug/10 19:00;jdcryans;So a review would be missing (although my patch is mostly 2755), but also the confirmation that it fixes Sam's problem. If not, then we should open a new jira and commit the fix there.;;;","10/Aug/10 23:23;karthik.ranga;+1 Patch looks good to me.;;;","18/Aug/10 19:34;jdcryans;Since I still didn't get the confirmation from Sam that this patch fixed his issue, I opened HBASE-2927 and posted the patch there. Keeping this jira open until we can tell for sure that the original issue is fixed.;;;","01/Sep/10 22:19;stack;Table enable/disable is different since hbase-2692 went in.  Should make this a non-issue.;;;","25/Sep/10 06:34;stack;Moving into 0.20.7.  This I believe is fixed in TRUNK (0.90).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DNS hiccups cause uncaught NPE in HServerAddress#getBindAddress,HBASE-2806,12468274,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,tsuna,tsuna,tsuna,30/Jun/10 18:40,20/Nov/15 12:43,14/Jul/23 06:06,05/Jul/10 19:35,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Yesterday at the Hadoop Summit, the ""HADOOP"" wireless network was using a pair of DNS servers that couldn't resolve {{localhost.}}.  This prevented me from starting HBase as the construction of the {{HMaster}} was failing with the following rather cryptic error:
{code}
2010-06-29 14:30:24,603 ERROR org.apache.hadoop.hbase.master.HMaster: Failed to start master
java.lang.RuntimeException: Failed construction of Master: class org.apache.hadoop.hbase.master.HMaster$LocalHMasternull
        at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:1217)
        at org.apache.hadoop.hbase.LocalHBaseCluster.<init>(LocalHBaseCluster.java:112)
        at org.apache.hadoop.hbase.master.HMaster.doMain(HMaster.java:1298)
        at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1355)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:1215)
        ... 3 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.HServerAddress.getBindAddress(HServerAddress.java:89)
        at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:204)
        at org.apache.hadoop.hbase.master.HMaster$LocalHMaster.<init>(HMaster.java:1230)
        ... 8 more
{code}
The {{NullPointerException}} in {{getBindAddress}} comes from the following line of code:
{code:java}
    return this.address.getAddress().getHostAddress();
{code}
where {{getAddress()}} was returning {{null}}.

I think the code should check for this case, log an appropriate error message (to point whoever is going to troubleshoot the problem in the right direction), and throw something else than an NPE.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/10 17:44;stack;0001-HBASE-2806-Log-a-proper-error-message-and-don-t-NPE-.patch;https://issues.apache.org/jira/secure/attachment/12448714/0001-HBASE-2806-Log-a-proper-error-message-and-don-t-NPE-.patch","05/Jul/10 19:31;streamy;HBASE-2806-compilefix.patch;https://issues.apache.org/jira/secure/attachment/12448717/HBASE-2806-compilefix.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26478,Reviewed,,,,Fri Nov 20 12:43:47 UTC 2015,,,,,,,,,,"0|i0hjbz:",100394,,,,,,,,,,,,,,,,,,,,,"02/Jul/10 00:58;hbasereviewboard;Message from: ""Benoit Sigoure"" <tsunanet@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/244/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

The code returns null when this happens.  Also, whenever an instance
of HServerAddress is created, the code will verify that the IP can
be retrieved and throw an IllegalArgumentException if it cannot.

This change also fixes some other random little things like:
  - Remove unnecessary parentheses.
  - Remove unnecessary uses of `this' outside of constructors.
  - Missing spaces after `if'.
  - Remove wildcard imports.
  - Remove unnecessary creation of objects.


This addresses bug HBASE-2806.
    http://issues.apache.org/jira/browse/HBASE-2806


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/HServerAddress.java 959479 

Diff: http://review.hbase.org/r/244/diff


Testing
-------

Code compiles.


Thanks,

Benoit


;;;","05/Jul/10 17:44;stack;Attaching patch that was +1'd over on review.hbase.org.;;;","05/Jul/10 17:49;stack;Thanks for the patch Benoît;;;","05/Jul/10 17:50;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/244/#review301
-----------------------------------------------------------

Ship it!


- stack



;;;","05/Jul/10 19:29;streamy;Trunk doesn't compile after this got committed;;;","05/Jul/10 19:31;streamy;I think this is all that's needed.  If no one responds soon I will commit it.;;;","05/Jul/10 19:35;streamy;Committed compile fix to trunk.;;;","06/Jul/10 16:59;tsuna;Sorry for the compilation breakage, this 1-line fix remained uncommitted in my local Git repo, I ran {{git commit --amend}} and forgot the {{-a}} to get this change in the patch.  Thanks for the fix Jonathan!;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove remaining Get code from Store.java,etc",HBASE-2803,12468173,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,29/Jun/10 16:29,20/Nov/15 12:40,14/Jul/23 06:06,16/Jul/10 23:01,,,,,,,,,,,,0.90.0,,,,,,,0,,,"There is still remaining Get code due to HBASE-2248, remove it!",,larsfrancke,streamy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26477,,,,,Fri Nov 20 12:40:54 UTC 2015,,,,,,,,,,"0|i0hjbj:",100392,,,,,,,,,,,,,,,,,,,,,"06/Jul/10 21:28;streamy;What's the status of this?  Looks like stack reviewed a week ago?

Is there still more Get code removal after this?  I still see a bunch of the Get tracker/matcher classes hanging around.;;;","06/Jul/10 21:48;ryanobjc;I already did all the work last week thereabouts, I have been ooto
since and I'll commit them soon.


;;;","06/Jul/10 21:52;streamy;I've not seen a patch that removes those tracker/matcher classes.  Is one out there?;;;","07/Jul/10 22:31;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/278/
-----------------------------------------------------------

(Updated 2010-07-07 15:25:32.452386)


Review request for hbase.


Summary
-------

HBASE-2803 continues to remove more get code from the codebase.


This addresses bug HBASE-2803.
    http://issues.apache.org/jira/browse/HBASE-2803


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 40205c4 
  src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java 7eac029 
  src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java 574e88a 
  src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 9c720b1 
  src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 6a58881 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java 9c1c95c 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java 48b8ac0 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestMemStore.java 4f27ea5 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java b15ae53 

Diff: http://review.hbase.org/r/278/diff


Testing
-------


Thanks,

Ryan


;;;","13/Jul/10 18:08;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/278/#review393
-----------------------------------------------------------


This patch has not been committed though +1'd w/ a few nits.

- stack



;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Append not enabled"" warning should not show if hbase root dir isn't on DFS",HBASE-2799,12468015,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,stack,tlipcon,tlipcon,28/Jun/10 06:07,20/Nov/15 12:43,14/Jul/23 06:06,02/Sep/10 17:32,0.90.0,,,,,,,,,,,0.90.0,,master,,,,,0,,,"HBASE-2762 added a warning on the master UI if append isn't enabled. However, when HBase is in standalone mode and using a file:/// rootdir, it shows the warning, which doesn't make sense.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/10 16:35;stack;2799-v2.txt;https://issues.apache.org/jira/secure/attachment/12453692/2799-v2.txt","01/Sep/10 23:19;stack;2799.txt;https://issues.apache.org/jira/secure/attachment/12453639/2799.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26474,Reviewed,,,,Fri Nov 20 12:43:08 UTC 2015,,,,,,,,,,"0|i0hjav:",100389,,,,,,,,,,,,,,,,,,,,,"01/Sep/10 23:20;stack;Marking patch available.  Patch will show message only if append not supported and it is hdfs (Added to FSUtils an isHDFS method and a test for this new method).  I tested it here locally and it does the right thing.  Commit?;;;","02/Sep/10 02:37;streamy;Looks like output of status not diff.;;;","02/Sep/10 16:35;stack;Duh.

Added more to the test.;;;","02/Sep/10 16:57;streamy;Looks good.  If test passes for you, +1 for commit.;;;","02/Sep/10 17:32;stack;Committed.  Thanks for review Jon.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Another NPE in ReadWriteConsistencyControl,HBASE-2797,12467983,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,ryanobjc,davelatham,davelatham,27/Jun/10 03:42,12/Oct/12 06:16,14/Jul/23 06:06,13/Jul/10 19:07,0.20.5,,,,,,,,,,,0.20.6,0.90.0,,,,,,0,,,"This occurred on a cluster with 46 slaves, running a couple MR jobs.  One doing heavy writes copying everything from one table to a new table with a different schema.  After one regionserver went down, about 40 of them died within an hour before it was caught and the jobs stopped.  Let me know if any other piece of context would be particularly helpful.

This exception appears in the .out file:

Exception in thread ""regionserver/192.168.41.2:60020"" java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.ReadWriteConsistencyControl.getThreadReadPoint(ReadWriteConsistencyControl.java:40)
        at org.apache.hadoop.hbase.regionserver.MemStore$MemStoreScanner.getNext(MemStore.java:532)
        at org.apache.hadoop.hbase.regionserver.MemStore$MemStoreScanner.seek(MemStore.java:558)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:320)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.checkReseek(StoreScanner.java:306)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.peek(StoreScanner.java:143)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:127)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:117)
        at java.util.PriorityQueue.siftDownUsingComparator(PriorityQueue.java:644)
        at java.util.PriorityQueue.siftDown(PriorityQueue.java:612)
        at java.util.PriorityQueue.poll(PriorityQueue.java:523)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.close(KeyValueHeap.java:151)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.close(HRegion.java:1971)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.closeAllRegions(HRegionServer.java:1610)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:621)
        at java.lang.Thread.run(Thread.java:619)",,davelatham,kannanm,pranavkhaitan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/10 23:44;pranavkhaitan;testDebugNPE.diff;https://issues.apache.org/jira/secure/attachment/12448835/testDebugNPE.diff",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26473,,,,,Tue Jul 13 19:07:53 UTC 2010,,,,,,,,,,"0|i08sbb:",49187,,,,,,,,,,,,,,,,,,,,,"27/Jun/10 04:33;stack;Ryan, can you take a look at this one boss?;;;","29/Jun/10 13:37;davelatham;Also getting them with the similar stack trace:

Exception in thread ""regionserver/192.168.41.19:60020.leaseChecker"" java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.ReadWriteConsistencyControl.getThreadReadPoint(ReadWriteConsistencyControl.java:40)
        at org.apache.hadoop.hbase.regionserver.MemStore$MemStoreScanner.getNext(MemStore.java:532)
        at org.apache.hadoop.hbase.regionserver.MemStore$MemStoreScanner.seek(MemStore.java:558)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:320)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.checkReseek(StoreScanner.java:306)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.peek(StoreScanner.java:143)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:127)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:117)
        at java.util.PriorityQueue.siftDownUsingComparator(PriorityQueue.java:644)
        at java.util.PriorityQueue.siftDown(PriorityQueue.java:612)
        at java.util.PriorityQueue.poll(PriorityQueue.java:523)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.close(KeyValueHeap.java:151)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.close(HRegion.java:1971)
        at org.apache.hadoop.hbase.regionserver.HRegionServer$ScannerListener.leaseExpired(HRegionServer.java:1962)
        at org.apache.hadoop.hbase.Leases.run(Leases.java:98);;;","06/Jul/10 23:44;pranavkhaitan;Got a similar NPE error while running a test for HBase-2265. Wrote a simple function which always replicates the NPE error so that it is easy to detect the cause.

java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.ReadWriteConsistencyControl.getThreadReadPoint(ReadWriteConsistencyControl.java:45)
	at org.apache.hadoop.hbase.regionserver.MemStore$MemStoreScanner.getNext(MemStore.java:532)
	at org.apache.hadoop.hbase.regionserver.MemStore$MemStoreScanner.seek(MemStore.java:559)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:73)
	at org.apache.hadoop.hbase.regionserver.TestStore.testDebugNPE(TestStore.java:280)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:168)
	at junit.framework.TestCase.runBare(TestCase.java:134)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:232)
	at junit.framework.TestSuite.run(TestSuite.java:227)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:46)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)

 ;;;","07/Jul/10 18:49;jdcryans;Pranav,

Ryan posted a patch for review here http://review.hbase.org/r/241/diff/

Can you try your test with that patch on? If it works, can you +1 the patch?;;;","07/Jul/10 22:54;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/241/
-----------------------------------------------------------

(Updated 2010-07-07 15:49:39.980588)


Review request for hbase.


Summary
-------

HBASE-2797 another NPE in ReadWriteConsistencyControl


This addresses bug HBASE-2797.
    http://issues.apache.org/jira/browse/HBASE-2797


Diffs
-----

  src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java 737d6af 
  src/test/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java 1a89d65 

Diff: http://review.hbase.org/r/241/diff


Testing
-------


Thanks,

Ryan


;;;","07/Jul/10 23:10;pranavkhaitan;This patch doesn't fix the error I was getting. I see that the fix was only in the peek function. However, the bug I am getting doesnt even touch the peek function. Just instantiating StoreScanner() gives that error. It is possible that the instantiation is not done in the right context but even then I would not expect this kind of error. I have attached a diff file which contains the function I wrote for this bug (its just 4 lines);;;","13/Jul/10 19:07;ryanobjc;committed to trunk and branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PE is confused about flushCommits,HBASE-2787,12467842,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,24/Jun/10 23:39,20/Nov/15 12:41,14/Jul/23 06:06,24/Jun/10 23:41,,,,,,,,,,,,0.90.0,,,,,,,0,,,"flushCommits is currently false by default, oh and:

{code}
        final String flushCommits = ""--flushCommits="";
        if (cmd.startsWith(flushCommits)) {
          this.flushCommits = Boolean.parseBoolean(cmd.substring(flushCommits.length()));
          continue;
        }

        final String writeToWAL = ""--writeToWAL="";
        if (cmd.startsWith(writeToWAL)) {
          this.flushCommits = Boolean.parseBoolean(cmd.substring(writeToWAL.length()));
          continue;
        }
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26469,,,,,Fri Nov 20 12:41:44 UTC 2015,,,,,,,,,,"0|i0hj8v:",100380,,,,,,,,,,,,,,,,,,,,,"24/Jun/10 23:41;jdcryans;Committed straight to trunk, flushCommits is now true by default and writeToWAL is now configured correctly.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestHLog.testSplit hangs,HBASE-2786,12467825,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,nspiegelberg,jdcryans,jdcryans,24/Jun/10 18:15,02/May/13 02:29,14/Jul/23 06:06,24/Jun/10 20:57,,,,,,,,,,,,0.20.6,0.90.0,,,,,,0,,,"This a blocker had it blocks and times out Hudson.

It seems that when we upgraded to latest of 0.20-append we got into a new situation where we can't recover a file that's empty if the original writer is still alive:

{noformat}
2010-06-24 10:41:20,645 DEBUG [main] wal.HLog(1281): Splitting hlog 4 of 4: hdfs://localhost:64456/hbase/testSplit/.logs/hlog.1277401279534, length=0
2010-06-24 10:41:20,645 INFO  [main] util.FSUtils(612): Recovering filehdfs://localhost:64456/hbase/testSplit/.logs/hlog.1277401279534
2010-06-24 10:41:20,647 WARN  [IPC Server handler 5 on 64456] namenode.FSNamesystem(1156): DIR* NameSystem.startFile: 
failed to create file /hbase/testSplit/.logs/hlog.1277401279534 for DFSClient_-1981892617 on client 127.0.0.1 because current leaseholder is trying to recreate file
...

2010-06-24 10:42:24,919 WARN  [IPC Server handler 0 on 64456] namenode.FSNamesystem(1156): DIR* NameSystem.startFile: 
failed to create file /hbase/testSplit/.logs/hlog.1277401279534 for DFSClient_-1981892617 on client 127.0.0.1 because current leaseholder is trying to recreate file.
2010-06-24 10:42:24,919 WARN  [main] util.FSUtils(631): Waited 64274ms for lease recovery on 
hdfs://localhost:64456/hbase/testSplit/.logs/hlog.1277401279534:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: 
failed to create file /hbase/testSplit/.logs/hlog.1277401279534 for DFSClient_-1981892617 on client 127.0.0.1 because current leaseholder is trying to recreate file.
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1058)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:1171)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.append(NameNode.java:396)
{noformat}

We could just not roll the latest log and it would probably fix the issue, but I wonder if we could change something in HDFS instead. Todd?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2345,,"24/Jun/10 19:24;nspiegelberg;HBASE-2786.patch;https://issues.apache.org/jira/secure/attachment/12447976/HBASE-2786.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26468,Reviewed,,,,Fri Jun 25 04:40:13 UTC 2010,,,,,,,,,,"0|i08scf:",49192,,,,,,,,,,,,,,,,,,,,,"24/Jun/10 18:20;nspiegelberg;in our branch, we have 

      Configuration new_conf = new Configuration(this.conf);
      new_conf.setBoolean(""dfs.support.append"", false);
      Path splitsdir = new Path(this.dir, ""splits"");
      List<Path> splits =
        HLog.splitLog(splitsdir, logdir, this.oldLogDir, this.fs, new_conf);
      verifySplits(splits, howmany);

I need to remember what the exact problem is, but that should temporarily fix your issue.;;;","24/Jun/10 18:55;jdcryans;Downgrading to major now that HBASE-2345 was committed with a temp fix.;;;","24/Jun/10 19:24;nspiegelberg;hey, this should permanently fix the issue.  The problem is that log rolling closes the writer that we care about, then creates a 2nd writer with empty data.  Need to close that 2nd writer or we'll infinite wait in the hfile.append() call.  Calling log.close() should be sufficient.  I think I quickly hobbled together the previous patch because I considered this a test of split correctness, not whether you could recover data on open files.;;;","24/Jun/10 20:57;jdcryans;I was under the impression that a close of some sort was needed, I committed that to trunk. Thanks Nicolas!;;;","25/Jun/10 04:40;apurtell;This happens on 0.20 branch too now that 0.20-append enables append by default. Committed there. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestScannerTimeout.test2772 is flaky,HBASE-2785,12467817,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,24/Jun/10 16:26,20/Nov/15 12:44,14/Jul/23 06:06,24/Jun/10 22:49,,,,,,,,,,,,0.90.0,,,,,,,0,,,I knew that that test could be flaky but it seemed to work fine for a while. Basically if the region server takes too long to abort (more than 6 seconds) then the client will detect that the scanner expired instead of just moving. Find a less flaky way.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/10 17:32;jdcryans;HBASE-2785.patch;https://issues.apache.org/jira/secure/attachment/12447960/HBASE-2785.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26467,Reviewed,,,,Fri Nov 20 12:44:00 UTC 2015,,,,,,,,,,"0|i0hj8n:",100379,,,,,,,,,,,,,,,,,,,,,"24/Jun/10 17:32;jdcryans;This patch sets a very high timeout client-side only for the test, and I also decreased the scanner timeout for the other test back to where it was.;;;","24/Jun/10 22:45;stack;+1;;;","24/Jun/10 22:49;jdcryans;Thanks for the review Stack, committed to trunk.;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZKW.createUnassignedRegion doesn't make sure existing znode is in the right state,HBASE-2781,12467760,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,karthik.ranga,jdcryans,jdcryans,23/Jun/10 23:43,20/Nov/15 12:44,14/Jul/23 06:06,14/Jul/10 15:41,,,,,,,,,,,,0.90.0,,,,,,,0,,,"In ZKW.createUnassignedRegion I see this comment:

{code}
      // check if this node already exists - 
      //   - it should not exist
      //   - if it does, it should be in the CLOSED state
{code}

And what I got is:

{noformat}
2010-06-23 15:42:05,823 INFO  [IPC Server handler 3 on 60362] master.ServerManager(457): Processing MSG_REPORT_PROCESS_OPEN: test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464. from h136.sfo.stumble.net,60365,1277332849712; 1 of 4
2010-06-23 15:42:05,867 INFO  [RegionServer:1.worker] regionserver.HRegionServer$Worker(1338): Worker: MSG_REGION_OPEN: test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
2010-06-23 15:42:05,870 DEBUG [RegionServer:1.worker] regionserver.RSZookeeperUpdater(157): Updating ZNode /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464 with [RS2ZK_REGION_OPENING] expected version = 0
2010-06-23 15:42:05,871 DEBUG [main-EventThread] master.HMaster(1158): Event NodeDataChanged with state SyncConnected with path /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:05,871 DEBUG [main-EventThread] master.ZKMasterAddressWatcher(64): Got event NodeDataChanged with path /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:05,871 DEBUG [main-EventThread] master.ZKUnassignedWatcher(95): ZK-EVENT-PROCESS: Got zkEvent NodeDataChanged state:SyncConnected path:/1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:05,872 INFO  [main-EventThread] regionserver.HRegionServer(379): Got ZooKeeper event, state: SyncConnected, type: NodeDataChanged, path: /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:05,872 DEBUG [MASTER_OPENREGION-10.10.1.136:60362-1] handler.MasterOpenRegionHandler(77): Event = RS2ZK_REGION_OPENING, region = 13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:05,874 DEBUG [RegionServer:1.worker] regionserver.HRegion(297): Creating region test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
2010-06-23 15:42:06,154 INFO  [RegionServer:1.worker] regionserver.HRegion(366): Onlined test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.; next sequenceid=1
2010-06-23 15:42:06,154 DEBUG [RegionServer:1.worker] regionserver.RSZookeeperUpdater(157): Updating ZNode /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464 with [RS2ZK_REGION_OPENED] expected version = 1\
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:06,249 ERROR [RegionServer:1.worker] regionserver.HRegionServer(1488): Failed to mark region test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464. as opened
java.io.IOException: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
Caused by: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:06,993 DEBUG [RegionServer:1] regionserver.HRegionServer(1569): closing region test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
2010-06-23 15:42:06,993 DEBUG [RegionServer:1] regionserver.HRegion(487): Closing test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.: disabling compactions & flushes
2010-06-23 15:42:06,993 DEBUG [RegionServer:1] regionserver.HRegion(512): Updates disabled for region, no outstanding scanners on test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
2010-06-23 15:42:06,993 DEBUG [RegionServer:1] regionserver.HRegion(519): No more row locks outstanding on region test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
2010-06-23 15:42:06,994 INFO  [RegionServer:1] regionserver.HRegion(531): Closed test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
2010-06-23 15:42:09,105 INFO  [master] master.ProcessServerShutdown(126): Region test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464. was in transition 
name=test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464., state=PENDING_OPEN on dead server h136.sfo.stumble.net,60365,1277332849712 - marking unassigned
2010-06-23 15:42:10,065 INFO  [IPC Server handler 2 on 60362] master.RegionManager(340): Assigning region test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464. to h136.sfo.stumble.net,60363,1277332849671
2010-06-23 15:42:10,067 DEBUG [IPC Server handler 2 on 60362] zookeeper.ZooKeeperWrapper(1079): While creating UNASSIGNED region 13bef4950ac6827ac32d87682b8b2464 exists, state = RS2ZK_REGION_OPENING
2010-06-23 15:42:10,126 WARN  [IPC Server handler 2 on 60362] zookeeper.ZooKeeperWrapper(1024): <localhost:/1,org.apache.hadoop.hbase.master.HMaster>Failed to create ZNode /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464 in ZooKeeper
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:10,127 DEBUG [IPC Server handler 2 on 60362] master.RegionManager(350): Created UNASSIGNED zNode test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464. in state M2ZK_REGION_OFFLINE
2010-06-23 15:42:10,245 INFO  [RegionServer:0] regionserver.HRegionServer(511): MSG_REGION_OPEN: test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
2010-06-23 15:42:11,248 INFO  [IPC Server handler 1 on 60362] master.ServerManager(457): Processing MSG_REPORT_PROCESS_OPEN: test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464. from h136.sfo.stumble.net,60363,1277332849671; 7 of 13
2010-06-23 15:42:13,795 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(1338): Worker: MSG_REGION_OPEN: test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
2010-06-23 15:42:13,797 ERROR [RegionServer:0.worker] regionserver.RSZookeeperUpdater(107): ZNode /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464 is not in CLOSED/OFFLINE state (state = RS2ZK_REGION_OPENING), will NOT open region.
2010-06-23 15:42:13,798 ERROR [RegionServer:0.worker] regionserver.HRegionServer(814): Error opening test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
java.io.IOException: ZNode /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464 is not in CLOSED/OFFLINE state (state = RS2ZK_REGION_OPENING), will NOT open region.
2010-06-23 15:42:13,800 ERROR [RegionServer:0.worker] regionserver.RSZookeeperUpdater(141): Aborting open of region 13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:13,800 DEBUG [RegionServer:0.worker] regionserver.RSZookeeperUpdater(157): Updating ZNode /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464 with [RS2ZK_REGION_CLOSED] expected version = 0
org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:13,802 ERROR [RegionServer:0.worker] regionserver.HRegionServer(1473): Failed to abort open region test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
java.io.IOException: org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
Caused by: org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
{noformat}

Basically:
 # A region server was opening the region
 # It was expired just before reporting that the region is opened, leaving the znode in the state RS2ZK_REGION_OPENING
 # The region gets reassigned, it sees that state, doesn't change it, but still outputs in the end ""Created UNASSIGNED zNode test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464. in state M2ZK_REGION_OFFLINE""
 # When the region server opens the region, it sees that the state is wrong and aborts opening the region

I think that the way to fix it is to change the state to what it should be.",,larsfrancke,streamy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/10 23:31;karthik.ranga;HBASE-2781-0.21.patch;https://issues.apache.org/jira/secure/attachment/12449403/HBASE-2781-0.21.patch","14/Jul/10 05:20;jdcryans;HBASE-2781-fix.patch;https://issues.apache.org/jira/secure/attachment/12449417/HBASE-2781-fix.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26466,Reviewed,,,,Fri Nov 20 12:44:09 UTC 2015,,,,,,,,,,"0|i0hj7r:",100375,,,,,,,,,,,,,,,,,,,,,"28/Jun/10 18:19;karthik.ranga;Just wanted to update - working on the test case for this, will upload patch along with the JUnit test.;;;","13/Jul/10 23:31;karthik.ranga;Adding the fix here, I will open a separate JIRA for adding a test case for this issue.;;;","14/Jul/10 00:08;jdcryans;Committed to trunk, thanks for the patch Karthik!;;;","14/Jul/10 04:36;jdcryans;Hate to do this but this patch was missing a few things. TestReplication failed again, for the same reason, because some parts of RegionManager are still calling createUnassignedRegion instead of createOrUpdateUnassignedRegion. Should we just redirect all the calls to the latter and delete the former?;;;","14/Jul/10 05:20;jdcryans;Patch that makes createUnassignedRegion private and changes the remaining callers to createOrUpdateUnassignedRegion.;;;","14/Jul/10 15:41;jdcryans;My little fix did it, closing this issue.;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update of hadoop jar in HBASE-2771 broke TestMultiClusters,HBASE-2775,12467665,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,streamy,streamy,23/Jun/10 06:21,20/Nov/15 12:41,14/Jul/23 06:06,23/Jun/10 20:34,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,TestMultiClusters failing following HBASE-2771,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HDFS-1240,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26462,,,,,Fri Nov 20 12:41:34 UTC 2015,,,,,,,,,,"0|i0hj6v:",100371,,,,,,,,,,,,,,,,,,,,,"23/Jun/10 06:34;streamy;Dug into MiniDFSCluster on latest hadoop-20-append branch.  Looks like this is our problem:

{noformat}
  public static final File BASE_DIR =
    new File(System.getProperty(""test.build.data"", ""build/test/data""), ""dfs/"");
{noformat}

This means you can't have multiple MiniDFSClusters in a single jvm.  Odd regression, digging more.;;;","23/Jun/10 06:38;streamy;Introduced by HDFS-909;;;","23/Jun/10 06:53;tlipcon;See HDFS-1240;;;","23/Jun/10 06:54;streamy;This will be fixed when HDFS-1240 gets committed (thanks todd);;;","23/Jun/10 20:08;jdcryans;In the mean time I created http://people.apache.org/~rawson/repo/org/apache/hadoop/hadoop-core/0.20.3-append-r956776+1240/ if anyone is interested.;;;","23/Jun/10 20:21;stack;Change our POM to pull that j-d?  Our tests will pass again won't they if this is in place?;;;","23/Jun/10 20:34;jdcryans;Modified the pom file, closing.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spin in ReadWriteConsistencyControl eating CPU (load > 40) and no progress running YCSB on clean cluster startup,HBASE-2774,12467664,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,23/Jun/10 06:14,20/Nov/15 12:40,14/Jul/23 06:06,24/Jun/10 04:45,,,,,,,,,,,,0.90.0,,,,,,,0,,,"
When I try to do a YCSB load, RSs will spin up massive load but make no progress.  Seems to happen to each RS in turn until they do their first flush.  They stay in the high-load mode for maybe 5-10 minutes or so and then fall out of the bad condition.

Here is my ugly YCSB command (Haven't gotten around to tidying it up yet):

{code}
$ java -cp build/ycsb.jar:/home/hadoop/current/conf/:/home/hadoop/current/hbase-0.21.0-SNAPSHOT.jar:/home/hadoop/current/lib/hadoop-core-0.20.3-append-r956776.jar:/home/hadoop/current/lib/zookeeper-3.3.1.jar:/home/hadoop/current/lib/commons-logging-1.1.1.jar:/home/hadoop/current/lib/log4j-1.2.15.jar  com.yahoo.ycsb.Client -load -db com.yahoo.ycsb.db.HBaseClient  -P workloads/5050 -p columnfamily=values -s -threads 100 -p recordcount=10000000
{code}

Cluster is 5 regionservers NOT running hadoop-core-0.20.3-append-r956776 but rather old head of branch-0.20 hadoop.

It seems that its easy to repro if you start fresh.  It might happen later in loading but it seems as though after first flush, we're ok.

It comes on pretty immediately.  The server that is taking on the upload has its load start to climb gradually up into the 40s then stays there.  Later it falls when condtion clears.

Here is content of my yahoo workload file:

{code}
recordcount=100000000
operationcount=100000000
workload=com.yahoo.ycsb.workloads.CoreWorkload

readallfields=true

readproportion=0.5
updateproportion=0.5
scanproportion=0
insertproportion=0

requestdistribution=zipfian
{code}

Here is my hbase-site.xml

{code}
  <property>
    <name>hbase.regions.slop</name>
    <value>0.01</value>
    <description>Rebalance if regionserver has average + (average * slop) regions.
    Default is 30% slop.
    </description>
  </property>

  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>XXXXXXXXX</value>
  </property>

<property>
  <name>hbase.regionserver.hlog.blocksize</name>
  <value>67108864</value>
  <description>Block size for HLog files. To minimize potential data loss,
    the size should be (avg key length) * (avg value length) * flushlogentries.
    Default 1MB.
  </description>
</property>

<property>
  <name>hbase.hstore.blockingStoreFiles</name>
  <value>25</value>
</property>

<property>
  <name>hbase.rootdir</name>
  <value>hdfs://svXXXXXX:9000/hbase</value>
  <description>The directory shared by region servers.</description>
</property>

<property>
  <name>hbase.cluster.distributed</name>
  <value>true</value>
</property>

<property>
  <name>zookeeper.znode.parent</name>
  <value>/stack</value>
  <description>
    the path in zookeeper for this cluster
  </description>
</property>

<property>
  <name>hfile.block.cache.size</name>
  <value>0.2</value>
  <description>
    The size of the block cache used by HFile/StoreFile. Set to 0 to disable.
  </description>
</property>


<property>
  <name>hbase.hregion.memstore.block.multiplier</name>
  <value>8</value>
  <description>
    Block updates if memcache has hbase.hregion.block.memcache
    time hbase.hregion.flush.size bytes.  Useful preventing
    runaway memcache during spikes in update traffic.  Without an
    upper-bound, memcache fills such that when it flushes the
    resultant flush files take a long time to compact or split, or
    worse, we OOME.
  </description>
</property>

<property>
<name>zookeeper.session.timeout</name>
<value>60000</value>
</property>


<property>
  <name>hbase.regionserver.handler.count</name>
  <value>60</value>
  <description>Count of RPC Server instances spun up on RegionServers
    Same property is used by the HMaster for count of master handlers.
    Default is 10.
  </description>
</property>

<property>
    <name>hbase.regions.percheckin</name>
    <value>20</value>
</property>

<property>
    <name>hbase.regionserver.maxlogs</name>
    <value>128</value>
</property>

<property>
    <name>hbase.regionserver.logroll.multiplier</name>
    <value>2.95</value>
</property>
{code}
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/10 03:22;stack;2774-v4.txt;https://issues.apache.org/jira/secure/attachment/12447920/2774-v4.txt","24/Jun/10 04:14;stack;2774-v5.txt;https://issues.apache.org/jira/secure/attachment/12447922/2774-v5.txt","23/Jun/10 23:05;stack;sync-wait3.txt;https://issues.apache.org/jira/secure/attachment/12447899/sync-wait3.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26461,Reviewed,,,,Fri Nov 20 12:40:35 UTC 2015,,,,,,,,,,"0|i0hj6n:",100370,,,,,,,,,,,,,,,,,,,,,"23/Jun/10 06:26;stack;I added logging of spins > 10k.  Seems to be frequent during problematic period.  Here is sampling:

{code}
SPUN=32067854
SPUN=11609771
SPUN=18253409
SPUN=7594706
SPUN=5145968
SPUN=3954805
SPUN=8363557
SPUN=17825817
SPUN=5544807
SPUN=52516
SPUN=7097147
SPUN=11689658
SPUN=1227793
SPUN=11703071
SPUN=23131
SPUN=53260
SPUN=45888
SPUN=19449
SPUN=11688378
SPUN=17349
SPUN=25103
SPUN=16421
SPUN=11001
SPUN=10975
SPUN=30119
SPUN=12685
SPUN=18986
SPUN=10724
SPUN=13987
SPUN=10626
SPUN=22122
SPUN=14818
{code}

There were other periods of big numbers like those above.;;;","23/Jun/10 22:22;stack;I tried 0.20.5.  It don't seem to have the issue -- least load doesn't go up crazy in same place (It doesn't have the multput though as Ryan notes)

I added logging.  What I see are lots of updates where there is no spin at all and then we'll hit a period where we can have crazy spinning as per this:

{code}
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=58ms, begin took=908ns, w=writeNumber=389286, completed=true, spun=4161, count=10
2010-06-23 15:11:59,897 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=40ms, begin took=1118ns, w=writeNumber=389234, completed=true, spun=0, count=10
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=50ms, begin took=838ns, w=writeNumber=389287, completed=true, spun=0, count=10
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=74ms, begin took=838ns, w=writeNumber=389289, completed=true, spun=9535, count=10
2010-06-23 15:11:59,897 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=64ms, begin took=1117ns, w=writeNumber=389230, completed=true, spun=0, count=10
2010-06-23 15:11:59,897 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=56ms, begin took=1118ns, w=writeNumber=389226, completed=true, spun=0, count=10
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=72ms, begin took=1118ns, w=writeNumber=389290, completed=true, spun=0, count=10
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=68ms, begin took=1396ns, w=writeNumber=389291, completed=true, spun=0, count=10
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=93ms, begin took=1047ns, w=writeNumber=389292, completed=true, spun=0, count=10
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=78ms, begin took=768ns, w=writeNumber=389293, completed=true, spun=0, count=10
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=69ms, begin took=907ns, w=writeNumber=389295, completed=true, spun=0, count=10
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=86ms, begin took=768ns, w=writeNumber=389296, completed=true, spun=0, count=10
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=76ms, begin took=768ns, w=writeNumber=389298, completed=true, spun=3842, count=10
2010-06-23 15:11:59,897 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=47ms, begin took=838ns, w=writeNumber=389225, completed=true, spun=0, count=10
2010-06-23 15:11:59,897 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=44ms, begin took=908ns, w=writeNumber=389223, completed=true, spun=0, count=10
2010-06-23 15:11:59,897 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=58ms, begin took=908ns, w=writeNumber=389214, completed=true, spun=0, count=10
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=66ms, begin took=1467ns, w=writeNumber=389300, completed=true, spun=0, count=10
2010-06-23 15:11:59,897 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=66ms, begin took=978ns, w=writeNumber=389211, completed=true, spun=0, count=10
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=62ms, begin took=768ns, w=writeNumber=389302, completed=true, spun=1437, count=10
2010-06-23 15:11:59,897 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=59228ms, begin took=2165ns, w=writeNumber=389195, completed=true, spun=12733714, count=10
2010-06-23 15:11:59,897 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=59184ms, begin took=1397ns, w=writeNumber=389196, completed=true, spun=9228124, count=10
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=152ms, begin took=908ns, w=writeNumber=389304, completed=true, spun=37705, count=10
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=179ms, begin took=1048ns, w=writeNumber=389303, completed=true, spun=0, count=10
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=81ms, begin took=1187ns, w=writeNumber=389305, completed=true, spun=0, count=10
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=83ms, begin took=1467ns, w=writeNumber=389306, completed=true, spun=0, count=10
2010-06-23 15:11:59,900 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=46ms, begin took=1117ns, w=writeNumber=389308, completed=true, spun=0, count=10
2010-06-23 15:11:59,897 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=59584ms, begin took=1467ns, w=writeNumber=389190, completed=true, spun=21534587, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=47ms, begin took=1676ns, w=writeNumber=389310, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=55ms, begin took=1117ns, w=writeNumber=389312, completed=true, spun=2258, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=50ms, begin took=1117ns, w=writeNumber=389313, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=57ms, begin took=768ns, w=writeNumber=389315, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=51ms, begin took=698ns, w=writeNumber=389316, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=46ms, begin took=699ns, w=writeNumber=389317, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=43ms, begin took=699ns, w=writeNumber=389318, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=41ms, begin took=698ns, w=writeNumber=389319, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=51ms, begin took=698ns, w=writeNumber=389320, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=55ms, begin took=698ns, w=writeNumber=389321, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=50ms, begin took=838ns, w=writeNumber=389322, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=44ms, begin took=698ns, w=writeNumber=389323, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=46ms, begin took=698ns, w=writeNumber=389324, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=42ms, begin took=699ns, w=writeNumber=389325, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=50ms, begin took=698ns, w=writeNumber=389326, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=39ms, begin took=699ns, w=writeNumber=389327, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=53ms, begin took=699ns, w=writeNumber=389328, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=43ms, begin took=699ns, w=writeNumber=389329, completed=true, spun=0, count=10
2010-06-23 15:11:59,901 INFO org.apache.hadoop.hbase.regionserver.HRegion: elapsed=50ms, begin took=699ns, w=writeNumber=389330, completed=true, spun=0, count=10
{code}

My nano-math is bad in the above... I should have divided by another 1000 so when I say 50ms above, it should be 0.05ms.;;;","23/Jun/10 22:27;tlipcon;What happens if you add:

{code}
if (spun > 5) Thread.yield();
{code}

any help?;;;","23/Jun/10 23:05;stack;This patch of Ryan's seems to fix the issue.  No longer do I see a spike in load.

The patch uses volatile longs in place of AtomicLong -- don't need the AtomicLong functionality -- and it then does a wait/notify instead of spinning.

The spun counter is not used... should return it out and log extreme counts?

Let me try Todd's suggestion next.;;;","23/Jun/10 23:18;stack;With Todds suggestion I see loads climb still.   Ryan's patch seems to do better -- load is less on startup.  It may be because it slow everything way down.  Let me check that.;;;","24/Jun/10 03:22;stack;Here is cleaned version of patch.

I ran loading to the end with both Todd suggestion and Ryan's patch in place.  There wasn't much in it:

Todd's suggestion:

{code}
[OVERALL],RunTime(ms), 668045
[OVERALL],Throughput(ops/sec), 14969.051486052585
[INSERT], Operations, 10000000
[INSERT], AverageLatency(ms), 5.7067132
[INSERT], MinLatency(ms), 0
[INSERT], MaxLatency(ms), 124154
[INSERT], 95thPercentileLatency(ms), 0
[INSERT], 99thPercentileLatency(ms), 1
{code}

Ryan's patch:

{code}
[OVERALL],RunTime(ms), 651014
[OVERALL],Throughput(ops/sec), 15360.652766299956
[INSERT], Operations, 10000000
[INSERT], AverageLatency(ms), 5.571351
[INSERT], MinLatency(ms), 0
[INSERT], MaxLatency(ms), 112391
[INSERT], 95thPercentileLatency(ms), 0
[INSERT], 99thPercentileLatency(ms), 0
{code}

only w/ Ryan's patch there wasn't the same spike on startup of loading (I wasn't seeing load 40 w/ Todd patch more like 11 or 12... w/ Ryan's we were down around load 5).
;;;","24/Jun/10 03:40;tlipcon;patch looks good with one nit: if the thread spinning gets interrupted, we should propagate the interruption status out of the loop. something like:

{code}
boolean interrupted = false;
while (condition) {
  try {
    wait();
  } catch (INterruptedException ie) {
    interrupted = true;
  }
}
if (interrupted) Thread.currentThread().interrupt();
{code}

Though we don't really use interruption much inside RS right now, I think this is better style in general - swallowing interruptions becomes a pain if you start to rely on them.;;;","24/Jun/10 04:14;stack;Reset interrupted flag if we get InterruptedException rather than ignore it (http://www.ibm.com/developerworks/java/library/j-jtp05236.html);;;","24/Jun/10 04:16;tlipcon;+1. commit to trunk and branch? maybe 0.20 also?;;;","24/Jun/10 04:45;stack;Committed to development branch and to trunk.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scan doesn't recover from region server failure,HBASE-2772,12467640,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,22/Jun/10 23:07,12/Oct/12 06:16,14/Jul/23 06:06,23/Jun/10 20:48,,,,,,,,,,,,0.20.6,0.90.0,,,,,,0,,,"Very simple, if the region server you are scanning from dies for any reason, your scanner will die too because it will not try to get a new lease and will reuse the same id.

It doesn't happen to region that moves because we keep them opened until the scanner is closed.

It could be affecting branch, I'm just not sure yet.

I found this issue with TestReplication from HBASE-2223.",,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/10 23:58;jdcryans;HBASE-2772.patch;https://issues.apache.org/jira/secure/attachment/12447773/HBASE-2772.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26459,Reviewed,,,,Wed Jun 23 20:50:43 UTC 2010,,,,,,,,,,"0|i08sbz:",49190,,,,,,,,,,,,,,,,,,,,,"22/Jun/10 23:51;jdcryans;Patch that fixes the issue and adds a unit test. I made it that if you go over the timeout from the client POV then it'll throw the Exception, else it will reset the scanner.

I had to up the timeout in the test, the other solution is to create a new unit test that would take longer.;;;","22/Jun/10 23:53;jdcryans;This is also in branch, same code that handles UnknownScannerException.;;;","22/Jun/10 23:58;jdcryans;Silly me, did a last minute cleanup that doesn't compile. New patch.;;;","23/Jun/10 18:20;stack;+1

Nice test.;;;","23/Jun/10 20:48;jdcryans;Thanks for the review Stack, committed to branch and trunk.;;;","23/Jun/10 20:50;jdcryans;Also committed to 0.89.20100621.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trivial typo in HBaseConfiguration deprecation message,HBASE-2769,12467630,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,tlipcon,tlipcon,tlipcon,22/Jun/10 21:54,20/Nov/15 12:42,14/Jul/23 06:06,22/Jun/10 22:03,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"""instantinating"" for ""instantiating"" - this has been annoying my OCD side.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/10 21:57;tlipcon;hbase-2769.txt;https://issues.apache.org/jira/secure/attachment/12447751/hbase-2769.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26457,,,,,Fri Nov 20 12:42:12 UTC 2015,,,,,,,,,,"0|i0hj5z:",100367,,,,,,,,,,,,,,,,,,,,,"22/Jun/10 21:57;tlipcon;Here's patch, just gonna commit this to trunk.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
setMaxRecoveryErrorCount reflection fails after HDFS-1209,HBASE-2767,12467617,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,tlipcon,tlipcon,tlipcon,22/Jun/10 20:42,20/Nov/15 12:43,14/Jul/23 06:06,23/Jun/10 05:36,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,HDFS-1209 adds a configuration parameter for changing the number of recovery retries. This has been applied in CDH3b2 and about to be applied in 0.20-append. HBaseTestingUtility.setMaxRecoveryErrorCount() fails once this patch has been applied.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/10 20:45;tlipcon;hbase-2767.txt;https://issues.apache.org/jira/secure/attachment/12447739/hbase-2767.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26456,Reviewed,,,,Fri Nov 20 12:43:22 UTC 2015,,,,,,,,,,"0|i0hj5j:",100365,,,,,,,,,,,,,,,,,,,,,"23/Jun/10 03:47;stack;+1 This patch does little.  Safe to commit.;;;","23/Jun/10 05:36;tlipcon;Committed to trunk and 0.89.20100621;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cross-port HADOOP-6833 IPC parameter leak bug,HBASE-2763,12467527,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,tlipcon,tlipcon,tlipcon,22/Jun/10 02:42,20/Nov/15 12:41,14/Jul/23 06:06,22/Jun/10 22:13,0.20.5,0.90.0,,,,,,,,,,0.90.0,,,,,,,0,,,There's a bug where any RPC call that throws an exception ends up leaking the parameter objects of that call. This was introduced by HBASE-2360,,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-6833,HBASE-2360,,,,,,,,,,,,,,,,"22/Jun/10 21:41;tlipcon;hbase-2763.txt;https://issues.apache.org/jira/secure/attachment/12447744/hbase-2763.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26454,Reviewed,,,,Fri Nov 20 12:41:17 UTC 2015,,,,,,,,,,"0|i0hj4n:",100361,,,,,,,,,,,,,,,,,,,,,"22/Jun/10 21:41;tlipcon;Tested this patch on a cluster overnight. Where YCSB used to run out of memory after less than a hour, the test completed successfully (ran for 4 hours) after the bug fix.;;;","22/Jun/10 21:55;stack;+1;;;","22/Jun/10 22:03;stack;Applied to 0.20 branch (I'm not going to sink the RC because of it; will roll a 0.20.6 on the heels of 0.20.5 instead -- there is this issue and the as yet unsolved HBASE-2766 that need to go into a 0.20.6).;;;","22/Jun/10 22:13;stack;Applied to 0.20, 0.89.2010... and to TRUNK.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetaScanner throws TableNotFoundException when specifying empty start row of a table,HBASE-2760,12467459,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,tlipcon,tlipcon,tlipcon,21/Jun/10 05:23,20/Nov/15 12:41,14/Jul/23 06:06,22/Jun/10 00:33,0.90.0,,,,,,,,,,,0.90.0,,Client,,,,,0,,,"Getting errors like this:
org.apache.hadoop.hbase.TableNotFoundException: Cannot find row in .META. for table: VerifiableEditor, row=VerifiableEditor,,00000000000000
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:104)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.prefetchRegionCache(HConnectionManager.java:733)
META contains:
{code}
hbase(main):001:0> scan '.META.'
ROW                                                      COLUMN+CELL                                                                                                                                                        
 VerifiableEditor,,1277097543936.6a2992842d685f48213bc33 column=info:regioninfo, timestamp=1277097544071, value=REGION => {NAME => 'VerifiableEditor,,1277097543936.6a2992842d685f48213bc33afa85ac94.', STARTKEY => '', ENDK
 afa85ac94.                                              EY => '', ENCODED => 6a2992842d685f48213bc33afa85ac94, TABLE => {{NAME => 'VerifiableEditor', FAMILIES => [{NAME => 'info', BLOOMFILTER => 'NONE', REPLICATION_SCOP
                                                         E => '0', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                        
 VerifiableEditor,,1277097543936.6a2992842d685f48213bc33 column=info:server, timestamp=1277097544317, value=monster04.sf.cloudera.com:60020                                                                                 
 afa85ac94.                                                                                                                                                                                                                 
 VerifiableEditor,,1277097543936.6a2992842d685f48213bc33 column=info:serverstartcode, timestamp=1277097544317, value=1277097337799                                                                                          
 afa85ac94.                                                                                                                                                                                                                 
1 row(s) in 0.2660 seconds
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/10 00:32;tlipcon;hbase-2760.txt;https://issues.apache.org/jira/secure/attachment/12447636/hbase-2760.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26452,Reviewed,,,,Fri Nov 20 12:41:01 UTC 2015,,,,,,,,,,"0|i0hj3z:",100358,,,,,,,,,,,,,,,,,,,,,"21/Jun/10 13:59;stack;+1 on patch (Your description seems off though, the bit where you say 'set to ZEROES instead of NINES' -- I see NINES in your patch).;;;","21/Jun/10 17:21;tlipcon;RB poster was broken temporarily. Review at:
http://review.hbase.org/r/211/

I'll commit this later today (give some other people a chance to look at it);;;","21/Jun/10 21:21;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/211/#review260
-----------------------------------------------------------

Ship it!


I already +1'd this patch over in the issue (this review.hbase.org seemed down for me this morning when I went to review).

- stack



;;;","22/Jun/10 00:33;tlipcon;Committed the patch from reviewboard to trunk (just uploaded here for later reference);;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
META region stuck in RS2ZK_REGION_OPENED state,HBASE-2758,12467456,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,karthik.ranga,tlipcon,tlipcon,21/Jun/10 04:16,20/Nov/15 12:43,14/Jul/23 06:06,23/Jun/10 03:32,0.90.0,,,,,,,,,,,0.90.0,,master,regionserver,,,,0,,,"In cluster testing trunk, I ended up with a situation where META was unassigned and no amount of restarting various pieces would fix it. On master startup, I see:

2010-06-20 21:08:05,431 DEBUG org.apache.hadoop.hbase.master.BaseScanner: Current assignment of .META.,,1.1028785192 is not valid;  serverAddress=, startCode=0 unknown.
2010-06-20 21:08:05,436 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: While creating UNASSIGNED region 1028785192 exists, state = RS2ZK_REGION_OPENED
2010-06-20 21:08:05,438 WARN org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: <monster01.sf.cloudera.com:/hbase,org.apache.hadoop.hbase.master.HMaster>Failed to create ZNode /hbase/UNASSIGNED/1028785192 in ZooKeeper
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /hbase/UNASSIGNED/1028785192
2010-06-20 21:08:05,438 DEBUG org.apache.hadoop.hbase.master.RegionManager: Created UNASSIGNED zNode .META.,,1.1028785192 in state M2ZK_REGION_OFFLINE

then on the RS:
2010-06-20 21:08:05,899 ERROR org.apache.hadoop.hbase.regionserver.RSZookeeperUpdater: ZNode /hbase/UNASSIGNED/1028785192 is not in CLOSED/OFFLINE state (state = RS2ZK_REGION_OPENED), will NOT open region.
2010-06-20 21:08:05,899 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: Error opening .META.,,1.1028785192
java.io.IOException: ZNode /hbase/UNASSIGNED/1028785192 is not in CLOSED/OFFLINE state (state = RS2ZK_REGION_OPENED), will NOT open region.

and the region never opens
",,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/10 03:06;karthik.ranga;HBASE-2758-0.21.patch;https://issues.apache.org/jira/secure/attachment/12447789/HBASE-2758-0.21.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26450,,,,,Fri Nov 20 12:43:43 UTC 2015,,,,,,,,,,"0|i0hj3r:",100357,,,,,,,,,,,,,,,,,,,,,"21/Jun/10 16:30;streamy;I think Karthik has a fix for this in another patch.  Basically, when a master starts up (before we fully handle master failover) he just needs to clean out all the znodes from /UNASSIGNED.  Should be a simple fix.;;;","21/Jun/10 16:30;streamy;Assigning to Karthik so he sees;;;","23/Jun/10 03:06;karthik.ranga;If the cluster was shutdown before the regions in transition in ZK was cleared, then it does not get assigned out on startup. Fix is to delete the UNASSIGNED znode in ZK on a new cluster start.;;;","23/Jun/10 03:19;streamy;Patch looks good.  This also fixes an existing race condition where the master node in ZK was put up before the master got the listing of regionservers.  Nothing would be blocking the RS from putting up their ephemeral nodes so it was possible that an HMaster thought it was a failover but it was a clean startup.  Test added in patch verifies that a cluster will startup even if there are unassigned znodes in zookeeper.

Running full test suite and then will commit.;;;","23/Jun/10 03:32;streamy;Confirmed with Karthik that he ran full test suite.  Committed to trunk.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regions not assigned after HBASE-2694 went in,HBASE-2757,12467422,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,tlipcon,stack,stack,20/Jun/10 02:31,20/Nov/15 12:40,14/Jul/23 06:06,22/Jun/10 00:32,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Looking into the Hudson failure, http://hudson.zones.apache.org/hudson/view/HBase/job/HBase-TRUNK/1339/ at the failing org.apache.hadoop.hbase.client.TestFromClientSide.testRegionCachePreWarm test, I seem to be seeing a case of regions being added to UNASSIGNED up in zk but then subsequently nothing.   Adjacent regions are being similarily added but these are being assigned out.  Somethings up.  It seems to be causing the above test failure.

Its hard to see in the logs.... 

Grep for this line:

    LOG.info(""Starting testRegionCachePreWarm"");

The unit test then does TEST_UTIL.createMultiRegions.

You'll see all regions being created and then they UNASSIGNED znodes are created.  Grep for the 'eee' row from testCachePrewarm table (be careful, there is also logging for testCachePrewarm2 in this log).

The last thing you'll see is:

2010-06-19 19:34:39,628 DEBUG [RegionManager.metaScanner] master.RegionManager(1006): Created UNASSIGNED zNode testCachePrewarm,eee,1276976076048.557068905bf2abfe84a5e49953a23c02. in state M2ZK_REGION_OFFLINE

For ddd and fff, you'll see those assigned out.

Maybe the test is not hanging about long enough but wonder why its skipped?",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/10 00:44;tlipcon;hbase-2757.txt;https://issues.apache.org/jira/secure/attachment/12447572/hbase-2757.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26449,Reviewed,,,,Fri Nov 20 12:40:42 UTC 2015,,,,,,,,,,"0|i0hj3j:",100356,,,,,,,,,,,,,,,,,,,,,"21/Jun/10 00:44;tlipcon;I think the issue is that we didnt wait for region assignment after creating the regions. This patch adds a countRows() which forces the regions to get assigned before we continue with the test. I'll run this on my hudson through 20 builds or so and report back.;;;","21/Jun/10 14:06;stack;Will the countrows populate the client cache undoing the premise of the failing test; i.e. that the client prefetches rows from meta?;;;","21/Jun/10 15:17;tlipcon;Yea, but I added another clearRegionCache call after the countRows to get it back to being empty;;;","22/Jun/10 00:32;tlipcon;Committed to trunk;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetaScanner.metaScan doesn't take configurations,HBASE-2756,12467420,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,20/Jun/10 00:08,20/Nov/15 12:41,14/Jul/23 06:06,21/Jun/10 17:17,,,,,,,,,,,,0.90.0,,,,,,,0,,,"HBASE-2468 added a bunch of code in MetaScanner.metaScan, and this particular line is wrong:

{code}
+    // if row is not null, we want to use the startKey of the row's region as
+    // the startRow for the meta scan.
+    if (row != null) {
+      HTable metaTable = new HTable(HConstants.META_TABLE_NAME);   <<<<<<<<<<<<<<<<<
+      Result startRowResult = metaTable.getRowOrBefore(startRow,
+          HConstants.CATALOG_FAMILY);
+      if (startRowResult == null) {
{code}

If the user specified any new configuration in his code, like ZK's parent znode, then it will miss it. This should use the HTable constructor that takes a Configuration and pass the one it already has.

I found this with my TestReplication test in HBASE-2223.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/10 00:51;jdcryans;HBASE-2756.patch;https://issues.apache.org/jira/secure/attachment/12447539/HBASE-2756.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26448,Reviewed,,,,Fri Nov 20 12:41:41 UTC 2015,,,,,,,,,,"0|i0hj3b:",100355,,,,,,,,,,,,,,,,,,,,,"20/Jun/10 00:51;jdcryans;Patch that passes the configuration object to HTable, and that adds a unit test for the multi clusters case. It also requires a fix that will be included soon in HBASE-2741.;;;","21/Jun/10 17:06;tlipcon;+1;;;","21/Jun/10 17:17;jdcryans;I committed only the fix to trunk, and I'm moving the test to HBASE-2741 (else it will add a breaking test and Stack is trying to get it green).;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate assignment of a region after region server recovery,HBASE-2755,12467417,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,streamy,kannanm,kannanm,19/Jun/10 21:29,22/Mar/11 01:34,14/Jul/23 06:06,30/Jul/10 05:02,,,,,,,,,,,,0.90.0,,master,,,,,0,,,"After a region server recovery, some regions may get assigned to duplicate region servers.

Note: I am based on a slightly older trunk (prior to the HBASE-2694). Nevertheless, I think HBASE-2694 doesn't address this case.

Scenario:

* Three region server setup (store285,286,287), with about 500 regions in the table overall.
* kill -9 and restart one of the region servers (store286).
* The 170 odd regions in the failed region server got assigned out. But two of the regions got assigned to multiple region servers.
* Looking at the log entries for one such region, it appears that there is some race condition that happens between the ProcessRegionOpen (a RegionServerOperation) and BaseScanner which causes the BaseScanner to think this region needs to be reassigned.

Relevant Logs:

Master detects that the server start message (from the restarted RS) is from a server it already knows about, but startcode is different. So, it triggers server recovery. Alternatively, the recovery will be triggered by ZNODE expiry in some cases depending on which ever event (restart of RS or Znode expiry) happens first. After that it does logs splits etc. for the failed RS; it then also removes the old region server/startcode from the deadservers map.

{code}
2010-06-17 10:26:06,420 INFO org.apache.hadoop.hbase.master.ServerManager: Server start rejected; we already have 10.138.95.182:60020 registered; existingServer=serverName=store286.xyz.com,60020,1276629467680, load=(requests=22, regions=171, usedHeap=6549, maxHeap=11993), newServer=serverName=store286.xyz.com,60020,1276795566511, load=(requests=0, regions=0, usedHeap=0, maxHeap=0)
2010-06-17 10:26:06,420 INFO org.apache.hadoop.hbase.master.ServerManager: Triggering server recovery; existingServer looks stale
2010-06-17 10:26:06,420 DEBUG org.apache.hadoop.hbase.master.ServerManager: Added=store286.xyz.com,60020,1276629467680 to dead servers, added shutdown processing operation

... split log processing...

2010-06-17 10:29:51,317 DEBUG org.apache.hadoop.hbase.master.RegionServerOperation: Removed store286.xyz.com,60020,1276629467680 from deadservers Map
{code}

What follows is the relevant log snippet for one of the regions that gets double assigned.

Master tries to assign the region to store285. 

At 10:30:20,006, in ProcessRegionOpen, we update META with information about the new assignment. However, just around the same time, BaseScanner processes this entry (at 10:30:20,009), but finds that the region is still assigned to the old region server. There have been some fixes for double assignment in BaseScanner because BaseScanner might be doing a stale read depending on when it started. But looks like there is still another hole left.

{code}
2010-06-17 10:30:10,186 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. to store285.xyz.com,60020,1276629468460

2010-06-17 10:30:11,701 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_PROCESS_OPEN: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. from store285.xyz.com,60020,1276629468460; 8 of
2010-06-17 10:30:12,800 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_PROCESS_OPEN: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. from store285.xyz.com,60020,1276629468460; 7 of
2010-06-17 10:30:13,905 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_PROCESS_OPEN: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. from store285.xyz.com,60020,1276629468460; 6 of
...
2010-06-17 10:30:20,001 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. from store285.xyz.com,60020,1276629468460; 1 of 3
2010-06-17 10:30:20,001 INFO org.apache.hadoop.hbase.master.RegionServerOperation: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. open on store285.xyz.com,60020,1276629468460
2010-06-17 10:30:20,006 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. in region .META.,,1 with startcode=1276629468460, server=store285.xyz.com:60020
2010-06-17 10:30:20,009 DEBUG org.apache.hadoop.hbase.master.BaseScanner: Current assignment of test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. is not valid;  serverAddress=store286.xyz.com:60020, startCode=1276629467680 unknown.
{code}

At this point BaseScanner calls ""this.master.getRegionManager().setUnassigned(info, true)"" to set the region to be unassigned (even though it is assigned to store285). And later, this region is given to another region server (store287).

{code}
2010-06-17 10:30:20,581 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. to store287.xyz.com,60020,1276629468678
2010-06-17 10:30:25,525 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. from store287.xyz.com,60020,1276629468678; 6 of 6
2010-06-17 10:30:25,531 INFO org.apache.hadoop.hbase.master.RegionServerOperation: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. open on store287.xyz.com,60020,1276629468678
2010-06-17 10:30:25,534 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. in region .META.,,1 with startcode=1276629468678, server=store287.xyz.com:60020
{code}


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/10 05:11;stack;bs.txt;https://issues.apache.org/jira/secure/attachment/12450793/bs.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26447,Reviewed,,,,Tue Mar 22 01:34:09 UTC 2011,,,,,,,,,,"0|i0hj33:",100354,,,,,,,,,,,,,,,,,,,,,"20/Jun/10 00:04;kannanm;The race condition is as follows:

ProcessRegionOpen (in Master) thread: In process()

Note: This is invoked on the master when a Region Server sends a
message to the master indicating it has successfully opened a region.

{code}
#a. Updates META to put newly-available region's location
#b. synchronized(master.getRegionManager()) {
      remove region from regionsInTransition list.
    }
{code}

BaseScanner Thread: In checkAssigned():

{code}
#c. Gets/reads regions current info from META (potentially stale).
    The info is stored in serverName (which the hostname/port/startcode).

#d. synchronized(master.getRegionManager()) {

      if (regionIsInTransition() ||
          regionIsFromDeadServer(serverName)) {
        return; <--- we don't come here...
      }

      storedInfo = this.master.getServerManager().getServerInfo(serverName);

      if (storedInfo == null) {
        Log(""Current assignment of <region> is not valid"");
        set region to unassigned;
      }
    }
{code}

So if sequence of events is #c, #a, #b, #d then we will end up with this
double assignment condition.

In #d, we don't early exit via the ""return"" in the first if check because the
region has already been removed from the regionsInTransition list during
step #b, and also the serverName (corresponding to the crashed region
server) has also been removed from the dead servers map much
earlier (@ 10:29:51,317).;;;","22/Jun/10 00:35;stack;Its as though we should update meta while the 'synchronized(master.getRegionManager()) {' is held... same for reading (This is probably liable to deadlock though).;;;","23/Jun/10 19:13;stack;@Jon and Karthik, you fellas consious of this one?  The 'regions-in-transition' effectively has moved to zk in your new master code.  How you going to prevent the above happening in your code base?  (In current code base, the update of meta must be done under lock on master.regionManager).  ;;;","23/Jun/10 19:15;stack;Assigning Jon to take a look..;;;","24/Jun/10 04:55;streamy;Yeah we had a conversation about this offline.  ZK-based transitions should definitely cover this.  We went through the specific scenario.  Patches on that stuff coming soon.;;;","29/Jul/10 00:25;jdcryans;Running 0.89.20100726, I hit that bug just when I started my little 8 nodes cluster (that uses a 5 nodes ZK ensemble hosted elsewhere) that has about 550 regions. When I disabled my table, 2 regions were left assigned (because they were already double assigned). One thing special about my configuration is that base scanner scans every 30 seconds instead of 60. Here's the evidence that it took the master a long time to even assign the region after figuring it was unassigned:

{noformat}
2010-07-28 15:05:18,585 DEBUG org.apache.hadoop.hbase.master.BaseScanner: Current assignment of TestTable,0030543447,1277496465702.4f30002dbe81eeb4c0a11892f4af8033. is not valid;  
serverAddress=sv2borg172:60020, startCode=1277492567399 unknown.
2010-07-28 15:05:18,586 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Creating UNASSIGNED region 4f30002dbe81eeb4c0a11892f4af8033 in state = M2ZK_REGION_OFFLINE
2010-07-28 15:05:18,588 DEBUG org.apache.hadoop.hbase.master.HMaster: Event NodeCreated with state SyncConnected with path /hbase-master/UNASSIGNED/4f30002dbe81eeb4c0a11892f4af8033
2010-07-28 15:05:18,588 DEBUG org.apache.hadoop.hbase.master.ZKMasterAddressWatcher: Got event NodeCreated with path /hbase-master/UNASSIGNED/4f30002dbe81eeb4c0a11892f4af8033
2010-07-28 15:05:18,588 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: <sv2borg169,sv2borg167,sv2borg166,sv2borg165,sv2borg164:/hbase-
master,org.apache.hadoop.hbase.master.HMaster>Created ZNode /hbase-master/UNASSIGNED/4f30002dbe81eeb4c0a11892f4af8033 in ZooKeeper
2010-07-28 15:05:18,588 DEBUG org.apache.hadoop.hbase.master.ZKUnassignedWatcher: ZK-EVENT-PROCESS: Got zkEvent NodeCreated state:SyncConnected path:/hbase-
master/UNASSIGNED/4f30002dbe81eeb4c0a11892f4af8033
2010-07-28 15:05:18,590 DEBUG org.apache.hadoop.hbase.master.RegionManager: Created/updated UNASSIGNED zNode TestTable,0030543447,1277496465702.4f30002dbe81eeb4c0a11892f4af8033.
 in state M2ZK_REGION_OFFLINE
2010-07-28 15:05:18,590 DEBUG org.apache.hadoop.hbase.master.ZKUnassignedWatcher: Got event type [ M2ZK_REGION_OFFLINE ] for region 4f30002dbe81eeb4c0a11892f4af8033
2010-07-28 15:05:44,314 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region TestTable,0030543447,1277496465702.4f30002dbe81eeb4c0a11892f4af8033. to 
sv2borg173,60020,1280354711806
{noformat}

After that, the actual updating of .META. happened almost as the same time when the BaseScanner was looking at that row and double assignment happened. This issue was always there, but it looks like assigning regions takes a lot longer than before (I've ran a lot of cluster with the base scanner scanning every 5 seconds in the past). I would guess that this is because of the new ZK calls, which are done synchronously at the moment.

I expect that anyone running with the default 60 seconds sleep but with 2-3x the number of regions will get double assignment too.;;;","29/Jul/10 00:44;jdcryans;I'm thinking that checkAssigned should check if the region is in transition before anything else. Something like:

{code}  
    throws IOException {
    synchronized (this.master.getRegionManager()) {
      boolean wasInTransition = this.master.getRegionManager().
        regionIsInTransition(info.getRegionNameAsString());
    }
    String serverName = null;
    String sa = hostnameAndPort;
    long sc = startCode;
    if (sa == null || sa.length() <= 0) {
      // Scans are sloppy.  They cache a row internally so may have data that
      // is a little stale.  Make sure that for sure this serverAddress is null.
      // We are trying to avoid double-assignments.  See hbase-1784.
      Get g = new Get(info.getRegionName());
      g.addFamily(HConstants.CATALOG_FAMILY);
      Result r = regionServer.get(meta.getRegionName(), g);
      if (r != null && !r.isEmpty()) {
        sa = getServerAddress(r);
        sc = getStartCode(r);
      }
    }
    if (sa != null && sa.length() > 0) {
      serverName = HServerInfo.getServerName(sa, sc);
    }
    HServerInfo storedInfo = null;
    synchronized (this.master.getRegionManager()) {
      /* We don't assign regions that are offline, in transition or were on
       * a dead server. Regions that were on a dead server will get reassigned
       * by ProcessServerShutdown
       */
      if (info.isOffline() || wasInTransition ||
          (serverName != null && this.master.getServerManager().isDead(serverName))) {
        return;
      }
      if (serverName != null) {
        storedInfo = this.master.getServerManager().getServerInfo(serverName);
      }

      // If we can't find the HServerInfo, then add it to the list of
      //  unassigned regions.
      if (storedInfo == null) {
        // The current assignment is invalid
        if (LOG.isDebugEnabled()) {
          LOG.debug(""Current assignment of "" + info.getRegionNameAsString() +
            "" is not valid; "" + "" serverAddress="" + sa +
            "", startCode="" + sc + "" unknown."");
        }
        // Now get the region assigned
        this.master.getRegionManager().setUnassigned(info, true);
      }
    }
{code}

You could even log if the value was different.;;;","29/Jul/10 03:22;kannanm;Was looking at your proposal. What about this scenario?

{code}
#1) Say RS1/TS1 hosting R died.
#2) META still has RS1/TS1 listed for R.
#3) Master: detects crash, asks RS2 to open R. Puts R in regionsInTransition. Removes RS1/TS1 from deadServers map.
#4) RS2 reports back that region has been opened.
#5) BaseScanner: Say, BaseScanner kicks off a scan of META
#6) Master: RS2 reports back to master than region has been opened, and Master:ProcessRegionOpen() 
    a) updates META with RS2/TS2 for R.
    b) And removes the region from regionsInTransition.
#7) BaseScanner: invokes checkAssigned(R). This will get passed RS1/TS1 since scan started before META was upda
#8) (With the proposed change) wasInTransition will still be false (because of #6b).
{code}

So it will still go ahead and assign the region to a new RS. Correct?;;;","29/Jul/10 04:35;kannanm;Posted a plausible patch here. 


Haven't uploaded formally into the JIRA, since I haven't actually test the code yet.



;;;","29/Jul/10 04:36;kannanm;Oops. Forgot to add the link:
http://pastebin.com/raw.php?i=MgY8CUBw
;;;","29/Jul/10 05:11;stack;Here's a version of Kannan's patch that applies to TRUNK.  I just tried it a few times and seems to work w/ small table (10M rows -- 65 regions) doing a few retarts and a disable/enable?  Let me make a bigger table and test.

J-D, you can sink the RC now that it looks like between Kannan and you we have a fix that will hold us over till Jon master big patch shows its face (smile);;;","29/Jul/10 17:57;jdcryans;Kannan's patch fixes the issue for me, and I also tested with a smaller base scanner sleep and twice as many regions.

Assignment seems a lot slower tho.;;;","29/Jul/10 19:10;kannanm;FYI. I got one unit test diff in my run yesterday with the patch.

{code}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hbase.regionserver.wal.TestLogRolling
-------------------------------------------------------------------------------
Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 61.669 sec <<< FAILURE!
testLogRollOnDatanodeDeath(org.apache.hadoop.hbase.regionserver.wal.TestLogRolling)  Time elapsed: 1.226 sec  <<< ERROR!
org.apache.hadoop.hbase.TableExistsException: org.apache.hadoop.hbase.TableExistsException: TestLogRolling
        at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:828)
        at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:792)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code}

I still need to see if it is related to the patch, or if the above is an unrelated intermittent issue. 

----------

With regards to assignment being a ""lot"" slower... we are doing one extra META get on assignment.
Not sure why that should make it a *lot* slower. But (re)assignments are fairly uncommon in a running system;
so this shouldn't hurt the normal codepaths at all.




;;;","29/Jul/10 21:06;kannanm;The TestHLogRolling diff is unrelated. It is happening in my env
even  without the patch. I think it is safe to ignore that as far as 
this JIRA is concerned.;;;","29/Jul/10 21:09;jdcryans;You're not hallucinating butterflies, hudson saw it too http://hudson.zones.apache.org/hudson/job/HBase-TRUNK/1411/testReport/;;;","30/Jul/10 04:56;stack;I've tested start/stop and disable/enable (though it never really works -- enable sort of does) and patch works for me.  Going to commit.  Will roll new RC.;;;","30/Jul/10 04:58;jdcryans;+1.;;;","30/Jul/10 05:02;stack;Committed to 0.89.20100726 and TRUNK.  Thanks for patch Kannan.  Thanks for review and testing J-D.;;;","22/Mar/11 01:34;stack;This was applied a while back.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseExecutorService needs to be multi-cluster friendly,HBASE-2741,12467141,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,karthik.ranga,jdcryans,jdcryans,17/Jun/10 00:04,20/Nov/15 12:40,14/Jul/23 06:06,22/Jun/10 18:30,,,,,,,,,,,,0.90.0,,,,,,,0,,,"While running TestReplication I bumped into:

{code}
2010-06-16 16:44:07,576 DEBUG [IPC Server handler 3 on 62423] master.RegionManager(357): Created UNASSIGNED zNode test,,1276731846828.de5dcd3df0fbc58207ce6ccff9ff2870. in state M2ZK_REGION_OFFLINE
2010-06-16 16:44:07,577 INFO  [RegionServer:0] regionserver.HRegionServer(511): MSG_REGION_OPEN: test,,1276731846828.de5dcd3df0fbc58207ce6ccff9ff2870.
2010-06-16 16:44:07,577 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(1358): Worker: MSG_REGION_OPEN: test,,1276731846828.de5dcd3df0fbc58207ce6ccff9ff2870.
2010-06-16 16:44:07,578 DEBUG [RegionServer:0.worker] regionserver.RSZookeeperUpdater(157): Updating ZNode /1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870 with [RS2ZK_REGION_OPENING] expected version = 0
2010-06-16 16:44:07,580 DEBUG [main-EventThread] master.HMaster(1142): Event NodeDataChanged with state SyncConnected with path /1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,580 DEBUG [main-EventThread] master.ZKMasterAddressWatcher(64): Got event NodeDataChanged with path /1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,580 DEBUG [main-EventThread] master.ZKUnassignedWatcher(71): ZK-EVENT-PROCESS: Got zkEvent NodeDataChanged state:SyncConnected path:/1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,580 INFO  [main-EventThread] regionserver.HRegionServer(379): Got ZooKeeper event, state: SyncConnected, type: NodeDataChanged, path: /1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,581 DEBUG [RegionServer:0.worker] regionserver.HRegion(294): Creating region test,,1276731846828.de5dcd3df0fbc58207ce6ccff9ff2870.
2010-06-16 16:44:07,582 DEBUG [MASTER_CLOSEREGION-master-1] handler.MasterOpenRegionHandler(70): Event = RS2ZK_REGION_OPENING, region = de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,582 DEBUG [MASTER_CLOSEREGION-master-1] handler.MasterOpenRegionHandler(81): NO-OP call to handling region opening event
2010-06-16 16:44:07,589 INFO  [RegionServer:0.worker] regionserver.HRegion(369): region test,,1276731846828.de5dcd3df0fbc58207ce6ccff9ff2870. available; sequence id is 1
2010-06-16 16:44:07,590 DEBUG [RegionServer:0.worker] regionserver.RSZookeeperUpdater(157): Updating ZNode /1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870 with [RS2ZK_REGION_OPENED] expected version = 1
2010-06-16 16:44:07,591 DEBUG [main-EventThread] master.HMaster(1142): Event NodeDataChanged with state SyncConnected with path /1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,591 DEBUG [main-EventThread] master.ZKMasterAddressWatcher(64): Got event NodeDataChanged with path /1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,592 DEBUG [main-EventThread] master.ZKUnassignedWatcher(71): ZK-EVENT-PROCESS: Got zkEvent NodeDataChanged state:SyncConnected path:/1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,591 INFO  [main-EventThread] regionserver.HRegionServer(379): Got ZooKeeper event, state: SyncConnected, type: NodeDataChanged, path: /1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,593 DEBUG [MASTER_CLOSEREGION-master-1] handler.MasterOpenRegionHandler(70): Event = RS2ZK_REGION_OPENED, region = de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,594 DEBUG [MASTER_CLOSEREGION-master-1] handler.MasterOpenRegionHandler(96): RS 10.10.1.130,62425,1276731832950 has opened region de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,594 ERROR [MASTER_CLOSEREGION-master-1] server.NIOServerCnxn$Factory$1(81): Thread Thread[MASTER_CLOSEREGION-master-1,5,main] died
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.ServerManager.processRegionOpen(ServerManager.java:607)
        at org.apache.hadoop.hbase.master.handler.MasterOpenRegionHandler.handleRegionOpenedEvent(MasterOpenRegionHandler.java:99)
        at org.apache.hadoop.hbase.master.handler.MasterOpenRegionHandler.process(MasterOpenRegionHandler.java:75)
        at org.apache.hadoop.hbase.executor.HBaseEventHandler.run(HBaseEventHandler.java:215)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:637)
{code}

This looks very new. Assigning Karthik as he was there recently.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/10 15:25;karthik.ranga;HBASE-2741-0.21.patch;https://issues.apache.org/jira/secure/attachment/12447707/HBASE-2741-0.21.patch","21/Jun/10 17:19;jdcryans;HBASE-2741-test.patch;https://issues.apache.org/jira/secure/attachment/12447609/HBASE-2741-test.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26445,Reviewed,,,,Fri Nov 20 12:40:43 UTC 2015,,,,,,,,,,"0|i0hj0f:",100342,,,,,,,,,,,,,,,,,,,,,"18/Jun/10 23:12;jdcryans;Debugging this with Karthik's help, we found out that the new HBaseExecutorService wasn't multi-cluster friendly because it was named ""master"", instead of using something less static like host:port. As a matter of fact, in my log I can also see:

{code}
2010-06-18 15:35:08,205 DEBUG [main] executor.HBaseExecutorService$HBaseExecutorServiceType(88): Executor service MASTER_CLOSEREGION already running on master
{code}

This was in fact detecting the other master's service.;;;","18/Jun/10 23:31;jdcryans;Also HBaseEventHandler has static members like serverManager, and since it's not static it means that when the second master boots up it replaces the instance with his own.;;;","21/Jun/10 17:19;jdcryans;The test I wrote in HBASE-2756, it has to be included in this jira now.;;;","21/Jun/10 17:23;jdcryans;Changing this jira's title to what it really is about.;;;","22/Jun/10 15:25;karthik.ranga;Attaching the patch file that JD already has tested...;;;","22/Jun/10 18:18;jdcryans;+1 but some nits:
 - Don't set the class attributes to null.
 - Some lines are way over 80 chars, like in ZKUnassignedWatcher

But it passes the test I wrote, going to commit with the little fixes.;;;","22/Jun/10 18:30;jdcryans;Committed to trunk, thanks for the patch Karthik!;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in ReadWriteConsistencyControl,HBASE-2740,12467140,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,ryanobjc,davelatham,davelatham,16/Jun/10 23:56,12/Oct/12 06:15,14/Jul/23 06:06,17/Jun/10 18:48,,,,,,,,,,,,0.20.5,,,,,,,0,,,"A region server running 0.20.5rc3 died with this exception:

2010-06-16 12:19:03,694 FATAL org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Replay of hlog required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: xxxxxxxxxx,1272579746382
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1041)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:896)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:262)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:149)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.ReadWriteConsistencyControl.getThreadReadPoint(ReadWriteConsistencyControl.java:40)
        at org.apache.hadoop.hbase.regionserver.MemStore$MemStoreScanner.getNext(MemStore.java:532)
        at org.apache.hadoop.hbase.regionserver.MemStore$MemStoreScanner.seek(MemStore.java:558)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:311)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.checkReseek(StoreScanner.java:297)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.peek(StoreScanner.java:143)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.updateReaders(StoreScanner.java:283)
        at org.apache.hadoop.hbase.regionserver.Store.notifyChangedReadersObservers(Store.java:621)
        at org.apache.hadoop.hbase.regionserver.Store.updateStorefiles(Store.java:607)
        at org.apache.hadoop.hbase.regionserver.Store.access$200(Store.java:88)
        at org.apache.hadoop.hbase.regionserver.Store$StoreFlusherImpl.commit(Store.java:1605)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1019)
        ... 3 more

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/10 00:21;ryanobjc;HBASE-2740.txt;https://issues.apache.org/jira/secure/attachment/12447294/HBASE-2740.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26444,,,,,Thu Jun 17 00:21:25 UTC 2010,,,,,,,,,,"0|i08sdj:",49197,,,,,,,,,,,,,,,,,,,,,"16/Jun/10 23:58;ryanobjc;this is because we are calling updateReaders() on a Scanner that was already 'reset'.  The updateReaders should guard against this condition.;;;","17/Jun/10 00:21;ryanobjc;a fix and a test for said fix;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix TestTimeRangeMapRed verification now that we store multiple versions with same TS,HBASE-2738,12467110,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,tlipcon,tlipcon,tlipcon,16/Jun/10 18:01,20/Nov/15 12:41,14/Jul/23 06:06,16/Jun/10 18:26,0.90.0,,,,,,,,,,,0.90.0,,test,,,,,0,,,"We recently introduced ""memstore timestamps"" which are internal, and allow a cell to exist twice with the same timestamp, from different puts. TestTimeRangeMapred now fails since it sees both the old and new version of the cells during verification. Setting the max versions of the verification scanner to 1 fixes it.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/10 18:03;tlipcon;hbase-2738.txt;https://issues.apache.org/jira/secure/attachment/12447245/hbase-2738.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26442,Reviewed,,,,Fri Nov 20 12:41:14 UTC 2015,,,,,,,,,,"0|i0hizz:",100340,,,,,,,,,,,,,,,,,,,,,"16/Jun/10 18:13;jdcryans;+1;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CME in ZKW introduced in HBASE-2694,HBASE-2737,12467106,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,karthik.ranga,jdcryans,jdcryans,16/Jun/10 17:50,20/Nov/15 12:43,14/Jul/23 06:06,20/Jun/10 01:02,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Saw this while tail'ing a log for something else:

{code}
2010-06-15 17:30:03,769 ERROR [main-EventThread] zookeeper.ClientCnxn$EventThread(490): Error while calling watcher
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.process(ZooKeeperWrapper.java:235)
{code}

Looks like the listeners list's iterator is used in an unprotected manner.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/10 00:00;karthik.ranga;HBASE-2737-0.21.patch;https://issues.apache.org/jira/secure/attachment/12447512/HBASE-2737-0.21.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26441,Reviewed,,,,Fri Nov 20 12:43:40 UTC 2015,,,,,,,,,,"0|i0hizr:",100339,,,,,,,,,,,,,,,,,,,,,"17/Jun/10 00:06;jdcryans;Assigning to Jon so that he takes a look.;;;","19/Jun/10 00:00;karthik.ranga;Making the register and unregister methods synchronized. Unit tests are passing. This change is so simple I am not putting it up on review board.;;;","20/Jun/10 01:02;jdcryans;I gave it a spin and everything looks fine, committed to trunk. Thanks for the patch Karthik!;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestFSErrorsExposed should catch IOE as well as RTE,HBASE-2734,12467044,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,tlipcon,tlipcon,tlipcon,15/Jun/10 23:34,20/Nov/15 12:43,14/Jul/23 06:06,15/Jun/10 23:36,0.90.0,,,,,,,,,,,0.90.0,,test,,,,,0,,,"Recently scanner construction changed such that it could throw IOE before iteration started. TestFSErrors is now failing since IOE is thrown, but the test only catches RTEs.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/10 23:34;tlipcon;hbase-2734.txt;https://issues.apache.org/jira/secure/attachment/12447184/hbase-2734.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26439,,,,,Fri Nov 20 12:43:05 UTC 2015,,,,,,,,,,"0|i0hiz3:",100336,,,,,,,,,,,,,,,,,,,,,"15/Jun/10 23:36;tlipcon;I just committed this since it's just a trivial test change and we already discussed it over in another issue.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBASE-2353 broke timestamp replacement on Puts when writeToWAL disabled,HBASE-2733,12467040,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,tlipcon,tlipcon,tlipcon,15/Jun/10 22:15,20/Nov/15 12:41,14/Jul/23 06:06,15/Jun/10 22:45,0.90.0,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"In refactoring for HBASE-2353, it ended up that updateKeys() was only called on KVs if writeToWAL was set to true. This caused failure of TestGetClosestRowBefore (though the real bug was in put)",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26438,Reviewed,,,,Fri Nov 20 12:41:02 UTC 2015,,,,,,,,,,"0|i0hiyv:",100335,,,,,,,,,,,,,,,,,,,,,"15/Jun/10 22:32;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/183/
-----------------------------------------------------------

Review request for hbase, stack and Ryan Rawson.


Summary
-------

I broke this in HBASE-2353 so it only worked with writeToWAL==true. This fixes and adds unit test. Also fixes TestGetClosestBefore


This addresses bug HBASE-2733.
    http://issues.apache.org/jira/browse/HBASE-2733


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 06e022c 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java 1c1cd4b 

Diff: http://review.hbase.org/r/183/diff


Testing
-------


Thanks,

Todd


;;;","15/Jun/10 22:34;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/183/#review232
-----------------------------------------------------------

Ship it!


lgtm


src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.hbase.org/r/183/#comment1035>

    nitpick to drop return from javadoc


- Jonathan



;;;","15/Jun/10 22:45;tlipcon;Committed, thanks for review, jgray.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"TestZooKeeper was broken, HBASE-2691 showed it",HBASE-2732,12467033,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,15/Jun/10 21:10,20/Nov/15 12:43,14/Jul/23 06:06,15/Jun/10 22:14,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Since I committed HBASE-2691, TestZooKeeper began failing. Looking at it, it's because it's setup with only 1 region server and we kill it... and it's not restarting. IIRC we were relying on that feature before but looks like we're not anymore.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/10 21:13;jdcryans;HBASE-2732.patch;https://issues.apache.org/jira/secure/attachment/12447171/HBASE-2732.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26437,Reviewed,,,,Fri Nov 20 12:43:50 UTC 2015,,,,,,,,,,"0|i0hiyn:",100334,,,,,,,,,,,,,,,,,,,,,"15/Jun/10 21:13;jdcryans;Patch that fixes the issue and cleans up a tiny bit.;;;","15/Jun/10 21:25;stack;Patch looks fine but what changed such that 2691 broke reinitialization of HRS?

;;;","15/Jun/10 21:41;jdcryans;Oh actually it's broken but not the way I was thinking. So when a session is expired we do this in the region server;

{code}
if (state == KeeperState.Expired) {
      LOG.error(""ZooKeeper session expired"");
      boolean restart =
        this.conf.getBoolean(""hbase.regionserver.restart.on.zk.expire"", false);
      if (restart) {
        restart();
      } else {
        abort(""ZooKeeper session expired"");
      }
{code}

But now we get the YouAreDeadException much sooner than the session expiration, which is good, but we don't do the restart check. Just adding it there will fix the issue. ;;;","15/Jun/10 22:04;jdcryans;So we are removing the notion of RS restart anyway, I'll just clean that up from TestZK and commit my patch above. Objections?;;;","15/Jun/10 22:14;jdcryans;Committed the patch with some more cleanup for RS restart (but not the code from HRS).;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flushCache should write to a tmp directory and then move into the store directory,HBASE-2729,12466950,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,tlipcon,tlipcon,15/Jun/10 01:56,20/Nov/15 12:40,14/Jul/23 06:06,23/Jun/10 19:07,0.90.0,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"Currently it appears that internalFlushCache writes directly to the target spot of the flushed data. The finally() block appends the metadata and closes the file as if nothing bad went wrong in case of an exception. This is really bad, since it means that an IOE in the middle of flushing cache could easily write a valid looking file with only half the data, which would then prevent us from recovering those edits during log replay.

Instead, it should flush to a tmp location and move it into the region dir only after it's successfully written.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/10 06:51;tlipcon;hbase-2729.txt;https://issues.apache.org/jira/secure/attachment/12447805/hbase-2729.txt","23/Jun/10 05:04;tlipcon;hbase-2729.txt;https://issues.apache.org/jira/secure/attachment/12447797/hbase-2729.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26435,Reviewed,,,,Fri Nov 20 12:40:54 UTC 2015,,,,,,,,,,"0|i0hiy7:",100332,,,,,,,,,,,,,,,,,,,,,"22/Jun/10 07:01;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/216/
-----------------------------------------------------------

Review request for hbase, stack and Ryan Rawson.


Summary
-------

Fixes bugs where an exception in the middle of flushing a file leaves a half-written StoreFile in the region dir, preventing that region from recovering, or, in the case of transient errors, causing silent loss of half a file's worth of data.

I also got rid of the compaction dir here, and am just using one region-wide tmp dir. Is there some reason this is a bad idea?


This addresses bug HBASE-2729.
    http://issues.apache.org/jira/browse/HBASE-2729


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1794df8 
  src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 04b7522 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java 9e5ca46 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java a65e947 

Diff: http://review.hbase.org/r/216/diff


Testing
-------

Ran TestCompaction and TestStore. Will start a cluster test running before I go to bed.


Thanks,

Todd


;;;","22/Jun/10 07:20;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/216/#review261
-----------------------------------------------------------

Ship it!


+1 (if it passes all tests).  Nit-picks below.


src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
<http://review.hbase.org/r/216/#comment1110>

    ? There was notion of a '_tmp' already?
    
    I'd say name it '.tmp'... since a '.' prefix seems to be our convention given logs dir at top-level has a '.' prefix.



src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
<http://review.hbase.org/r/216/#comment1111>

    Why not keep old name and just move dirs?  Why create a new unique name?



src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java
<http://review.hbase.org/r/216/#comment1112>

    Nice test.


- stack



;;;","22/Jun/10 07:24;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>


bq.  On 2010-06-22 00:14:05, stack wrote:
bq.  > +1 (if it passes all tests).  Nit-picks below.

Cool, I'll run it through my Hudson overnight, plus running a cluster test on my 5-node test cluster now. Will commit tomorrow midday with changes below addressed assuming testing goes OK.


bq.  On 2010-06-22 00:14:05, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/regionserver/Store.java, line 333
bq.  > <http://review.hbase.org/r/216/diff/1/?file=1529#file1529line333>
bq.  >
bq.  >     ? There was notion of a '_tmp' already?
bq.  >     
bq.  >     I'd say name it '.tmp'... since a '.' prefix seems to be our convention given logs dir at top-level has a '.' prefix.

yea, the bulk load stuff uses a tmp dir if you ask the regionserver to load a file which is stored on a different filesystem than the region itself.

I'll switch to .tmp though as you suggest (and move it into HConstants to replace the now-unused compactions.dir or what have you)


bq.  On 2010-06-22 00:14:05, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/regionserver/Store.java, line 464
bq.  > <http://review.hbase.org/r/216/diff/1/?file=1529#file1529line464>
bq.  >
bq.  >     Why not keep old name and just move dirs?  Why create a new unique name?

I was worried that what's unique in the old dir might not be unique in the new


bq.  On 2010-06-22 00:14:05, stack wrote:
bq.  > src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java, line 411
bq.  > <http://review.hbase.org/r/216/diff/1/?file=1531#file1531line411>
bq.  >
bq.  >     Nice test.

thx! at some point I'd like to combine this with the one in TestFSErrorsExposed and make a little mini-framework out of it... something for next month perhaps.


- Todd


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/216/#review261
-----------------------------------------------------------



;;;","22/Jun/10 23:02;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/216/
-----------------------------------------------------------

(Updated 2010-06-22 15:55:16.514925)


Review request for hbase, stack and Ryan Rawson.


Changes
-------

TestMasterTransitions exposed a clunker of a bug here. In the previous iteration, I had put the tmp directory in /hbase/table/.tmp instead of /hbase/table/region/.tmp - I thought region.basedir was the region dir, not the table dir. So, when a new region was opened, it removed the tmp files being written by other region servers and made the world explode.

This iteration renames those variables to be more clear, and fixes the bug so the tmp dir is within the region, not the table.


Summary
-------

Fixes bugs where an exception in the middle of flushing a file leaves a half-written StoreFile in the region dir, preventing that region from recovering, or, in the case of transient errors, causing silent loss of half a file's worth of data.

I also got rid of the compaction dir here, and am just using one region-wide tmp dir. Is there some reason this is a bad idea?


This addresses bug HBASE-2729.
    http://issues.apache.org/jira/browse/HBASE-2729


Diffs (updated)
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1794df8 
  src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 04b7522 
  src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java dc38b3b 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java 9e5ca46 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java a65e947 

Diff: http://review.hbase.org/r/216/diff


Testing
-------

Ran TestCompaction and TestStore. Will start a cluster test running before I go to bed.


Thanks,

Todd


;;;","23/Jun/10 05:04;tlipcon;Here's patch that was previously up on reviewboard (rb is down due to EC2 suckage)
One change on top of what was on reviewboard: had to update heap sizes for HRegion and Store to get tests to pass again.;;;","23/Jun/10 06:44;stack;The patch has ' ""_tmp"");' in it rather than '.tmp'.

We just going to let compaction dross pile up over life of a region?

-        doRegionCompactionCleanup();


Otherwise, patch looks good to me.  I'm +1 on commit.;;;","23/Jun/10 06:50;tlipcon;bq. The patch has ' ""_tmp"");' in it rather than '.tmp'.

oops... let me double check this is the latest patch, I thought I fixed that. Will make sure it's right before commit.

bq. We just going to let compaction dross pile up over life of a region?

I don't think a compaction should leave data in tmp except if the compaction fails, right? Looking at my load test I'm running right now, the .tmp dirs are there but they're empty, even though compactions have run.;;;","23/Jun/10 06:51;tlipcon;Aha, here is the real patch...;;;","23/Jun/10 18:45;stack;+1 (basedir -> tableDir is improvement);;;","23/Jun/10 19:07;tlipcon;Committed to 0.89.20100621 and trunk;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for HADOOP-4829,HBASE-2728,12466946,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,15/Jun/10 00:37,12/Oct/12 06:15,14/Jul/23 06:06,18/Jun/10 20:32,,,,,,,,,,,,0.20.5,,,,,,,0,,,Users who have a HADOOP-4829 patched hadoop will run into the issue that closing a RS cleanly result into data loss because the FileSystem will be closed before the regions are. Cloudera is an example. We need to support those users.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/10 22:28;jdcryans;HBASE-2728-v2.patch;https://issues.apache.org/jira/secure/attachment/12447288/HBASE-2728-v2.patch","15/Jun/10 00:39;jdcryans;HBASE-2728.patch;https://issues.apache.org/jira/secure/attachment/12447086/HBASE-2728.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26434,Reviewed,,,,Fri Jun 18 20:32:46 UTC 2010,,,,,,,,,,"0|i08sjr:",49225,,,,,,,,,,,,,,,,,,,,,"15/Jun/10 00:39;jdcryans;Patch that supports both cases.;;;","15/Jun/10 00:51;tlipcon;Only question: are we sure that that Filesystem.get is the first instantiation of the filesystem? If the filesystem is already in the Cache, then setting the conf var at this point won't do anything.;;;","15/Jun/10 00:54;jdcryans;I did look around and this is currently the case. This is in the very first steps of RS instantiation.;;;","15/Jun/10 01:03;tlipcon;Here's a thought. What if we did something like:
FileSystem fs = FileSystem.get(conf);
fs.close();
fs = FileSystem.get(conf); // now we know we have a fresh instance?

Or, if we're certain we always get the filesystem instance from this var, we could use FileSystem.newInstance?

My only nervousness is that this might break again and we wouldn't notice (since we don't seem to have any good unit tests for it);;;","15/Jun/10 23:01;stack;An issue I have is that RS needs to make it so tests can override shutdown of FS else when we kill or stop RSs during tests, they'll pull the FS out from under everything else that is currently running in a minihbasecluster.;;;","15/Jun/10 23:09;tlipcon;For tests, can't we use FileSystem.newInstance so each one gets their own?;;;","15/Jun/10 23:36;stack;We could but it'd be a lot of work to do on branch.  In trunk we do this for the newer tests.

In fact my comment is a little confused on revisit.  This is for branch, not for trunk so reviewing j-ds' patch in that light, I don't think it so bad.  The Todd suggestion of closing the fs then reopening to be sure we have a good FS instance will for sure mangle tests.  Did you test it against apache and cloudera hadoop j-d?

;;;","15/Jun/10 23:39;jdcryans;I tested my patch against both on a fully distributed 1 node setup, not with unit tests.;;;","16/Jun/10 05:35;stack;I looked at this patch again:

{code}
-      Runtime.getRuntime().removeShutdownHook(hdfsClientFinalizer);
+      boolean registered =
+          Runtime.getRuntime().removeShutdownHook(hdfsClientFinalizer);
+      if (!registered) {
+        LOG.info(""The HDFS shutdown hook isn't where we expect it, "" +
+            ""will call close during shutdown"");
+        hdfsSupportsAutoCloseDisabling = true;
+      }
{code}

In above, I'd say you should do better explaination in log message.. mention that you are going to presume fs.automatic.close is in place.

Do you think it would pay to do better introspection up earlier in this method looking for hdfsClientFinalizer explicitly in Cache -- then you'd know you have an hdfs w/ hadoop-4829 in place?  Maybe not . Maybe thats what I should do on trunk and this is good enough for branch, presuming all tests pass on both apache and cloudera?;;;","16/Jun/10 20:59;jdcryans;Running against cloudera's release (that would also be the same with any patched hdfs), TestRegionRebalancing fails. The issue is that we do something very dirty in HRS for tests: 

{code} 
  /** 
   * Set the hdfs shutdown thread to run on exit. Pass null to disable 
   * running of the shutdown test. Needed by tests. 
   * @param t Thread to run. Pass null to disable tests. 
   * @return Previous occupant of the shutdown thread position. 
   */ 
  public Thread setHDFSShutdownThreadOnExit(final Thread t) { 
    Thread old = this.hdfsShutdownThread; 
    this.hdfsShutdownThread = t; 
    return old; 
  } 
{code} 

So the tests pass t=null. but if we don't use the thread then we still shutdown HDFS. The clean solution is to set shutdownHDFS.set(false) in that method and the check already in place will do the work.;;;","16/Jun/10 22:10;stack;OK.  Sounds good.  I'm +1 on applying this to the branch (after making above changes).;;;","16/Jun/10 22:28;jdcryans;Patch that works on both releases that I will commit once 0.20.5 gets released (soon I hope).;;;","16/Jun/10 22:33;stack;+1;;;","18/Jun/10 20:32;jdcryans;Committed to branch, meaning it's in 0.20.5 since we're doing a RC4.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Splits writing one file only is untenable; need dir of recovered edits ordered by sequenceid.",HBASE-2727,12466945,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,stack,stack,14/Jun/10 23:39,20/Nov/15 12:40,14/Jul/23 06:06,16/Jul/10 22:29,,,,,,,,,,,,0.90.0,,,,,,,0,,,"This issue comes of tlipcon doing a bit of human unit testing.  His speculation is:

Let a region X deploy to server A.  Server A opens the region, then closes it.
Let region X now deploy to server B.  Server B now crashes.

Both server A and server B now have edits for region X in their WALs.

The processing of server crashes is currently sequential. 

If server A crashes before server B, server A will write out a file of recovered edits for region X but region X was not deployed on server A so, the file will just sit there unused.  The processing of server B crash will overwrite the recovered edits file written by the split of server A wal.  This is ok.

But if somehow, server B processing is done before server A's, then interesting issues will likely arise; in the main, there is danger that the server B's recovered edits could be overwritten.

Another issue comes up in the review of hbase-1025.  During the replay of edits on region deploy, if the hosting regionserver crashes before we have processed all of the recovered edits, we could lose some (the recovery of the regionserver that is replaying the edits could overwrite the log of edits only partially replayed).

Discussing up on IRC, whats needed is a directory of edits to replay ordered by sequenceid.  On recovery, we play the oldest through to the newest removing the edits only on successfully replay.

Making blocker on 0.21 since this is a correctness issue.",,hammer,kannanm,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/10 05:35;stack;2727-v2.txt;https://issues.apache.org/jira/secure/attachment/12449538/2727-v2.txt","16/Jul/10 06:05;stack;2727-v4.txt;https://issues.apache.org/jira/secure/attachment/12449633/2727-v4.txt","16/Jul/10 20:00;stack;2727-v6.txt;https://issues.apache.org/jira/secure/attachment/12449696/2727-v6.txt","16/Jul/10 22:22;stack;2727-v7.txt;https://issues.apache.org/jira/secure/attachment/12449719/2727-v7.txt",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26433,Reviewed,,,,Fri Nov 20 12:40:45 UTC 2015,,,,,,,,,,"0|i0hixz:",100331,,,,,,,,,,,,,,,,,,,,,"09/Jul/10 22:26;tlipcon;This is also really important for recovery performance.

I was doing load testing on a 5 node cluster overnight, and failure of one region server took nearly 30 minutes to recover, since replay is so much slowed by the fact that the recovering server is *rewriting* and syncing edits to a new log.;;;","15/Jul/10 05:35;stack;Here's a start.  Needs testing and unit tests.;;;","16/Jul/10 06:05;stack;This passes all tests.  Still need to add tests that crash a RS that has a R that is replaying edits from a split to ensure we do not lose edits when the R is opened subsequently.

After study, the fix for HBASE-1025, ""Reconstruction log playback has no bounds on memory used"" turns out to be not so smart.  It played recovered edits using the 'normal' Region put/delete paths with edits applied to the WAL.  Notion was that should we need to flush -- because of global memory pressure or because a region was in excess of configure memstore size -- then we'd just use the default flush mechanism.  Well, this won't work should we crash during a replay.  The default flush uses the regionserver/HLog sequenceid.  When replaying recovered edits, the effected region is not yet online; its contribution to the regionserver/hlog sequenceid has not yet been made.  Therefore, the current regionserver/hlog sequenceid could mess us up.  If it is far in excess of the recovering regions sequenceid and we crash during recovery, then on next replay, we'll skip all edits.

This patch does replay of recovered edits all in the scope of the recovering region; flushes, if they have to happen, are done using sequenceids that make sense in the context of this region only.  We don't use the regionserver/hlog sequenceid.  Should we crash during recovery, we'll go through same recovery again w/ initial sequenceid gotten from storefiles and this regions recovered edits rather than from hlog/regionserver.

;;;","16/Jul/10 20:00;stack;Added tests to demonstrate new facility whereby we can have more than one recovered edits file and that we'll replay the edits in the right order in the face of multiple edit files.

Regards the scenarios from the description above:

For 1, its highly unlikely but if for some reason we somehow process server shutdown B before we process server shutdown A and somehow, the assignment of the region in question does not happen until AFTER both server shutdowns have been processed, now its the case that wal splits will not overwrite since they'll be differently named -- named for the first sequenceid in the file -- and secondly, on replay of the edits on region deploy, we'll replay the edits from oldest to newest.

For 2, if we crash during replay of split edits, they'll be in place next time the region is deployed; we do not remove split replay edits until AFTER we've played them all and a flush has completed.

Here is commit message that gives overview on changes:

The replay of recovered edits has been changed again.  We no longer replay by calling Region#get and Region#delete and no longer add the replays to the RS WAL.  Instead we just add them to the memstore as we used to keeping account of the region memstore size.  If the memstore grows too large, we'll flush -- but NOT by using the general flush mechanism.  Instead we'll flush inline using sequenceids that make sense in the current replay context -- the sequenceids are from the regions storefiles and from its split recovered edits, NOT those of the hosting regionserver/hlog.  We need to do this to avoid case where the hosting regionserver/hlog sequenceid is somehow in excess of ours.    We don't want to add a storefile that has an inflated sequenceid in case we crash between the flush and the completion of the replay of edits (We'll miss edits on the second replay if we have a storefile w/ an excessive sequenceid).

Did other cleanup in HRegion.  We don't need to monitor minimal sequenceids in families.  That was silly.

Added ability to replay one or more recovered edits files.

In HLog, writes splts into a subdir of the region named recovered.edits rather than to a file named recovered.edits.

Added tests too.;;;","16/Jul/10 20:05;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/329/
-----------------------------------------------------------

Review request for hbase and Ryan Rawson.


Summary
-------

See notes made over in hbase-2727


This addresses bug hbase-2727.
    http://issues.apache.org/jira/browse/hbase-2727


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 40205c4 
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java 7044891 
  src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogMethods.java PRE-CREATION 
  src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java 3fff2fa 
  src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java 9053d39 

Diff: http://review.hbase.org/r/329/diff


Testing
-------

All related tests seem to pass.  A few are failing for me but seem unrelated.  Digging in while this review goes on.


Thanks,

stack


;;;","16/Jul/10 22:13;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/329/#review419
-----------------------------------------------------------

Ship it!


Brief comments:
- radix sort in the list of files
- <= on the sequence id to discard
- some trailing whitespace

+1 lgtm

- Ryan



;;;","16/Jul/10 22:22;stack;Here is what I'm going to commit.  Includes consideration of Ryans' comments (thanks for great review Ryan).;;;","16/Jul/10 22:29;stack;committed.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Shutdown hook management is gone in trunk; restore",HBASE-2725,12466939,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,stack,stack,14/Jun/10 22:20,20/Nov/15 12:41,14/Jul/23 06:06,16/Jun/10 23:32,,,,,,,,,,,,0.90.0,,,,,,,0,,,Shutdown hook handling is gone from trunk.  Investigate (I think I did this cleaning up tests but was over-enthusiastic pruning).,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/10 07:19;stack;2725.txt;https://issues.apache.org/jira/secure/attachment/12447208/2725.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26432,Reviewed,,,,Fri Nov 20 12:41:48 UTC 2015,,,,,,,,,,"0|i0hixj:",100329,,,,,,,,,,,,,,,,,,,,,"16/Jun/10 05:22;stack;I looked into this.  Here are the two relevant commits that cleaned branch of the shutdown hook.

SHA:	6feb8a5a4366c4e577d60d3626b505269e92000c
Author:	Michael Stack <stack@apache.org>
Date:	Tue Oct 06 2009 18:16:15 GMT-0700 (PDT)
Subject:	HBASE-1887 Update hbase trunk to latests on hadoop 0.21 branch so we can all test sync/append
Refs:	20091006
Parent:	c0a3fa39f6dcc7c73e1756445174bf0b681f1c34

The above got rid of all the reflection to find the hook in hdfs and instead moved us to use the fs.automatic.close that is in hadoop 0.21 (HADOOP-4829).  I see that the setting of the fs.automatic.close==false is still in place in TRUNK.

SHA:	55160fed23b974cab37d618ddde5e25a719fbd21
Author:	Michael Stack <stack@apache.org>
Date:	Tue May 18 2010 12:24:36 GMT-0700 (PDT)
Subject:	HBASE-2449 Local HBase does not stop properly
Parent:	f992c3051ea70b696086bf63769f7a8f097ba3dc

This patch just removes all of the shutdown hook handling because its intefering w/ unit tests and running in local mode w/o consideration for why we had the shutdown hook in the first place -- to run orderly shutdown of the regionserver on signal.

So, need to restore the facility where on signal, we flush memstores and close out regions nicely and then go down.  With durable hbase, we can now tolerate a kill -9 but subsequent startup will be slow as master has to split all regionserver logs.

I can't just look at the hadoop version and then do old school or new since as J-D discovered over in HBASE-2728, HADOOP-4829 is in place in the cloudera distribution (i.e. power-hadoop-0.20).;;;","16/Jun/10 07:19;stack;First cut.  Not finished yet.  Adds a ShutdownHookManager that has all the ugly stuff from interrogation of context and then the running of the hooks.;;;","16/Jun/10 22:07;stack;Looking for a reviewer;;;","16/Jun/10 22:09;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/187/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Adds a ShutdownHook class that is package private to regionserver.  Manages the running of the regionserver shutdown before running the fs shutdown hook.  New class has a main so its easy to check stuff works against the different hadoops; 0.20, 0.21 and cloudera 0.20.


This addresses bug hbase-2725.
    http://issues.apache.org/jira/browse/hbase-2725


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java fe9aa8a 
  src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java PRE-CREATION 
  src/main/java/org/apache/hadoop/hbase/regionserver/Stoppable.java PRE-CREATION 
  src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 479c661 

Diff: http://review.hbase.org/r/187/diff


Testing
-------


Thanks,

stack


;;;","16/Jun/10 22:54;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/187/#review244
-----------------------------------------------------------

Ship it!


Looks good to me, nice cleanup compared to branch!

- Jean-Daniel



;;;","16/Jun/10 23:32;stack;Committed.  Thanks for review J-D.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestFromClientSide fails for client region cache prewarm on Hudson,HBASE-2720,12466867,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,mingjielai,mingjielai,mingjielai,14/Jun/10 00:30,20/Nov/15 12:43,14/Jul/23 06:06,12/Sep/10 05:44,0.90.0,,,,,,,,,,,0.90.0,,Client,test,,,,0,,,"TestFromClientSide failed by HBASE-2468 patch: http://hudson.zones.apache.org/hudson/job/HBase-TRUNK/1322/testReport/junit/org.apache.hadoop.hbase.client/TestFromClientSide/testRegionCachePreWarm/

It seems the number of actual cached regions was less than expected (as configured) on hudson. ",hudson,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/10 14:39;stack;2720-softening.txt;https://issues.apache.org/jira/secure/attachment/12447601/2720-softening.txt","08/Sep/10 18:09;mingjielai;HBase-2720.patch;https://issues.apache.org/jira/secure/attachment/12454130/HBase-2720.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26430,Reviewed,,,,Fri Nov 20 12:43:10 UTC 2015,,,,,,,,,,"0|i0hiwn:",100325,,,,,,,,,,,,,,,,,,,,,"20/Jun/10 02:33;stack;The apache hudson failure http://hudson.zones.apache.org/hudson/view/HBase/job/HBase-TRUNK/1339/testReport/ seems to be because of HBASE-2757.  There is not enough in the meta for the cache to pre-warm itself on.;;;","21/Jun/10 14:39;stack;I committed this 'softening' of the cache test will hbase-2757 is 'fixed'.  Will leave issue open till we can go back to the more stringent version of this test (Want to get build generally passing again -- I want to play w/ new site and api generation appearing on hudson... requires passing build);;;","08/Sep/10 18:09;mingjielai;HBASE-2757 seemed to be the cause of the issue. 

I changed the test case back to the original version, and attach the patch here. 

The case passed locally, and at our local Hudson. Let's try it at Apache Hudson. 

(I didn't realized it is still open. Oops.)

;;;","12/Sep/10 05:44;stack;I tried the patch a few times here locally and it seems to work fine, committing.  Thanks Mingjie.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cached region location that went stale won't recover if asking for first row,HBASE-2712,12466759,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,11/Jun/10 18:06,12/Oct/12 06:15,14/Jul/23 06:06,11/Jun/10 20:49,0.20.4,,,,,,,,,,,0.20.5,0.90.0,,,,,,0,,,"Let's say that:

 - A client cached the location of some region, not the first one in the table
 - The RS that was holding it fails
 - The first thing the client does after the failure is trying to reach the first row of that region

This will never recover, since HCM.deleteCachedLocation doesn't delete if the row we asked for is the first row in a region. This looks a lot like HBASE-1920, but there isn't enough information in that jira to say that it's the same thing.

This is a blocker, and it kills 0.20.5 RC2 (sorry).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/10 20:07;jdcryans;HBASE-2712-trunk.patch;https://issues.apache.org/jira/secure/attachment/12446896/HBASE-2712-trunk.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26426,Reviewed,,,,Fri Jun 11 20:49:45 UTC 2010,,,,,,,,,,"0|i08siv:",49221,,,,,,,,,,,,,,,,,,,,,"11/Jun/10 18:15;streamy;This could very much be what I tripped over in HBASE-1920.  Let's keep the other jira open for now.  Good catch JD.;;;","11/Jun/10 18:45;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/170/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Patch against branch that does some refactoring and that fixes the bug at the same time. Adds a unit test that doesn't pass without the refactoring.

Urgent to review.


This addresses bug HBASE-2712.
    http://issues.apache.org/jira/browse/HBASE-2712


Diffs
-----

  /branches/0.20/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java 953796 
  /branches/0.20/src/test/org/apache/hadoop/hbase/client/TestHCM.java PRE-CREATION 

Diff: http://review.hbase.org/r/170/diff


Testing
-------


Thanks,

Jean-Daniel


;;;","11/Jun/10 18:56;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/170/#review188
-----------------------------------------------------------



/branches/0.20/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java
<http://review.hbase.org/r/170/#comment865>

    getCachedLocation does the right thing?



/branches/0.20/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java
<http://review.hbase.org/r/170/#comment864>

    rl will never be null?


- stack



;;;","11/Jun/10 18:56;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/170/#review189
-----------------------------------------------------------



/branches/0.20/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java
<http://review.hbase.org/r/170/#comment866>

    This looks like a much better approach :)
    
    Isn't there an NPE if this returns null?


- Jonathan



;;;","11/Jun/10 19:04;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>


bq.  On 2010-06-11 11:52:48, stack wrote:
bq.  > /branches/0.20/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java, line 850
bq.  > <http://review.hbase.org/r/170/diff/1/?file=1246#file1246line850>
bq.  >
bq.  >     getCachedLocation does the right thing?

Yep, that one did, so that's why we should reuse it instead.


bq.  On 2010-06-11 11:52:48, stack wrote:
bq.  > /branches/0.20/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java, line 851
bq.  > <http://review.hbase.org/r/170/diff/1/?file=1246#file1246line851>
bq.  >
bq.  >     rl will never be null?

Doh


- Jean-Daniel


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/170/#review188
-----------------------------------------------------------



;;;","11/Jun/10 19:17;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/170/
-----------------------------------------------------------

(Updated 2010-06-11 12:14:26.490410)


Review request for hbase.


Changes
-------

Fixed the potential NPE, and this one is against trunk.


Summary
-------

Patch against branch that does some refactoring and that fixes the bug at the same time. Adds a unit test that doesn't pass without the refactoring.

Urgent to review.


This addresses bug HBASE-2712.
    http://issues.apache.org/jira/browse/HBASE-2712


Diffs (updated)
-----

  /trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java 953830 
  /trunk/src/test/java/org/apache/hadoop/hbase/client/TestHCM.java PRE-CREATION 

Diff: http://review.hbase.org/r/170/diff


Testing
-------


Thanks,

Jean-Daniel


;;;","11/Jun/10 19:31;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/170/#review193
-----------------------------------------------------------

Ship it!


looks good to me

- Jonathan



;;;","11/Jun/10 20:07;jdcryans;Final patch from rb.;;;","11/Jun/10 20:49;jdcryans;Committed to branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shell should have default terminal width when detection fails,HBASE-2710,12466705,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kannanm,tlipcon,tlipcon,11/Jun/10 01:24,20/Nov/15 12:41,14/Jul/23 06:06,11/Jun/10 01:27,0.90.0,,,,,,,,,,,0.90.0,,shell,,,,,0,,,"HBASE-2632 added code to detect terminal width. On some terminals (eg emacs shell) this returns 0, which resulted in infinite loops in the formatter. Should default to 100 when we can't detect width.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/10 01:25;tlipcon;hbase-2710.txt;https://issues.apache.org/jira/secure/attachment/12446826/hbase-2710.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26425,Reviewed,,,,Fri Nov 20 12:41:58 UTC 2015,,,,,,,,,,"0|i0hivb:",100319,,,,,,,,,,,,,,,,,,,,,"11/Jun/10 01:25;tlipcon;Patch from Kannan (he sent via mail);;;","11/Jun/10 01:27;tlipcon;Committed. Thanks for patch, Kannan.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't recover from a dead ROOT server if any exceptions happens during log splitting,HBASE-2707,12466680,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,jdcryans,jdcryans,10/Jun/10 18:44,20/Nov/15 12:42,14/Jul/23 06:06,26/Jun/10 00:06,,,,,,,,,,,,0.90.0,,,,,,,0,,,"There's an almost easy way to get stuck after a RS holding ROOT dies, usually from a GC-like event. It happens frequently to my TestReplication in HBASE-2223.

Some logs:

{code}
2010-06-10 11:35:52,090 INFO  [master] wal.HLog(1175): Spliting is done. Removing old log dir hdfs://localhost:55814/user/jdcryans/.logs/10.10.1.63,55846,1276194933831
2010-06-10 11:35:52,095 WARN  [master] master.RegionServerOperationQueue(183): Failed processing: ProcessServerShutdown of 10.10.1.63,55846,1276194933831; putting onto delayed todo queue
java.io.IOException: Cannot delete: hdfs://localhost:55814/user/jdcryans/.logs/10.10.1.63,55846,1276194933831
        at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1179)
        at org.apache.hadoop.hbase.master.ProcessServerShutdown.process(ProcessServerShutdown.java:298)
        at org.apache.hadoop.hbase.master.RegionServerOperationQueue.process(RegionServerOperationQueue.java:149)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:456)
Caused by: java.io.IOException: java.io.IOException: /user/jdcryans/.logs/10.10.1.63,55846,1276194933831 is non empty
2010-06-10 11:35:52,097 DEBUG [master] master.RegionServerOperationQueue(126): -ROOT- isn't online, can't process delayedToDoQueue items
2010-06-10 11:35:53,098 DEBUG [master] master.RegionServerOperationQueue(126): -ROOT- isn't online, can't process delayedToDoQueue items
2010-06-10 11:35:53,523 INFO  [main.serverMonitor] master.ServerManager$ServerMonitor(131): 1 region servers, 1 dead, average load 14.0[10.10.1.63,55846,1276194933831]
2010-06-10 11:35:54,099 DEBUG [master] master.RegionServerOperationQueue(126): -ROOT- isn't online, can't process delayedToDoQueue items
2010-06-10 11:35:55,101 DEBUG [master] master.RegionServerOperationQueue(126): -ROOT- isn't online, can't process delayedToDoQueue items
{code}

The last lines are my own debug. Since we don't process the delayed todo if ROOT isn't online, we'll never reassign the regions. ",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2223,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/10 17:58;stack;2707-0.20.txt;https://issues.apache.org/jira/secure/attachment/12448135/2707-0.20.txt","25/Jun/10 23:58;stack;2707-test.txt;https://issues.apache.org/jira/secure/attachment/12448111/2707-test.txt","10/Jun/10 22:17;jdcryans;HBASE-2707.patch;https://issues.apache.org/jira/secure/attachment/12446804/HBASE-2707.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26424,Reviewed,,,,Fri Nov 20 12:42:39 UTC 2015,,,,,,,,,,"0|i0hiun:",100316,,,,,,,,,,,,,,,,,,,,,"10/Jun/10 22:17;jdcryans;Stack and I talked a lot about it, here's what we came up with. It's very hard for me to come up with a unit test since it's all deep in the master and very much time-based, but I tested the patch with TestReplication a lot and 1) it doesn't fail anymore and 2) I see in the logs that the master does the right thing.

Should I commit this?;;;","11/Jun/10 04:24;stack;I think we need  a test for this because i can't see how -ROOT- is recovered if the code in processervershutdown#process is like this:

{code}
    if (!rootRescanned) {
      // Scan the ROOT region
      Boolean result = new ScanRootRegion(
          new MetaRegion(master.getRegionManager().getRootRegionLocation(),
              HRegionInfo.ROOT_REGIONINFO), this.master).doWithRetries();
      if (result == null) {
        // Master is closing - give up
        return true;
      }

      if (LOG.isDebugEnabled()) {
        LOG.debug(""Process server shutdown scanning root region on "" +
          master.getRegionManager().getRootRegionLocation().getBindAddress() +
          "" finished "" + Thread.currentThread().getName());
      }
      rootRescanned = true;
    }
{code}

If the current server being processed held the -ROOT- and the first thing we do processing a shutdown of a server that held root is to clear the root location, how is the above code succeeding?

Assigning myself to mess w/ a test.;;;","11/Jun/10 04:25;stack;Oh, regards the patch, I think it definetly an improvement over what was there before (what was there before was silly -- it made this bug that J-D filed).;;;","24/Jun/10 00:18;jdcryans;I'm marking this a as blocker for HBASE-2223, my test fails sometimes without the patch. Also I must have ran my test with this patch almost 40 times (I'm almost always running it) and I never got any issue. ;;;","25/Jun/10 17:10;jdcryans;So actually the code of process is looks like:

{code}
LOG.info(""Log split complete, meta reassignment and scanning:"");
    if (this.isRootServer) {
      LOG.info(""ProcessServerShutdown reassigning ROOT region"");
      master.getRegionManager().reassignRootRegion();
      isRootServer = false;  // prevent double reassignment... heh.
    }

    for (MetaRegion metaRegion : metaRegions) {
      LOG.info(""ProcessServerShutdown setting to unassigned: "" + metaRegion.toString());
      master.getRegionManager().setUnassigned(metaRegion.getRegionInfo(), true);
    }
    // one the meta regions are online, ""forget"" about them.  Since there are explicit
    // checks below to make sure meta/root are online, this is likely to occur.
    metaRegions.clear();

    if (!rootAvailable()) {
      // Return true so that worker does not put this request back on the
      // toDoQueue.
      // rootAvailable() has already put it on the delayedToDoQueue
      return true;
    }

    if (!rootRescanned) {
      // Scan the ROOT region
      Boolean result = new ScanRootRegion(
          new MetaRegion(master.getRegionManager().getRootRegionLocation(),
              HRegionInfo.ROOT_REGIONINFO), this.master).doWithRetries();
      if (result == null) {
        // Master is closing - give up
        return true;
      }

      if (LOG.isDebugEnabled()) {
        LOG.debug(""Process server shutdown scanning root region on "" +
          master.getRegionManager().getRootRegionLocation().getBindAddress() +
          "" finished "" + Thread.currentThread().getName());
      }
      rootRescanned = true;
    }
{code}

So if the RS had -ROOT-, it will be reassigned right away and then the method returns if !rootAvailable. Later when we come back and root was assigned, process server shutdown will finish its job. This is how the code you pasted succeeds.;;;","25/Jun/10 17:28;stack;So its broken then?  We assign -ROOT- but don't recover its edits?;;;","25/Jun/10 17:35;stack;Hmm... chatted with J-D and he points out that the above runs AFTER logs are split so I had it incorrect.  Above should be good.;;;","25/Jun/10 23:58;stack;Test that puts off the processing of the shutdown of the server that was carrying root.  This test never completes.  With the patch in place, it does.;;;","26/Jun/10 00:03;jdcryans;+1 but minor nit, why isn't DELAY private?;;;","26/Jun/10 00:06;stack;Committed.  Thanks for review J-D (I removed DELAY altogether).;;;","26/Jun/10 17:58;stack;Backport to 0.20.  Please review.  I'd like this to go into 0.20 since it hoses cluster if it ever happens.;;;","26/Jun/10 17:59;stack;Moving into 0.20.6.;;;","26/Jun/10 18:26;stack;Taking out of 0.20.6.  Open separate issue.  The attached 0.20 patch is not enough.  The change would be more major than this patch presumes.;;;","15/Jul/10 23:54;stack;Latest iteration.  What is currently in place is currently broke it turns out.  More on this later.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ui not working in distributed context,HBASE-2703,12466512,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,08/Jun/10 23:42,20/Nov/15 12:41,14/Jul/23 06:06,10/Jun/10 23:23,,,,,,,,,,,,0.90.0,,,,,,,0,,,"UI is not showing when you put hbase on a cluster; this is since we renamed webapps dir as hbase-webapps.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/10 23:21;stack;2703-v2.txt;https://issues.apache.org/jira/secure/attachment/12446811/2703-v2.txt","08/Jun/10 23:45;stack;2703.txt;https://issues.apache.org/jira/secure/attachment/12446642/2703.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26423,,,,,Fri Nov 20 12:41:11 UTC 2015,,,,,,,,,,"0|i0hiu7:",100314,,,,,,,,,,,,,,,,,,,,,"08/Jun/10 23:45;stack;This patch seems to fix it. Main change is this in bin/hbase:

{code}
@@ -158,11 +158,15 @@ fi

 # For releases, add hbase & webapps to CLASSPATH
 # Webapps must come first else it messes up Jetty
-if [ -d ""$HBASE_HOME/webapps"" ]; then
+if [ -d ""$HBASE_HOME/hbase-webapps"" ]; then
   CLASSPATH=${CLASSPATH}:$HBASE_HOME
 fi
 for f in $HBASE_HOME/hbase*.jar; do
-  if [ -f $f ]; then
+  if [[ $f = *sources.jar ]]
+  then
+    : # Skip sources.jar
+  elif [ -f $f ]
+  then
     CLASSPATH=${CLASSPATH}:$f;
   fi
 done
{code}

The first change seems to fix it.  The second gets rid of a needless unjarring that was going on because sources has jsp in it.

I'm still at this.  I might add a bit more to the patch.  It looks like WEB-INF/web.xml is being generated into our src/main/resources...  rather than under target/.....  Will try fix that while in here. ;;;","09/Jun/10 16:10;kannanm;stack: yes, just ran into this! Should we just pick the patch as provided or is the other part you mention about WEB-INF/web.xml also needed?;;;","10/Jun/10 23:21;stack;M bin/hbase
+ Remove add_maven_target_dir_to_classpath.  Not used.
+ Add target/hbase-webapps to CP and fix fact that when deployed, we were looking for webapps in 'webapps' instead of in 'hbase-webapps'.  Don't include the sources.jar in CP because it included jsps under webapps dir and we were undoing the webapp from here.

M pom.xml
+ We were looking for resources to include jar in wrong place.
+ We were also generating the web.xml under srcs instead of under target (though this was broke because we no WEB-INF dir checked in.  W/o it, the web.xml was failing silently).
+ We were missing logo.
+ Now we generate web.xml, etc., into target build dir AND we include the generated web.xml stuff in the jar, rather than directly what was under src.
+ I put all the source generation into the one maven-antrun-plugin section rather than have two (version and the web stuff).  Also made them generate code both into 'generated-sources' instead of into 'generated-sources' and 'jspc' dir.  I messed around here trying to be smart doing conditionals in ant so we only did ant stuff if dirs didn't exist but ant just pissed me off.  You can't set a property in one 'task' and then condidtionally run the second 'task' only if the property is not set.  Each task starts w/ a clean environment it seems (This is a mvn issue).  Then, in ant, w/i a target, I cannot do a conditional execution.  There is an 'if' task but its in an ant contrib that looks way old and stale.  I don't want to use old stuff.  Now we run each time but seems to go fast.

Tested it in local, assembled and on a cluster.  Seems to work.  Committing.
;;;","10/Jun/10 23:23;stack;Committed.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"LeaseStillHeldException totally ignored by RS, wrongly named",HBASE-2691,12466410,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,08/Jun/10 00:41,20/Nov/15 12:41,14/Jul/23 06:06,09/Jun/10 00:11,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Currently region servers don't handle org.apache.hadoop.hbase.Leases$LeaseStillHeldException in any way that's useful so what happens right now is that it tries to report to the master and this happens:

{code}

2010-06-07 17:20:54,368 WARN  [RegionServer:0] regionserver.HRegionServer(553): Attempt=1
org.apache.hadoop.hbase.Leases$LeaseStillHeldException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:94)
        at org.apache.hadoop.hbase.RemoteExceptionHandler.checkThrowable(RemoteExceptionHandler.java:48)
        at org.apache.hadoop.hbase.RemoteExceptionHandler.checkIOException(RemoteExceptionHandler.java:66)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:541)
        at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.run(MiniHBaseCluster.java:173)
        at java.lang.Thread.run(Thread.java:637)
{code}

Then it will retry until the watch is triggered telling it that the session's expired! Instead, we should be a lot more proactive initiate abort procedure.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/10 00:09;jdcryans;HBASE-2691.patch;https://issues.apache.org/jira/secure/attachment/12446646/HBASE-2691.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26421,Reviewed,,,,Fri Nov 20 12:41:33 UTC 2015,,,,,,,,,,"0|i0hirr:",100303,,,,,,,,,,,,,,,,,,,,,"08/Jun/10 04:15;stack;On a reportForDuty, we have code that will reject HRS with lease still held BUT it'll tickle the expire-of-the-region shutdown processing.  The RS will be continually rejected until soon after the shutdown processing has gotten past its initial steps.  Then the RS is let in.

Where are you when this has happened?  Just started?  What session has expired?  The RS in ZK?;;;","08/Jun/10 04:37;jdcryans;The RS's session's expired, it reports back to the master right after that (it's marked dead in the master) and trips into:

{code}

  private void checkIsDead(final String serverName, final String what)
  throws LeaseStillHeldException {
    if (!isDead(serverName)) return;
    LOG.debug(""Server "" + what + "" rejected; currently processing "" +
      serverName + "" as dead server"");
    throw new Leases.LeaseStillHeldException(serverName);
  }
{code}

Which I see in the log. then on the HRS side this falls into:

{code}

          } catch (Exception e) { // FindBugs REC_CATCH_EXCEPTION
            if (e instanceof IOException) {
              e = RemoteExceptionHandler.checkIOException((IOException) e);
            }
            tries++;
            if (tries > 0 && (tries % this.numRetries) == 0) {
              // Check filesystem every so often.
              checkFileSystem();
            }
            if (this.stopRequested.get()) {
              LOG.info(""Stop requested, clearing toDo despite exception"");
              toDo.clear();
              continue;
            }
              LOG.warn(""Attempt="" + tries, e);
            // No point retrying immediately; this is probably connection to
            // master issue.  Doing below will cause us to sleep.
            lastMsg = System.currentTimeMillis();
{code}

Which throws the stack trace I pasted in this jira's description. IMO, and taking into account the last comment in that code, we shouldn't retry. Instead, we should catch LeaseStillHeldException separately from this big catch(Exception) and treat it as an emergency shut down.;;;","08/Jun/10 05:36;stack;How you going to tell difference between LeaseStillHeldException thrown when we're processing shutdown of a RS that was on the same host and port as this RS? (The scenario is the RS fails and is restarted quickly, so fast, it checks in at master before master even knows it dead).;;;","08/Jun/10 17:21;jdcryans;This would be the second part of the fix, as referred to in the title of this jira, that using Leases.LeaseStillHeldException is wrong. Before ZK, we were using Leases for the region server registrations in the master but now we only use its exception. Instead, I think we could have finer grained and more relevant exceptions. What about ServerAlreadyExistingException and ServerAlreadyConsideredDeadException? (I'm not good at naming stuff);;;","08/Jun/10 17:30;stack;.bq What about ServerAlreadyExistingException and ServerAlreadyConsideredDeadException? (I'm not good at naming stuff)

Doing above and purging LeaseStillHeldException as you suggest is a good idea.  It solves differentiating the different startup/dead-server circumstances.

Regards naming, they ain't too bad.  The latter could be YouAreDeadException (with its message holding info on why its considered dead).  The former could be PleaseHoldException (its message would be why the holdup).;;;","08/Jun/10 23:07;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/164/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Patch that first adds a new testing facility to send exceptions to region servers when they report in. 
Then I added YouAreDeadException and PleaseHoldException that replaces the ambiguous lease still held exception, and both are treated differently.
Finally, I added a very simple test that makes sure that telling a RS that its dead will actually end up in killing it.


This addresses bug HBASE-2691.


Diffs
-----

  /trunk/src/main/java/org/apache/hadoop/hbase/PleaseHoldException.java PRE-CREATION 
  /trunk/src/main/java/org/apache/hadoop/hbase/YouAreDeadException.java PRE-CREATION 
  /trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 952836 
  /trunk/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java 952836 
  /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 952836 
  /trunk/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java 952836 
  /trunk/src/test/java/org/apache/hadoop/hbase/TestYouAreDead.java PRE-CREATION 

Diff: http://review.hbase.org/r/164/diff


Testing
-------


Thanks,

Jean-Daniel


;;;","08/Jun/10 23:22;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/164/#review160
-----------------------------------------------------------

Ship it!


+1 after making the mostly javadoc changes suggested below.


/trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.hbase.org/r/164/#comment793>

    Javadoc missing here... you need to explain the bit about how you can send a exception back to RS



/trunk/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
<http://review.hbase.org/r/164/#comment794>

    Fatten the message passed back to the regionserver.



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
<http://review.hbase.org/r/164/#comment795>

    Not needed (after chatting w/ j-d)



/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
<http://review.hbase.org/r/164/#comment796>

    Same here



/trunk/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
<http://review.hbase.org/r/164/#comment797>

    Remove



/trunk/src/test/java/org/apache/hadoop/hbase/TestYouAreDead.java
<http://review.hbase.org/r/164/#comment799>

    These single test classes are killing us...regards overall unit tests times... can we not put it into an existing test class....?


- stack



;;;","08/Jun/10 23:56;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>


bq.  On 2010-06-08 16:18:09, stack wrote:
bq.  > /trunk/src/test/java/org/apache/hadoop/hbase/TestYouAreDead.java, line 27
bq.  > <http://review.hbase.org/r/164/diff/1/?file=1103#file1103line27>
bq.  >
bq.  >     These single test classes are killing us...regards overall unit tests times... can we not put it into an existing test class....?

Will be all merged into TestMasterWrongRS, that I rename to TestKillingServersFromMaster


- Jean-Daniel


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/164/#review160
-----------------------------------------------------------



;;;","09/Jun/10 00:09;jdcryans;Patch that will be committed.;;;","09/Jun/10 00:11;jdcryans;Committed to trunk but not to branch, I think it was a bit invasive.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestMasterWrongRS flaky in trunk,HBASE-2684,12466386,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,tlipcon,tlipcon,07/Jun/10 21:54,20/Nov/15 12:42,14/Jul/23 06:06,08/Jun/10 22:00,0.90.0,,,,,,,,,,,0.90.0,,test,,,,,0,,,"I think this is just a flaky test. I saw:

java.lang.AssertionError: expected:<2> but was:<3>
on the first:
    assertEquals(2, cluster.getLiveRegionServerThreads().size());

My guess is that the 2 second sleep is not good enough. We should probably either force a heartbeat somehow, or hook in so we can wait until there's been a heartbeat, rather than sleeping a hardcoded amount of time.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/10 22:00;jdcryans;HBASE-2684.patch;https://issues.apache.org/jira/secure/attachment/12446630/HBASE-2684.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26419,Reviewed,,,,Fri Nov 20 12:42:48 UTC 2015,,,,,,,,,,"0|i0hirj:",100302,,,,,,,,,,,,,,,,,,,,,"08/Jun/10 21:30;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/163/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Patch that uses MiniHBaseCluster.waitOnRegionServer instead of sleeping, and adds a timeout to the test.


This addresses bug HBASE-2684.


Diffs
-----

  /trunk/src/test/java/org/apache/hadoop/hbase/master/TestMasterWrongRS.java 952744 

Diff: http://review.hbase.org/r/163/diff


Testing
-------


Thanks,

Jean-Daniel


;;;","08/Jun/10 21:45;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/163/#review158
-----------------------------------------------------------

Ship it!


+1 after making below suggested fix.


/trunk/src/test/java/org/apache/hadoop/hbase/master/TestMasterWrongRS.java
<http://review.hbase.org/r/163/#comment791>

    This looks wrong.  Shouldn't the index be '1' for the second server?  Its the index you used at start of the test to get reference to seconServer.
    
    Otherwise, looks good.. go ahead and commit


- stack



;;;","08/Jun/10 21:57;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>


bq.  On 2010-06-08 14:40:49, stack wrote:
bq.  > /trunk/src/test/java/org/apache/hadoop/hbase/master/TestMasterWrongRS.java, line 71
bq.  > <http://review.hbase.org/r/163/diff/2/?file=1096#file1096line71>
bq.  >
bq.  >     This looks wrong.  Shouldn't the index be '1' for the second server?  Its the index you used at start of the test to get reference to seconServer.
bq.  >     
bq.  >     Otherwise, looks good.. go ahead and commit

After chatting with Stack and looking at the code, it appears I'm actually doing the right thing (he had another method in mind that did not remove the region server from the list).


- Jean-Daniel


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/163/#review158
-----------------------------------------------------------



;;;","08/Jun/10 22:00;jdcryans;Patch I'm committing.;;;","08/Jun/10 22:00;jdcryans;Committed to trunk.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make it obvious in the documentation that ZooKeeper needs permanent storage,HBASE-2683,12466376,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,07/Jun/10 19:52,12/Oct/12 06:15,14/Jul/23 06:06,18/Jun/10 21:30,,,,,,,,,,,,0.20.5,0.90.0,,,,,,0,,,"If our users let HBase manage ZK, they probably won't bother combing through hbase-default.xml to figure that they need to set hbase.zookeeper.property.dataDir to something else than /tmp. It probably happened to deinspanjer in prod today and that's a show stopper.

The fix would be, at least, to improve the Getting Started documentation to include that configuration in the ""Fully-Distributed Operation"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26418,,,,,Fri Jun 18 21:30:46 UTC 2010,,,,,,,,,,"0|i08shr:",49216,,,,,,,,,,,,,,,,,,,,,"18/Jun/10 21:30;jdcryans;Committed a small paragraph to branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MemStore should retain multiple KVs with the same timestamp when memstoreTS differs,HBASE-2670,12466239,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,tlipcon,tlipcon,tlipcon,05/Jun/10 04:43,20/Nov/15 12:42,14/Jul/23 06:06,15/Jun/10 22:38,0.20.5,0.90.0,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"There appears to be a bug in HBASE-2248 as committed to trunk. See following failing test:
http://hudson.zones.apache.org/hudson/job/HBase-TRUNK/1296/testReport/junit/org.apache.hadoop.hbase/TestAcidGuarantees/testAtomicity/
Think this is the same bug we saw early on in 2248 in the 0.20 branch, looks like the fix didn't make it over.",,jmhsieh,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-4570,,,,HBASE-2616,HBASE-2474,,,,,,,,,,,,"15/Jun/10 22:35;tlipcon;hbase-2670.txt;https://issues.apache.org/jira/secure/attachment/12447180/hbase-2670.txt","10/Jun/10 22:07;tlipcon;hbase-2670.txt;https://issues.apache.org/jira/secure/attachment/12446800/hbase-2670.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26415,Reviewed,,,,Fri Nov 20 12:42:58 UTC 2015,,,,,,,,,,"0|i0hipb:",100292,,,,,,,,,,,,,,,,,,,,,"08/Jun/10 03:05;tlipcon;The issue is that the memstore timestamps are lost once we flush the memstore to an HFile, and we immediately change over open scanners to scan from the HFile in updateReaders()

I think the fix is one of the following:
- in updateReaders, scan ahead the memstore reader until the next row, and cache those KVs internally. Then when we hit the end of the cache, do the actual reseek in the HFile at the begining of the next row.
- in updateReaders, simply mark a flag that we need to update as soon as we hit the next row. Then do the reseek lazily in next();;;","08/Jun/10 03:30;tlipcon;Actually, I misunderstood this a little bit. The above describes the issue when using intra-row scanning (see HBASE-2673). Without intra-row scanning, since updateReaders is synchronized, it should only be called when the StoreScanner is between rows.

So I think the issue may be how updateReaders itself works. It uses peak() on the heap to find the next row it's going to, and then seeks to that one. updateReaders, though, is called by a different thread with a different readpoint set. So that seek pulls in values for the next row that are different from what will be read.

I'll try to make a patch for this tomorrow.;;;","08/Jun/10 03:33;ryanobjc;I'm committing a fix to StoreScanner that will do essentially ""#2"" in your option list.  ;;;","08/Jun/10 03:35;tlipcon;How about you post a patch for review instead of just committing? This stuff is bug prone, we should do code reviews.;;;","08/Jun/10 20:17;ryanobjc;#1 is the old memstore scanner - it would get an 'entire row' at a time and then chomp on it's internal array.  This made memstore scans really slow.  The whole index hbase saga.

#2 workish, and is what is implemented in HBASE-2616 (which is now in branch and trunk).

The problem with #2, and you will see it in HRegionScanner is we no longer update the read point ""between rows"".  Since by the time we figure out we are on the next row, we've already peek()ed the value off the scanner, we cant switch to a different read point, or else you will get the first KeyValue from one read point, and the rest from another (sounds familiar?)

As for why did I commit and not await review, this is because the hudson instance at hudson.hbase.org is one of the few machines that trips against this failure frequently.  I haven't been able to repro on my dev environment, ever.    And now we have 11 successful branch builds of 0.20, thus indicating that our bug is now fixed.;;;","08/Jun/10 20:35;tlipcon;This isn't fixed in trunk, see: http://hudson.zones.apache.org/hudson/job/HBase-TRUNK/1308/
Specifically:
http://hudson.zones.apache.org/hudson/job/HBase-TRUNK/1308/testReport/org.apache.hadoop.hbase/TestAcidGuarantees/testAtomicity/;;;","08/Jun/10 21:45;tlipcon;Still not fixed after your recent patch, plus it broke two other tests in trunk. Specifically, getScanner() on the client now throws IOExceptions eagerly instead of on the first .next() call, so TestFSErrorsExposed failed. It's an easy test fix, but it seems strange to me that getScanner() could throw IOE before you even start to iterate on it?;;;","09/Jun/10 00:19;tlipcon;I think I'm understanding the issue here. Here's my theory:

When we reseek, we reseek to the keyvalue that was previously the ""next"" in the scanner by way of peeking before reseeking. However, the peek takes into account the old read point, so we get a situation where our peek sees the old version of a column that has multiple versions, and we seek there. Then when we seek forward from there, we see the rows with newer timestamps because we're no longer restricted by the old read point.

I'm working on a test case and patch.;;;","09/Jun/10 00:28;ryanobjc;In This case the fix should be to reseek to firstonrow of the peeked row. We
are guarenteed to be between rows so this works.

https://issues.apache.org/jira/browse/HBASE-2670?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12876897#action_12876897]
in the scanner by way of peeking before reseeking. However, the peek takes
into account the old read point, so we get a situation where our peek sees
the old version of a column that has multiple versions, and we seek there.
Then when we seek forward from there, we see the rows with newer timestamps
because we're no longer restricted by the old read point.
following failing test:
http://hudson.zones.apache.org/hudson/job/HBase-TRUNK/1296/testReport/junit/org.apache.hadoop.hbase/TestAcidGuarantees/testAtomicity/
looks like the fix didn't make it over.
;;;","09/Jun/10 00:30;tlipcon;We're not guaranteed to be between rows in trunk due to intra-row scanning, so we need a special flag to tell us when we should be between rows.

I'm right now running TestAcidGuarantees against 0.20 to see if this exists in 0.20.5 rcs.;;;","09/Jun/10 00:41;tlipcon;Yep, this is in branch too. Anyone mind if I commit the TestAcidGuarantees case to the 20 branch?
Going to continue to work on a fix and a more targeted test case that fails even on laptops.;;;","09/Jun/10 01:52;ryanobjc;On the subject of getScanner() failing, that is how it used to work
before some of the 2248 related fixes went it.  I don't think we want
to work too hard to move the IOExceptions around, you are going to
have to be able to handle the exception at any time.




;;;","10/Jun/10 22:07;tlipcon;It turns out that the majority of this issue was that the comparator used by MemStore didn't take into account the ""memstore logical timestamp"" (memstoreTS). Thus, a new writer could overwrite older entries if it did a write in the same millisecond. A concurrent reader would then see a partial row because the new entries would be invisible, thus ""revealing"" cells with a lower ""real"" TS.

This patch adds two tests to TestMemStore that check for the correct behavior.

I believe there is still a separate issue with multi-row scans and updateReaders(), but I'll open a separate JIRA with separate tests for that one.;;;","11/Jun/10 01:40;tlipcon;This patch causes issues with the incrementColumnValues() tests, which rely on updating a KV in-place in the memstore.

Not sure what we should do here... on one hand it would be nice for incrementColumnValue to have proper semantics, on the other hand, we don't want to insert a new KV for every increment. Ryan, any thoughts?;;;","11/Jun/10 21:32;ryanobjc;ICVs dont take part in this new concurrency system.  They are not actually being 'updated in place' but KeyValues are being 'overwritten'.  Given there is only 1 row/column involved in an ICV, there is no multi-KeyValue coherency to worry about here.   ICVs use memstoreTS==0 even (and are always included in every scan).

In the future I think it might be possible to insert a new KV then delete the old one out of memstore... that shouldn't cause any issues with scanners, but we can do that later.;;;","11/Jun/10 21:35;tlipcon;K, makes sense, I'll update the patch to ensure that the ICVs maintain equal memstoreTS (it seems in the current system they're getting double-inserted).;;;","11/Jun/10 21:40;ryanobjc;as per our discussion there are several unresolved issues with this patch:

- apparently ICVs (how? why? where?)
- extra KeyValues in the flush that would have previously been combined
- lets not add new client dependencies;;;","11/Jun/10 21:42;tlipcon;bq. extra KeyValues in the flush that would have previously been combined

I thought we determined this was fine, since we already have potential duplicate keyvalue issues between separate HFiles, and this isn't any better/worse?

bq. lets not add new client dependencies

I don't think this patch changes anything client side.

Will work on the ICV thing this afternoon.;;;","11/Jun/10 21:55;ryanobjc;unfortunately 'KeyValue' is part of the the Client API.




;;;","11/Jun/10 22:20;tlipcon;ah, crap, I forgot that. Can we discuss the client side dependencies issue over in HBASE-2714? (this isn't a regression, we already use guava in a couple other places in the client).;;;","12/Jun/10 04:33;stack;.bq Can we discuss the client side dependencies issue over in HBASE-2714?

Yes;;;","14/Jun/10 23:28;tlipcon;Changing name of this JIRA to reflect the specific issue I'm solving with the patch. Will open a second JIRA for the other atomicity issue regarding reseeks.;;;","14/Jun/10 23:35;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/180/
-----------------------------------------------------------

Review request for hbase and Ryan Rawson.


Summary
-------

This changes the memstore comparator, improves the atomicity tests, and also fixes ICV to continue to have the right behavior even with the comparator change.


This addresses bug hbase-2670.
    http://issues.apache.org/jira/browse/hbase-2670


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/KeyValue.java 71284cf 
  src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 2a0dcee 
  src/test/java/org/apache/hadoop/hbase/MultithreadedTestUtil.java 7c062d7 
  src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 75f3c8b 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestMemStore.java 9833d76 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java 0566af7 

Diff: http://review.hbase.org/r/180/diff


Testing
-------

Unit tests.


Thanks,

Todd


;;;","15/Jun/10 22:06;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/180/#review229
-----------------------------------------------------------

Ship it!


+1

I some minors in the below and then some questions but these should not get in way of a commit.


src/main/java/org/apache/hadoop/hbase/KeyValue.java
<http://review.hbase.org/r/180/#comment1016>

    Is this going to be correct always?  Cloning, we don't want the src memstoreTS?



src/main/java/org/apache/hadoop/hbase/KeyValue.java
<http://review.hbase.org/r/180/#comment1017>

    What we going to do about N versions all of same r/f/q/ts but of different memstoreTS?  We're not going to suppress them just yet?  We're going to punt till hbase-1485?  The multiple versions make it out to store files too?



src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java
<http://review.hbase.org/r/180/#comment1018>

    Prefix w/ 'TODO' to make this work-to-do more findable.



src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java
<http://review.hbase.org/r/180/#comment1019>

    Nice



src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java
<http://review.hbase.org/r/180/#comment1020>

    Go ahead and remove then boss.


- stack



;;;","15/Jun/10 22:35;tlipcon;New patch addresses comments from stack's review - it was just clarification of some comments in the code, so I'll go ahead and commit. Thanks Stack.;;;","15/Jun/10 22:36;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>


bq.  On 2010-06-15 15:03:40, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/KeyValue.java, line 558
bq.  > <http://review.hbase.org/r/180/diff/1/?file=1345#file1345line558>
bq.  >
bq.  >     Is this going to be correct always?  Cloning, we don't want the src memstoreTS?

yea, this clone is necessary so that incrementColumnValue() doesn't blow up to create a new memstore entry for every increment. We could put it in that code, but I think it's cleaner to just be part of clone. I think it's best in clone so that a foo.compareTo(foo.clone()) == 0 as an invariant.


bq.  On 2010-06-15 15:03:40, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/KeyValue.java, line 1306
bq.  > <http://review.hbase.org/r/180/diff/1/?file=1345#file1345line1306>
bq.  >
bq.  >     What we going to do about N versions all of same r/f/q/ts but of different memstoreTS?  We're not going to suppress them just yet?  We're going to punt till hbase-1485?  The multiple versions make it out to store files too?

Yea, leaving that off since the problem already exists and it's a bit of a heavy change.


bq.  On 2010-06-15 15:03:40, stack wrote:
bq.  > src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java, line 140
bq.  > <http://review.hbase.org/r/180/diff/1/?file=1348#file1348line140>
bq.  >
bq.  >     Prefix w/ 'TODO' to make this work-to-do more findable.

ah, I mean that we don't need to verify at this point because the writer hasn't actually written any rows yet! will clarify


- Todd


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/180/#review229
-----------------------------------------------------------



;;;","15/Jun/10 22:38;tlipcon;Committed to trunk.;;;","16/Jun/10 01:11;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/180/#review235
-----------------------------------------------------------

Ship it!


ok looks good.  I'm also prepping a new way to do ICVs in a different issue (HBASE-2501).


src/main/java/org/apache/hadoop/hbase/KeyValue.java
<http://review.hbase.org/r/180/#comment1066>

    the extra KVs will make it out to hfile, but considering the problem I just don't see a better solution right now. Perhaps collapsing extra version during the flush, but then the question is _which_ extra versions?
    
    Maybe this will lead us to using an extra timestamp in every KeyValue on disk and in memory, but I hope not.


- Ryan



;;;","16/Jun/10 04:18;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-06-15 18:06:09, Ryan Rawson wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/KeyValue.java, line 1306
bq.  > <http://review.hbase.org/r/180/diff/1/?file=1345#file1345line1306>
bq.  >
bq.  >     the extra KVs will make it out to hfile, but considering the problem I just don't see a better solution right now. Perhaps collapsing extra version during the flush, but then the question is _which_ extra versions?
bq.  >     
bq.  >     Maybe this will lead us to using an extra timestamp in every KeyValue on disk and in memory, but I hope not.

.bq ...but then the question is _which_ extra versions?

For now, I'd think we'd keep most recently written.

.bq Maybe this will lead us to using an extra timestamp in every KeyValue on disk and in memory

Maybe.  When we tease out deletes and how ts's are meant to work then we'll learn whether or not they are needed I'd imagine.


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/180/#review235
-----------------------------------------------------------



;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HCM.shutdownHook causes data loss with hbase.client.write.buffer != 0,HBASE-2669,12466236,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,tsuna,tsuna,05/Jun/10 02:21,20/Nov/15 12:44,14/Jul/23 06:06,18/Oct/10 23:58,,,,,,,,,,,,0.90.0,,Client,,,,,0,,,"In my application I set {{hbase.client.write.buffer}} to a reasonably small value (roughly 64 edits) in order to try to batch a few {{Put}} together before talking to HBase.  When my application does a graceful shutdown, I call {{HTable#flushCommits}} in order to flush any pending change to HBase.  I want to do the same thing when I get a {{SIGTERM}} by using {{Runtime#addShutdownHook}} but this is impossible since {{HConnectionManager}} already registers a shutdown hook that invokes {{HConnectionManager#deleteAllConnections}}.  This static method closes all the connections to HBase and then all connections to ZooKeeper.  Because all shutdown hooks run in parallel, my hook will attempt to flush edits while connections are getting closed.

There is no way to guarantee the order in which the hooks will execute, so I propose that we remove the hook in the HCM altogether and provide some user-visible API they call in their own hook after they're done flushing their stuff, if they really want to do a graceful shutdown.  I expect that a lot of users won't use a hook though, otherwise this issue would have cropped up already.  For those users, connections won't get ""gracefully"" terminated, but I don't think that would be a problem since the underlying TCP socket will get closed by the OS anyway, so things like ZooKeeper and such should realize that the connection has been terminated and assume the client is gone, and do the necessary clean-up on their side.

An alternate fix would be to leave the hook in place by default but keep a reference to it and add a user-visible API to be able to un-register the hook.  I find this ugly.

Thoughts?",,larsfrancke,tsuna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2952,,,,,,,,,,,,,,,,,"18/Oct/10 22:23;stack;2669-v2.txt;https://issues.apache.org/jira/secure/attachment/12457494/2669-v2.txt","28/Jul/10 17:01;stack;2669.txt;https://issues.apache.org/jira/secure/attachment/12450726/2669.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26414,Reviewed,,,,Fri Nov 20 12:44:05 UTC 2015,,,,,,,,,,"0|i0hip3:",100291,,,,,,,,,,,,data loss,,,,,,,,,"05/Jun/10 16:18;stack;In old days, we had similar prob w/ hdfs.  We wanted to run a shutdown cleanup of hbase  hook but hdfs would be running its clean up at same time and we couldn't guarantee order.

Using reflection, we looked for hdsf hook, if present, unregistered it but kept a reference and then in our shutdown hook, after was done, we'd call the hdfs one.  Lets fix this benoit.  Mind if I move it out of 0.20.5 though?  Its a prob. but not end of world and I'd like to get a 0.20.5 rolled today.  Thanks.;;;","05/Jun/10 16:19;stack;hmm... well yeah, its pretty critical prob.  want to work on this today then?;;;","05/Jun/10 20:33;stack;Moving out of 0.20.5.  There is a workaround for now.  See suppressHdfsShutdownHook in HRegionServer for how to remove the installed HCM shutdown hook and install your own instead... running the HCM one after yours.   Its ugly but it works.  While the implication is dataloss if you SIGTERM the hosting application, this is sort of a new request/feature, or putting it another way, its a problem we've had for a long time, not particular to 0.20.5.  I don't thnk it a blocker.  Sink the coming RC if you think otherwise Benoit by voting against it but for now, I'd like to show there is movement around 0.20.5 release by posting a new RC. ;;;","05/Jun/10 20:58;stack;This shutdown hook registration is buried way deap.  Its way down in HCM and its registered on static initialization.  You can't really get an HCM legitimately, not w/o doing some instanceof and casting of an HConnection returned when you do a getConnection.   What would you suggest Benoit?  The HRS#suppressHdfsShutdownHook looks pretty good to me compared to exposing HCM and letting user get at the shutdown hook that way?;;;","05/Jun/10 23:07;tsuna;As you wrote Stack, there's no easy way to get to the HCM's hook.  First of all, the HCM itself doesn't retain a reference to it, so right now it's just impossible to reach.  Even if a reference was retained, the {{suppressHdfsShutdownHook}} hack in {{HRegionServer}} seems too ugly and fragile to me.  We don't need to expose this internal implementation detail of the HCM to user.  Instead we can just have a static method in {{HTable}} that the user can call to perform a graceful shutdown.

But I also doubt this hook is useful at all.  As I said, I'm not sure we need to properly close the connections ourselves.  The OS will take care of them anyway, and whatever the client was talking to will be notified that the socket was closed on the other side.  Maybe we can just remove the hook altogether and get away with it.

BTW, sorry I didn't mean to close 0.20.5 with this issue, I'm working with {{trunk}} so this has nothing to do with 0.20.5 anyway.  I also noticed that {{suppressHdfsShutdownHook}} has gone away in trunk.  Not sure why, but that's a good thing.;;;","28/Jul/10 15:14;hector.izquierdo;An ugly workaround:

private Thread getEvilHTableShutdownHook() throws Exception {

        Class clazz = Class.forName(""java.lang.ApplicationShutdownHooks"");

        Field[] fields = clazz.getDeclaredFields();

        for (Field field : fields) {

            if (field.getType() == IdentityHashMap.class) {
                field.setAccessible(true);
                IdentityHashMap<Thread, Thread> hooks = (IdentityHashMap<Thread, Thread>) field.get(null);

                for (Map.Entry<Thread, Thread> entries : hooks.entrySet()) {
                    System.out.println(entries.getKey().getName());
                    if (entries.getValue().getName().equals(""HCM.shutdownHook"")) {
                        return entries.getValue();
                    }
                }

            }
        }

        return null;
    };;;","28/Jul/10 16:54;stack;Nice trick Héctor (/me puts it in backpocket)

HBASE-1999 added the shutdown hook to get around noise in zk logs about expired sessions during the running of big MR jobs.  I thought it was needed for unit tests but I see that tests explicitly call HCM#deleteAllConnections when shutting down minicluster.  Running patch with the shutdown hook removed seems fine.;;;","28/Jul/10 17:01;stack;How about this Beniôt?  It removes the shutdown hook and then instead inside in TOF, on close, it does explicit HConnectionManager.deleteAllConnections(true);  I haven't tried it yet.  If you think it way to go, I'll try it doing MR to see if it beaks anything.;;;","02/Aug/10 21:44;tsuna;I'm OK with your patch stack, thanks for putting it together.

Minor stylistic nit: it's not clear to me why the import of {{MetaScanner.MetaScannerVisitor}} has moved around in {{HMaster.java}}, lines are now out of order.  Plus, I don't think {{HMaster.java}} should be changed as part of this issue, as this issue has nothing to do with the master.;;;","25/Aug/10 00:29;jdcryans;You want to commit this Stack?;;;","17/Oct/10 04:46;stack;Let me fix up this patch and make it apply to trunk.  I think its general drift is fine.  Whats missing now is a bunch of explaination of how Connections work and are shared -- of how the sharing is keyed by Configuration and of how if you want a clean shutdown of your tables, then you will need to do the ugly HConnectionManager.deleteConnection stuff for now, in 0.90, at least.

Running tests.;;;","18/Oct/10 22:23;stack;Version 2.  In this version, we add more explicit closeups of HConnection and then add a bunch of javadoc explaining how HConnection works and how its cleanup is done.

{code}
A set of changes that allow doing away with shutdown hook in client.

M src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java
  Removed unused import and changed message from info to debug
  -- when info it shows in shell whenever we run a command.
M src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
  Changed message from info to debug -- when info it shows in shell
  whenever we run a command.
M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
  Changed order; online region before we tell everyone where the
  region is (I changed this order recently but reviewing comments
  and issues I can't figure why I did it -- I think there was a
  reason but can't recall so just put this back until we trip
  over the issue again.  My change made it so that we had
  strange issue where we'd get a NSRE though the region was coming
  up here on this server... rather than do retries of NSREs,
  put it into online servers before updating zk... again).
M src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  Removed noisy message
M src/main/java/org/apache/hadoop/hbase/master/LogCleanerDelegate.java
  Have this interface implement Stoppable... Some implemenations
  need their Stop called.
M src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java
  Startup wont work w/o this change.
M src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java
  Wrap the chore run so we can call the stop on all log cleaners.
M src/main/java/org/apache/hadoop/hbase/master/TimeToLiveLogCleaner.java
M src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java
  Implement Stoppable
M src/main/java/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.java
  Cleanup all connections on way out.
M src/main/java/org/apache/hadoop/hbase/util/HMerge.java
  Cleanup proxies.... was false.
M src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
M src/main/java/org/apache/hadoop/hbase/client/HConnection.java
  Javadoc explaining how HConnections work.
M src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
  Make it so we make a new Configuration and that we then
  do our own cleanup when pool is shutdown.
M src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
  Use alternate method now the one that takes a Connection removed.
M src/main/java/org/apache/hadoop/hbase/client/HTable.java
  More javadoc on how HConnection works.
{code};;;","18/Oct/10 23:58;stack;Committed.  Thanks for the review Jon.  Did all you suggested on commit (Did not add back your RIT logging -- you can do taht if you need it).;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestHLog.testSplit failing in trunk,HBASE-2667,12466163,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,04/Jun/10 06:35,20/Nov/15 12:41,14/Jul/23 06:06,04/Jun/10 16:08,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Build is broke because this test won't pass.

Seems to be two issues at least:

1. we need to change the test so it keeps logs in one subdir, archives in another and split products in yet another as per regionserver.  New split code does cleanup of old logs dir if successful.  In this test it was removing the created splits so when verify went to run, the log no longer existed
2. The verify of splits tests that sequence numbers are in order.  They don't seem to be (the assertion failure was masked by a close reader exception).",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/10 06:38;stack;2667-prelim.txt;https://issues.apache.org/jira/secure/attachment/12446333/2667-prelim.txt","04/Jun/10 16:07;stack;2667.txt;https://issues.apache.org/jira/secure/attachment/12446352/2667.txt","05/Jun/10 01:01;clehene;HBASE-2667-v2.patch;https://issues.apache.org/jira/secure/attachment/12446392/HBASE-2667-v2.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26413,,,,,Fri Nov 20 12:41:18 UTC 2015,,,,,,,,,,"0|i0hion:",100289,,,,,,,,,,,,,,,,,,,,,"04/Jun/10 06:38;stack;Some preliminary work.  Does clean up dir locations in test.  Adds logging.  No fix yet.;;;","04/Jun/10 16:07;stack;The test checked sequenceid ordering in the produced split.  They were being written in the reverse order -- newest first.  The reconstruction log over in HStore expects to read the edits oldest through to newest so changed how we make the recovered edits files.

Here is the change:

{code}
@@ -1552,10 +1552,11 @@ public class HLog implements HConstants, Syncable {
         try {
           int editsCount = 0;
           WriterAndPath wap = logWriters.get(region);
-          for (ListIterator<Entry> iterator = entries.listIterator();
-               iterator.hasNext();) {
-            Entry logEntry =  iterator.next();
-
+          // We put edits onto the Stack ordered oldest sequence id to newest.
+          // Pop them off starting with the oldest.
+          for (ListIterator<Entry> iterator = entries.listIterator(entries.size());
+               iterator.hasPrevious();) {
+            Entry logEntry =  iterator.previous();
{code};;;","04/Jun/10 16:08;stack;Committed.  Change is small and build is broke.;;;","05/Jun/10 01:01;clehene;Added a new unit test TestHLogSplit.testSplitPreservesEdits that checks edits integrity after split. 

Changed the looping through the edits to a simple for and fixed parseHLog to append (addLast) to the list of edits.  ;;;","05/Jun/10 05:54;stack;You stuff is cleaner than my crap Cosmin.  I committed it.  Thanks.;;;","05/Jun/10 07:18;clehene;My previous, cleaner stuff broke it :P. ;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/TestStoreReconstruction broke in trunk,HBASE-2665,12466156,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,04/Jun/10 03:22,11/Jun/22 23:44,14/Jul/23 06:06,04/Jun/10 03:33,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/10 03:32;stack;r.txt;https://issues.apache.org/jira/secure/attachment/12446320/r.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26411,,,,,Fri Jun 04 03:33:34 UTC 2010,,,,,,,,,,"0|i0hio7:",100287,,,,,,,,,,,,,,,,,,,,,"04/Jun/10 03:32;stack;dir of logs can't have anything in it anymore else triggers alarms in new log splitting code; e.g. this test was putting the archiving dir beside the test log file.  The split code was thinking it a log file first and failing, and then when I tried to skip dirs, out the other end it was complaining an orphan log file had shown up.  We might want to revisit the stringency later.  For now, test works like regionserver with separate dir for logs and archives;;;","04/Jun/10 03:33;stack;Committed small patch to fix build.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestHRegion.testCheckAndDelete_ThatDeleteWasWritten fail in trunk,HBASE-2664,12466151,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,,tlipcon,tlipcon,04/Jun/10 01:42,20/Nov/15 12:43,14/Jul/23 06:06,04/Jun/10 04:48,,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,Seems like a bug in the recently committed patch. See http://hudson.zones.apache.org/hudson/job/HBase-TRUNK/1289/testReport/org.apache.hadoop.hbase.regionserver/TestHRegion/testCheckAndDelete_ThatDeleteWasWritten/,,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/10 04:47;stack;sleep.txt;https://issues.apache.org/jira/secure/attachment/12446322/sleep.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26410,,,,,Fri Nov 20 12:43:43 UTC 2015,,,,,,,,,,"0|i0hinz:",100286,,,,,,,,,,,,,,,,,,,,,"04/Jun/10 04:47;stack;Patch that sleeps before doing second set of puts so they have a later timestamp for sure.  Intent of test seems to be put two of same r/f/q but of different ts then after delete, the older version prevails.  Without a pause, the second put overwrote the first in memstore so only one instance.

I'm going to commit to see if fixes build.  Please review on commit.  We can reopen if fix is not to anyones liking.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestScannerResource.testScannerResource broke in trunk,HBASE-2662,12466136,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,03/Jun/10 22:50,20/Nov/15 12:41,14/Jul/23 06:06,03/Jun/10 22:53,,,,,,,,,,,,0.90.0,,,,,,,0,,,"{code}
Error Message

Illegal character <58>. Family names cannot contain control characters or colons: a:
Stacktrace

java.lang.IllegalArgumentException: Illegal character <58>. Family names cannot contain control characters or colons: a:
	at org.apache.hadoop.hbase.HColumnDescriptor.isLegalFamilyName(HColumnDescriptor.java:278)
	at org.apache.hadoop.hbase.HColumnDescriptor.<init>(HColumnDescriptor.java:240)
	at org.apache.hadoop.hbase.HColumnDescriptor.<init>(HColumnDescriptor.java:212)
	at org.apache.hadoop.hbase.HColumnDescriptor.<init>(HColumnDescriptor.java:169)
	at org.apache.hadoop.hbase.HColumnDescriptor.<init>(HColumnDescriptor.java:158)
	at org.apache.hadoop.hbase.rest.TestScannerResource.setUp(TestScannerResource.java:106)
	at junit.framework.TestCase.runBare(TestCase.java:132)
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/10 22:51;stack;tsr.txt;https://issues.apache.org/jira/secure/attachment/12446295/tsr.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26409,,,,,Fri Nov 20 12:41:24 UTC 2015,,,,,,,,,,"0|i0hinr:",100285,,,,,,,,,,,,,,,,,,,,,"03/Jun/10 22:51;stack;Fix whats passed HCD.;;;","03/Jun/10 22:53;stack;Applied small patch to fix broke test.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
REST (stargate) TableRegionModel Regions need to be updated to work w/ new region naming convention from HBASE-2531,HBASE-2658,12465993,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,02/Jun/10 18:41,20/Nov/15 12:43,14/Jul/23 06:06,21/Oct/10 20:40,,,,,,,,,,,,0.90.0,,,,,,,0,,,"One reason TestTableResource was failing was because comparing region names as strings was failing because the two below no longer matched.  My guess is that the rest stuff is not using the new means of constructing region names.  See HBASE-2531

TestTableResource,,1275503739792.30a45563321be3ec11841b0f1e79d687.
TestTableResource,,1275503739792",,eyang,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26408,Reviewed,,,,Fri Nov 20 12:43:04 UTC 2015,,,,,,,,,,"0|i0hin3:",100282,,,,,,,,,,,,,,,,,,,,,"02/Jun/10 18:43;stack;Over in hbase-2657 I loosed the below changing it to a startsWith from an equals test:

{code}
        HRegionInfo hri = e.getKey();
        String hriRegionName = hri.getRegionNameAsString();
        String regionName = region.getName();
        if (hriRegionName.startsWith(regionName)) {
{code}

Restore when fix for this here.;;;","02/Jun/10 18:50;stack;Assigned Andrew since it seems like a pretty big, if simple, change needed.  The 'id' in TableRegionModel needs to change form long to String?;;;","19/Oct/10 05:35;stack;Is this needed for 0.90 Andrew?  I'd think so.;;;","21/Oct/10 18:36;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1063/
-----------------------------------------------------------

Review request for hbase and Andrew Purtell.


Summary
-------

Andrew, can you look at this?  Its making the TableRegionModel do the new HRegionInfo name format.  Has a few tests even.  I also changed the TestTableResource back to old style compare of region names and now it passes.


This addresses bug hbase-2658.
    http://issues.apache.org/jira/browse/hbase-2658


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java 1026078 
  trunk/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java 1026078 
  trunk/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java 1026078 
  trunk/src/main/java/org/apache/hadoop/hbase/rest/model/TableRegionModel.java 1026078 
  trunk/src/main/java/org/apache/hadoop/hbase/util/MetaUtils.java 1026078 
  trunk/src/test/java/org/apache/hadoop/hbase/rest/TestTableResource.java 1026078 
  trunk/src/test/java/org/apache/hadoop/hbase/rest/model/TestTableRegionModel.java 1026078 

Diff: http://review.cloudera.org/r/1063/diff


Testing
-------


Thanks,

stack


;;;","21/Oct/10 20:28;hbasereviewboard;Message from: ""Andrew Purtell"" <apurtell@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1063/#review1602
-----------------------------------------------------------

Ship it!


Looks good.

- Andrew



;;;","21/Oct/10 20:40;stack;Committed. Resolving.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestTableResource is broken in trunk,HBASE-2657,12465992,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,02/Jun/10 18:36,20/Nov/15 12:41,14/Jul/23 06:06,02/Jun/10 18:48,,,,,,,,,,,,0.90.0,,,,,,,0,,,Column name is illegal -- its 'test:' -- when constructing a HCD.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/10 18:47;stack;2657.txt;https://issues.apache.org/jira/secure/attachment/12446169/2657.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26407,,,,,Fri Nov 20 12:41:08 UTC 2015,,,,,,,,,,"0|i0himv:",100281,,,,,,,,,,,,,,,,,,,,,"02/Jun/10 18:42;stack;There is also hbase-2658.  For now I've loosed a test check to be tightened again over in fix for hbase-2658.;;;","02/Jun/10 18:47;stack;Pass a column family name minus the ':' creating HCD.  Also changed check so its starts with rather than equals till HBASE-2658 is fixed so test will pass.;;;","02/Jun/10 18:48;stack;Committed small change.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HMaster.getRegionTableClosest should not return null for closed regions,HBASE-2656,12465990,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,tlipcon,tlipcon,tlipcon,02/Jun/10 17:55,11/Jun/22 23:44,14/Jul/23 06:06,22/Jun/10 00:33,0.90.0,,,,,,,,,,,,,master,,,,,0,,,"Raised in the review of HBASE-2560: there are a couple functions in HMaster which return null when a region has not been deployed. Instead, they should return a Pair<HRegionInfo, HServerAddress> where only the address is null (since the info is still in meta!)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2560,,,,,,,,,,,,,"21/Jun/10 00:06;tlipcon;hbase-2656.txt;https://issues.apache.org/jira/secure/attachment/12447570/hbase-2656.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26406,Reviewed,,,,Tue Jun 22 00:33:44 UTC 2010,,,,,,,,,,"0|i0himn:",100280,,,,,,,,,,,,,,,,,,,,,"11/Jun/10 23:10;karthik.ranga;TestMaster was the test to tracki this behavior, which seems to have changed since HBASE-2656.

This is a comment to verify what has changed in TestMaster and fix the test so that the correct functionality is verified. There is a TODO in the code now:
      /**
       * TODO: The assertNull below used to work before moving all RS->M 
       * communication to ZK, find out why this test's behavior has changed.
       * Tracked in HBASE-2656.
        assertNull(pair);
      */
;;;","19/Jun/10 06:58;stack;Making this unexplained behavior a blocker.

I just disabled this test, in essence, removing the assertion that the pair returned is not null because this opposite assertion is not always true up on hudson -- see http://hudson.zones.apache.org/hudson/view/HBase/job/HBase-TRUNK/1336/ for example.;;;","21/Jun/10 14:46;stack;I skimmed patch Todd.  Looks good.  You want it reviewed?  You think it fixes this issue?;;;","21/Jun/10 18:13;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/214/
-----------------------------------------------------------

Review request for hbase and stack.


Summary
-------

Would be good to get these things out of HMaster entirely to share code with other META lookup stuff, but here's a first pass cleanup.


This addresses bug HBASE-2656.
    http://issues.apache.org/jira/browse/HBASE-2656


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/HRegionInfo.java d6f9611 
  src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java 4c0f392 
  src/main/java/org/apache/hadoop/hbase/master/HMaster.java 66dc697 
  src/test/java/org/apache/hadoop/hbase/master/TestMaster.java a5f4d69 

Diff: http://review.hbase.org/r/214/diff


Testing
-------


Thanks,

Todd


;;;","21/Jun/10 21:27;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/214/#review259
-----------------------------------------------------------

Ship it!


+1 We can do others as we go.  Commit.  I wonder though if Pair is not constraining?  We should return all in the row.  Can do that later if proves to be the case.


src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
<http://review.hbase.org/r/214/#comment1109>

    Generally useful method.  Good.


- stack



;;;","22/Jun/10 00:33;tlipcon;Committed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Several tests failing after bulk output commit,HBASE-2654,12465938,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,tlipcon,tlipcon,tlipcon,02/Jun/10 06:58,20/Nov/15 12:43,14/Jul/23 06:06,02/Jun/10 17:59,,,,,,,,,,,,0.90.0,,mapreduce,,,,,0,,,"Several tests are failing on Hudson after the commit of HBASE-1923 - see http://hudson.zones.apache.org/hudson/job/HBase-TRUNK/1282/testReport/
These tests passed on local build, something seems to be different about the hudson environment.",,hammer,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/10 17:50;tlipcon;hbase-2654.txt;https://issues.apache.org/jira/secure/attachment/12446163/hbase-2654.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26405,Reviewed,,,,Fri Nov 20 12:43:59 UTC 2015,,,,,,,,,,"0|i0himf:",100279,,,,,,,,,,,,,,,,,,,,,"02/Jun/10 06:58;tlipcon;Will look at this first thing tomorrow.;;;","02/Jun/10 15:17;apurtell;No valid guava pomfile in the public repos.

Downloading: http://mirrors.ibiblio.org/pub/mirrors/maven2//com/google/guava/guava/r03/guava-r03.pom
1K downloaded  (guava-r03.pom)
[WARNING] *** CHECKSUM FAILED - Checksum failed on download: local = '7a37041386ee39a1fbb3efd3c4c6932809cb5887'; remote = '1cbd6fab2460050ff7147b6d8536f39c8f535067' - IGNORING
;;;","02/Jun/10 15:19;apurtell;Sorry, previous was truncated:

[INFO] Unable to find resource 'com.google.guava:guava-parent:pom:r03' in repository temp-hadoop (http://people.apache.org/~rawson/repo/)
Downloading: http://mirrors.ibiblio.org/pub/mirrors/maven2//com/google/guava/guava-parent/r03/guava-parent-r03.pom
2K downloaded  (guava-parent-r03.pom)
[WARNING] *** CHECKSUM FAILED - Checksum failed on download: local = '79354ca8e73923f79529ec475e4803e605b168e7'; remote = '3853400652152af4ec2973cd5eb355fb311d477d' - RETRYING
Downloading: http://mirrors.ibiblio.org/pub/mirrors/maven2//com/google/guava/guava-parent/r03/guava-parent-r03.pom
2K downloaded  (guava-parent-r03.pom)
[WARNING] *** CHECKSUM FAILED - Checksum failed on download: local = '79354ca8e73923f79529ec475e4803e605b168e7'; remote = '3853400652152af4ec2973cd5eb355fb311d477d' - IGNORING
Downloading: http://mirror.facebook.net/maven/repository//com/google/guava/guava/r03/guava-r03.jar
[INFO] Unable to find resource 'com.google.guava:guava:jar:r03' in repository mirror.facebook.net (http://mirror.facebook.net/maven/repository/)
Downloading: http://download.java.net/maven/2//com/google/guava/guava/r03/guava-r03.jar
[INFO] Unable to find resource 'com.google.guava:guava:jar:r03' in repository java.net (http://download.java.net/maven/2/)
Downloading: http://google-maven-repository.googlecode.com/svn/repository//com/google/guava/guava/r03/guava-r03.jar
[INFO] Unable to find resource 'com.google.guava:guava:jar:r03' in repository googlecode (http://google-maven-repository.googlecode.com/svn/repository/)
Downloading: http://repository.codehaus.org//com/google/guava/guava/r03/guava-r03.jar
[INFO] Unable to find resource 'com.google.guava:guava:jar:r03' in repository codehaus (http://repository.codehaus.org/)
Downloading: http://people.apache.org/~rawson/repo//com/google/guava/guava/r03/guava-r03.jar
[INFO] Unable to find resource 'com.google.guava:guava:jar:r03' in repository temp-hadoop (http://people.apache.org/~rawson/repo/)
Downloading: http://mirrors.ibiblio.org/pub/mirrors/maven2//com/google/guava/guava/r03/guava-r03.jar
828K downloaded  (guava-r03.jar)
[WARNING] *** CHECKSUM FAILED - Checksum failed on download: local = '0c510ad67cad9fe2efaf1cc18e767c40854cc8e4'; remote = '2728255dac51834aec0ad0a7f69bc35d48ed47fb' - RETRYING
Downloading: http://mirrors.ibiblio.org/pub/mirrors/maven2//com/google/guava/guava/r03/guava-r03.jar
828K downloaded  (guava-r03.jar)
[WARNING] *** CHECKSUM FAILED - Checksum failed on download: local = '0c510ad67cad9fe2efaf1cc18e767c40854cc8e4'; remote = '2728255dac51834aec0ad0a7f69bc35d48ed47fb' - IGNORING
;;;","02/Jun/10 15:43;stack;Looks like it found it in ibiblio but corrupt download?  Maybe next time it builds it'll download a good version?  Is this from hudson.hbase.org or apache hudson?;;;","02/Jun/10 17:00;stack;I just got this up on a linux machine:

{code}
828K downloaded  (guava-r03.jar)
[WARNING] *** CHECKSUM FAILED - Checksum failed on download: local = '0c510ad67cad9fe2efaf1cc18e767c40854cc8e4'; remote = '2728255dac51834aec0ad0a7f69bc35d48ed47fb' - RETRYING
Downloading: http://mirrors.ibiblio.org/pub/mirrors/maven2//com/google/guava/guava/r03/guava-r03.jar
1376K downloaded  (mockito-all-1.8.4.jar)
828K downloaded  (guava-r03.jar)
[WARNING] *** CHECKSUM FAILED - Checksum failed on download: local = '0c510ad67cad9fe2efaf1cc18e767c40854cc8e4'; remote = '2728255dac51834aec0ad0a7f69bc35d48ed47fb' - IGNORING
[INFO] [antrun:run {execution: JSPC}]
[INFO] Executing tasks
{code}

I removed the corrupt jar from my .m2 repo and on second download it was again corrupt.  Investigating.    Seems like same issue on mac.

;;;","02/Jun/10 17:03;tlipcon;strange... want me to shove guava in my apache.org maven repo? I can also file a ticket on google code.

This must be passing on my machines since I have it in my cache from last week.;;;","02/Jun/10 17:17;tlipcon;Looks like it's a known issue with the guava r03 push.

http://code.google.com/p/guava-libraries/issues/detail?id=354&q=maven

I'll see if I can convince maven to put a copy in my mvn repo.;;;","02/Jun/10 17:20;stack;I saw that issue but seemed to be some ivy mess.  I tried downloading just the pieces of guava but that has checksum issues too.  There are also two refs to guava in pom.xml.  Thought that it but no.   Maybe jgray can put it in the fb repo if you can't get it going on todd;;;","02/Jun/10 17:50;tlipcon;I managed to deploy guava to my apache.org repo using:
mvn deploy -DaltDeploymentRepository=todd-apache.org::default::scpexe://todd@people.apache.org/home/todd/public_html/repo
from the guava r03 tag checkout

This patch adds my repository to the list. I verified that it worked for me, but would be good for someone else to check.;;;","02/Jun/10 17:55;stack;+1 

Cleaned corrupt guava from my local repo., applied the patch and it downloaded cleanly:

{code}
Downloading: http://people.apache.org/~todd/repo//com/google/guava/guava/r03/guava-r03.jar
828K downloaded  (guava-r03.jar)
[INFO] [antrun:run {execution: JSPC}]
{code};;;","02/Jun/10 17:59;tlipcon;Resolved. I will check back on Guava in a month or so to see if they've put up new releases in their repo (they've done releases since the one we're using, but not in maven);;;","02/Jun/10 18:00;tlipcon;[if anyone continues to run into this, please rm -Rf ~/.m2/repository/com/google/guava and try again.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HLog writer can do 1-2 sync operations after lease has been recovered for split process.,HBASE-2645,12465888,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,tlipcon,clehene,clehene,01/Jun/10 17:57,15/Oct/13 04:46,14/Jul/23 06:06,16/Nov/12 23:54,0.90.4,,,,,,,,,,,0.95.0,,Filters,,,,,0,,,"TestHLogSplit.testLogCannotBeWrittenOnceParsed is failing. 

This test starts a thread that writes one edit to the log, syncs and counts. During this, a HLog.splitLog operation is started. splitLog recovers the log lease before reading the log, so that the original regionserver could not wake up and write after the split process started.  
The test compares the number of edits reported by the split process and by the writer thread. Writer thread (called zombie in the test) should report <=  than the splitLog (sync() might raise after the last edit gets written and the edit won't get counted by zombie thread). However it appears that the zombie counts 1-2 more edits. So it looks like it can sync without a lease.

This might be a hdfs-0.20 related issue. ",,clehene,hammer,hudson,qwertymaniac,stack,tlipcon,wjiangwen,yuzhihong@gmail.com,zhihyu@ebaysf.com,,,,,,,,,,,,,,,,,,,,,HDFS-1186,,,,,,,,,,HBASE-7636,,,,,,,,,,,,,,,,,"17/Nov/12 00:35;stack;2645-addendum.txt;https://issues.apache.org/jira/secure/attachment/12553870/2645-addendum.txt","29/Oct/12 23:43;stack;2645.txt;https://issues.apache.org/jira/secure/attachment/12551281/2645.txt","08/Nov/12 19:03;stack;2645_hacking.txt;https://issues.apache.org/jira/secure/attachment/12552691/2645_hacking.txt","30/Oct/12 21:47;stack;2645v2.txt;https://issues.apache.org/jira/secure/attachment/12551421/2645v2.txt","31/Oct/12 04:40;stack;2645v3.txt;https://issues.apache.org/jira/secure/attachment/12551477/2645v3.txt","16/Nov/12 22:36;stack;2645v4.txt;https://issues.apache.org/jira/secure/attachment/12553853/2645v4.txt","06/Nov/12 08:03;stack;hdfs_1.0_editswriter_recoverlease.txt;https://issues.apache.org/jira/secure/attachment/12552239/hdfs_1.0_editswriter_recoverlease.txt","06/Nov/12 08:03;stack;hdfs_trunk_editswriter_recoverlease.txt;https://issues.apache.org/jira/secure/attachment/12552238/hdfs_trunk_editswriter_recoverlease.txt","06/Nov/12 19:18;stack;org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit-output.txt;https://issues.apache.org/jira/secure/attachment/12552325/org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit-output.txt","31/Oct/12 16:49;zhihyu@ebaysf.com;org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit-output.txt;https://issues.apache.org/jira/secure/attachment/12551576/org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit-output.txt",,,,,,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26403,,,,,Mon Sep 23 18:30:23 UTC 2013,,,,,,,,,,"0|i02bnz:",11491,,,,,,,,,,,,0.96notable,,,,,,,,,"01/Jun/10 19:00;stack;Made it a blocker.;;;","03/Jun/10 05:14;tlipcon;Definitely an 20-append bug, somewhat known... wrote a test case and a preliminary fix for it in HDFS tonight, will swing it by the FB guys.;;;","04/Jun/10 03:52;tlipcon;This passes on my current snapshot of CDH3b2 available in my maven repo. If you edit pom.xml to enable snapshots for my apache.org repository, and change the hadoop version to 0.20.2-CDH3b2-SNAPSHOT you can verify.;;;","04/Jun/10 18:02;apurtell;We need for it to pass on the 0.20-append branch.;;;","04/Jun/10 18:11;tlipcon;Right, the linked HDFS JIRA is slated for append 20 branch. Dhruba will hopefully commit it there after he gets in the rest of the patches.;;;","24/Aug/10 23:14;jdcryans;Are we actually going to have this in time for 0.90? If not, punt?;;;","24/Aug/10 23:47;tlipcon;I don't think we have to do anything on the HBase side here - we just need to commit a couple more things on the HDFS side. With CDH3b2 it already works.;;;","05/Oct/10 21:02;ryanobjc;Dhruba can you please commit the proper patch to 0.20-append so we can close this issue?  In the mean time we'll move it to 0.92 since many people are running cdh3b2.;;;","25/Oct/10 23:09;ryanobjc;moving fix version to 0.92;;;","21/Jun/11 20:31;stack;Moving out of 0.92.  Pull it in again anyone if you want to work on it.;;;","21/Jun/11 20:31;stack;Moving out of 0.92.  Pull it in again anyone if you want to work on it.;;;","25/Jul/11 09:18;vams;Hi Todd,may be in this place this question is irrelevant. But please let me know whether we can implement a distributed hashing in HBase for fast lookup/ scanning purpose?? i want to implement scalable data structure i.e DHT in Hbase, for that how can i proceed? Thank you.;;;","28/Feb/12 06:59;larsh;No activity. Moving out of 0.94. Is this really a blocker if it can go unfixed for such a long time?;;;","29/Oct/12 23:43;stack;Update tests so it will run.;;;","30/Oct/12 21:47;stack;Reenabled the test.  Had to refactor some since the WAL generating code has changed since originally commented out to use different table and region names.  Test passes for me locally after running it multiple times.  HDFS seems to be doing the right thing for us.;;;","30/Oct/12 21:47;stack;Submitting patch;;;","30/Oct/12 22:44;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12551421/2645v2.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 6 new or modified tests.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 83 warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 5 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.client.TestShell

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/3186//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3186//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3186//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3186//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3186//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3186//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3186//console

This message is automatically generated.;;;","30/Oct/12 23:02;zhihyu@ebaysf.com;Partial change from another JIRA seemed to get into patch v2:
{code}
+    define_test ""create should be able to set table options"" do
{code};;;","31/Oct/12 04:40;stack;Thanks Ted.

This is what I'll commit.  Its the fixed test minus the pollution introduced in v2 (that caused TestShell) to pass.  I ran TestShell locally a few times w/ v3 and it passes.  Going to commit.;;;","31/Oct/12 04:51;stack;Reenabled and updated the test that was failing.  It passes now.  Original issue seems to have been hdfs problem since fixed by our move to hdfs 1.0+.;;;","31/Oct/12 05:34;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12551477/2645v3.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 85 warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 3 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.io.hfile.TestForceCacheImportantBlocks

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/3194//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3194//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3194//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3194//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3194//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3194//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3194//console

This message is automatically generated.;;;","31/Oct/12 16:43;zhihyu@ebaysf.com;{code}
r1403977 | stack | 2012-10-30 21:48:37 -0700 (Tue, 30 Oct 2012) | 1 line

HLog writer can do 1-2 sync operations after lease has been recovered for split process.
{code}
I don't see JIRA number in the checkin message.

Second, in https://builds.apache.org/job/PreCommit-HBASE-Build/3197//testReport/org.apache.hadoop.hbase.regionserver.wal/TestHLogSplit/testLogCannotBeWrittenOnceParsed/ :
{code}
Error Message

The log file could have at most 1 extra log entry, but can't have less. Zombie could write 6559 and logfile had only 6520 in logfile=hdfs://localhost:44865/hbase/t1/bbb/recovered.edits/0000000000000000001

Stacktrace

java.lang.AssertionError: The log file could have at most 1 extra log entry, but can't have less. Zombie could write 6559 and logfile had only 6520 in logfile=hdfs://localhost:44865/hbase/t1/bbb/recovered.edits/0000000000000000001
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.testLogCannotBeWrittenOnceParsed(TestHLogSplit.java:190)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
{code};;;","31/Oct/12 16:49;zhihyu@ebaysf.com;I can reproduce the test failure locally.
Attaching test output.
{code}
testLogCannotBeWrittenOnceParsed(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit)  Time elapsed: 14.517 sec  <<< FAILURE!
java.lang.AssertionError: The log file could have at most 1 extra log entry, but can't have less. Zombie could write 6212 and logfile had only 5655 in logfile=hdfs://localhost:59702/hbase/t1/bbb/recovered.edits/0000000000000000001
  at org.junit.Assert.fail(Assert.java:93)
  at org.junit.Assert.assertTrue(Assert.java:43)
  at org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.testLogCannotBeWrittenOnceParsed(TestHLogSplit.java:190)
{code};;;","31/Oct/12 22:34;zhihyu@ebaysf.com;Another test failure:
https://builds.apache.org/job/PreCommit-HBASE-Build/3204/testReport/junit/org.apache.hadoop.hbase.regionserver.wal/TestHLogSplit/testLogCannotBeWrittenOnceParsed/;;;","01/Nov/12 16:48;zhihyu@ebaysf.com;The test failed in HBase-TRUNK build #3504;;;","01/Nov/12 21:02;stack;[~ted_yu], your repeat of info in here, info that is public and available to all is of no help; its just spam in our mailboxes.  Neither is your cut-off paste of a failure on your local machine w/ no context such as what your local machine is, if it fails always, etc.  I committed the patch because it passed locally on repeated runs and passed hadoopqa.;;;","01/Nov/12 21:15;zhihyu@ebaysf.com;Here is information about my environment:
{code}
java version ""1.6.0_35""
Java(TM) SE Runtime Environment (build 1.6.0_35-b10-428-11M3811)
Java HotSpot(TM) 64-Bit Server VM (build 20.10-b01-428, mixed mode)

Darwin T32 11.4.2 Darwin Kernel Version 11.4.2: Thu Aug 23 16:25:48 PDT 2012; root:xnu-1699.32.7~1/RELEASE_X86_64 x86_64
{code}
Initially I wasn't sure whether the test failure only happened with JDK 1.6

Later I saw the test failure on Jenkins where JDK 1.7 was used.;;;","02/Nov/12 04:31;stack;It fails for me too locally if I keep running it.  If I do the below, it usually passes but it can also fail:

{code}
$ for i in `seq 0 9`; do echo ""here=$i""; mvn  test -Dtest=TestHLogSplit#testLogCannotBeWrittenOnceParsed  -PlocalTests; mv hbase-server/target/surefire-reports hbase-server/target/$i; done &> /tmp/test.txt &
{code}

I tried w/ 1.0.4 and it also fails.

Will revert for now.  If a bug in HDFS, will file an issue.;;;","02/Nov/12 04:37;yuzhihong@gmail.com;Thanks for the verification, Stack. 

Appreciate it. ;;;","02/Nov/12 14:53;hudson;Integrated in HBase-TRUNK #3508 (See [https://builds.apache.org/job/HBase-TRUNK/3508/])
    HBASE-2645 HLog writer can do 1-2 sync operations after lease has been recovered for split process; REVERT -- TEST FAILS (Revision 1404875)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
;;;","04/Nov/12 17:31;hudson;Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #248 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/248/])
    HBASE-2645 HLog writer can do 1-2 sync operations after lease has been recovered for split process; REVERT -- TEST FAILS (Revision 1404875)

     Result = FAILURE
stack : 
Files : 
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
;;;","06/Nov/12 08:02;stack;Been playing w/ this one.  I can't reproduce in hdfs on trunk or on 1.0 branch (rough patches attached).  If after calling recoverFileLease, I open the file and keep reading to the end, counting edits, usually I do it once and the number of edits is constant, even if I open the file again and read.  Other times I see the number of edits climb w/ each opening of the file, often over seconds.;;;","06/Nov/12 08:03;stack;Messy patch I was using on hdfs trunk.;;;","06/Nov/12 08:03;stack;hdfs 1.0 branch version.;;;","06/Nov/12 19:16;stack;Adding logging, I can see the 'master' return from the recover file lease call against the namenode.  We then go to open the 'wal' to read the edits out of it splitting.  Meantime, I see that the writer thread is still progressing adding edits until it stalls in a sync call.  This means the read will miss edits (lots in this test case).;;;","06/Nov/12 19:18;stack;Log showing us missing writes.;;;","08/Nov/12 19:03;stack;Hacked up patch.  Has two different users on two different filesystems (different dfsclients).  Enables the hdfs logging so I can see the above is indeed the case.  The 'regionserver' thread hangs in sync until it gets an IOE 'Error Recovery for block.... failed because recovery from primary datanode ... failed 6 times'...after 40 seconds (ugh) not the lease exception I'd expect.  ;;;","08/Nov/12 19:39;stack;Hmmm... this patch sortof works but I want to learn more about this 40 second hang in writer.  Running more tests.;;;","16/Nov/12 22:05;stack;Yeah, looks like the reenabled test works.  It just is taking a while to complete because the mocked regionserver thread is taking its time dying.  I made HDFS-4203 for failing faster (after Todd helped out on what is going on).  Let me run it a few times local to makes sure it good.;;;","16/Nov/12 22:36;stack;Cleaned up patch.  Reenables the test and does some refactoring so different filesystems running pseudo master and regionserver threads.

Ran it locally ten times and passes.;;;","16/Nov/12 23:37;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12553853/2645v4.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 99 warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 24 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/3355//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3355//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3355//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3355//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3355//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3355//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/3355//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/3355//console

This message is automatically generated.;;;","16/Nov/12 23:54;stack;Committing to trunk.  Will reopen if fails again.  Passes for me when run repeatedly locally.;;;","17/Nov/12 00:08;yuzhihong@gmail.com;Might be a typo:
{code}
+    ZOMBIE = User.getCurrent().getName() + ""-robber"";
{code}
;;;","17/Nov/12 00:34;stack;Doesn't seem used.  Applied addendum to address above comment.;;;","17/Nov/12 00:35;stack;This is what I applied.;;;","17/Nov/12 11:52;hudson;Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #266 (See [https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/266/])
    HBASE-2645 HLog writer can do 1-2 sync operations after lease has been recovered for split process (Revision 1410651)
HBASE-2645 HLog writer can do 1-2 sync operations after lease has been recovered for split process (Revision 1410631)

     Result = SUCCESS
stack : 
Files : 
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java

stack : 
Files : 
* /hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
;;;","19/Nov/12 09:22;clehene;Wow, I think it was one of the longest lived blockers. Glad to see it fixed. Thanks :) ;;;","23/Sep/13 18:30;stack;Marking closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Investigate sync 'voodoo' splitting WALs,HBASE-2644,12465885,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,01/Jun/10 17:10,20/Nov/15 12:41,14/Jul/23 06:06,05/Oct/10 22:24,,,,,,,,,,,,0.90.0,,,,,,,0,,,"The sequence file sync seems to actually help for some weird reason when recovering parts of edits (odd -- it looks like it just adds a marker to the file).  Investigate.  This comes of review of hbase-2437.   Below is copied from http://review.hbase.org/r/74/

{code}
no point to call .sync() here, it just wastes a bunch of IO to write ""sync markers"" which we don't make any real use of.
Cosmin Lehene 6 days, 23 hours ago (May 25th, 2010, 9:07 a.m.)
sync() used to call syncFs(). It looks like HBASE-2544 changed things a bit, but it doesn't only add the SequenceFile sync marker.

I added this after I've seen inconsistent results when running splitLog on bigger hlogs. Try copying a log from the cluster locally and run splitLog from the command line a few times without flushing it after each append. I used to get inconsistent results between runs and calling sync fixed it.

There's this ""//TODO: test the split of a large (lots of regions > 500 file). In my tests it seems without hflush""  in the TestHLogSplit. 

We could do some testing to figure out why would log entries be lost when running locally.

What would be a better way to flush the writer?
Todd Lipcon 5 days, 19 hours ago (May 26th, 2010, 1:31 p.m.)
This seems really voodoo.. if anything we're probably masking a real bug by doing this. Can you write a unit test which shows this problem (even if it takes 30 minutes to run, would be good to have in our arsenal)
Cosmin Lehene 2 days, 18 hours ago (May 29th, 2010, 2:13 p.m.)
I can't reproduce it on hdfs-0.20. I can't compile hdfs-0.21 (again) for some reason. I'll give it another try some other time. 

Added the test. Also tried with a real 60MB log file. 
I'm not sure if we should leave the test active.
{code}",,clehene,dhruba,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26402,,,,,Fri Nov 20 12:41:22 UTC 2015,,,,,,,,,,"0|i0hikv:",100272,,,,,,,,,,,,,,,,,,,,,"04/Jun/10 06:15;tlipcon;Mind if I downgrade this to major, since we can't reproduce on 0.20, and no one plans on deploying on 0.21 in short term?
(if we want to start really supporting HDFS 0.21 we should add a Hudson build for trunk on 0.21);;;","04/Jun/10 06:41;stack;Go for it.

Should we move it out even?  Should running on 0.21 and 0.20 hadoop be a target for the next major hbase release?;;;","04/Jun/10 06:46;tlipcon;IMO support for 0.21 is ""nice to have"" but if it doesnt quite work right at first I don't think it's a big deal. 0.21 itself is being billed as ""might have some issues, 0.21.0 not for production uses"";;;","05/Oct/10 22:24;ryanobjc;we removed sync markers after we found at SU they could cause apparent corruption of HLogs (ouch!).  They don't exist anymore. Fixed.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Figure how to deal with eof splitting logs,HBASE-2643,12465884,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,nspiegelberg,stack,stack,01/Jun/10 17:07,20/Nov/15 12:41,14/Jul/23 06:06,03/Sep/10 05:58,0.89.20100621,,,,,,,,,,,0.89.20100924,0.90.0,,,,,,0,,,"When splitting the WAL and encountering EOF, it's not clear what to do. Initial discussion of this started in http://review.hbase.org/r/74/ - summarizing here for brevity:

We can get an EOFException while splitting the WAL in the following cases:
- The writer died after creating the file but before even writing the header (or crashed halfway through writing the header)
- The writer died in the middle of flushing some data - sync() guarantees that we can see _at least_ the last edit, but we may see half of an edit that was being written out when the RS crashed (especially for large rows)
- The data was actually corrupted somehow (eg a length field got changed to be too long and thus points past EOF)

Ideally we would know when we see EOF whether it was really the last record, and in that case, simply drop that record (it wasn't synced, so therefore we dont need to split it). Some open questions:
  - Currently we ignore empty files. Is it ok to ignore an empty log file if it's not the last one?
  - Similarly, do we ignore an EOF mid-record if it's not the last log file?",,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2935,HBASE-2933,,,HBASE-2889,,,,,,,,,,,,,"25/Aug/10 18:51;nspiegelberg;HBASE-2643.patch;https://issues.apache.org/jira/secure/attachment/12453070/HBASE-2643.patch","03/Sep/10 15:22;stack;ch03s02.html;https://issues.apache.org/jira/secure/attachment/12453782/ch03s02.html",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26401,Reviewed,,,,Fri Nov 20 12:41:55 UTC 2015,,,,,,,,,,"0|i0hikn:",100271,,,,,,,,,,,,,,,,,,,,,"04/Jun/10 06:23;tlipcon;Updated description to a summary of the issue at hand - refer back to original review for original discussion.;;;","12/Aug/10 11:19;richlackey@roamingcloud.com;There is an assumption that no RuntimeException will occur within splitLog. While such an exception is remote, it is possible. Should a RuntimeException occur, then it will percolate through HMaster, which will not join the cluster.
 
This is a general condition. The assumption in HDFS is that this cannot occur (or will be caught by upper layer), which is to say that none of the lower layers catches Exception to prevent the RuntimeException, e.g., NullPointerException, from percolating through. If the SequenceFile contains garbage (or has been corrupted), then the opportunity for the underlying DataInputStream to throw a RuntimeException increases.

The solution is to add a catch for Exception in splitLog and consider the log corrupt.
;;;","12/Aug/10 13:45;stack;Good stuff Richard.  If a runtime exception splitting logs, we should fail to start the cluster?  It means likely dataloss.  Or, there is a flag in hbase now -- its in the splitlog stuff -- IIRC which says fail-if-any-error OR try-and=keep-going across errors.  If this flag is set, we should catch the RuntimeException, log it and not start the master.  Otherwise, we should log the exception and then let the master proceed.;;;","12/Aug/10 14:42;richlackey@roamingcloud.com;Since the exception is likely to be the result of a corrupted log, I interpreted that to be within the realm of the flag setting -- a generalization of the intent. 

It seems like adding the catch, logging the exception, and following the flag setting, would add more predictive behavior. At a minimum it provides more documentation at startup, which should permit problem source isolation.;;;","24/Aug/10 22:22;nspiegelberg;We have encountered this EOF problem in our test cluster this week.  Is there a use case where an EOF could lead to data loss instead of just indicating data truncation due to connection failure?  HDFS throws a ChecksumException IOE with corrupt disk data, so EOF should only indicate application-level corruption.  It seems like we should handle the EOF case differently than normal IOEs and proceed even when 'hbase.hlog.split.skip.errors' == false.;;;","25/Aug/10 00:32;nspiegelberg;IIIRC:

[5:09pm] nspiegelberg: I'm just wondering if there's a reason for us to not continue through an EOF
[5:09pm] St^Ack: I can't think of one
[5:10pm] St^Ack: what you say is reasonable nspiegelberg
[5:10pm] St^Ack: least till we learn otherwise
[5:10pm] St^Ack: I have to go lads... pick up kid
[5:10pm] kannan: i agree... we could check to see if the IOE was an EOF and continue on....;;;","25/Aug/10 18:51;nspiegelberg;Changed the LOG from warn to info level after internal review;;;","03/Sep/10 05:58;stack;OK.  Lets go w/ this (We keep going of EOF in any WAL during spilts even if 'hbase.hlog.split.skip.errors' == false.).  I wrote the 'decision' into new WAL chapter in the hbase 'book'.  Thanks for the patch Nicolas.;;;","03/Sep/10 15:22;stack;Here is what the doc of the eof handling looks like currently (For Nicolas).  To be improved.;;;","24/Sep/10 18:12;jdcryans;Adding this to the latest 0.89;;;","24/Sep/10 19:08;jdcryans;While testing 0.89.20100924, I got this:

{noformat}
Tests run: 14, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 33.948 sec <<< FAILURE!
testEOFisIgnored(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit)  Time elapsed: 0.179 sec  <<< ERROR!
java.io.IOException: hdfs://localhost:62668/hbase/hlog/hlog.dat.0, pos=1012, edit=9
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.addFileInfoToException(SequenceFileLogReader.java:165)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:137)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:122)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.parseHLog(HLog.java:1564)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1323)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1210)
        at org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.testEOFisIgnored(TestHLogSplit.java:317)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
        at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
        at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
        at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:140)
        at org.apache.maven.surefire.Surefire.run(Surefire.java:109)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:290)
        at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1017)
Caused by: java.io.EOFException
        at java.io.DataInputStream.readFully(DataInputStream.java:180)
        at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:63)
        at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:101)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1937)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1837)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1883)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:135)
        ... 35 more
{noformat}

And looking at the code it doesn't check if the source of the IOE is a EOF. I don't see it failing on trunk, was it handled in the scope of another jira?;;;","24/Sep/10 19:13;nspiegelberg;@jd : This bug was introduced by HBASE-2889.  It is psuedo-fixed in the trunk & I have a more permanent fix up on the JIRA.  Use reflection to maintain the original error type and added pertinent info.;;;","24/Sep/10 20:23;jdcryans;Thanks Nicolas!;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bring root and meta over to new naming scheme (hbase-2531),HBASE-2639,12465826,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,01/Jun/10 04:33,11/Jun/22 23:43,14/Jul/23 06:06,16/Jul/14 22:06,,,,,,,,,,,,,,,,,,,0,,,hbase-2531 introduced new region naming scheme.  root and meta were left using old scheme.  This issue is about doing work to bring these over too.,,apurtell,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26399,,,,,Wed Jul 16 22:06:49 UTC 2014,,,,,,,,,,"0|i02dif:",11790,,,,,,,,,,,,,,,,,,,,,"01/Jun/10 20:36;stack;Here is a Kannan comment from tail of hbase-2531.

{code}
Todd/Stack: ROOT and the first META region are handled in a special hard-coded way in code. See HRegionInfo::ROOT_REGIONINFO and HRegion::FIRST_META_REGIONINFO.

For example, their region id is hardcoded to 0 & 1 (and they do not use timestamps).

ROOT,,0
.META.,,1

Their Jenkins hash encoded names: 70236052 & 1028785192 respectively do not collide either. Not changing this simplifies handling upgrade from an older version of HBase.

This should not be an issue for splits of META. If META splits, it should use the new format region names (with timestamps for the region id portion) and md5 hash for encoded names.
{code}

Yea, they are done different to the rest and I was thinking later than when .META. splits, it'll split in new style so there should be no clash of filesystem directory names.  I'll leave this issue open though because it'd be best to have these align anyways if only to save on the ""why do these work different?"" questions, down the road.;;;","16/Jul/14 22:06;apurtell;Handled by subsequent issues;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ImmutableBytesWritable ignores offset in several cases,HBASE-2635,12465814,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,tlipcon,tlipcon,tlipcon,01/Jun/10 00:40,12/Oct/12 06:15,14/Jul/23 06:06,01/Jun/10 20:55,0.20.4,0.90.0,,,,,,,,,,0.20.5,0.90.0,,,,,,0,,,"HBASE-2378 improved ImmutableBytesWritable's comparator, but there's still a bug! We assume offset = 0, which makes the comparator function incorrectly when the ImmutableBytesWritable is a slice into a larger byte array.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/10 06:17;tlipcon;hbase-2635.txt;https://issues.apache.org/jira/secure/attachment/12445999/hbase-2635.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26397,Reviewed,,,,Tue Jun 01 20:55:12 UTC 2010,,,,,,,,,,"0|i08skn:",49229,,,,,,,,,,,,,,,,,,,,,"01/Jun/10 06:28;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/110/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Various fixes + unit test


This addresses bug HBASE-2635.
    http://issues.apache.org/jira/browse/HBASE-2635


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java 0a9ec4b 
  src/test/java/org/apache/hadoop/hbase/io/TestImmutableBytesWritable.java 1e8886b 

Diff: http://review.hbase.org/r/110/diff


Testing
-------


Thanks,

Todd


;;;","01/Jun/10 20:32;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/110/#review109
-----------------------------------------------------------

Ship it!


+1


src/main/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java
<http://review.hbase.org/r/110/#comment661>

    Why this change?  Faster and good enough?


- stack



;;;","01/Jun/10 20:40;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/110/#review110
-----------------------------------------------------------



src/main/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java
<http://review.hbase.org/r/110/#comment662>

    Before we were hashing the entire byte array, not just the section of bytes corresponding to this writable. So the hashcode could be different even if the sliced bytes were the same. See new unit test for hashcode.


- Todd



;;;","01/Jun/10 20:55;tlipcon;Committed to trunk and 0.20 branch. Thanks for review, Stack.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
REST XML schema description up on the wiki is out of date,HBASE-2633,12465743,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,apurtell,apurtell,apurtell,30/May/10 19:51,12/Oct/12 06:16,14/Jul/23 06:06,06/Jun/10 07:24,,,,,,,,,,,,0.20.6,0.90.0,,,,,,0,,,The REST XML schema description up on the wiki is out of date. Fix it. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/10 17:07;apurtell;HBASE-2633.patch;https://issues.apache.org/jira/secure/attachment/12446157/HBASE-2633.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26396,,,,,Wed Jun 02 17:25:54 UTC 2010,,,,,,,,,,"0|i08sbj:",49188,,,,,,,,,,,,,,,,,,,,,"30/May/10 23:42;stack;or rather than update it., mark it obsolete and add a pointer over to stargate doc;;;","02/Jun/10 17:25;stack;Oh, that kinda schema?  That looks great.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix link to SVN repository on HBase website,HBASE-2622,12465636,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,hammer,hammer,hammer,28/May/10 09:43,11/Jun/22 23:41,14/Jul/23 06:06,29/May/10 01:15,,,,,,,,,,,,,,documentation,,,,,0,,,"We currently point to http://svn.apache.org/repos/asf/hadoop/hbase on the website, when we should point to http://svn.apache.org/repos/asf/hbase (and similar).",,hammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/May/10 09:55;hammer;HBASE-2622.patch;https://issues.apache.org/jira/secure/attachment/12445765/HBASE-2622.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26392,,,,,Sat May 29 01:15:12 UTC 2010,,,,,,,,,,"0|i0hiin:",100262,,,,,,,,,,,,,,,,,,,,,"28/May/10 09:55;hammer;Note that patch is relative to site/. I can change if necessary. The patch includes a diff for a PDF which is nasty. I can remove that too.;;;","29/May/10 01:15;stack;I put a site up at hbase.apache.org.  As part of the process I fixed links including this issues svn locations.  I added to our old location an entry in the hadoop .htaccess file that will redirect any hadoop.apache.org/hbase reference up to hbase.apache.org.

Resolving.  Assigning Jeff cos he did work.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix broken link in HFile Javadoc,HBASE-2621,12465635,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,hammer,hammer,hammer,28/May/10 09:38,20/Nov/15 12:40,14/Jul/23 06:06,28/May/10 18:23,,,,,,,,,,,,0.90.0,,documentation,io,,,,0,,,"There's a bad link in the HFile javadoc; should point to HBASE-61, but instead point to HBASE-3315. Also cleaned up the reference to TFile.",,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/May/10 09:39;hammer;HBASE-2621;https://issues.apache.org/jira/secure/attachment/12445764/HBASE-2621",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26391,Reviewed,,,,Fri Nov 20 12:40:31 UTC 2015,,,,,,,,,,"0|i0hiif:",100261,,,,,,,,,,,,,,,,,,,,,"28/May/10 09:54;hammer;Note that patch is relative to trunk/. I can change if necessary.;;;","28/May/10 18:23;tlipcon;Commited. Thanks for the fix, Jeff.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
REST tests don't use ephemeral ports,HBASE-2620,12465615,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,apurtell,tlipcon,tlipcon,28/May/10 03:27,20/Nov/15 12:41,14/Jul/23 06:06,01/Jun/10 03:58,,,,,,,,,,,,0.90.0,,REST,test,,,,0,,,"Saw this failure on my hudson:

java.net.BindException: Address already in use
...elided...
	at org.apache.hadoop.hbase.rest.HBaseRESTClusterTestBase.startServletContainer(HBaseRESTClusterTestBase.java:60)
	at org.apache.hadoop.hbase.rest.HBaseRESTClusterTestBase.setUp(HBaseRESTClusterTestBase.java:27)
	at org.apache.hadoop.hbase.rest.TestTableResource.setUp(TestTableResource.java:63)
",,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/10 03:57;apurtell;HBASE-2620.patch;https://issues.apache.org/jira/secure/attachment/12445992/HBASE-2620.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26390,,,,,Fri Nov 20 12:41:12 UTC 2015,,,,,,,,,,"0|i0hii7:",100260,,,,,,,,,,,,,,,,,,,,,"01/Jun/10 03:58;apurtell;Committed.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase shell 'alter' command cannot set table properties to False,HBASE-2619,12465605,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,,bowlinearl,bowlinearl,27/May/10 22:07,20/Nov/15 12:42,14/Jul/23 06:06,27/May/10 22:24,0.20.4,,,,,,,,,,,0.90.0,,scripts,,,,,0,,,"The alter table command in the HBase shell is broken. Specifically, attempting to set boolean table options like BLOCKCACHE to False does not result in the expected behavior.

This is due to bugs in the file src/main/ruby/hbase/admin.rb, in the hcd() function. There many statements that look like this:

arg[IN_MEMORY]? JBoolean.valueOf(arg[IN_MEMORY]): HColumnDescriptor::DEFAULT_IN_MEMORY

The intent of this code is to 1) determine if a given parameter is present in the associative array, 2) use the supplied value if it exists, or 3) use the default value if it does not exist. However, in the case of boolean parameters that have been set to False this code instead evaluates the variable (to False) and then chooses the default parameter value, which is incorrect.

The attached patch fixes the issue.",CentOS,larsfrancke,,,,,,,,,,,,,,,,,,,,,60,60,,0%,60,60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/May/10 22:11;bowlinearl;hbase_shell_alter_v1.patch;https://issues.apache.org/jira/secure/attachment/12445714/hbase_shell_alter_v1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26389,Reviewed,,,,Fri Nov 20 12:42:40 UTC 2015,,,,,,,,,,"0|i0hihz:",100259,,,,,,,,,,,,hirb shell alter table,,,,,,,,,"27/May/10 22:10;bowlinearl;Fixes the alter table command in the HBase shell.;;;","27/May/10 22:11;bowlinearl;Patch to fix the alter table issue.;;;","27/May/10 22:24;stack;Committed to trunk.  Thank you for the ruby foo Christo.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Load balancer falls into pathological state if one server under average - slop; endless churn",HBASE-2617,12465520,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,stack,stack,26/May/10 23:26,12/Oct/12 06:15,14/Jul/23 06:06,27/May/10 18:14,,,,,,,,,,,,0.20.5,,,,,,,0,,,"I'm looking at a 0.20.4 cluster of 80 fast machines.  It runs fine for a while and then falls over into crazy balancing churn (My view on logs is sporadic but have a log before me where this is happening).  Things I see that seem to be of 0.20.4 particularly:

+ We don't reach an equilibrium or at least it takes so long, its as though it wasn't every going to happen
+ Master log filled w/ open, close of one or two regions  usually the same ones over and over (The Regions that are candidates to close are provided by the RS.  They are ordered by hash of their name.  We return the top ten from this Set every time.  So, we always close the same regions all the time even if we just opened it)
+ Often, we'll tell an RS to close a region.  It will do the job.  In 0.20.4 we made it so if RS has any work at all for the master, that we return immediately rather than wait for the reporting period to elaspse.   So, on these fast machines, it can be back near immediately if it just opened another some other region, say.  It can get assigned the region it just closed.  Seems to happen frequently enough.

For example, look at the below extract featuring a single regions life.  Its opened and closed 5 times in about 1/2 a second:

{code}
2010-05-25 11:01:05,488 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessRegionClose of GenericMetaStore,139757491,1274779304880, false, reassign: true
2010-05-25 11:01:05,489 INFO org.apache.hadoop.hbase.master.ProcessRegionClose$1: region set as unassigned: GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,490 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region GenericMetaStore,139757491,1274779304880 to a025.example.com,60020,1274744064673
2010-05-25 11:01:05,510 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: GenericMetaStore,139757491,1274779304880 from a025.example.com,60020,1274744064673; 1 of 1
2010-05-25 11:01:05,510 INFO org.apache.hadoop.hbase.master.RegionServerOperation: GenericMetaStore,139757491,1274779304880 open on 10.209.32.189:60020
2010-05-25 11:01:05,511 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row GenericMetaStore,139757491,1274779304880 in region .META.,,1 with startcode=1274744064673, server=10.209.32.189:60020
2010-05-25 11:01:05,548 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,552 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_CLOSE: GenericMetaStore,139757491,1274779304880 from a025.example.com,60020,1274744064673; 1 of 2
2010-05-25 11:01:05,552 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessRegionClose of GenericMetaStore,139757491,1274779304880, false, reassign: true
2010-05-25 11:01:05,552 INFO org.apache.hadoop.hbase.master.ProcessRegionClose$1: region set as unassigned: GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,556 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region GenericMetaStore,139757491,1274779304880 to a028.example.com,60020,1274747560769
2010-05-25 11:01:05,578 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: GenericMetaStore,139757491,1274779304880 from a028.example.com,60020,1274747560769; 1 of 1
2010-05-25 11:01:05,578 INFO org.apache.hadoop.hbase.master.RegionServerOperation: GenericMetaStore,139757491,1274779304880 open on 10.209.32.185:60020
2010-05-25 11:01:05,579 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row GenericMetaStore,139757491,1274779304880 in region .META.,,1 with startcode=1274747560769, server=10.209.32.185:60020
2010-05-25 11:01:05,599 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,605 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_CLOSE: GenericMetaStore,139757491,1274779304880 from a028.example.com,60020,1274747560769; 1 of 2
2010-05-25 11:01:05,605 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessRegionClose of GenericMetaStore,139757491,1274779304880, false, reassign: true
2010-05-25 11:01:05,606 INFO org.apache.hadoop.hbase.master.ProcessRegionClose$1: region set as unassigned: GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,607 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region GenericMetaStore,139757491,1274779304880 to sjc1c104.example.com,60020,1274747062601
2010-05-25 11:01:05,640 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: GenericMetaStore,139757491,1274779304880 from sjc1c104.example.com,60020,1274747062601; 1 of 1
2010-05-25 11:01:05,640 INFO org.apache.hadoop.hbase.master.RegionServerOperation: GenericMetaStore,139757491,1274779304880 open on 10.209.42.181:60020
2010-05-25 11:01:05,641 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row GenericMetaStore,139757491,1274779304880 in region .META.,,1 with startcode=1274747062601, server=10.209.42.181:60020
2010-05-25 11:01:05,723 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,729 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_CLOSE: GenericMetaStore,139757491,1274779304880 from sjc1c104.example.com,60020,1274747062601; 1 of 4
2010-05-25 11:01:05,729 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessRegionClose of GenericMetaStore,139757491,1274779304880, false, reassign: true
2010-05-25 11:01:05,730 INFO org.apache.hadoop.hbase.master.ProcessRegionClose$1: region set as unassigned: GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,731 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region GenericMetaStore,139757491,1274779304880 to sjc1c091.example.com,60020,1274747056415
2010-05-25 11:01:05,751 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: GenericMetaStore,139757491,1274779304880 from sjc1c091.example.com,60020,1274747056415; 1 of 1
2010-05-25 11:01:05,752 INFO org.apache.hadoop.hbase.master.RegionServerOperation: GenericMetaStore,139757491,1274779304880 open on 10.209.42.238:60020
2010-05-25 11:01:05,752 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row GenericMetaStore,139757491,1274779304880 in region .META.,,1 with startcode=1274747056415, server=10.209.42.238:60020
2010-05-25 11:01:05,775 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,780 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_CLOSE: GenericMetaStore,139757491,1274779304880 from sjc1c091.example.com,60020,1274747056415; 1 of 2
2010-05-25 11:01:05,780 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessRegionClose of GenericMetaStore,139757491,1274779304880, false, reassign: true
2010-05-25 11:01:05,780 INFO org.apache.hadoop.hbase.master.ProcessRegionClose$1: region set as unassigned: GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,808 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region GenericMetaStore,139757491,1274779304880 to sjc1a003.example.com,60020,1274747057557
2010-05-25 11:01:05,828 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: GenericMetaStore,139757491,1274779304880 from sjc1a003.example.com,60020,1274747057557; 1 of 1
2010-05-25 11:01:05,828 INFO org.apache.hadoop.hbase.master.RegionServerOperation: GenericMetaStore,139757491,1274779304880 open on 10.209.32.148:60020
2010-05-25 11:01:05,829 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row GenericMetaStore,139757491,1274779304880 in region .META.,,1 with startcode=1274747057557, server=10.209.32.148:60020
{code}

The culprit seems to be the code that wants to bring up underloaded regionservers up to average.  That and something about the lightly loaded servers math that is off.

I'm marking this as a blocker.  I'm not sure why its not more common.  There were some issues on this cluster regards disks filling but though such an event may have provoked the issue, we should have evened out eventually.

Making this a blocker on 0.20.5.   Need to fix it for this user at least.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/10 07:42;stack;2617-v2-trunk.txt;https://issues.apache.org/jira/secure/attachment/12447810/2617-v2-trunk.txt","27/May/10 17:26;stack;2617-v2.txt;https://issues.apache.org/jira/secure/attachment/12445676/2617-v2.txt","27/May/10 16:45;stack;2617.txt;https://issues.apache.org/jira/secure/attachment/12445674/2617.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26388,Reviewed,,,,Wed Jun 23 20:31:43 UTC 2010,,,,,,,,,,"0|i08sd3:",49195,,,,,,,,,,,,,,,,,,,,,"26/May/10 23:50;stack;Issue seems to be brought on by a combination of a change made back in 0.20.1 (HBASE-1868) and new liveness where we send master as soon as we have something to send.   I think I know how to reproduce.  Working on that first.  Then patch.;;;","27/May/10 16:45;stack;Patch that mostly cleans up confusing logging that was announcing we're assigning but then we might not in the end and then does this, the core change, removing the below:

{code}
-    if (nRegionsToAssign <= nregions) {
-      // I do not know whats supposed to happen in this case.  Assign one.
-      LOG.debug(""Assigning one region only (playing it safe..)"");
-      assignRegions(regionsToAssign, 1, info, returnMsgs);
{code}

This 'assigning one region' was breaking the balancer.  If lots of servers and only a few lightly loaded, then we'd go into a flurry.;;;","27/May/10 17:14;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/97/
-----------------------------------------------------------

Review request for hbase and Ryan Rawson.


Summary
-------

Fix for pathological balancer.  Mostly logging changes to remove confusion as to what the balancer is up to but also removed a mistake where we'd always give a server one region ('just in case"") if regions to assign though it might be a server that is loaded.  In clusters w/ lots of nodes with a few of the nodes not in-balance, the lightly loaded nodes would never get regions so we'd just keep reassigning and never settle.


This addresses bug hbase-2617.


Diffs
-----

  branches/0.20/src/java/org/apache/hadoop/hbase/master/RegionManager.java 948860 

Diff: http://review.hbase.org/r/97/diff


Testing
-------


Thanks,

stack


;;;","27/May/10 17:26;stack;Yet more detail in logs.;;;","27/May/10 17:34;hbasereviewboard;Message from: ""Jonathan Gray"" <jgray@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/97/#review85
-----------------------------------------------------------

Ship it!


This seems to fix the problem described.  I recall one other weird issue in the load balancing when reviewing that code before the recent rewrite for trunk, but can't find it now.  The way I had read this previously was it seemed to almost always fall into the ""don't know what to do, just assign one"" branch, but looks better now.

Do we have a way to add additional regionservers in a cluster test but not during the initialization where we do it now?  That might be one way we could test interesting load balancing situations.

Only nitpick is the log messages used to say overloaded but now say ""more than their fair share""... overloaded seemed descriptive enough :)

In any case, this is better than what is there now.  +1

- Jonathan


On 2010-05-27 10:13:14, stack wrote:
^bq.  
^bq.  -----------------------------------------------------------
^bq.  This is an automatically generated e-mail. To reply, visit:
^bq.  http://review.hbase.org/r/97/
^bq.  -----------------------------------------------------------
^bq.  
^bq.  (Updated 2010-05-27 10:13:14)
^bq.  
^bq.  
^bq.  Review request for hbase and Ryan Rawson.
^bq.  
^bq.  
^bq.  Summary
^bq.  -------
^bq.  
^bq.  Fix for pathological balancer.  Mostly logging changes to remove confusion as to what the balancer is up to but also removed a mistake where we'd always give a server one region ('just in case"") if regions to assign though it might be a server that is loaded.  In clusters w/ lots of nodes with a few of the nodes not in-balance, the lightly loaded nodes would never get regions so we'd just keep reassigning and never settle.
^bq.  
^bq.  
^bq.  This addresses bug hbase-2617.
^bq.  
^bq.  
^bq.  Diffs
^bq.  -----
^bq.  
^bq.    branches/0.20/src/java/org/apache/hadoop/hbase/master/RegionManager.java 948860 
^bq.  
^bq.  Diff: http://review.hbase.org/r/97/diff
^bq.  
^bq.  
^bq.  Testing
^bq.  -------
^bq.  
^bq.  
^bq.  Thanks,
^bq.  
^bq.  stack
^bq.  
^bq. 


;;;","27/May/10 18:14;stack;I applied patch.  Thanks for review Jon (I changed message to say 'overloaded' again).;;;","23/Jun/10 07:43;stack;Forgot to apply to trunk.  Just did.  Attached patch.;;;","23/Jun/10 20:31;tlipcon;Committed this to 0.89.20100621 as well (strange to have a bug fix in 0.20.5 but not this dev release);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestHRegion.testWritesWhileGetting flaky on trunk,HBASE-2616,12465487,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,ryanobjc,tlipcon,tlipcon,26/May/10 16:43,12/Oct/12 06:15,14/Jul/23 06:06,19/Jun/10 19:22,,,,,,,,,,,,0.20.5,,regionserver,,,,,0,,,"Saw this failure on my internal hudson:

junit.framework.AssertionFailedError: expected:<\x00\x00\x00\x96> but was:<\x00\x00\x01\x00>
	at org.apache.hadoop.hbase.HBaseTestCase.assertEquals(HBaseTestCase.java:684)
	at org.apache.hadoop.hbase.regionserver.TestHRegion.testWritesWhileGetting(TestHRegion.java:2334)
",,hammer,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2627,,,,HBASE-2670,,,,HBASE-2474,,,,,,,,,,,,,"05/Jun/10 01:25;ryanobjc;HBASE-2616.patch;https://issues.apache.org/jira/secure/attachment/12446394/HBASE-2616.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26387,,,,,Sat Jun 19 19:22:31 UTC 2010,,,,,,,,,,"0|i08skf:",49228,,,,,,,,,,,,,,,,,,,,,"03/Jun/10 21:43;stack;Marking against 0.20.5 and moving comment I put in duplicate issue here: ""Made this a blocker. See how builds on hudson.hbase.org alternate between successful build and a build that fails this test. This is the only issue in way of a 0.20.5."";;;","05/Jun/10 01:25;ryanobjc;here is an candidate patch... give it a shot on your hudson instance guys.;;;","05/Jun/10 04:54;stack;Thanks Ryan.  I committed it after all tests passed locally (0.20 build takes one hour for me!).  Committed so can try it up on Andrew's super flakey ec2 hudson server.  If it passes 4 times w/o failure, we said that means good enough to ship.  Trying it now.;;;","05/Jun/10 04:55;stack;Oh, ""Andrew's super flakey ec2 hudson server"" is AKA hudson.hbase.org.;;;","05/Jun/10 04:56;stack;The next build that should have this patch ie #70.;;;","05/Jun/10 05:02;tlipcon;Ryan, why does the test now flush the cache before joining? Are we masking an issue here?;;;","05/Jun/10 20:48;stack;Ten builds have passed up on hudson.hbase.org since I committed patch last night.

I just committed this:

{code}
Index: src/test/org/apache/hadoop/hbase/regionserver/TestHRegion.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestHRegion.java      (revision 951761)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestHRegion.java      (working copy)
@@ -2269,8 +2269,6 @@
 
     putThread.done();
 
-    region.flushcache();
-
     putThread.join();
     putThread.checkNoError();
{code}

... to try and get an answer to Todd's question.  In local tests its fine.  Lets see how it does up on hudson.hbase.org (Its a nice day in SF.  Ryan might not be around for a while).;;;","05/Jun/10 21:11;ryanobjc;The flushcache is an old fix that helped make the tests less flakey.  As you may remember there are 2 threads:
- getting thread which when finished sets a 'done' flag
- putting thread

well the putting thread can get caught up in the HRegion calls when the cache is full (this happens on my laptop frequently), so before we wait on the put thread to be finished, we flush the cache which unblocks the put thread.

Note this happens _AFTER_ the asserts, therefore this has _nothing_ to do with the test failures.  This is how the code is on trunk, and how it should be.;;;","05/Jun/10 21:30;stack;Ok... starting build 81.  This build will be w/ the removed flush above.;;;","05/Jun/10 21:31;stack;http://hudson.hbase.org/job/hbase-branch-0.20/81/;;;","05/Jun/10 21:58;tlipcon;OK, that seems reasonable. A comment to that effect would be nice, though, so people reading the test cases understand it's for convenience/test speed, not because it has something to do with the test.;;;","05/Jun/10 22:03;ryanobjc;well its not just about test speed, because without this the test
never finishes on my laptop :-)



;;;","05/Jun/10 22:06;tlipcon;hm, then I'm confused :) If the put thread is blocked on full hregions, why doesn't that trigger a flush of its own?;;;","05/Jun/10 22:19;ryanobjc;it has to do because we are running with a standalone HRegion without
all the corresponding background threads.  normally yes, in this case
not as much!




;;;","05/Jun/10 22:25;stack;Bldg #81 passed and #82 is running.  Not definitive proof but....  Let me put back the flush.  Going to go ahead and cut a 0.20.5 RC.  Thanks all.;;;","05/Jun/10 22:27;stack;hmmm.. just thinking that all these tests up on hudson.hbase.org are passing because its w/e and ec2 is quiet.  Monday when the load comes on again, it'll all start failing again I suppose.  Anyways, cutting a 0.20.5.;;;","06/Jun/10 05:16;stack;So build #82 failed, that'd be w/o the flush, but so did #83 which seems to be w/ the fush put back (#84 succeeded).  I'm going to go ahead and roll the candidate for now but there may be something in here still.  I'm going to keep triggering builds on this hudson.hbase.org to see if we get any more fails.;;;","07/Jun/10 17:28;stack;Builds still failing on this test: See #90, #91 and #94 builds.;;;","07/Jun/10 23:33;stack;This test runs for 2minutes and 30seconds up on hudson.  Does it have to?  The testWritesWhileScanning  runs 30 seconds.;;;","07/Jun/10 23:40;ryanobjc;the test should not take that long, but it is slow ... seems like the
get() calls are strangely slow, i think because of the nature of
piling multiple version on top of older versions.  maybe some
improvements in the get/scan calls in the single thick row case would
make this faster.


;;;","07/Jun/10 23:41;tlipcon;wait, how did test performance come up here? shouldn't we fix it before we make it faster? :);;;","18/Jun/10 20:28;jdcryans;Looks like it was committed, can we close this?;;;","19/Jun/10 19:22;stack;Closing for Ryan. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M/R on bulk imported tables,HBASE-2615,12465486,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,azzadev,azzadev,26/May/10 16:40,12/Oct/12 06:15,14/Jul/23 06:06,06/Jun/10 06:51,0.20.3,0.20.4,,,,,,,,,,0.20.5,0.90.0,,,,,,0,,,"We are bulk importing using loadtable.rb and running M/R jobs using HBase as input.

We're taking the following steps:
1a. Load HBase with a M/R job using the normal API. 
OR
1b. Load HBase with bulk import.

THEN

2a. Using the shell, do a ""count"" over the table.
OR
2b. Run a M/R job that scans the whole HBase table (and nothing else).

Of the 4 combos, 3 are fine: 1a+2a, 1a+2b, 1b+2a.  We're having trouble with 1b+2b.  When we run the M/R job, it doesn't seem to read in any records, but there are no explicit errors in either the Hadoop or HBase logs.

Any ideas on what might be wrong with the bulk import to cause this problem?  We confirmed this problem exists in both hbase-0.20.3 and hbase-0.20.4.

We have created dummy data (see attached). This is the test case:

After loading the data into HDFS. In hbase shell:
create 'tiny', 'values'

Execute: 
{HBASE-HOME}/bin/hbase org.jruby.Main {HBASE-HOME}/bin/loadtable.rb tiny tinytable

Then run the simple row counter
{HADOOP-HOME}/bin/hadoop jar {HBASE-HOME}/hbase-0.20.x.jar rowcounter tiny values

Notice that map input records read is always zero. We confirmed that other mapreduce jobs do not execute the map function at all, always returning 0 records.

We also ran a major_compaction of all Hbase tables (.META. and .ROOT. as well) but this did not fix the problem.","os.arch=amd64; os.version=2.6.9-67.ELsmp; java.version=1.6.0_15; java.vendor=Sun Microsystems Inc.
",azzadev,hammer,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/10 06:01;stack;2615.txt;https://issues.apache.org/jira/secure/attachment/12446230/2615.txt","26/May/10 16:49;azzadev;dummydata.tar.gz;https://issues.apache.org/jira/secure/attachment/12445563/dummydata.tar.gz",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26386,Reviewed,,,,Sun Jun 06 06:51:16 UTC 2010,,,,,,,,,,"0|i08sdr:",49198,,,,,,,,,,,,,,,,,,,,,"26/May/10 16:49;azzadev;Dummy HDFS data (properly formatted HFile) of 1000 records.;;;","27/May/10 23:09;stack;One problem is that we are making a region too many.  The first region has the empty string as the start and end rows.  This must be a bug in add_table.rb.   I deleted the bad region (it had nothing in it) but still the mr count fails.  Looking more.;;;","28/May/10 20:29;stack;Using hfile tool, I confirmed that the keys in the two files are ordered:

./bin/hbase org.apache.hadoop.hbase.io.hfile.HFile
;;;","02/Jun/10 22:19;stack;So, add table tool makes regions:

tiny,,1275509545408
tiny,user2028689897,1275509581851

This seems right.  Last key in one of the two files is user2026413677/values:field1/9223372036854775807/Put/vlen=1023.  First key in the second file is: user2028689897/values:field1/9223372036854775807/Put/vlen=1019 

Now I'm trying to dig in whats different between scan in shell and scan in MR.;;;","03/Jun/10 00:10;stack;The scanner is skipping all entries because timestamps on values do not fit within the Scanner's timerange specification.

Looking at the data files, indeed the timestamp is user1000319044/values:field1/9223372036854775807/Put/vlen=1023 is large, maximum?

How were these files written?  Maybe a bug in our hfile writer?;;;","03/Jun/10 00:14;stack;Was one of the KeyValue constructors that doesn't specify timestamps used emitting from map or reduce task?  If so, sounds like a documentation bug at the least.;;;","03/Jun/10 00:22;stack;So, let me fix the Record Writer so it catches this case.  Comes of a chat up on IRC w/ Todd:

{code}
17:15 < tlipcon> why does the MR job filter on timestamp?
17:16 < St^Ack> scanner does
17:16 < St^Ack> rowcounter will count whatever fits inside the Scan spec
17:16 < tlipcon> but counter doesn't specify a time range, does it?
17:16 < St^Ack> no
17:16 < St^Ack> but whats in data files is maximum timestamp
17:16 < tlipcon> is that Special?
17:16 < St^Ack> yeah, usually
17:16 < St^Ack> it gets replaced by current
17:17 < St^Ack> but writing hfiles, you are kinda bypassing the stuff that does the convertion for you
17:17 < tlipcon> ah I see
17:18 < tlipcon> good catch
17:18 < St^Ack> at least a doc fix...
17:18 < St^Ack> not sure what else we could do to stop folks hanging themselves
17:18 < tlipcon> HFileOutputFormat's recordwriter could be smarter
17:19 < tlipcon> check the KVs going out before writing
17:19 < St^Ack> add LATEST_TIMESTAMP check in there and substitute currentTimeMillis I suppose
17:20 < St^Ack> i think that'd work
{code};;;","03/Jun/10 00:49;azzadev;We modified the KeyValue constructor in our data generation script to include current timestamp and MR jobs work. Thanks, nice catch!

Perhaps, the API could only expose the constructor interfaces that require a timestamp or add a MIN/current timestamp by default to tuples instead of MAX to guarantee it being read.;;;","03/Jun/10 06:01;stack;Small patch and test.  Review please.

(I tried posting to review.hbase.org but it complains 'The file 'src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat.java' (r2a272af) could not be found in the repository' which seems to be way wrong);;;","03/Jun/10 06:02;stack;.bq Perhaps, the API could only expose the constructor interfaces that require a timestamp or add a MIN/current timestamp by default to tuples instead of MAX to guarantee it being read.

I made it so if a kv with maximum timestamp is emitted, we'll set it instead to now before writing.;;;","03/Jun/10 06:42;tlipcon;+1, lgtm

Sorry about reviewboard error, crontab was screwed up after switching to the new ec2 instance, should be fixed now.;;;","05/Jun/10 16:45;stack;Thanks for review Todd.  Committed to branch and trunk (on branch, no test because it junit4 added);;;","05/Jun/10 21:07;ryanobjc;so after this patch, we no longer allow user set timestamps?  Are you sure that's a good idea?;;;","05/Jun/10 21:40;stack;They can... We only change the timstamp to now if the timestamp in the kv is LATEST_TIMESTAMP (we chatted in gtalk and I showed Ryan  how the code does this -- he agreed it ok);;;","05/Jun/10 21:56;tlipcon;Looks like this actually broke the build. The problem seems to be that changing the timestamp at write time changes the sort order. See: http://hudson.zones.apache.org/hudson/job/HBase-TRUNK/1300/testReport/org.apache.hadoop.hbase.mapreduce/TestHFileOutputFormat/test_LATEST_TIMESTAMP_isReplaced/;;;","05/Jun/10 22:18;stack;Yeah, if timestamp changed, then we'd be out of order (ts should go from bigger to smaller).  Iwasn't using comparator... I committed fix that uses ts less than previosu;;;","06/Jun/10 06:51;stack;Closing.  It builds fine now on hudson after my fixup.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
killing server in TestMasterTransitions causes NPEs and test deadlock,HBASE-2614,12465425,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,apurtell,apurtell,26/May/10 03:01,20/Nov/15 12:43,14/Jul/23 06:06,05/Jun/10 05:23,,,,,,,,,,,,0.90.0,,,,,,,0,,,,,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/10 04:50;stack;2614.txt;https://issues.apache.org/jira/secure/attachment/12446398/2614.txt","05/Jun/10 05:21;stack;2615-v2.txt;https://issues.apache.org/jira/secure/attachment/12446402/2615-v2.txt","26/May/10 03:02;apurtell;org.apache.hadoop.hbase.master.TestMasterTransitions-output.txt.gz;https://issues.apache.org/jira/secure/attachment/12445523/org.apache.hadoop.hbase.master.TestMasterTransitions-output.txt.gz",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26385,,,,,Fri Nov 20 12:43:30 UTC 2015,,,,,,,,,,"0|i0hihj:",100257,,,,,,,,,,,,,,,,,,,,,"26/May/10 03:59;tlipcon;I've been seeing this test deadlock as well on my hudson.;;;","03/Jun/10 19:52;stack;I see  (Too many open files) in this log.

A few other things of interest are that the Master won't go down because it thinks there is still a regionserver alive.  Its stuck here.
{code}
Thread 144 (RegionManager.rootScanner):
  State: TIMED_WAITING
  Blocked count: 63
  Waited count: 333
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.hbase.master.RegionManager.waitForRootRegionLocation(RegionManager.java:1161)
    org.apache.hadoop.hbase.master.RootScanner.scanRoot(RootScanner.java:45)
    org.apache.hadoop.hbase.master.RootScanner.maintenanceScan(RootScanner.java:79)
    org.apache.hadoop.hbase.master.BaseScanner.chore(BaseScanner.java:154)
    org.apache.hadoop.hbase.Chore.run(Chore.java:68)
{code}

Other issue is that in our minihbasecluster, when we are asked to start a new server, we'll wait till its online only here, the new regionserver crashed on startup.  Need to insert isAlive check into loop waiting on new server to come online.   Patch coming.;;;","04/Jun/10 03:39;apurtell;I didn't think to raise the fh handle limits on our Hudson's user account. Will do so now, but it is a surprise the tests are so greedy. Glad this revealed a problem anyway.


;;;","04/Jun/10 03:43;stack;I'd say leave it.

Its actually exposed two problems that I see.

1. We are processing a server shutdown.  We don't remove the server from list of servers until the processing has completed so as far as master is concerned, there is still a RS out there and he won't go down till it does.   The server shutdown can't complete because dfs is gone.
2. In our test framework, a RS may fail to start properly but we'll just wait till it does anyways.

I'd say leave it.  Next time it might reveal more interesting stuff.;;;","04/Jun/10 06:47;stack;This test just hung up apache hudson with this over and over:

{code}
2010-06-04 05:34:59,307 INFO  [RegionServer:4.worker] regionserver.HRegionServer$Worker(1361): Regionserver blocked by TESTING_MSG_BLOCK_RS; false
2010-06-04 05:34:59,474 INFO  [RegionServer:2] regionserver.HRegionServer(488): MSG_REGIONSERVER_QUIESCE
2010-06-04 05:34:59,499 INFO  [RegionServer:4] regionserver.HRegionServer(488): MSG_REGIONSERVER_QUIESCE
2010-06-04 05:35:00,070 DEBUG [master] master.RegionManager(596): telling root scanner to stop
2010-06-04 05:35:00,070 DEBUG [master] master.RegionManager(600): telling meta scanner to stop
2010-06-04 05:35:00,070 DEBUG [master] master.RegionManager(604): meta and root scanners notified
...
{code}

Something went wonky.

See http://hudson.zones.apache.org/hudson/view/HBase/job/HBase-TRUNK/1291/artifact/trunk/target/surefire-reports/org.apache.hadoop.hbase.master.TestMasterTransitions-output.txt;;;","05/Jun/10 04:50;stack;This patch addresses issues seen in the log attached.

In the main, the issue is that a regionserver failed to init completely because of:

{code}
java.lang.NullPointerException
    at org.apache.hadoop.ipc.Client$Connection.handleConnectionFailure(Client.java:351)
    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:313)
    at org.apache.hadoop.ipc.Client$Connection.access$1700(Client.java:176)
    at org.apache.hadoop.ipc.Client.getConnection(Client.java:860)
    at org.apache.hadoop.ipc.Client.call(Client.java:720)
    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
    at $Proxy7.getProtocolVersion(Unknown Source)
    at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
    at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
    at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
    at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
    at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:95)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.init(HRegionServer.java:746)
    at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.init(MiniHBaseCluster.java:161)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:427)
    at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.run(MiniHBaseCluster.java:169)
    at java.lang.Thread.run(Thread.java:619)
{code}

The was inside its run method trying to get filesystem instance AFTER it had registered itself with zk.

Our test utils wait on the regionserver to set its online flag before letting things proceed.  There was on provision for failed startup so test harness was waiting for ever on an online flag that would never be set.   The thread dumps were showing this for the testing harness:

{code}
Thread 770 (Thread-692):
  State: TIMED_WAITING
  Blocked count: 134
  Waited count: 1362
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread.waitForServerOnline(JVMClusterUtil.java:60)
    org.apache.hadoop.hbase.MiniHBaseCluster.startRegionServer(MiniHBaseCluster.java:227)
    org.apache.hadoop.hbase.master.TestMasterTransitions.testAddingServerBeforeOldIsDead2413(TestMasterTransitions.java:281)
    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    java.lang.reflect.Method.invoke(Method.java:597)
    org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    org.junit.internal.runners.statements.FailOnTimeout$1.run(FailOnTimeout.java:28)
Thread 539 (org.apache.hadoop.hdfs.server.datanode.DataXceiver@19789a96):
{code}

Now, the RS crashes after its registered w/ zk so it goes to abort, ONLY, in the abort, we throw a NPE because presumption is that metrics have been initialized, only we haven't got that far in init, so the abort is aborted; i.e. we don't call the stop method.  So, the RS is sort of still alive, alive enough to keep on hosting the zk client polling master as though the RS were still alive.

Meanwhile over on the master, we want to go out because test says time for shutdown only we won't shutdown till all regionservers have closed and deregistered themselves from the master.   The above RS failed after it repoted to the master for duty so master knows about it and the RS is in a zombie state that keeps up its zk lease, so master thinks a RS out these is still alive.

Patch makes it so test framework won't wait if RS has shutdownRequested set (which it will when aborting), it fixes abort so no presumptions about metrics being initialized, and over on master, we'll print out servers we're waiting on up in main loop too to make this kind of thing easier debugging going forward.  ;;;","05/Jun/10 04:54;tlipcon;Is this related to https://issues.apache.org/jira/browse/HBASE-2441 ?
Also, the NPE in handleConnectionFailure sounds interesting. Can you file a JIRA for that one? I think I fixed a similar bug in Hadoop IPC but maybe I missed the ""side port"" over to HBase IPC?;;;","05/Jun/10 05:21;stack;This version adds a config that stops us dumping out all regions in the one go.  Be less lumpy.  Get a few cycles in there rather than do all in one go.  This hopefully addresses the latter hang we saw because we missed a barrier (We needed an extra cycle  for close to go across before we did server abort).  If still too brittle, I'll dig more.;;;","05/Jun/10 05:23;stack;Committed small(ish) patch that should help w/ builds.  Will open new issue if this set not good enough.;;;","05/Jun/10 05:26;tlipcon;patch looks good to me. we'll see if my Hudson barfs less :);;;","05/Jun/10 05:33;stack;Just saw your message now Todd.  Yeah, first part of 2441 is duplicated in this patch.  Why not just commit the rest of 2441.  Let me go +1 it.  Its no brainer whereas the unit test would be a pain to cast.  Let me file a new issue for the original NPE.  We should probably try and just bring over original RPC from hadoop anyways wholesale.  It'll make Gary Helmling's life bring over hadoop security.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ValueFilter copy pasted javadoc from QualifierFilter,HBASE-2610,12465292,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,24/May/10 17:44,12/Oct/12 06:15,14/Jul/23 06:06,26/May/10 08:49,,,,,,,,,,,,0.20.5,0.90.0,,,,,,0,,,"I see this in ValueFilter:

{code}
  /**
   * Constructor.
   * @param valueCompareOp the compare op for column qualifier matching
   * @param valueComparator the comparator for column qualifier matching
   */
  public ValueFilter(final CompareOp valueCompareOp,
      final WritableByteArrayComparable valueComparator) {
    super(valueCompareOp, valueComparator);
  }
{code}

Obviously it's not for column qualifier matching but for value matching.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26383,Reviewed,,,,Wed May 26 08:49:19 UTC 2010,,,,,,,,,,"0|i08shz:",49217,,,,,,,,,,,,,,,,,,,,,"24/May/10 18:50;jdcryans;Marking it against 0.20.6 since 0.20.5 already has an RC.;;;","26/May/10 08:49;stack;Committed branch and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"NativeException: org.apache.hadoop.hbase.client.NoServerForRegionException: No server address listed in .META. for region item,,1274235159640 ",HBASE-2606,12465234,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,spullara,spullara,24/May/10 00:32,12/Oct/12 06:15,14/Jul/23 06:06,25/May/10 00:28,0.20.4,,,,,,,,,,,0.20.5,,,,,,,0,,,"Though the region is listed when you attempt to access it you get an error that it is not assigned to a region.  In the log you find this repeating over and over:

2010-05-23 16:10:08,958 INFO org.apache.hadoop.hbase.master.ServerManager: 1 region servers, 0 dead, average load 5.0
2010-05-23 16:10:09,519 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scanning meta region {server: 10.211.55.4:60020, regionname: -ROOT-,,0, startKey: <>}
2010-05-23 16:10:09,529 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scan of 1 row(s) of meta region {server: 10.211.55.4:60020, regionname: -ROOT-,,0, startKey: <>} complete
2010-05-23 16:10:52,178 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scanning meta region {server: 10.211.55.4:60020, regionname: .META.,,1, startKey: <>}
2010-05-23 16:10:52,188 DEBUG org.apache.hadoop.hbase.master.BaseScanner: Current assignment of item,,1274235159640 is not valid;  serverAddress=, startCode=0 unknown.
2010-05-23 16:10:52,193 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scan of 3 row(s) of meta region {server: 10.211.55.4:60020, regionname: .META.,,1, startKey: <>} complete
2010-05-23 16:10:52,193 INFO org.apache.hadoop.hbase.master.BaseScanner: All 1 .META. region(s) scanned
2010-05-23 16:10:53,139 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region item,,1274235159640 to hbase,60020,1274655730505
2010-05-23 16:10:53,143 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: item,,1274235159640 from hbase,60020,1274655730505; 1 of 1
2010-05-23 16:10:53,143 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: PendingOpenOperation from hbase,60020,1274655730505
2010-05-23 16:10:53,143 INFO org.apache.hadoop.hbase.master.RegionServerOperation: item,,1274235159640 open on 10.211.55.4:60020
2010-05-23 16:10:53,146 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row item,,1274235159640 in region .META.,,1 with startcode=1274655730505, server=10.211.55.4:60020

Doing a flush and a major_compaction of the table resets it. I have a VM snapshot that has the behavior and it is reproducible at will.  Attached are the log files from the .META. region at jdcryans request.","Linux x86_64, java 1.6.0_20, hbase 0.20.4, CDH3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/10 00:36;spullara;info.tar.gz;https://issues.apache.org/jira/secure/attachment/12445274/info.tar.gz",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26382,,,,,Tue May 25 00:28:37 UTC 2010,,,,,,,,,,"0|i08sjz:",49226,,,,,,,,,,,,,,,,,,,,,"24/May/10 00:36;spullara;These are the 3 log files for .META. before the major_compaction that fixes the issue:


[root@rytym ~]# hadoop fs -ls /hbase/.META./102*/info
Found 2 items
-rw-r--r--   3 root supergroup       5397 2010-05-23 13:30 /hbase/.META./1028785192/info/1715090119177974984
-rw-r--r--   3 root supergroup        889 2010-05-23 16:02 /hbase/.META./1028785192/info/558364561142792861
[root@rytym ~]# hbase shell
HBase Shell; enter 'help<RETURN>' for list of supported commands.
Version: 0.20.4, r941076, Tue May  4 16:23:06 PDT 2010
hbase(main):001:0> flush '.META.'
0 row(s) in 0.0410 seconds
hbase(main):002:0> quit
[root@rytym ~]# hadoop fs -ls /hbase/.META./102*/info
Found 3 items
-rw-r--r--   3 root supergroup       5397 2010-05-23 13:30 /hbase/.META./1028785192/info/1715090119177974984
-rw-r--r--   3 root supergroup        889 2010-05-23 16:02 /hbase/.META./1028785192/info/558364561142792861
-rw-r--r--   3 root supergroup       2498 2010-05-23 16:16 /hbase/.META./1028785192/info/6863843601709995931
;;;","24/May/10 23:53;ryanobjc;these keys are in the 'middle file':

K: item,,1274235159640/info:server/9223372036854775807/DeleteColumn/vlen=0 V:
K: item,,1274235159640/info:server/1274556669196/Put/vlen=17 V: 10.211.55.4:60020
K: item,,1274235159640/info:serverstartcode/9223372036854775807/DeleteColumn/vlen=0 V:
K: item,,1274235159640/info:serverstartcode/1274556669196/Put/vlen=8 V: \x00\x00\x01\x28\xC1\x7E\xA2j

In this case we have a DeleteColumn on info:server and info:serverstartcode at the timestamp ""9223372036854775807"" == Long.MAX_VALUE.  

So any further updates would be masked by these deletes.

;;;","24/May/10 23:59;jdcryans;IIRC that middle file comes from an oldlogfile...;;;","25/May/10 00:26;ryanobjc;Yes, we are missing HBASE-1880, and I will apply it to 0.20 branch

On Mon, May 24, 2010 at 5:00 PM, Jean-Daniel Cryans (JIRA)

;;;","25/May/10 00:28;ryanobjc;fixed by porting HBASE-1880 to 0.20 branch (which was old 0.20_pre_durability);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"BaseScanner says ""Current assignment of X is not valid"" over and over for same region",HBASE-2599,12465182,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,22/May/10 04:45,20/Nov/15 12:43,14/Jul/23 06:06,29/May/10 00:41,,,,,,,,,,,,0.90.0,,,,,,,0,,,"From IRC today

{code}
12:41 < cmorgan> hey guys. I'm having a recent  issue with a single node cluster running 0.20.4. After stopping for a backup I now get region assignment churn. Seems master keeps thinking that region
                 assignment is not valid even when it is. Following is a log snippet:
12:41 < cmorgan> [21/05/10 00:59:42] 3443246 [        HMaster] DEBUG ter.RegionServerOperationQueue  - Processing todo: PendingOpenOperation from localhost.,7802,1274425405680
12:41 < cmorgan> [21/05/10 00:59:42] 3443246 [        HMaster] INFO  e.master.RegionServerOperation  - net_troove_coin_account_AccountCredentials,,1234913258116 open on 127.0.0.1:7802
12:41 < cmorgan> [21/05/10 00:59:42] 3443246 [        HMaster] INFO  e.master.RegionServerOperation  - Updated row net_troove_coin_account_AccountCredentials,,1234913258116 in region .META.,,1 with
                 startcode=1274425405680, server=127.0.0.1:7802
12:41 < cmorgan> [21/05/10 00:59:42] 3443246 [        HMaster] DEBUG ter.RegionServerOperationQueue  - Processing todo: PendingOpenOperation from localhost.,7802,1274425405680
12:41 < cmorgan> [21/05/10 00:59:42] 3443246 [        HMaster] INFO  e.master.RegionServerOperation  - net_troove_application_request_TemporaryRequest,,1234913268355 open on 127.0.0.1:7802
12:41 < cmorgan> [21/05/10 00:59:42] 3443247 [        HMaster] INFO  e.master.RegionServerOperation  - Updated row net_troove_application_request_TemporaryRequest,,1234913268355 in region .META.,,1 with
                 startcode=1274425405680, server=127.0.0.1:7802
12:41 < cmorgan> [21/05/10 00:59:42] 3443247 [ger.metaScanner] DEBUG adoop.hbase.master.BaseScanner  - Current assignment of net_troove_coin_account_AccountEntry,,1271448856984 is not valid;
                 serverAddress=127.0.0.1:7802, startCode=1274425405680 unknown.
12:41 < cmorgan> [21/05/10 00:59:42] 3443248 [ger.metaScanner] DEBUG adoop.hbase.master.BaseScanner  - Current assignment of net_troove_coin_account_AccountEntry-Base_EntryDay_DESCENDING,,1273266418876
                 is not valid;  serverAddress=127.0.0.1:7802, startCode=1274425405680 unknown.
12:41 < cmorgan> [21/05/10 00:59:42] 3443251 [ger.metaScanner] DEBUG adoop.hbase.master.BaseScanner  - Current assignment of net_troove_coin_bank_BankStatement,,1266433980935 is not valid;
                 serverAddress=127.0.0.1:7802, startCode=1274425405680 unknown.

12:58 < cmorgan> stack: I'd been running with 0.20.4 for a week or so starting/stopping every night. Now this happens...

14:11 < cmorgan> stack: some more info: On our mini production server the regionserver is getting ""My address is localhost.:7802"" (notice the dot after localhost). But the master is also sometimes
                 referring to it as 127.0.0.1. I just used the same data and config on my laptop, and its binding to my external LAN ip (""My address is 10.0.1.4:7802""). Under this setup hbase comes up
                 stable (no region assignment churn).

{code}

Looking at this, I think issue is that when we register a server we use a getServerName on a HServerInfo provided by the regionserver (though we are on the master side) but BaseScanner uses a getServerName that is made by doing a dns lookup using the IP that it finds in the server column of .META.  My sense is that is possible for the regionserver hostname and what the master finds when it does a lookup against dns can disagree, fatally.

This issue seems popular over last few weeks.  Was reported at least once more on a standalone instance and also on krispykola's 15-node ec2 cluster (He went back to 0.20.3 and then it went away?).  It made for what looked like double-assignment in his case (Our attempt at caching DNS names may be amiss -- I tihnk tht the main diff between 0.20.3 and 0.20.4 in this area).

My thought is to purge DNS from the HServerInfo passed by the RS to Master on startup and heartbeating and to use IPs only (and even then, the IP that the master tells the RS to use, its remote address as seen by the master).  We might have to do this fix for 0.20.5 since it seems to happen more in 0.20.4.

I'm looking into this.  Opinions welcome.",,hammer,hbasereviewboard,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/May/10 08:08;stack;2599-0.20.txt;https://issues.apache.org/jira/secure/attachment/12445536/2599-0.20.txt","27/May/10 23:20;stack;2599-trunk.txt;https://issues.apache.org/jira/secure/attachment/12445718/2599-trunk.txt","24/Sep/10 22:23;jdcryans;HBASE-2599-0.20-refresh.patch;https://issues.apache.org/jira/secure/attachment/12455530/HBASE-2599-0.20-refresh.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26381,Reviewed,,,,Fri Nov 20 12:43:33 UTC 2015,,,,,,,,,,"0|i0hifz:",100250,,,,,,,,,,,,,,,,,,,,,"22/May/10 20:59;stack;Or, maybe better is to use hostnames everywhere -- hostnames in the .META. and hostnames in the HServerInfo#getServerName result.

We should probably go this route only, not allow for fact that may be disagreement over what a RS is called because Master may get one answer doing DNS lookup wherease the RS might get another. 

So, the HServerInfo that is passed between Master and RegionServer, instead of being filled out by the RS and then updated with the remote address as seen by the master on the RS's initial checkin leaving the 'name' (hostname) in HServerInfo as whatever the RS set it too, instead, lets change things so the Master fully specifies the HServerInfo and then the RS whenever it communicates w/ the master, it always proffers the HSI it got from the master, not a HSI that is a composite of RS and Master setting.

Let me make a patch.;;;","22/May/10 22:07;stack;Or, we put into .META. the hostname the RS gave us.  The RS gets its hostname by doing:

{code}
    machineName = DNS.getDefaultHost(
        conf.get(""hbase.regionserver.dns.interface"",""default""),
        conf.get(""hbase.regionserver.dns.nameserver"",""default""));
{code};;;","22/May/10 22:14;stack;Fix this too:

{code}
    // Set the address for now even tho it will not be persisted on HRS side
    // If the address given is not the default one, use IP given by the user.
    if (serverInfo.getServerAddress().getBindAddress().equals(DEFAULT_HOST)) {
      String rsAddress = HBaseServer.getRemoteAddress();
      serverInfo.setServerAddress(new HServerAddress(rsAddress,
        serverInfo.getServerAddress().getPort()));
    }
{code};;;","22/May/10 22:25;stack;I should add port to the server in .META.  Currently its the ip of the RS only.;;;","25/May/10 19:38;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/85/
-----------------------------------------------------------

Review request for hbase and Ryan Rawson.


Summary
-------

First cut at a patch that removes DNS lookups from HServerInfo and that writes the regionserver hostname into the .META. (with a port).


This addresses bug hbase-2599.
    http://issues.apache.org/jira/browse/hbase-2599


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/HServerInfo.java 43906a5 
  src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java 57f745e 
  src/main/java/org/apache/hadoop/hbase/master/HMaster.java 603159c 
  src/main/java/org/apache/hadoop/hbase/master/ProcessRegionOpen.java 14433b8 
  src/main/java/org/apache/hadoop/hbase/master/ServerManager.java 94bbc34 
  src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java fb65ed1 
  src/main/resources/webapps/master/table.jsp 095d6dd 
  src/test/java/org/apache/hadoop/hbase/TestServerInfo.java PRE-CREATION 

Diff: http://review.hbase.org/r/85/diff


Testing
-------

Not finished yet.


Thanks,

stack


;;;","25/May/10 22:55;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/88/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Version of hbase-2599 for 0.20 branch for 0.20.5.

It cuts DNS lookups from HServerInfo and uses the regionservers name everywhere instead of ip sometimes and a lookedup hostname at other times.  Also puts hostname:port into .META. rather than IP.


This addresses bug hbase-2599.


Diffs
-----

  branches/0.20/src/java/org/apache/hadoop/hbase/ClusterStatus.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/HServerInfo.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/master/BaseScanner.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/master/HMaster.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/master/ProcessRegionOpen.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/master/ServerManager.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 948218 
  branches/0.20/src/test/org/apache/hadoop/hbase/TestServerInfo.java PRE-CREATION 
  branches/0.20/src/webapps/master/table.jsp 948218 

Diff: http://review.hbase.org/r/88/diff


Testing
-------

Doing now.


Thanks,

stack


;;;","25/May/10 22:57;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/88/
-----------------------------------------------------------

(Updated 2010-05-25 15:55:50.626407)


Review request for hbase.


Changes
-------

Changed summary


Summary (updated)
-------

Version of hbase-2599 for 0.20 branch for 0.20.5.

It cuts DNS lookups from HServerInfo and uses the regionservers name everywhere instead of ip sometimes and a lookedup hostname at other times.  Also puts hostname:port into .META. rather than IP.


This addresses bug hbase-2599.


Diffs
-----

  branches/0.20/src/java/org/apache/hadoop/hbase/ClusterStatus.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/HServerInfo.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/master/BaseScanner.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/master/HMaster.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/master/ProcessRegionOpen.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/master/ServerManager.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 948218 
  branches/0.20/src/test/org/apache/hadoop/hbase/TestServerInfo.java PRE-CREATION 
  branches/0.20/src/webapps/master/table.jsp 948218 

Diff: http://review.hbase.org/r/88/diff


Testing
-------

Doing now.


Thanks,

stack


;;;","25/May/10 23:05;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/88/
-----------------------------------------------------------

(Updated 2010-05-25 16:04:24.285624)


Review request for hbase.


Changes
-------

Added some description


Summary (updated)
-------

Version of hbase-2599 for 0.20 branch for 0.20.5.

It cuts DNS lookups from HServerInfo and uses the regionservers name everywhere instead of ip sometimes and a lookedup hostname at other times.  Also puts hostname:port into .META. rather than IP.  Removing DNS lookups makes it so there is no possibility for disagreement over hostname if master gets one answer for an IP and the regionserver has another.

Other notes:

It replaces variable servername with hostnameandport so its clear whats going on.

Does some cleanup in HServerInfo renaming data member 'name' as 'hostname' and then purges DNS caching and lookups from this class.  It removes some unnecessary setters and does some javadoc fix ups.

Removes the dumb serverAddressToServerInfo from ServerManager.


This addresses bug hbase-2599.


Diffs
-----

  branches/0.20/src/java/org/apache/hadoop/hbase/ClusterStatus.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/HServerInfo.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/master/BaseScanner.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/master/HMaster.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/master/ProcessRegionOpen.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/master/ServerManager.java 948218 
  branches/0.20/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 948218 
  branches/0.20/src/test/org/apache/hadoop/hbase/TestServerInfo.java PRE-CREATION 
  branches/0.20/src/webapps/master/table.jsp 948218 

Diff: http://review.hbase.org/r/88/diff


Testing
-------

Doing now.


Thanks,

stack


;;;","25/May/10 23:07;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/88/#review68
-----------------------------------------------------------


- Jean-Daniel




;;;","25/May/10 23:31;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/88/#review69
-----------------------------------------------------------


I tried it, I think there's some other places we need to review the HSA stuff, see these lines I picked from a log:

2010-05-25 16:26:08,555 INFO org.apache.hadoop.hbase.master.ServerManager: Received start message from: hbasedev,60020,1274829968544
2010-05-25 16:26:08,561 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Updated ZNode /hbase/rs/1274829968544 with data 127.0.0.1:60020
2010-05-25 16:26:31,712 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: -ROOT-,,0 from hbasedev,60020,1274829968544; 1 of 1
2010-05-25 16:26:31,726 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: SetData of ZNode /hbase/root-region-server with 127.0.0.1:60020
2010-05-25 16:26:31,727 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scanning meta region {server: 127.0.0.1:60020, regionname: -ROOT-,,0, startKey: <>}
2010-05-25 16:26:32,742 INFO org.apache.hadoop.hbase.master.RegionServerOperation: .META.,,1 open on hbasedev,60020,1274829968544

Basically there's a very nice mix of IPs and hostnames.

- Jean-Daniel




;;;","26/May/10 06:30;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/88/#review75
-----------------------------------------------------------

Ship it!


+1 lets do it

- Ryan




;;;","26/May/10 06:37;ryanobjc;+1 lets go with it. 

There are some IP things being inserted into ZK, but that isnt part of the original problem, so let's go with what we have for now (simpler change)

;;;","26/May/10 08:08;stack;Here is what got reviewed.

I'm now kinda reluctant to commit as a test of a rolling restart reassigns all regions when the master comes up -- so really it'd require a complete restart of cluster.  Thinking on it.;;;","26/May/10 08:17;stack;@jdcryans Thanks for trying my patch.  Those ips are coming out of zkwatcher -- serialized HSA into zk znodes -- and are ips for root and meta used by the masters HCM instance.  I did not change these, only stuff related to .META. and generation of the key identifying regionservers.  I started to go this route but change runs deep.  Also, veers into recording master address in a HSI which is a RS thing.  I ran into a wall too inside in ChangeTableState#getMetaRegion where only a HSA to hand.  Gave up on changing any more.;;;","26/May/10 21:03;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/88/#review81
-----------------------------------------------------------

Ship it!


this seems reasonable, but only way we'll know is testing on various systems with different kinds of DNS setups. Can we get those who have had issues to try this and make sure it fixes, before we release it?


branches/0.20/src/java/org/apache/hadoop/hbase/ClusterStatus.java
<http://review.hbase.org/r/88/#comment409>

    I don't like that this function is called getServerNames and returns hostnameports.



branches/0.20/src/test/org/apache/hadoop/hbase/TestServerInfo.java
<http://review.hbase.org/r/88/#comment410>

    using the junit4 expected thing here doesn't quite work, cuz we don't know that it didn't throw on line 34, right?


- Todd


On 2010-05-25 16:04:24, stack wrote:
^bq.  
^bq.  -----------------------------------------------------------
^bq.  This is an automatically generated e-mail. To reply, visit:
^bq.  http://review.hbase.org/r/88/
^bq.  -----------------------------------------------------------
^bq.  
^bq.  (Updated 2010-05-25 16:04:24)
^bq.  
^bq.  
^bq.  Review request for hbase.
^bq.  
^bq.  
^bq.  Summary
^bq.  -------
^bq.  
^bq.  Version of hbase-2599 for 0.20 branch for 0.20.5.
^bq.  
^bq.  It cuts DNS lookups from HServerInfo and uses the regionservers name everywhere instead of ip sometimes and a lookedup hostname at other times.  Also puts hostname:port into .META. rather than IP.  Removing DNS lookups makes it so there is no possibility for disagreement over hostname if master gets one answer for an IP and the regionserver has another.
^bq.  
^bq.  Other notes:
^bq.  
^bq.  It replaces variable servername with hostnameandport so its clear whats going on.
^bq.  
^bq.  Does some cleanup in HServerInfo renaming data member 'name' as 'hostname' and then purges DNS caching and lookups from this class.  It removes some unnecessary setters and does some javadoc fix ups.
^bq.  
^bq.  Removes the dumb serverAddressToServerInfo from ServerManager.
^bq.  
^bq.  
^bq.  This addresses bug hbase-2599.
^bq.  
^bq.  
^bq.  Diffs
^bq.  -----
^bq.  
^bq.    branches/0.20/src/java/org/apache/hadoop/hbase/ClusterStatus.java 948218 
^bq.    branches/0.20/src/java/org/apache/hadoop/hbase/HServerInfo.java 948218 
^bq.    branches/0.20/src/java/org/apache/hadoop/hbase/master/BaseScanner.java 948218 
^bq.    branches/0.20/src/java/org/apache/hadoop/hbase/master/HMaster.java 948218 
^bq.    branches/0.20/src/java/org/apache/hadoop/hbase/master/ProcessRegionOpen.java 948218 
^bq.    branches/0.20/src/java/org/apache/hadoop/hbase/master/ServerManager.java 948218 
^bq.    branches/0.20/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 948218 
^bq.    branches/0.20/src/test/org/apache/hadoop/hbase/TestServerInfo.java PRE-CREATION 
^bq.    branches/0.20/src/webapps/master/table.jsp 948218 
^bq.  
^bq.  Diff: http://review.hbase.org/r/88/diff
^bq.  
^bq.  
^bq.  Testing
^bq.  -------
^bq.  
^bq.  Doing now.
^bq.  
^bq.  
^bq.  Thanks,
^bq.  
^bq.  stack
^bq.  
^bq. 


;;;","27/May/10 01:03;jdcryans;The patch won't apply now that HBASE-2613 went in. It will actually make it leaner.;;;","27/May/10 18:45;stack;I'm not going to apply this to branch.  Too risky not to mention the fact that it breaks rolling restart.  Going to apply to TRUNK.  Should be enough time before release for any issues w/ different DNS configs. to surface (So, seems like we required reverse lookups to work before this patch; with it they are no longer needed).

Todd, I'll address your two items on commit (Thanks for the review). On getting fellas to try it, its a bit tough.  Seems like the error is often transient; e.g. clint above was running fine for days then something changed so localhost and 127.0.0.1 weren't the same any more.

;;;","27/May/10 23:20;stack;Version for trunk that has todd suggested changes.  Will apply soon.;;;","29/May/10 00:41;stack;Thanks all for the review.  This patch changes how we do DNS.  Should make it more straight-forward but you never know.  We need to keep an eye on it.;;;","24/Sep/10 22:23;jdcryans;I still see people with this issue from time to time, and they don't all want to upgrade to 0.89, so here's an updated version of that patch for 0.20

Also I have a 0.20.6 distro patched with it here: http://people.apache.org/~jdcryans/hbase-0.20.6.tar.gz;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBASE-2587 hardcoded the port that dfscluster runs on,HBASE-2591,12465153,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,21/May/10 19:37,20/Nov/15 12:40,14/Jul/23 06:06,21/May/10 19:38,,,,,,,,,,,,0.90.0,,,,,,,0,,," HBASE-2587 ""Coral where tests write data when running and make sure clean target removes all written"" hardcoded the port that dfscluster runs on.  Makes it so can't run tests in shared environment.


Here's a fix:

{code}
diff --git a/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java b/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
index 8795ba6..238e804 100644
--- a/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
+++ b/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
@@ -173,8 +173,8 @@ public class HBaseTestingUtility {
     else this.clusterTestBuildDir = dir;
     System.setProperty(TEST_DIRECTORY_KEY, this.clusterTestBuildDir.toString());
     System.setProperty(""test.cache.data"", this.clusterTestBuildDir.toString());
-    this.dfsCluster = new MiniDFSCluster(12345, this.conf, servers, true,
-      true, true, null, null, null, null);
+    this.dfsCluster = new MiniDFSCluster(0, this.conf, servers, true, true,
+      true, null, null, null, null);
     return this.dfsCluster;
   }
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26378,,,,,Fri Nov 20 12:40:32 UTC 2015,,,,,,,,,,"0|i0hief:",100243,,,,,,,,,,,,,,,,,,,,,"21/May/10 19:38;stack;Committed.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed parse of branch element in saveVersion.sh,HBASE-2590,12465146,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,tsuna,stack,stack,21/May/10 17:25,20/Nov/15 12:42,14/Jul/23 06:06,24/May/10 17:02,,,,,,,,,,,,0.90.0,,,,,,,0,,,"A mvn build would fail because of the string it was getting back from svn:

{code}
      [exec] not part of the command.
      Execute:Java13CommandLauncher: Executing 'sh' with arguments:
       '/home/X/Y/hadoop/branches/V.V/hbase/hbase-trunk/src/saveVersion.sh'
        '0.21.0-SNAPSHOT'
         '/home/X/Y/hadoop/branches/V.V/hbase-trunk/target/generated-sources'
        The ' characters around the executable and arguments are
         not part of the command.
         [exec] sed: -e expression #6, char 48: unterminated `s' command
         [exec] Result: 1
{code}

Path amended in the above to protect the innocent.

The failure was around parse of branch.  Branch is not used.",,larsfrancke,tsuna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/10 03:43;tsuna;0001-Fix-HBASE-2590.patch;https://issues.apache.org/jira/secure/attachment/12445248/0001-Fix-HBASE-2590.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26377,Reviewed,,,,Fri Nov 20 12:42:02 UTC 2015,,,,,,,,,,"0|i0hie7:",100242,,,,,,,,,,,,,,,,,,,,,"23/May/10 03:43;tsuna;Does this patch fix the problem for you?;;;","23/May/10 03:46;tsuna;Interesting, I just saw your commit #947082 and the file you're fixing in that commit doesn't match what I have in trunk from hbase.git.  Is the Git repo out of sync with the SVN repo?;;;","23/May/10 05:29;stack;If I diff git and svn I see no difference.

git
* trunk                                              5fc7f69 HBASE-2598 Move NINES and ZEROS to client package and change visibility to default

svn
$ svn info
Path: .
URL: https://svn.apache.org/repos/asf/hbase/trunk
Repository Root: https://svn.apache.org/repos/asf
Repository UUID: 13f79535-47bb-0310-9956-ffa450edef68
Revision: 947358
Node Kind: directory
Schedule: normal
Last Changed Author: stack
Last Changed Rev: 947349
Last Changed Date: 2010-05-22 16:38:16 -0700 (Sat, 22 May 2010)

$ diff src/saveVersion.sh ../hbase/src/saveVersion.sh emits nothing

The committed file seems to work.  Here is its product:

{code}
~/hbase$ more target/generated-sources/org/apache/hadoop/hbase/package-info.java 
/*
 * Generated by src/saveVersion.sh
 */
@VersionAnnotation(version=""0.21.0-SNAPSHOT"", revision=""5fc7f694d9f979ae7b044f3abc8bca2cd05c54af"", 
                         user=""stack"", date=""Sat May 22 22:27:35 PDT 2010"", url=""git://face/home/stack/hbase"")
{code}

I had to commit this though to make the REVSION work again for git:

{code}
$ git diff !$
git diff src/saveVersion.sh
diff --git a/src/saveVersion.sh b/src/saveVersion.sh
index 73778b4..9607daa 100755
--- a/src/saveVersion.sh
+++ b/src/saveVersion.sh
@@ -31,7 +31,7 @@ cwd=`pwd`
 if [ -d .svn ]; then
   revision=`svn info | sed -n -e 's/Last Changed Rev: \(.*\)/\1/p'`
   url=`svn info | sed -n -e 's/URL: \(.*\)/\1/p'`
-elif [ -d ../.git ]; then
+elif [ -d .git ]; then
   revision=`git log -1 --pretty=format:""%H""`
   hostname=`hostname`
   url=""git://${hostname}${cwd}""
{code}

Does your patch even work Benoit?;;;","23/May/10 06:10;tsuna;Oh I see, I didn't realize you committed the first change way before I posted my patch.  JIRA doesn't show changes in the issue by default and there was no other indication of any work being done on it.  I couldn't reproduce the problem or see the same file you edited because I already had pulled your fix without knowing it!

So my patch doesn't fix anything, it just cleans up the script.  By removing the last unnecessary call to `sed' we can make sure we'll never run into this problem again.;;;","24/May/10 17:02;stack;Committed the cleanup.  Thanks Benoit.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestHRegion.testWritesWhileScanning flaky on trunk,HBASE-2589,12465144,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,tlipcon,tlipcon,tlipcon,21/May/10 16:51,20/Nov/15 12:43,14/Jul/23 06:06,24/May/10 16:36,0.90.0,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"IIRC we added this test in response to a bug in HBASE-2248 in the old branch. It's failing about half the time on my internal Hudson. i=36 expected:<1000> but was:<0>
",,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/10 15:57;tlipcon;hbase-2589.txt;https://issues.apache.org/jira/secure/attachment/12445344/hbase-2589.txt","24/May/10 02:47;tlipcon;hbase-2589.txt;https://issues.apache.org/jira/secure/attachment/12445277/hbase-2589.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26376,Reviewed,,,,Fri Nov 20 12:43:22 UTC 2015,,,,,,,,,,"0|i0hidz:",100241,,,,,,,,,,,,,,,,,,,,,"24/May/10 02:51;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/79/
-----------------------------------------------------------

Review request for hbase and Ryan Rawson.


Summary
-------

Test wasn't waiting until the put thread had started before it started reading, so would often fail having read 0 columns.


This addresses bug HBASE-2589.
    http://issues.apache.org/jira/browse/HBASE-2589


Diffs
-----

  src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java 44e7e6d 

Diff: http://review.hbase.org/r/79/diff


Testing
-------

Ran it a few times on my box locally, seems to pass now where it used to fail most of the time.


Thanks,

Todd


;;;","24/May/10 02:55;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/79/#review47
-----------------------------------------------------------



src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
<http://review.hbase.org/r/79/#comment232>

    should probably use the accessor here.



src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
<http://review.hbase.org/r/79/#comment233>

    do we get any meaningful sleep here? I have not really seen Thread.yield in a long time


- Ryan




;;;","24/May/10 03:01;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/79/
-----------------------------------------------------------

(Updated 2010-05-23 19:59:07.555469)


Review request for hbase and Ryan Rawson.


Changes
-------

Switched to a short Thread.sleep instead.
Also got rid of the accessor, since I'm only accessing that variable internally.


Summary
-------

Test wasn't waiting until the put thread had started before it started reading, so would often fail having read 0 columns.


This addresses bug HBASE-2589.
    http://issues.apache.org/jira/browse/HBASE-2589


Diffs (updated)
-----

  src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java 44e7e6d 

Diff: http://review.hbase.org/r/79/diff


Testing
-------

Ran it a few times on my box locally, seems to pass now where it used to fail most of the time.


Thanks,

Todd


;;;","24/May/10 03:03;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/79/#review48
-----------------------------------------------------------

Ship it!


- Ryan




;;;","24/May/10 15:53;stack;Is what is attached here the r2 that was posted to review. hbase.org and that ryan flagged 'ship it'?  I can't tell.  I'd like to commit.;;;","24/May/10 15:57;tlipcon;Oops, no, it wasn't. Now it is :);;;","24/May/10 16:36;stack;Committed to TRUNK.  Thanks for the patch Todd.  Thanks for the review Ryan.;;;","28/May/10 19:26;apurtell;Ported and committed to 0.20 branch. ;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move hbase webapps to an hbase-webapps dir,HBASE-2586,12465075,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,tlipcon,tlipcon,tlipcon,20/May/10 22:24,20/Nov/15 12:40,14/Jul/23 06:06,26/May/10 02:56,,,,,,,,,,,,0.90.0,,build,,,,,0,,,Having our webapps in a dir named webapps/ is confusing the hadoop http servers in miniclusters. This was causing my MiniMRCluster jobs to not work correctly. Moving our dir to a new name fixed the issue.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/10 22:25;tlipcon;move-webapps.txt;https://issues.apache.org/jira/secure/attachment/12445112/move-webapps.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26375,,,,,Fri Nov 20 12:40:53 UTC 2015,,,,,,,,,,"0|i0hidb:",100238,,,,,,,,,,,,,,,,,,,,,"20/May/10 22:25;tlipcon;Have not tested mvn assembly with this, but did test in-situ;;;","24/May/10 18:14;tlipcon;Can someone take a look at this? My minimr tests don't work properly without it.;;;","24/May/10 18:46;ryanobjc;The app should be rooted at /

There is an open JIRA for this already.

https://issues.apache.org/jira/browse/HBASE-2586?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12870753#action_12870753]
without it.
servers in miniclusters. This was causing my MiniMRCluster jobs to not work
correctly. Moving our dir to a new name fixed the issue.
;;;","26/May/10 02:56;apurtell;TestMasterTransitions is hanging forever as I'm trying to get a trunk build on a new Hudson instance working. But this is also getting in the way:

{noformat}
2010-05-26 02:01:05,483 WARN  [main] reflect.NativeMethodAccessorImpl(?): Failed
 startup of context org.mortbay.jetty.webapp.WebAppContext@508aeb74{/,file:/hudson/workspace/hbase-trunk/target/classes/webapps/hdfs}
java.io.FileNotFoundException: file:/hudson/workspace/hbase-trunk/target/classes/webapps/hdfs
{noformat}

The patch on this issue fixes the problem when tested locally. Need to commit to test end to end from git mirror. Will revert if there is a problem. ;;;","26/May/10 03:00;ryanobjc;What about HBASE-2369 guys?  Does this also fix that?


;;;","26/May/10 04:06;tlipcon;Stack was saying this patch had some issues in ""assembly"" form. I always do my testing ""in-situ"" - ie I NFS mount my development tree and just use start/stop scripts from there. In that form, with this patch, the JSPs are rooted at the right URL, but can't speak to the jar form.;;;","26/May/10 08:38;stack;Yeah, broke when distributed.  I'll look into it.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make webapps work in distributed mode again and make webapps deploy at / instead of at /webapps/master/master.jsp,HBASE-2583,12465003,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,20/May/10 08:14,20/Nov/15 12:43,14/Jul/23 06:06,20/May/10 09:03,,,,,,,,,,,,0.90.0,,,,,,,0,,,"UI is not showing up in right place when you run hbase unless you run it in-situ; it don't work properly when distributed mode.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/10 09:01;stack;2583.txt;https://issues.apache.org/jira/secure/attachment/12445036/2583.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26373,,,,,Fri Nov 20 12:43:56 UTC 2015,,,,,,,,,,"0|i0hid3:",100237,,,,,,,,,,,,,,,,,,,,,"20/May/10 08:24;tlipcon;It works in distributed mode if it's an ""in situ"" distributed mode - eg my build tree is on an NFS filer and works fine. So it's something about how classpath is build for the non-source-tree case post-assembly;;;","20/May/10 09:01;stack;Patch that provides the few 'odd' things we need to make the webapp work again:

+ Add to assembly prescription the writing out of a webapps directory that is at same level as lib, bin, and conf.  Without this, webapps don't get mounted at root.  Jetty undoes the webapp that is inside the jar into a tmp dir.  In this tmp dir it'll unpack it at webapp/webapps/master and webapp is adde to root instead.  This might be fixable.  For now do what we used to and what hadoop currently does.
+ Don't include the jsps
+ For some reason, we need jasper-compiler jar in our classpath at runtime.  Without it we get IS_SECURITY_ENABLED exception and webapps fail to deploy.

While here, sneaked in caching of the generated classpath so that subsequent runs of bin/hbase go faster (cache is kept in the maven target dir so a mvn clean will force a regeneration).

I also had remnant references to 'core' still in bin/hbase that I cleared out.;;;","20/May/10 09:03;stack;Committed.  UI in distributed mode is pretty useful (smile).;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestTableSchemaModel not passing after commit of blooms,HBASE-2582,12464981,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,apurtell,streamy,streamy,20/May/10 00:57,20/Nov/15 12:41,14/Jul/23 06:06,20/May/10 02:31,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,TestTableSchemaModel is failing because commit of blooms changed the existing column family setting.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/10 01:19;apurtell;HBASE-2582.patch;https://issues.apache.org/jira/secure/attachment/12445010/HBASE-2582.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26372,,,,,Fri Nov 20 12:41:08 UTC 2015,,,,,,,,,,"0|i0hicv:",100236,,,,,,,,,,,,,,,,,,,,,"20/May/10 00:57;streamy;REST seems to want 'none' but it's being set as something else now.  Tried some quick fixes but didn't get it to pass.;;;","20/May/10 00:59;streamy;Assigning to apurtell so he sees this.  Anyone can fix it but want to notify andrew that the value of the bloomfilter field is now different (to support row and row+col level blooms, not just binary on/off).  Should make sure all is good w/ REST.;;;","20/May/10 01:05;streamy;assertEquals(getApurtellAttention(), reality)  yay!  test passes!;;;","20/May/10 02:31;apurtell;Committed. All tests pass locally for me.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bloom commit broke TestShell,HBASE-2581,12464980,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,streamy,streamy,streamy,20/May/10 00:43,20/Nov/15 12:41,14/Jul/23 06:06,20/May/10 00:54,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,TestShell is not passing on hudson after bloom commit.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/10 00:44;streamy;HBASE-2581-TestShell-fix.patch;https://issues.apache.org/jira/secure/attachment/12445007/HBASE-2581-TestShell-fix.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26371,,,,,Fri Nov 20 12:41:19 UTC 2015,,,,,,,,,,"0|i0hicn:",100235,,,,,,,,,,,,,,,,,,,,,"20/May/10 00:53;streamy;Changing to just be TestShell.  I need apurtell help on other test.;;;","20/May/10 00:54;streamy;Committed to trunk.  TestShell passes.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestHRegion.testDelete_mixed() failing on hudson,HBASE-2576,12464970,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,streamy,streamy,streamy,19/May/10 23:55,20/Nov/15 12:41,14/Jul/23 06:06,20/May/10 00:09,0.90.0,,,,,,,,,,,0.90.0,,test,,,,,0,,,,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/10 00:05;streamy;HBASE-2576-v1.patch;https://issues.apache.org/jira/secure/attachment/12445004/HBASE-2576-v1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26370,,,,,Fri Nov 20 12:41:56 UTC 2015,,,,,,,,,,"0|i0hibj:",100230,,,,,,,,,,,,,,,,,,,,,"20/May/10 00:05;streamy;Adds another sleep to ensure timestamps have changed between a delete and subsequent put.;;;","20/May/10 00:06;jdcryans;+1;;;","20/May/10 00:09;streamy;Committed to trunk.  Thanks for review jd.

/me crosses fingers for hudson green light;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
client.HConnectionManager$TableServers logs non-printable binary bytes,HBASE-2573,12464948,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,Fixed,tsuna,tsuna,tsuna,19/May/10 21:10,20/Nov/15 12:41,14/Jul/23 06:06,19/May/10 21:21,,,,,,,,,,,,0.90.0,,Client,,,,,0,,,"Just a minor annoyance, sometimes the client code logs non-printable binary bytes that mess up my terminal when DEBUG logging is turned on.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/May/10 21:11;tsuna;0001-HBASE-2573-Don-t-log-non-printable-bytes.patch;https://issues.apache.org/jira/secure/attachment/12444981/0001-HBASE-2573-Don-t-log-non-printable-bytes.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26368,Reviewed,,,,Fri Nov 20 12:41:46 UTC 2015,,,,,,,,,,"0|i0hib3:",100228,,,,,,,,,,,,,,,,,,,,,"19/May/10 21:11;tsuna;Patch that fixes the issue.;;;","19/May/10 21:12;tsuna;Patch available.;;;","19/May/10 21:21;stack;Thank you for the patch Benoît  .  Resolving.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 hbase/bin/set_meta_block_caching.rb:72: can't convert Java::JavaLang::String into String (TypeError) - little issue with script,HBASE-2572,12464915,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,19/May/10 17:02,20/Nov/15 12:44,14/Jul/23 06:06,19/May/10 17:03,,,,,,,,,,,,0.90.0,,,,,,,0,,,,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26367,,,,,Fri Nov 20 12:44:11 UTC 2015,,,,,,,,,,"0|i0hiav:",100227,,,,,,,,,,,,,,,,,,,,,"19/May/10 17:03;stack;Commiitted to trunk.;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin/hbase doesn't work in-situ in maven,HBASE-2562,12464744,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,tlipcon,tlipcon,tlipcon,18/May/10 01:38,20/Nov/15 12:42,14/Jul/23 06:06,18/May/10 04:12,0.90.0,,,,,,,,,,,0.90.0,,build,,,,,0,,,"on trunk I can't use the normal start-hbase.sh scripts, etc, after doing mvn install - instead have to do the whole assembly nonsense, which is a pain when trying to quickly iterate on changes.",,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/10 01:39;tlipcon;hbase-2562.txt;https://issues.apache.org/jira/secure/attachment/12444756/hbase-2562.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26363,Reviewed,,,,Fri Nov 20 12:42:46 UTC 2015,,,,,,,,,,"0|i0hi9r:",100222,,,,,,,,,,,,,,,,,,,,,"18/May/10 01:39;tlipcon;Here's a patch that gets the stuff working for me again. I havent tried actually doing a tarball build/deploy after these changes, though.;;;","18/May/10 04:12;stack;Committed.  Thanks Todd.  Added a MAVEN_HOME handling in case mvn is elsewhere, not on the path.;;;","18/May/10 04:20;stack;I changed the wiki removing the bit about building classpath using maven dependency plugin to run hbase in-situ.;;;","19/May/10 01:26;jdcryans;There's a typo in /tmp/hbase-core-tets-classpath.txt, also we should probably print something when it's fetching jars (I thought it was stuck until Todd explained to me that maven was doing a full web crawl for fun).;;;","19/May/10 03:43;stack;Fixed spelling mistake.  It did not hamper functionality.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scanning .META. while split in progress yields IllegalArgumentException,HBASE-2561,12464741,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,tlipcon,tlipcon,tlipcon,18/May/10 01:15,20/Nov/15 12:43,14/Jul/23 06:06,19/May/10 04:36,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"Running scan '.META.' from the shell throws IllegalArgumentException if a split is running at the same time:

hbase(main):004:0> scan '.META.'
ROW                          COLUMN+CELL                                                                      
 VerifiableEditor,,127414503 column=info:regioninfo, timestamp=1274145178356, value=REGION => {NAME => 'Verifi
 3318                        ableEditor,,1274145033318', STARTKEY => '', ENDKEY => '-1942612687<1274143362177>
                             ', ENCODED => 1741581486, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'Ver
                             ifiableEditor', FAMILIES => [{NAME => 'info', REPLICATION_SCOPE => '0', COMPRESSI
                             ON => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMO
                             RY => 'false', BLOCKCACHE => 'true'}]}}                                          
 VerifiableEditor,,127414503 column=info:server, timestamp=1274145178356, value=                              
 3318                                                                                                         

ERROR: java.lang.IllegalArgumentException: offset (0) + length (8) exceed the capacity of the array: 0
",,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/10 01:24;tlipcon;hbase-2561.txt;https://issues.apache.org/jira/secure/attachment/12444754/hbase-2561.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26362,Reviewed,,,,Fri Nov 20 12:43:51 UTC 2015,,,,,,,,,,"0|i0hi9j:",100221,,,,,,,,,,,,,,,,,,,,,"18/May/10 01:24;tlipcon;Been a couple years since I wrote much ruby, so would appreciate a review, and pointer to how to write a unit test if applicable, but this seems to fix the issue for me.;;;","19/May/10 04:36;stack;I tried it.  Scan still works.  Committing.  Unit test is kinda hard to do in here.  You could do standalone but patch looks like it does the job and its small change.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalArgumentException when manually splitting table from web UI,HBASE-2560,12464740,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,tlipcon,tlipcon,tlipcon,18/May/10 01:10,11/Jun/22 23:33,14/Jul/23 06:06,02/Jun/10 18:12,0.90.0,,,,,,,,,,,,,master,,,,,0,,,"Clicked split once, then again, got an error:

http://monster01.sf.cloudera.com:60010/table.jsp?action=split&name=VerifiableEditor&key=
java.lang.IllegalArgumentException: Not a host:port pair: 
	at org.apache.hadoop.hbase.HServerAddress.(HServerAddress.java:57)
	at org.apache.hadoop.hbase.master.HMaster.getTableRegions(HMaster.java:841)
	at org.apache.hadoop.hbase.master.HMaster.modifyTable(HMaster.java:981)
",,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2656,,,,,,,,,,,,,,,,,"02/Jun/10 01:41;tlipcon;hbase-2560.txt;https://issues.apache.org/jira/secure/attachment/12446089/hbase-2560.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26361,Reviewed,,,,Wed Jun 02 18:12:53 UTC 2010,,,,,,,,,,"0|i0hi9b:",100220,,,,,,,,,,,,,,,,,,,,,"18/May/10 01:13;tlipcon;I think the issue is that getTableRegions fails since there are some split-in-progress rows in .META. that have no region server assigned.;;;","18/May/10 01:30;tlipcon;-            if (value != null) {
+            if (value != null && value.length > 0) {

fixed the issue, need to write a unit test though (should be straightforward enough);;;","02/Jun/10 01:41;tlipcon;Patch fixes the issue and includes a unit test. Note that there is a TODO in the unit test - would appreciate comments on what the right behavior is there.;;;","02/Jun/10 01:45;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/112/
-----------------------------------------------------------

Review request for hbase and Jonathan Gray.


Summary
-------

Patch fixes the issue and includes a unit test. Note that there is a TODO in the unit test - would appreciate comments on what the right behavior is there.


This addresses bug HBASE-2560.
    http://issues.apache.org/jira/browse/HBASE-2560


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/master/HMaster.java c713245 
  src/test/java/org/apache/hadoop/hbase/master/TestMaster.java PRE-CREATION 

Diff: http://review.hbase.org/r/112/diff


Testing
-------


Thanks,

Todd


;;;","02/Jun/10 01:45;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/112/#review118
-----------------------------------------------------------



src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.hbase.org/r/112/#comment669>

    I'll get rid of this before commit, just was useful when trying to figure out what was going on with the TODO below


- Todd



;;;","02/Jun/10 05:12;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/112/#review119
-----------------------------------------------------------

Ship it!


Looks good to me.


src/test/java/org/apache/hadoop/hbase/master/TestMaster.java
<http://review.hbase.org/r/112/#comment671>

    You don't unregister.. I don't think it matters...



src/test/java/org/apache/hadoop/hbase/master/TestMaster.java
<http://review.hbase.org/r/112/#comment672>

    That would be an improvement.  You'd get the 'table region closest' even though the entry didn't have a server address yet.


- stack



;;;","02/Jun/10 18:12;tlipcon;Committed this and opened HBASE-2656 for the improvement on getTableRegionClosest. Thanks for review, Stack.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[mvn] Our javadoc overview -- ""Getting Started"", requirements, etc. -- is not carried across by mvn javadoc:javadoc target",HBASE-2558,12464723,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,,stack,stack,17/May/10 20:54,20/Nov/15 12:41,14/Jul/23 06:06,08/Jun/10 08:16,,,,,,,,,,,,0.90.0,,,,,,,0,,,"I see this http://jira.codehaus.org/browse/MJAVADOC-278 which does not bode well.  I messed around upping the plugin version and explicitly specifying the overview.html file to use but no luck.  Come back and figure this or an alternative.

Made it a blocker.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/10 00:22;stack;2558-v2.txt;https://issues.apache.org/jira/secure/attachment/12446549/2558-v2.txt","08/Jun/10 08:15;stack;2558-v3.txt;https://issues.apache.org/jira/secure/attachment/12446580/2558-v3.txt","05/Jun/10 07:09;stack;2558.txt;https://issues.apache.org/jira/secure/attachment/12446404/2558.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26360,,,,,Fri Nov 20 12:41:27 UTC 2015,,,,,,,,,,"0|i0hi8v:",100218,,,,,,,,,,,,,,,,,,,,,"05/Jun/10 07:09;stack;Hmm... the overview seems to make it over fine now.

This patch adds copying target/site over into assembly as a new docs dir.  If site target called before assembly:assembly, all docs including javadoc will be included.

Also copy over our hbase-default.xml into conf dir where folks are used to seeing it -- only it doesn't work yet.

Add our old xdoc articles, and a site.xml.  Need to add a css and mess around w/ site.xml more to get stuff to show in right place.  Currently this patch makes things WORSE.

Bit of work to do yet.
;;;","08/Jun/10 00:22;stack;Add in our doc and images where mvn wants to find it (cleanup old location), add a site.xml, first cut at menu layout.. more to do.  hbase-default.xml is showing up in conf now.;;;","08/Jun/10 08:15;stack;Brings over doc from 0.20.  I fixed it up. xdoc format changed.  Did a home page and fixed up nav menus.  This should do for now.  Going to commit.;;;","08/Jun/10 08:16;stack;Committed.;;;","08/Jun/10 17:32;stack;I just committed a bit more cleanup, added a faq, shortened news (added old news doc.), etc.  Now I'm definetly done till get feedback.  Hopefully hudson will start publishing site now trunk is working again.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revisit IncrementColumnValue implementation in 0.22,HBASE-2553,12464577,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,15/May/10 00:52,20/Nov/15 12:41,14/Jul/23 06:06,16/Jul/10 23:11,,,,,,,,,,,,0.90.0,,,,,,,0,,,"right now we are using too much of the old get code, we need to review that and constrain how this works but without breaking ICV.

Also we should be resetting the timestamp on every ICV call, and removing the older version.  Instead of 'updating' an ICV ""in place"" we should be adding a new one, removing the old one from memstore (if it is there).  This will play well with the atomic approach added in HBASE-2248 since we are only touching 1 KeyValue at a time.",,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26359,Reviewed,,,,Fri Nov 20 12:41:30 UTC 2015,,,,,,,,,,"0|i0hi87:",100215,,,,,,,,,,,,,,,,,,,,,"16/Jul/10 00:19;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/325/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

rework ICV by no longer requiring getWithCode() and also by keeping the timestamp 'fresh' instead of updating 'in place'.


This addresses bug HBASE-2553.
    http://issues.apache.org/jira/browse/HBASE-2553


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java ae94578 
  src/main/java/org/apache/hadoop/hbase/regionserver/Store.java d2f65f4 
  src/main/java/org/apache/hadoop/hbase/util/Bytes.java 1b46f2d 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java f732466 

Diff: http://review.hbase.org/r/325/diff


Testing
-------


Thanks,

Ryan


;;;","16/Jul/10 20:25;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/325/#review412
-----------------------------------------------------------

Ship it!


+1

There are a few minors in the below perhaps worth considering.


src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java
<http://review.hbase.org/r/325/#comment1756>

    Why not just use newKv?  If there is a kv in front of newKv, then our counters are going to be off?



src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java
<http://review.hbase.org/r/325/#comment1759>

    See KeyValue.matchingQualifier() or KeyValue.matchingColumn



src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
<http://review.hbase.org/r/325/#comment1760>

    There is a tab here?


- stack



;;;","16/Jul/10 23:11;ryanobjc;committed;;;","16/Jul/10 23:17;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>


bq.  On 2010-07-16 13:23:13, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java, line 404
bq.  > <http://review.hbase.org/r/325/diff/1/?file=2838#file2838line404>
bq.  >
bq.  >     See KeyValue.matchingQualifier() or KeyValue.matchingColumn

they dont take offset,length so i wasnt able to use them


bq.  On 2010-07-16 13:23:13, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java, line 386
bq.  > <http://review.hbase.org/r/325/diff/1/?file=2838#file2838line386>
bq.  >
bq.  >     Why not just use newKv?  If there is a kv in front of newKv, then our counters are going to be off?

good point


bq.  On 2010-07-16 13:23:13, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/regionserver/Store.java, line 1301
bq.  > <http://review.hbase.org/r/325/diff/1/?file=2839#file2839line1301>
bq.  >
bq.  >     There is a tab here?

fixed


- Ryan


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/325/#review412
-----------------------------------------------------------



;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Specify default filesystem in both the new and old way (needed if we are to run on 0.20 and 0.21 hadoop),HBASE-2546,12464506,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,14/May/10 05:29,20/Nov/15 12:43,14/Jul/23 06:06,14/May/10 05:30,,,,,,,,,,,,0.90.0,,master,,,,,0,,,"I couldn't start a distributed cluster because master wanted to keep using the local filesystem.  Setting default filesystem using both old and new way seems the way to go:

{code}
Index: core/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
===================================================================
--- core/src/main/java/org/apache/hadoop/hbase/master/HMaster.java      (revision 944113)
+++ core/src/main/java/org/apache/hadoop/hbase/master/HMaster.java      (working copy)
@@ -165,6 +165,9 @@
     // default localfs.  Presumption is that rootdir is fully-qualified before
     // we get to here with appropriate fs scheme.
     this.rootdir = FSUtils.getRootDir(this.conf);
+    // Cover both bases, the old way of setting default fs and the new.
+    // We're supposed to run on 0.20 and 0.21 anyways.
+    this.conf.set(""fs.default.name"", this.rootdir.toString());
     this.conf.set(""fs.defaultFS"", this.rootdir.toString());
     this.fs = FileSystem.get(this.conf);
     checkRootDir(this.rootdir, this.conf, this.fs);
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26357,,,,,Fri Nov 20 12:43:27 UTC 2015,,,,,,,,,,"0|i0hi6v:",100209,,,,,,,,,,,,,,,,,,,,,"14/May/10 05:30;stack;Committed.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Unresponsive region server, potential deadlock",HBASE-2545,12464505,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,tlipcon,kjirapinyo,kjirapinyo,14/May/10 05:27,12/Oct/12 06:15,14/Jul/23 06:06,14/May/10 18:52,0.20.4,,,,,,,,,,,0.20.5,0.90.0,regionserver,,,,,0,,,"We have a 15-node (14RS+1Master) hbase cluster.  We just recently upgraded from 0.20.3 to 0.20.4.  This cluster does have colocated hadoop MR, but we mostly use another MR cluster to hit it.  Upon start, the cluster runs the jobs fine for about an hour.  Afterwards, an RS seems to have locked up.  Doing a get for a row in region being served by that region server hangs (cannot even ctrl+c out of the hbase shell).  Attached is the thread dump.  Verified in UI that the affect server runs on 0.20.4 and not 0.20.3.","Ubuntu 8.04.4 LTS, Hadoop 0.20.2, Amazon EC2 x-large cluster",davelatham,tsuna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/10 18:25;stack;2545-trunk.txt;https://issues.apache.org/jira/secure/attachment/12444522/2545-trunk.txt","14/May/10 18:01;tlipcon;hbase-2545.txt;https://issues.apache.org/jira/secure/attachment/12444519/hbase-2545.txt","14/May/10 18:01;tlipcon;hbase-2545.txt;https://issues.apache.org/jira/secure/attachment/12444518/hbase-2545.txt","14/May/10 17:22;tlipcon;hbase-2545.txt;https://issues.apache.org/jira/secure/attachment/12444514/hbase-2545.txt","14/May/10 17:15;tlipcon;hbase-2545.txt;https://issues.apache.org/jira/secure/attachment/12444512/hbase-2545.txt","14/May/10 05:28;kjirapinyo;hbase-hadoop-regionserver-mi-prod-hbase05.ec2.biz360.com.out;https://issues.apache.org/jira/secure/attachment/12444470/hbase-hadoop-regionserver-mi-prod-hbase05.ec2.biz360.com.out",,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26356,Reviewed,,,,Fri May 14 18:52:51 UTC 2010,,,,,,,,,,"0|i08se7:",49200,,,,,,,,,,,,,,,,,,,,,"14/May/10 05:31;stack;Made this blocker.  If a deadlock in 0.20.4, then we need a 0.20.5 quick.;;;","14/May/10 17:05;tlipcon;This is the same issue recently reported on the list. There appears to be an infinite loop in ExplicitColumnTracker. Looking into it as JD is on vacation.;;;","14/May/10 17:15;tlipcon;This is a stupid bug. 2 line fix attached. If ExplicitColumnTracker ever spins once, it spins forever.

But clearly we need a unit test for this!!! And we need to evaluate how we managed to release with such a bug - clearly we're missing something in our prerelease QA procedure.;;;","14/May/10 17:22;tlipcon;er, take two, uploaded wrong thing.;;;","14/May/10 17:48;stack;Patch looks right.  Kris is trying it on his cluster now.  I made him a jar with this included: http://people.apache.org/~stack/hbase-0.20.4-2524.jar  Working on a unit test now.

;;;","14/May/10 18:01;tlipcon;Patch with unit test. It times out without the fix due to infinite loop. Some please doublecheck that my ""expected"" is right in the unit test - I don't know the scanner semantics that well.;;;","14/May/10 18:01;tlipcon;Man, I am awful at uploading today... this one really has the unit test!;;;","14/May/10 18:25;stack;Version for trunk... makes trunk work like branch.;;;","14/May/10 18:26;stack;I applied to branch.;;;","14/May/10 18:47;streamy;@stack, this bug does not exist in trunk.  no need to change the code to match branch (and the change u made isn't actually the same since recursive is never set to true).  that style is how this bug has been introduced twice.;;;","14/May/10 18:51;streamy;I opened HBASE-2549 to review the trackers after 2248 gets committed to trunk.  This code was once all clean but it's been slightly crufted through normal evolution and bug fixes.  I've got a bunch of early-out and seek-forward / block skipping optimizations I'm working on which are related to these return codes and I will review this as part of 2549.;;;","14/May/10 18:52;stack;OK.  Thanks Jon for review.  I applied just the test to TRUNK.  On branch 0.20, I applied test and patch.  Resolving this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forward port branch 0.20 WAL to TRUNK,HBASE-2544,12464491,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,13/May/10 23:54,20/Nov/15 12:43,14/Jul/23 06:06,13/May/10 23:59,,,,,,,,,,,,0.90.0,,,,,,,0,,,Tests are failing on TRUNK.  The WAL implementation in TRUNK does not do the hookups for hdfs-200 properly.  Fix this before forward porting other issues from branch that are not in trunk.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/May/10 23:58;stack;2544.txt;https://issues.apache.org/jira/secure/attachment/12444447/2544.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26355,,,,,Fri Nov 20 12:43:22 UTC 2015,,,,,,,,,,"0|i0hi6n:",100208,,,,,,,,,,,,,,,,,,,,,"13/May/10 23:58;stack;Adds to TestHLog the testAppend.  In TRUNK, we don't run shutdown hook on exit of minihbasecluster.  Added to HRS in minihbasecluster the running of a shutdown of the filesystem made just for the HRS.  Makes it so master can pick up files the HRS used to be writing on pseudo-kill of HRS (Fixed failing TestMasterTransitions test).   Moved some of the dirty reflection that was done in HLog constructor in 0.20 out to SequenceFileLogWriter.  Added in support for hdfs-826 from branch and the new splitting that Nicolas did.  ;;;","13/May/10 23:59;stack;Committed.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot start ZK before the rest in tests anymore,HBASE-2539,12464392,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,12/May/10 21:32,20/Nov/15 12:41,14/Jul/23 06:06,18/May/10 23:23,,,,,,,,,,,,0.90.0,,,,,,,0,,,"HBASE-2414 added a coarse check to see if the test cluster is up, but that prevents from starting ZK before everything else. Needed for replication's unit testing.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2223,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/10 22:11;jdcryans;HBASE-2539-v2.patch;https://issues.apache.org/jira/secure/attachment/12444838/HBASE-2539-v2.patch","12/May/10 22:53;jdcryans;HBASE-2539.patch;https://issues.apache.org/jira/secure/attachment/12444355/HBASE-2539.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26354,Reviewed,,,,Fri Nov 20 12:41:10 UTC 2015,,,,,,,,,,"0|i0hi5j:",100203,,,,,,,,,,,,,,,,,,,,,"12/May/10 22:53;jdcryans;Patch that checks in startMiniCluster that a ZK cluster wasn't already started to decide if we need to check for a data dir.

Also adds a check in TestZooKeeper.;;;","12/May/10 22:58;stack;+ 1 except this comment is wrong:

+    // If we already put up a cluster, fail. Unless only zk was started

Looks like in the code, you allow that a dfs cluster might also be started and you'll skip the cluster check
;;;","18/May/10 22:11;jdcryans;Better approach for this jira, instead of doing weird conditions to detect if we need to check if the cluster's running I give it a separate temporary directory in case we start Zookeeper apart from the rest of the components.;;;","18/May/10 22:45;stack;Does the separate zk dir get cleaned up when test is done?

;;;","18/May/10 22:54;stack;J-D showed me that we register dirs for delete on exit inside in the setup of tmp dir (This seems broke at mo. but thats a different issue).  So +1 on commit.;;;","18/May/10 23:23;jdcryans;Committed to trunk, thanks for looking at it Stack.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32-bit encoding of regionnames waaaaaaayyyyy too susceptible to hash clashes,HBASE-2531,12464206,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,kannanm,stack,stack,11/May/10 05:30,14/Apr/21 05:57,14/Jul/23 06:06,01/Jun/10 03:52,,,,,,,,,,,,0.90.0,,,,,,,0,moved_from_0_20_5,,"Kannan tripped over two regionnames that hashed the same:

Here is code demo'ing that his two names hash the same:

{code}
package org;

import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.hbase.util.JenkinsHash;


public class Testing {
  public static void main(final String [] args) {
    System.out.println(encodeRegionName(Bytes.toBytes(""test1,6838000000,1273541236167"")));
    System.out.println(encodeRegionName(Bytes.toBytes(""test1,0520100000,1273541610201"")));
  }

  /**
   * @param regionName
   * @return the encodedName
   */
  public static int encodeRegionName(final byte [] regionName) {
    return Math.abs(JenkinsHash.getInstance().hash(regionName, regionName.length, 0));
  }
}
{code}

Need new encoding mechanism.  Will need to migrate old regions to new schema.",,dhruba,hammer,kannanm,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ZOOKEEPER-704,,,,,,,,,HBASE-2600,,,,,,,,,,,,,"29/May/10 01:54;kannanm;HBASE-2531_v2.patch;https://issues.apache.org/jira/secure/attachment/12445830/HBASE-2531_v2.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26351,Reviewed,,,,Fri Nov 20 12:42:26 UTC 2015,,,,,,,,,,"0|i0hi3z:",100196,Changes format of region name.  Adds an md5 suffix.  Suffix is now the name used as directory name in filesystem.,,,,,,,,,,,,,,,,,,,,"11/May/10 05:48;stack;After some discussion up on IRC -- todd, posix4e, jdcryans, kannan -- and thought was that UUID would make the most sense.;;;","11/May/10 06:04;stack;This change would need to include runtime migration of old format to the new.;;;","11/May/10 18:14;kannanm;Do we ever need to figure out the encoded name, given just the region name (<tablename>, <startkey>, <timestamp>)?

If not, and say, we go with UUIDs to generate future encoded names, then is there any work needed to done for migration at all? 

After an upgrade, for a period, there'll be a mix of old and new format of encoded names, and when splits etc. happens and old region go away, gradually the system should only be left with new format names.;;;","11/May/10 18:37;dhruba;Please allow me to interject myself in this conversation.

It appears that UUID proposal will work. However, it always leaves the possibility of data corruption open. In the rare case when you might run two region servers on the same machine (if ever!), then we might get a chance of UUID collision, especially for a workload when region splitting occurs v frequently. The UUID approach also seems to imply that some sort of ""migration of old format to new format"" is required.

Isn't it more elegant and easier if we do the following: ""when a region server splits a region it needs to create a new name, It can come up with a random number as it currently does and then try to create a znode in zookeeper with that name, if it already exists, then the region server can generate a new name. if the znode does not exist, then it will create the znode before creating the new region with the region name. will that work?"" This will let us avoid any need for ""migration"" while preventing any possibility of uuid collisions.;;;","11/May/10 18:56;tlipcon;dhruba: I had suggested a similar thing on IRC last night, and people seemed to think that UUID would be easier to implement. The chances of multiple UUIDs generated in the same nanotime on the same machine seems pretty small, no?;;;","11/May/10 19:35;dhruba;I agree that the ""chance of UUID collision is low"". But is it really true (especially if this needs a ""migration"" as described in earlier comments in this JIRA) that the ZK approach is harder/complex to implement?;;;","11/May/10 21:34;kannanm;Literature online seems to suggest that version 1 (timestamp based UUIDs) are unique for all practical purposes.  [Also read that <<If two UUIDs are generated in sequence fast enough that the timestamp matches the previous UUID, the timestamp is incremented by 1.>>]. 

Some references: 

http://stackoverflow.com/questions/703035/when-are-you-truly-forced-to-use-uuid-as-part-of-the-design
http://blogs.msdn.com/oldnewthing/archive/2008/06/27/8659071.aspx

Re: migration, in a ZK based approach, wouldn't moving the pre-existing list of encoded region names to ZK require some work?

For UUID based approach, it seems like there shouldn't really be any migration work. Stack: can you confirm?

I;;;","12/May/10 23:48;stack;Bulk move of 0.20.5 issues into 0.21.0 after vote that we merge branch into TRUNK up on list.;;;","18/May/10 00:36;kannanm;Stack: Pinging regarding my question on what migration work would be needed? (See my update @ 11/May/10 02:14 PM).;;;","18/May/10 05:10;stack;@Kannan As I see it, we just need to make sure that the system can work with both styles of naming; that it reads in the old stuff without issue and that at same time, for any new region created, we should use the UUID form writing new region directory names.  Somehow, we also need to drop this notion of encoding the region name.  Going forward it will not be needed since the UUID will actually be part of the region name.

I agree that getting zk in the mix or even hdfs for that matter making region dirctory names complicates something that could be real simple if we use UUIDs.;;;","18/May/10 05:58;dhruba;> Going forward it will not be needed since the UUID will actually be part of the region name.

@Stack, thanks for the info.  If the UUD is part of the regionname, then does this mean that nothing needs to be done for this JIRA? If that is not the case and we want to do somethinkg for this JIRS, can you pl explain what needs to be done? Thanks.;;;","18/May/10 23:19;stack;@Dhruba

Some good stuff related to this issue came up on IRC this afternoon.  In particular, we probably want region names to sort such that children of splits are inserted after their parent.  One of the childs of a split will have same startkey as the parent, the one that is carrying the lower range of the split.  The only differentiator is the 3rd part of the regionname where regionnames are formatted as

{code}
<tablename> ',' <startkey> ',' <timestamp>
{code}

.... and since it currently timestamp, the child will go into META after the parent (Things should still work even if child goes in before parent but of that I'm not certain).

So, we probably want to keep this attribute of regionnames.

This would seem to rule out UUID as the 3rd component of regionnames since they are effectively 'random' (version 1 was time-based but as Kannan points out, they are prefixed with MAC address and anyways, java doesn't do version 1 UUID).
;;;","18/May/10 23:36;stack;... continuing

As to what needs to be done, at a minimum, we need to change the way we name dirs in the filesystem.  Currently its done as follows

{code}
  /**
   * @param regionName
   * @return the encodedName
   */
  public static int encodeRegionName(final byte [] regionName) {
    return Math.abs(JenkinsHash.getInstance().hash(regionName, regionName.length, 0));
  }
{code}

The minimally intrusive thing would be to change the above hashing to instead return a byte array or a String and have the function md5 or sha-1 the regionName  so there is some relation between the regionname and hash, or just return a UUID, a product that cannot be related to the regionname.  We'd then need to go through code base and make sure that everywhere we deal with the encoded name of the region, that we can handle BOTH the new style byte [] or String format and the old format int.

Since we cannot derive the regionname from the UUID, we must be careful we do not misplace the UUID.  We'd have to save it into the regions HRegionInfo object.

md5/sha-1 would be superior because we can always go from regionname to the encoded name.

I was thinking (and I think Kannan the same), that rather than timestamp alone as the 3rd component of the regionname, that rather we'd make it so the 3rd portion of the regionname serve two functions: its current one as differentiator between child and parent (see previous comment) but that this 3rd component would also be what we use for the region directory in the filesystem.   Timestamp alone would not be enough.  After this afternoon's IRC discussions, UUID isn't suitable.  We'd have to tag on something extra.  It could be an md5 of the startkey or it could just be jenkins hash of the startkey since likelihood of hash-of-startkey+timestamp would clash is unlikely.

I liked this later option because you'd read the regionname and would be able to then easily find the region's dir in the filesystem. 

This would be a more intrusive change than the one above where we just change hash function.

;;;","19/May/10 00:32;kannanm;Writing out the details of one possible solution. Comments welcome.

Old region names continue to have the format: <tablename>,<startkey>,<timestamp>. For region names in this format, the encoded name will continue to be the old JenkinsHash implementation.
New region names have the format: <tablename>,<startkey>,<timestamp>,<dirname> where <dirname> is the md5 hash of the <tablename>,<startkey>,<timestamp> and will serve as encoded name/directory name in FS for the region.

This preserves the property that child regions (splits) will have a region name that sorts higher than the parent.

Search to determine what region serves a particular key is done today by building a key of the form:
  <tablename>,<searchkey>,99999999999999

That's 14 9's. On our test cluster I noticed region names of the form: test1,0013440000,1273816773769. That's 13 digits for the timestamp part. Going forward, we'll have region names of the form: test1,<key>,<13digit-ts>,<md5hash>. But the 14 9's based search key would continue to work just fine even with the new format region names since '9' > ',' .

;;;","19/May/10 00:34;tlipcon;Regardless of what we change it to, can I suggest that we add a new class RegionCode or RegionId which serves as a ""typesafe id""? This makes it harder to have bugs where we mix up the various strings used to refer to a region (at the expense of a small amount of memory for the wrapper class).;;;","19/May/10 04:08;stack;Yes to Todd suggestion.

Kannan, I'm down w/ your suggesion except for bit where ',' is also the delimiter between timestamp and dirname.   Use a '.' or something instead.   Special meta region comparator code looks for the ',' characters dividing up the parts of a meta key doing sorting.  The extra ',' will throw it off and you'll get a headache trying to sort out how this comparator works.  it gets really interesting when meta splits. (though currently this is disabled).... for then you have meta regionnames that look like this:  meta,TestTable,SOMESTARTKEY,TS,TS... then throw in fact that starkeys can be binary and my sense is that about now you feel a migrane coming on.

I'm good w/ md5.  128 bits vs 160 bits for sha-1 (which seems overkill).  Or we could keep jenkins hash -- 32 bits -- because and use timestamp+jenkins_hash naming dir.  A collision is unlikely?;;;","19/May/10 15:08;kannanm;Stack: Good point on the META keys. A ""."" or ""-"" would work better. Those sort lower than a ""9"" as well. 

Re: timestamp+jenkins_hash--- my feeling is that while we are in the process of modifying the format, might as well go with something stronger like md5 or sha1. Those would be more bits that TS+jenkins hash.



;;;","19/May/10 17:17;stack;.bq Re: timestamp+jenkins_hash--- my feeling is that while we are in the process of modifying the format, might as well go with something stronger like md5 or sha1. Those would be more bits that TS+jenkins hash.

Better to be safe, agreed.;;;","28/May/10 23:46;kannanm;will post a prelim patch on this soon on review board. Unit tests pass. Did some cluster testing, and things seemed fine. Migration tests from older version of HBase servers needs to be done.;;;","29/May/10 00:17;kannanm;http://review.hbase.org/r/104/

;;;","29/May/10 00:18;hbasereviewboard;Message from: ""Kannan Muthukkaruppan"" <kannan@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/104/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

The new format for a region name contains its encodedName. The encoded name also serves as the directory name for the region in the filesystem.

New region name format:

      <tablename>,<startkey>,<regionIdTimestamp>/<encodedName>/

where, <encodedName> is a hex version of the MD5 hash of <tablename>,<startkey>,<regionIdTimestamp>
 
The old region name format remains:
     <tablename>,<startkey>,<regionIdTimestamp>

For region names in the old format, the encoded name is a 32-bit JenkinsHash integer value (in its decimal notation, string form). 

**NOTE**
  
ROOT, the first META region, and regions created by an older version of HBase (0.20 or prior) will continue to use the old region name format.


In the logs & web ui, old format region names will show up as:
   <tablename>,<startkey>,<regionIdTimestamp>(<jenkinshashEncodedName>)
New format region names will show up as:
    <tablename>,<startkey>,<regionIdTimestamp>/<md5hashEncodedName>/


This addresses bug HBASE-2531.


Diffs
-----

  trunk/bin/add_table.rb 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 949322 
  trunk/src/main/resources/hbase-webapps/master/table.jsp 949322 
  trunk/src/main/resources/hbase-webapps/regionserver/regionserver.jsp 949322 
  trunk/src/test/java/org/apache/hadoop/hbase/TestEmptyMetaInfo.java 949322 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java 949322 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java 949322 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionInfo.java 949322 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 949322 

Diff: http://review.hbase.org/r/104/diff


Testing
-------

unit tests pass. ran some 


Thanks,

Kannan


;;;","29/May/10 01:04;hbasereviewboard;Message from: ""Kannan Muthukkaruppan"" <kannan@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/104/
-----------------------------------------------------------

(Updated 2010-05-28 18:03:20.789555)


Review request for hbase.


Summary
-------

The new format for a region name contains its encodedName. The encoded name also serves as the directory name for the region in the filesystem.

New region name format:

      <tablename>,<startkey>,<regionIdTimestamp>/<encodedName>/

where, <encodedName> is a hex version of the MD5 hash of <tablename>,<startkey>,<regionIdTimestamp>
 
The old region name format remains:
     <tablename>,<startkey>,<regionIdTimestamp>

For region names in the old format, the encoded name is a 32-bit JenkinsHash integer value (in its decimal notation, string form). 

**NOTE**
  
ROOT, the first META region, and regions created by an older version of HBase (0.20 or prior) will continue to use the old region name format.


In the logs & web ui, old format region names will show up as:
   <tablename>,<startkey>,<regionIdTimestamp>(<jenkinshashEncodedName>)
New format region names will show up as:
    <tablename>,<startkey>,<regionIdTimestamp>/<md5hashEncodedName>/


This addresses bug HBASE-2531.


Diffs
-----

  trunk/bin/add_table.rb 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 949322 
  trunk/src/main/resources/hbase-webapps/master/table.jsp 949322 
  trunk/src/main/resources/hbase-webapps/regionserver/regionserver.jsp 949322 
  trunk/src/test/java/org/apache/hadoop/hbase/TestEmptyMetaInfo.java 949322 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java 949322 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java 949322 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionInfo.java 949322 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 949322 

Diff: http://review.hbase.org/r/104/diff


Testing (updated)
-------

unit tests pass. ran some cluster tests, and things seemed to work ok. Yet to try some migration test (upgrading from an older server).


Thanks,

Kannan


;;;","29/May/10 01:52;hbasereviewboard;Message from: ""Kannan Muthukkaruppan"" <kannan@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/104/
-----------------------------------------------------------

(Updated 2010-05-28 18:50:19.120776)


Review request for hbase.


Changes
-------

Forgot to include one new file (hbase/util/MD5Hash.java).

BTW, also manually ran a test with a cluster created using an older HBase -- with regions in old format. Then restarted cluster with new HBase. Newly created regions were in new format as expected. And over time, as old regions, split they migrated over to the new format.


Summary
-------

The new format for a region name contains its encodedName. The encoded name also serves as the directory name for the region in the filesystem.

New region name format:

      <tablename>,<startkey>,<regionIdTimestamp>/<encodedName>/

where, <encodedName> is a hex version of the MD5 hash of <tablename>,<startkey>,<regionIdTimestamp>
 
The old region name format remains:
     <tablename>,<startkey>,<regionIdTimestamp>

For region names in the old format, the encoded name is a 32-bit JenkinsHash integer value (in its decimal notation, string form). 

**NOTE**
  
ROOT, the first META region, and regions created by an older version of HBase (0.20 or prior) will continue to use the old region name format.


In the logs & web ui, old format region names will show up as:
   <tablename>,<startkey>,<regionIdTimestamp>(<jenkinshashEncodedName>)
New format region names will show up as:
    <tablename>,<startkey>,<regionIdTimestamp>/<md5hashEncodedName>/


This addresses bug HBASE-2531.


Diffs (updated)
-----

  trunk/bin/add_table.rb 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java 949322 
  trunk/src/main/java/org/apache/hadoop/hbase/util/MD5Hash.java PRE-CREATION 
  trunk/src/main/resources/hbase-webapps/master/table.jsp 949322 
  trunk/src/main/resources/hbase-webapps/regionserver/regionserver.jsp 949322 
  trunk/src/test/java/org/apache/hadoop/hbase/TestEmptyMetaInfo.java 949322 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java 949322 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java 949322 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionInfo.java 949322 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java 949322 

Diff: http://review.hbase.org/r/104/diff


Testing
-------

unit tests pass. ran some cluster tests, and things seemed to work ok. Yet to try some migration test (upgrading from an older server).


Thanks,

Kannan


;;;","29/May/10 21:51;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/104/#review100
-----------------------------------------------------------


It looks great to me.  Only objection is '/' as delimiter in new format.  Would prefer something doesn't require escaping when searching in tools such as vim.  How about a '.'?  I'm going to try it out now.  Will report back.


trunk/bin/add_table.rb
<http://review.hbase.org/r/104/#comment585>

    I'm sorry you had to edit this file Kannan



trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
<http://review.hbase.org/r/104/#comment586>

    Are you tied to '/'?  Could we use '.' instead?  I'm in vi and having to search for instances of a regionname.  I'll have to escape the / before I begin my search?  If it was a '.' or  '-' I wouldn't have to?



trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
<http://review.hbase.org/r/104/#comment587>

    This will come out funny in javadoc I think.  You might have to change the '<' into < for them to show properly.



trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
<http://review.hbase.org/r/104/#comment588>

    Nice.  It'll be easy to change then.



trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
<http://review.hbase.org/r/104/#comment589>

    Nitpick: This could be written as:
    
    return (regionName.length >= 1) && (regionName[regionName.length - 1] == ENC_SEPARATOR);



trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
<http://review.hbase.org/r/104/#comment591>

    good



trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
<http://review.hbase.org/r/104/#comment592>

    Whats happening here?  Why not just return the cached String?  Does it not include the new encoded suffix?



trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
<http://review.hbase.org/r/104/#comment593>

    What changed on this line (and the one before).  Just watching out for you.  Any changes in hfile cause Ryan to perk up and come looksee.



trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
<http://review.hbase.org/r/104/#comment594>

    Be careful.  Looks only one real change in this file (though the diff reports many more than that)



trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.hbase.org/r/104/#comment595>

    This is great - getting rid of having to tag on the encoding



trunk/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
<http://review.hbase.org/r/104/#comment596>

    What changed on this line?



trunk/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
<http://review.hbase.org/r/104/#comment597>

    What changed on these lines?



trunk/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
<http://review.hbase.org/r/104/#comment598>

    Yeah, what changed here?



trunk/src/main/java/org/apache/hadoop/hbase/util/MD5Hash.java
<http://review.hbase.org/r/104/#comment599>

    Throw a RuntimeException I'd say.



trunk/src/main/resources/hbase-webapps/master/table.jsp
<http://review.hbase.org/r/104/#comment600>

    This is good


- stack



;;;","30/May/10 00:11;hbasereviewboard;Message from: ""Kannan Muthukkaruppan"" <kannan@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/104/#review102
-----------------------------------------------------------



trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
<http://review.hbase.org/r/104/#comment601>

    will do.



trunk/src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
<http://review.hbase.org/r/104/#comment602>

    old style region names don't have their encoded name in the regionNameStr. So I check for that here, and append the encoded name for those regions so that the logs, web UI, etc. will display the encoded named for those regions.



trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
<http://review.hbase.org/r/104/#comment603>

    will get rid of all whitespace diffs.



trunk/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
<http://review.hbase.org/r/104/#comment604>

    Will get rid of all the whitespace diffs. After the big whitespace cleanup landed in 0.20, I decided to set my eclipse to kill trailing whitespaces. But looks like trunk still has a bunch of whitespaces.



trunk/src/main/java/org/apache/hadoop/hbase/util/MD5Hash.java
<http://review.hbase.org/r/104/#comment605>

    ok...


- Kannan



;;;","30/May/10 00:15;hbasereviewboard;Message from: ""Kannan Muthukkaruppan"" <kannan@facebook.com>



;;;","31/May/10 18:28;stack;I tried it.  I saw stuff like this:

{code}
2010-05-31 11:19:10,258 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_SPLIT_INCLUDES_DAUGHTERS: TestTable,0010151443,1275329539809(717050107): Daughters; TestTable,0010151443,1275329944907/80f60878ba8994c3458931a127d77377/, TestTable,0010376061,1275329944907/9058243430462cd436fc06b748c0aaca/ from sv2borg187,60020,1275329765174; 1 of 1
{code}

That looks really good as does a scan of .META. (I can see parents and daughters.... we should get rid of the new-style encoded field in HRegionInfo, but we can do that later).

Since there are so few things to change -- and changing the separator I can do -- you want me to fix the above white-spacing, etc., issues on commit Kannan; IOW, you don't have to make a new patch?

Looks great Kannan.;;;","31/May/10 18:30;stack;oh, what I did was load an hbase and then I changed the jar to have your patch and restarted... then started loading the table again to prove the patch worked where there was a mix of old and new style encodings.  The table seems healthy after letting some splits go on and I don't see exceptions, etc. ;;;","31/May/10 20:19;kannanm;Sure-- feel free to update and commit the patch. Thanks Stack.;;;","01/Jun/10 03:52;stack;Committed.  Thanks for sweet patch Kannan.;;;","01/Jun/10 04:22;tlipcon;Why aren't we also changing ROOT and the first META? I'm afraid this will come back to bite us when we actually try to fix meta splitting.;;;","01/Jun/10 04:37;stack;.bq Why aren't we also changing ROOT and the first META? I'm afraid this will come back to bite us when we actually try to fix meta splitting.

I opened a new issue, HBASE-2639, to bring these over too.;;;","01/Jun/10 17:50;kannanm;Todd/Stack: ROOT and the *first* META region are handled in a special hard-coded way in code. See HRegionInfo::ROOT_REGIONINFO and HRegion::FIRST_META_REGIONINFO.

For example, their region id is hardcoded to 0 & 1 (and they do not use timestamps).

-ROOT-,,0
.META.,,1

Their Jenkins hash encoded names: 70236052 & 1028785192 respectively do not collide either.  Not changing this simplifies handling upgrade from an older version of HBase.

This should not be an issue for splits of META. If META splits, *it should* use the new format region names (with timestamps for the region id portion) and md5 hash for encoded names.;;;","01/Jun/10 20:31;stack;@Kannan I moved discussion of root and meta over to hbase-2639.  I'll comment there.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBASE-2165 removed compactionQueueSize metric,HBASE-2530,12464199,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ruifang,kannanm,kannanm,11/May/10 03:34,12/Oct/12 06:15,14/Jul/23 06:06,22/May/10 00:45,0.20.4,,,,,,,,,,,0.20.5,,,,,,,0,,,"Per RS compactionQueueSize metric is no longer reported. HBASE-2165 (r932137) seems to have reverted more stuff than just the fragmentation display alone.

Would be nice to have this added back.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/10 18:26;ruifang;2530.patch;https://issues.apache.org/jira/secure/attachment/12444820/2530.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26350,,,,,Wed May 26 07:58:47 UTC 2010,,,,,,,,,,"0|i08skv:",49230,,,,,,,,,,,,,,,,,,,,,"18/May/10 18:26;ruifang;Recovering compactionQueueSize using code from revision 932098;;;","19/May/10 18:51;streamy;Added Ruifang as a contributor (yay) and assigned to her.  Patch looks good.;;;","19/May/10 20:11;ruifang;Hi Jon,

Is there any further steps for this?

Thanks,
Ruifang


;;;","19/May/10 22:06;stack;Thank you for the patch.  Will apply after 0.20.5 is released to the 0.20 branch (or if we make a new release candidate, will add it to 0.20.5 then).;;;","22/May/10 00:45;streamy;This is in trunk, thanks.;;;","22/May/10 00:46;streamy;Marked wrong fix version;;;","22/May/10 22:31;stack;Chatted w/ Jon.  This is for 0.20... already in 0.21.;;;","26/May/10 07:58;stack;Committed to 0.20 branch for 0.20.5 release.  Thanks for the patch Ruifang Ge.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If one-RS only and its restarted before its shutdown is completely processed, we'll never complete shutdown processing",HBASE-2525,12464099,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,10/May/10 00:52,20/Nov/15 12:43,14/Jul/23 06:06,12/May/10 22:40,,,,,,,,,,,,0.90.0,,,,,,,0,,,"This is why TestZooKeeper is currently broken in build.  There is a single RS only in that test.  Its restarted using the hbase.regionserver.restart.on.zk.expire facility in the RS.  HBASE-2413 added blocking new servers of same host+port until the old is processed but if the old was only server carrying root and meta, and we won't let in the new server until the old server is processed AND processing of old server shutdown inline has assignment of root and meta, we're stuck.",,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/10 21:07;stack;2525-v2.txt;https://issues.apache.org/jira/secure/attachment/12444339/2525-v2.txt","10/May/10 00:53;stack;2525.txt;https://issues.apache.org/jira/secure/attachment/12444072/2525.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26349,Reviewed,,,,Fri Nov 20 12:43:35 UTC 2015,,,,,,,,,,"0|i0hi2v:",100191,,,,,,,,,,,,,,,,,,,,,"10/May/10 00:53;stack;Adds a bit of logging and enables dfs.append for the test.;;;","12/May/10 21:07;stack;Patch that blocks new servers coming in ONLY for as long as it takes for the expiration to process.  Previous we'd block until server with same host+port cleared the deadservers map making for server shutout if only one region in cluster.  On review, we don't need be this extreme.  We can let in server w/ same host+port but different startcode.  Looking at process server shutdown, it should be fine; e.g. in PSS, the scanner of meta looking for regions that were on the dead server pays attention to startcode:

{code}
        if (serverName == null || !deadServer.equals(serverName)) {
          // This isn't the server you're looking for - move along
          continue;
        }
{code}

... so if regions meantime assigned to the new server of same host+port, it should be grand.;;;","12/May/10 21:21;jdcryans;Looks good to me, loves the refactoring (except for the line that adds a trailing whitespace).;;;","12/May/10 22:40;stack;Thanks for the review J-D.  Committed branch and trunk.;;;","12/May/10 23:53;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StoreFileScanner.seek swallows IOEs,HBASE-2519,12463847,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,tlipcon,tlipcon,tlipcon,06/May/10 05:26,20/Nov/15 12:41,14/Jul/23 06:06,23/May/10 22:22,,,,,,,,,,,,0.90.0,,regionserver,,,,,0,moved_from_0_20_5,,"If a storefilescanner fails to seek, it silently swallows the IOE and returns false as if it were the end of the scanner.

This means that we can silently lose data if an IOE occurs for one of the store files during a compaction.",,dhruba,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2231,,,,,,,,,,,,,,,,,"07/May/10 15:41;tlipcon;hbase-2519-prelim.txt;https://issues.apache.org/jira/secure/attachment/12443975/hbase-2519-prelim.txt","21/May/10 22:21;tlipcon;hbase-2519.txt;https://issues.apache.org/jira/secure/attachment/12445215/hbase-2519.txt","19/May/10 22:32;tlipcon;hbase-2519.txt;https://issues.apache.org/jira/secure/attachment/12444993/hbase-2519.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26347,Reviewed,,,,Fri Nov 20 12:41:10 UTC 2015,,,,,,,,,,"0|i0hi1z:",100187,,,,,,,,,,,,,,,,,,,,,"07/May/10 08:06;tlipcon;In fact this needs to bubble all the way through to client - right now we also silently return no rows in a scanner in ScannerCallable on the client side if we get an IOE that isn't a DoNotRetryIOException or NSRE.;;;","07/May/10 15:37;stack;In above when you say 'client', you mean the thread orchestrating the compaction or do you mean client using HTable API?;;;","07/May/10 15:41;tlipcon;All the way to the client - see attached patch for example (will write a targeted unit test before commit);;;","12/May/10 23:48;stack;Bulk move of 0.20.5 issues into 0.21.0 after vote that we merge branch into TRUNK up on list.;;;","19/May/10 22:32;tlipcon;Patch and test case attached.;;;","21/May/10 20:54;tlipcon;Anyone got time to look at this patch?;;;","21/May/10 21:02;ryanobjc;it needs to be rebased again unfortunately. ;;;","21/May/10 22:21;tlipcon;Rebased;;;","21/May/10 22:25;tlipcon;Review here: http://review.hbase.org/r/75/;;;","21/May/10 22:32;apurtell;+1
Like the new fault injection tests.
;;;","23/May/10 05:40;stack;I had two minor nits in review.  I can fix them on commit.  That ok?;;;","23/May/10 22:22;stack;Committed.  Thanks for patch Todd.  Thanks for review over on review.hbase.org Ryan.;;;","24/May/10 05:31;tlipcon;Stack: looks like you missed the test case when you committed this - TestFSErrorsExposed.java isn't in trunk.;;;","24/May/10 15:47;stack;duh (fixed);;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Ugly IOE when region is being closed; rather, should NSRE",HBASE-2516,12463792,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,dploeg,stack,stack,05/May/10 16:37,20/Nov/15 12:43,14/Jul/23 06:06,24/May/10 15:44,,,,,,,,,,,,0.90.0,,,,,,,0,moved_from_0_20_5,,"I'm running 80/20 YCSB (80% reads/20% writes).  I see this from time to time in logs (especially if I do big fat bulk upload at same time -- having trouble making ycsb do heavy loading at mo):

{code}
2010-05-05 06:57:01,165 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates disabled for region, no outstanding scanners on usertable,user1431413702,1273040674721
2010-05-05 06:57:01,165 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: No more row locks outstanding on region usertable,user1431413702,1273040674721
2010-05-05 06:57:01,178 DEBUG org.apache.hadoop.hbase.regionserver.Store: closed cf
2010-05-05 06:57:01,178 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed usertable,user1431413702,1273040674721
2010-05-05 06:57:01,178 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer:
java.io.IOException: Region usertable,user1431413702,1273040674721 closed
        at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1179)
        at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1172)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2506)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2493)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1742)
        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
{code}

This should be a NSRE.

J-D took a look and fact that we do an IOE 'region closed' is way old, from before the time forgot, so its just always been there just more obvious now that Get is a Scan.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2486,,,,,,,,,,,,,"23/May/10 00:17;dploeg;HBASE-2516.patch;https://issues.apache.org/jira/secure/attachment/12445243/HBASE-2516.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26346,Reviewed,,,,Fri Nov 20 12:43:50 UTC 2015,,,,,,,,,,"0|i0hi1b:",100184,,,,,,,,,,,,,,,,,,,,,"12/May/10 23:48;stack;Bulk move of 0.20.5 issues into 0.21.0 after vote that we merge branch into TRUNK up on list.;;;","24/May/10 15:44;stack;Thanks for the patch Daniel.  Committed.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangeTableState considers split&&offline regions as being served,HBASE-2515,12463725,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,jdcryans,jdcryans,jdcryans,05/May/10 00:45,20/Nov/15 12:43,14/Jul/23 06:06,11/May/10 00:19,,,,,,,,,,,,0.90.0,,,,,,,0,,,"A region is considered as being serverd and unserved at the same time in the ChangeTableState (and TableOperation) class. This translates to logs like this:

{code}
2010-05-04 17:26:01,073 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_SPLIT_INCLUDES_DAUGHTERS: TestTable,0000518811,1273019008135: Daughters; 
TestTable,0000518811,1273019159867, TestTable,0000584541,1273019159867 from 10.10.1.177,60020,1273018776034; 1 of 1
<<disable is called>>
2010-05-04 17:26:25,893 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Skipping region REGION => {NAME => 'TestTable,0000518811,1273019008135', STARTKEY => '0000518811', ENDKEY => '0000650817', 
ENCODED => 143183187, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'TestTable', FAMILIES => [{NAME => 'info', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 
'false', BLOCKCACHE => 'true'}]}} because it is offline and split
2010-05-04 17:26:25,902 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region TestTable,0000518811,1273019008135 to setClosing list
2010-05-04 17:26:27,008 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Skipping region REGION => {NAME => 'TestTable,0000518811,1273019008135', STARTKEY => '0000518811', ENDKEY => '0000650817', 
ENCODED => 143183187, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'TestTable', FAMILIES => [{NAME => 'info', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY =>
 'false', BLOCKCACHE => 'true'}]}} because it is offline and split
2010-05-04 17:26:27,018 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region TestTable,0000518811,1273019008135 to setClosing list
{code}

The region gets stuck in transition! More details in a comment to come.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/10 23:45;jdcryans;HBASE-2515-branch+test.patch;https://issues.apache.org/jira/secure/attachment/12444016/HBASE-2515-branch%2Btest.patch","05/May/10 18:23;jdcryans;HBASE-2515.patch;https://issues.apache.org/jira/secure/attachment/12443750/HBASE-2515.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26345,Reviewed,,,,Fri Nov 20 12:43:52 UTC 2015,,,,,,,,,,"0|i0hi13:",100183,,,,,,,,,,,,,,,,,,,,,"05/May/10 00:47;jdcryans;TableOperation.call:
{code}
          if (!isBeingServed(serverName) || !isEnabled(info)) {
            unservedRegions.add(info);
          }
          processScanItem(serverName, info);
{code}

The two helper methods:
{code}
  protected boolean isBeingServed(String serverName) {
    boolean result = false;
    if (serverName != null && serverName.length() > 0) {
      HServerInfo s = master.getServerManager().getServerInfo(serverName);
      result = s != null;
    }
    return result;
  }

  protected boolean isEnabled(HRegionInfo info) {
    return !info.isOffline();
  }
{code}

And in ChangeTableState.processScanItem:
{code}
    if (isBeingServed(serverName)) {
      HashSet<HRegionInfo> regions = this.servedRegions.get(serverName);
      if (regions == null) {
        regions = new HashSet<HRegionInfo>();
      }
      regions.add(info);
      this.servedRegions.put(serverName, regions);
    }
{code}

An example of a split region:
{code}
 TestTable,,1273020409729    column=info:regioninfo, timestamp=1273020435277, value=REGION => {NAME => 'TestTa
                             ble,,1273020409729', STARTKEY => '', ENDKEY => '', ENCODED => 713340189, OFFLINE 
                             => true, SPLIT => true, TABLE => {{NAME => 'TestTable', FAMILIES => [{NAME => 'in
                             fo', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '6
                             5536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                            
 TestTable,,1273020409729    column=info:server, timestamp=1273020410555, value=10.10.1.177:60020             
 TestTable,,1273020409729    column=info:serverstartcode, timestamp=1273020410555, value=1273020312801 
{code}

Lots of information, but as you can see the table is OFFLINE and SPLIT but still has the assignment information. It's not enabled, so it is considered unserved but then it does have serverinfo so it is considered served.;;;","05/May/10 00:58;jdcryans;Also, I don't this is a blocker for 0.20.4, looking at the SVN history those lines were there for quite a while... ;;;","05/May/10 02:57;stack;This indeed looks wrong. How does it manifest itself?  You have a log?;;;","05/May/10 03:08;jdcryans;@Stack, look at the description of this jira.;;;","05/May/10 03:14;stack;I saw the description.  What I want to know is why this doesn't happen always on every split.  What is different that manufactures the lines pasted?  You don't explain.;;;","05/May/10 03:19;jdcryans;This happens on disable, not split (since this is called by ChangeTableState). An easy way of getting the error is running PE to get some splits and, before the master deletes the parent, run a disable.;;;","05/May/10 18:23;jdcryans;Ugly hack that works. In CompactSplitThread.split I replace the serveraddress and startcode with empty byte arrays (since we can't delete in a Put).

This is now what I get when disabling after fresh splits:
{code}
2010-05-05 11:17:44,827 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_SPLIT_INCLUDES_DAUGHTERS: TestTable,0000515726,1273083450063:
 Daughters; TestTable,0000515726,1273083461755, TestTable,0000782411,1273083461755 from 10.10.1.177,60020,1273083301200; 1 of 1
...
2010-05-05 11:17:52,260 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_SPLIT_INCLUDES_DAUGHTERS: TestTable,,1273083450063: Daughters; 
TestTable,,1273083471668, TestTable,0000256008,1273083471668 from 10.10.1.177,60020,1273083301200; 1 of 1
...

<<calling disable>>

2010-05-05 11:18:00,366 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing unserved regions
2010-05-05 11:18:00,367 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Skipping region REGION => {NAME => 'TestTable,,1273083425170', STARTKEY => '', ENDKEY =>
 '', ENCODED => 334691522, OFFLINE => true, SPLIT => true, ... because it is offline and split
2010-05-05 11:18:00,367 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Skipping region REGION => {NAME => 'TestTable,,1273083450063', STARTKEY => '', ENDKEY => 
'0000515726', ENCODED => 572799610, OFFLINE => true, SPLIT => true, ... because it is offline and split
2010-05-05 11:18:00,367 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Skipping region REGION => {NAME => 'TestTable,0000515726,1273083450063', STARTKEY => 
'0000515726', ENDKEY => '', ENCODED => 1192166465, OFFLINE => true, SPLIT => true, ...because it is offline and
 split
2010-05-05 11:18:00,367 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing regions currently being served
2010-05-05 11:18:00,369 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region TestTable,,1273083471668 to setClosing list
2010-05-05 11:18:00,370 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region TestTable,0000782411,1273083461755 to setClosing list
2010-05-05 11:18:00,371 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region TestTable,0000256008,1273083471668 to setClosing list
2010-05-05 11:18:00,379 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region TestTable,0000515726,1273083461755 to setClosing list
{code}

So in this trace only the offline and split regions were skipped, and they weren't put on the setClosing list which would have got them in the regionsInTransition list.

Not ready for commit, Stack said he wanted to try his new fancy test framework to unit test this.;;;","07/May/10 23:45;jdcryans;Same patch on branch, with a unit test that also uncovered HBASE-2522 (but the assertion is commented out).;;;","08/May/10 05:01;stack;What going on here?

{code}
+    // This fails, we still assign the 2 daughters of the split
+    // HBASE-2522
+    //assertEquals(2, cluster.getMaster().getClusterStatus().getRegionsCount());
{code}

You meant to leave it out?

Fix above and then +1 on commit.
;;;","10/May/10 23:29;jdcryans;No I meant to leave it in, but I can remove it and post it back in HBASE-2522 right away.;;;","11/May/10 00:19;jdcryans;Committed to branch and trunk without the commented out assertion, thanks for the review Stack!;;;","12/May/10 23:53;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RegionServer should refuse to be assigned a region that use LZO when LZO isn't available,HBASE-2514,12463722,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,ryanobjc,tsuna,tsuna,05/May/10 00:06,20/Nov/15 12:43,14/Jul/23 06:06,23/Oct/10 00:40,0.20.1,0.20.2,0.20.3,0.20.4,0.20.5,0.20.6,0.89.20100621,,,,,0.90.0,,regionserver,,,,,1,,,"If a RegionServer is assigned a region that uses LZO but the required libraries aren't installed on that RegionServer, the server will fail unexpectedly after throwing a {{java.lang.ClassNotFoundException: com.hadoop.compression.lzo.LzoCodec}}

{code}

2010-05-04 16:57:27,258 FATAL org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Replay of hlog required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: tsdb,,1273011287339
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:994)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:887)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:255)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:142)
Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: com.hadoop.compression.lzo.LzoCodec
        at org.apache.hadoop.hbase.io.hfile.Compression$Algorithm$1.getCodec(Compression.java:91)
        at org.apache.hadoop.hbase.io.hfile.Compression$Algorithm.getCompressor(Compression.java:196)
        at org.apache.hadoop.hbase.io.hfile.HFile$Writer.getCompressingStream(HFile.java:388)
        at org.apache.hadoop.hbase.io.hfile.HFile$Writer.newBlock(HFile.java:374)
        at org.apache.hadoop.hbase.io.hfile.HFile$Writer.checkBlockBoundary(HFile.java:345)
        at org.apache.hadoop.hbase.io.hfile.HFile$Writer.append(HFile.java:517)
        at org.apache.hadoop.hbase.io.hfile.HFile$Writer.append(HFile.java:482)
        at org.apache.hadoop.hbase.regionserver.Store.internalFlushCache(Store.java:558)
        at org.apache.hadoop.hbase.regionserver.Store.flushCache(Store.java:522)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:979)
        ... 3 more
Caused by: java.lang.ClassNotFoundException: com.hadoop.compression.lzo.LzoCodec
        at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:315)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:330)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:250)
        at org.apache.hadoop.hbase.io.hfile.Compression$Algorithm$1.getCodec(Compression.java:87)
        ... 12 more
{code}",,deinspanjer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2681,,,,HBASE-2681,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26344,,,,,Fri Nov 20 12:43:44 UTC 2015,,,,,,,,,,"0|i0d493:",74465,,,,,,,,,,,,,,,,,,,,,"22/Jun/10 17:12;deinspanjer;I ran into this when upgrading to 0.20.5-rc5.  I forgot to copy the hadoop-gpl-compression jar.  The RS didn't shut down though, it just kept spewing those exceptions.;;;","24/Jul/10 03:59;stack;Relating to HBASE-2681.  HBASE-2681 says to fall back to NONE if lib is not available.  It even has a patch.  I don't think that right after thinking on it.  I think refusing to open it is way to go.  Ryan brings up point over in the other issue that while the CF might say LZO, the store files may have be written with something else, some other compression that we need a lib for.  So, on open, its not good enough to look at the CF config. only.  We need to as part of storefile open -- which happens as part of region deploy anyways -- at this time verify that we have the codecs we need to deploy region.;;;","24/Jul/10 04:02;stack;Making this critical and bringing into 0.90.0.  I think this of import given we want to talk up use of lzo.;;;","04/Aug/10 08:47;tsuna;We hit that problem today on some of our prod machines at StumbleUpon.  {{!cool}}.;;;","25/Aug/10 19:09;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/719/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Trying to make some progress on this issue, here's an untested patch that adds testing of codecs when the region server starts, then uses that to check the schema of regions it tries to open. That's basically what this jira's scope is about.

Some things I don't like the way I did it:
 - For all the users that don't do LZO, they will have a WARN every time a RS starts (in its log)
 - Does not cover cases where the files are LZOed, but the schema is NONE or GZ

Finally, there's no way to let user know about the errors unless he checks the logs, but a least we won't throw ugly exceptions.


This addresses bug HBASE-2514.
    http://issues.apache.org/jira/browse/HBASE-2514


Diffs
-----

  /trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 989204 
  /trunk/src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java 989204 

Diff: http://review.cloudera.org/r/719/diff


Testing
-------


Thanks,

Jean-Daniel


;;;","18/Oct/10 23:12;ryanobjc;im thinking we might want to do a different approach, since checking the schema isnt comprehensive enough as JD pointed out above.  Instead if we had a list of ""required codecs"" we could abort the regionserver during startup.  This would allow the admin to take action to correct the deployment.

I'm not sure there is a good reason to have only part of your RS being able to open LZO regions and others passing and having the master bounce regions around until they stick.;;;","22/Oct/10 23:39;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/1046/
-----------------------------------------------------------

(Updated 2010-10-22 16:38:10.455573)


Review request for hbase.


Summary
-------

RegionServer should refuse to be assigned a region that use LZO when LZO isn't available

- instead of checking regions during open, i do a check and throw during the construction of the RS that is is able to instantiate the listed codecs.


This addresses bug HBASE-2514.
    http://issues.apache.org/jira/browse/HBASE-2514


Diffs (updated)
-----

  trunk/CHANGES.txt 1024074 
  trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java 1024073 
  trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java 1024073 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1024073 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 1024074 
  trunk/src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java 1024073 
  trunk/src/test/java/org/apache/hadoop/hbase/util/TestCompressionTest.java PRE-CREATION 

Diff: http://review.cloudera.org/r/1046/diff


Testing
-------


Thanks,

Ryan


;;;","23/Oct/10 00:40;ryanobjc;committed, with extra feature which needs documentation: HBASE-3146;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hbase-2414 added bug where we'd tight-loop if no root available.,HBASE-2513,12463720,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,tsuna,stack,stack,04/May/10 23:38,20/Nov/15 12:41,14/Jul/23 06:06,05/May/10 22:13,,,,,,,,,,,,0.90.0,,,,,,,0,,,,,larsfrancke,tsuna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/May/10 23:38;stack;2512-fix-for-2414-bug.patch;https://issues.apache.org/jira/secure/attachment/12443658/2512-fix-for-2414-bug.patch","05/May/10 04:47;stack;2513-v2.patch;https://issues.apache.org/jira/secure/attachment/12443677/2513-v2.patch","05/May/10 14:23;stack;2513-v3.patch;https://issues.apache.org/jira/secure/attachment/12443710/2513-v3.patch","05/May/10 21:46;tsuna;2513-v4.patch;https://issues.apache.org/jira/secure/attachment/12443775/2513-v4.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26343,Reviewed,,,,Fri Nov 20 12:41:48 UTC 2015,,,,,,,,,,"0|i0hi0v:",100182,,,,,,,,,,,,,,,,,,,,,"04/May/10 23:38;stack;Fix;;;","05/May/10 04:47;stack;Benoit found issue in the patch I committed... I passed a fancy flag but didn't actually use it (Param even had a fat stream of javadoc).  Here is a better patch.   Please review Benoit.;;;","05/May/10 05:02;tsuna;In your patch, you're adding a {{skipDelayedToDos}} argument to {{RegionServerOperationQueue#process}} but you don't use it... so I don't understand how it's supposed to fix anything;;;","05/May/10 06:58;tsuna;Comments on v2 of the patch:
* Please indent the code properly.
** The way the condition of the {{switch}} statement is wrapped around looks suspicious to me.
** The {{if}} statement on 1 line in {{process}}.
* In {{process}}, since you don't actually need the reference to the {{HServerAddress}} of the root region, I think you should just pass a {{boolean}} that indicates whether or not the root region is online (or, alternatively, whether or not the ""delayed todos"" should be done first, as you sort of tried to do in your first patch).
* If the root region is offline, the next thing we wanna do is bring it back online.  Are we guaranteed that the next {{poll}} on {{delayedToDoQueue}} will give us this task?  It seems your code assumes this but I'm not familiar enough with it to confirm that it's the case.;;;","05/May/10 14:23;stack;Thanks for the review Benoit.

This version fixes the indentation and adds some comments that hopefully will alleviate suspicions.  While true serveraddress is not needed inside the method, it seemed sillier to me exposing the internals of how the processing of events happens up in the parent method --  i.e. caller knowing about delay queue -- hence passing of root region location and letting the process method use it as it wants.

This patch is a fix for a bug introduced by 2414.  2414 started a refactoring of master queue processing moving it out of master.  Its not done.  2512 is about finishing the work.  There is a ways to go yet before queue processing is standalone testable.  This fix is about putting back a test that I left out when I did 2414 (It was testing root region location down in the process of operations method).  As I see it, by the time 2512 is done, the queue processing will likely make use of root location and more info from master context that I'll need to pass in.

I'd suggest that this patch is good for the moment addressing the spin you were seeing yesterday.

Thanks.;;;","05/May/10 21:46;tsuna;Trivial changes to the v3 of the patch.;;;","05/May/10 21:47;tsuna;+1 with the trivial changes I made in the v4 attached.;;;","05/May/10 22:13;stack;Applied branch and trunk.  Assigning to you Benoit since you did bulk of work on this.  Thanks for review.;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Apply HBASE-2509 (""NPEs in various places, HRegion.get, HRS.close"") to trunk after hbase-2248 goes in",HBASE-2511,12463713,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,stack,stack,04/May/10 22:20,20/Nov/15 12:42,14/Jul/23 06:06,18/May/10 03:07,,,,,,,,,,,,0.90.0,,,,,,,0,,,Passing to Ryan.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26342,,,,,Fri Nov 20 12:42:03 UTC 2015,,,,,,,,,,"0|i0hi0f:",100180,,,,,,,,,,,,,,,,,,,,,"18/May/10 03:07;ryanobjc;committed to trunk;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"NPEs in various places, HRegion.get, HRS.close",HBASE-2509,12463620,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,04/May/10 00:10,12/Oct/12 06:14,14/Jul/23 06:06,04/May/10 22:18,0.20.4,,,,,,,,,,,0.20.4,,,,,,,0,,,"ttr on irc reported that he was unable to get/scan sometimes, was getting NPEs.

The root cause is a delayed init of the RegionScanner.storeHeap means it can be null, not all accessors of it (specifically in close()) checked for that.",,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/May/10 00:26;ryanobjc;HBASE-2509-v2.txt;https://issues.apache.org/jira/secure/attachment/12443528/HBASE-2509-v2.txt","04/May/10 00:10;ryanobjc;HBASE-2509.txt;https://issues.apache.org/jira/secure/attachment/12443527/HBASE-2509.txt","04/May/10 21:49;stack;hbase-2509-v3.patch;https://issues.apache.org/jira/secure/attachment/12443648/hbase-2509-v3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26341,Reviewed,,,,Tue May 04 22:18:37 UTC 2010,,,,,,,,,,"0|i08st3:",49267,,,,,,,,,,,,,,,,,,,,,"04/May/10 00:17;tlipcon;Given that getStoreHeap() is public also, do we need to initHeap there too?;;;","04/May/10 00:26;ryanobjc;you are totally correct todd. here is an updated version of the patch which removes the call.;;;","04/May/10 00:29;radarcg;We had a similar issue during a map/reduce job. 

St^Ack provided a quick patch located at http://people.apache.org/~stack/ttr/ to see if the cause and solution is well understood. Results will be posted in a following comment.  I believe the patch to be the following code:

diff --git src/java/org/apache/hadoop/hbase/regionserver/HRegion.java src/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index 4ed4cbd..8d92fb7 100644
--- src/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ src/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -1974,7 +1974,10 @@ public class HRegion implements HConstants, HeapSize { // , Writable{
     }
 
     public void close() {
-      storeHeap.close();
+      if (storeHeap != null) {
+        storeHeap.close();
+        storeHeap = null;
+      }
     }

Below is the stack trace that occurred during the job, and also during 'get' and 'scan' commands. The following (located in full at http://pastebin.com/74bJeqTY) stack traces occurred upon upgrading from 0.20.3 to 0.20.4.


org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 172.28.10.204:60020 for region user_events,320\x7Cqf24mcristobal\x7Ca,1272349551722, row '321\x7C754435419575754318\x7Cc', but failed after 10 attempts.
Exceptions:
java.io.IOException: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:887)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:877)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1744)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.close(HRegion.java:1977)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2513)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2496)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1742)
        ... 6 more
 
java.io.IOException: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:887)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:877)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1744)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.close(HRegion.java:1977)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2513)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2496)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1742)
        ... 6 more
 
java.io.IOException: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:887)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:877)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1744)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.close(HRegion.java:1977)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2513)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2496)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1742)
        ... 6 more;;;","04/May/10 14:40;stack;Can we get a unit test for this one?  Seems easy enough conjuring Charles's context.;;;","04/May/10 21:49;stack;Here's Ryan's patch with a unit test. Condition is kinda hard to manufacture. I had to kinda force it by artificially setting closing flag on region and then adding new threads.... rather than do a straight close.   But I get a NPE so for sure its possible.

Committing.;;;","04/May/10 22:01;stack;Applied to the two branches.  I can't apply to TRUNK till hbase-2248 goes in.;;;","04/May/10 22:18;stack;Resolving. Opening new issue to bring into 0.21.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"PriorityQueue isn't thread safe, KeyValueHeap uses it that way",HBASE-2503,12463371,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,jdcryans,jdcryans,jdcryans,29/Apr/10 20:52,12/Oct/12 06:15,14/Jul/23 06:06,07/May/10 20:59,0.20.3,,,,,,,,,,,0.20.5,0.90.0,,,,,,0,,,"In the same spirit as HBASE-2077, but a bit different (at least to me). Dave Latham had the following NPE killing a RS:

{code}
Exception in thread ""regionserver/192.168.41.2:60020.leaseChecker"" java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:127)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:117)
        at java.util.PriorityQueue.siftDownUsingComparator(PriorityQueue.java:644)
        at java.util.PriorityQueue.siftDown(PriorityQueue.java:612)
        at java.util.PriorityQueue.poll(PriorityQueue.java:523)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.close(KeyValueHeap.java:151)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.close(HRegion.java:1862)
        at org.apache.hadoop.hbase.regionserver.HRegionServer$ScannerListener.leaseExpired(HRegionServer.java:1959)
        at org.apache.hadoop.hbase.Leases.run(Leases.java:98)
{code}

He also has the same stack traces from 2077. The PQ javadoc says this class is not thread safe, but it is used by the leaseChecker thread and the client threads. We need to use something like the BlockingPriorityQueue instead.",,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2077,,,,,,,,,,,,,,,,,"06/May/10 00:44;jdcryans;HBASE-2503.patch;https://issues.apache.org/jira/secure/attachment/12443800/HBASE-2503.patch","12/May/10 23:55;jdcryans;HBASE-2503_0.20.4.patch;https://issues.apache.org/jira/secure/attachment/12444363/HBASE-2503_0.20.4.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26337,Reviewed,,,,Fri May 14 19:35:22 UTC 2010,,,,,,,,,,"0|i08sdz:",49199,,,,,,,,,,,,,,,,,,,,,"29/Apr/10 21:02;ryanobjc;i think it makes sense to put the sync block at the HRegion.RegionScanner level - that is all public methods should have 'synchronized' on them.;;;","29/Apr/10 21:20;jdcryans;Yeah might as well... And it relates even more to 2077 than I thought. So here if a next is waiting on a close... the result is a bit unclear to me. The user would see this as the end of a scan as a null would be returned by this.storeHeap.peek()? So adding to the sync'ed methods, I think we need a ""closed' marker on RegionScanner that we would need to verify in next() and set in close().;;;","06/May/10 00:44;jdcryans;This patch synchronizes RegionScanner's public methods, removes a close() that wasn't used, adds a new attribute to check it the scanner was closed and adds a new test to see if we get the exception. I cannot find an elegant way to test the NPE directly, so I cannot guarantee that the original issue is fixed but now only 1 thread can access RegionScanner.;;;","07/May/10 03:42;tlipcon;Looks good to me.;;;","07/May/10 20:59;jdcryans;Thanks for the reviewed Todd.

It passes the test, committed to branch and trunk.;;;","12/May/10 23:53;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","12/May/10 23:55;jdcryans;Patch against 0.20.4 for those who get those NPEs. No commit needed.;;;","14/May/10 19:35;stack;Applied to branch for 0.20.5 release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition when disabling a table leaves regions in transition,HBASE-2499,12463257,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,28/Apr/10 20:45,12/Oct/12 06:14,14/Jul/23 06:06,29/Apr/10 01:25,0.20.3,,,,,,,,,,,0.20.4,0.90.0,,,,,,0,,,"A lot of people reported that weren't able to add/delete a column because only some of the regions got the modification. I personally thought it was due to the CME bug in the Master, but I'm able to easily reproduce on 0.20.4 on a 1800 regions table.

Since 0.20.3, we now call disableTable after every retry to make sure we don't miss any region. This creates a race where while we scan .META. in TableOperation, a region could be reported as closed after we scanned the row. We end up processing it like if it was assigned and we put it back into regionsInTransition. We need to either query .META. before processing each region or make some more check to see if the region was closed.

This kills the RC in my book.

In the mean time, anyone getting this can restart their HBase and it will pick up the change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/10 23:25;jdcryans;HBASE-2499-v2.patch;https://issues.apache.org/jira/secure/attachment/12443138/HBASE-2499-v2.patch","28/Apr/10 22:11;jdcryans;HBASE-2499.patch;https://issues.apache.org/jira/secure/attachment/12443129/HBASE-2499.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26335,Reviewed,,,,Thu Apr 29 01:25:06 UTC 2010,,,,,,,,,,"0|i08sw7:",49281,,,,,,,,,,,,,,,,,,,,,"28/Apr/10 22:11;jdcryans;This patch adds a new check to .META. (expensive!) before calling setClosing. Also, to lower the .META. stress, it prevents unnecessary puts in postProcessMeta. I'm now able to disable my big table without any issue.;;;","28/Apr/10 22:17;jdcryans;Testing a bit more with instrumentation, the first part of the patch does nothing. I won't commit it if this patch is +1ed.;;;","28/Apr/10 22:24;stack;+1 on patch (Reuse the BaseScanner method I'm about to open up with a quit now.;;;","28/Apr/10 23:25;jdcryans;Patch uses HBASE-2497 and fixes the condition (and tested that it works like it should).;;;","29/Apr/10 00:03;stack;+1 if all tests pass.;;;","29/Apr/10 01:25;jdcryans;Committed to the branches and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProcessServerShutdown throws NullPointerException for offline regions,HBASE-2497,12463207,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,mkurucz,mkurucz,mkurucz,28/Apr/10 13:16,20/Nov/15 12:43,14/Jul/23 06:06,28/Apr/10 22:47,0.20.3,,,,,,,,,,,0.90.0,,master,,,,,0,,,"When a regionsserver dies the master can run into the following bug.

2010-04-27 17:20:37,303 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessServerShutdown of dell106.cluster,60020,1272377612991
2010-04-27 17:20:37,303 INFO org.apache.hadoop.hbase.master.RegionServerOperation: process shutdown of server dell106.cluster,60020,1272377612991: logSplit: true, rootRescanned: true, numberOfMetaRegions: 1, onlineMetaRegions.size(): 1
2010-04-27 17:20:01,637 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Log split complete, meta reassignment and scanning:
2010-04-27 17:20:01,653 DEBUG org.apache.hadoop.hbase.master.ProcessServerShutdown$ScanRootRegion: process server shutdown scanning root region on 10.1.3.124
2010-04-27 17:20:01,664 DEBUG org.apache.hadoop.hbase.master.RegionServerOperation: process server shutdown scanning root region on 10.1.3.124 finished master
2010-04-27 17:20:01,683 DEBUG org.apache.hadoop.hbase.master.ProcessServerShutdown$ScanMetaRegions: process server shutdown scanning .META.,,1 on 10.1.3.104:60020
2010-04-27 17:20:18,087 DEBUG org.apache.hadoop.hbase.master.ProcessServerShutdown$ScanMetaRegions: Exception in RetryableMetaOperation:
2010-04-27 17:20:18,118 WARN org.apache.hadoop.hbase.master.HMaster: Adding to delayed queue: ProcessServerShutdown of dell106.cluster,60020,1272377612991
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:100)
        at org.apache.hadoop.hbase.master.ProcessServerShutdown.process(ProcessServerShutdown.java:345)
        at org.apache.hadoop.hbase.master.HMaster.processToDoQueue(HMaster.java:509)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:448)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.util.Bytes.toLong(Bytes.java:487)
        at org.apache.hadoop.hbase.util.Bytes.toLong(Bytes.java:461)
        at org.apache.hadoop.hbase.master.ProcessServerShutdown.scanMetaRegion(ProcessServerShutdown.java:147)
        at org.apache.hadoop.hbase.master.ProcessServerShutdown$ScanMetaRegions.call(ProcessServerShutdown.java:264)
        at org.apache.hadoop.hbase.master.ProcessServerShutdown$ScanMetaRegions.call(ProcessServerShutdown.java:250)
        at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:69)
        ... 3 more


The problem is in ProcessServerShutdown.java at line 148-149:

146	        String serverAddress = 
147	          Bytes.toString(values.getValue(CATALOG_FAMILY, SERVER_QUALIFIER));
148	        long startCode =
149	          Bytes.toLong(values.getValue(CATALOG_FAMILY, STARTCODE_QUALIFIER));
150	        String serverName = null;
151	        if (serverAddress != null && serverAddress.length() > 0) {
152	          serverName = HServerInfo.getServerName(serverAddress, startCode);
153	        }

It should be modified to:

146	        String serverAddress = 
147	          Bytes.toString(values.getValue(CATALOG_FAMILY, SERVER_QUALIFIER));
150	        String serverName = null;
151	        if (serverAddress != null && serverAddress.length() > 0) {
148	          long startCode =
149	            Bytes.toLong(values.getValue(CATALOG_FAMILY, STARTCODE_QUALIFIER));
152	          serverName = HServerInfo.getServerName(serverAddress, startCode);
153	        }

As Bytes.toLong cannot handle the null pointer returned by getValue for missing  STARTCODE_QUALIFIER of offline regions in META.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/10 17:52;stack;2497-v2.patch;https://issues.apache.org/jira/secure/attachment/12443104/2497-v2.patch","28/Apr/10 13:25;mkurucz;pss_diff.txt;https://issues.apache.org/jira/secure/attachment/12443080/pss_diff.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26334,Reviewed,,,,Fri Nov 20 12:43:33 UTC 2015,,,,,,,,,,"0|i0hhzb:",100175,,,,,,,,,,,,,,,,,,,,,"28/Apr/10 13:25;mkurucz;change in master/ProcessServerShutdown.java ;;;","28/Apr/10 14:44;jdcryans;Isn't this the same as HBASE-2479 or/and HBASE-2428?;;;","28/Apr/10 17:52;stack;May I apply the attached patch instead Miklos?  It reuses code that checks returns out of meta scan results elsewhere?  Thank.;;;","28/Apr/10 20:20;mkurucz;Allright.
I would also like to note that this same defective code exists in master/TableOperation.java at lines 106-107 in 0.20.3, but is already fixed in trunk.;;;","28/Apr/10 22:47;stack;Thank you for the patch Miklos Kurucz.  I applied it to the two branches and trunk (including suggested fix to TableOperation).;;;","12/May/10 23:53;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Does not apply new.name parameter to CopyTable.,HBASE-2494,12463084,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,,ohyoonsik,ohyoonsik,27/Apr/10 08:56,20/Nov/15 12:42,14/Jul/23 06:06,27/Apr/10 13:23,0.90.0,,,,,,,,,,,0.90.0,,mapreduce,,,,,0,,,Checking new.name parameter is wrong.  Change 'rsClassArgKey' variable to 'newNameArgKey'.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/10 08:59;ohyoonsik;param.patch;https://issues.apache.org/jira/secure/attachment/12442941/param.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26333,Reviewed,,,,Fri Nov 20 12:42:16 UTC 2015,,,,,,,,,,"0|i0hhyn:",100172,,,,,,,,,,,,,,,,,,,,,"27/Apr/10 13:23;stack;Applied to TRUNK.  Thank you for the patch Yoonsik Oh.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
master.jsp uses absolute links to table.jsp. This broke when master.jsp moved under webapps/master,HBASE-2491,12462998,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,,civascu,civascu,26/Apr/10 08:24,20/Nov/15 12:43,14/Jul/23 06:06,26/Apr/10 20:05,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"In the latest trunks / 0.21 branch, the master's web UI sits under /webapps/master. All links to the specific tables fail to work, because the master.jsp uses absolute links for them - e.g. href=""/table.jsp"".
Although the fact that the master pages shouldn't be under /webapps/master ( see HBASE-2369), these links should still be relative to the master page - href=""table.jsp"", considering that they are not called from anywhere else.","CentOS 5.3, x64
MacOSX 10.6.3
HBase - 0.21 on both",larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2369,,,,,,,,,,,,,,,,,"26/Apr/10 08:29;civascu;HBASE-2491.patch;https://issues.apache.org/jira/secure/attachment/12442836/HBASE-2491.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26330,Reviewed,,,,Fri Nov 20 12:43:58 UTC 2015,,,,,,,,,,"0|i0hhy7:",100170,,,,,,,,,,,,,,,,,,,,,"26/Apr/10 08:29;civascu;proposed fix - makes links relative
no tests - it's quite a trivial change;;;","26/Apr/10 20:05;stack;Thank you for the patch Cristian.  Applied to TRUNK.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uncaught exceptions in receiving IPC responses orphan clients,HBASE-2487,12462983,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,tlipcon,tlipcon,tlipcon,26/Apr/10 05:48,20/Nov/15 12:40,14/Jul/23 06:06,27/Apr/10 00:37,,,,,,,,,,,,0.90.0,,IPC/RPC,,,,,0,,,"This is HADOOP-6723, see that issue for details.",,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/10 05:50;tlipcon;hbase-2487.txt;https://issues.apache.org/jira/secure/attachment/12442817/hbase-2487.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26329,Reviewed,,,,Fri Nov 20 12:40:56 UTC 2015,,,,,,,,,,"0|i0hhxj:",100167,,,,,,,,,,,,,,,,,,,,,"27/Apr/10 00:37;stack;Applied branch and trunk.  Thanks for the patch Todd.;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Persist Master in-memory state so on restart or failover, new instance can pick up where the old left off",HBASE-2485,12462917,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,streamy,stack,stack,24/Apr/10 04:03,20/Nov/15 12:42,14/Jul/23 06:06,19/Oct/10 00:09,,,,,,,,,,,,0.90.0,,,,,,,0,moved_from_0_20_5,,"Today there was some good stuff up on IRC on how transitions won't always make it across Master failovers in multi-master deploy because transitions are kept in in-memory structure up in the Master and so on master crash, the new master will be missing state  on startup (Todd was main promulgator of this observation and of the opinion that while  master rewrite is scheduled for 0.21, some part needs to be done for 0.20.5).  A few suggestions were made: transitions should be file-backed somehow, etc.  Let this issue be about the subset we want to do for 0.20.5.

Of the in-memory state queues, there is at least the master tasks queue -- process region opens, closes, regionserver crashes, etc. -- where tasks must be done in order and IIRC, tasks are fairly idempotent (at least in the server crash case, its multi-step and we'll put the crash event back on the queue if we cannot do all steps in the one go).  Perhaps this queue could be done using the new queue facility in zk 3.3.0 (I haven't looked to check if possible, just suggesting).  Another suggestion was a file to which we'd append queue items, requeueing, and marking the file with task complete, etc.  On Master restart or fail-over, we'd replay the queue log.

There is also the Map of regions-in-transition.  Yesterday we learned that there is a bug where server shutdown processing does not iterate the Map of regions-in-transition.  This Map may hold regions that are in ""opening"" or ""opened"" state but haven't yet had the fact added to .META. by master.  Meantime the hosting server can crash.  Regions that were opening will stay in the regions-in-transition and those in opened-but-not-yet-added-to-meta will go ahead and add a crashed server to .META. (Currently regions-in-transition does not record server the region opening/open is happening on so it doesn't have enough info to be processed as part of server shutdown).

Regions-in-transition also needs to be persistant.  On startup, regions-in-transition can get kinda hectic on a big cluster.  Ordering is not so important here I believe.  A directory in zk might work (For 1M regions in a big cluster, that'd be about 2M creates and 2M deletes during startup -- thats too much?).  Or we could write a WAL-like log again of region  transitions (We'd have to develop a little vocabulary) that got reread by a new master.


",,anty,dhruba,eli,hammer,larsfrancke,streamy,tlipcon,tsuna,,,,,,,,,,,,,,,,,,,,,,,,HBASE-1742,,,,,,HBASE-2692,HBASE-934,,,,,HBASE-1750,HBASE-2405,,,,,,,,,,,,"06/May/10 21:01;karthik.ranga;HBase-State-Transitions.docx;https://issues.apache.org/jira/secure/attachment/12443907/HBase-State-Transitions.docx",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26328,,,,,Fri Nov 20 12:42:46 UTC 2015,,,,,,,,,,"0|i0hhxb:",100166,,,,,,,,,,,,,,,,,,,,,"06/May/10 21:01;karthik.ranga;Adding an initial design of how to handle region transitions.;;;","07/May/10 19:12;stack;Doc is great.  Here's a few comments.

+ I think you should start your proposal w/ some high-level intents: e.g. Only messages from Master to RS over RPC are of import, are ""commands""; messages from RS to Master are just informational (load, split) OR, intent is moving the intransitions out of Master to zk so intransiitions weathers a master restart.
+ Startup could be tricky.  Here we are hoisting all regions in .META. up into the unassigned in zk.  I was wondering about the case where the copy from .META. to zk/UNASSIGNED is only partially done say because master crashes.  What happens?  Maybe it'll be OK?  If the  meta startcode does not match that of a running regionserver, then the region has not yet been assigned so add it to zk/UNASSIGNED.
+ In Close Region RS Flow, did we agree closing is of no use?   There is nothing master can do really if closing is taking for ever?
+ Up in zk, unfortunately, znodes will have to be named using the regions encoded name.  Will make it a little tough following region flow.  Perhaps the fix is to make encoded name of a region more prevalent in logs.
+ We said opening was nice to have rather than necessary?
+ I wonder if you need a new message from Master to RS where you can ask the RS what regions it has deployed? Be best if we didn't need it.  We shouldn't need it I suppose.;;;","12/May/10 00:27;karthik.ranga;Hey Stack,

Excellent feedback, thanks!

1. Will add a modified doc soon, absolutely agree with you comment:
""intent is moving the intransitions out of Master to zk so intransiitions weathers a master restart.""
2. wrt Master restarts: jgray and I were discussing, it will be a scheme similar to zk/UNASSIGNED, but in a different location. And a RS will be handed a bunch of regions using one zk node update, and will have to ack the bulk open in one zk node update. Will fill in once the details are clearer, but it will not follow the exact same scheme.
3. Yes, closing is of no use.
4. Agreed
5. Yes, opening is a nice to have. I am taking the following approach: let the RS report opening progress, but master will ignore them for the first cut.
6. No, I don't think that would be needed in the current scheme. The RS would just update the state of the region to ""OPENED"" and master can infer from there.

We have already started coding some parts, will update once there is more progress...;;;","12/May/10 23:48;stack;Bulk move of 0.20.5 issues into 0.21.0 after vote that we merge branch into TRUNK up on list.;;;","01/Sep/10 05:46;streamy;Implementation of this moved into HBASE-2692.  We should add some tests before closing this perhaps.;;;","01/Sep/10 16:16;stack;Yes.  This needs a test and we need to add Karthik's document to the hbase book before we can close this issue I'd say.;;;","01/Sep/10 18:32;stack;When we write unit test to demonstrate this issue fixed, be sure to include coverage for the case described over inHBASE-1742 where an open comes in during master down and it causes loss of region onlining.;;;","05/Oct/10 21:24;streamy;Working on unit tests for this over the course of this week.;;;","19/Oct/10 00:09;streamy;Newly committed master failover unit tests all passing on hudson.  Resolving!;;;","19/Oct/10 04:51;stack;Just to say that the attached is good on how things used to work.  It also puts up a few simple axioms on how things are to be in the new master with listings of general transition flows.  The hbase 'book' has the committed versions of these flows.  I also took from the doc. description of how splits are now in new master.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some tests do not use ephemeral ports,HBASE-2483,12462841,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,streamy,tlipcon,tlipcon,23/Apr/10 05:12,20/Nov/15 12:43,14/Jul/23 06:06,19/May/10 20:57,0.90.0,,,,,,,,,,,0.90.0,,test,,,,,0,,,"For example, seems like most of the tests bind the master to port 60000. This doesn't work on shared build machines where multiple hbase test targets might run concurrently.",,hammer,kannanm,larsfrancke,streamy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/May/10 20:50;streamy;HBASE-2483-v1.patch;https://issues.apache.org/jira/secure/attachment/12444979/HBASE-2483-v1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26327,,,,,Fri Nov 20 12:43:27 UTC 2015,,,,,,,,,,"0|i0hhwv:",100164,,,,,,,,,,,,,,,,,,,,,"17/May/10 18:28;streamy;This might fix broken hudson on trunk;;;","19/May/10 20:41;jdcryans;It's broken by the new master testing stuff. MiniHBaseCluster.init expects to catch a BindException but now it's getting a RuntimeException with the bind being a cause.;;;","19/May/10 20:50;streamy;Sets master port to 0.  Tests seem to still pass.;;;","19/May/10 20:53;jdcryans;+1, makes my multi-cluster replication test happy.;;;","19/May/10 20:57;streamy;Committed to trunk.  Hopefully this fixes hudson.;;;","06/Jun/10 01:41;tlipcon;Committed this to 0.20 branch as well (Stack +1ed offline);;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
regions in transition do not get reassigned by master when RS crashes,HBASE-2482,12462835,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,tlipcon,tlipcon,tlipcon,23/Apr/10 00:38,12/Oct/12 06:15,14/Jul/23 06:06,03/May/10 19:54,0.20.5,,,,,,,,,,,0.20.5,0.90.0,master,,,,,0,,,"Very similar to HBASE-1928, but for the general case (not just ROOT/META):

If a region is in transition on a RS when the RS crashes, the master does not remove it from regionsInTransition when processing the RS shutdown. This is fairly easy to trigger by bringing up a RS and kill -9ing it just as it starts to get regions assigned. Those regions will get permanently lost since they're stuck in regionsInTransition and thus don't get assigned by the metascanner.",,hammer,kannanm,tatsuya6502,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/10 02:17;stack;2482-unittest.txt;https://issues.apache.org/jira/secure/attachment/12443434/2482-unittest.txt","03/May/10 19:24;stack;2482-v3-combined-patch.txt;https://issues.apache.org/jira/secure/attachment/12443488/2482-v3-combined-patch.txt","26/Apr/10 06:00;tlipcon;hbase-2482.txt;https://issues.apache.org/jira/secure/attachment/12442820/hbase-2482.txt",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26326,Reviewed,,,,Mon May 17 16:50:22 UTC 2010,,,,,,,,,,"0|i08sk7:",49227,,,,,,,,,,,,,,,,,,,,,"23/Apr/10 00:43;tlipcon;You also can't disable the table, since it won't offline the regions it thinks are still pending open.;;;","23/Apr/10 07:20;tlipcon;Assigning this to myself... will probably be a large patch and will require much testing, so should not block 0.20.4.;;;","23/Apr/10 15:18;stack;Making a blocker.  Drops a region.;;;","23/Apr/10 15:19;stack;Bringing into 0.20.5 and 0.21 so we don't lose track of it (can move it out later if needed).;;;","23/Apr/10 18:58;tlipcon;I think my plan is this: first do a ""bandaid"" fix where we just scan regionsInTransition when processing shutdown messages, trying to do the right thing for each. After that bandaids the problem RegionManager needs a pretty widespread cleanup to collapse the code that does controlled shutdown vs lost zknode shutdown, etc.;;;","26/Apr/10 06:00;tlipcon;Here's a preliminary patch that has gone through cluster testing, but could probably still be improved a bit (needs unit tests, and I think a lot of the meta/root special casing could be removed now)

Also please note the ""I DONT BELIEVE YOU WILL EVER SEE THIS"" message. If someone can produce this message I will give them a cookie! Otherwise we should remove this code.

I've been testing this for the last 9 hours or so with a RS failure once every minute and no regions have ""fallen out"" of the cluster. Before this fix, it used to ""lose"" a region after an hour or two.;;;","26/Apr/10 21:10;stack;Interesting bringing the split lock local to master.  Makes sense (especially since not used in RegionManager at all!)

For this:

{code}
+    // TODO Why do we do this now instead of at processing time?
     closeMetaRegions();
{code}

... looking at it, I can only think that removing -ROOT- and .META. from online status holds up other status transitions.  This change was added by hbase-1457.

Let me see if I can make a test for this.



;;;","03/May/10 02:17;stack;First cut at unit test.  Needs an edit but looks to be working.  Adds a protected 'killl' to regionserver which simulates RS kill (does no cleanup but shutdown of socket).  Also added new HMsg called TEST_MSG_BLOCK_RS.  When RS receives this from master, it just waits until closed, aborted or killed.  It blocks the worker queue.

The way the test works is that it adds a new RS to small cluster, waits on load balancer to move some regions to new server.  As soon as some are open, we send a close of them all followed by a TEST_MSG_BLOCK_RS.  The closes go through, balancer assigns the new server some of the closed regions only the TEST_MSG_BLOCK_RS is in place.

Let me make a patch that includes Todds patch and cleaned up test next.;;;","03/May/10 19:24;stack;A patch that combines Todds fix and a unit test.  Todds fix changes RegionState significantly but its for the better and it looks like it works.  Best to get this change in now rather than later.

This version does not include the kill method added to the regionserver by previous version. Its not needed.;;;","03/May/10 19:54;stack;Applied branch and trunk.  Thanks for the patch Todd.;;;","12/May/10 23:54;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","17/May/10 16:50;stack;I applied this to 0.20 branch so it will be included by 0.20.5.  Patches going into branch need review.   In this case, I reviewed Todd's patch.   Hopefully thats ok.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Client is not getting UnknownScannerExceptions; they are being eaten",HBASE-2481,12462834,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,stack,stack,23/Apr/10 00:26,22/Jul/16 06:46,14/Jul/23 06:06,24/Apr/10 17:16,0.20.4,,,,,,,,,,,0.20.4,0.90.0,,,,,,0,,,"This was reported by mudphone on IRC and confirmed by myself in quick test.  If the client takes too long going back to the RS, the RS will throw an UnknownScannerException but it doesn't get back to the client.  Instead, the client scan silently ends.  Marking this blocker.  Its actually in 0.20.4.  Thats what I was testing.  Mayhaps an RC sinker?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-16266,,,,,,,,,,,,,,,,,"24/Apr/10 00:35;jdcryans;HBASE-2481.patch;https://issues.apache.org/jira/secure/attachment/12442722/HBASE-2481.patch","27/May/10 20:27;stack;scannertimeouttest.txt;https://issues.apache.org/jira/secure/attachment/12445698/scannertimeouttest.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26325,Reviewed,,,,Thu Jan 27 11:45:31 UTC 2011,,,,,,,,,,"0|i08son:",49247,,,,,,,,,,,,,,,,,,,,,"23/Apr/10 00:30;tlipcon;Yes, I think this should block release.;;;","23/Apr/10 01:52;apurtell;+1 to sink release

Good catch.;;;","23/Apr/10 21:37;stack;Here is script to demo problem.  It scans 99 rows in a PE table then pauses for 60 seconds.  When it resumes, there is nothing else to fetch (though table has millions of rows) and over on serverside is a USE:

{code}
$ more bin/slow_scan.rb 
# To run: ./bin/hbase org.jruby.Main bin/slow_scan.rb
include Java
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.HConstants
import org.apache.hadoop.hbase.client.HTable
import org.apache.hadoop.hbase.client.Scan
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.util.FSUtils
import org.apache.hadoop.fs.FileSystem
import java.lang.Thread

# Get configuration to use.
c = HBaseConfiguration.new()

# Set hadoop filesystem configuration using the hbase.rootdir.
# Otherwise, we'll always use localhost though the hbase.rootdir
# might be pointing at hdfs location.
c.set(""fs.default.name"", c.get(HConstants::HBASE_DIR))
fs = FileSystem.get(c)

# Clean mentions of table from .META.
# Scan the .META. and remove all lines that begin with tablename
t = HTable.new(c, ""TestTable"")
scan = Scan.new()
scanner = t.getScanner(scan)
count = 0
while (result = scanner.next())
  rowid = Bytes.toString(result.getRow())
  puts rowid
  count = count + 1 
  # If count == 100, pause for 60 seconds.
  if count == 100 
    Thread.sleep(1000 * 60)
  end
end
scanner.close()
{code};;;","23/Apr/10 23:27;jdcryans;This was caused by HBASE-1671, this changed in ScannerCallable: 

{code} 
   public Result [] call() throws IOException { 
     if (scannerId != -1L && closed) { 
- server.close(scannerId); 
- scannerId = -1L; 
+ close(); 
     } else if (scannerId == -1L && !closed) { 
- // open the scanner 
- scannerId = openScanner(); 
+ this.scannerId = openScanner(); 
     } else { 
- Result [] rrs = server.next(scannerId, caching); 
+ Result [] rrs = null; 
+ try { 
+ rrs = server.next(scannerId, caching); 
+ } catch (IOException e) { 
+ IOException ioe = null; 
+ if (e instanceof RemoteException) { 
+ ioe = RemoteExceptionHandler.decodeRemoteException((RemoteException)e); 
+ } 
+ if (ioe != null && ioe instanceof NotServingRegionException) { 
+ // Throw a DNRE so that we break out of cycle of calling NSRE 
+ // when what we need is to open scanner against new location. 
+ // Attach NSRE to signal client that it needs to resetup scanner. 
+ throw new DoNotRetryIOException(""Reset scanner"", ioe); 
+ } 
+ } 
       return rrs == null || rrs.length == 0? null: rrs; 
     } 
      
{code} 

We now eat the exception if it's not NSRE, throwing it if the exception is a DoNotRetryIOException is the right thing to do, but the client code is still broken. In HTable.ClientScanner.next: 

{code} 
try { 
            // Server returns a null values if scanning is to stop. Else, 
            // returns an empty array if scanning is to go on and we've just 
            // exhausted current region. 
            values = getConnection().getRegionServerWithRetries(callable); 
            if (skipFirst) { 
              skipFirst = false; 
              // Reget. 
              values = getConnection().getRegionServerWithRetries(callable); 
            } 
          } catch (DoNotRetryIOException e) { 
            Throwable cause = e.getCause(); 
            if (cause == null || !(cause instanceof NotServingRegionException)) { 
              throw e; 
            } 
            // Else, its signal from depths of ScannerCallable that we got an 
            // NSRE on a next and that we need to reset the scanner. 
            if (this.lastResult != null) { 
              this.scan.setStartRow(this.lastResult.getRow()); 
              // Skip first row returned. We already let it out on previous 
              // invocation. 
              skipFirst = true; 
            } 
            // Clear region 
            this.currentRegion = null; 
            continue; 
          } catch (IOException e) { 
            if (e instanceof UnknownScannerException && 
                lastNext + scannerTimeout < System.currentTimeMillis()) { 
              ScannerTimeoutException ex = new ScannerTimeoutException(); 
              ex.initCause(e); 
              throw ex; 
            } 
            throw e; 
          } 
{code} 

We catch the DoNotRetryIOException first and in the other catch clause we check for UnknownScannerException, which extends DoNotRetryIOException... so ScannerTimeoutException is never used! Easy fix.;;;","24/Apr/10 00:35;jdcryans;Patch that adds test, does the right catching and adds a message to the scanner timeout exception. I'm still not satisfied by how we handle exceptions around that part of the code.;;;","24/Apr/10 04:34;stack;Patch looks good enough for 0.20.4.  Let me see if I can get mudphone to test.;;;","24/Apr/10 17:00;stack;I tried the patch and now we seem to do the right thing:

{code}
0000000097
0000000098
0000000099




2010-04-24T09:58:57.642-0700: 62.527: [GC 62.527: [ParNew: 17024K->1870K(19136K), 0.0045580 secs] 21509K->6356K(83008K), 0.0045980 secs] [Times: user=0.03 sys=0.00, real=0.00 sec
org/apache/hadoop/hbase/client/HTable.java:1991:in `next': org.apache.hadoop.hbase.client.ScannerTimeoutException: 60007ms passed since the last invocation, timeout is currently )
        from sun.reflect.GeneratedMethodAccessor2:-1:in `invoke'
        from sun/reflect/DelegatingMethodAccessorImpl.java:25:in `invoke'
        from java/lang/reflect/Method.java:597:in `invoke'
        from org/jruby/javasupport/JavaMethod.java:298:in `invokeWithExceptionHandling'
        from org/jruby/javasupport/JavaMethod.java:259:in `invoke'
        from org/jruby/java/invokers/InstanceMethodInvoker.java:36:in `call'
        from org/jruby/runtime/callsite/CachingCallSite.java:70:in `call'
        from bin/slow_scan.rb:27:in `__file__'
        from bin/slow_scan.rb:-1:in `load'
        from org/jruby/Ruby.java:577:in `runScript'
        from org/jruby/Ruby.java:480:in `runNormally'
        from org/jruby/Ruby.java:354:in `runFromMain'
        from org/jruby/Main.java:229:in `run'
        from org/jruby/Main.java:110:in `run'
        from org/jruby/Main.java:94:in `main'
Complete Java stackTrace
org.apache.hadoop.hbase.client.ScannerTimeoutException: 60007ms passed since the last invocation, timeout is currently set to 60000
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.next(HTable.java:1991)
        at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.jruby.javasupport.JavaMethod.invokeWithExceptionHandling(JavaMethod.java:298)
        at org.jruby.javasupport.JavaMethod.invoke(JavaMethod.java:259)
        at org.jruby.java.invokers.InstanceMethodInvoker.call(InstanceMethodInvoker.java:36)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:70)
        at bin.slow_scan.__file__(slow_scan.rb:27)
        at bin.slow_scan.load(slow_scan.rb)
        at org.jruby.Ruby.runScript(Ruby.java:577)
        at org.jruby.Ruby.runNormally(Ruby.java:480)
        at org.jruby.Ruby.runFromMain(Ruby.java:354)
        at org.jruby.Main.run(Main.java:229)
        at org.jruby.Main.run(Main.java:110)
        at org.jruby.Main.main(Main.java:94)
Caused by: org.apache.hadoop.hbase.UnknownScannerException: org.apache.hadoop.hbase.UnknownScannerException: Name: 3092528963968774042
        at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1889)
        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
...
{code};;;","24/Apr/10 17:16;stack;Committed to two branches and trunk.;;;","27/May/10 20:27;stack;Here is the unit test done for 0.20.   It passes.  Added check of iterating a Scan too.;;;","27/Jan/11 11:45;hudson;Integrated in HBase-TRUNK #1721 (See [https://hudson.apache.org/hudson/job/HBase-TRUNK/1721/])
    HBASE-2481  max seq id in flushed file can be larger than its correct value causing data loss during recovery
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HLog sequence number is obtained outside updateLock,HBASE-2476,12462605,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,tlipcon,tlipcon,tlipcon,21/Apr/10 02:36,20/Nov/15 12:43,14/Jul/23 06:06,27/Apr/10 04:06,0.20.5,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"In one of the overloads of HLog.append, obtainSeqNum() is called before this.updateLock is acquired. This means that it's possible for some other thread to have a higher sequence number but achieve its write first, and hence the lastSeqWritten map can become incorrect.",,kannanm,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/10 05:10;tlipcon;hbase-2476.txt;https://issues.apache.org/jira/secure/attachment/12442815/hbase-2476.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26322,Reviewed,,,,Fri Nov 20 12:43:13 UTC 2015,,,,,,,,,,"0|i0hhvz:",100160,,,,,,,,,,,,,,,,,,,,,"26/Apr/10 05:10;tlipcon;Trivial fix;;;","27/Apr/10 04:06;stack;Committed to branch (HBASE-1393 fixed this on TRUNK).  Thanks for the patch Todd.;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[stargate] Required ordering of JSON name/value pairs when performing Insert/Update,HBASE-2475,12462586,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,apurtell,apurtell,20/Apr/10 22:08,11/Jun/22 23:32,14/Jul/23 06:06,11/May/14 11:05,,,,,,,,,,,,,,,,,,,0,moved_from_0_20_5,,"From Tyler Coffin up on hbase-user@

{quote}
I am using the Stargate REST interface to HBase for inserting data. When using JSON to transmit the query content, I have found that specific ordering of key/value pairs within the JSON string is required in order for the query to succeed (otherwise a response of 'HTTP/1.1 500 Row key is invalid' error is thrown if ""key"" and ""Cell"" are reversed).

Example:
This string receives the above error:

{noformat}
{""Row"":[{""Cell"":[{""column"":""bWVzc2FnZTptc2c="",""$"":""Zm9vYmFy""}],""key"":""MTIzNAo=""}]}
{noformat}

This is the valid equivalent string:

{noformat}
{""Row"":[{""key"":""MTIzNAo="",""Cell"":[{""column"":""bWVzc2FnZTptc2c="",""$"":""Zm9vYmFy""}]}]}
{noformat}

As you can see the only difference between these two instances is that the ""key"" and ""Cell"" name/value pairs have their order reversed.

In the equivalent XML notation, the ordering is specifically required per the schema. However with JSON Objects (i.e. name/value pairs) order is not required (JSON Arrays are ordered, but not Objects). Some JSON libraries will preserve ordering of Objects but not all  which is how I discovered this problem in the first place because I was using the Perl JSON library which does not guarantee order). 
I'm unsure if this is a bug in the REST implementation or an inconvenient ambiguity in the JSON specification. Regardless I thought I'd share this discovery with the community for feedback (or at the very least to document this for users' future reference).

For reference this is the table schema for the above query:

{noformat}
{NAME => 'reftrack', FAMILIES => [{NAME => 'message', COMPRESSION =>
'NONE', VERSIONS => '1', TTL => '2147483647', BLOCKSIZE => '65536',
IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}
{noformat}
{quote}",,apurtell,avandana,jxiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2567,,,,,,,,,,,,,,,,,,,"03/Jun/12 21:51;apurtell;HBASE-2475.patch;https://issues.apache.org/jira/secure/attachment/12530709/HBASE-2475.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26321,,,,,Sun May 11 11:05:13 UTC 2014,,,,,,,,,,"0|i02d2f:",11718,,,,,,,,,,,,,,,,,,,,,"20/Apr/10 22:10;apurtell;Thanks for the report, very helpful.

bq. In the equivalent XML notation, the ordering is specifically required per the schema. 

... and Jersey adds a marshaller and unmarshaller to the JAXB framework to produce JSON. This is an artifact of jersey-json or something dumb we did when hooking up JAXB.  I'll write up some unit tests and look at this soon.;;;","12/May/10 23:48;stack;Bulk move of 0.20.5 issues into 0.21.0 after vote that we merge branch into TRUNK up on list.;;;","19/May/10 07:08;apurtell;Removed ordering directives on some bindings as part of HBASE-2542 and HBASE-2567.;;;","03/Jun/12 10:29;apurtell;Some JSON field ordering issues remain according to reports. These may be an artifact of how JSON is hooked up to JAXB. The weird '$' special field for specifying a value is due to how the bindings work, for example. Reopening for more investigation and at least a documentation update. ;;;","03/Jun/12 21:07;apurtell;If the ""$"" value field does not come last in a CellModel, Jersey won't deserialize the model correctly. We can

1. Document this clearly and also add a troubleshooting section entry.

2. As an alternative, elsewhere we use Jackson as a JSON serializer/deserializer. Jackson seems widely considered to be better behaved. JacksonJsonProvider in jackson-jaxrs is a drop in replacement for Jersey's JSON binding for JAX-RS. However, the JSON representation of HBase REST may change as a consequence. This would depend on how configurable JacksonJsonProvider is, and if all backwards compatible behaviors can be specified. Digging around Jackson javadoc for a while was inconclusive. ;;;","03/Jun/12 21:51;apurtell;For option 2 I tried the quick hack attached and:

{noformat}
2012-06-03 23:43:16,085 WARN  [1224001988@qtp-1382067420-0] log.Slf4jLog(76): /users/TheRealMT/info:password: org.codehaus.jackson.map.exc.UnrecognizedPropertyException: Unrecognized field ""Row"" (Class org.apache.hadoop.hbase.rest.model.CellSetModel), not marked as ignorable
 at [Source: org.mortbay.jetty.HttpParser$Input@264d107e; line: 1, column: 9] (through reference chain: org.apache.hadoop.hbase.rest.model.CellSetModel[""Row""])
2012-06-03 23:43:16,097 DEBUG [main] client.Client(148): PUT http://localhost:46871/users/TheRealMT/info:password 500 Unrecognized field ""Row"" (Class org.apache.hadoop.hbase.rest.model.CellSetModel), not marked as ignorable  at [Source: org.mortbay.jetty.HttpParser$Input@264d107e; line: 1, column: 9] (through reference chain: org.apache.hadoop.hbase.rest.model.CellSetModel[""Row""]) in 1520 ms
{noformat}

So we can certainly pursue an alternate implementation with Jackson but the JSON representation and all related documentation will change.

I will note that the error messages provided by Jackson are much better than Jersey in contrast silently accepting input it thinks is broken.;;;","11/May/14 11:05;apurtell;Resolved by new JSON serializer in newer versions;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in 2248 - mixed version reads (not allowed by spec),HBASE-2474,12462576,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,,ryanobjc,ryanobjc,20/Apr/10 20:42,20/Nov/15 12:42,14/Jul/23 06:06,21/Apr/10 19:34,0.20.4,0.20.5,0.90.0,,,,,,,,,0.90.0,,,,,,,0,,,"While doing a concurrent read/write test, the reader eventually gets a situation where the first column in the result set has the wrong 'value' than the rest of the result set (of 50 columns or so).  The test (included) does puts of 50 columns with all the same (Random) value. The reader validates that all values are equal, and fails.",,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2616,HBASE-2670,,,,,,,,,,,,,,,,"20/Apr/10 23:55;ryanobjc;HBASE-2474.txt;https://issues.apache.org/jira/secure/attachment/12442367/HBASE-2474.txt","20/Apr/10 20:45;ryanobjc;TestAcidGuarantees.java;https://issues.apache.org/jira/secure/attachment/12442346/TestAcidGuarantees.java",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26320,Reviewed,,,,Fri Nov 20 12:42:06 UTC 2015,,,,,,,,,,"0|i0hhvr:",100159,,,,,,,,,,,,,,,,,,,,,"20/Apr/10 20:45;ryanobjc;from Todd Lipcon;;;","21/Apr/10 00:25;tlipcon;lgtm, +1;;;","21/Apr/10 18:25;tlipcon;looks like this is committed, shall we resolve? I'll contribute the test case along with some others I'm working on in a different JIRA.;;;","21/Apr/10 19:34;stack;Thanks Todd.  Resolved.;;;","12/May/10 23:53;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","17/May/10 22:28;ryanobjc;committed to trunk;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Splitting logs, we'll make an output file though the region no longer exists",HBASE-2471,12462564,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,stack,stack,20/Apr/10 17:56,20/Nov/15 12:41,14/Jul/23 06:06,03/Nov/10 23:45,,,,,,,,,,,,0.90.0,,,,,,,0,moved_from_0_20_5,,"The ""human unit tester"" (Kannan) last night wondered what happens splitting logs and we come across an edit whose region has since been removed.  Taking a look, it looks like we'll create the output file and write the edits for the no-longer-extant region anyways.  This will leave litter in the filesystem -- region split files that will never be used nor removed.  This issue is about verifying that indeed this is whats happening (We do SequenceFile.createWriter with the overwrite flag set to true which tracing seems to mean create all intermediary directories -- to be verified) and if it indeed is happening, fixing split so unless the region dir exists, don't write out edits.. just drop them.",,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/10 23:34;jdcryans;HBASE-2471-v2.patch;https://issues.apache.org/jira/secure/attachment/12458775/HBASE-2471-v2.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26319,Reviewed,,,,Fri Nov 20 12:41:55 UTC 2015,,,,,,,,,,"0|i0hhv3:",100156,,,,,,,,,,,,,,,,,,,,,"20/Apr/10 18:06;tlipcon;How would the region get removed between the RS failure and the HM splitting the logs? Don't the regions need to be assigned in order for the table to be dropped or a split to happen?;;;","20/Apr/10 18:17;stack;HLog files contain edits for all regions on a particular RegionServer.  HLogs get GC'd only after all edits in a file have been persisted.   HLogs may stick around a good while if any one of a RS's Regions is a laggard flushing.  While the HLog lives, some other Region X may have split and been removed, and then the RS crashed.  On replay, we'll trip over Region X's edits though Region X had been removed.;;;","20/Apr/10 18:28;tlipcon;Good point, I was thinking that pre-split the region would be flushed, and thus the logs would be expired. But the logs could have records for other unflushed regions.;;;","12/May/10 23:48;stack;Bulk move of 0.20.5 issues into 0.21.0 after vote that we merge branch into TRUNK up on list.;;;","05/Oct/10 22:35;stack;Should be easy fix.  From j-d ""If regiondir does not exist, do not create a recovered.edits file... skip""... In split logs need to keep a list of non-existent regions tooo...  so we don't check fs for each encountered edit.;;;","03/Nov/10 23:34;jdcryans;Patch that I'm about to commit. It's different from what I posted on RB because some other unit tests needed to be changed and only figured it out when running the full test suite.;;;","03/Nov/10 23:45;jdcryans;Committed patch to trunk. It expect some sort of destabilization as some tests looked flaky when I ran the full suite, so I might have to fix more tests in the near future.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Various Bytes.* functions silently ignore invalid arguments,HBASE-2463,12462306,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,tsuna,tsuna,tsuna,17/Apr/10 08:55,20/Nov/15 12:42,14/Jul/23 06:06,18/Apr/10 05:51,,,,,,,,,,,,0.90.0,,util,,,,,0,,,"Many functions in {{hbase.util.Bytes}} silently ignore invalid arguments.
For instance, {{Bytes.toInt(null)}} deliberately returns {{-1}}.  There are tons of cases like that.
* All functions that are given a null pointer should throw an NPE.
* All functions that are given otherwise invalid arguments should throw an {{IllegalArgumentException}}.

Anyone relying on ""special return values"" such as {{-1}} for {{Bytes.toInt(null)}} is guilty of writing broken code ({{-1}} is a valid return value for {{toInt}}!).",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/10 22:12;tsuna;ASF.LICENSE.NOT.GRANTED--0001-Don-t-silently-ignore-invalid-arguments-and-properly.patch;https://issues.apache.org/jira/secure/attachment/12442059/ASF.LICENSE.NOT.GRANTED--0001-Don-t-silently-ignore-invalid-arguments-and-properly.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26317,Reviewed,,,,Fri Nov 20 12:42:05 UTC 2015,,,,,,,,,,"0|i0hhtj:",100149,,,,,,,,,,,,,,,,,,,,,"17/Apr/10 09:00;ryanobjc;generally i would avoid putting in explicit NPE checks if they were going to fail 1 or 2 lines later. moar code is badddd
;;;","17/Apr/10 09:53;tsuna;Agreed.  My plan was to remove the explicit check for {{null}} and let the JVM throw the NPE automagically.;;;","17/Apr/10 22:12;tsuna;Patch that fixes the issue.;;;","18/Apr/10 05:51;stack;Committed to TRUNK. Thanks for the patch Benoît;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split doesn't handle IOExceptions when creating new region reference files,HBASE-2461,12462302,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,tlipcon,tlipcon,17/Apr/10 02:39,20/Nov/15 12:43,14/Jul/23 06:06,04/Aug/10 00:02,,,,,,,,,,,,0.90.0,,regionserver,,,,,0,moved_from_0_20_5,,"I was testing an HDFS patch which had a bug in it, so it happened to throw an NPE during a split with the following trace:

2010-04-16 19:18:20,727 ERROR org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction failed for region TestTable,-1945465867<1271449232310>,1271453785648
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.enqueueCurrentPacket(DFSClient.java:3124)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.flushInternal(DFSClient.java:3220)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:3306)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3255)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86)
        at org.apache.hadoop.fs.FileSystem.createNewFile(FileSystem.java:560)
        at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:95)
        at org.apache.hadoop.hbase.io.Reference.write(Reference.java:129)
        at org.apache.hadoop.hbase.regionserver.StoreFile.split(StoreFile.java:498)
        at org.apache.hadoop.hbase.regionserver.HRegion.splitRegion(HRegion.java:682)
        at org.apache.hadoop.hbase.regionserver.CompactSplitThread.split(CompactSplitThread.java:162)
        at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:95)

After that, my region was gone, any further writes to it would fail.",,larsfrancke,streamy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2001,,,,,,,,,,,,,"04/Aug/10 00:02;stack;2461-v10.txt;https://issues.apache.org/jira/secure/attachment/12451178/2461-v10.txt","28/Jul/10 23:04;stack;2461-v2.txt;https://issues.apache.org/jira/secure/attachment/12450767/2461-v2.txt","30/Jul/10 14:20;stack;2461-v3.txt;https://issues.apache.org/jira/secure/attachment/12450906/2461-v3.txt","31/Jul/10 06:46;stack;2461-v4.txt;https://issues.apache.org/jira/secure/attachment/12450957/2461-v4.txt","02/Aug/10 00:00;stack;2461-v6.txt;https://issues.apache.org/jira/secure/attachment/12451000/2461-v6.txt","03/Aug/10 01:24;stack;2461-v7.txt;https://issues.apache.org/jira/secure/attachment/12451097/2461-v7.txt","03/Aug/10 20:25;stack;2461-v8.txt;https://issues.apache.org/jira/secure/attachment/12451153/2461-v8.txt","17/Jun/10 06:04;stack;2461.txt;https://issues.apache.org/jira/secure/attachment/12447313/2461.txt","03/Aug/10 04:09;stack;ugly_but_might_work.txt;https://issues.apache.org/jira/secure/attachment/12451102/ugly_but_might_work.txt",,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26316,Reviewed,,,,Fri Nov 20 12:43:40 UTC 2015,,,,,,,,,,"0|i0hht3:",100147,,,,,,,,,,,,,,,,,,,,,"17/Apr/10 02:44;stack;Marking as a blocker and moving into 0.20.5 and 0.21.;;;","17/Apr/10 17:11;apurtell;I think this, and all related tightening up what we do when IOE from FS, should be a subtask of HBASE-1964. At the Hackathon we'd like to start chasing these down with a modified DFSClient that we can inject faults, either random with adjustable probabilities or always on particular code paths. ;;;","12/May/10 23:48;stack;Bulk move of 0.20.5 issues into 0.21.0 after vote that we merge branch into TRUNK up on list.;;;","16/Jun/10 23:51;stack;Inside in split, at a particular point, we close the parent region.  Thereafter, we start in splitting the parents files.  If an exception, we'll come up out of the split method but the parent is not reopened.  Parent should be set read-only or something and only closed after daughters are created and registered in META.  Assigning myself.;;;","17/Jun/10 06:04;stack;This issue highlights how exceptions post close of the region-to-be-split -- a necessary action if the split is to come out clean -- can poke a hole in an online table.

This patch starts down a road of treating the split operation inside in the regionserver as a 'transaction'.  There is a prepare step and an execute step.  Should the execute fail -- execute step has stuff like close of region, update of meta table with new split codes -- then we'll call rollback.  The rollback will try and fixup the failed split by doing things like reopening region if appropriate and fixing up meta if necessary.

If the rollback fails, we'll kill the regionserver so that the processing of the server shutdown gets the effected regions back on line again.

Patch is not ready yet.;;;","28/Jul/10 23:04;stack;v2 -- still not done.. going to be fun testing this.;;;","30/Jul/10 14:20;stack;Taking a different approach.... not done yet.;;;","31/Jul/10 06:46;stack;Implementation done.  Tests next.;;;","02/Aug/10 00:00;stack;This seems to work.  Needs more tests first though before I post it to review board.;;;","03/Aug/10 01:24;stack;Posting this to review board.

Patch that keeps a journal during split transaction.  If split fails, call to rollback will restore the parent to original open condition by backing up whatever transaction steps completed.

The transaction spans split checks, closing of parent region and creation of daughters up to the addition of parent offlining to .META.  Once the .META. edit has been made, we cannot rollback -- we have to go forward.  This means that the basescanner fixup that will add missing daughter regions should the regionserver crash after parent region edit but before its added daughters is still required, in some form at least.

This patch includes a test of the new split code but only run against an HRegion, not in server context.  The split code is buried in heart of the regionserver and created on startup.  I stared at it for a while and injecting fault was just forbidding.  Its like bramble; there are so many spikes in the way of getting your finger down into the running split I ended up passing on it.

+ M src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java
(split): Most of the split code has been moved out to the new SplitTransaction class.
Now this method prepares the split transaction, executes, and if failure does rollback.

+ M src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
(splitLock) Removed. Doesn't seem necessary.  Just  made close method synchronized.
(SPLITDIR) Moved to new SplitTransaction
Moved cleanup of half-done splits into SplitTransaction.  It'll know better how to do this.
Moved split code into SplitTransaction class.

+ M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
Made this class implement new OnlineRegions interface

+ A src/main/java/org/apache/hadoop/hbase/regionserver/OnlineRegions.java
New Interface that allows you add/remove regions from oline regions.  This Interface
adds little.  Was just trying to make it so I didn't have to have server context doing
tests but in the end I just passed null for the case of no server context.  Could remove
this.

+ A src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
New class that encapsulates all to do w/ splitting ""transaction"".

+ A src/main/java/org/apache/hadoop/hbase/util/PairOfSameType.java
Minor utility class

+M src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
(loadRegion) Added loading a region

+ M src/test/java/org/apache/hadoop/hbase/io/TestImmutableBytesWritable.java
(testSpecificCompare) Unrelated change

+ M src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
Change because of new manner in which splits are run.  Added a splitRegions method.

+ A src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java
Test of region splitting code in region context.  Testing in server context would take
a bunch of work making it so could insert mock instance of SplitTransaction.






;;;","03/Aug/10 01:35;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/474/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

Posting this to review board.

Patch that keeps a journal during split transaction.  If split fails, call to rollback will restore the parent to original open condition by backing up whatever transaction steps completed.

The transaction spans split checks, closing of parent region and creation of daughters up to the addition of parent offlining to .META.  Once the .META. edit has been made, we cannot rollback -- we have to go forward.  This means that the basescanner fixup that will add missing daughter regions should the regionserver crash after parent region edit but before its added daughters is still required, in some form at least.

This patch includes a test of the new split code but only run against an HRegion, not in server context.  The split code is buried in heart of the regionserver and created on startup.  I stared at it for a while and injecting fault was just forbidding.  Its like bramble; there are so many spikes in the way of getting your finger down into the running split I ended up passing on it.

+ M src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java
(split): Most of the split code has been moved out to the new SplitTransaction class.
Now this method prepares the split transaction, executes, and if failure does rollback.

+ M src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
(splitLock) Removed. Doesn't seem necessary.  Just  made close method synchronized.
(SPLITDIR) Moved to new SplitTransaction
Moved cleanup of half-done splits into SplitTransaction.  It'll know better how to do this.
Moved split code into SplitTransaction class.

+ M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
Made this class implement new OnlineRegions interface

+ A src/main/java/org/apache/hadoop/hbase/regionserver/OnlineRegions.java
New Interface that allows you add/remove regions from oline regions.  This Interface
adds little.  Was just trying to make it so I didn't have to have server context doing
tests but in the end I just passed null for the case of no server context.  Could remove
this.

+ A src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
New class that encapsulates all to do w/ splitting ""transaction"".

+ A src/main/java/org/apache/hadoop/hbase/util/PairOfSameType.java
Minor utility class

+M src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
(loadRegion) Added loading a region

+ M src/test/java/org/apache/hadoop/hbase/io/TestImmutableBytesWritable.java
(testSpecificCompare) Unrelated change

+ M src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
Change because of new manner in which splits are run.  Added a splitRegions method.

+ A src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java
Test of region splitting code in region context.  Testing in server context would take
a bunch of work making it so could insert mock instance of SplitTransaction.


This addresses bug HBASE-2461.
    http://issues.apache.org/jira/browse/HBASE-2461


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java 7589db3 
  src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 6dc41a4 
  src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 6a54736 
  src/main/java/org/apache/hadoop/hbase/regionserver/OnlineRegions.java PRE-CREATION 
  src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java PRE-CREATION 
  src/main/java/org/apache/hadoop/hbase/util/PairOfSameType.java PRE-CREATION 
  src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 4d09fe9 
  src/test/java/org/apache/hadoop/hbase/io/TestImmutableBytesWritable.java 43fa6dd 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java 98bd3e5 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java PRE-CREATION 

Diff: http://review.cloudera.org/r/474/diff


Testing
-------

Basic unit tests are passing.


Thanks,

stack


;;;","03/Aug/10 04:09;stack;This might work for injecting a mocked SplitTransaction so I can test failed split in running cluster.  Would need to provide alternate HRS implementation but unit test utility is already setup to allow this.;;;","03/Aug/10 20:24;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/474/
-----------------------------------------------------------

(Updated 2010-08-03 13:10:44.610359)


Review request for hbase.


Changes
-------

Found minor issue where wasnt' returning if failed rollback.


Summary
-------

Posting this to review board.

Patch that keeps a journal during split transaction.  If split fails, call to rollback will restore the parent to original open condition by backing up whatever transaction steps completed.

The transaction spans split checks, closing of parent region and creation of daughters up to the addition of parent offlining to .META.  Once the .META. edit has been made, we cannot rollback -- we have to go forward.  This means that the basescanner fixup that will add missing daughter regions should the regionserver crash after parent region edit but before its added daughters is still required, in some form at least.

This patch includes a test of the new split code but only run against an HRegion, not in server context.  The split code is buried in heart of the regionserver and created on startup.  I stared at it for a while and injecting fault was just forbidding.  Its like bramble; there are so many spikes in the way of getting your finger down into the running split I ended up passing on it.

+ M src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java
(split): Most of the split code has been moved out to the new SplitTransaction class.
Now this method prepares the split transaction, executes, and if failure does rollback.

+ M src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
(splitLock) Removed. Doesn't seem necessary.  Just  made close method synchronized.
(SPLITDIR) Moved to new SplitTransaction
Moved cleanup of half-done splits into SplitTransaction.  It'll know better how to do this.
Moved split code into SplitTransaction class.

+ M src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
Made this class implement new OnlineRegions interface

+ A src/main/java/org/apache/hadoop/hbase/regionserver/OnlineRegions.java
New Interface that allows you add/remove regions from oline regions.  This Interface
adds little.  Was just trying to make it so I didn't have to have server context doing
tests but in the end I just passed null for the case of no server context.  Could remove
this.

+ A src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
New class that encapsulates all to do w/ splitting ""transaction"".

+ A src/main/java/org/apache/hadoop/hbase/util/PairOfSameType.java
Minor utility class

+M src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
(loadRegion) Added loading a region

+ M src/test/java/org/apache/hadoop/hbase/io/TestImmutableBytesWritable.java
(testSpecificCompare) Unrelated change

+ M src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
Change because of new manner in which splits are run.  Added a splitRegions method.

+ A src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java
Test of region splitting code in region context.  Testing in server context would take
a bunch of work making it so could insert mock instance of SplitTransaction.


This addresses bug HBASE-2461.
    http://issues.apache.org/jira/browse/HBASE-2461


Diffs (updated)
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java 7589db3 
  src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 6dc41a4 
  src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 6a54736 
  src/main/java/org/apache/hadoop/hbase/regionserver/OnlineRegions.java PRE-CREATION 
  src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java PRE-CREATION 
  src/main/java/org/apache/hadoop/hbase/util/PairOfSameType.java PRE-CREATION 
  src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 4d09fe9 
  src/test/java/org/apache/hadoop/hbase/io/TestImmutableBytesWritable.java 43fa6dd 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java 98bd3e5 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java PRE-CREATION 

Diff: http://review.cloudera.org/r/474/diff


Testing
-------

Basic unit tests are passing.


Thanks,

stack


;;;","03/Aug/10 20:25;stack;Includes minor fix where we weren't returning if failure doing rollback.

I looked at adding test failing a split on a running cluster but the contortions are too extreme -- CompactSplitThread is created in HRS#reinitialize() -- and then our split is run inside in CST#split so we'd need to put in place first our own CST and then, CST has to be subclassable or amenable to injection.  Currently it is not.  So, I could mess w/ making what we load for CST configurable but then what about the other threads, LogRoller, the Worker thread, etc., why not make them configurable while I'm at it.

...but then I shouldn't even be doing this.  There are containers that will stitch it all together for us and that can be easily changed at test time to run an alternative.  See http://www.picocontainer.org/ or spring

And the RS and Master are about to change w/ master rewrite.

I'm going to pass on trying to test split failure on running cluster till at least after master rewrite goes in.;;;","03/Aug/10 22:53;hbasereviewboard;Message from: ""Jean-Daniel Cryans"" <jdcryans@apache.org>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/474/#review643
-----------------------------------------------------------

Ship it!


Few minor comments, else it looks very good


src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java
<http://review.cloudera.org/r/474/#comment2414>

    I wonder how that will play with new master code



src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.cloudera.org/r/474/#comment2415>

    garbage



src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
<http://review.cloudera.org/r/474/#comment2416>

    red



src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
<http://review.cloudera.org/r/474/#comment2417>

    Here we could retry for a long time if the region server that holds meta/root died not long ago, blocking accesses to that region.



src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
<http://review.cloudera.org/r/474/#comment2418>

    incomplete doc?



src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
<http://review.cloudera.org/r/474/#comment2419>

    cleanup



src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
<http://review.cloudera.org/r/474/#comment2420>

    good idea


- Jean-Daniel



;;;","04/Aug/10 00:02;stack;Here's what I committed... includes j-d comments.;;;","04/Aug/10 00:02;stack;Committed.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add_table.rb deletes any tables for which the target table name is a prefix,HBASE-2460,12462274,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,tlipcon,tlipcon,tlipcon,16/Apr/10 18:29,20/Nov/15 12:42,14/Jul/23 06:06,17/Apr/10 02:49,0.20.4,,,,,,,,,,,0.90.0,,scripts,,,,,0,,,"If you have a table foobar and a table foo, add_table.rb on table foo will delete all of the META entries for foobar. We're missing the ',' in the scan.",,hammer,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/10 18:55;tlipcon;ASF.LICENSE.NOT.GRANTED--hbase-2460.txt;https://issues.apache.org/jira/secure/attachment/12441995/ASF.LICENSE.NOT.GRANTED--hbase-2460.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26315,Reviewed,,,,Fri Nov 20 12:42:32 UTC 2015,,,,,,,,,,"0|i0hhsv:",100146,,,,,,,,,,,,,,,,,,,,,"17/Apr/10 02:49;stack;Applied to branches and trunk.  Thanks for fix Todd.;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Client stuck in TreeMap,remove",HBASE-2458,12462248,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,tlipcon,tlipcon,tlipcon,16/Apr/10 13:48,20/Nov/15 12:44,14/Jul/23 06:06,17/Apr/10 02:35,0.20.4,,,,,,,,,,,0.90.0,,Client,,,,,0,,,Testing 0.20_pre_durability@934691 my client got permanently stuck with one thread looping inside TreeMap.remove. See attached stack.,,karthik.ranga,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2023,,,,,,,,,,,,,"16/Apr/10 18:54;tlipcon;ASF.LICENSE.NOT.GRANTED--hbase-2458.txt;https://issues.apache.org/jira/secure/attachment/12441994/ASF.LICENSE.NOT.GRANTED--hbase-2458.txt","16/Apr/10 13:48;tlipcon;ASF.LICENSE.NOT.GRANTED--stack.txt;https://issues.apache.org/jira/secure/attachment/12441961/ASF.LICENSE.NOT.GRANTED--stack.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26314,Reviewed,,,,Fri Nov 20 12:44:01 UTC 2015,,,,,,,,,,"0|i0hhsn:",100145,,,,,,,,,,,,,,,,,,,,,"16/Apr/10 13:54;tlipcon;I think the issue is unsynchronized access to the SoftValueSortedMap for each tableLocations in HConnectionManager - it can eventually get the treemap into a corrupt state where it infinite loops in the removal.;;;","16/Apr/10 14:07;tlipcon;In particular, HConnectionManager.locateRegionInMeta calls getCachedLocation outside of the regionLockObject. Although this appears to be ""read only"", the SoftValueSortedMap calls through to treemap.remove and rq.poll, both of which are mutative.

Possible fixes: 1) add synchronization around getCachedLocation in HCM.locateRegionInMeta, or 2) Add synchronization internal to SoftValueSortedMap so that it becomes threadsafe.


[this appears to have been introduced by HBASE-2034];;;","16/Apr/10 14:26;tlipcon;oops, I meant client, not RS here.;;;","16/Apr/10 14:28;tlipcon;Also, CHANGES.txt has the wrong number for the sync block change that caused this. It's really 2023;;;","16/Apr/10 18:22;stack;Change CHANGES.txt everywhere.  Thanks.

I'm looking at changing the SVSM to internally use a ConcurrentSkipListMap.;;;","16/Apr/10 18:27;tlipcon;I changed it to synchronize all of its methods as a quick fix and it seemed to work OK so far (at least haven't run into it again yet);;;","16/Apr/10 18:54;tlipcon;Here's the simple patch to just synchronize SoftValueSortedMap;;;","17/Apr/10 02:20;stack;Patch looks good.  I thought the returns out of headmap and tailmap unsafe if iterated but Todd points out that head/tailmap makes copies so should be fine.;;;","17/Apr/10 02:35;stack;Committed to the two branches and trunk.  Thanks for the patch Todd.;;;","26/Apr/10 15:26;stack;I forgot to add this to CHANGES.txt on 0.20_pre_durability.  I added it just now.;;;","12/May/10 23:54;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RS gets stuck compacting region ad infinitum,HBASE-2457,12462210,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,,tlipcon,tlipcon,16/Apr/10 05:04,20/Nov/15 12:42,14/Jul/23 06:06,17/Apr/10 19:49,0.20.4,,,,,,,,,,,0.90.0,,,,,,,0,,,"Testing 0.20_pre_durability@934643, I ended up in a state where one region server got stuck compacting a single region over and over again forever. This was with a special config with very low flush threshold in order to stress test flush/compact code.",,anty,hammer,kannanm,karthik.ranga,larsfrancke,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/10 04:32;stack;ASF.LICENSE.NOT.GRANTED--bandaid-v2.patch;https://issues.apache.org/jira/secure/attachment/12442035/ASF.LICENSE.NOT.GRANTED--bandaid-v2.patch","16/Apr/10 05:05;tlipcon;ASF.LICENSE.NOT.GRANTED--log.gz;https://issues.apache.org/jira/secure/attachment/12441922/ASF.LICENSE.NOT.GRANTED--log.gz","16/Apr/10 05:05;tlipcon;ASF.LICENSE.NOT.GRANTED--stack;https://issues.apache.org/jira/secure/attachment/12441923/ASF.LICENSE.NOT.GRANTED--stack",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26313,,,,,Fri Nov 20 12:42:11 UTC 2015,,,,,,,,,,"0|i0hhsf:",100144,,,,,,,,,,,,,,,,,,,,,"16/Apr/10 05:05;tlipcon;Here's the full log and stack of the stuck RS;;;","16/Apr/10 05:08;tlipcon;Audit logs are just showing the ""stuck"" RS doing this:

2010-04-15 22:08:00,574 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=todd,todd,adm,git   ip=/192.168.42.43       cmd=listStatus  src=/hbase/test1/1736416594/actions  dst=null        perm=null
2010-04-15 22:08:00,576 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=todd,todd,adm,git   ip=/192.168.42.43       cmd=mkdirs      src=/hbase/test1/compaction.dir/1736416594   dst=null        perm=todd:supergroup:rwxr-xr-x
2010-04-15 22:08:00,577 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=todd,todd,adm,git   ip=/192.168.42.43       cmd=delete      src=/hbase/test1/compaction.dir/1736416594   dst=null        perm=null
2010-04-15 22:08:01,575 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=todd,todd,adm,git   ip=/192.168.42.43       cmd=listStatus  src=/hbase/test1/1736416594/actions     dst=null        perm=null

(ie a noop compaction);;;","16/Apr/10 05:24;tlipcon;Managed to use the log level servlet to get debug logs of the no-op compactions:

2010-04-15 22:24:05,594 WARN org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Region test1,4993900000,1271390277054 has too many store files, putting it back at the end of the flush queue.
2010-04-15 22:24:05,594 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction requested for region test1,4993900000,1271390277054/1736416594 because: regionserver/192.168.42.43:60020.cacheFlusher
2010-04-15 22:24:05,594 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting compaction on region test1,4993900000,1271390277054
2010-04-15 22:24:05,597 DEBUG org.apache.hadoop.hbase.regionserver.Store: Skipped compaction of 1 file; compaction size of actions: 231.5m; Skipped 7 files, size: 242232921
2010-04-15 22:24:05,598 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region test1,4993900000,1271390277054 in 0sec
;;;","16/Apr/10 05:44;tlipcon;Looks like the compaction code is basically deciding that there's nothing worth compacting here. Here are the files in the order that the compaction code is looking at them (oldest to newest) with their sizes
{noformat}
7876304170849844721 2010-04-16 03:58:04
-rw-r--r--   3 todd supergroup  134676288 2010-04-15 20:58 /hbase/test1/1736416594/actions/7876304170849844721
3289606064411356106 2010-04-16 03:59:43
-rw-r--r--   3 todd supergroup   61309324 2010-04-15 20:59 /hbase/test1/1736416594/actions/3289606064411356106
4995952634622872563 2010-04-16 04:00:42
-rw-r--r--   3 todd supergroup   24381446 2010-04-15 21:00 /hbase/test1/1736416594/actions/4995952634622872563
6876448782185582993 2010-04-16 04:01:03
-rw-r--r--   3 todd supergroup   12103493 2010-04-15 21:01 /hbase/test1/1736416594/actions/6876448782185582993
7381870009831588255 2010-04-16 04:01:18
-rw-r--r--   3 todd supergroup    5855317 2010-04-15 21:01 /hbase/test1/1736416594/actions/7381870009831588255
5274234463019618354 2010-04-16 04:01:23
-rw-r--r--   3 todd supergroup    2712677 2010-04-15 21:01 /hbase/test1/1736416594/actions/5274234463019618354
3688488928995595533 2010-04-16 04:01:32
-rw-r--r--   3 todd supergroup    1194376 2010-04-15 21:01 /hbase/test1/1736416594/actions/3688488928995595533
5321716733066884905 2010-04-16 04:01:33
-rw-r--r--   3 todd supergroup     532824 2010-04-15 21:01 /hbase/test1/1736416594/actions/5321716733066884905
{noformat};;;","16/Apr/10 05:52;stack;Here is the code:

{code}
      if (!majorcompaction && !references) {
        // Here we select files for incremental compaction.  
        // The rule is: if the largest(oldest) one is more than twice the 
        // size of the second, skip the largest, and continue to next...,
        // until we meet the compactionThreshold limit.
        for (point = 0; point < countOfFiles - 1; point++) {
          if ((fileSizes[point] < fileSizes[point + 1] * 2) && 
               (countOfFiles - point) <= maxFilesToCompact) {
            break;
          }
          skipped += fileSizes[point];
        }
        filesToCompact = new ArrayList<StoreFile>(filesToCompact.subList(point,
          countOfFiles));
...
{code}

Todd did a listing of the Store:

{code}
-rw-r--r--   3 todd supergroup  134676288 2010-04-15 20:58 /hbase/test1/1736416594/actions/7876304170849844721
-rw-r--r--   3 todd supergroup   61309324 2010-04-15 20:59 /hbase/test1/1736416594/actions/3289606064411356106
-rw-r--r--   3 todd supergroup   24381446 2010-04-15 21:00 /hbase/test1/1736416594/actions/4995952634622872563
-rw-r--r--   3 todd supergroup    1194376 2010-04-15 21:01 /hbase/test1/1736416594/actions/3688488928995595533
-rw-r--r--   3 todd supergroup    2712677 2010-04-15 21:01 /hbase/test1/1736416594/actions/5274234463019618354
-rw-r--r--   3 todd supergroup     532824 2010-04-15 21:01 /hbase/test1/1736416594/actions/5321716733066884905
-rw-r--r--   3 todd supergroup   12103493 2010-04-15 21:01 /hbase/test1/1736416594/actions/6876448782185582993
-rw-r--r--   3 todd supergroup    5855317 2010-04-15 21:01 /hbase/test1/1736416594/actions/7381870009831588255
{code}

If you trace, the olders is > 2 * the next oldest, and so on.  Some of the times are same so not sure how that plays out.... (the above is not strictly ordered ... those of same time may not be proper chronological order).

;;;","16/Apr/10 06:45;stack;Ugly suggested bandaid so can cut RC: http://pastie.org/922715 ;;;","16/Apr/10 06:57;tlipcon;This issue got me thinking about why the heuristic is the way it is, and I don't quite follow.

I stepped away for a minute and tried to come up with a cost model for why it is we do minor compactions. Let me run this by everyone:

- The cost of a doing compaction is the cost of reading all of the files plus the cost of writing the newly compacted file. The new file might be a bit smaller than the sum of the originals, so the total cost = sum(compacted file size) + writes cost factor * reduction factor * sum(compacted file size). Let's call this (1 + WF)*sum(file sizes)
- The cost of *not* doing a compaction is just that reads are slower because we have to seek more. Let's define R as some unknown factor which describes how important read performance is to us, and S the seek time for each additional store. So the cost of not doing the compaction is R*S*num stores.

So if we combine these, we want to basically minimize (1 + WF)*size(files to compact) - R*S*count(files to compact).

So here's a simple algorithm to minimize that:
{noformat}
sort files by increasing size
for each file:
  if (1+WF)*size(file) < R*S:
    add file to compaction set
  else:
    break
{noformat}

If you rearrange that inequality, you can make it: if size(file) < (R*S)/(1+WF). Basically, what this is saying is that we should just make file size the heuristic, and tune the minimum file size based on the ratio between how much we care about having a small number of stores vs saving sequential IO on compactions.

The one flaw if you look closely is that we can't actually sort by increasing size and compact some set of the smallest ones, because we have to only compact a set of files that are contiguous in the sequence. I think we can slightly tweak the algorithm, though, to optimize the same objective but take that restriction into account.
;;;","16/Apr/10 17:13;streamy;Great stuff todd.  We've been doing a lot of discussion of these same heuristics recently.  Some recent commits have actually changed some things that will impact what we can/can't do when deciding which files to compact.  For one, deletes are no longer processed during minor compactions.  We're also not using (to my knowledge) relative ages of storefiles right now for anything, so I'm not sure it still holds that we can only compact neighbors.

I plan on working at this stuff during the hackathon.;;;","16/Apr/10 17:22;streamy;I'm +1 to commit the bandaid patch until we can get smarter.;;;","16/Apr/10 17:40;stack;Thanks Todd.  I like your cut at a better algorithm.  As per Jon, it may be the case that the contiguous-only no longer applies after yesterdays commit that leaves deletes in files until major compaction.  Also, anything that improves our i/o profile is ++ultra super important.

So, I suggest that we move the compaction review out of this issue into a new blocker against 0.20.5.  This new issue would be about trying your suggestion and breaking out compaction so it standalone/testable.  You're interested in this one Jon?

For this issue, for 0.20.4, I suggest we apply the band-aid.  For the super pathological case you turned up above, the band-aid will mean we only do two files at a time compressing -- way sub-optimal but at least we wouldn't be stuck.;;;","16/Apr/10 17:46;tlipcon;I thought about this a bit on the train this morning (guess this long commute is good for something!)

The real root of this problem is that we are getting ""noop"" compactions when we are blocked on a compaction happening. Rather than changing the compaction algorithm like in stack's patch, could we simply make it so that it checks whether there are currently flushes blocked on a compaction, and if that's the case, force one to happen?;;;","17/Apr/10 04:31;stack;.bq ...could we simply make it so that it checks whether there are currently flushes blocked on a compaction, and if that's the case, force one to happen?

Thats not easy given current structure.

Here's a second cut at a band-aid.  This differs from first in that we'll compact the last 4 files in the list at a minimum.  Review please.  I'm testing it now.;;;","17/Apr/10 04:33;stack;@Todd BTW, that posted log is crazy.  Flush at 2MB!;;;","17/Apr/10 16:12;stack;All tests w/ this patch in place.  I've been running it overnight under cluster loading and nothing untoward.  We might be compacting a bit more aggressively now doing the last 4 files in list if > 4 but that is probably for the better.;;;","17/Apr/10 19:49;stack;Ok.  I applied the bandaid so I can make an RC.  On commit I changed it so we compact the last 3 files rather than the last 4 to be a little more conservative.  I did not apply to TRUNK.;;;","19/Apr/10 14:46;apurtell;
   [[ Old comment, sent by email on Fri, 16 Apr 2010 17:45:20 +0000 ]]

+1 on Stack's strategy to get a RC out the door (today?)


;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","17/May/10 22:24;stack;I applied this to TRUNK.  It wasn't there.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deleteChangedReaderObserver spitting warnings after HBASE-2248,HBASE-2456,12462208,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,tlipcon,tlipcon,16/Apr/10 03:24,20/Nov/15 12:44,14/Jul/23 06:06,16/Apr/10 05:24,0.20.4,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"I'm seeing very occasional ""Not in set"" warnings on region servers under heavy concurrent read/write test after HBASE-2248. Here's a log:

http://pastebin.com/1Vm9C7Uf",,kannanm,larsfrancke,qwertymaniac,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/10 05:07;stack;ASF.LICENSE.NOT.GRANTED--2456.patch;https://issues.apache.org/jira/secure/attachment/12441924/ASF.LICENSE.NOT.GRANTED--2456.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26312,Reviewed,,,,Fri Nov 20 12:44:08 UTC 2015,,,,,,,,,,"0|i0hhs7:",100143,,,,,,,,,,,,,,,,,,,,,"16/Apr/10 04:50;tlipcon;logs with stack trace here:
http://pastebin.com/jZuRXBbQ;;;","16/Apr/10 05:07;stack;Check closing flag.  If already set, don't go through with second close.;;;","16/Apr/10 05:24;stack;Committed.  Thanks for review (on IRC) Todd and Ryan (had to commit twice on pre_durability because i got it wrong first time through);;;","19/Apr/10 14:44;ryanobjc;
   [[ Old comment, sent by email on Thu, 15 Apr 2010 21:42:31 -0700 ]]

I've seen those before, the issue is closing a scanner more than once.
there should be a guard, but these are in fact harmless.

there is more concurrency going on with less row locks :-)



;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Cached an already cached block"" RTE in RS after split",HBASE-2455,12462203,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,tlipcon,tlipcon,16/Apr/10 01:52,12/Oct/12 06:15,14/Jul/23 06:06,10/Sep/10 17:56,0.20.4,,,,,,,,,,,0.20.5,,,,,,,0,,,"It seems there's a race right after splits where this RTE is thrown. See logs here: http://pastebin.com/TDFpid5r
(this seems to have been introduced by HBASE-2248)",,kannanm,karthik.ranga,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26311,,,,,Fri Sep 10 17:56:16 UTC 2010,,,,,,,,,,"0|i08sjj:",49224,,,,,,,,,,,,,,,,,,,,,"10/Sep/10 17:56;tlipcon;Pretty sure this got fixed in a later rev of HBASE-2248 in 0.20.4 or 0.20.5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
".META. by-passes cache; BLOCKCACHE=>'false'",HBASE-2451,12462101,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,stack,stack,15/Apr/10 04:49,20/Nov/15 12:43,14/Jul/23 06:06,15/Apr/10 23:45,,,,,,,,,,,,0.90.0,,,,,,,0,,,"In a new install, if I describe '.META.', it says:

{code}
DESCRIPTION                                                             ENABLED                               
 {NAME => '.META.', IS_META => 'true', MEMSTORE_FLUSHSIZE => '16384', F true                                  
 AMILIES => [{NAME => 'historian', COMPRESSION => 'NONE', VERSIONS => '                                       
 2147483647', TTL => '604800', BLOCKSIZE => '8192', IN_MEMORY => 'false                                       
 ', BLOCKCACHE => 'false'}, {NAME => 'info', COMPRESSION => 'NONE', VER                                       
 SIONS => '10', TTL => '2147483647', BLOCKSIZE => '8192', IN_MEMORY =>                                        
 'false', BLOCKCACHE => 'true'}]} 
{code}

BLOCKCACHE is 'true' for the 'info' family (Yes historian is still in 0.20 branch).

But, if I add logging to hfile and storefile and store, blockcache is 'false' -- there is no cache constructed for use by the hfile.

This is killing cluster performance.  

It looks like a problem parsing the 'true' value in columnfamily.  I'll put up a patch in the morning.  Meantime, marking as blocker on 0.20.4.",,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/10 19:29;stack;ASF.LICENSE.NOT.GRANTED--meta.txt;https://issues.apache.org/jira/secure/attachment/12441865/ASF.LICENSE.NOT.GRANTED--meta.txt","15/Apr/10 23:23;stack;ASF.LICENSE.NOT.GRANTED--meta3.txt;https://issues.apache.org/jira/secure/attachment/12441886/ASF.LICENSE.NOT.GRANTED--meta3.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26310,Reviewed,,,,Fri Nov 20 12:43:06 UTC 2015,,,,,,,,,,"0|i0hhrb:",100139,,,,,,,,,,,,,,,,,,,,,"15/Apr/10 17:57;stack;I'm currently trying to figure why this is the case:

{code}
hbase(main):001:0> scan '-ROOT-'
2010-04-15T10:54:17.901-0700: 8.334: [GC 8.334: [ParNew: 19136K->2112K(19136K), 0.0270080 secs] 24761K->10598K(83008K), 0.0270680 secs] [Times: user=0.13 sys=0.04, real=0.03 secs] 
ROW                          COLUMN+CELL                                                                      
 .META.,,1                   column=info:regioninfo, timestamp=1271352114030, value=REGION => {NAME => '.META.
                             ,,1', STARTKEY => '', ENDKEY => '', ENCODED => 1028785192, TABLE => {{NAME => '.M
                             ETA.', IS_META => 'true', MEMSTORE_FLUSHSIZE => '16384', FAMILIES => [{NAME => 'h
                             istorian', VERSIONS => '2147483647', COMPRESSION => 'NONE', TTL => '604800', BLOC
                             KSIZE => '8192', IN_MEMORY => 'false', BLOCKCACHE => 'false'}, {NAME => 'info', V
                             ERSIONS => '10', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '8192',
                              IN_MEMORY => 'false', BLOCKCACHE => 'false'}]}}                                 
 .META.,,1                   column=info:server, timestamp=1271352175741, value=10.20.20.189:60020            
 .META.,,1                   column=info:serverstartcode, timestamp=1271352175741, value=1271352174642        
1 row(s) in 0.0930 seconds
hbase(main):002:0> describe '.META.'
DESCRIPTION                                                             ENABLED                               
 {NAME => '.META.', IS_META => 'true', MEMSTORE_FLUSHSIZE => '16384', F true                                  
 AMILIES => [{NAME => 'historian', COMPRESSION => 'NONE', VERSIONS => '                                       
 2147483647', TTL => '604800', BLOCKSIZE => '8192', IN_MEMORY => 'false                                       
 ', BLOCKCACHE => 'false'}, {NAME => 'info', COMPRESSION => 'NONE', VER                                       
 SIONS => '10', TTL => '2147483647', BLOCKSIZE => '8192', IN_MEMORY =>                                        
 'false', BLOCKCACHE => 'true'}]}    
{code}

i.e. if you scan -ROOT-, it has BLOCKCACHE false for the .META. region but if I describe it, then its 'true'.;;;","15/Apr/10 18:01;stack;Problem is in our bootstrapping code:

{code}
  private void bootstrap() throws IOException {
    LOG.info(""BOOTSTRAP: creating ROOT and first META regions"");
    try {
      // Bootstrapping, make sure blockcache is off.  Else, one will be
      // created here in bootstap and it'll need to be cleaned up.  Better to
      // not make it in first place.  Turn off block caching for bootstrap.
      // Enable after.
      setBlockCaching(HRegionInfo.ROOT_REGIONINFO, false);
      setBlockCaching(HRegionInfo.FIRST_META_REGIONINFO, false);
      HRegion root = HRegion.createHRegion(HRegionInfo.ROOT_REGIONINFO,
        this.rootdir, this.conf);
      HRegion meta = HRegion.createHRegion(HRegionInfo.FIRST_META_REGIONINFO,
        this.rootdir, this.conf);
      // Add first region from the META table to the ROOT region.
      HRegion.addRegionToMETA(root, meta);
      root.close();
      root.getLog().closeAndDelete();
      meta.close();
      meta.getLog().closeAndDelete();
      setBlockCaching(HRegionInfo.ROOT_REGIONINFO, true);
      setBlockCaching(HRegionInfo.FIRST_META_REGIONINFO, true);
    } catch (IOException e) {
      e = RemoteExceptionHandler.checkIOException(e);
      LOG.error(""bootstrap"", e);
      throw e;
    }
  }
{code}

Master has a descriptor where the blockcache is on but the regionserver hosting the .META. reads what was persisted.;;;","15/Apr/10 18:44;jdcryans;This really explains a lot.;;;","15/Apr/10 19:29;stack;Testing this patch  now.  I see LruCache actually doing something now on the HRS carrying .META.  Will let it run a while.;;;","15/Apr/10 19:30;stack;I need to write a script to fix this for instances that are currently in place.;;;","15/Apr/10 23:23;stack;Patch that enables IN_MEMORY and BLOCKCACHE on catalog tables on bootstrap.  Includes script that will fixup catalog tables when run (though subsequently requires cluster restart or redeploy of the .META. for the change to take effect).;;;","15/Apr/10 23:27;stack;I've been running this patch up on cluster.   Here is what I see now:

{code}
2010-04-15 16:23:47,892 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Cache Stats: Sizes: Total=6.0434265MB (6336992), Free=602.95654MB (632245792), Max=609.0MB (638582784), Counts: Blocks=177, Access=1022416, Hit=356196, Miss=666220, Evictions=0, Evicted=0, Ratios: Hit Ratio=34.83865559101105%, Miss Ratio=65.16134142875671%, Evicted/Run=NaN
{code}

First it takes a while for the ratios to build up... it start out at 100% misses.  The above is skewed some by the fact that the cache is so big relative to the amount of data that needs to caching (this is a big upload and only meta data is being cached).  Its also skewed because meta changing a lot durinng this upload.  This is better than what we had previous but looks like more work can be done in here (this table is pegged in_memory).  Will commit this after review for 0.20.4.  We can do other improvements elsewhere.;;;","15/Apr/10 23:33;ryanobjc;+1 do it;;;","15/Apr/10 23:45;stack;Committed to branches and trunk.  Thanks for review Ryan.;;;","12/May/10 23:53;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Local HBase does not stop properly,HBASE-2449,12462091,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,larsgeorge,larsgeorge,14/Apr/10 22:47,20/Nov/15 12:41,14/Jul/23 06:06,18/May/10 19:24,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"Just tried running a local hbase instance after a 

{code}
mvn -D skipTests=true package assembly:assembly
{code}

and then unpacking the SNAPSHOT tar ball in $HBASE_HOME/target

When trying to stop it with stop-hbase.sh it would not stop but print dots continuously. 

Thread dump shows:

{noformat}
Full thread dump Java HotSpot(TM) 64-Bit Server VM (14.3-b01-101 mixed mode):

""Attach Listener"" daemon prio=9 tid=0x0000000102b52800 nid=0x14eb13000 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""481404130@qtp0-0-EventThread"" daemon prio=5 tid=0x0000000102cf1800 nid=0x152dac000 waiting on condition [0x0000000152dab000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000001075778d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:477)

""481404130@qtp0-0-SendThread(localhost:2181)"" daemon prio=5 tid=0x0000000102914800 nid=0x152a56000 runnable [0x0000000152a55000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:136)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:69)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
	- locked <0x0000000107577b58> (a sun.nio.ch.Util$1)
	- locked <0x0000000107577b70> (a java.util.Collections$UnmodifiableSet)
	- locked <0x0000000107577ae0> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1066)

""LruBlockCache.EvictionThread"" daemon prio=5 tid=0x0000000101eab800 nid=0x155452000 in Object.wait() [0x0000000155451000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000001094034c0> (a org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread.run(LruBlockCache.java:509)
	- locked <0x00000001094034c0> (a org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread)

""DestroyJavaVM"" prio=5 tid=0x0000000102800800 nid=0x100501000 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""ProcessThread:-1"" prio=5 tid=0x0000000102a5f800 nid=0x14f248000 waiting on condition [0x000000014f247000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000001091d2e40> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
	at org.apache.zookeeper.server.PrepRequestProcessor.run(PrepRequestProcessor.java:103)

""SyncThread:0"" prio=5 tid=0x0000000102a0e000 nid=0x14f145000 waiting on condition [0x000000014f144000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000001091d2d50> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
	at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:94)

""SessionTracker"" prio=5 tid=0x00000001029be000 nid=0x14f042000 in Object.wait() [0x000000014f041000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000001091e42f8> (a org.apache.zookeeper.server.SessionTrackerImpl)
	at org.apache.zookeeper.server.SessionTrackerImpl.run(SessionTrackerImpl.java:142)
	- locked <0x00000001091e42f8> (a org.apache.zookeeper.server.SessionTrackerImpl)

""NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181"" daemon prio=5 tid=0x000000010297f800 nid=0x14ef3f000 runnable [0x000000014ef3e000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:136)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:69)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
	- locked <0x00000001091c1240> (a sun.nio.ch.Util$1)
	- locked <0x00000001091c1228> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000001091ea248> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
	at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:232)

""Low Memory Detector"" daemon prio=5 tid=0x0000000101878000 nid=0x14e90d000 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""CompilerThread1"" daemon prio=9 tid=0x0000000102840800 nid=0x14e80a000 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""CompilerThread0"" daemon prio=9 tid=0x000000010283f800 nid=0x14e707000 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Signal Dispatcher"" daemon prio=9 tid=0x000000010283f000 nid=0x14e604000 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Surrogate Locker Thread (CMS)"" daemon prio=5 tid=0x000000010283e000 nid=0x14e501000 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Finalizer"" daemon prio=8 tid=0x000000010282e800 nid=0x14e157000 in Object.wait() [0x000000014e156000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000001091efa30> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
	- locked <0x00000001091efa30> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)

""Reference Handler"" daemon prio=10 tid=0x000000010282d800 nid=0x14e054000 in Object.wait() [0x000000014e053000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000001091ef988> (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:485)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
	- locked <0x00000001091ef988> (a java.lang.ref.Reference$Lock)

""VM Thread"" prio=9 tid=0x0000000102826800 nid=0x14df51000 runnable 

""Gang worker#0 (Parallel GC Threads)"" prio=9 tid=0x0000000102802000 nid=0x106205000 runnable 

""Gang worker#1 (Parallel GC Threads)"" prio=9 tid=0x0000000102802800 nid=0x106308000 runnable 

""Concurrent Mark-Sweep GC Thread"" prio=9 tid=0x0000000101800800 nid=0x14dc5a000 runnable 
""VM Periodic Task Thread"" prio=10 tid=0x0000000101881000 nid=0x14ea10000 waiting on condition 

""Exception Catcher Thread"" prio=10 tid=0x0000000102801800 nid=0x103101000 runnable 
JNI global references: 1175
{noformat}
",MacOS,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/10 08:04;stack;stop.txt;https://issues.apache.org/jira/secure/attachment/12444775/stop.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26309,,,,,Fri Nov 20 12:41:44 UTC 2015,,,,,,,,,,"0|i0hhqv:",100137,,,,,,,,,,,,,,,,,,,,,"18/May/10 08:04;stack;This works for local mode.  Need to test on distributed.  The running of a zk instance inside the VM when in local mode was incomplete.  It did not manage the shutdown of the zk instance.  Added this by adding special HMaster class that will close out zk when all is done.

Also removes from HRegionServer the running of shutdown hook to do clean up of filesystem.  Its not needed now we have a subclass of HRS over in MiniHBaseCluster that manages shutdown hook when all runs in the one VM.;;;","18/May/10 19:24;stack;I tested on cluster and local mode.  Committing.;;;","18/May/10 21:14;tsuna;Thanks for fixing this, this has been bugging me for a long time.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scanner threads are interrupted without acquiring lock properly,HBASE-2448,12462075,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,tlipcon,tlipcon,tlipcon,14/Apr/10 19:24,20/Nov/15 12:41,14/Jul/23 06:06,20/Apr/10 21:09,,,,,,,,,,,,0.90.0,,master,,,,,0,,,"There are a few places where scanner threads are interrupted with .interrupt() instead of .interruptIfAlive(). This means that if they're in the midst of the checkFileSystem operation, it'll end up catching the interruption there, determine that the filesystem is down, and shut down the whole server. Other nasties can also result.",,hammer,kannanm,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/10 20:53;tlipcon;ASF.LICENSE.NOT.GRANTED--hbase-2448.txt;https://issues.apache.org/jira/secure/attachment/12441769/ASF.LICENSE.NOT.GRANTED--hbase-2448.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26308,Reviewed,,,,Fri Nov 20 12:41:23 UTC 2015,,,,,,,,,,"0|i0hhqn:",100136,,,,,,,,,,,,,,,,,,,,,"14/Apr/10 20:53;tlipcon;Here's a patch which changes around the interruptions to only trigger the sleeper to end its sleep early (or skip its next sleep cycle)

To test, I set my meta scan frequency to 1ms so that the HMaster is constantly scanning. Without this patch it ends up crashing the master pretty quickly when regions are reassigned, etc. With the patch I managed to run load test for some time without seeing it reoccur.;;;","20/Apr/10 21:09;stack;Applied to two branches and TRUNK.  Thanks for the patch Todd.;;;","24/Apr/10 17:18;stack;Actually, I only just now applied this patch to TRUNK.;;;","12/May/10 23:53;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogSyncer.addToSyncQueue doesn't check if syncer is still running before waiting,HBASE-2447,12462072,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,tlipcon,tlipcon,tlipcon,14/Apr/10 19:17,20/Nov/15 12:43,14/Jul/23 06:06,24/Apr/10 04:29,,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"In testing GC pause scenarios with kill -STOP, I got the regionserver into a situation where it was blocked forever while shutting down (also blocking clients, since the RPCs were still pinging). The root issue is that, if the log syncer has an error just as more edits are being done, addToSyncQueue() can go to sleep waiting on a syncer which has just died.",,kannanm,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/10 20:49;tlipcon;ASF.LICENSE.NOT.GRANTED--hbase-2447.txt;https://issues.apache.org/jira/secure/attachment/12441767/ASF.LICENSE.NOT.GRANTED--hbase-2447.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26307,Reviewed,,,,Fri Nov 20 12:43:34 UTC 2015,,,,,,,,,,"0|i0hhqf:",100135,,,,,,,,,,,,,,,,,,,,,"14/Apr/10 20:49;tlipcon;Here's a patch that I believe fixes the issue (at least I haven't seen it reoccur since). The way I triggered the problem was to pause a RS with kill -STOP for 62 seconds - when it came back from sleeping state all if its writes to hlogs would fail, but pending writes would still try to sync. It correctly tried to shut down but was left in a state where multiple threads were waiting on the syncer, but the syncer had already exited, so it never shut down.;;;","20/Apr/10 21:50;stack;I like the STOP/CONT trick.  I'm trying it to see if I can get into same state...;;;","23/Apr/10 21:44;jdcryans;If we remove group commit, then we don't need this patch?;;;","23/Apr/10 23:54;tlipcon;Yea, but that hflush improvement is still reasonably experimental, I think. I think we ought to commit this first, and if we commit the other patch later that's fine.;;;","24/Apr/10 00:11;jdcryans;Fair enough.

About the patch, instead of creating a new attribute in the thread, would it be possible to use HLog.closed? Or check if logSyncerThread.isAlive? In any case, I'm not satisfied by those kind of solutions. What happens if the thread is instead set closing the moment after the check is done? We would still be hanging on signal? It's a shame that we can't timeout that method.;;;","24/Apr/10 00:17;tlipcon;bq. happens if the thread is instead set closing the moment after the check is done

This can't happen, since the boolean is modified inside the lock, and it acquires the lock before checking the boolean.

bq.  It's a shame that we can't timeout that method.

We could spin and call await() with a timeout, but it just seems like more of a pain.

bq. would it be possible to use HLog.closed

I don't think so, because we need to set this from within the finally clause of the log syncer thread itself - we can't call close() from within logsyncer, because it would wait for itself.;;;","24/Apr/10 00:28;jdcryans;bq. This can't happen, since the boolean is modified inside the lock, and it acquires the lock before checking the boolean.

/me facedesk

Could we instead consider checking lock. hasWaiters(queueEmpty) before queueEmpty.signal()? Normally the thread will always be waiting there since that's how it releases the lock. If not, then the thread is dead?;;;","24/Apr/10 00:51;tlipcon;bq. hasWaiters(queueEmpty) before queueEmpty.signal

That seems really dirty to me... what's the problem you see with the flag? It's an extremely common practice to set a condition before signalling and check the condition before waiting.

(your proposed solution also doesn't work if we call addToSyncQueue before the syncer thread has started, probably not possible in current code, but still worth avoiding the problem);;;","24/Apr/10 01:27;jdcryans;Just exploring other solutions out loud ;);;;","24/Apr/10 04:29;stack;Committed.  I tried repro'ing but I ain't too good at waiting around so missed repeatedly.  Patch standalone looks good.  Thanks for reviewing it too J-D.  Thanks for the patch Todd.  Applied branch and trunk.;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IPC client can throw NPE if socket creation fails,HBASE-2443,12462007,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,tlipcon,tlipcon,tlipcon,14/Apr/10 01:51,20/Nov/15 12:42,14/Jul/23 06:06,20/Apr/10 23:04,0.20.3,,,,,,,,,,,0.90.0,,IPC/RPC,,,,,0,,,"If the socket factory fails to create a socket (eg Too many open files), then HBaseClient.handleConnectionFailure will throw NPE instead of an IOE subclass",,hammer,kannanm,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/10 01:52;tlipcon;ASF.LICENSE.NOT.GRANTED--hbase-2443.txt;https://issues.apache.org/jira/secure/attachment/12441674/ASF.LICENSE.NOT.GRANTED--hbase-2443.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26306,Reviewed,,,,Fri Nov 20 12:42:19 UTC 2015,,,,,,,,,,"0|i0hhpz:",100133,,,,,,,,,,,,,,,,,,,,,"14/Apr/10 01:52;tlipcon;Here's a patch. We should write a unit test and integrate this in Hadoop too.;;;","14/Apr/10 04:40;stack;Bringing into 0.20.4/0.21;;;","20/Apr/10 23:04;stack;Committed to two branches and TRUNK.  After looking at patch, all it does is test for null.;;;","26/Apr/10 02:19;tlipcon;Filed in hadoop as HADOOP-6724;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log lease recovery catches IOException too widely,HBASE-2442,12462006,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,tlipcon,tlipcon,tlipcon,14/Apr/10 01:41,20/Nov/15 12:40,14/Jul/23 06:06,17/May/10 21:47,0.20.3,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"In HLog.recoverLog, all IOExceptions are caught. This code should only spin for the case of AlreadyBeingCreatedException, but currently will spin on any IOException, even though others may well be permanent state (eg the log directory has been chowned to someone else)",,hammer,kannanm,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/10 05:44;tlipcon;hbase-2442.txt;https://issues.apache.org/jira/secure/attachment/12442816/hbase-2442.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26305,Reviewed,,,,Fri Nov 20 12:40:39 UTC 2015,,,,,,,,,,"0|i0hhpr:",100132,,,,,,,,,,,,,,,,,,,,,"14/Apr/10 04:46;stack;Asking Cosmin if fix for this is part of hbase-2437 (over in hbase-2437);;;","20/Apr/10 21:56;stack;Easy fix.  Bringing into 0.20.5 and 0.21.;;;","26/Apr/10 05:44;tlipcon;I imagine this will get incorporated into Cosmin's work, but here's a patch that I've been testing with, seems to work (and cleans up the logs somewhat);;;","27/Apr/10 04:12;stack;Committed to 0.20 branch.  Moving this issue now to 0.21.  Cosmin's patch will probably subsume this patch there.;;;","17/May/10 21:47;stack;This was committed as part of ""HBASE-2544  Forward port branch 0.20 WAL to TRUNK"";;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase can get stuck if updates to META are blocked,HBASE-2439,12461995,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kannanm,kannanm,kannanm,13/Apr/10 21:55,20/Nov/15 12:40,14/Jul/23 06:06,14/Apr/10 22:42,,,,,,,,,,,,0.90.0,,,,,,,0,,,"(We noticed this on a import-style test in a small test cluster.)

If compactions are running slow, and we are doing a lot of region splits, then, since META has a much smaller hard-coded memstore flush size (16KB), it quickly accumulates lots of store files. Once this exceeds ""hbase.hstore.blockingStoreFiles"", flushes to META become no-ops. This causes METAs memstore footprint to grow. Once this exceeds ""hbase.hregion.memstore.block.multiplier * 16KB"", we block further updates to META.

In my test setup:
  hbase.hregion.memstore.block.multiplier = 4.
and,
  hbase.hstore.blockingStoreFiles = 15.

And we saw messages of the form:

{code}
2010-04-09 18:37:39,539 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 23 on 60020' on region .META.,,1: memstore size 64.2k is >= than blocking 64.0k size
{code}

Now, if around the same time the CompactSplitThread does a compaction and determines it is going split the region. As part of finishing the split, it wants to update META about the daughter regions. 

It'll end up waiting for the META to become unblocked. The single CompactSplitThread is now held up, and no further compactions can proceed.  META's compaction request is itself blocked because the compaction queue will never get cleared.

This essentially creates a deadlock and the region server is able to not progress any further. Eventually, each region server's CompactSplitThread ends up in the same state.

",,hammer,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/10 05:14;kannanm;ASF.LICENSE.NOT.GRANTED--2439_0.20_dont_block_meta.txt;https://issues.apache.org/jira/secure/attachment/12441689/ASF.LICENSE.NOT.GRANTED--2439_0.20_dont_block_meta.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26303,Reviewed,,,,Fri Nov 20 12:40:44 UTC 2015,,,,,,,,,,"0|i0hhpj:",100131,,,,,,,,,,,,,,,,,,,,,"13/Apr/10 22:01;kannanm;One option might be to never block updates to .META. even if it violates the blockingStoreFiles limit. 

Also, we could perhaps now bump up the MEMSTORE_FLUSH_SIZE of META now that we have sync support at the HDFS level.;;;","13/Apr/10 22:12;jdcryans;bq. Also, we could perhaps now bump up the MEMSTORE_FLUSH_SIZE of META now that we have sync support at the HDFS level.

Was done in trunk, for branch it should be also like that if HDFS-200 and pals are there.;;;","13/Apr/10 22:29;streamy;Yeah we should backport that change to 0.20.5

Even so, we should still probably never ever block writes into META memstore.  Hard to think of a time when this would really make sense (or cause less problems than just letting the updates in).;;;","13/Apr/10 23:10;kannanm;#1. >>> Was done in trunk.

In the branch, the HTableDescriptor constructor for META & ROOT does a ""setMemStoreFlushSize(16 * 1024)"" and I don't see that call in trunk.

{code}
  /**
   * Private constructor used internally creating table descriptors for 
   * catalog tables: e.g. .META. and -ROOT-.
   */
protected HTableDescriptor(final byte [] name, HColumnDescriptor[] families) {
    this.name = name.clone();
    this.nameAsString = Bytes.toString(this.name);
    setMetaFlags(name);
    for(HColumnDescriptor descriptor : families) {
      this.families.put(descriptor.getName(), descriptor);
    }
    setMemStoreFlushSize(16 * 1024);
  }
{code}

So is my understanding correct that in trunk, ROOT & META simply use the default (64MB) memstore flush size? Is that the change we want for branch?

#2. I'll also make the changes to never block updates for meta region.  I plan to do so by removing both of the following restrictions for meta regions:

a) blockingStoreFiles limit check in MemStoreFlusher.java:flushRegion():

{code}
  else if (isTooManyStoreFiles(region)) {
      LOG.warn(""Region "" + region.getRegionNameAsString() + "" has too many "" +
          ""store files, putting it back at the end of the flush queue."");
      server.compactSplitThread.compactionRequested(region, getName());
      // If there's only this item in the queue or they are all in this
      // situation, we will loop at lot. Sleep a bit.
      try {
        Thread.sleep(1000);
      } catch (InterruptedException e) { } // just continue
      flushQueue.add(region);
      // Tell a lie, it's not flushed but it's ok
      return true;
    }
{code}

b)  memstoresize violation restriction in HRegion.java:checkResources().

{code}
 private void checkResources() {
    boolean blocked = false;
    while (this.memstoreSize.get() > this.blockingMemStoreSize) {
      requestFlush();
      if (!blocked) {
        LOG.info(""Blocking updates for '"" + Thread.currentThread().getName() +
{code};;;","14/Apr/10 05:14;kannanm;Patch uploaded.  ant test pases.

Repeated  my test cluster import test. No longer runs into the stuck state described earlier.;;;","14/Apr/10 05:23;tlipcon;I can verify this bug - I saw it in my testing this afternoon as well. Relevant logs:

2010-04-13 19:19:01,322 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region test1,4542214000,1271211529201 in 1sec
2010-04-13 19:19:01,322 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting compaction on region test1,4230893000,1271197630557
2010-04-13 19:19:01,328 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_OPEN: test1,9652090000,1271197954171
2010-04-13 19:19:01,371 INFO org.apache.hadoop.hbase.regionserver.HRegion: region test1,4983930000,1271198319326/845482654 available; sequence id is 2300024
2010-04-13 19:19:01,371 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_OPEN: test1,8588812000,1271198227526
2010-04-13 19:19:01,375 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region test1,4230893000,1271197630557 in 0sec
2010-04-13 19:19:01,376 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting split of region test1,4230893000,1271197630557
2010-04-13 19:19:01,379 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed test1,4230893000,1271197630557
2010-04-13 19:19:01,383 INFO org.apache.hadoop.hbase.regionserver.HRegion: region test1,8588812000,1271198227526/314680607 available; sequence id is 2302647
2010-04-13 19:19:01,383 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_OPEN: test1,9652090000,1271197954171
2010-04-13 19:19:01,385 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_OPEN: test1,8075160000,1271198620343
2010-04-13 19:19:01,399 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_OPEN: test1,4300550000,1271198160421
2010-04-13 19:19:01,419 INFO org.apache.hadoop.hbase.regionserver.HRegion: region test1,9652090000,1271197954171/173125735 available; sequence id is 2299793
2010-04-13 19:19:01,420 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_OPEN: test1,8075160000,1271198620343
2010-04-13 19:19:01,433 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_OPEN: test1,4662590000,1271197688651
2010-04-13 19:19:01,448 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 11 on 60020' on region .META.,,1: memstore size 32.0k is >= than blocking 3
2.0k size

eventually the RS just devolves into repeatedly writing:
2010-04-13 19:20:17,396 WARN org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Region .META.,,1 has too many store files, putting it back at the end of the flush queue.

I'll try Kannan's patch;;;","14/Apr/10 05:26;tlipcon;[From code review standpoint, +1 on the patch. I think we should commit the whole thing
for the durability branch, and everything but the table descriptor change for the 0.20.4 branch];;;","14/Apr/10 17:36;kannanm;@Todd: 

<<< eventually the RS just devolves into repeatedly writing:
2010-04-13 19:20:17,396 WARN org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Region .META.,,1 has too many store files, putting it back at the end of the flush queue.
>>>

yes, that's exactly what I ran into as well.

Re: <<<< I think we should commit the whole thing for the durability  branch, 
and everything but the table descriptor change for the 0.20.4 branch >>>>

I am little confused about 0.20.4 vs. 0.20.5 vs. durability branch. I see 
some issues moved to 0.20.5.  Is 0.20's trunk now essentially 0.20.5? 
And is there a separate durability branch outside of the 0.20.x series?

regards,
Kannan;;;","14/Apr/10 17:42;stack;@Kannan We branched 0.20 to  make 0.20_pre_durability.  From this new branch we will take a 0.20.4.  It is critical fixes minus the big changes you fellas have been at.  We need to get the critical fixes out there sooner than 0.20.5+hdfs durability patches and testing.  Hence a 0.20.4.

Let me commit.;;;","14/Apr/10 22:42;stack;Applied to 0.20 branch as is.  Applied to 0.20_pre_durability and TRUNK w/o the change to table descriptor as per Todd suggestion (In former to minimize change and in latter because patch failed since limit had already been removed).  Thanks for the patch Kannan.;;;","19/Apr/10 14:45;ryanobjc;
   [[ Old comment, sent by email on Tue, 13 Apr 2010 15:28:24 -0700 ]]

The primary reason to do blocking store files is that compactions can
oome a server, so we protect ourselves.  With 0.20 that should no
longer be the case. I have compacted 100 files of 10gb, so i am in
favor of removing this feature.

On Tue, Apr 13, 2010 at 3:01 PM, Kannan Muthukkaruppan (JIRA)

;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor HLog splitLog,HBASE-2437,12461944,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,clehene,clehene,clehene,13/Apr/10 12:06,20/Nov/15 12:43,14/Jul/23 06:06,01/Jun/10 18:02,0.90.0,,,,,,,,,,,0.90.0,,master,,,,20/Apr/10 00:00,0,,,"the HLog.splitLog got really long and complex and hard to verify for correctness. 
I started to refactor it and also ported changes from hbase-2337 that deals with premature deletion of log files in case of errors. Further improvements will be possible, however the scope of this issue is to clean the code and make it behave correctly (i.e. not lose any edits)  

Added a suite of unit tests that might be ported to 0.20 as well.

Added a setting (hbase.skip.errors - feel free to suggest a better name) that, when set to false will make the process less tolerant to failures or corrupted files:  in case a log file is corrupted or an error stops the process from consistently splitting the log, will abort the entire operation to avoid losing any edits. When hbase.skip.errors is on any corrupted files will be partially parsed and then moved to the corrupted logs archive (see hbase-2337). 

Like hbase-2337 the splitLog method will first split all the logs and then proceed to archive them. If any splitted log file (oldlogfile.log) that is the result of an earlier splitLog attempt is found in the region directory, it will be deleted - this is safe since we won't move the original log files until the splitLog process completes.",,clint.morgan,larsfrancke,posix4e,tlipcon,,,,,,,,,,,,,,,,,,432000,432000,,0%,432000,432000,,,,,HBASE-2312,,,,,,,HBASE-2337,HBASE-2935,,,,,,,,,,,,,,,,,"21/May/10 00:01;stack;2437-v2.txt;https://issues.apache.org/jira/secure/attachment/12445119/2437-v2.txt","21/May/10 06:31;stack;2437-v3.txt;https://issues.apache.org/jira/secure/attachment/12445138/2437-v3.txt","21/May/10 19:09;stack;2437-v4.patch;https://issues.apache.org/jira/secure/attachment/12445187/2437-v4.patch","20/May/10 18:07;stack;2437.txt;https://issues.apache.org/jira/secure/attachment/12445089/2437.txt","30/May/10 00:44;clehene;HBASE-2437-v5.patch;https://issues.apache.org/jira/secure/attachment/12445864/HBASE-2437-v5.patch","22/Apr/10 18:12;clehene;HBASE-2437_for_HBase-0.21_with_unit_tests_for_HDFS-0.21.patch;https://issues.apache.org/jira/secure/attachment/12442605/HBASE-2437_for_HBase-0.21_with_unit_tests_for_HDFS-0.21.patch",,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,2,Reviewed,,,,Fri Nov 20 12:43:48 UTC 2015,,,,,,,,,,"0|i00xan:",3326,,,,,,,,,,,,,,,,,,,,,"13/Apr/10 12:07;clehene;This issue will incorporate some of the changes in HBASE-2337;;;","13/Apr/10 16:02;apurtell;Cosmin, thanks for taking this on. We're aiming for 0.20.5 to be branch's answer to data durability and correctness issues. So that starts with HDFS-200 and related issues, but all issues which affect probability of data loss should be rolled in as well.  What do you think about targeting your changes for that release also? ;;;","14/Apr/10 04:46;stack;Does fix for HBASE-2442 belong in here Cosmin?;;;","15/Apr/10 06:34;clehene;I haven't use recoverLog because it was failing in my unit tests. So I don't check size at all and just catch EOF when opening the reader.;;;","15/Apr/10 06:38;clehene;Andrew, I think HBASE-2337 addresses most problems already. I think we could reuse the unit tests I'm writing for 0.20 branch and fix if necessary. ;;;","21/Apr/10 22:55;tlipcon;How's this going, Cosmin? I'm itching to fix a couple bugs in error handling during log splitting and don't want to duplicate work.;;;","22/Apr/10 07:17;clehene;Todd, I'm sending the pach today. It still needs a little work, but the refactoring is done and could use a review, so we could coordinate. ;;;","22/Apr/10 18:12;clehene;The patch is not final, so intended for trunk, but I'd appreciate a code review.

some of the changes:
* splitLog was refactored - the logic can be followed easier now
* logs are left in place is something goes wrong
* if split is interrupted, or crashes, the second split will start from zero (having all original log files), hence will delete any oldlogfile.log found  under the regionserver if any. 
* protect from zombie HRS that writes some more to hlog after split started (using recoverLog)
* protect from deleting a log file that was created by a zombie HRS after split has started.
* skip.errors=true means whenever something goes wrong and might lose edits we abort leaving logs in place
* skip.errors=false tolerate some errors: if a corrupted hlog file is encountered, read what you can and continue, then archive the corrupted log file.
* deal with empty log files
* etc.

* added unit test for the above mentioned
* unit test class has tools to generate log files, leave them open, corrupt them, etc.

The unit (or rather integration) tests are designed for hdfs-0.21, but could be adapted with small changes.

I initially did the refactoring trying to avoid the recoverLog method (that opens the file for append, then closes it to make sure a file is closed) because it took to long to wait for the lease. However if a regionserver that was considered dead (zombie) keeps writing to those files, the only way to work around that so we won't lose edits is to make sure it's closed (Trying to rename the file before splitting it will allow a writer thread to keep writing even after the rename for a few seconds.) I created testLogCannotBeWrittenOnceParsed for this.


In unit tests I set the lease period for a file to 100ms in the setUp method to avoid waiting 60 seconds in the unit tests. 
getDFSCluster().getNamesystem().leaseManager.setLeasePeriod(100, 50000);

Apparently on hdfs-0.20 getNameSystem is not available.

Also I use hflush() in unit test to write data to a log file and then leave it open. If not flushed and left open the changes might not be seen by the reader.  hflush() could be avoided if the open file scenarios could be ignored. 
;;;","04/May/10 05:37;stack;Patch looks great.

+ Please change the name of this file as part of your refactoring ""oldlogfile.log""
+ ""logs are left in place is something goes wrong"" .. should they be moved aside?
+ HBaseTestingUtility has been splintered into smaller pieces since you made this patch so these additions of yours fit well with that general direction.
+ I love all the tests.  I like name of this thread: ZombieLastLogWriterRegionServer
+ How does this test, testLogCannotBeWrittenOnceParsed, work?  The ZombieLastLogWriterRegionServer can only write one more edit at most (?) but seems like splitlogs proceeds from the first through to the last as it currently does.  Why couldn't the old Zombie writer add a bunch of edits to the last file while all other files are being split?

;;;","04/May/10 06:10;stack;@Cosmin You seen HBASE-2471?  Related?;;;","04/May/10 07:18;clehene;@Stack, thanks for the review! 

I'll incorporate the changes and answer the questions :)

Regarding HBASE-2471. It doesn't check for the destination directory. I can incorporate it. The unit tests need to be changed to pre-create the region directories. ;;;","04/May/10 14:38;stack;@Cosmin Patch is big.  I need to take another look.  I'd love to get this patch into 0.20.5.  Its great.;;;","12/May/10 00:13;posix4e;I can't see to get this to apply to trunk. Is their an update, or should I do it?;;;","14/May/10 06:10;stack;@Cosmin Any chance of an update.  Now is the time to get this in w/ its fancy tests.  Thanks.;;;","14/May/10 09:39;clehene;I'm out of office. I'll get back on the 18th. 




;;;","17/May/10 18:54;clint.morgan;I've just been looking at HLog splitting for transactional hbase. I'd like to be able to reuse the same mechanism except the HLog keys change a bit (subclass them), and the log directories are different.

To allow these extensions, I was factoring the log splitting into its own class (HLogSplitter). And making the methods instance rather than static. This has the additional benefit of isolating this rather tricky splitting concern from HLog, which I think makes it read better IMO.

Does this sound reasonable? Would you like to do it as part of this patch? Otherwise, I'll wait until this is applied and propose this approach in another issue.;;;","17/May/10 19:04;stack;Sounds good to me Clint.  Maybe as separate issue?  I think Cosmin is off around the wilds of the mid-west at moment on holidays.  Hopefully hes not lost forever and we get him back soon.;;;","17/May/10 19:22;tlipcon;+1 on splitting into a new class. I'm adding some stuff into log splitting logic for HBASE-2231 as well, and it's just messy.

Does anyone know when Cosmin's coming back? If he's not coming back soon, can we have someone finish off this patch for him this week?;;;","17/May/10 19:29;stack;If he's not back by morrow, I'll have a go at it.  I've spent some time both in Cosmin's patch and in hlog as part of this forward port.;;;","17/May/10 23:49;clehene;Hey :), my flight has just been canceled. I'll get back on the 19th - I'm in Chicago. 
This patch is functionally complete. It's been running on our cluster for a while, but could use some more testing. I was just hoping to get the review and make the final changes. Moving it in it's own class sounds right. I also thought about that. So I'd appreciate someone apply the patch (hope it still applies) and try it. 
;;;","20/May/10 13:46;clehene;Ouch, it seems a lot of stuff has moved in trunk. Can't apply patch either.
;;;","20/May/10 18:07;stack;Version of Cosmin's patch that will apply to TRUNK.  Compiles but thats all I've done just yet.  Need to do a bit more review, make sure tests pass, then will put it up on review.hbase.org.;;;","21/May/10 00:01;stack;Working on the split log tests.  They depend on a feature that is in 0.21 that is not in 0.20 hdfs... being able to set namenode soft lease in unit tests.  Still working on this.;;;","21/May/10 06:31;stack;This patch doesn't have the changes that were made by earlier patches adding startminidfscluster to HBaseTestingUtility.  That was added as part of cleanup of HBaseTestingUtility over in HBASE-2587.

This v3 is now failing just three of the ten or so new tests that Cosmin added.  I'm going through them.  In part, tests fail because hdfs-0.20 is different to hdfs-0.21 it seems; e.g. fs.listStatus return null if file does not exist in 0.20 hdfs but throws FNFE in hadoop 0.21 (Need to verify this is indeed the case).  This is going to cause fun when we go about the work making it so same hbase runs on both hadoop 0.20 and 0.21.

This patch adds some dirty reflection to allow setting a lower soft limit on file leases in namenode, a public method in hadoop 0.21 but not accessible in hdfs 0.20 thats necessary to a bunch of the new Cosmin tests.;;;","21/May/10 16:46;tlipcon;bq. . fs.listStatus return null if file does not exist in 0.20 hdfs but throws FNFE in hadoop 0.21 (Need to verify this is indeed the case). This is going to cause fun when we go about the work making it so same hbase runs on both hadoop 0.20 and 0.21.

If this is true it should be considered a bug in the 0.21 release, as the FileSystem API is supposed to be compatible between releases. Let me ping Tom.;;;","21/May/10 17:42;stack;This changed it:

{code}
r806746 | cdouglas | 2009-08-21 15:50:10 -0700 (Fri, 21 Aug 2009) | 4 lines

HDFS-538. Per the contract elucidated in HADOOP-6201, throw
FileNotFoundException from FileSystem::listStatus rather than returning
null. Contributed by Jakob Homan.
{code};;;","21/May/10 19:09;stack;All tests pass now.  Also moved the split log to regionserver.wal package from master package where it seems to better belong.  Posted this patch to review.hbase.org.;;;","21/May/10 19:14;stack;OK.  Posted to review.hbase.org.;;;","23/May/10 20:43;tlipcon;Review comments at http://review.hbase.org/r/74/#review40;;;","25/May/10 10:56;clehene;Thanks for review Todd!
I started working on it. I'll post the changes/suggestions in the review



;;;","25/May/10 21:33;hbasereviewboard;Message from: ""Cosmin Lehene"" <clehene@adobe.com>



Working on it. Some of the stuff might need more input


- Cosmin


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/74/#review33
-----------------------------------------------------------


On 2010-05-21 12:11:59, stack wrote:


;;;","25/May/10 21:41;hbasereviewboard;Message from: ""Cosmin Lehene"" <clehene@adobe.com>



Let's have a separate issue for that



done - for both reader and writer



done - both reader and writer impl



the original version of this function determined me to start refactoring in the first place. I'll add the description but if it's still confusing it might need more work.



I know and tried to get a better name when created it. Can you suggest something better? I can't figure a short descriptive enough name



perhaps hbase.regionserver.hlog.splitlog.batch.size?



done



done



fixed



done



Fixed



done



done



sync() used to call syncFs(). It looks like HBASE-2544 changed things a bit, but it doesn't only add the SequenceFile sync marker.

I added this after I've seen inconsistent results when running splitLog on bigger hlogs. Try copying a log from the cluster locally and run splitLog from the command line a few times without flushing it after each append. I used to get inconsistent results between runs and calling sync fixed it.

There's this ""//TODO: test the split of a large (lots of regions > 500 file). In my tests it seems without hflush""  in the TestHLogSplit. 

We could do some testing to figure out why would log entries be lost when running locally.

What would be a better way to flush the writer?



done



fixed



I don't know what it's supposed to mean either :)



fixed



I'd like to be able to investigate the trailing garbage. I don't think this should ever happen (do you see any scenarios?). If it did we might lose data. We used to fix NameNode edits for fsImage by adding a missing byte to a corrupted entry.

I'd like to reflect more on this, maybe see other opinions. 



I'd rather have these differences dealt at the lowest level (writers) and abstracted than spread across code.
What do you think? 



done 
I'll need help setting guava as a maven dependency.



createNewSplitter is fine.
It's a Callable, changed to submit in order to check for the result in case one of the writers failed (see comment below)



more aspects here:
I think the reported size will be >0 after recover, even if file has no records. I was asking if we should add logic to check if it's the last log. 
EOF for non zero length, non zero records file means file is corrupted. 



see above comment



what's the other JIRA? see my above comments.



my previous comment got lost somehow.
Todd suggested submitting a Callable<Void> to executor thread.
I wonder if we could use getCompletedTaskCount. Documentation sais it's an estimation, however it's an estimation only during runtime and seems to be correct after shutdown finishes (I looked in the source as well)

Another option would be ExecutorCompletionService which seems to be suited for this kind of job.


- Cosmin


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/74/#review40
-----------------------------------------------------------


On 2010-05-21 12:11:59, stack wrote:


;;;","26/May/10 09:02;hbasereviewboard;Message from: ""Cosmin Lehene"" <clehene@adobe.com>



correction: Todd suggested submitting a Callable<Void> to executor thread and then do a Future.get() and catch. 



changed it


- Cosmin


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/74/#review40
-----------------------------------------------------------


On 2010-05-21 12:11:59, stack wrote:


;;;","26/May/10 20:35;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>



Oh, it's much much improved! Thanks! I still think a ""high level overview"" would be good to see.



why .regionserver.? I'd say just hbase.hlog.split.batch.size or something



if this applies only to hlog splitting, maybe hbase.hlog.split.skip.errors



Which do you mean by ""writers"" here? I'd support factoring this function out into an HdfsUtil class somewhere.



I agree if it has no records (I think - do we syncfs after writing the sequencefile header?). But there's the case where inside SequenceFile we call create, but never actually write any bytes. This is still worth recovering.

In general I think a corrupt tail means we should drop that record (incomplete written record) but not shut down. This is only true if it's the tail record, though.



Can't find it now... does my above comment make sense?



This seems really voodoo.. if anything we're probably masking a real bug by doing this. Can you write a unit test which shows this problem (even if it takes 30 minutes to run, would be good to have in our arsenal)



The case where this happens is if you crash in the middle of appending a long edit. Consider the case where a single edit might have 1MB of data (large rows). We can easily crash in the middle of transferring it, before we call sync on the edit. In this case, the client never received an ""ack"" for the write, so we can feel free to throw it away (this isn't data loss, it's correct operation).


- Todd


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/74/#review40
-----------------------------------------------------------


On 2010-05-21 12:11:59, stack wrote:


;;;","29/May/10 21:17;hbasereviewboard;Message from: ""Cosmin Lehene"" <clehene@adobe.com>



;;;","29/May/10 21:56;clehene;Changed to reflect http://review.hbase.org/r/74/
I left some TODOs see review comments.
;;;","30/May/10 00:45;clehene;I've noticed testLogCannotBeWrittenOnceParsed sometimes fails. I reduced the ""zombie"" sleep time to 1 and it fails most of the times. 
It seems the ""zombie"" thread can write and sync (+syncFs) after the log recovery. 
;;;","01/Jun/10 16:54;stack;@Cosmin I was trying to commit but testLogCannotBeWrittenOnceParsed fails for me every time too.  Can you fix?  This is only thing in way of a commit.  I've made issues for whats still outstanding in the review over at review.hbase.org;;;","01/Jun/10 17:02;stack;@Cosmin:  Here is how to add your guava dependency:

{code}
pynchon-8:trunk stack$ svn diff pom.xml
Index: pom.xml
===================================================================
--- pom.xml     (revision 950141)
+++ pom.xml     (working copy)
@@ -448,6 +448,7 @@
     <slf4j.version>1.5.8</slf4j.version>
     <stax-api>1.0.1</stax-api>
     <thrift.version>0.2.0</thrift.version>
+    <guava.version>r03</guava.version>
   </properties>
 
   <dependencyManagement>
@@ -697,10 +698,15 @@
       <version>${commons-math.version}</version>
       <scope>test</scope>
     </dependency>
-        <dependency>
+     <dependency>
       <groupId>org.apache.hadoop</groupId>
       <artifactId>hadoop-test</artifactId>
     </dependency>
+     <dependency>
+       <groupId>com.google.guava</groupId>
+       <artifactId>guava</artifactId>
+       <version>${guava.version}</version>
+    </dependency>
   </dependencies>
 
   <!--
{code};;;","01/Jun/10 17:59;stack;OK, after talking to Cosmin, going to make an issue to deal with the zombie testLogCannotBeWrittenOnceParsed issue.  Cosmin thinks it may be an hdfs issue (HBASE-2645).  Lets not let it get in the way of this commit and carry on investigation elsewhere.  For now I'll comment out the test.;;;","01/Jun/10 18:02;stack;Committed.  Thanks for the nice patch Cosmin.;;;","23/Jun/10 22:48;nspiegelberg;Hey, late peer review, I know.  I had a couple things I saw during the peer review that I would like to get resolution on...

1. FSUtils.recoverFileLease() - does the InterruptedException handler need to set Thread.currentThread().interrupt()?  What about users actually trying to kill the master?
2. parseHLog() - the comment says that len==0 can still happen with append support? At least for 0.20, that's only if the file wasn't closed.  However, we just did that immediately before in recoverFileLease. don't mind the code, but comment should change or be clarified
3. writeEditsBatchToRegions() - same thing with InterruptedException.  maybe want to have something higher up the call stack notice isInterrupted() and display a Log.info() message.  At the very least, we definitely don't want to delete the log directory in splitLog() if the user interrupts these threads and skipErrors==true
4. archiveLogs() - by default, we are archiving successfully split logs ala 'processedLogs'?  I'm not sure we want to do that by default.  I think people are mainly interested in problematic logs that couldn't survive the transition.  Having this as an optional toggle is okay, but a naive user wouldn't know he has these trash items.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZK settings for initLimit/syncLimit shouldn't have been removed from hbase-default.xml,HBASE-2431,12461608,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,apurtell,kannanm,kannanm,09/Apr/10 00:36,20/Nov/15 12:41,14/Jul/23 06:06,09/Apr/10 00:49,,,,,,,,,,,,0.90.0,,,,,,,0,,,"The initLimit/syncLimit ZK settings were removed from hbase-default.xml as part of HBASE-2392 (upgrade to ZK 3.3). 

But these settings are needed. Without them, if you try to start a ZK quorum of more than 1 server, you'll get the following error:

{code}
java.lang.IllegalArgumentException: initLimit is not set
at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parseProperties(QuorumPeerConfig.java:246)
at org.apache.hadoop.hbase.zookeeper.HQuorumPeer.main(HQuorumPeer.java:76)
{code}",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26302,,,,,Fri Nov 20 12:41:19 UTC 2015,,,,,,,,,,"0|i0hhov:",100128,,,,,,,,,,,,,,,,,,,,,"09/Apr/10 00:37;stack;Patrick made this related issue: https://issues.apache.org/jira/browse/ZOOKEEPER-736;;;","09/Apr/10 00:49;apurtell;Committed to trunk and branch. ;;;","12/May/10 23:54;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in ProcessRegionClose because meta is offline kills master and thus the cluster,HBASE-2428,12461564,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,stack,stack,08/Apr/10 18:37,12/Oct/12 06:15,14/Jul/23 06:06,30/Apr/10 07:02,,,,,,,,,,,,0.20.5,0.90.0,,,,,,0,,,"This issue was born of study done in hbase-2413.  The meta went offline and we were processing a region close at the same time.  The close processing fell into a NPE loop and wouldn't get out of it killing master and effectivly killing the cluster:

{code}
2010-03-31 17:50:57,004 INFO org.apache.hadoop.hbase.master.ServerManager: hbasetest020.X.X.X,60020,1270077892989 znode expired
2010-03-31 17:50:57,004 INFO org.apache.hadoop.hbase.master.RegionManager: META region removed from onlineMetaRegions
2010-03-31 17:51:15,385 INFO org.apache.hadoop.hbase.master.ServerManager: Received start message from: hbasetest020.X.X.X,60020,1270083075377
2010-03-31 17:51:15,399 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Updated ZNode /hbase/rs/1270083075377 with data 10.18.35.215:60020
2010-03-31 17:51:15,870 DEBUG org.apache.hadoop.hbase.master.RegionManager: Server is overloaded: load=5, avg=3.0, slop=0.3
2010-03-31 17:51:15,870 DEBUG org.apache.hadoop.hbase.master.RegionManager: Choosing to reassign 2 regions. mostLoadedRegions has 5 regions in it.
2010-03-31 17:51:15,870 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region test1,3147000000,1270081876965
2010-03-31 17:51:15,870 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region test1,9352000000,1270080893514
2010-03-31 17:51:15,870 INFO org.apache.hadoop.hbase.master.RegionManager: Skipped 0 region(s) that are in transition states
2010-03-31 17:51:15,878 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_CLOSE: test1,3147000000,1270081876965 from hbasetest019.X.X.X,60020,1270082983630; 1 of 2
2010-03-31 17:51:15,879 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_CLOSE: test1,9352000000,1270080893514 from hbasetest019.X.X.X,60020,1270082983630; 2 of 2
2010-03-31 17:51:44,897 DEBUG org.apache.hadoop.hbase.master.ProcessRegionClose$1: Trying to contact region server for regionName '.META.,,1', but failed after 10 attempts.
Exception 1:
java.io.IOException: Call to /10.18.35.215:60020 failed on local exception: java.io.EOFExceptionException 1:
java.net.ConnectException: Connection refusedException 1:
java.net.ConnectException: Connection refusedException 1:
java.net.ConnectException: Connection refusedException 1:
java.net.ConnectException: Connection refusedException 1:
java.net.ConnectException: Connection refusedException 1:
java.net.ConnectException: Connection refusedException 1:
org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: .META.,,1
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2282)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.delete(HRegionServer.java:1989)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
Exception 1:
org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: .META.,,1
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2282)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.delete(HRegionServer.java:1989)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)

org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: .META.,,1
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2282)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.delete(HRegionServer.java:1989)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)

        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:94)
        at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:74)
        at org.apache.hadoop.hbase.master.ProcessRegionClose.process(ProcessRegionClose.java:63)
        at org.apache.hadoop.hbase.master.HMaster.processToDoQueue(HMaster.java:494)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:429)
2010-03-31 17:51:44,899 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessRegionClose of test1,1230000000,1270081673808, false, reassign: true
2010-03-31 17:51:44,899 DEBUG org.apache.hadoop.hbase.master.ProcessRegionClose$1: Exception in RetryableMetaOperation:
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:64)
        at org.apache.hadoop.hbase.master.ProcessRegionClose.process(ProcessRegionClose.java:63)
        at org.apache.hadoop.hbase.master.HMaster.processToDoQueue(HMaster.java:494)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:429)
2010-03-31 17:51:44,900 WARN org.apache.hadoop.hbase.master.HMaster: Processing pending operations: ProcessRegionClose of test1,1230000000,1270081673808, false, reassign: true
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:96)
        at org.apache.hadoop.hbase.master.ProcessRegionClose.process(ProcessRegionClose.java:63)
        at org.apache.hadoop.hbase.master.HMaster.processToDoQueue(HMaster.java:494)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:429)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:64)
        ... 3 more
{code}

... and so on.

Marking a blocker for 0.20.5.
{code}",,kannanm,karthik.ranga,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2479,,,,HBASE-2479,,,,,,,,,,,,,"17/May/10 17:06;stack;2428-for0.20.txt;https://issues.apache.org/jira/secure/attachment/12444706/2428-for0.20.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26301,,,,,Mon May 17 17:12:11 UTC 2010,,,,,,,,,,"0|i08sdb:",49196,,,,,,,,,,,,,,,,,,,,,"22/Apr/10 19:05;tlipcon;HBASE-2479 looks like the same thing, ish.;;;","28/Apr/10 06:42;stack;The unit test over in HBASE-2414 has turned up root cause.  If a RegionServerOperation is unable to complete -- ProcessRegionClose, ProcessServerShutdown are all RegionServerOperations, the events processed by the HMaster -- say because meta is offlined, its put on a DelayQueue.  On construction of the RegionServerOperation the 'delay' is set.  The DelayQueue is processed before new events when the master is going about its business.  Whats happening above is that the ProcessRegionClose failed to complete because meta was offline.  It was put on the DelayQueue.  The first put on the DelayQueue was probably fine... we waited some time and then retried but the meta still hadn't come online.  We were put on the DelayQueue again only now our delay had turned negative.  Since delayqueue is processed first, all we did was try to process the close even though in the other more live todo queue, we might have the process of the regionserver shutdown waiting, the process of the regionserver that was carrying .meta.  We'd never recover.

Will make a patch in morning.;;;","30/Apr/10 07:02;stack;The patch ocmmitted over in hbase-2414 had fixes for this issue and a unit test.  The patch there is big.  Here are the two pieces of the patch that in particular fixed this issue:

In HMaster process todo queue, when op.process failed... we'd fall into this block:
{code}
+    } catch (Exception ex) {
+      // There was an exception performing the operation.
+      if (ex instanceof RemoteException) {
+        try {
+          ex = RemoteExceptionHandler.decodeRemoteException(
+            (RemoteException)ex);
+        } catch (IOException e) {
+          ex = e;
+          LOG.warn(""main processing loop: "" + op.toString(), e);
+        }
+      }
+      LOG.warn(""Failed processing: "" + op.toString() +
+        ""; putting onto delayed todo queue"", ex);
+      putOnDelayQueue(op);
...
{code}

The fix is new method putOnDelayQueue(op).  Before we used to just add it direct to the delay queue.   This method first resets the RegionServerOperation expiration to some time in the future.  Previous it wasn't being reset so it just didn't stay in the delay queue.  It came straight out.

Here is part of fix that got rid of the NPEs.
{code}
Index: src/java/org/apache/hadoop/hbase/master/ProcessRegionClose.java
===================================================================
--- src/java/org/apache/hadoop/hbase/master/ProcessRegionClose.java	(revision 939172)
+++ src/java/org/apache/hadoop/hbase/master/ProcessRegionClose.java	(working copy)
@@ -58,6 +58,13 @@
 
   @Override
   protected boolean process() throws IOException {
+    if (!metaRegionAvailable()) {
+      // We can't proceed unless the meta region we are going to update
+      // is online. metaRegionAvailable() has put this operation on the
+      // delayedToDoQueue, so return true so the operation is not put
+      // back on the toDoQueue
+      return true;
+    }
     Boolean result = null;
     if (offlineRegion || reassignRegion) {
       result =
{code}
{code};;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","17/May/10 17:06;stack;This patch distills from 2414 the fix for this issue.  I want to apply to 0.20.  Please review.;;;","17/May/10 17:12;streamy;This looks good to me, thanks for explanation stack.  +1 for commit to branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove fragmentation indicator for 0.20.4... fix in 0.20.5.,HBASE-2422,12461473,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,stack,stack,07/Apr/10 23:41,12/Oct/12 06:15,14/Jul/23 06:06,08/Apr/10 21:53,,,,,,,,,,,,0.20.4,,,,,,,0,,,HBASE-2165 is about working on fragmentation indicator to make it less intrusive.  Currently it can get in way of displaying UI on big cluster.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2165,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26299,Incompatible change,,,,Thu Apr 08 21:53:31 UTC 2010,,,,,,,,,,"0|i08srb:",49259,,,,,,,,,,,,,,,,,,,,,"08/Apr/10 21:53;jdcryans;Resolved in the branches.

I marked this issue as incompatible since it removes a whole feature. I also set this as a sub task of HBASE-2165 since we want to come up with an improved solution. Also need to create a jira for trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Put hangs for 10 retries on failed region servers,HBASE-2421,12461453,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,ryanobjc,jdcryans,jdcryans,07/Apr/10 20:10,20/Nov/15 12:40,14/Jul/23 06:06,17/May/10 21:36,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Since MultiPut got in, instead of calling getRegionLocationForRowWithRetries we now call getRegionServerWithRetries to send an array list of Puts. The problem is that if the region server failed, we'll still retry the 10 times in a backoff fashion even tho we get connections refused. This is also true for a single put since it's the same code path.

Marking as critical since it almost disables our responsiveness to machine failures in certain cases where we are already sending a batch of edits when the server fails. Assigning to Ryan since he's been there recently.",,hammer,kannanm,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/10 03:34;ryanobjc;ASF.LICENSE.NOT.GRANTED--HBASE-2421-2.txt;https://issues.apache.org/jira/secure/attachment/12441788/ASF.LICENSE.NOT.GRANTED--HBASE-2421-2.txt","07/May/10 01:57;jdcryans;HBASE-2421-trunk.patch;https://issues.apache.org/jira/secure/attachment/12443918/HBASE-2421-trunk.patch","08/Apr/10 00:16;ryanobjc;HBASE-2421.txt;https://issues.apache.org/jira/secure/attachment/12441095/HBASE-2421.txt","28/Apr/10 07:53;tlipcon;hbase-2421.txt;https://issues.apache.org/jira/secure/attachment/12443049/hbase-2421.txt",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26298,Reviewed,,,,Fri Nov 20 12:40:49 UTC 2015,,,,,,,,,,"0|i0hhnj:",100122,,,,,,,,,,,,,,,,,,,,,"07/Apr/10 20:13;ryanobjc;the solution is _not_ to switch to using ""getRegionLocationForRowWithRetries"" since there is no row involved in this call - it is a series of rows that span multiple regions.  Probably getRSWR needs to detect and fail faster if a connection is refused.;;;","15/Apr/10 03:34;ryanobjc;slightly better version with todd's help;;;","16/Apr/10 14:59;tlipcon;+        server.shutdownHDFS.set(false); // no, bad.

Can you add a setter to HRegionServer rather than making this public? That comment also should be made more explanatory - not clear what's going on there.


+      if (singletonList) {
+        if (singleRowCause != null)
+          throw new IOException(singleRowCause);
+      }

I missed this when looking over your shoulder, but I think this if needs to go inside the following if (!list.isEmpty()) block. Otherwise if we have a retryable exception for a single-row get, then succeed on the second try, we'll still throw the exception from the first try. Alternatively, instead of moving the if, you could reset singleRowCause at the top of each inner loop.


- should augment javadoc to explain what happens in case of exceptions;;;","28/Apr/10 07:53;tlipcon;Here's a patch with those fixes applied.;;;","01/May/10 00:39;stack;I applied to branch.  Need new patch for trunk.  Its a bit different and I'm not confident I'll do right thing.;;;","07/May/10 01:24;jdcryans;I'm looking at applying this to trunk and I see that this wasn't applied to branch:

{code}
--- src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
+++ src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
@@ -116,6 +116,8 @@ public class LocalHBaseCluster implements HConstants {
       try {
         server = regionServerClass.getConstructor(HBaseConfiguration.class).
           newInstance(conf);
+        // Servers should not shut down HDFS, since they share an instance.
+        server.shutdownHDFS.set(false);
{code}

Was it intentional?;;;","07/May/10 01:43;jdcryans;Also this from HCM.processBatchOfPuts isn't used at all:

{code}
List<Put> permFails = new ArrayList<Put>();
{code};;;","07/May/10 01:57;jdcryans;Patch against trunk minus the stuff from my last two comments. It passes TestMultiParallelPut.;;;","07/May/10 17:37;stack;The shutdown hook stuff has been cleaned up, removed in 2431 for branch and trunk.  Let me commit that first.  IIRC, on commit to trunk of above, it had already changed, been handled elsewhere.

+1 on remove of permFails.

Patch looks good but will change after 2431 cleans up shutdown hook stuff.  I should commit in a little while.

;;;","17/May/10 21:36;stack;Forward-ported to TRUNK.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HCM.locateRootRegion fails hard on ""Connection refused""",HBASE-2417,12461437,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,07/Apr/10 17:58,12/Oct/12 06:15,14/Jul/23 06:06,08/Apr/10 18:20,0.20.3,,,,,,,,,,,0.20.4,0.90.0,,,,,,0,,,"While running some tests on replication, I saw that our client does something dumb if it tries to contact a dead region server that held the ROOT region in HCM.locateRootRegion. Will post stack trace in a comment.

The problem here is that we don't retry at all, the exception will come straight out of HCM like it's the end of the world.",,kannanm,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/10 18:13;jdcryans;HBASE-2417-v2.patch;https://issues.apache.org/jira/secure/attachment/12441190/HBASE-2417-v2.patch","08/Apr/10 00:55;jdcryans;HBASE-2417.patch;https://issues.apache.org/jira/secure/attachment/12441099/HBASE-2417.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26297,Reviewed,,,,Thu Apr 08 21:10:54 UTC 2010,,,,,,,,,,"0|i08snr:",49243,,,,,,,,,,,,,,,,,,,,,"07/Apr/10 18:00;jdcryans;Here's a stack trace:

{code}

java.lang.reflect.UndeclaredThrowableException
        at $Proxy12.getRegionInfo(Unknown Source)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRootRegion(HConnectionManager.java:1013)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:629)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.relocateRegion(HConnectionManager.java:611)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:762)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:634)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.relocateRegion(HConnectionManager.java:611)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:762)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:638)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.processBatchOfPuts(HConnectionManager.java:1332)
        at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:629)
        at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:513)
        at org.apache.hadoop.hbase.client.HTable.put(HTable.java:491)
...
Caused by: java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:404)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:309)
        at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:839)
        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:716)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:252)
        ... 39 more
{code}

The solution is to catch a Throwable like getRegionServerWithRetries does and convert it. Also in locateRootRegion we don't want to retry on a same server, if the server is dead we need to look back in ZK.;;;","08/Apr/10 00:55;jdcryans;Using the normal way to contact a region server, but without retries since we already manage them in the locateRootRegion method. Tested with my replication stuff and it works well.;;;","08/Apr/10 04:44;stack;+1 on commit.  It took me a while but use of the ServerCallable is what gets you catching of the above Throwable?;;;","08/Apr/10 05:56;jdcryans;Yeah took me a while too. So basically if there's a normal exception like NSRE then it will return null (that I catch). HCM could use some refactoring love.;;;","08/Apr/10 18:13;jdcryans;v2 now rolls back to calling the server directly but refactors the processing of exceptions and does the right thing (calling getRSWithoutRetries sometimes entered an infinite loop finding ROOT). ;;;","08/Apr/10 18:20;jdcryans;Committed to branch, branch pre-durab, and trunk. Stack reviewed the patch with me.;;;","08/Apr/10 21:10;ryanobjc;interesting to note, that previously getRSWithoutRetries was never previously used :-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enhance test suite to be able to specify distributed scenarios,HBASE-2414,12461334,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,karthik.ranga,karthik.ranga,07/Apr/10 00:44,20/Nov/15 12:42,14/Jul/23 06:06,30/Apr/10 06:55,0.20.3,,,,,,,,,,,0.90.0,,test,,,,,1,,,"We keep finding good cases that are reasonably hard to test, yet the test suite does not encode these. 
For example: 
HBASE-2413 Master does not respect generation stamps, may result in meta getting permanently offlined
HBASE-2312 Possible data loss when RS goes into GC pause while rolling HLog

I am sure there are many more such ""scenarios"" we should put into the unit tests. 

",,larsfrancke,mkurucz,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/10 06:19;stack;ASF.LICENSE.NOT.GRANTED--directcluster.txt;https://issues.apache.org/jira/secure/attachment/12441586/ASF.LICENSE.NOT.GRANTED--directcluster.txt","22/Apr/10 17:57;stack;master2.txt;https://issues.apache.org/jira/secure/attachment/12442602/master2.txt","27/Apr/10 07:16;stack;testmaster-v11.patch;https://issues.apache.org/jira/secure/attachment/12442930/testmaster-v11.patch","28/Apr/10 06:35;stack;testmaster-v14.txt;https://issues.apache.org/jira/secure/attachment/12443044/testmaster-v14.txt","29/Apr/10 02:38;stack;testmaster-v16.txt;https://issues.apache.org/jira/secure/attachment/12443158/testmaster-v16.txt","23/Apr/10 08:05;stack;testmaster-v4.patch;https://issues.apache.org/jira/secure/attachment/12442655/testmaster-v4.patch","23/Apr/10 19:17;stack;testmaster-v5.patch;https://issues.apache.org/jira/secure/attachment/12442700/testmaster-v5.patch","24/Apr/10 05:47;stack;testmaster-v7.patch;https://issues.apache.org/jira/secure/attachment/12442735/testmaster-v7.patch","26/Apr/10 03:52;stack;testmaster-v8.patch;https://issues.apache.org/jira/secure/attachment/12442811/testmaster-v8.patch",,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26296,Reviewed,,,,Fri Nov 20 12:42:00 UTC 2015,,,,,,,,,,"0|i0hhnb:",100121,,,,,,,,,,,,,,,,,,,,,"07/Apr/10 00:45;karthik.ranga;Something along the following lines would help accomplish this:

- In a version of the tests, the master and region server threads each have a method to receive a string representation of the task they have to carry out
- They use reflection and execute the call only when called by the external caller. That way we can ""serialize"" the distributed execution.
;;;","13/Apr/10 06:19;stack;Here is an ugly part-patch.  Its an attempt at doing a cluster that has no RPC.  Master and RegionServer would invoke each other's methods directly rather than go via RPC.  This is not the right approach -- subclassing HMaster and RS and trying to shutdown the networking.  It'd take for ever to do and it'd be ugly when done.

Let me think more on making it so regionservers and masters take a 'script' as K suggests above.;;;","22/Apr/10 17:57;stack;Failed attempt 2: Starts a master with disabled scanners and then fakes it out by invoking master methods directly calling region start and region server heartbeat methods, passing back the region open, etc. messages.  All works pretty good coming up but then master wants to connect to the non-running meta to add edis tot the table itself.  We could blank this networking out but in at least a few of the tests we want to build updates to fail legitimately because .meta. is not available.   This tack may be worth further dev but we need something better.

Looking now at bringing up the minihdfscluster and then doing direct injection as per the above patch on the individual daemons to get what I want.  Also looking at changing the proxy so its not over rpc but goes local instead.;;;","23/Apr/10 08:05;stack;This approach seems more promising.  It makes all the interfaces used networking configurable and then provides direct connect between master and regionservers with no network involved.  The instances are readily available so can do stuff like remove meta and then send in a close message.  Not done yet.  Just posting what I have so far.  Will keep at it.;;;","23/Apr/10 08:07;stack;Marking blocker and pulling into 0.20.5 and 0.21;;;","23/Apr/10 19:17;stack;This version better integrates the use of nonetwork implementations of pertinent interfaces.  Not done yet.  Need to work it into HBaseTestingUtility better and still need to prove that testing cluster transistion with this stuff is viable (it still looks so, just need to do the proof still).   If want to take a look now, start with TestClusterTransistions.;;;","24/Apr/10 05:47;stack;Cleaned up errors on shutdown.  Now clean start/stop of nonetwork cluster (and old stuff confirmed working as it used to).  Next is try to do actual unit test.;;;","26/Apr/10 03:52;stack;This patch is not for review.  Its a mess that has two testing techniques wrapped up in it still and is in need of cleanup.

My pursuit of a 'direct'/nonetwork hbase ran into the weeds; i.e the 'first' technique.  All of the client logic down in HConnnectionManager#TableServers would need to be redone so its 'direct'.  Subclassing HConnectionManager#TableServers helped in that I could leverage what was already there but still, a lot to be done. So, i tried doing reproduction of cluster bad-cases using old-school minihbasecluster machinations (technique 'two').

The pursuit of technique 'one' opened up our minihbasecluster code making it so I was able to write a simple test to repro what is seen here in the stack trace at the head of this issue (patch includes the code in the TestClusterTransition junit test).  

Here is what my test is showing...

{code}
2010-04-25 20:31:07,946 INFO  [RegionServer:1] regionserver.HRegionServer(649): aborting server at: 192.168.1.106:63335
2010-04-25 20:31:07,956 INFO  [main-EventThread] master.ServerManager$ServerExpirer(831): 192.168.1.106,63335,1272252645013 znode expired
2010-04-25 20:31:07,957 INFO  [main-EventThread] master.RegionManager(795): META region removed from onlineMetaRegions
2010-04-25 20:31:07,958 DEBUG [RegionServer:1] zookeeper.ZooKeeperWrapper(682): Closed connection with ZooKeeper
2010-04-25 20:31:07,958 INFO  [RegionServer:1] regionserver.HRegionServer(696): RegionServer:1 exiting
2010-04-25 20:31:07,960 INFO  [main] regionserver.HRegionServer(261): My address is 192.168.1.106:0
2010-04-25 20:31:07,961 INFO  [main] ipc.HBaseRpcMetrics(52): Initializing RPC Metrics with hostName=HRegionServer, port=63412
2010-04-25 20:31:07,962 INFO  [main] regionserver.MemStoreFlusher(102): globalMemStoreLimit=32.6m, globalMemStoreLimitLowMark=20.4m, maxHeap=81.4m
2010-04-25 20:31:07,963 INFO  [main] regionserver.HRegionServer$MajorCompactionChecker(984): Runs every 1000000ms
2010-04-25 20:31:07,968 DEBUG [HMaster] master.HMaster(506): Processing todo: ProcessServerShutdown of 192.168.1.106,63335,1272252645013
...
# While processing server shutdown in came the new RS instance w/ same port and load balance kicks in....
...
2010-04-25 20:31:08,187 INFO  [RegionServer:1] regionserver.HRegionServer(1202): HRegionServer started at: 192.168.1.106:63412
2010-04-25 20:31:08,188 DEBUG [RegionServer:1] zookeeper.ZooKeeperWrapper(398): Read ZNode /hbase/root-region-server got 192.168.1.106:63333
2010-04-25 20:31:08,218 DEBUG [pool-1-thread-1] regionserver.HLog$1(1278): Thread got 53 to process
2010-04-25 20:31:08,222 DEBUG [IPC Server handler 4 on 60000] master.RegionManager$LoadBalancer(1447): Server is overloaded: load=15, avg=7.5, slop=0.3
...
# Then fell into... this while processing a close region

2010-04-25 20:31:08,360 DEBUG [HMaster] master.HMaster(506): Processing todo: ProcessRegionClose of 2428,fff,1272252656267, false, reassign: true
2010-04-25 20:31:08,362 DEBUG [HMaster] master.RetryableMetaOperation(95): Exception in RetryableMetaOperation: 
java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:65)
	at org.apache.hadoop.hbase.master.ProcessRegionClose.process(ProcessRegionClose.java:89)
	at org.apache.hadoop.hbase.master.HMaster.processToDoQueue(HMaster.java:510)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:445)
2010-04-25 20:31:08,367 WARN  [HMaster] master.HMaster(546): Processing pending operations: ProcessRegionClose of 2428,fff,1272252656267, false, reassign: true
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:96)
	at org.apache.hadoop.hbase.master.ProcessRegionClose.process(ProcessRegionClose.java:89)
	at org.apache.hadoop.hbase.master.HMaster.processToDoQueue(HMaster.java:510)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:445)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:65)
	... 3 more
..

and so on...
{code}

It required some knowledge of minihbasecluster internals but its not too bad methinks.  I've added a bunch of doc. so others can follow.  

Let me clean up and repro more of the recent cluster failings in unit test scenario using minihbasecluster.;;;","27/Apr/10 07:16;stack;Cleaned up patch.  Adds a 'kill' method to HRS.  Included TestClusterTransition test will not shutdown because we are trying to do a close and can't because meta is not online nor will it ever come online (HBASE-2428).;;;","27/Apr/10 19:26;stack;Trying to use powermock and mockito inside a minihbasecluster context I run into the following issues (after struggling past similar around commons logging):

{code}

Testcase: testRegionCloseWhenNoMetaHBase2428 took 1.659 sec
        Caused an ERROR
loader constraint violation: loader (instance of org/powermock/core/classloader/MockClassLoader) previously initiated loading for a different type with name ""javax/management/MBeanServer""
java.lang.LinkageError: loader constraint violation: loader (instance of org/powermock/core/classloader/MockClassLoader) previously initiated loading for a different type with name ""javax/management/MBeanServer""
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:700)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:545)
        at org.powermock.core.classloader.MockClassLoader.loadUnmockedClass(MockClassLoader.java:190)
        at org.powermock.core.classloader.MockClassLoader.loadModifiedClass(MockClassLoader.java:148)
        at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:63)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:254)
        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:399)
        at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:53)
        at org.apache.hadoop.ipc.metrics.RpcActivityMBean.<init>(RpcActivityMBean.java:70)
        at org.apache.hadoop.ipc.metrics.RpcMetrics.<init>(RpcMetrics.java:64)
        at org.apache.hadoop.ipc.Server.<init>(Server.java:1028)
        at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:488)
        at org.apache.hadoop.ipc.RPC.getServer(RPC.java:450)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:191)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:279)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:956)
        at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:275)
        at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:200)
        at org.apache.hadoop.hbase.TestClusterTransitions.beforeAllTests(TestClusterTransitions.java:44)
        at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.executeTest(PowerMockJUnit44RunnerDelegateImpl.java:309)
        at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.executeTestInSuper(PowerMockJUnit47RunnerDelegateImpl.java:112)
        at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.executeTest(PowerMockJUnit47RunnerDelegateImpl.java:73)
        at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runBeforesThenTestThenAfters(PowerMockJUnit44RunnerDelegateImpl.java:297)
        at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:222)
        at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.runMethods(PowerMockJUnit44RunnerDelegateImpl.java:161)
        at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$1.run(PowerMockJUnit44RunnerDelegateImpl.java:135)
        at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.run(PowerMockJUnit44RunnerDelegateImpl.java:133)
        at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.run(JUnit4TestSuiteChunkerImpl.java:112)
        at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.run(AbstractCommonPowerMockRunner.java:55)
{code}

The above classloading uglyness looks like it would take a while to undo -- if its possible at all -- and when done, I'd end up with a fragile test environment is my guess.  I'm going to pass on this combo unless others have input.;;;","28/Apr/10 06:35;stack;Attached patch should have sufficient vocabulary for writing tests that can repro recent rash of issues.  It adds means of adding listeners to master events -- both before and after event happens.  The before lets you put off the event.  it'll be requeued, added to the DelayQueue (this happens currently in Master if an issue processing a RegionServerOperation, it gets requeued to be tried again later).  The patch also adds to minihbasecluster simple means of sending regionserver ""events'.

Patch includes test that reproduces hbase-2428.  Let me add more unit tests for other issues.;;;","29/Apr/10 02:38;stack;I want to commit this patch.  Can I get a review.  Its kinda polluted in that it contains:

1. Means of testing state transitions across the master.  Tests can register listeners.  In the listener you should be able to delay, cancel, count, etc., RegionServerOperations.  It should be possible in the listener simulating uglyness seen out on live clusters.
2. A fix for the HBASE-2428 bug.
3. Facility added to TestHBaseUtility and MiniHBaseCluster
4. I started refactoring of the queue of RegionServerOperations in master moving it out to a separate file making it testable but then I ran into fact that RegionServerOperations each have a reference to master AND they can put themselves back on the queue -- the circularity baffles.  This has to be fixed but will do in a separate patch.

Here is a commit message with some detail on the commit:

{code}
M src/test/org/apache/hadoop/hbase/HBaseTestingUtility.java
  Broke up startMiniHBaseCluster into smaller methods so can mix and
  match pieces of minihbasecluster toward other ends.
  (setupClusterBuildDir, isRunningCluster, getMiniHBaseCluster): Added.
M src/test/org/apache/hadoop/hbase/TestInfoServers.java
M src/test/org/apache/hadoop/hbase/TestRegionRebalancing.java
M src/test/org/apache/hadoop/hbase/HBaseClusterTestCase.java
M src/test/org/apache/hadoop/hbase/regionserver/TestLogRolling.java
M src/test/org/apache/hadoop/hbase/regionserver/DisabledTestRegionServerExit.java
M src/test/org/apache/hadoop/hbase/mapreduce/TestTableIndex.java
M src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java
  Ripple from change of MiniHBaseCluster.getRegionThreads to
  getRegionServerThreads.
M src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java
  Added new MiniHBaseClusterMaster that is override of HMaster so
  I can piggyback messages for designated regionservers atop the
  heartbeat: close region, etc.
  (getServerWithMeta, addMessageToSendRegionServer): Added.
A src/test/org/apache/hadoop/hbase/master/TestRegionServerOperationQueue.java
  Stubbed out test of new RegionServerOperationQueue class.
A src/test/org/apache/hadoop/hbase/master/TestMasterTransistions.java
  Test master cluster transistions.  Includes unit test of hbase-2428.
M src/test/org/apache/hadoop/hbase/util/TestMigration.java
  Disable migration test.  Nothing to migrate yet and besides it was
  trying to load a 0.19 hbase data tar.gz that has since been removed.
M src/contrib/stargate/build.xml
  Added a copyright.
M src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
  Documentation and moved some methods from down at tail of the class
  where they were in among static methods used parsing cmd-line usage
  up above the usage and master startup static methods.
  Added fix for issue broken by an hbase-1215 commit where we were
  looking at wrong address (Grep for r796326 for more).
M src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
  Moved bulk out to new JVMClusterUtils class and made accessible.
  Added passing of HMaster.class to instantiate to facilitate
  passing of TestHMaster.class.
M src/java/org/apache/hadoop/hbase/master/RegionServerOperationQueue.java
  The RegionServerOperations queues moved out to their own class from
  Master.  Allows listeners to register and get notice before and after
  a RegionServerOperation is processed.  Includes part of bug fix for
  hbase-2428. When an error processing a RegionServerOperation, we'd fall
  into the IOException catch.  We'd then put the operation back on the delay
  queue for later processing only we'd not reset its expiration.  It
  would therefore run again immmediately... fail again, and so on.
  Changed the return from process to be an enum rather than true/false
  so I don't have to have do things like call checkfs down in here and
  I don't need to have a master instance around.
M src/java/org/apache/hadoop/hbase/master/ServerManager.java
  How we add RegionServerOperation instances has changed to go via
  RegionServerOperationQueue now.
M src/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java
  (getDeadServerAddress): Added.
M src/java/org/apache/hadoop/hbase/master/RegionServerOperationListener.java
  Listener interface to implement if interested in watching RegionServerOperations.
M src/java/org/apache/hadoop/hbase/master/HMaster.java
  Moved the RegionServerOperation code out of here to
  RegionServerOperationQueue.
  (adornRegionServerAnswer, constructMaster): Added.
M src/java/org/apache/hadoop/hbase/master/ProcessRegionOpen.java
  Comment.
M src/java/org/apache/hadoop/hbase/master/ProcessRegionClose.java
  Added in *fix* for 2428 NPE.  For now did what happens in
  ProcessRegionOpen for symmetry's sake but it needs to be replaced.
M src/java/org/apache/hadoop/hbase/master/RegionServerOperation.java
  (resetExpiration): Added.
M src/java/org/apache/hadoop/hbase/master/ProcessRegionStatusChange.java
  Javaadoc.
M src/java/org/apache/hadoop/hbase/util/Threads.java
  (threadDumpingIsAlive): Added from LocalHBaseCluster.
  (sleep): Added.
M src/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java
  New class that has facility moved from LocalHBaseCluster with added
  javadoc and made accessible.  Needed testing.
{code};;;","29/Apr/10 18:17;karthik.ranga;Awesome stuff Stack! Patch looks great, just a couple of small comments:

- Revert src/java/org/apache/hadoop/hbase/master/ProcessRegionStatusChange.java (no change in file)
- src/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java:60 Don't need row parameter in ""ToDoEntry(final  byte [] row, final HRegionInfo  info)"" anymore.
;;;","30/Apr/10 00:58;stack;Thanks Karthik.  Will fix above on commit.;;;","30/Apr/10 06:55;stack;Committed to branch and trunk.  Thanks for review Karthik.  I committed with your suggested changes.  I'm resolving this issue though I'm sure there is probably a better way of testing cluster transitions.  This should do for now.  We can open a new JIRA for NG or improvements.;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Master does not respect generation stamps, may result in meta getting permanently offlined",HBASE-2413,12461310,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,karthik.ranga,karthik.ranga,06/Apr/10 22:20,20/Nov/15 12:41,14/Jul/23 06:06,08/May/10 06:28,0.20.3,,,,,,,,,,,0.90.0,,master,,,,,0,,,"This happens if the RS is restarted before the zk node expires. The sequence is as follows:

1. RS1 dies - lets say its server string was HOST1:PORT1:TS1
2. In a few seconds RS1 is restarted, it comes up as HOST1:PORT1:TS2 (TS2 is more recent than TS1)
3. Master gets a start up message from RS1 with the server name as HOST1:PORT1:TS2
4. Master adds this as a new RS, tries to red
---- The master does not use the generation stamps to detect that RS1 has already restarted.
---- Also, if RS1 contained meta, master would try to go to HOST1:PORT1:TS1. It would end up talking to HOST1:PORT1:TS2, which spews a bunch of not serving region exceptions.
5. zk node expires for HOST1:PORT1:TS1
6. Master tries to process shutdown for HOST1:PORT1:TS1 - this probably interferes with HOST1:PORT1:TS2  and ends up somehow removing the reassign meta in the master's queue.
---- Meta never comes online and master continues logging the following exception indefinitely:

2010-04-06 11:02:23,988 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessRegionClose of test1,7094000000,1270220428234, false, reassign: true
2010-04-06 11:02:23,988 DEBUG org.apache.hadoop.hbase.master.ProcessRegionClose$1: Exception in RetryableMetaOperation:
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:64)
        at org.apache.hadoop.hbase.master.ProcessRegionClose.process(ProcessRegionClose.java:63)
        at org.apache.hadoop.hbase.master.HMaster.processToDoQueue(HMaster.java:494)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:429)

",,hammer,kannanm,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/May/10 05:12;stack;2413-v10.patch;https://issues.apache.org/jira/secure/attachment/12444030/2413-v10.patch","05/May/10 06:26;stack;2413-v4.txt;https://issues.apache.org/jira/secure/attachment/12443685/2413-v4.txt","05/May/10 23:50;stack;2413-v6.patch;https://issues.apache.org/jira/secure/attachment/12443797/2413-v6.patch","07/May/10 06:47;stack;2413-v7.patch;https://issues.apache.org/jira/secure/attachment/12443930/2413-v7.patch","07/May/10 18:35;stack;2413-v8.patch;https://issues.apache.org/jira/secure/attachment/12443988/2413-v8.patch","07/May/10 21:55;stack;2413-v9.patch;https://issues.apache.org/jira/secure/attachment/12444004/2413-v9.patch","12/Apr/10 21:09;stack;ASF.LICENSE.NOT.GRANTED--newserver-v3.txt;https://issues.apache.org/jira/secure/attachment/12441546/ASF.LICENSE.NOT.GRANTED--newserver-v3.txt","08/Apr/10 07:38;stack;newserver.txt;https://issues.apache.org/jira/secure/attachment/12441123/newserver.txt",,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26295,Reviewed,,,,Fri Nov 20 12:41:42 UTC 2015,,,,,,,,,,"0|i0hhn3:",100120,,,,,,,,,,,,,,,,,,,,,"07/Apr/10 00:43;karthik.ranga;Fix is to have master recognize the RS1 restart before the zk node expires and handle it right away. There should only be one entry for each HOST:PORT - the one with the latest TS.
;;;","07/Apr/10 00:55;stack;So, looking at log sent via back channel, a few things:

1. When the restarted server shows up and we treat it as though it a new server, the load balancer rules the cluster unbalanced and starts to shed regions.  This is before the znode expires.  Looking in log, we'll have succesfully done a bit of distribution before the we start the processing of the failed server instance.  This is bad because this is churn.
2. Suspicious in the master is the serverAddressToServerInfo Map.  Its keyed by server+port.  When znode expires, server is removed from this list using server+port for key.  Will remove the server that had reported in as new ten seconds or so earlier.  Looking still for damage this causes.
;;;","07/Apr/10 06:18;streamy;Definitely need a good checkup/cleanup on where we use server+port vs server+port+startcode.  So I guess we already have the map to check if we exist when receiving a startup.  Good stuff guys.

@stack, assign to u?;;;","08/Apr/10 07:38;stack;Untested patch posted so we can discuss.  It does nothing but make code that was already in place actually work.

Patch looks at new server and if it has same host+port as a currently registered server, we reject it.  On RS side, it pauses and then retries forever.

A more involved soln. would have us run the code that is executed when znode expires but then when znode actually expires, the code will be run again and we'd have to be careful we recognized the difference between the two runs and not knock out a server that was legit.;;;","08/Apr/10 08:31;stack;As to why we failed allocate meta, looking in log I see two restarts of 019 ( 2010-03-31 17:37:39,882 and 2010-03-31 17:49:43,667).  After the first restart, all looks to be restored to normal only if you look at the emissions from load balancer it says:

 1782 2010-03-31 17:48:46,521 INFO org.apache.hadoop.hbase.master.ServerManager: 2 region servers, 0 dead, average load 9.0

So we are off kilter.   There should be 3 servers showing at this stage.  If you look at this message in src, you'll see that count is from a map keyed by host+port.  019 was removed from list as part of the processing of the crash.

So, there will be more churn of regions than there should be.

Next 020 expires at 2010-03-31 17:50:57,004.  It was carrying .META.  It expired 'naturally' w/ znode expired.  Start message comes in at '2010-03-31 17:51:15,385'.  We remove meta from online regions list so we will immediately reassign it.  Meantime we are processing a message close from 018 because balancer is working to balance the churning cluster. 

The message close can't complete because it has built in the expectation that meta is available.

Missing from this patch is review of all of these message processing items in the master package.  Need to make sure that close, open, etc., are requeued to try later if meta is null (as was the case here).

Need to figure how to write a test for this stuff.







;;;","08/Apr/10 18:02;karthik.ranga;<< A more involved soln. would have us run the code that is executed when znode expires but then when znode actually expires, the code will be run again and we'd have to be careful we recognized the difference between the two runs and not knock out a server that was legit. >>

I was thinking of something like this, not sure how easily this would translate to code:

1. In the server restart and sending a new start code, we process the shutdown of the older instance:

if (this.serverAddressToServerInfo.containsKey(hostAndPort)) {
     if(newStartCode > currentStartCode) {
        // process shutdown of current incarnation of RS
        // register new incarnation of RS
     }
}


2. On the znode expire path:

// znodeExpiredHostAndPort = ...
// znodeExpiredStartCode = ...
if(serverAddressToServerInfo.containsKey(znodeExpiredHostAndPort) && znodeExpiredStartCode >= currentStartCode) {
  // process shutdown
} else {
  // no op - this should already have been handled
}
;;;","08/Apr/10 18:30;stack;Let me redo my patch to do as Karthik suggests.  Chatting on IRC, his suggestion would be effectivey immune from clock drift as long as the restart of the RS was done on the same host (the problem comes of same host+port combo so exception would be the unlikely new machine w/ same host+port coming up before old host's znode had expired).  Will make a separate issue to review processing open, close, etc. to ensure alls well for these processings when meta is offline (an offline meta while processing a close is what killed this cluster).

Moving this issue to 0.20.5.;;;","12/Apr/10 21:09;stack;Here is a cleaner patch.  Changed server expired to only expire servers it has registered.;;;","12/Apr/10 21:14;stack;Here is more on the patch.

+ If server we've not seen before comes in and has a startcode > startcode for currently registered server of same host+port, then call serverExpire (synchronized).
+ When the zk watcher is triggered, we call serverExpire only now serverExpire first checks server is registered and not on deadServers list before it proceeds.

Need to add tests.  Patch posted for review.;;;","05/May/10 06:26;stack;This patch adds a unit test and updates v3 of the patch to match whats now in branch.  The test fails even with the ""fix"" in place; meta is never redeployed.  Fix must not be complete.  More work to do.;;;","05/May/10 23:50;stack;Still not done.  Jiggering test so can have two filesystems in place so another can grab lease of former to recover files of *killed* regionserver.;;;","07/May/10 06:47;stack;Attached is my v7 patch.  I've also posted it here, http://review.hbase.org/r/36/, into Todds fancy-pants new code review tool (which is excellent).  Please review.;;;","07/May/10 07:03;stack;All tests passed locally.;;;","07/May/10 07:13;tlipcon;Posted a review here:
http://review.hbase.org/r/36/#review1
(JIRA integration forthcoming in the next couple weeks, I suppose!);;;","07/May/10 18:35;stack;This patch includes fixes for all review comments made by Todd over in http://review.hbase.org/r/36/.  I'm not done yet.  I want to add more tests.  This is just interrim version.;;;","07/May/10 21:55;stack;Main changes from previous patch are implementation of Todd suggestions. 
Below we note changes other than those suggested by Todd.
Also uploaded patch to review.hbase.org.

M  src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java
  In here, if RS constructor, we conjure a new Configuration instance
  that has in it a differnent user so that later when we get a
  filesystem, then we get a filesystem instance per regionserver instance.
  (getLiveRegionServerThreads): Return live regionservers.
A  src/test/org/apache/hadoop/hbase/master/TestServerManager.java
  Add small junit test for isDead method.
A src/test/org/apache/hadoop/hbase/master/TestMasterTransitions.java
  Renamed from ...
D src/test/org/apache/hadoop/hbase/master/TestMasterTransistions.java
  This misspelled name.
M src/java/org/apache/hadoop/hbase/HServerInfo.java
  Redid javaodc.  Changed toString output.  Changed getName to getHostName
M src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
  Added getLiverRegionServers
M src/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java
  Formatting changes.
M src/java/org/apache/hadoop/hbase/master/HMaster.java
  getName changed to getHostName.
M  src/java/org/apache/hadoop/hbase/HServerAddress.java
  Javadoc cleanup.;;;","07/May/10 22:02;stack;Pushed v9 to http://review.hbase.org/r/36/.  Tests look like they are all passing (not done yet).;;;","08/May/10 05:12;stack;This patch is the result of two rounds of tlipcon review over on http://review.hbase.org/r/36/;;;","08/May/10 05:30;tlipcon;+1, assuming it passes tests;;;","08/May/10 06:28;stack;Applied branch and trunk.  Thanks for the double-review over in http://review.hbase.org/r/36/ Todd.;;;","12/May/10 23:54;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spurious warnings from util.Sleeper,HBASE-2410,12461182,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,apurtell,apurtell,05/Apr/10 22:09,20/Nov/15 12:43,14/Jul/23 06:06,05/Apr/10 23:41,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Many of this in the logs

{noformat}
WARN util.Sleeper: We slept 59993ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://wiki.apache.org/hadoop/Hbase/Troubleshooting#A9
{noformat}

except this is the MetaScanner and the sleeper has a period of 60000ms. I have one of 30000ms in Stargate which is doing the same thing now.

I see the intent but these spurious warnings defeat the intent here. Marked as blocker as this will cause no end of users writing in on the list. Should be a test of the actual sleep time against the configured interval, not a constant.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/10 23:02;jdcryans;HBASE-2410.patch;https://issues.apache.org/jira/secure/attachment/12440807/HBASE-2410.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26294,Reviewed,,,,Fri Nov 20 12:43:30 UTC 2015,,,,,,,,,,"0|i0hhmn:",100118,,,,,,,,,,,,,,,,,,,,,"05/Apr/10 22:16;jdcryans;Oh geez my bad. I wanted to lower the value that makes us print out the message but I didn't think about the master with long running sleeps. I guess we need to go back to a multiplier.;;;","05/Apr/10 23:02;jdcryans;The condition to print the message is now 2 times the sleep period. I'm also adding the time we should have slept. Sounds good?;;;","05/Apr/10 23:32;stack;J-D and I chatted.  2x is too little if sleep of one second.  He's going to add minimum of ten seconds and 2x (10 second GC pause is worth noting).;;;","05/Apr/10 23:41;jdcryans;I committed to branch and trunk a fix that checks for a delta of 10 seconds between the time slept and the period.;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Close, split, open of regions in RegionServer are run by a single thread only.",HBASE-2405,12461053,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,stack,stack,02/Apr/10 18:47,20/Nov/15 12:42,14/Jul/23 06:06,01/Sep/10 05:26,,,,,,,,,,,,0.90.0,,,,,,,0,moved_from_0_20_5,,"JGray and Karthik observed yesterday that a regoin open message arrived at the regionserver but that the regionserver worker thread did not get around to the actually opening until 45 seconds later (region offline for 45 seconds).  We only run a single Worker thread in a regoinserver processing open, close, and splits.  In this case, a long running close (or two) held up the worker thread.  We need to run more than a single worker.  A pool of workers?  Should opens be prioritized?",,hammer,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2485,,,,,,,,,,,,,,,,,"06/Apr/10 00:20;stack;nsre.txt;https://issues.apache.org/jira/secure/attachment/12440817/nsre.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26291,,,,,Fri Nov 20 12:42:20 UTC 2015,,,,,,,,,,"0|i0hhm7:",100116,,,,,,,,,,,,,,,,,,,,,"02/Apr/10 19:25;apurtell;Multithreaded open and close seems a good idea. Maybe not for splits and compactions, or keep two worker pools. Former is good for liveness with minimal impact on HDFS. Latter can have high impact on HDFS, e.g. 20 servers (+DNs) with e.g. 4 split/compact threads each is 160 concurrent streaming reads or writes when a table is major compacted. ;;;","02/Apr/10 21:19;streamy;Thanks for opening this stack.

I'm unsure if we really need multi-threaded Workers to process the messages, or if we should just have the worker we have now be able to do things more asynchronously.  Right now it's bad that we allow any compacting outside of the CompactSplitThread, we should be shuffling everything through there so that we respect compaction limits.  I'm thinking we need some way to allow things like compactions to know that they need to trigger some other action once completed.  Maybe the compaction requests could carry some reference to an optional callback method... seems like there's a bunch of cool stuff we could do with that.

I'm worried about introducing parallelism where we don't currently have it so think we should try to avoid it where possible.  As long as we don't block in the thread processing messages, should not be a problem.

Stack, how does this relate to possible changes from the master rewrite.  If the regionservers are no longer just processing a message queue coming from the master, and instead are processing events triggered by zk watchers and such, maybe a move towards parallelism in message processing is necessary anyways.  Thoughts?;;;","02/Apr/10 21:37;stack;@Andrew I like two queues idea; liveness queue and background-task queue. We'd have multiple threads servicing the liveness queue but perhaps the single worker doing the compactions and splits queue.

@Jon Yes, all should go via compactsplitthread.  Lets fix that and make even manual requests for compactions go this route.  Understood about the worry introducing more parallellism but ain't sure how else to do it if you don't want to ""block the thread processing the message"" (even if we redo close so we dbl-flush, it'll still take the same amount of elapsed time to close, it'll just be taking writes longer before it puts up the close flag).  And good point that close, open, etc., will come in via callbacks though I envisioned the callback would just add to the aforementioned queues and then return -- not wait on close/open to complete.;;;","02/Apr/10 23:12;streamy;The way we prevent blocking is by not doing anything long-running in the thread that processes messages.  If it's a close, he can asynchronously trigger the close to happen (into compaction queue w/ a callback perhaps).

Or we have a process thread that handles all incoming messages, decides what to do and hands that off to a set of workers or something.  Anyways, I'm not against adding worker pools just want to think about all possible designs.;;;","06/Apr/10 00:20;stack;I'm going to put up two executor pools -- one with a single thread to do the old QUIESCE, SPLIT, and COMPACTs and then another pool with perhaps 4 or 5 threads to do concurrent OPEN and CLOSEs.

Attached patch cleans up the ugly NSRE thread dump in the RS log.  Will roll that into this patch.;;;","06/Apr/10 00:34;streamy;Do we need to shuffle open/close compactions and flushes via a shared thread pool w/ current compactions and flushes?  This kind of thing needs to be configurable and strictly enforced imo;;;","06/Apr/10 04:37;stack;So worker handles stuff like master ordained OPENs, CLOSEs, QUESCE's as well as SPLITs, COMPACTs, and FLUSHEs.  The latter are user ordained but are passed out as events by the master.   I looked at them and SPLITs and COMPACTIONs are just queued. A compaction/split requested is called on the compactSplitThread.  FLUSH is run inline.

What we want to do here is make it so a long-running close (or open) doesn't hinder other OPENs, the phenomenon observed by Karthik.  The way around this is to run each open/close in its own thread.  You cool w/ that JGray?;;;","06/Apr/10 16:31;streamy;The issue is that an OPEN and CLOSE can/will trigger compactions and flushes.  These should not be done inline, they should be queued like user triggered events are.  That's where the new complexity comes in.

On OPEN, if the region contains references, will be compacted.  Does this currently happen inline in the worker?  I think that might be queued.  If not, it should be.

On CLOSE, we flush the region.  This flush should be turned into a double-flush and done through a separate thread pool that can be configured w/ a max concurrent flushes.

I don't want to run opens/closes in their own threads because then you don't have easy ways to configure how much you want to use HDFS at once.  maxConcurrentCompactions and maxConcurrentFlushes should (exist and) be strictly followed.;;;","06/Apr/10 16:38;apurtell;bq. maxConcurrentCompactions and maxConcurrentFlushes should (exist and) be strictly followed

+1 

but from my point of view it would be fine if that can be accomplished along with multithreaded OPENs and CLOSEs.



      

;;;","07/Apr/10 23:53;stack;Moving to 0.20.5 but also marking it critical.  Being single-threaded only makes onlining/offlining of regions take longer than need be.;;;","12/May/10 23:48;stack;Bulk move of 0.20.5 issues into 0.21.0 after vote that we merge branch into TRUNK up on list.;;;","13/May/10 05:27;streamy;Going to look at this as part of HBASE-2485;;;","02/Jun/10 00:07;stack;Just to say I ran into a recent issue where a region that was taking a very long time to open -- bug -- blocked the opening of all those behind it.;;;","07/Jun/10 22:28;posix4e;Does distributed log splitting change what we should do here?;;;","25/Aug/10 00:08;jdcryans;Jon, do you still want to do this as part of HBASE-2485 in the scope of 0.90? IMO we already have enough.;;;","25/Aug/10 01:13;streamy;this is for free with the new master, already implemented in the branch.  opens/closes are multi-threaded.;;;","01/Sep/10 05:26;stack;Opens and closes are done by executors now.    By default regionservers have 3 opener handlers (with one extra each for meta and root).   Configuration can up or down the number.  Splits are not yet done by executors nor compactors and flushers but thats coming in a different issue.

Closing as fixed by HBASE-2692;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[stargate] set maxVersions on gets,HBASE-2402,12460980,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,apurtell,apurtell,apurtell,01/Apr/10 23:33,12/Oct/12 06:14,14/Jul/23 06:06,01/Apr/10 23:36,,,,,,,,,,,,0.20.4,0.90.0,,,,,,0,,,"Support setting maxVersions on get. Append query parameter ""v"" to the end of resource URIs on GETs, e.g. ""?v=3"" for max 3 versions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/10 23:35;apurtell;HBASE-2402.patch;https://issues.apache.org/jira/secure/attachment/12440554/HBASE-2402.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26290,,,,,Thu Apr 01 23:36:00 UTC 2010,,,,,,,,,,"0|i08sxr:",49288,,,,,,,,,,,,,,,,,,,,,"01/Apr/10 23:36;apurtell;Committed to trunk and 0.20 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forced splits only act on the first family in a table,HBASE-2399,12460959,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,mingma,streamy,streamy,01/Apr/10 16:47,20/Nov/15 12:40,14/Jul/23 06:06,19/Aug/11 04:02,0.20.3,,,,,,,,,,,0.92.0,,regionserver,,,,,0,moved_from_0_20_5,,"While working on a patch for HBASE-2375, I came across a few bugs in the existing code related to splits.

If a user triggers a manual split, it flips a forceSplit boolean to true and then triggers a compaction (this is very similar to my current implementation for HBASE-2375).  However, the forceSplit boolean is flipped back to false at the beginning of Store.compact().  So the force split only acts on the first family in the table.  If that Store is not splittable for some reason (it is empty or has only one row), then the entire region will not be split, regardless of what is in other families.

Even if there is data in the first family, the midKey is determined based solely on that family.  If it has two rows and the next family has 1M rows, we pick the split key based on the two rows.",,davelatham,erwin00776,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2375,,,,,,,,,,,,,,,,,,,"01/Apr/10 16:49;streamy;HBASE-2399-test-v1.patch;https://issues.apache.org/jira/secure/attachment/12440525/HBASE-2399-test-v1.patch","13/Aug/11 01:21;mingma;HBASE-2399-trunk.patch;https://issues.apache.org/jira/secure/attachment/12490312/HBASE-2399-trunk.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26288,,,,,Fri Nov 20 12:40:40 UTC 2015,,,,,,,,,,"0|i0hhlj:",100113,,,,,,,,,,,,,,,,,,,,,"01/Apr/10 16:49;streamy;Adds a second test to force split that has an empty family first and the full family second.  Test fails.;;;","01/Apr/10 16:50;streamy;Will have a patch for HBASE-2375 that includes a fix for this.;;;","01/Apr/10 18:45;streamy;The posted patch is itself broken, but turns out that the existing test doesn't actually fail if the table doesn't split.  It waits around for the split, but even if it goes past the max loops, it doesn't actually ever fail any assertions.

I'm also fixing the test itself in my HBASE-2375 patch.  More to come.;;;","01/Apr/10 20:42;hbasebuild;Dumb bug.

;;;","01/Apr/10 22:16;stack;Sorry, above comment was from me.;;;","05/Apr/10 17:37;streamy;HBASE-2375 patch includes test and fix for this.;;;","12/May/10 23:48;stack;Bulk move of 0.20.5 issues into 0.21.0 after vote that we merge branch into TRUNK up on list.;;;","25/Aug/10 00:02;jdcryans;Do we have enough time do this if 0.90.0 is due for HW? Punt?;;;","25/Aug/10 01:07;streamy;This fix is fairly trivial.  I held it up because of larger changes to splits for HBASE-2375.  If that jira ends up getting punted then I will implement a simple fix for this jira for 0.90.;;;","05/Oct/10 20:53;streamy;Punting to 0.92 for now;;;","11/Aug/11 17:41;mingma;Jonathan, if you aren't looking this, I can investigate this more. It looks like in trunk the issue  of ""if the first family isn't splittable, then whole region isn't split"" has been fixed as part of split/compaction refactoring. The issue about ""using first splittable family's midkey for the whole region"" is still there. It can be fixed by picking the midkey of the family with largest store files size.;;;","12/Aug/11 03:51;yuzhihong@gmail.com;@Ming:
You can assign this to yourself since you know what to do.;;;","12/Aug/11 22:59;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1484/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

1. Add tests for forcesplit multi-column-family scenarios.
2. Modify HRegion so that it picks splitpoint based on largest store, instead of the first splittable store. It applies to both forcesplit and automatic split.


This addresses bug hbase-2399.
    https://issues.apache.org/jira/browse/hbase-2399


Diffs
-----

  http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1157283 
  http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 1157283 
  http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java 1157283 

Diff: https://reviews.apache.org/r/1484/diff


Testing
-------


Thanks,

Ming

;;;","13/Aug/11 00:35;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1484/#review1437
-----------------------------------------------------------

Ship it!



http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java
<https://reviews.apache.org/r/1484/#comment3314>

    Remove whitespace.



http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java
<https://reviews.apache.org/r/1484/#comment3316>

    This was used for debugging, right ?
    We may not need it.


- Ted


On 2011-08-12 22:58:55, Ming Ma wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1484/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-12 22:58:55)
bq.  
bq.  
bq.  Review request for hbase.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  1. Add tests for forcesplit multi-column-family scenarios.
bq.  2. Modify HRegion so that it picks splitpoint based on largest store, instead of the first splittable store. It applies to both forcesplit and automatic split.
bq.  
bq.  
bq.  This addresses bug hbase-2399.
bq.      https://issues.apache.org/jira/browse/hbase-2399
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1157283 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 1157283 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java 1157283 
bq.  
bq.  Diff: https://reviews.apache.org/r/1484/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Ming
bq.  
bq.

;;;","13/Aug/11 01:21;mingma;Fix the issues raised by Ted.;;;","13/Aug/11 01:43;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1484/#review1438
-----------------------------------------------------------



http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
<https://reviews.apache.org/r/1484/#comment3332>

    whitespace



http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java
<https://reviews.apache.org/r/1484/#comment3333>

    whitespace here and below


- Jonathan


On 2011-08-12 22:58:55, Ming Ma wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1484/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-12 22:58:55)
bq.  
bq.  
bq.  Review request for hbase.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  1. Add tests for forcesplit multi-column-family scenarios.
bq.  2. Modify HRegion so that it picks splitpoint based on largest store, instead of the first splittable store. It applies to both forcesplit and automatic split.
bq.  
bq.  
bq.  This addresses bug hbase-2399.
bq.      https://issues.apache.org/jira/browse/hbase-2399
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1157283 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 1157283 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java 1157283 
bq.  
bq.  Diff: https://reviews.apache.org/r/1484/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Ming
bq.  
bq.

;;;","13/Aug/11 03:31;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1484/#review1441
-----------------------------------------------------------

Ship it!


+1 after fixing the white space (can you make a new patch Ming)  Good stuff.


http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<https://reviews.apache.org/r/1484/#comment3335>

    Nice javadoc



http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java
<https://reviews.apache.org/r/1484/#comment3336>

    Yeah, there is more in here... you can see it up here in review board ming.



http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java
<https://reviews.apache.org/r/1484/#comment3337>

    Nice test.


- Michael


On 2011-08-12 22:58:55, Ming Ma wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1484/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-12 22:58:55)
bq.  
bq.  
bq.  Review request for hbase.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  1. Add tests for forcesplit multi-column-family scenarios.
bq.  2. Modify HRegion so that it picks splitpoint based on largest store, instead of the first splittable store. It applies to both forcesplit and automatic split.
bq.  
bq.  
bq.  This addresses bug hbase-2399.
bq.      https://issues.apache.org/jira/browse/hbase-2399
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1157283 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 1157283 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java 1157283 
bq.  
bq.  Diff: https://reviews.apache.org/r/1484/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Ming
bq.  
bq.

;;;","15/Aug/11 17:59;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1484/
-----------------------------------------------------------

(Updated 2011-08-15 17:58:09.305302)


Review request for hbase.


Changes
-------

Thanks, folks. Here is the patch that fixes the whitespace issues. Should I upload the patch to jira?


Summary
-------

1. Add tests for forcesplit multi-column-family scenarios.
2. Modify HRegion so that it picks splitpoint based on largest store, instead of the first splittable store. It applies to both forcesplit and automatic split.


This addresses bug hbase-2399.
    https://issues.apache.org/jira/browse/hbase-2399


Diffs (updated)
-----

  http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1157283 
  http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 1157283 
  http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java 1157283 

Diff: https://reviews.apache.org/r/1484/diff


Testing
-------


Thanks,

Ming

;;;","15/Aug/11 22:07;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/1484/#review1462
-----------------------------------------------------------

Ship it!


- Ted


On 2011-08-15 17:58:09, Ming Ma wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/1484/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-08-15 17:58:09)
bq.  
bq.  
bq.  Review request for hbase.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  1. Add tests for forcesplit multi-column-family scenarios.
bq.  2. Modify HRegion so that it picks splitpoint based on largest store, instead of the first splittable store. It applies to both forcesplit and automatic split.
bq.  
bq.  
bq.  This addresses bug hbase-2399.
bq.      https://issues.apache.org/jira/browse/hbase-2399
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 1157283 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 1157283 
bq.    http://svn.apache.org/repos/asf/hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java 1157283 
bq.  
bq.  Diff: https://reviews.apache.org/r/1484/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Ming
bq.  
bq.

;;;","16/Aug/11 01:16;yuzhihong@gmail.com;Integrated to TRUNK.

Thanks for the patch Ming.

Thanks for the review Michael.

This patch is only for TRUNK. So not attaching the latest patch is Okay.;;;","16/Aug/11 05:44;hudson;Integrated in HBase-TRUNK #2117 (See [https://builds.apache.org/job/HBase-TRUNK/2117/])
    HBASE-2399  Forced splits only act on the first family in a table (Ming Ma)

tedyu : 
Files : 
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java
* /hbase/trunk/CHANGES.txt
;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in HLog.append when calling writer.getLength,HBASE-2398,12460866,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kannanm,jdcryans,jdcryans,31/Mar/10 22:12,20/Nov/15 12:40,14/Jul/23 06:06,01/Apr/10 23:01,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Doing some test uploads on a fresh single node setup, I'm seeing non-fatal NPEs in the region server log coming from HLog.append when calling writer.getLength. I'll post a log in a follow-up comment.

Assigning to Nicolas so that he takes a look.",,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2359,,,,,,,,,,,,,"01/Apr/10 00:53;kannanm;2398_0.20_v1.txt;https://issues.apache.org/jira/secure/attachment/12440430/2398_0.20_v1.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26287,Reviewed,,,,Fri Nov 20 12:40:30 UTC 2015,,,,,,,,,,"0|i0hhlb:",100112,,,,,,,,,,,,,,,,,,,,,"31/Mar/10 22:13;jdcryans;Here's a stack trace of the problem:

{code}
2010-03-31 15:10:13,761 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: 
java.lang.NullPointerException
	at org.apache.hadoop.io.SequenceFile$Writer.getLength(SequenceFile.java:1048)
	at org.apache.hadoop.hbase.regionserver.HLog.append(HLog.java:750)
	at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1581)
	at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1356)
	at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1324)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1789)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.multiPut(HRegionServer.java:2529)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
2010-03-31 15:10:13,763 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 22 on 60020, 
call multiPut(org.apache.hadoop.hbase.client.MultiPut@5d363c50) from 10.10.1.177:59822: error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:881)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:871)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1798)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.multiPut(HRegionServer.java:2529)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.io.SequenceFile$Writer.getLength(SequenceFile.java:1048)
	at org.apache.hadoop.hbase.regionserver.HLog.append(HLog.java:750)
	at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1581)
	at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1356)
	at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1324)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1789)
	... 6 more
{code};;;","31/Mar/10 22:20;kannanm;Seems related to my fix for HBASE-2359. I can take a look.;;;","31/Mar/10 22:56;nspiegelberg;Troubleshooted with Kannan.  HBASE-2359 introduced a concurrency issue with HLog.writer being accessed outside HLog.updateLock.  Kannan is working on the fix... ;;;","31/Mar/10 23:14;kannanm;Nicolas spotted that there is a race condition now because the this.writer.getLength() is not done under the updatelock, and a log roll might be happening on the side. 

During the log roll, there is a brief window where a call to this.writer.close() has happened, and the new writer hasn't been allocated. 

If SequenceFile.java:getLength() checked for out being NULL and returned zero, that would work. That fix should probably be done too to hadoop.

To work with older versions of hadoop which do not have this patch, Nicolas suggested the moving the log roll logic to hflush()-- there we are already under the updateLock. And that seems like a good centralized place to decide on log rolls as well. 






;;;","31/Mar/10 23:18;tlipcon;If we move the log roll to hflush(), will it actually perform the roll or just trigger it? It seems bad to put an HDFS create() call on the critical path to a read.;;;","31/Mar/10 23:19;tlipcon;Er, sorry... above should read: ""... on the critical path to a WRITE"". :);;;","31/Mar/10 23:26;nspiegelberg;Just triggers a log roll.  The current code snippet (which will remain unchanged) is

      if (listener != null) {
        listener.logRollRequested();
      }
;;;","31/Mar/10 23:26;jdcryans;@ Kannan, would make sense.

@ Todd, it's async.;;;","31/Mar/10 23:29;tlipcon;cool, sounds good to me then!;;;","01/Apr/10 00:53;kannanm;patch uploaded.

Was able to repro the problem prior to doing the fix on a local setup using a multi-threaded client which load lots of rows. I did see the occasional non-fatal NPE in the logs. The NPEs went away after the fix.

Running ant test right now.;;;","01/Apr/10 01:09;kannanm;ant test completed successfully.;;;","01/Apr/10 01:10;kannanm;fyi... Jonathan/Nicolas have reviewed this. 

;;;","01/Apr/10 23:01;stack;Committed to branch (and to trunk though it doesn't really make sense there yet until hbase-2401 goes in).  Thanks for the patch Kannan.;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bytes.toStringBinary escapes printable chars,HBASE-2397,12460858,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,apurtell,apurtell,apurtell,31/Mar/10 21:34,12/Oct/12 06:15,14/Jul/23 06:06,06/Jun/10 08:40,,,,,,,,,,,,0.20.5,0.90.0,,,,,,0,,,"Bytes.toStringBinary hex-escapes printable chars such as '@', '$', '#', '%', '&', '*', '(', ')', '{', '}', '[', ']', ';', ',', '~', '|'. Why?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/10 07:42;apurtell;HBASE-2397.patch;https://issues.apache.org/jira/secure/attachment/12446433/HBASE-2397.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26286,Incompatible change,,,,Sat Jun 12 00:16:10 UTC 2010,,,,,,,,,,"0|i08sl3:",49231,,,,,,,,,,,,,,,,,,,,,"07/Apr/10 23:24;stack;It shouldn't.  Any chance of a patch Andrew?;;;","12/May/10 23:48;stack;Bulk move of 0.20.5 issues into 0.21.0 after vote that we merge branch into TRUNK up on list.;;;","06/Jun/10 08:40;apurtell;Committed. ;;;","06/Jun/10 09:02;ryanobjc;Why are incompatible changes going into branch? Also the code was that way
to make regexp of regions in the logs easier... also possibly one other
reason as well.

https://issues.apache.org/jira/browse/HBASE-2397?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel]
'%', '&', '*', '(', ')', '{', '}', '[', ']', ';', ',', '~', '|'. Why?
;;;","06/Jun/10 14:48;apurtell;I flagged that out of an abundance of caution. Its an incompatable change if anyone is relying on it to produce row keys. But I wonder about that.

Change it if you don't like it. This jira was in place for a long time with plenty of time for comment.

I also debated if such a minor code change needed time for review. For that, I may have been wrong. From now on I'll put up even one liners on review board and wait for an ack before doing anything.


;;;","07/Jun/10 16:04;stack;.bq I also debated if such a minor code change needed time for review. For that, I may have been wrong. From now on I'll put up even one liners on review board and wait for an ack before doing anything.

Please do not go to such an extreme Andrew.  IMO, you did nothing 'wrong' going ahead and committing a small fix for an issue that has been hanging out there for a good month and more w/o a response to your query (I was of your opinion until Ryan explained the why -- see my remark of April 7th above).  I also agree w/ your caution flagging the item as incompatible change.

@Ryan, please make an issue detailing what of the above to revert.;;;","07/Jun/10 20:22;ryanobjc;my only concern was putting too much into branch - 100 bug fixes per minor release is silly.

as for the other printables, well originally I limited the set to make regexps easier, and possibly another reason.;;;","12/Jun/10 00:16;jdcryans;This was committed to branch and 0.20.5 is still in RC, so I have to include it in the new one I'm rolling out.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableServers.isMasterRunning() skips checking master status if master field is not null,HBASE-2391,12460608,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,ted_yu,ted_yu,29/Mar/10 21:34,20/Nov/15 12:41,14/Jul/23 06:06,01/Sep/10 18:41,0.20.3,,,,,,,,,,,0.90.0,,Client,,,,,0,,,"We call HBaseAdmin.isMasterRunning()  to see if client has connection with HBase.

While going over TableServers.isMasterRunning() in HConnectionManager, I see this:
    public boolean isMasterRunning() {
      if (this.master == null) {
        try {
          getMaster();
         
        } catch (MasterNotRunningException e) {
          return false;
        }
      }
      return true;
    }
When isMasterRunning() is called the first time, if master is obtained successfully, master field would contain reference to HMasterInterface. Subsequent calls to isMasterRunning() wouldn't throw MasterNotRunningException even if master server stops responding to clients.

I think master.isMasterRunning() should be called if master isn't null.

J-D pointed out that:
I think this method wasn't updated when we moved to Zookeeper (since
in pre-0.20, dead master = dead cluster), also looking at when this is
called, I only see it from HMerge and HBaseAdmin.isMasterRunning()...
which in turn isn't called anywhere in the java code (I think we call
it in the shell tho).
",CentOS,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26285,,,,,Fri Nov 20 12:41:30 UTC 2015,,,,,,,,,,"0|i0hhjz:",100106,,,,,,,,,,,,,,,,,,,,,"01/Sep/10 18:41;stack;Fixed by hbase-2692.  It does this now:

{code}
    public boolean isMasterRunning()
    throws MasterNotRunningException, ZooKeeperConnectionException {
      if (this.master == null) {
        getMaster();
      }
      boolean isRunning = master.isMasterRunning();
      if(isRunning) {
        return true;
      }
      throw new MasterNotRunningException();
    }
{code};;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTable - delete / put unnecessary sync. ,HBASE-2389,12460582,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,kaykay.unique,kaykay.unique,29/Mar/10 18:20,20/Nov/15 12:41,14/Jul/23 06:06,30/Mar/10 05:15,,,,,,,,,,,,0.90.0,,,,,,,0,,,"HTable is not thread-safe , but some of the methods seem to have a synchronized block  (put/delete) etc. 

It might as well be better to remove them altogether. 
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/10 18:27;kaykay.unique;HBASE-2389.patch;https://issues.apache.org/jira/secure/attachment/12440088/HBASE-2389.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26284,Reviewed,,,,Fri Nov 20 12:41:50 UTC 2015,,,,,,,,,,"0|i0hhjj:",100104,,,,,,,,,,,,,,,,,,,,,"30/Mar/10 05:15;stack;Applied branch and trunk.  Thanks for the patch Kay Kay.;;;","12/May/10 23:54;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bulk insert with multiple reducers broken due to improper ImmutableBytesWritable comparator,HBASE-2378,12460276,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,tlipcon,lansa,lansa,25/Mar/10 19:24,12/Oct/12 06:15,14/Jul/23 06:06,30/Mar/10 05:36,0.20.3,,,,,,,,,,,0.20.4,0.90.0,mapreduce,,,,,0,,,"If I run MR to prepare HFIles with more than one reducer then some values for keys are not appeared in the table after loadtable.rb script execution. With one reducer everything works fine.

References:
http://hadoop.apache.org/hbase/docs/r0.20.3/api/org/apache/hadoop/hbase/mapreduce/package-summary.html#bulk
- the row id must be formatted as a ImmutableBytesWritable
- MR job should ensure a total ordering among all keys

MAPREDUCE-366  (patch-5668-3.txt)
- TotalOrderPartitioner that uses the new API (attached)

HBASE-2063
- patched HFileOutputFormat (attached)

Input data (attached):
* my_sample_log_1k.txt - sample data, input for MyHFilesWriter

Source (attached):
* MyKeyComparator.java - comparator for my ImmutableBytesWritable keys
* TestTotalOrderPartitionerForMyKeys.java - test case for my keys (note that I've set up MyKeyComparator to pass that test)
* MyHFilesWriter.java	 - My MR job to prepare HFiles
* HFileOutputFormat.java - from MAPREDUCE-366
* TotalOrderPartitioner.java - from MAPREDUCE-366
* MySampler.java - My RandomSampler based on Sampler from MAPREDUCE-366 BUT I've put the following string into getSample method (without that string it doesn't work):
{code}
            reader.initialize(splits.get(i), new TaskAttemptContext(job.getConfiguration(), new TaskAttemptID()));
{code}


Test case:
# comment the following string in MyHFilesWriter: //job.setSortComparatorClass(MyKeyComparator.class);
# hadoop jar keyvalue-poc.jar MyHFilesWriter -in /test_hbase/my_sample_log_1k.txt -out /test_hbase/hfiles/01/ -r 1
# hadoop jar keyvalue-poc.jar MyHFilesWriter -in /test_hbase/my_sample_log_1k.txt -out /test_hbase/hfiles/02/ -r 2
# hbase> create 'tst_hfiles_01', {NAME => 'vals'}
# hbase> create 'tst_hfiles_02', {NAME => 'vals'}
# hbase org.jruby.Main /usr/lib/hbase-0.20/bin/loadtable.rb tst_hfiles_01 /test_hbase/hfiles/01
# hbase org.jruby.Main /usr/lib/hbase-0.20/bin/loadtable.rb tst_hfiles_02 /test_hbase/hfiles/02
# check values for keys
# uncomment the following string in MyHFilesWriter: //job.setSortComparatorClass(MyKeyComparator.class);
# hadoop jar keyvalue-poc.jar MyHFilesWriter -in /test_hbase/my_sample_log_1k.txt -out /test_hbase/hfiles/03/ -r 2

for example, results:
{code}
hbase(main):006:0* count 'tst_hfiles_01', 100 
Current count: 100, row: 0.14.USA.IL.602.ELMHURST.1.1.0.0                                                     
Current count: 200, row: 0.245.USA.ME.500.PORTLAND.1.1.0.0                                                    
Current count: 300, row: 0.34.USA.FL.Rollup.Rollup.1.1.0.0                                                    
Current count: 400, row: 0.443.USA.CA.803.LOS.ANGELES.1.1.0                                                   
Current count: 500, row: 0.8.USA.CO.751.CASTLE.ROCK.1.1.0                                                     
Current count: 600, row: 1.14.DZA.Rollup.Rollup.Rollup.1.1.0.1                                                
Current count: 700, row: 1.159.SWE.AB.Rollup.Rollup.1.1.0.1                                                   
Current count: 800, row: 1.17.USA.TN.659.CLARKSVILLE.1.1.0.1                                                  
Current count: 900, row: 1.220.USA.MI.505.SOUTHFIELD.1.1.0.1                                                  
999 row(s) in 0.0930 seconds
hbase(main):007:0> count 'tst_hfiles_02', 100
Current count: 100, row: 0.231.USA.GA.524.BUFORD.1.1.0.1                                                      
Current count: 200, row: 0.4.USA.VA.573.Rollup.1.1.0.0                                                        
Current count: 300, row: 0.9.ROU.B.-1.BUCHAREST.1.1.0.0                                                       
Current count: 400, row: 1.16.USA.IA.679.Rollup.1.1.1.0                                                       
Current count: 500, row: 1.245.NOR.03.-1.OSLO.1.1.0.0                                                         
Current count: 600, row: 0.245.GBR.ENG.826005.BEXLEY.1.1.0.1                                                  
Current count: 700, row: 0.48.GBR.ENG.826027.Rollup.1.1.0.1                                                   
Current count: 800, row: 1.14.SWE.Rollup.Rollup.Rollup.1.1.0.1                                                
Current count: 900, row: 1.201.GBR.ENG.826005.LONDON.1.1.0.1                                                  
999 row(s) in 0.1630 seconds
hbase(main):008:0> get 'tst_hfiles_01', '0.14.USA.IL.602.ELMHURST.1.1.0.0'
COLUMN                       CELL                                                                             
 vals:key0                   timestamp=1269542753914, value=0                                                 
 vals:key1                   timestamp=1269542753914, value=14                                                
 vals:key2                   timestamp=1269542753914, value=USA                                               
 vals:key3                   timestamp=1269542753914, value=IL                                                
 vals:key4                   timestamp=1269542753914, value=602                                               
 vals:key5                   timestamp=1269542753914, value=ELMHURST                                          
 vals:key6                   timestamp=1269542753914, value=1                                                 
 vals:key7                   timestamp=1269542753914, value=1                                                 
 vals:key8                   timestamp=1269542753914, value=0                                                 
 vals:key9                   timestamp=1269542753914, value=0                                                 
 vals:val0                   timestamp=1269542753914, value=2                                                 
11 row(s) in 0.0160 seconds
hbase(main):009:0> get 'tst_hfiles_02', '0.14.USA.IL.602.ELMHURST.1.1.0.0'
COLUMN                       CELL                                                                             
0 row(s) in 0.0220 seconds
{code}

with MyKeyComparator
{code}
java.io.IOException: Added a key not lexically larger than previous key=.103.FRA.V.-1.LYON.1.1.0.0valskey0'XXX, lastkey=1.20.USA.AOL.0.AOL.1.1.0.0valsval0'XXX
	at org.apache.hadoop.hbase.io.hfile.HFile$Writer.checkKey(HFile.java:551)
	at org.apache.hadoop.hbase.io.hfile.HFile$Writer.append(HFile.java:513)
	at org.apache.hadoop.hbase.io.hfile.HFile$Writer.append(HFile.java:481)
	at com.contextweb.hadoop.hbase.mapred.HFileOutputFormat$1.write(HFileOutputFormat.java:77)
	at com.contextweb.hadoop.hbase.mapred.HFileOutputFormat$1.write(HFileOutputFormat.java:49)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:508)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.hadoop.hbase.mapreduce.KeyValueSortReducer.reduce(KeyValueSortReducer.java:46)
	at org.apache.hadoop.hbase.mapreduce.KeyValueSortReducer.reduce(KeyValueSortReducer.java:35)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:566)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
{code}",,alexlod,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/10 19:26;lansa;HFileOutputFormat.java;https://issues.apache.org/jira/secure/attachment/12439814/HFileOutputFormat.java","25/Mar/10 19:26;lansa;MyHFilesWriter.java;https://issues.apache.org/jira/secure/attachment/12439815/MyHFilesWriter.java","25/Mar/10 19:27;lansa;MyKeyComparator.java;https://issues.apache.org/jira/secure/attachment/12439816/MyKeyComparator.java","25/Mar/10 20:10;lansa;MyReadPerformance.java;https://issues.apache.org/jira/secure/attachment/12439821/MyReadPerformance.java","25/Mar/10 19:27;lansa;MySampler.java;https://issues.apache.org/jira/secure/attachment/12439817/MySampler.java","25/Mar/10 19:27;lansa;TestTotalOrderPartitionerForMyKeys.java;https://issues.apache.org/jira/secure/attachment/12439818/TestTotalOrderPartitionerForMyKeys.java","25/Mar/10 19:28;lansa;TotalOrderPartitioner.java;https://issues.apache.org/jira/secure/attachment/12439819/TotalOrderPartitioner.java","29/Mar/10 07:19;tlipcon;hbase-2378.txt;https://issues.apache.org/jira/secure/attachment/12440043/hbase-2378.txt","25/Mar/10 19:26;lansa;my_sample_log_1k.txt;https://issues.apache.org/jira/secure/attachment/12439813/my_sample_log_1k.txt",,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26282,Incompatible change,Reviewed,,,Mon Apr 26 16:06:45 UTC 2010,,,,,,,,,,"0|i08slz:",49235,,,,,,,,,,,,,,,,,,,,,"25/Mar/10 20:10;lansa;Attached is the read test (MyReadPerformance) to check empty vals:

* tst_hfiles_01 (prepared with one reducer)
{code}
$ hadoop jar keyvalue-poc.jar MyReadPerformance -in /test_hbase/my_sample_log_1k.txt -out /test_hbase/res/01/ -table tst_hfiles_01
...
10/03/25 23:16:49 INFO mapred.JobClient:   MyReadPerformance$Counters
10/03/25 23:16:49 INFO mapred.JobClient:     TOTAL_READ_ROWS=999
10/03/25 23:16:49 INFO mapred.JobClient:     BAD_RECORDS=1
10/03/25 23:16:49 INFO mapred.JobClient:     TOTAL_READ_TIME=72
10/03/25 23:16:49 INFO mapred.JobClient:   Job Counters 
...
{code}

* tst_hfiles_02 (prepared with two reducers)
{code}
$ hadoop jar keyvalue-poc.jar MyReadPerformance -in /test_hbase/my_sample_log_1k.txt -out /test_hbase/res/02/ -table tst_hfiles_02
...
10/03/25 23:17:40 INFO mapred.JobClient:   MyReadPerformance$Counters
10/03/25 23:17:40 INFO mapred.JobClient:     TOTAL_READ_ROWS=482
10/03/25 23:17:40 INFO mapred.JobClient:     BAD_RECORDS=1
10/03/25 23:17:40 INFO mapred.JobClient:     TOTAL_READ_TIME=59
10/03/25 23:17:40 INFO mapred.JobClient:     NULL_VALUE=517
10/03/25 23:17:40 INFO mapred.JobClient:   Job Counters 
...
{code}
;;;","25/Mar/10 20:15;jdcryans;Assigning to Stack so that he takes a look.;;;","29/Mar/10 04:44;tlipcon;Hi Ruslan,

Looking at your MyKeyComparator class, it appears that you're incorrectly implementing the raw comparison function. The raw forms of the writables are prefixed with a length, which you're including in the comparison. The built in ImmutableBytesWritable.Comparator class already implements raw comparison, so the MyKeyComparator class shouldn't be necessary. This should explain why your _03 case is failing with an IOE.

So the question is why tst_hfiles_02 appears to count correctly but not return all the expected rows. I'll look into this some more. Thanks.;;;","29/Mar/10 07:00;tlipcon;I eventually traced this down to a bug in ImmutableBytesWritable. Writing a test case and patch now, though it's a scary one, since the fix may break other applications that rely on the broken behavior!;;;","29/Mar/10 07:19;tlipcon;Here's a fix as well as a test case. The core of the issue was that ImmutableBytesWritable, when comparing Java objects in RAM (as opposed to in byte form) counted shorter strings as less than longer strings, even if the first characters were higher.

I ran Ruslan's code and was able to successfully use the resulting table. The one modification I made was to explicitly set the sorting class to ImmutableBytesWritable.Comparator, though I think it would work without that change.;;;","29/Mar/10 07:20;tlipcon;Updating description to match the fix.;;;","30/Mar/10 05:36;stack;Committed to branch and trunk.  Thanks for the patch Todd.  Its an ""incompatible"" change but what was there previous was broke, broke, broke.  Has to be fixed.;;;","26/Apr/10 16:06;stack;Committed to 0.20_pre_durability too (0.20._pre_durability was cut before this issue's commit so 0.20_pre_durability was missing it).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove confusing log message of how ""BaseScanner GET got different address/startcode than SCAN""",HBASE-2373,12460177,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,kannanm,kannanm,24/Mar/10 23:58,12/Oct/12 06:14,14/Jul/23 06:06,30/Mar/10 06:27,0.20.3,,,,,,,,,,,0.20.4,,,,,,,0,,,"Seeing some of these errors in the HBase master's logs:

{code}
2010-03-24 16:52:17,003 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET on test1,8947030000,1269474720493 got different address than SCAN: sa=10.18.34.217:60020, serverAddress=
2010-03-24 16:52:17,003 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET on test1,8947030000,1269474720493 got different startcode than SCAN: sc=1269456397807, serverAddress=0
2010-03-24 16:52:17,018 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET on test1,8953040000,1269474720493 got different address than SCAN: sa=10.18.34.215:60020, serverAddress=
2010-03-24 16:52:17,018 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET on test1,8953040000,1269474720493 got different startcode than SCAN: sc=1269456397735, serverAddress=0
{code}

Ideas? 
",,dhruba,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26281,,,,,Thu Apr 01 06:23:33 UTC 2010,,,,,,,,,,"0|i08ssf:",49264,,,,,,,,,,,,,,,,,,,,,"25/Mar/10 00:05;jdcryans;This happens because scanners don't respect row locks so Stack at some point added this fix where we do a get after every scan in the .META. scanner to make sure we have the correct data. If not, it prints that out. Upon inspection of your master log you should see exactly what happened with that region.;;;","25/Mar/10 00:18;kannanm;So is this harmless/expected?

I will take a look at the master logs to see if there is anything interesting going one with these regions.
;;;","25/Mar/10 00:21;jdcryans;It could be harmless... or not, you should have a look yes.;;;","25/Mar/10 01:22;kannanm;JD: 

So the region had just gotten opened on a RS. The SCAN had the stale value, but the GET has the correct value. 

{code}                                                                                                                                                                                                                                                                                                                      
2010-03-24 16:52:04,977 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_SPLIT_INCLUDES_DAUGHTERS: test1,8947030000,1269465561731: Daughters; test1,8947030000,1269474720493, test1,8953040000,1269474720493 from test007.xyz.com,60020,1269456397807; 1 of 1
2010-03-24 16:52:05,027 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region test1,8947030000,1269474720493 to test007.xyz.com,60020,1269456397807
2010-03-24 16:52:05,801 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: test1,8947030000,1269474720493 from test007.xyz.com,60020,1269456397807; 1 of 1
2010-03-24 16:52:05,802 INFO org.apache.hadoop.hbase.master.RegionServerOperation: test1,8947030000,1269474720493 open on 10.18.34.217:60020
2010-03-24 16:52:05,826 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row test1,8947030000,1269474720493 in region .META.,,1 with startcode=1269456397807, server=10.18.34.217:60020
2010-03-24 16:52:17,003 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET on test1,8947030000,1269474720493 got different address than SCAN: sa=10.18.34.217:60020, serverAddress=
2010-03-24 16:52:17,003 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET on test1,8947030000,1269474720493 got different startcode than SCAN: sc=1269456397807, serverAddress=0
{code}

I guess this is indeed harmless. Please confirm. And I'll close this issue out.
;;;","25/Mar/10 19:06;saint.ack@gmail.com;Yeah. I was logging to build evidence a suspected phenomeon was  
actually happening.  Let me fix logging so it don't look like a  
problem or just remove it




On Mar 24, 2010, at 5:05 PM, ""Jean-Daniel Cryans (JIRA)"" <jira@apache.org 

;;;","27/Mar/10 00:26;kannanm;Stack: Should we close out this issue? If you are planning to change the logging, do you want to reassign to yourself?;;;","30/Mar/10 06:23;stack;Made 'subject' more specific;;;","30/Mar/10 06:27;stack;I just removed the confusing log mesage (Its in branch only).;;;","01/Apr/10 06:23;stack;Turns out there was an instance of this message in trunk also -- a little different.  I removed it too just now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"trunk, shell command 'list' does not work. tables exist and respond to other commands however",HBASE-2371,12460157,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kovyrin,ryanobjc,ryanobjc,24/Mar/10 20:26,20/Nov/15 12:40,14/Jul/23 06:06,30/May/10 18:04,,,,,,,,,,,,0.90.0,,scripts,,,,,0,,,"list show no tables, but the webui shows many tables.  describe and exists both work on specific table names that do exist.",,hammer,larsfrancke,tlipcon,tsuna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-1654,,,,,,,,,,,,,,,,,"30/May/10 15:32;kovyrin;HBASE-2371.patch;https://issues.apache.org/jira/secure/attachment/12445878/HBASE-2371.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26279,Reviewed,,,,Fri Nov 20 12:40:34 UTC 2015,,,,,,,,,,"0|i0hhgf:",100090,,,,,,,,,,,,,,,,,,,,,"23/May/10 21:01;tlipcon;Alexey: are you working on this? we need this fixed ASAP for people working with trunk.;;;","25/May/10 21:45;kovyrin;Unfortunately I've been really loaded at work for the last 1.5 months and didn't have a chance to look into this issue since then. I will look into this issue on this weekend (May 29-31). If it needs to be addressed faster. Please consider to rolling my patch back until a better version available.;;;","30/May/10 15:32;kovyrin;Fixed the 'list' command + added an optional regexp parameter to filter the output:

{quote}
hbase(main):001:0> help 'list'

COMMAND: list
          List all tables in hbase. Optional regular expression parameter could
          be used to filter the output. Examples:

            hbase> list
            hbase> list 'abc.*'

hbase(main):002:0>
{quote};;;","30/May/10 18:04;tlipcon;+1, tested that list works now and that the help output shows up.

Thanks for the fix, Alexey. Committed to trunk.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
saveVersion.sh doesnt properly grab the git revision,HBASE-2370,12460054,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,23/Mar/10 23:50,20/Nov/15 12:42,14/Jul/23 06:06,24/Mar/10 00:01,,,,,,,,,,,,0.90.0,,,,,,,0,,,saveVersion.sh runs in $BASE/core which does not have a .git directory which is what it is testing for.  Needs to test for ../.git i think,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26278,,,,,Fri Nov 20 12:42:31 UTC 2015,,,,,,,,,,"0|i0hhg7:",100089,,,,,,,,,,,,,,,,,,,,,"24/Mar/10 00:01;ryanobjc;committed to trunk - small patch to check ../.git not .git;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hbase master.jsp not rooted correct in trunk,HBASE-2369,12460052,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,ryanobjc,ryanobjc,23/Mar/10 23:45,20/Nov/15 12:41,14/Jul/23 06:06,26/May/10 08:40,,,,,,,,,,,,0.90.0,,,,,,,0,maven,,"The path seems to be /webapps/master/master.jsp not /master.jsp, plus there is no automatic redirect like there used to be.",,larsfrancke,psmith@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2491,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26277,,,,,Fri Nov 20 12:41:03 UTC 2015,,,,,,,,,,"0|i0hhfz:",100088,,,,,,,,,,,,,,,,,,,,,"24/Mar/10 08:28;psmith@apache.org;if the destination path inside the hbase-core jar should just be the root, then it really should just be a matter of relocating the files from core/src/main/resources/webapps/ up a directory (that is, so 'master' and the other directories sits as a directory under 'resources').

any ideas where the redirection happened previously?  I'm not familiar with that.;;;","24/Mar/10 08:30;psmith@apache.org;oh, hang on, I'm mistaken here.  Actually in 0.20.3 the jsp's never appeared in the jar at all, they just appeared in the assembly.

_but_, what then is the point of precompiling them during the build?  are the JSP's needed at all?  Won't the fact that the precompiled JSP class files (ala 'org/apache/hadoop/hbase/generated/master/master_jsp.class') be enough?;;;","26/Apr/10 08:27;civascu;proposed fix - replace absolute links with relative ones;;;","26/Apr/10 08:28;civascu;posted comment on wrong issue - got fooled by too many open tabs.;;;","26/May/10 08:40;stack;This was fixed by HBASE-2583 (but broke again by HBASE-2586 but thats another story);;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Double-assignment around split,HBASE-2365,12459959,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,stack,stack,23/Mar/10 07:39,12/Oct/12 06:15,14/Jul/23 06:06,01/Apr/10 06:19,,,,,,,,,,,,0.20.4,0.90.0,,,,,,0,,,"Its looking like we have a split updating .META. with daughter regions and then before we process the split in master, one of the daughters has already been assigned.  On processing of the split, we assign daughter again.

I thought this had been fixed previously?  Doesn't seem so.  Need to look again.

Here is evidence for region named:

{code}summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E7\x258C\x25AE\x25E5\x258E\x25BF\x25E5\x2586\x2580\x25E9\x25B9\x25B0\x25E6\x2591\x25A9\x25E6\x2593\x25A6\x25E6\x259D\x2590\x25E6\x2596\x2599\x25E5\x258E\x2582\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E9\x2585\x258D\x25E4\x25BB\x25B6\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017{code}

First master-side:

{code}
2010-03-15 16:06:52,153 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E7\x258C\x25AE\x25E5\x258E\x25BF\x25E5\x2586\x2580\x25E9\x25B9\x25B0\x25E6\x2591\x25A9\x25E6\x2593\x25A6\x25E6\x259D\x2590\x25E6\x2596\x2599\x25E5\x258E\x2582\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E9\x2585\x258D\x25E4\x25BB\x25B6\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017 to cactus208,60020,12686305486412010-03-15 16:06:52,156 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scan of 245 row(s) of meta region {server: 172.16.1.209:60020, regionname: .META.,,1, startKey: <>} complete2010-03-15 16:06:52,156 INFO org.apache.hadoop.hbase.master.BaseScanner: All 1 .META. region(s) scanned2010-03-15 16:06:52,841 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E5\x25A4\x25A7\x25E8\x25BF\x259E\x25E5\x2588\x2598\x25E5\x25B8\x2588\x25E5\x2582\x2585\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E5\x2587\x25B9\x25E9\x2599\x25B7\x25E4\x25BF\x25AE\x25E5\x25A4\x258D\x25E6\x2596\x25BD\x25E5\x25B7\x25A5\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E7\x2594\x25A8\x25E5\x2593\x2581\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017 to cactus210,60020,12686305508862010-03-15 16:06:54,377 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_SPLIT: summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E5\x25A4\x25A7\x25E8\x25BF\x259E\x25E5\x2588\x2598\x25E5\x25B8\x2588\x25E5\x2582\x2585\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E5\x2587\x25B9\x25E9\x2599\x25B7\x25E4\x25BF\x25AE\x25E5\x25A4\x258D\x25E6\x2596\x25BD\x25E5\x25B7\x25A5\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E7\x2594\x25A8\x25E5\x2593\x2581\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268611349836: Daughters; summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E5\x25A4\x25A7\x25E8\x25BF\x259E\x25E5\x2588\x2598\x25E5\x25B8\x2588\x25E5\x2582\x2585\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E5\x2587\x25B9\x25E9\x2599\x25B7\x25E4\x25BF\x25AE\x25E5\x25A4\x258D\x25E6\x2596\x25BD\x25E5\x25B7\x25A5\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E7\x2594\x25A8\x25E5\x2593\x2581\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017, summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E7\x258C\x25AE\x25E5\x258E\x25BF\x25E5\x2586\x2580\x25E9\x25B9\x25B0\x25E6\x2591\x25A9\x25E6\x2593\x25A6\x25E6\x259D\x2590\x25E6\x2596\x2599\x25E5\x258E\x2582\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E9\x2585\x258D\x25E4\x25BB\x25B6\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017 from cactus209,60020,1268630548451; 1 of 32010-03-15 16:06:54,388 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E7\x258C\x25AE\x25E5\x258E\x25BF\x25E5\x2586\x2580\x25E9\x25B9\x25B0\x25E6\x2591\x25A9\x25E6\x2593\x25A6\x25E6\x259D\x2590\x25E6\x2596\x2599\x25E5\x258E\x2582\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E9\x2585\x258D\x25E4\x25BB\x25B6\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017 to cactus209,60020,1268630548451
{code}

Its hard to read but above is an assignment, the split message, then what seems to be same region being assigned again.

Here is RS side on 209 server:

{code}
2010-03-15 16:06:29,727 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_OPEN: summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E7\x258C\x25AE\x25E5\x258E\x25BF\x25E5\x2586\x2580\x25E9\x25B9\x25B0\x25E6\x2591\x25A9\x25E6\x2593\x25A6\x25E6\x259D\x2590\x25E6\x2596\x2599\x25E5\x258E\x2582\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E9\x2585\x258D\x25E4\x25BB\x25B6\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017
2010-03-15 16:06:29,792 INFO org.apache.hadoop.hbase.regionserver.HRegion: region summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E7\x258C\x25AE\x25E5\x258E\x25BF\x25E5\x2586\x2580\x25E9\x25B9\x25B0\x25E6\x2591\x25A9\x25E6\x2593\x25A6\x25E6\x259D\x2590\x25E6\x2596\x2599\x25E5\x258E\x2582\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E9\x2585\x258D\x25E4\x25BB\x25B6\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017/1011052036 available; sequence id is 199443346
2010-03-15 16:06:29,792 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_OPEN: summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E5\x25A4\x25A7\x25E8\x25BF\x259E\x25E5\x2588\x2598\x25E5\x25B8\x2588\x25E5\x2582\x2585\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E5\x2587\x25B9\x25E9\x2599\x25B7\x25E4\x25BF\x25AE\x25E5\x25A4\x258D\x25E6\x2596\x25BD\x25E5\x25B7\x25A5\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E7\x2594\x25A8\x25E5\x2593\x2581\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017
2010-03-15 16:06:29,793 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting compaction on region summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E7\x258C\x25AE\x25E5\x258E\x25BF\x25E5\x2586\x2580\x25E9\x25B9\x25B0\x25E6\x2591\x25A9\x25E6\x2593\x25A6\x25E6\x259D\x2590\x25E6\x2596\x2599\x25E5\x258E\x2582\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E9\x2585\x258D\x25E4\x25BB\x25B6\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017
2010-03-15 16:06:29,944 INFO org.apache.hadoop.hbase.regionserver.HRegion: region summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E5\x25A4\x25A7\x25E8\x25BF\x259E\x25E5\x2588\x2598\x25E5\x25B8\x2588\x25E5\x2582\x2585\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E5\x2587\x25B9\x25E9\x2599\x25B7\x25E4\x25BF\x25AE\x25E5\x25A4\x258D\x25E6\x2596\x25BD\x25E5\x25B7\x25A5\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E7\x2594\x25A8\x25E5\x2593\x2581\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017/1971466363 available; sequence id is 199443345
2010-03-15 16:06:32,750 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_CLOSE_WITHOUT_REPORT: summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E7\x258C\x25AE\x25E5\x258E\x25BF\x25E5\x2586\x2580\x25E9\x25B9\x25B0\x25E6\x2591\x25A9\x25E6\x2593\x25A6\x25E6\x259D\x2590\x25E6\x2596\x2599\x25E5\x258E\x2582\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E9\x2585\x258D\x25E4\x25BB\x25B6\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017: Duplicate assignment
{code}

See how we end with 'Duplicate assignment' message?",,hammer,kannanm,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/10 19:03;stack;2365-xtra.patch;https://issues.apache.org/jira/secure/attachment/12440642/2365-xtra.patch","31/Mar/10 06:38;stack;2365.patch;https://issues.apache.org/jira/secure/attachment/12440309/2365.patch","23/Mar/10 07:44;stack;zheng.tgz;https://issues.apache.org/jira/secure/attachment/12439553/zheng.tgz",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26275,Reviewed,,,,Fri Apr 02 19:03:28 UTC 2010,,,,,,,,,,"0|i08snz:",49244,,,,,,,,,,,,,,,,,,,,,"23/Mar/10 07:43;stack;Marking blocker.;;;","23/Mar/10 07:44;stack;Zheng's logs from around time of error.;;;","24/Mar/10 23:59;jdcryans;mudphone on IRC got the same problem:

{code}
2010-03-24 07:04:39,139 DEBUG org.apache.hadoop.hbase.master.BaseScanner: Current assignment of furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256383584000:4923724a-d6ee-6d96-25cf-215b3201b5f6,1269414278319 is not valid;  serverAddress=, startCode=0 unknown.
2010-03-24 07:04:39,141 DEBUG org.apache.hadoop.hbase.master.BaseScanner: Current assignment of furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256810375918:53ba3cc0-cf02-1c15-3733-0ff8455ed5ad,1269414278319 is not valid;  serverAddress=, startCode=0 unknown.
2010-03-24 07:04:39,198 DEBUG org.apache.hadoop.hbase.master.RegionManager: Assigning for address: 10.194.146.47:60020, startcode: 1269312324850, load: (requests=277, regions=74, usedHeap=977, maxHeap=1993): total nregions to assign=2, nregions to reach balance=0, isMetaAssign=false
2010-03-24 07:04:39,198 DEBUG org.apache.hadoop.hbase.master.RegionManager: Doing for address: 10.194.146.47:60020, startcode: 1269312324850, load: (requests=277, regions=74, usedHeap=977, maxHeap=1993) nregions: 2 and nRegionsToAssign: 2
2010-03-24 07:04:39,198 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256810375918:53ba3cc0-cf02-1c15-3733-0ff8455ed5ad,1269414278319 to ip-10-194-146-47.ec2.internal,60020,1269312324850
2010-03-24 07:04:39,198 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256383584000:4923724a-d6ee-6d96-25cf-215b3201b5f6,1269414278319 to ip-10-194-146-47.ec2.internal,60020,1269312324850
2010-03-24 07:04:39,469 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_SPLIT: furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256383584000:4923724a-d6ee-6d96-25cf-215b3201b5f6,1269198117631: Daughters; furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256383584000:4923724a-d6ee-6d96-25cf-215b3201b5f6,1269414278319, furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256810375918:53ba3cc0-cf02-1c15-3733-0ff8455ed5ad,1269414278319 from ip-10-243-63-175.ec2.internal,60020,1269312324858; 1 of 3
2010-03-24 07:04:39,474 DEBUG org.apache.hadoop.hbase.master.RegionManager: Assigning for address: 10.243.63.175:60020, startcode: 1269312324858, load: (requests=16, regions=76, usedHeap=1280, maxHeap=1993): total nregions to assign=2, nregions to reach balance=2, isMetaAssign=false
2010-03-24 07:04:39,474 DEBUG org.apache.hadoop.hbase.master.RegionManager: Assigning one region only (playing it safe..)
2010-03-24 07:04:39,474 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256810375918:53ba3cc0-cf02-1c15-3733-0ff8455ed5ad,1269414278319 to ip-10-243-63-175.ec2.internal,60020,1269312324858
2010-03-24 07:04:39,559 DEBUG org.apache.hadoop.hbase.master.RegionManager: Assigning for address: 10.194.95.159:60020, startcode: 1269312324900, load: (requests=183, regions=75, usedHeap=1268, maxHeap=1993): total nregions to assign=1, nregions to reach balance=1, isMetaAssign=false
2010-03-24 07:04:39,561 DEBUG org.apache.hadoop.hbase.master.RegionManager: Assigning one region only (playing it safe..)
2010-03-24 07:04:39,561 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256383584000:4923724a-d6ee-6d96-25cf-215b3201b5f6,1269414278319 to ip-10-194-95-159.ec2.internal,60020,1269312324900
2010-03-24 07:04:39,731 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scan of 454 row(s) of meta region {server: 10.195.9.219:60020, regionname: .META.,,1, startKey: <>} complete
2010-03-24 07:04:39,732 INFO org.apache.hadoop.hbase.master.BaseScanner: All 1 .META. region(s) scanned
2010-03-24 07:04:40,253 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_PROCESS_OPEN: furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256383584000:4923724a-d6ee-6d96-25cf-215b3201b5f6,1269414278319 from ip-10-194-146-47.ec2.internal,60020,1269312324850; 1 of 3
2010-03-24 07:04:40,253 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256810375918:53ba3cc0-cf02-1c15-3733-0ff8455ed5ad,1269414278319 from ip-10-194-146-47.ec2.internal,60020,1269312324850; 2 of 3
2010-03-24 07:04:40,253 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256383584000:4923724a-d6ee-6d96-25cf-215b3201b5f6,1269414278319 from ip-10-194-146-47.ec2.internal,60020,1269312324850; 3 of 3
2010-03-24 07:04:40,253 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: PendingOpenOperation from ip-10-194-146-47.ec2.internal,60020,1269312324850
2010-03-24 07:04:40,253 INFO org.apache.hadoop.hbase.master.RegionServerOperation: furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256810375918:53ba3cc0-cf02-1c15-3733-0ff8455ed5ad,1269414278319 open on 10.194.146.47:60020
2010-03-24 07:04:40,254 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256810375918:53ba3cc0-cf02-1c15-3733-0ff8455ed5ad,1269414278319 in region .META.,,1 with startcode=1269312324850, server=10.194.146.47:60020
2010-03-24 07:04:40,254 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: PendingOpenOperation from ip-10-194-146-47.ec2.internal,60020,1269312324850
2010-03-24 07:04:40,254 INFO org.apache.hadoop.hbase.master.RegionServerOperation: furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256383584000:4923724a-d6ee-6d96-25cf-215b3201b5f6,1269414278319 open on 10.194.146.47:60020
2010-03-24 07:04:40,255 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256383584000:4923724a-d6ee-6d96-25cf-215b3201b5f6,1269414278319 in region .META.,,1 with startcode=1269312324850, server=10.194.146.47:60020
2010-03-24 07:04:40,496 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_PROCESS_OPEN: furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256810375918:53ba3cc0-cf02-1c15-3733-0ff8455ed5ad,1269414278319 from ip-10-243-63-175.ec2.internal,60020,1269312324858; 1 of 2
2010-03-24 07:04:40,496 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256810375918:53ba3cc0-cf02-1c15-3733-0ff8455ed5ad,1269414278319 from ip-10-243-63-175.ec2.internal,60020,1269312324858; 2 of 2
2010-03-24 07:04:40,496 DEBUG org.apache.hadoop.hbase.master.ServerManager: region server 10.243.63.175:60020 should not have opened region furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256810375918:53ba3cc0-cf02-1c15-3733-0ff8455ed5ad,1269414278319
2010-03-24 07:04:40,582 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_PROCESS_OPEN: furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256383584000:4923724a-d6ee-6d96-25cf-215b3201b5f6,1269414278319 from ip-10-194-95-159.ec2.internal,60020,1269312324900; 1 of 2
2010-03-24 07:04:40,582 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256383584000:4923724a-d6ee-6d96-25cf-215b3201b5f6,1269414278319 from ip-10-194-95-159.ec2.internal,60020,1269312324900; 2 of 2
2010-03-24 07:04:40,582 DEBUG org.apache.hadoop.hbase.master.ServerManager: region server 10.194.95.159:60020 should not have opened region furtive_production_consumer_events_d4a03af6-1d95-4b77-9aac-22389852d901,1256383584000:4923724a-d6ee-6d96-25cf-215b3201b5f6,1269414278319
{code}

I think the problem is that in this case the master was able to assign, but the split report came in just before the the first batch of assignments were reported and since we only do this:

{code}

  private void doRegionAssignment(final RegionState rs,
      final HServerInfo sinfo, final ArrayList<HMsg> returnMsgs) {
    String regionName = rs.getRegionInfo().getRegionNameAsString();
    LOG.info(""Assigning region "" + regionName +
        "" to "" + sinfo.getServerName());
    rs.setPendingOpen(sinfo.getServerName());
    synchronized (this.regionsInTransition) {
      this.regionsInTransition.put(regionName, rs);
    }

    returnMsgs.add(new HMsg(HMsg.Type.MSG_REGION_OPEN, rs.getRegionInfo()));
  }
{code}

Then we don't have the 3 fields in the .META. entry as our check expects.;;;","25/Mar/10 18:10;jdcryans;Diving a bit more, RegionManager.doRegionAssignment doesn't change the .META. entry and this is why we don't see it in ServerManager.assignSplitDaughter, it only setPendingOpen.

So one more thing we could check in assignSplitDaughter is if the region is pendingOpen or assigned (doing the call to .META.). Or instead we could add a new 
method called ""isOpened' to RegionManager and not have to do the check on .META. directly?;;;","25/Mar/10 20:46;kannanm;BaseScanner.java:checkAssigned() has some comments related to double-assignment:

{code}
   // Scans are sloppy. They don't respect row locks and they get and
    // cache a row internally so may have data that is stale. Make sure that for
    // sure we have the right server and servercode. We are trying to avoid
    // double-assignments. See hbase-1784. Will have to wait till 0.21 hbase
    // where we use zk to mediate state transitions to do better.
{code}

And it seems to deal with the staleness of the scanner by doing a GET for the relevant key. But the following line:

{code}
       // Now get the region assigned
        this.master.regionManager.setUnassigned(info, true);
{code}

uses the potentially stale ""info"" passed as an argument to checkAssigned(). Should this instead 
retrieve the HRegionInfo from the result of the GET?
;;;","25/Mar/10 21:20;stack;.bq ""...uses the potentially stale ""info"" passed as an argument to checkAssigned(). Should this instead retrieve the HRegionInfo from the result of the GET?""

It should though our having a problem because of regioninfo is stale is likely rare (the Get was added to find new server or startcode info that was added between scanner#next and our acting on info found.

@J-D The check for info:regioninfo + info:server + info:startcode being present was for the case where assignment had happened before master got split message AND the region had been opened too before the split message came in.  The problem seen by Zheng and by mudphone above is that the region is assigned (opening) but not yet opened.  During this time the message comes in and we blindly reassign it seems.  It looks like we can just check regionsInTransition or if we want to be more precise, regionsInTransition and that the state is 'opening'.  Let me study some more.  Will put up a patch.

;;;","25/Mar/10 21:49;stack;@J-D Pardon me, we are saying effectively same soln. when you say ""...if the region is pendingOpen..."";;;","31/Mar/10 01:21;stack;Issue was something else:

Code was changed.  Oriinally it was this:

{code}
732908       jimk       if (force || (!s.isPendingOpen() && !s.isOpen())) {
732908       jimk         s.setUnassigned();
732908       jimk       }
{code}

It got changed to this:

{code}
stack:0.20 Stack$ svn diff -rr892983:893381  src/java/org/apache/hadoop/hbase/master/RegionManager.java 
Index: src/java/org/apache/hadoop/hbase/master/RegionManager.java
===================================================================
--- src/java/org/apache/hadoop/hbase/master/RegionManager.java  (revision 892983)
+++ src/java/org/apache/hadoop/hbase/master/RegionManager.java  (revision 893381)
@@ -957,7 +957,7 @@
         s = new RegionState(info);
         regionsInTransition.put(info.getRegionNameAsString(), s);
       }
-      if (force || (!s.isPendingOpen() && !s.isOpen())) {
+      if (force || (!s.isPendingOpen() || !s.isOpen())) {
{code}

J-D spotted it.

Looking into it more.;;;","31/Mar/10 06:38;stack;This patch restores the test to how it used to be.  All tests but the deletes failure in TestClient pass.  I grepped the test output for case-insenstive 'double' and 'assignment' but found nothing.  What you think J-D?  Can I commit?;;;","31/Mar/10 18:15;jdcryans;I ran the test 10 times with the patch, looks ok... it's a shame I didn't document that change back in HBASE-2065. So +1 since for this case it does what we want.;;;","01/Apr/10 06:19;stack;Committed branch and trunk.  Thanks for reviewing and testing J-D (Though, I'm sure this change will come back to bite us).;;;","02/Apr/10 19:03;stack;The above 'xtra' was committed (mistakenly) as part of ""HBASE-1537 Intra-row scanning"".  The xtra addresses the commment made by Kannan that would be using a stale HRegionInfo checking assignments.  The 'xtra' uses what we got in the GET.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WALEdit broke replication scope,HBASE-2361,12459903,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,22/Mar/10 19:28,20/Nov/15 12:44,14/Jul/23 06:06,30/Mar/10 17:19,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Before HBASE-2283, each KV had a HLogKey with a replication scope. Now a key applies to a list of KVs that spans multiple families so a single scope doesn't work anymore. Multiple possible solutions:

 - Each KV have their own scope. We already ruled that out in a previous jira since that means the scope would end up in the HFiles.
 - Store pairs of scope/KV in WALEdit instead of straight KVs.
 - Have 2 parallel lists in WALEdit, one for KVs and the other for scopes.
 - Subclass KV and add the scope there, those would be created when inserted in the WAL and would contain the KV stored in the HFiles.

I'm sure there are other solutions, discuss.",,hammer,karthik.ranga,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2359,"23/Mar/10 01:43;jdcryans;HBASE-2361-2.patch;https://issues.apache.org/jira/secure/attachment/12439535/HBASE-2361-2.patch","30/Mar/10 01:52;jdcryans;HBASE-2361-3.patch;https://issues.apache.org/jira/secure/attachment/12440167/HBASE-2361-3.patch","22/Mar/10 21:43;jdcryans;HBASE-2361.patch;https://issues.apache.org/jira/secure/attachment/12439511/HBASE-2361.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26274,Reviewed,,,,Fri Nov 20 12:44:00 UTC 2015,,,,,,,,,,"0|i0hhf3:",100084,,,,,,,,,,,,,,,,,,,,,"22/Mar/10 21:43;jdcryans;This patch implements the third solution. It adds a new list for scopes which by default is set to local. It also optimizes reading the fields so that we only set the size of each list to what's needed.;;;","22/Mar/10 22:31;stack;Patch looks good.  Option #3 seems fine in above.

+ Would it be worthwhile making the ""optimization"" that if no replication config, do not create a scopes List?  Or would this make things a little involved?
+ REPLICATION_SCOPE_LOCAL should be in HLog rather than in HConstants?  Is it ever used outside of HLog?
+ For usual case, are we making two kvs arrays everytime we do a readFields?  Once when the WALEdit is constructed as part of construction and then again inside inside the if/elses?  We should make it once only?  Before this patch inside in readFields, we were doing a new ArrayList() when perhaps it should have been if (!this.kvs.empty()) this.kvs.clear() or new ArrayList().  Same for scopes List.
+ Is this right: +      kvs = new ArrayList<KeyValue>(0);?   Should be '1'?
;;;","22/Mar/10 22:41;jdcryans;bq. + Would it be worthwhile making the ""optimization"" that if no replication config, do not create a scopes List? Or would this make things a little involved?

Not sure how WALEdit could know that.

bq.  REPLICATION_SCOPE_LOCAL should be in HLog rather than in HConstants? Is it ever used outside of HLog?

Yes it is.

bq. For usual case, are we making two kvs arrays everytime we do a readFields? 

Yeah had the same idea. And make those finals?

bq. + Is this right: + kvs = new ArrayList<KeyValue>(0);? Should be '1'?

Oversight, thx.;;;","23/Mar/10 01:43;jdcryans;After some more discussions with Stack, here's a fifth solution that also includes some of his comments:

 - A Map is kept to keep the scope for each family contained in the WAL edit.
 - That map is always null when replication is off. For that we need a setter that mdc_replication uses to set the scopes when needed.
 - I moved the fixed overhead to a final variable and played with the heapSize a bit, not 100% sure it's done right since this class doesn't implement HeapSize.
 - The kvs list is now final and cleared when reading fields.;;;","23/Mar/10 04:32;stack;There is an extraneous comment:

+      // Set it to the right size since we know it

Does this method need to take a NavigableMap or SortedMap:

+  public void setScopes (Map<byte[], Integer> scopes) {

... to underline you can't have a Map with a byte [] unless it sorted.

Otherwise patch looks good to me J-D.

;;;","23/Mar/10 17:06;jdcryans;I will include Stack's comment on commit (minor stuff). I'm also marking this issue as depending on HBASE-2359 since I'm not sure at this point what's happening with the heapSize modifications.;;;","30/Mar/10 01:52;jdcryans;Rebased after HBASE-2359 went in.;;;","30/Mar/10 03:34;stack;+1 on commit;;;","30/Mar/10 17:19;jdcryans;Committed to trunk, thanks for the review Stack!;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WALEdit doesn't implement HeapSize,HBASE-2359,12459895,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kannanm,jdcryans,jdcryans,22/Mar/10 18:29,20/Nov/15 12:43,14/Jul/23 06:06,28/Mar/10 06:20,0.20.3,,,,,,,,,,,0.90.0,,,,,,,0,,,"WALEdit from HBASE-2283 defines a method heapSize() but doesn't implement HeapSize.

 - Make it implement the interface.
 - Add a test to TestHeapSize.",,kannanm,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2398,,,,,,,,,,,,,,,HBASE-2361,,"26/Mar/10 23:57;kannanm;2359_0.20_v1.txt;https://issues.apache.org/jira/secure/attachment/12439934/2359_0.20_v1.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26273,Reviewed,,,,Fri Nov 20 12:43:36 UTC 2015,,,,,,,,,,"0|i0hhev:",100083,,,,,,,,,,,,,,,,,,,,,"22/Mar/10 19:12;kannanm;@Stack: I can do this if you want. So feel free to reassign.;;;","22/Mar/10 19:26;stack;@Kannan Be my guest.  I'm on your other issue -- the end-to-end test.;;;","23/Mar/10 06:02;kannanm;The only place heapSize gets used for WALEdit is here in HLog.java:doWrite():

{code}
     this.editsSize.addAndGet(logKey.heapSize() + logEdit.heapSize());
{code}

where <i>editSize</i> is the size of current log, and is used in figuring when to rotate logs.

{code}
   if (this.editsSize.get() > this.logrollsize) {
        requestLogRoll();
    }
{code}

Strictly speaking for HLogKey & WALEdit we are interested in the serialized size rather than the heap size. 

Jonathan suggested perhaps adding an interface called WriteableSize, and changing HLogKey & WALEdit to implement that interface instead of HeapSize.

Another possibility---  SequenceFile.Writer has a ""getLength"" method which looks like what we want. We could  just use that instead of maintaining ""editsSize"" in HLog, i.e. replace the above logic with something like:

{code}
  if (this.writer.getLength() > this.logrollsize) {
     requestLogRoll();
  }
{code}

We can then do away with implementing any size method for HLogKey & WALEdit. 

I'll give it a try... but let me know if you have suggestions/feedback.



;;;","23/Mar/10 07:01;stack;.bq Strictly speaking for HLogKey & WALEdit we are interested in the serialized size rather than the heap size. 

Yes.  Good point.  HeapSize is not whats wanted at all.

.bq Another possibility--- SequenceFile.Writer has a ""getLength""... then do away with implementing any size method for HLogKey & WALEdit. 

+1 

This is cutting to the chase, its keeping tabs on the WAL file length that is point of sizing WALEdit+HLogKey. We'll never want to compress WAL logs so the comment in SF#getLength about inaccuracy when compressing does not apply.. getLength is out.getPos which is a long kept up by FSDataOutputStream.  

;;;","23/Mar/10 16:59;streamy;+1 on just using the getLength() method and dropping heapSize/writableSize, as long as that call is constant time which I assume it is.;;;","23/Mar/10 17:04;jdcryans;We'll need to assume to the same for Sequencefile.Reader.getPosition() since we will want to keep account of how many bytes we read while splitting logs and ideally keep it to a fixed size. I currently do that for HBASE-2223 in order to not blow up the heap when tailing the HLogs.;;;","26/Mar/10 23:57;kannanm;patch uploaded...;;;","27/Mar/10 00:03;kannanm;ran some load tests to make sure log rolling was happening as expected. Running unit test right now.;;;","27/Mar/10 15:01;kannanm;unit tests passed.;;;","28/Mar/10 06:20;stack;Nice patch Kannan.  Thanks. Committed to branch and trunk (had to add getLength to Writer Interface in HLog).;;;","12/May/10 23:53;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Store doReconstructionLog will fail if oldlogfile.log is empty and won't load region,HBASE-2358,12459890,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,clehene,clehene,clehene,22/Mar/10 18:01,12/Oct/12 06:15,14/Jul/23 06:06,22/Mar/10 21:40,0.20.3,,,,,,,,,,,0.20.4,0.90.0,regionserver,,,,23/Mar/10 00:00,0,,,"doReconstructionLog doesn't handle empty files correctly:
{code}
    FileStatus stat = this.fs.getFileStatus(reconstructionLog);
    if (stat.getLen() <= 0) {
      LOG.warn(""Passed reconstruction log "" + reconstructionLog +
        "" is zero-length. Deleting existing file"");
       fs.delete(reconstructionLog, false);
      return -1;
    }
{code}

Notice it actually compares the length of the array instead of the file length.

It should call getLen() and delete the file afterwards
{code}
   FileStatus stat = this.fs.getFileStatus(reconstructionLog);
    if (stat.getLen() <= 0) {
      LOG.warn(""Passed reconstruction log "" + reconstructionLog +
        "" is zero-length. Deleting existing file"");
       fs.delete(reconstructionLog, false);
      return -1;
    }
{code}

Also. This is a situation that shouldn't happen as an empty oldlogfile.log should be deleted when HMaster does the split in HLog.splitLog().
I couldn't figure what would make it leave it there as I also see in the logs that other empty logs are deleted. This might expose a thornier situation.",Any,tlipcon,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,HBASE-1994,,,,,,,,,,,,,"22/Mar/10 18:04;clehene;HBASE-2358.patch;https://issues.apache.org/jira/secure/attachment/12439479/HBASE-2358.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,3,Reviewed,,,,Tue Mar 23 06:02:02 UTC 2010,,,,,,,,,,"0|i00xav:",3327,,,,,,,,,,,,,,,,,,,,,"22/Mar/10 18:04;clehene;Attaching a patch to check for file length correctly and delete the empty file. ;;;","22/Mar/10 18:05;clehene;Linking HBASE-1994.;;;","22/Mar/10 18:06;clehene;redoing the link with ""relates to"" instead of ""blocks"";;;","22/Mar/10 19:27;stack;Add into 0.20.4.;;;","22/Mar/10 21:40;stack;Applied to branch and trunk.  I agree that the fact that this file is zero in first place is symptom of some other problem but empty log shouldn't get in the way of our deploying a region.  Thanks for the patch Cosmin.;;;","22/Mar/10 21:51;tlipcon;Are you seeing this issue in practice, Cosmin?

To me it seems like a sign of a serious problem, so we should get at the root of it (not just band-aid it);;;","22/Mar/10 22:00;stack;I can revert if thought is that this is a ""band-aid"" and that its better to have the horrorshow failure because it'll bring spotlight to root cause.;;;","22/Mar/10 22:07;tlipcon;eh, I'm sensitive to the needs of people running in production who can tolerate a bit of dataloss. For them, the bandaid makes sense. I opened HBASE-2363 to discuss maybe making band-aids a tunable option.;;;","23/Mar/10 05:59;clehene;Todd,

We've just seen this yesterday. It happened on a cluster that was running a trunk snapshot from Feburary. It was a fix for the previous ""band-aid"" and I didn't get to look on the HLog code, but I'll give it thorough look. 

Also, I suggest we instrument the code to measure coverage and run a server for a while to identify dead or seldom touched code so we could review it. 
;;;","23/Mar/10 06:02;tlipcon;bq. Also, I suggest we instrument the code to measure coverage and run a server for a while to identify dead or seldom touched code so we could review it. 

Great idea! I'll add this to the ticket about end-to-end testing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unsychronized logWriters map is mutated from several threads in HLog splitting,HBASE-2355,12459822,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,tlipcon,tlipcon,tlipcon,22/Mar/10 06:34,12/Oct/12 06:14,14/Jul/23 06:06,22/Mar/10 18:16,0.20.3,,,,,,,,,,,0.20.4,0.90.0,master,,,,,0,,,"In splitLog, the logWriters map is an unsynchronized collection, and the spawned threads mutate it. In practice I've now seen several times a situation where one of the puts into this map is lost, and a thread ends up renaming a file it's in the process of writing to, causing those edits to be lost when the log is split.",,dhruba,hammer,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/10 06:44;tlipcon;hbase-2355.txt;https://issues.apache.org/jira/secure/attachment/12439433/hbase-2355.txt",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26271,,,,,Mon Mar 22 22:23:53 UTC 2010,,,,,,,,,,"0|i08t0n:",49301,,,,,,,,,,,,,,,,,,,,,"22/Mar/10 06:44;tlipcon;Trivial patch.

This seems to fix the issue in my testing - I'm now able to kill region servers and not lose edits (with patched hdfs), whereas I was running into the splitting error maybe 2/3 of the time before.;;;","22/Mar/10 18:16;apurtell;Committed to trunk and 0.20 branch. ;;;","22/Mar/10 22:23;kannanm;Nice catch Todd!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBASE-2283 removed bulk sync optimization for multi-row puts,HBASE-2353,12459817,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,tlipcon,ryanobjc,ryanobjc,22/Mar/10 03:11,20/Nov/15 12:43,14/Jul/23 06:06,13/Jun/10 18:55,,,,,,,,,,,,0.90.0,,,,,,,0,moved_from_0_20_5,,"previously to HBASE-2283 we used to call flush/sync once per put(Put[]) call (ie: batch of commits).  Now we do for every row.  

This makes bulk uploads slower if you are using WAL.  Is there an acceptable solution to achieve both safety and performance by bulk-sync'ing puts?  Or would this not work in face of atomic guarantees?

discuss!",,hammer,kannanm,karthik.ranga,larsfrancke,techbuddy,tlipcon,tsuna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/May/10 23:49;jdcryans;HBASE-2353_def_log_flush.patch;https://issues.apache.org/jira/secure/attachment/12443661/HBASE-2353_def_log_flush.patch","13/Jun/10 18:56;tlipcon;hbase-2353.txt;https://issues.apache.org/jira/secure/attachment/12446986/hbase-2353.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26269,Reviewed,,,,Fri Nov 20 12:43:56 UTC 2015,,,,,,,,,,"0|i0hhdz:",100079,,,,,,,,,,,,,,,,,,,,,"22/Mar/10 08:31;apurtell;I believe this also affects 0.20 branch as both group commit and 2283 were backported, so 2283 would have clobbered group commit on branch also.;;;","22/Mar/10 17:47;kannanm;
The important thing is that memstore edits should happen after append+sync.

Currently, batch put is simply a loop around append/sync/memstore-edit per put.

If we tried to move to a model, where we first do append for each row, then a common sync, and then all the memstore changes-- then we would end up having to ""lock"" all the rows for the entire duration; (rather than the current model, which locks one row at a time.)

Also, the code structure would get uglier I think -- right now batch put pretty much is a thin wrapper around single Puts.

This was a conscious change in HBASE-2283 for restoring correctness of semantics. But I should have perhaps called it out explicitly.

@Ryan: Is the bulk upload case now noticeably slower?

@Andrew: You are right that this affects 0.20 also. But you might be mixing the ""group commit"" and ""multi-row put"" terminology. Group commit should not have been cloberred by HBASE-2283. But yes, HBASE-2283 does remove, for correctness, the ""batch sync"" optimization in multi-row puts.

;;;","22/Mar/10 17:48;kannanm;update the title of the bug to be more specific.;;;","23/Mar/10 00:13;stack;@Ryan What if for batch put, first we wrote all that is in the batch out to the WAL and then per edit, you called the individual put method but with the do-not-WAL flag set to true?  Would that get you your speed back?  But how to treat errors?  What do you tell the client if you've written the WAL but you fail to update memstore?  How were errors treated previously?  Seems like you have to reason about this stuff on a row by row basis?;;;","23/Mar/10 00:18;tlipcon;Under what scenarios would you fail to update memstore? It seems to me that those scenarios necessitate a full RS stop.;;;","23/Mar/10 00:26;ryanobjc;the existing code has a return code interpreted as 'failed past index
i' and the client will retry a number of times.  so that might work.

;;;","23/Mar/10 04:24;stack;.bq Under what scenarios would you fail to update memstore? It seems to me that those scenarios necessitate a full RS stop. 

I suppose, i was thinking memstore update would fail because the RS had crashed/stopped.  Can't think of any reason we'd part fail.  Client wouldn't get a return code though edits had gone in because the bulk put had not completed (client would see an exception).

Then there is the case where we add N of the M edits to the WAL file before we hit some HDFS issue that forces us return to the client.  In this case, wouldn't you have to report the bulk put had completely failed since edits had no edits had made it to the MemStore?

It seems like you have to process the bulk put, row by row.

;;;","24/Mar/10 00:53;ryanobjc;I have new numbers, basically my bulk puts are now much slower than previously.  This is a killer for us.  Single thread import performance is now down to 2000-6000 rows/sec, down from 16,000+.  

The first fix to this is to bring back deferred log flush.  I have a forthcoming patch. 

Here are my arguments:

- There is no multi-row atomicity guarantee. Having other clients see the partial results of your batch put is acceptable because that is our consistency model - per row. That is the defacto situation right now anyways.
- If the call succeeds, then we expect the puts to be durable.  By ensuring syncFs() call returns before returning to the client we have this.
- Partial failure by exception leaves the HLog in an uncertain state.  The client will not know how many rows were successfully made durable, and thus would be required to redo the put.   
- Partial ""failure"" by return code means only part of the rows were made durable and available to other clients.  This is normal and covered by the above cases I think.

Given this, what makes the most sense?  It seems like hlog.append() then syncFs() of all the puts, THEN memstore mutate is the way to go. In HRS.put our protection from 'over memory' is this call :

        this.cacheFlusher.reclaimMemStoreMemory();

which will synchronously flush until we arent going to go over memory.  If we somehow fail to add to memstore, it would be OOME which would kill the RS anyways.  Considering the data for the Put is already in memory and we are just adjusting data structure nodes, it seems unlikely that we'd be in this case often/ever.;;;","24/Mar/10 01:19;kannanm;<<< The first fix to this is to bring back deferred log flush. I have a forthcoming patch. >>>

How are the numbers looking with the posted patch (of not syncing if deferred log flush is set?).

<<<Given this, what makes the most sense? It seems like hlog.append() then syncFs() of all the puts, THEN memstore mutate is the way to go. In HRS.put our protection from 'over memory' is this call :>>>

Are you talking about how to improve things for batch puts in the default case (i.e. when deferred log flush is not set?). Is so, can you refer to my update on <22/Mar/10 05:47 PM> -- do you plan to lock all the rows for the entire duration? Or reget the locks before doing the memstore mutates? If you reget the row locks, I think you'll introduce other correctness issues.




;;;","24/Mar/10 01:22;kannanm;@ Saw JD's update on another JIRA. Looks like default in trunk is deferred log flush is set.

So my <<<Are you talking about how to improve things for batch puts in the default case (i.e. when deferred log flush is not set?).>>> should simply read <<< Are you talking about how to improve things for batch puts when deferred log flush is not set?>>>
;;;","24/Mar/10 01:30;ryanobjc;We cant do things like hold row locks for any substantial length of
time, that introduces the opportunities to deadlock.

My original code patch moved the sync to the top level at the end of
the put(Put[]) call.  Maybe for this particular use case that might be
the solution.  It isn't perfect, but we dont want to cripple this use
case (hbase is great if you can wait the months to load your data...)

On Tue, Mar 23, 2010 at 6:20 PM, Kannan Muthukkaruppan (JIRA)
;;;","24/Mar/10 01:46;tlipcon;bq. We cant do things like hold row locks for any substantial length of time, that introduces the opportunities to deadlock.

Howso? Time of locking doesn't introduce deadlock, just order of locking. We could sort the rows first to avoid internal deadlock. Of course, the fact that we expose locks to users does break this - if a user has a lock on row C, and we try to lock A,B,C, we'll block on that row while holding the others. If a user locks in the ""wrong order"" the problem's even worse because we'd deadlock against the client.

So, I don't think we can hold multiple row locks at the same time, no matter how short a period we do it for, assuming row locks continue to be user-exposed.

Unfortunately, the opposite problem is just as bad... if we do log(a) memstore(a) log(b) memstore(b) syncAll(), then edit A becomes visible before it's synced, and that's a no-no.

I don't have any good solutions, but here's a bad solution: HBASE-2332 (remove user-exposed row locks from region server). That JIRA could be made a little bit less drastic and say that user-exposed row locks are advisory locks (eg like flock) and they don't block IO (just other lock operations). That is to say, decouple the user exposed locks from the internal locks needed for consistency purposes.;;;","24/Mar/10 01:51;kannanm;<<< Maybe for this particular use case that might be the solution. It isn't perfect, but we dont want to cripple this use case (hbase is great if you can wait the months to load your data...>>>

But that has the issue of memstore edits happening before sync, which we don't want.;;;","24/Mar/10 01:56;ryanobjc;are we sure we dont want memstore edits happening before sync in a bulk load case?

Are we sure that holding hundreds (or thousands) of row locks open for like 10-20 seconds is a good idea?

I know people point to disabling the WAL completely to be fast, but I don't think we have to choose between fast (enough) and reasonably safe.;;;","24/Mar/10 01:59;kannanm;Ryan: So for your use case, if you are planning to go with deferredLogFlush to true, is this still a serious issue?
;;;","24/Mar/10 02:01;ryanobjc;With deferred log flush we are in a better position.  Do you think that we should provide a bulk commit code path that (sort of) mimics the group commit code path?  

Another thing to watch out for is people running the default config then writing nasty email/twitter notes because our out of the box performance isnt good unless you do tweaks X,Y,Z.  It'd be nice to be fully performant without tweaking values such as deferredLogFlush.  ;;;","24/Mar/10 02:02;ryanobjc;Are you willing to cripple the performance of hbase put speed to
ensure certain log atomic order of operation guarantees in a bulk
insert case?

On Tue, Mar 23, 2010 at 6:52 PM, Kannan Muthukkaruppan (JIRA)
;;;","24/Mar/10 02:05;tlipcon;{quote}
Are you willing to cripple the performance of hbase put speed to
ensure certain log atomic order of operation guarantees in a bulk
insert case?
{quote}

I think it's crucial that correctness is an _option_. That is to say, I am strongly
against any design that _precludes_ correct operation of bulk puts (where correct
is the set of guarantees we're talking about in HBASE-2294 - visible edits must be
durable, etc). I also am totally with you that for cases like mapreduce bulk loads
there should be a ""throw caution to the wind and shove that data in as fast as
our little hard drives can spin"" _option_.;;;","24/Mar/10 02:07;tlipcon;bq. Another thing to watch out for is people running the default config then writing nasty email/twitter notes because our out of the box performance isnt good unless you do tweaks X,Y,Z

I think this is simply a matter of education. I personally care less about the opinions of people on twitter than I do about correct operation. Correct operation has its costs. HBase has been ""cheating"" for some time and hence had great performance. I am certain that it's going to be slower once it's correct, and absolutely OK with that.;;;","24/Mar/10 02:18;apurtell;@Todd: Unfortunately the project will suffer if HBase is not as fast as reasonable for the default config. That is not strictly a technical consideration but is an important point. We have a bit of a PR battle going on because another project wants to be the prom queen. It is not possible to pretend that is not happening.
;;;","24/Mar/10 02:23;tlipcon;If the ""other project"" isn't correct, make the PR battle should be about that :) If the ""other project"" _is_ correct and still faster, we shouldn't compete by cheating, we should optimize the correct path!;;;","24/Mar/10 06:50;stack;/me sorry, late to the game

Correctness must be an option, if not the way we ship by default.  We can choose to not ship it as default but hbase has to be able to be correct (I'm thinking that default we might ship with deferred logging if it improves our speed some but we must be clear to user about the cost to them of not syncing each row mutation).

Bulk put is a relatively new feature.  Its addition made for some nice upload numbers but our write speed before bulk put had been fine, at least compared to the competition (See Y! paper).

What if you added flags to the bulk put Ryan that allowed you ask for a ""sloppy"" bulk put behavior for those of us who are fine redoing the bulk put if it doesn't all go in.  The sloppy bulk put would write all to the WAL first (you might look at making yourself a special version of WALEdit and HLogKey for this case... would need special handling at split time, etc.), sync, and then do he memstore update (the latter could be done by calling single-row put with WAL set to false) w/o locking (or write the WAL afterward... though I think writing it first better).  By default the bulk put would run row-by-row, WAL, sync, memstore-update.;;;","24/Mar/10 07:00;apurtell;bq. I'm thinking that default we might ship with deferred logging if it improves our speed

I was thinking the same with my above comment, and/or other similar trade offs as they are available.;;;","24/Mar/10 07:03;tlipcon;I like the idea of flags to decide on when to give up correctness.

Here's a pseudocode idea that may be able to maintain correctness and speed... just brainstorming (there may be flaws!)

{code}
def bulk_put(rows_to_write):
  while not rows_to_write.empty():
    minibatch = []

    # all minibatches must get at least one row
    row = rows_to_write.take()
    row.lock()
    minibatch.append(row)

    # try to grab as many more locks as we can
    # without blocking (prevents deadlock)
    while not rows_to_write.empty():
      row = rows_to_write.peek()
      if row.trylock():
        rows_to_write.take()
        minibatch.append(row)
      else:
        break
    # we now have locks on a number of rows
    write_to_hlog(minibatch)
    sync_hlog()
    write_to_memstore(minibatch)
    unlock_all_rows(minibatch)
{code}

Essentially the thought here is that we try to lock as many rows together as we can without risking deadlock. This algorithm is deadlock free because we'll never block on a lock while holding another. So in the uncontended case, this algorithm turns into the ""lock all rows, write all to hlog, sync, write all to memstore"" but in the pathological contended case it turns into a sync per row.

A couple variations are certainly available (eg loop all the way through rows_to_write making a minibatch of anything lockable might be better), but this general idea might be worth exploring?;;;","24/Mar/10 18:22;kannanm;Having ""durable"" semantics as the default, and providing an optional  ""sloppy"" overload of Put (which does the deferredLogFlush optimization) seems better to me. 

With regards to making the basic multi-row Put case faster, I guess it would help the single-threaded client case the most. If the client was already multi-threaded, then the group commit benefits would already be kicking in to amortize the cost of the syncs.  Todd's suggestion of optimistically acquiring locks in batches of rows sounds good to me.


;;;","24/Mar/10 19:22;apurtell;I disagree. I think higher performing options should be the default. I want durability as much if not more that others. However, the only users of out of the box configurations are prototypers, evaluators, and benchmarkers (and in this last case only the naïve ones) and it is good strategy to seek to avoid being labeled slow again by new users, unnecessarily. Any move into production requires some attention paid to configuration changes and tuning. As long as we provide clear guidance and detail what deferred log flushing trades away, we get the best result here in my opinion.
;;;","24/Mar/10 20:23;streamy;I agree with Andrew.  We were always burdened in the Postgres world by having horrid out-of-the-box performance, but awesome out-of-the-box durability.  

Being able to adjust these things for performance vs durability guarantees is awesome.

Whichever the default is, there must be ample documentation explaining the various knobs and various trade-offs.  People running HBase without diving into those docs are far more likely to be testing for performance, not durability.  They will also likely not be in a production environment, or on clusters large enough and be running for enough time that node failure is likely.;;;","24/Mar/10 20:26;tlipcon;We've derailed this JIRA a fair amount, but I'll add to the pile. One solution might be to ship with example configurations, one for speed, one for correctness, and let users pick at setup time which they want to base their -site off of.;;;","24/Mar/10 20:32;streamy;There are probably some jiras laying around for some kind of CLI that would generate configuration after taking some user input about their requirements, cluster information, etc... Could boil that into there if it ever gets done.  But since those things are always nice-to-haves that rarely get built, shipping with a few sample hbase-site's could be a good solution.

Part of the initial setup in the docs would be to decide which to use and rename it to hbase-site.xml.

Even still, there will need to be defaults, so can we still fight about that? ;);;;","24/Mar/10 20:33;streamy;HBASE-1173 was what I was thinking of I guess.  Has been closed.;;;","24/Mar/10 20:42;apurtell;Well worked example configs or some kind of wizard would be great to have, but to get back on track a little that means sufficient flexibility in the implementation to support those options. The proposal above to turn on and off features on the write path according to flags is a good one, a generic and extensible mechanism that can support many different policies. And, at no time should we accept a change that permanently reduces performance without providing a credible alternative, unless there is no such option.
;;;","24/Mar/10 20:49;kannanm;@ Andrew/Jonathan: I see your point of view. The default isn't a major issue for contention for me-- was just stating a personal preference, but it wouldn't bother me a whole lot either way assuming the choices are well documented.
;;;","24/Mar/10 20:49;tlipcon;Andrew, I agree with you in sentiment, but I also want to weigh in code complexity and QA. Specific to this issue (trying to drag this runaway train back!) I think it is very dangerous to have a configuration option that changes the _order_ in which key operations occur. It makes it very tough to reason about how different processes will affect the system, especially if this sort of config option proliferates. And the matrix of configurations that we'll have to test explodes in a really bad way.;;;","24/Mar/10 21:14;apurtell;@Todd: At least in the scope of this jira the choice is between two sets of behaviors that are not difficult to explain or reason about in my opinion. Allowing some reordering (or even violation in consequence of failure) for the pretty specific use case of bulk importing, or more generally, high speed insertion of regeneratable data, I think is ok. We should have the option.
;;;","25/Mar/10 16:16;karthik.ranga;Jumping in late here... just wanted to throw in my opinion.

I feel that having the option to configure the behavior is good. I also feel that we should make correctness the default - because it takes someone some amount of working knowledge to differentiate between the two. When I think of any DB (whose internals I do not know), I always assume that it preserves data. And I almost always expect to tweak some settings to get better performance if it does not cut my needs - but do not expect to have to tweak something to get absolute data correctness.

Another fallout of this ""sloppy"" option is that there is a possibility of data changing from underneath the application using it. The memstore may return a certain value when the application queries it, then region server goes down, replays the log and now the application may get a different answer (this case the correct one).  While ok for the most part, it may not play nice with some application not aware of this. And its pretty hard to debug as well :)

Just my 2 cents.;;;","25/Mar/10 16:29;apurtell;I like Todd's suggestion to have multiple example configs tuned for different trade offs that might be useful for different use cases but also vetted to be sane. I think that can satisfy everyone.;;;","25/Mar/10 16:44;streamy;+1 on shipping with multiple example configs.  As long as getting started docs are clear, I don't care so much what the shipped-with defaults are and am fine w/ karthik's take that we should ship w/ correctness/durability.;;;","25/Mar/10 16:55;apurtell;bq. +1 on shipping with multiple example configs.

I think that is settled. 

What about Ryan's original point that bulk importers now only have the option to turn off writes to the WAL? Can we get a performance option and correctness? What about the ""minibatching"" concept?;;;","25/Mar/10 16:59;tlipcon;I think the minibatching should actually give us both. Just need to find some time to write the code and give it a benchmark! :);;;","25/Mar/10 17:02;streamy;I like the optimistic mini-batching concept as a potential performance+correctness solution.

I think in many heavy-write scenarios, there is a lot of region concurrency, but often little or no per-row concurrency.  Even for non-import situations like a web application with user activity writes, you don't expect much row contention.;;;","05/Apr/10 18:21;jdcryans;Marking as blocker.

Testing trunk with PE seqWrite, it's now a bit more than 5x slower (what took 55 secs now takes 320). Deferred log flushing would help here but it would still be slower than the bulk sync optimization we had. This is a huge performance regression, even if they get durability some of our users will see MR jobs that took maybe an hour now take more than 5... for a variety of reasons this is enough to make this issue a blocker.

I think shipping with configs is good, but it won't solve this problem.

This mini-batching solution sounds awesome, unsure how soon we can get it tho.

Like Ryan was initially saying, bringing deferred log flush in 0.20 would be an easy task since it's a few lines to fix. The issue then is to decide whether we want to ship with this turned on or off by default (we already had a vote on this issue for trunk in November, we decided to enable it by default for all tables). Also if we turn this on, how big would the window be (currently 1 second in trunk).

I would like to point out that the MySQL binary log isn't flushed for every edit by default. See http://dev.mysql.com/doc/refman/5.0/en/binary-log.html, grep for ""sync_binlog"". We can't rely on HDFS to flush the HLog so we do it with the polling timeout, also we already force flush catalog edits and tables with deferred log flush disabled are flushing others edits. We could set a very small window, say 100ms?, and everyone is free to change it for their own tables.;;;","05/Apr/10 18:28;tlipcon;bq. I would like to point out that the MySQL binary log isn't flushed for every edit by default.

FWIW the binlog in MySQL is used for point-in-time-recovery from backups, and for replication, but it's not the analogue of the HLog. The InnoDB transaction log is the thing like HLog, and its default is to sync on every commit (but also allows tuning to be periodic);;;","05/Apr/10 18:38;apurtell;I am seeing similar performance killing effects on the write path benching up on EC2. I was so concerned have switched from m1.large to c1.xlarge and am currently getting a new baseline using the larger instance types, presumably with better i/o characteristics, with 0.20.3 and 0.20.4 with dfs.support.append=false. When I switch dfs.support.append=true I expect to go off a cliff.;;;","05/Apr/10 18:39;tlipcon;BTW, regarding defaults, I'm OK with deferred flush on or off, so long as it's very clearly documented in the ""getting started"" guide, and we provide an example config for ""absolute durability"" as well.;;;","05/Apr/10 18:50;ryanobjc;I think Todd was going to try implementing his algorithm above. Lets see how
that looks.

On Apr 5, 2010 11:22 AM, ""Jean-Daniel Cryans (JIRA)"" <jira@apache.org>
wrote:


[
https://issues.apache.org/jira/browse/HBASE-2353?page=com.atlassian.jira.plugin.system.issue.
..
Jean-Daniel Cryans updated HBASE-2353:
--------------------------------------

        Priority: Blocker  (was: Major)
   Fix Version/s: 0.20.4

Marking as blocker.

Testing trunk with PE seqWrite, it's now a bit more than 5x slower (what
took 55 secs now takes 320). Deferred log flushing would help here but it
would still be slower than the bulk sync optimization we had. This is a huge
performance regression, even if they get durability some of our users will
see MR jobs that took maybe an hour now take more than 5... for a variety of
reasons this is enough to make this issue a blocker.

I think shipping with configs is good, but it won't solve this problem.

This mini-batching solution sounds awesome, unsure how soon we can get it
tho.

Like Ryan was initially saying, bringing deferred log flush in 0.20 would be
an easy task since it's a few lines to fix. The issue then is to decide
whether we want to ship with this turned on or off by default (we already
had a vote on this issue for trunk in November, we decided to enable it by
default for all tables). Also if we turn this on, how big would the window
be (currently 1 second in trunk).

I would like to point out that the MySQL binary log isn't flushed for every
edit by default. See http://dev.mysql.com/doc/refman/5.0/en/binary-log.html,
grep for ""sync_binlog"". We can't rely on HDFS to flush the HLog so we do it
with the polling timeout, also we already force flush catalog edits and
tables with deferred log flush disabled are flushing others edits. We could
set a very small window, say 100ms?, and everyone is free to change it for
their own tables.



;;;","04/May/10 23:49;jdcryans;Patch to fix deferred log flush in trunk, HBASE-2283 stripped the calls. I also set it to false by default, less durability would be an option.;;;","12/May/10 23:48;stack;Bulk move of 0.20.5 issues into 0.21.0 after vote that we merge branch into TRUNK up on list.;;;","04/Jun/10 08:33;tlipcon;I'm going to take a stab at the optimistic mini-batching technique suggested above.

Andrew: did you get any final numbers with your EC2 testing of my HDFS-side sync parallelization? In my tests here I saw similar performance on trunk vs 20 when my patches were included, but haven't done a real rigorous comparison.;;;","04/Jun/10 18:10;apurtell;Todd: So much has been in flux, we've been waiting for it to settle. ;;;","11/Jun/10 07:52;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/167/
-----------------------------------------------------------

Review request for hbase, Kannan Muthukkaruppan and Ryan Rawson.


Summary
-------

I implemented the ""mini batching"" idea we talked about on the JIRA.

This currently breaks some of the error handling, so I dont intend to commit as is, but everyone is busy so wanted to put a review up now while I tidy up the rest.


This addresses bug HBASE-2353.
    http://issues.apache.org/jira/browse/HBASE-2353


Diffs
-----

  src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java 6b6d098 
  src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java a1baff4 
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java 034690e 

Diff: http://review.hbase.org/r/167/diff


Testing
-------

Some PEs on a real sync-enabled cluster, seems faster but haven't done scientific benchmarking.


Thanks,

Todd


;;;","11/Jun/10 17:03;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/167/#review181
-----------------------------------------------------------

Ship it!


Looks good to me.  Minor comments below.


src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.hbase.org/r/167/#comment848>

    Should this be public?  Isn't it just used internally?



src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.hbase.org/r/167/#comment847>

    Is this a copy?



src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.hbase.org/r/167/#comment849>

    You were going to replace these w/ something from guava (or is this it?)



src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.hbase.org/r/167/#comment850>

    Same here



src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.hbase.org/r/167/#comment851>

    This creates new Map, pass in Map.Entry instead?



src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.hbase.org/r/167/#comment852>

    I hate that this is even an option (smile)



src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.hbase.org/r/167/#comment853>

    w can never be null here?  (There was null check previous)



src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
<http://review.hbase.org/r/167/#comment854>

    Missing javadoc on new param


- stack



;;;","11/Jun/10 17:26;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>


bq.  On 2010-06-11 09:59:29, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 1439
bq.  > <http://review.hbase.org/r/167/diff/1/?file=1237#file1237line1439>
bq.  >
bq.  >     Should this be public?  Isn't it just used internally?

good call, will make it package private


bq.  On 2010-06-11 09:59:29, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 1442
bq.  > <http://review.hbase.org/r/167/diff/1/?file=1237#file1237line1442>
bq.  >
bq.  >     Is this a copy?

Yep. I did it the simple way following the pseudocode in the JIRA, but we can definitely do it more efficiently by indexing into a list or arraylist. My bet is that the efficiency gains are marginal compared to the cost of actually writing to the WAL, etc, and this patch should already be a big gain over what we used to have.

After this is committed let's file a followup JIRA to remove extra shallow copies. I'll add a TODO in the code also.


bq.  On 2010-06-11 09:59:29, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 1467
bq.  > <http://review.hbase.org/r/167/diff/1/?file=1237#file1237line1467>
bq.  >
bq.  >     You were going to replace these w/ something from guava (or is this it?)

The guava Preconditions thing is supposed to be more for parameter checking, etc, than for internal assertions. Currently these don't get run, but I think we should enable -ea at least for unit tests.


bq.  On 2010-06-11 09:59:29, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 1482
bq.  > <http://review.hbase.org/r/167/diff/1/?file=1237#file1237line1482>
bq.  >
bq.  >     This creates new Map, pass in Map.Entry instead?

I thought both .keySet() and .entrySet() were just views into the existing map? (and p.getFamilyMap() just returns a member)


bq.  On 2010-06-11 09:59:29, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 1777
bq.  > <http://review.hbase.org/r/167/diff/1/?file=1237#file1237line1777>
bq.  >
bq.  >     w can never be null here?  (There was null check previous)

yea, since it's assigned first in the try block, and that function doesn't throw exceptions.


- Todd


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/167/#review181
-----------------------------------------------------------



;;;","11/Jun/10 18:01;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-06-11 09:59:29, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 1442
bq.  > <http://review.hbase.org/r/167/diff/1/?file=1237#file1237line1442>
bq.  >
bq.  >     Is this a copy?
bq.  
bq.  Todd Lipcon wrote:
bq.      Yep. I did it the simple way following the pseudocode in the JIRA, but we can definitely do it more efficiently by indexing into a list or arraylist. My bet is that the efficiency gains are marginal compared to the cost of actually writing to the WAL, etc, and this patch should already be a big gain over what we used to have.
bq.      
bq.      After this is committed let's file a followup JIRA to remove extra shallow copies. I'll add a TODO in the code also.

Fine by me


bq.  On 2010-06-11 09:59:29, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 1467
bq.  > <http://review.hbase.org/r/167/diff/1/?file=1237#file1237line1467>
bq.  >
bq.  >     You were going to replace these w/ something from guava (or is this it?)
bq.  
bq.  Todd Lipcon wrote:
bq.      The guava Preconditions thing is supposed to be more for parameter checking, etc, than for internal assertions. Currently these don't get run, but I think we should enable -ea at least for unit tests.

NM.  Just saw this ""By default, Surefire enables JVM assertions for the execution of your test cases.""  So, keep your asserts as is (we should all take them on)


bq.  On 2010-06-11 09:59:29, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 1482
bq.  > <http://review.hbase.org/r/167/diff/1/?file=1237#file1237line1482>
bq.  >
bq.  >     This creates new Map, pass in Map.Entry instead?
bq.  
bq.  Todd Lipcon wrote:
bq.      I thought both .keySet() and .entrySet() were just views into the existing map? (and p.getFamilyMap() just returns a member)

Looks like I'm wrong, at least regards JDK1.6.  Internally it seems to use entrySet.  Below is from java.util.AbstractMap:


    public Set<K> keySet() {
    if (keySet == null) {
        keySet = new AbstractSet<K>() {
        public Iterator<K> iterator() {
            return new Iterator<K>() {
            private Iterator<Entry<K,V>> i = entrySet().iterator();
....


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/167/#review181
-----------------------------------------------------------



;;;","11/Jun/10 19:21;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>


bq.  On 2010-06-11 09:59:29, stack wrote:
bq.  > src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java, line 1439
bq.  > <http://review.hbase.org/r/167/diff/1/?file=1237#file1237line1439>
bq.  >
bq.  >     Should this be public?  Isn't it just used internally?
bq.  
bq.  Todd Lipcon wrote:
bq.      good call, will make it package private

Actually, all the other put methods are also public, even though HRegion itself is basically internal to the RS


- Todd


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/167/#review181
-----------------------------------------------------------



;;;","11/Jun/10 22:21;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/167/
-----------------------------------------------------------

(Updated 2010-06-11 15:18:10.087969)


Review request for hbase, Kannan Muthukkaruppan and Ryan Rawson.


Changes
-------

Significantly changed patch - now batch put returns a list of status codes. Also added some tests to make sure all the behavior is correct with locking, bad families, etc.

I'd like to change HRegionInterface and HTable to return lists of status codes at some point too, but I think that can wait for a later patch. This one at least gets our performance back up by batching the syncs.


Summary
-------

I implemented the ""mini batching"" idea we talked about on the JIRA.

This currently breaks some of the error handling, so I dont intend to commit as is, but everyone is busy so wanted to put a review up now while I tidy up the rest.


This addresses bug HBASE-2353.
    http://issues.apache.org/jira/browse/HBASE-2353


Diffs (updated)
-----

  src/main/java/org/apache/hadoop/hbase/HConstants.java 1e59533 
  src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java 62617ac 
  src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java adc505b 
  src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java 541ec9b 
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java ba04820 
  src/test/java/org/apache/hadoop/hbase/MultithreadedTestUtil.java 870f925 
  src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java 8a5206c 
  src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java e75767d 

Diff: http://review.hbase.org/r/167/diff


Testing
-------

Some PEs on a real sync-enabled cluster, seems faster but haven't done scientific benchmarking.


Thanks,

Todd


;;;","11/Jun/10 22:54;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/167/#review197
-----------------------------------------------------------

Ship it!


Looks good to me.  No comments.

- stack



;;;","12/Jun/10 00:59;hbasereviewboard;Message from: ""Ryan Rawson"" <ryanobjc@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/167/#review204
-----------------------------------------------------------


generally looks interesting and sound. This may need to be reconciled with http://review.hbase.org/r/151/ - batch operations for put/delete/get.

- Ryan



;;;","12/Jun/10 04:25;stack;@Todd Translating the above comments by myself and Ryan, I think thats two +1s you got there.;;;","13/Jun/10 18:55;tlipcon;Committed to trunk. Thanks for review, Stack and Ryan.;;;","13/Jun/10 18:56;tlipcon;Attaching patch I committed (r2 from review board);;;","17/Jun/10 03:28;hbasereviewboard;Message from: ""Kannan Muthukkaruppan"" <kannan@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/167/#review247
-----------------------------------------------------------


Todd: I know this is already reviewed and committed. I just got around to reviewing this and the related HBase-2733. Looks really good!!

One side effect of this change is that now a single WALEdit could contain data for more than 1 row. Previously all KVs inside a WALEdit corresponded to the same row. I don't think it should cause any problems, but thought I should call it out in case others can think of potential issues.




- Kannan



;;;","17/Jun/10 05:27;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-06-16 20:24:22, Kannan Muthukkaruppan wrote:
bq.  > Todd: I know this is already reviewed and committed. I just got around to reviewing this and the related HBase-2733. Looks really good!!
bq.  > 
bq.  > One side effect of this change is that now a single WALEdit could contain data for more than 1 row. Previously all KVs inside a WALEdit corresponded to the same row. I don't think it should cause any problems, but thought I should call it out in case others can think of potential issues.
bq.  > 
bq.  > 
bq.  >

I think its going to be alright Kannan.  The patch over in hbase-1025, the fixup to the replay of split edits, is applying the kvs it finds in a WALEdit value one at a time.  That they are for different rows but of the same region, it should be fine I'd say.


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.hbase.org/r/167/#review247
-----------------------------------------------------------



;;;","20/Aug/14 19:52;techbuddy;Hi,

I wanted to see the file diff for the fix to this issue, but it seems http://review.hbase.org/r/167 is broken.
Could someone point me to the diff url?

If it's migrated to git, how do I get the pull request,if any.

-thanks
SB;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Small values for hbase.client.retries.number and ipc.client.connect.max.retries breaks long ops in hbase shell,HBASE-2352,12459800,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kovyrin,kovyrin,kovyrin,21/Mar/10 18:14,20/Nov/15 12:43,14/Jul/23 06:06,03/Jun/10 19:05,0.90.0,,,,,,,,,,,0.90.0,,Client,,,,,0,,,"After switching hbase shell to small retries numbers (to make Puts to invalida coordinates friendlier to users), we broke long operations on tables (like disabling, truncating, etc) because they use the same config values to control client retries.
Need to roll the change back (will provide a patch) and make a better fix for puts later (after HBASE-2330 is resolved)",,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/10 14:29;kovyrin;HBASE-2352-rollback-changes.patch;https://issues.apache.org/jira/secure/attachment/12445877/HBASE-2352-rollback-changes.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26268,Reviewed,,,,Fri Nov 20 12:43:51 UTC 2015,,,,,,,,,,"0|i0hhdr:",100078,,,,,,,,,,,,,,,,,,,,,"19/May/10 19:26;streamy;Alexey, any plans to work on this stuff?  If not I may be able to find someone.;;;","25/May/10 21:44;kovyrin;Unfortunately I've been really loaded at work for the last 1.5 months and didn't have a chance to look into this issue since then. I will look into this issue on this weekend (May 29-31). If it needs to be addressed faster. Please consider to rolling my patch back until a better version available.;;;","30/May/10 14:29;kovyrin;Roll back the change in hbase.client.retries.number and find a better way to make Puts to non-existent columns friendlier.;;;","30/May/10 14:30;kovyrin;Attached hbase.client.retries.number changing patch, please review;;;","03/Jun/10 19:05;stack;This went in a few days ago.  Resolving.  Thanks for patch Alexey.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Usage of FilterList slows down scans,HBASE-2346,12459663,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,seelmann,seelmann,19/Mar/10 15:28,12/Oct/12 06:15,14/Jul/23 06:06,09/Apr/10 00:11,0.20.3,,,,,,,,,,,0.20.4,0.90.0,Filters,,,,,0,,,"When using a FilterList the scan is much slower compared to a scan with only a single filter (tested SingleColumnValueFilter and PrefixFilter).

The difference is extrem for very small ranges: if the range is only 10 rows the scan is 10 times slower when using the FilterList.

Is the cause just GC or object serialization/deserialization?

For a simple test I used the PerformanceEvaluation tool and created the TestTable with only 10(!) rows:
$ bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation --rows=10 sequentialWrite 1

The attached test performs 100 scan using diffent filters. The filter is then wrapped into a FilterList (with only a single filter). This FilterList is then nested two more times into another FilterList. For each nested level the scan gets slower and slower.

The test created the following output:

Scan Null Filter (10): 391ms
Scan FilterList with Null Filter (0): 4788ms
Scan Nested FilterList with Null Filter (0): 8303ms
Scan Nested Nested FilterList with Null Filter (0): 11915ms

Scan SingleColumValueFilter Equal (0): 257ms
Scan FilterList with SingleColumValueFilter Equal (0): 4121ms
Scan Nested FilterList with SingleColumValueFilter Equal (0): 7965ms
Scan Nested Nested FilterList with SingleColumValueFilter Equal (0): 11600ms

Scan SingleColumValueFilter Not Equal (10): 912ms
Scan FilterList with SingleColumValueFilter Not Equal (10): 4542ms
Scan Nested FilterList with SingleColumValueFilter Not Equal (10): 8459ms
Scan Nested Nested FilterList with SingleColumValueFilter Not Equal (10): 11513ms

Scan PrefixFilter (10): 306ms
Scan FilterList with PrefixFilter (10): 3695ms
Scan Nested FilterList with PrefixFilter (10): 7762ms
Scan Nested Nested FilterList with PrefixFilter (10): 11721ms

Get: 245ms
","Ubuntu 9.10
Sund JDK 1.6.0_16
HBase 0.20.3 (standalone and pseudo-distributed)",tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/10 23:45;jdcryans;HBASE-2346.patch;https://issues.apache.org/jira/secure/attachment/12441233/HBASE-2346.patch","19/Mar/10 15:30;seelmann;ScanFilterTest.java;https://issues.apache.org/jira/secure/attachment/12439281/ScanFilterTest.java",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26267,Reviewed,,,,Fri Apr 09 00:11:38 UTC 2010,,,,,,,,,,"0|i08sm7:",49236,,,,,,,,,,,,,,,,,,,,,"19/Mar/10 16:35;stack;Thanks for the test.  Should help debug whats going on.

I'm pulling this into 0.20.4.  FilterList is important piece of functionality.  Need to figure why its slow.;;;","08/Apr/10 23:45;jdcryans;Creating a Configuration is super expensive, so this patch reuses the same static object. Discussing with Stack, we don't think there's any impact (apart from better speed). Here's the numbers I get from running the test with the patch:

{code}

Scan Null Filter (0): 123ms
Scan FilterList with Null Filter (0): 359ms
Scan Nested FilterList with Null Filter (0): 296ms
Scan Nested Nested FilterList with Null Filter (0): 293ms

Scan SingleColumValueFilter Equal (0): 124ms
Scan FilterList with SingleColumValueFilter Equal (0): 169ms
Scan Nested FilterList with SingleColumValueFilter Equal (0): 129ms
Scan Nested Nested FilterList with SingleColumValueFilter Equal (0): 126ms

Scan SingleColumValueFilter Not Equal (0): 136ms
Scan FilterList with SingleColumValueFilter Not Equal (0): 117ms
Scan Nested FilterList with SingleColumValueFilter Not Equal (0): 128ms
Scan Nested Nested FilterList with SingleColumValueFilter Not Equal (0): 207ms

Scan PrefixFilter (0): 155ms
Scan FilterList with PrefixFilter (0): 117ms
Scan Nested FilterList with PrefixFilter (0): 136ms
Scan Nested Nested FilterList with PrefixFilter (0): 106ms

Get: 440ms
{code};;;","09/Apr/10 00:03;stack;I'd say commit J-D.  The change is localized to FiltterList and I don't see how it can harm.;;;","09/Apr/10 00:11;jdcryans;Committed to trunk, branch and pre-durab.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InfoServer and hence HBase Master doesn't fully start if you have HADOOP-6151 patch,HBASE-2344,12459467,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,kannanm,kannanm,kannanm,18/Mar/10 00:58,20/Nov/15 12:42,14/Jul/23 06:06,19/Mar/10 04:58,,,,,,,,,,,,0.90.0,,,,,,,0,,,"This is was due to a minor issue on the HBase side. The hadoop HttpServer prior to HADOOP-6151 was more tolerant than now.

In /org/apache/hadoop/hbase/util/InfoServer.java, addDefaultApps() adds a null key to the defaultContexts map. After HADOOP-6151, the HttpServer code raises a NPE. And hence HBase master doesn't fully start.

Will submit the patch shortly.
",,hammer,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/10 01:19;kannanm;2344_0.20_v1.patch;https://issues.apache.org/jira/secure/attachment/12439109/2344_0.20_v1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26266,Reviewed,,,,Fri Nov 20 12:42:50 UTC 2015,,,,,,,,,,"0|i0hhcf:",100072,,,,,,,,,,,,,,,,,,,,,"18/Mar/10 01:19;kannanm;Patch attached. Unit tests pass.;;;","19/Mar/10 04:58;stack;Applied to branch (already on trunk).  Thanks for the patch Kannan.;;;","12/May/10 23:53;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log recovery: deleted items may be resurrected,HBASE-2338,12459407,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,aravind.menon,kannanm,kannanm,17/Mar/10 17:34,20/Nov/15 12:44,14/Jul/23 06:06,01/Apr/10 13:29,0.20.4,,,,,,,,,,,0.90.0,,,,,,,0,,,"While working on HBASE-2283, noticed that if you do a put followed by a delete, and then crash the RS, and trigger log recovery to happen, then deleted entries may be resurrected. 

Suprisingly, the issue only affected delete of a specific column. Full row delete didn't run into this issue.

---

Code inspection revealed that we might have an issue with timestamps & WAL stuff for delete that come in with ""LATEST"" timestamp. [Note: The ""LATEST"" timestamp is syntax sugar/hint to the RS to convert it to ""now"". ]

Basically, in:

{code}
delete(byte [] family, List<KeyValue> kvs, boolean writeToWAL)
{code}

the ""kv.updateLatestStamp(byteNow);"" time stamp massaging (from LATEST to now) happens *after* the WAL log.append() call. So the KeyValue entries written to the HLog do not have the massaged timestamp. On recovery, when these entries are replayed, we add them back to reconstructionCache but don't do anything with timestamps. 

The above could be the potential source of the problem. But there could be more to the problem than my simple analysis. For instance, we still don't know why full row delete worked fine, but delete of a specific column didn't work ok. Forking this off as a separate issue from HBASE-2283.

[Note: Aravind is starting to take a look at this issue.]
",,aravind.menon,dhruba,hammer,kannanm,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/10 23:16;aravind.menon;HBASE-2338-v2.patch;https://issues.apache.org/jira/secure/attachment/12440420/HBASE-2338-v2.patch","31/Mar/10 23:02;aravind.menon;delete.patch;https://issues.apache.org/jira/secure/attachment/12440417/delete.patch","23/Mar/10 20:29;aravind.menon;delete.patch;https://issues.apache.org/jira/secure/attachment/12439605/delete.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26265,Reviewed,,,,Fri Nov 20 12:44:08 UTC 2015,,,,,,,,,,"0|i0hhbb:",100067,,,,,,,,,,,,,,,,,,,,,"23/Mar/10 20:25;aravind.menon;When keys for a particular key version are deleted, the timestamp of the delete key-value pair must match the timestamp of the put key-value. Currently, this timestamp is fixed only in the memstore and not in the log file. The result is that if the regionserver crashes before flushing memstore to a store file, the delete is lost on recovery. This patch fixes it so that the log file and memstore k-v pairs have identical timestamp.;;;","26/Mar/10 17:48;stack;Patch looks good except for this change:

{code}
-            System.out.println(""K: "" + Bytes.toStringBinary(kv.getKey()) +
+            System.out.println(""K: "" + kv +
{code}

Does that do the right thing?  Print out the key only?

Thanks.

;;;","26/Mar/10 18:38;kannanm;Yes, it prints out the key only. We wondered about that too... but checked the output and the code to confirm.;;;","27/Mar/10 02:34;ryanobjc;not only does it only print out the key, it prints out the key in
formatted form and includes the value length.

On Fri, Mar 26, 2010 at 11:40 AM, Kannan Muthukkaruppan (JIRA)
;;;","28/Mar/10 05:16;stack;Committed to branch and trunk.  Thanks for the patch Aravind.  Makes sense now after looking -- the lone kv is actually kv.toString().;;;","30/Mar/10 00:47;stack;Commit broke build.  Reopening.  TestClient.testDeletes fails.  We add two values each at different timestamps.  We then do two deletes w/o ts.  This should be purging both values but seems to only get rid of one.  I'm taking a look at it...;;;","30/Mar/10 01:16;aravind.menon;This seems to be an intermittent error. Kannan and I are seeing occasional failure of TestCLient.testDeletes, but this is not consistent, it passes several times also. Seems like a timing related issue. We are looking into this.;;;","30/Mar/10 03:41;aravind.menon;I think we found the issue. testDelete fails when we try to delete the two latest versions of the same column within a column family. The specific test that fails is the following (TestClient.java, line 1585, simplified):

put. = new Put(ROWS[2]);
put.add(FAMILIES[1], QUALIFIER, ts[0], VALUES[0]);
put.add(FAMILIES[1], QUALIFIER, ts[1], VALUES[1]);
ht.put(put);

delete = new Delete(ROWS[2]);
delete.deleteColumn(FAMILIES[1], QUALIFIER);
delete.deleteColumn(FAMILIES[1], QUALIFIER);

get = new Get(ROWS[2]);
get.addFamily(FAMILIES[1]);
get.setMaxVersions(Integer.MAX_VALUE);
result = ht.get(get);
assertTrue(""Expected 0 key but received "" + result.size(),
        result.size() == 0);

Previously for deleting specific column versions, after deleting a particular keyvalue, the memstore was immediately updated, before the next delete was processed in the loop (HRegion.java, line 1215). Thus, on deleting subsequent keyvalues, the ""get"" to retrieve the latest timestamp would return the correct value. 

With the patch, we have changed the memstore update order. The timestamps for all keyvalues are updated first, before the keyvalues are written to log and memstore. So, if there are two deletes, they would both see the same latest version number for that column, and both would delete the same version. 
;;;","30/Mar/10 04:51;stack;Good on you lads.  You figured it.  Were you going to post another patch?;;;","30/Mar/10 05:11;kannanm;To handle this correctly, one way would be to look at the list of KVs in the Delete for duplicates and determine how many versions of a column we are trying to delete. Sigh! If we determine that we are say trying to delete N versions of a column, then find the timestamps for the latest N versions (instead of one at a time), and update the N KVs with those timestamps. Then proceed to update the log and memstore.
;;;","31/Mar/10 23:02;aravind.menon;For cell version deletes, try to remember how many versions if a cell have been deleted, and update timestamps of new deletes accordingly.;;;","31/Mar/10 23:16;aravind.menon;Reupping patch with proper path and naming.;;;","01/Apr/10 13:29;stack;v2 looks good. Applied to branch and it fixed the build.  Just applied to trunk.  Thanks for persisting Aravind.;;;","01/Apr/10 13:30;stack;Added Aravind as contributor and assigned him this issue.;;;","12/May/10 23:53;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log recovery: splitLog deletes old logs prematurely,HBASE-2337,12459405,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,nspiegelberg,kannanm,kannanm,17/Mar/10 17:25,20/Nov/15 12:42,14/Jul/23 06:06,05/Oct/10 21:11,,,,,,,,,,,,0.90.0,,,,,,,0,,,"splitLog()'s purpose is to take a bunch of commit logs of a crashed RS and create per-region logs. splitLog() runs in the master.  There are two cases where splitLog() might end up deleting an old log before actually creating (sync/closing) the newly created logs. If the master crashes in between deletion of the old log and creation of the new log, then edits could be lost irrecoverably.

More specifically here are the two issues we (Nicolas, Aravind and I) noticed:

Issue #1: The old logs are read one at a time. An in memory structure, logEntries (a map from region name to edits for the region), is populated. And the old logs are closed. Then the in-memory map is written out to per region files. Fix: We should move the file deletion to later.

Issue #2: There is another little case. The per-region log file is written under the region directory (named oldlogfile.log or the constant HREGION_OLDLOGFILE_NAME). Before the master creates the file, it checks to see if there is already a file with that name, and if so, it renames it to oldlogfile.log.old, and then creates file oldlogfile.log again, and copies over the contents of oldlogfile.log.old to oldlogfile.log. It then proceeds to delete ""oldlogfile.log.old"", even though it hasn't closed/sync'ed ""oldlogfile.log"" yet. 

--

I think we should be able to restructure the code such that all deletion of old logs happens *after* the new logs have been created (i.e. written to & closed).






",,dhruba,hammer,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2437,,HBASE-2933,,,,HBASE-2363,,,,,,,,,,,,,"25/Mar/10 18:29;nspiegelberg;HBASE-2337-20.4.patch;https://issues.apache.org/jira/secure/attachment/12439805/HBASE-2337-20.4.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26264,Reviewed,,,,Fri Nov 20 12:42:25 UTC 2015,,,,,,,,,,"0|i0hhb3:",100066,,,,,,,,,,,,,,,,,,,,,"17/Mar/10 18:07;stack;Making it a blocker on 0.20.4.  If ye don't take it, I will.  As part of this issue perhaps we could get rid of the naming convention used for per-region files: i.e oldlogfile.log and oldlogfile.log.old.  Its the most brain-dead naming that ever I've come across.;;;","25/Mar/10 18:29;nspiegelberg;Important changes

1. Workflow is now for(steps in logs) { read[n]; write[n]; } delete(logs);
2. Don't worry about oldlogfile recovery, just delete and start over, since logs are deleted last
3. corrupt logs are (configurably) moved into a special directory for post-mortem analysis
3. super-paranoid error handling (probably should tune this). in our case, better to stop HMaster than ignore a critical filesystem error.;;;","25/Mar/10 18:33;tlipcon;Marking related to HBASE-2363 - the ""give up or keep going"" behavior should be tunable as part of that JIRA (I don't think you have to make that change now, just adding the link so that when we do that jira we remember to come back to this one);;;","25/Mar/10 18:59;jdcryans;Minor nits, some lines you added are over the 80 chars limit (like line 1273) and some statements don't have a blank before conditions, like:

{code}
+  for(Path p : finishedFiles) {
{code}

Also I don't think this comment makes sense since you pass a default value:

{code}
+    // store corrupt logs for post-mortem analysis (empty string = discard)
+    final String corruptDir = 
+      conf.get(""hbase.regionserver.hlog.splitlog.corrupt.dir"", "".corrupt"");
{code} 

I'll try this patch later this afternoon.;;;","25/Mar/10 20:06;nspiegelberg;@jd I'll do syntactic cleanups.  I should clarify the comment, however it's logically accurate.  I wanted to give users the ability to not store corruptions and just delete them.  To that end, if you explicitly set 'hbase.regionserver.hlog.splitlog.corrupt.dir' in your configuration to '', or empty string, it should delete instead of move corruptFiles.  I need to write some unit tests to verify functionality.  Alternate suggestions on handling this?  ;;;","25/Mar/10 20:19;jdcryans;Ah ok I see... seems counter-intuitive tho. Maybe something like hbase.regionserver.hlog.splitlog.archivecorruptedfiles.enabled in addition to the current configuration would help?;;;","25/Mar/10 22:40;jdcryans;Tested it on a local setup, first did normal kill -9 of a region servers and everything went well. Next run I also killed the master while it was splitting the logs and then restarted the whole thing and Nicolas' patch did its magic. ;;;","25/Mar/10 22:50;kannanm;nice! thanks for the additional testing JD (killing the master while splitting... evil :).;;;","30/Mar/10 07:08;stack;I applied patch to branch.  Thanks for the nice patch Nicolas (and to J-D for review and test).

Forward-porting is not easy.  Too much has changed.  I spent some time on it but will put it aside till we are doing 0.21.;;;","03/Jun/10 22:33;stack;@Nicolas Would you mind taking a look at this?  TRUNK is very different now but looking over your patch the spirit is there in that we'll not remove files at all now, not immediately at least; instead they are archived.  Does trunk do enough?  (IMO it seems so but would appreciate your opinion boss -- thanks).;;;","23/Jun/10 17:55;clehene;2437 should fix this for 0.21. Is this patch in 0.20 branch? If so should this issue still be open?;;;","23/Jun/10 17:59;stack;@Cosmin Yeah, I think this is fixed.  Was just waiting on Nicolas's OK (He said he'd get to it...).;;;","23/Jun/10 19:55;nspiegelberg;sorry, I was off on HDFS tasks.  reviewed 1025 yesterday and should get to 2437 soon;;;","24/Aug/10 23:54;jdcryans;HBASE-2437 was committed, can we close this Nicolas?;;;","25/Aug/10 01:32;nspiegelberg;there are still outstanding issues from the HBASE-2437 peer review.  I think this JIRA served as an open placeholder until they were resolved.  Maybe we should just file separate JIRAs?;;;","05/Oct/10 21:11;stack;Resolving as done.  Opened HBASE-3084 to take care of Nicolas comments made against hbase-2437 post-commit.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix build broken with HBASE-2334,HBASE-2336,12459379,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,larsfrancke,larsfrancke,larsfrancke,17/Mar/10 12:03,20/Nov/15 12:42,14/Jul/23 06:06,17/Mar/10 12:23,0.90.0,,,,,,,,,,,0.90.0,,build,,,,,0,,,"HBASE-2334 was a bit to eager to put SLF4J in the ""test"" scope. Thrift needs SLF4J.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/10 12:04;larsfrancke;hbase2336-1.patch;https://issues.apache.org/jira/secure/attachment/12439028/hbase2336-1.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26263,,,,,Fri Nov 20 12:42:49 UTC 2015,,,,,,,,,,"0|i0hhav:",100065,,,,,,,,,,,,,,,,,,,,,"17/Mar/10 12:23;larsgeorge;Trivial fix, committed after testing.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mapred package docs don't say zookeeper jar is a dependency.,HBASE-2335,12459353,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,ryanobjc,ryanobjc,17/Mar/10 06:05,12/Oct/12 06:15,14/Jul/23 06:06,08/Apr/10 16:56,0.20.3,,,,,,,,,,,0.20.4,,,,,,,0,,,"in:
http://hadoop.apache.org/hbase/docs/r0.20.3/api/org/apache/hadoop/hbase/mapred/package-summary.html

we dont say the classpath needs zookeeper-x.y.z.jar - which it does.

But this package does say so:
http://hadoop.apache.org/hbase/docs/r0.20.3/api/org/apache/hadoop/hbase/mapreduce/package-summary.html

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26262,,,,,Thu Apr 08 16:56:56 UTC 2010,,,,,,,,,,"0|i08sqv:",49257,,,,,,,,,,,,,,,,,,,,,"08/Apr/10 16:56;stack;Committed to the three branches the below:

{code}
Index: core/src/main/java/org/apache/hadoop/hbase/mapred/package-info.java
===================================================================
--- core/src/main/java/org/apache/hadoop/hbase/mapred/package-info.java (revision 931831)
+++ core/src/main/java/org/apache/hadoop/hbase/mapred/package-info.java (working copy)
@@ -37,26 +37,22 @@
 changes across your cluster but the cleanest means of adding hbase configuration
 and classes to the cluster <code>CLASSPATH</code> is by uncommenting
 <code>HADOOP_CLASSPATH</code> in <code>$HADOOP_HOME/conf/hadoop-env.sh</code>
-and adding the path to the hbase jar and <code>$HBASE_CONF_DIR</code> directory.
-Then copy the amended configuration around the cluster.
-You'll probably need to restart the MapReduce cluster if you want it to notice
-the new configuration.
-</p>
+adding hbase dependencies here.  For example, here is how you would amend
+<code>hadoop-env.sh</code> adding the
+built hbase jar, zookeeper (needed by hbase client), hbase conf, and the
+<code>PerformanceEvaluation</code> class from the built hbase test jar to the
+hadoop <code>CLASSPATH</code>:
 
-<p>For example, here is how you would amend <code>hadoop-env.sh</code> adding the
-built hbase jar, hbase conf, and the <code>PerformanceEvaluation</code> class from
-the built hbase test jar to the hadoop <code>CLASSPATH<code>:
-
 <blockquote><pre># Extra Java CLASSPATH elements. Optional.
 # export HADOOP_CLASSPATH=
-export HADOOP_CLASSPATH=$HBASE_HOME/build/test:$HBASE_HOME/build/hbase-X.X.X.jar:$HBASE_HOME/build/hbase-X.X.X-test.jar:$HBASE_HOME/conf</pre></blockquote>
+export HADOOP_CLASSPATH=$HBASE_HOME/build/hbase-X.X.X.jar:$HBASE_HOME/build/hbase-X.X.X-test.jar:$HBASE_HOME/conf:${HBASE_HOME}/lib/zookeeper-X.X.X.jar</pre></blockquote>
 
 <p>Expand <code>$HBASE_HOME</code> in the above appropriately to suit your
 local environment.</p>
 
-<p>After copying the above change around your cluster, this is how you would run
-the PerformanceEvaluation MR job to put up 4 clients (Presumes a ready mapreduce
-cluster):
+<p>After copying the above change around your cluster (and restarting), this is
+how you would run the PerformanceEvaluation MR job to put up 4 clients (Presumes
+a ready mapreduce cluster):
 
 <blockquote><pre>$HADOOP_HOME/bin/hadoop org.apache.hadoop.hbase.PerformanceEvaluation sequentialWrite 4</pre></blockquote>
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Put errors are burried by the RetriesExhaustedException,HBASE-2330,12459293,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,kovyrin,kovyrin,16/Mar/10 17:14,11/Jun/22 23:19,14/Jul/23 06:06,18/Jan/11 22:21,,,,,,,,,,,,,,Client,,,,,0,,,"When a user tries to put into non-existing column family, all the NoSuchColumnFamilyException errors get buried (many of them, since we retry 10 times by default) and then we throw useless RetriesExhaustedException that tells user nothing but the fact that the put operation has failed.",,kovyrin,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-3452,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26261,,,,,Wed Jan 19 05:39:15 UTC 2011,,,,,,,,,,"0|i0hh9z:",100061,,,,,,,,,,,,,,,,,,,,,"16/Mar/10 17:21;kovyrin;This issue makes it impossible for hbase shell to explain user a reason his put has failed.;;;","16/Mar/10 17:46;ryanobjc;There is an exception called DoNotRetryIOException That might be handy....

On Mar 16, 2010 10:16 AM, ""Alexey Kovyrin (JIRA)"" <jira@apache.org> wrote:

Put errors are burried by the RetriesExhaustedException
-------------------------------------------------------

                Key: HBASE-2330
                URL: https://issues.apache.org/jira/browse/HBASE-2330
            Project: Hadoop HBase
         Issue Type: Bug
         Components: client
           Reporter: Alexey Kovyrin


When a user tries to put into non-existing column family, all the
NoSuchColumnFamilyException errors get buried (many of them, since we retry
10 times by default) and then we throw useless RetriesExhaustedException
that tells user nothing but the fact that the put operation has failed.

--
This message is automatically generated by JIRA.
-
You can reply to this email to add a comment to the issue online.
;;;","18/Jan/11 22:21;ryanobjc;was fixed by HBASE-2898;;;","19/Jan/11 05:39;larsgeorge;I ran into the same issue (HBASE-3452 duped it). Very unpleasant. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
filter.RegexStringComparator does not work with certain bytes,HBASE-2323,12459049,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,tsuna,tsuna,tsuna,14/Mar/10 02:38,12/Oct/12 06:16,14/Jul/23 06:06,15/Mar/10 18:52,0.20.3,,,,,,,,,,,0.20.6,0.90.0,Filters,,,,,0,,,"I'm trying to use {{RegexStringComparator}} in conjunction with {{RowFilter}}.  One of my row keys contained the byte 0xA, which turns out to be the ASCII code for the newline character (\n).  When the row key is converted to a string in order to use the regexp facility of the Java standard library, it becomes a string containing two lines and my regexp does not match.

I believe the solution is to compile the regexp with the {{DOTALL}} flag.  Luckily, this flag can be ""passed"" by the client by prefixing the regexp with {{(?s)}} so people working with an older version of HBase can work around this issue without having to upgrade.


Second problem: One of my row keys contained the sequence {{0x00 0x00 0x9D}} ({{0x9D}} = -99 when stored in a Java {{byte}}) but in {{compareTo}} the row key is transformed in a {{String}} using {{Bytes.toString}}, which just assumes that the byte array is an UTF8 encoded string.  Java ""cleverly"" substituted the 0x9D byte with 0x63 (character '?').  In my case, I want to use encoding ISO-8859-1 as it preserves every byte when the byte array is converted to a {{String}} and back to a byte array, unlike UTF-8 or ASCII.  Should we add a new method to {{RegexStringComparator}} to allow the user to specify their own {{Charset}} instance?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/10 10:08;tsuna;0001-HBASE-2323-Kill-some-trailing-whitespaces.patch;https://issues.apache.org/jira/secure/attachment/12438746/0001-HBASE-2323-Kill-some-trailing-whitespaces.patch","12/Jul/10 05:27;tsuna;0001-HBASE-2323-filter.RegexStringComparator-does-not-wor.patch;https://issues.apache.org/jira/secure/attachment/12449205/0001-HBASE-2323-filter.RegexStringComparator-does-not-wor.patch","14/Mar/10 10:08;tsuna;0002-HBASE-2323-Compile-the-pattern-with-DOTALL.patch;https://issues.apache.org/jira/secure/attachment/12438747/0002-HBASE-2323-Compile-the-pattern-with-DOTALL.patch","14/Mar/10 10:08;tsuna;0003-HBASE-2323-Allow-the-client-to-specify-a-custom-char.patch;https://issues.apache.org/jira/secure/attachment/12438748/0003-HBASE-2323-Allow-the-client-to-specify-a-custom-char.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26259,Reviewed,,,,Mon Jul 12 21:51:33 UTC 2010,,,,,,,,,,"0|i08scn:",49193,,,,,,,,,,,,,,,,,,,,,"14/Mar/10 10:08;tsuna;Patches to fix the issue.;;;","14/Mar/10 10:10;tsuna;The patches attached preserve backwards compatibility.

Only the last patch is potentially disruptive with backwards compatibility, but the extra {{if}} statement in {{readFields}} makes it work even with older clients that won't serialize the new charset attribute.

If the patch is included in 0.20.4, then we can remove the extra {{if}} statement as that release won't be backwards compatible with older clients anyway.;;;","15/Mar/10 18:52;stack;Committed branch and trunk.  Thanks for the patch Benoît.  I remove the if from readFields as per your suggestion on the 0.20 commit.  I left it in place on TRUNK in case folks are are using older version of filter there.;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","12/Jul/10 05:27;tsuna;Patch rebased for 0.20.  Stack's previous commit in 0.20 (#923387) was ""lost"" when 0.20 was renamed {{0.20_pre_durability}};;;","12/Jul/10 21:51;stack;Commit for 0.20.6.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deadlock between put and cacheflusher in 0.20 branch,HBASE-2322,12459045,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,stack,stack,14/Mar/10 01:07,12/Oct/12 06:15,14/Jul/23 06:06,16/Apr/10 01:02,,,,,,,,,,,,0.20.4,0.90.0,,,,,,0,,,"{code}
Found one Java-level deadlock:
=============================
""IPC Server handler 59 on 60020"":
  waiting for ownable synchronizer 0x00007fec9eb050f8, (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync),
  which is held by ""IPC Server handler 54 on 60020""
""IPC Server handler 54 on 60020"":
  waiting to lock monitor 0x000000004190e950 (object 0x00007fec64f25258, a java.util.HashSet),
  which is held by ""regionserver/10.20.20.186:60020.cacheFlusher""
""regionserver/10.20.20.186:60020.cacheFlusher"":
  waiting for ownable synchronizer 0x00007fec651df998, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),
  which is held by ""IPC Server handler 19 on 60020""
""IPC Server handler 19 on 60020"":
  waiting for ownable synchronizer 0x00007fec9eb050f8, (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync),
  which is held by ""IPC Server handler 54 on 60020""

Java stack information for the threads listed above:
===================================================
""IPC Server handler 59 on 60020"":
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007fec9eb050f8> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
        at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:807)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1299)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1281)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1789)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
""IPC Server handler 54 on 60020"":
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.request(MemStoreFlusher.java:172)
        - waiting to lock <0x00007fec64f25258> (a java.util.HashSet)
        at org.apache.hadoop.hbase.regionserver.HRegion.requestFlush(HRegion.java:1549)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1534)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1318)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1281)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1789)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
""regionserver/10.20.20.186:60020.cacheFlusher"":
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007fec651df998> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:235)
        - locked <0x00007fec64f25258> (a java.util.HashSet)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:149)
""IPC Server handler 19 on 60020"":
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007fec9eb050f8> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
        at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:807)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:980)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:873)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:241)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushSomeRegions(MemStoreFlusher.java:352)
        - locked <0x00007fec64ed96f0> (a org.apache.hadoop.hbase.regionserver.MemStoreFlusher)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.reclaimMemStoreMemory(MemStoreFlusher.java:321)
        - locked <0x00007fec64ed96f0> (a org.apache.hadoop.hbase.regionserver.MemStoreFlusher)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1783)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)

Found 1 deadlock.
{code}",,dhruba,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2248,,,,,,,,,,,,,,,,,"16/Mar/10 06:46;tlipcon;hbase-2322.png;https://issues.apache.org/jira/secure/attachment/12438900/hbase-2322.png",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26258,,,,,Fri Apr 16 01:02:15 UTC 2010,,,,,,,,,,"0|i08spz:",49253,,,,,,,,,,,,,,,,,,,,,"14/Mar/10 16:34;stack;Here is the code to match the first of the thread dump above.

{code}
  public void put(Put put, Integer lockid, boolean writeToWAL)
  throws IOException {
    checkReadOnly();

    // Do a rough check that we have resources to accept a write.  The check is
    // 'rough' in that between the resource check and the call to obtain a
    // read lock, resources may run out.  For now, the thought is that this
    // will be extremely rare; we'll deal with it when it happens.
    checkResources();
    newScannerLock.writeLock().lock();
...
{code}

Above seems like a pain... no new scanners during a Put?  Doesn't it also mean one put at a time only?   Investigating....;;;","14/Mar/10 16:58;ryanobjc;The fix for This is part of 2248. As is, the lock helps keep atomic row
reads.

On Mar 14, 2010 12:35 PM, ""stack (JIRA)"" <jira@apache.org> wrote:


   [
https://issues.apache.org/jira/browse/HBASE-2322?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12845102#action_12845102]

stack commented on HBASE-2322:
------------------------------

Here is the code to match the first of the thread dump above.

{code}
 public void put(Put put, Integer lockid, boolean writeToWAL)
 throws IOException {
   checkReadOnly();

   // Do a rough check that we have resources to accept a write.  The check
is
   // 'rough' in that between the resource check and the call to obtain a
   // read lock, resources may run out.  For now, the thought is that this
   // will be extremely rare; we'll deal with it when it happens.
   checkResources();
   newScannerLock.writeLock().lock();
...
{code}

Above seems like a pain... no new scanners during a Put?  Doesn't it also
mean one put at a time only?   Investigating....

java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync),
java.util.HashSet),
java.util.concurrent.locks.ReentrantLock$NonfairSync),
java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync),
java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:807)
org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1299)
org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1281)
org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1789)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
org.apache.hadoop.hbase.regionserver.MemStoreFlusher.request(MemStoreFlusher.java:172)
org.apache.hadoop.hbase.regionserver.HRegion.requestFlush(HRegion.java:1549)
org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1534)
org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1318)
org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1281)
org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1789)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
java.util.concurrent.locks.ReentrantLock$NonfairSync)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:235)
org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:149)
java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:807)
org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:980)
org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:873)
org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:241)
org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushSomeRegions(MemStoreFlusher.java:352)
org.apache.hadoop.hbase.regionserver.MemStoreFlusher)
org.apache.hadoop.hbase.regionserver.MemStoreFlusher.reclaimMemStoreMemory(MemStoreFlusher.java:321)
org.apache.hadoop.hbase.regionserver.MemStoreFlusher)
org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1783)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)

--
This message is automatically generated by JIRA.
-
You can reply to this email to add a comment to the issue online.
;;;","15/Mar/10 05:19;tlipcon;got this out of jcarder - looks like the same thing, right? also can confirm it's gone with HBASE-2248;;;","15/Mar/10 05:22;tlipcon;(note that some of the ""holding:"" and ""taking:"" lines say writeLock when they're really readLock... this RWLock support in jcarder is ""experimental"" but I think it mainly works ;-);;;","15/Mar/10 05:30;ryanobjc;This lock is being pruned back by 2248. An attempt to create atomic row
reads.

On Mar 14, 2010 10:23 PM, ""Todd Lipcon (JIRA)"" <jira@apache.org> wrote:


   [
https://issues.apache.org/jira/browse/HBASE-2322?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12845188#action_12845188]

Todd Lipcon commented on HBASE-2322:
------------------------------------

(note that some of the ""holding:"" and ""taking:"" lines say writeLock when
they're really readLock... this RWLock support in jcarder is ""experimental""
but I think it mainly works ;-)

java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync),
java.util.HashSet),
java.util.concurrent.locks.ReentrantLock$NonfairSync),
java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync),
java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:807)
org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1299)
org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1281)
org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1789)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
org.apache.hadoop.hbase.regionserver.MemStoreFlusher.request(MemStoreFlusher.java:172)
org.apache.hadoop.hbase.regionserver.HRegion.requestFlush(HRegion.java:1549)
org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1534)
org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1318)
org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1281)
org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1789)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
java.util.concurrent.locks.ReentrantLock$NonfairSync)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:235)
org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:149)
java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:807)
org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:980)
org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:873)
org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:241)
org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushSomeRegions(MemStoreFlusher.java:352)
org.apache.hadoop.hbase.regionserver.MemStoreFlusher)
org.apache.hadoop.hbase.regionserver.MemStoreFlusher.reclaimMemStoreMemory(MemStoreFlusher.java:321)
org.apache.hadoop.hbase.regionserver.MemStoreFlusher)
org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1783)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)

--
This message is automatically generated by JIRA.
-
You can reply to this email to add a comment to the issue online.
;;;","16/Mar/10 03:39;stack;@Todd The jcarder find looks like the same thing.  Any chance of getting line numbers in there so can be for sure for sure?;;;","16/Mar/10 03:42;tlipcon;stack: yep, line number support is next on my jcarder todo list. Hopefully will get it done tonight or tomorrow on the train :);;;","16/Mar/10 06:46;tlipcon;Added line number support to jcarder, here's the pretty pic.;;;","16/Mar/10 15:15;stack;Thanks Todd.  This deadlock looks like its made from a different combination.  I need to make sure that after hbase-2248 goes in, that this combo is not possible.;;;","08/Apr/10 06:02;stack;I just ran a PE sequentialWrite against small cluster using 0.20_pre_durability branch and deadlocked pretty soon into the job.  See below.

{code}
Java stack information for the threads listed above:
===================================================
""IPC Server handler 59 on 60020"":
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007f8d8a3a30c8> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
        at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:807)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1292)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1274)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1794)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
""IPC Server handler 45 on 60020"":
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.request(MemStoreFlusher.java:172)
        - waiting to lock <0x00007f8d71145b08> (a java.util.HashSet)
        at org.apache.hadoop.hbase.regionserver.HRegion.requestFlush(HRegion.java:1543)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1528)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1311)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1274)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1794)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
""regionserver/10.20.20.189:60020.cacheFlusher"":
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007f8d7113f0d8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:253)
        - locked <0x00007f8d71145b08> (a java.util.HashSet)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:149)
""IPC Server handler 31 on 60020"":
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007f8d8a3a30c8> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
        at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:807)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:972)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:865)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:259)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushSomeRegions(MemStoreFlusher.java:379)
        - locked <0x00007f8d7111ba30> (a org.apache.hadoop.hbase.regionserver.MemStoreFlusher)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.reclaimMemStoreMemory(MemStoreFlusher.java:348)
        - locked <0x00007f8d7111ba30> (a org.apache.hadoop.hbase.regionserver.MemStoreFlusher)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1788)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)

Found 1 deadlock.
{code};;;","15/Apr/10 00:21;stack;I ran Todd's jcarder doing sequentialWriter of 1 and a scan...... should probably do more than 1... but it says no cycles.  I did the following to get it in there:

{code}
-exec ""$JAVA"" $JAVA_HEAP_MAX $HBASE_OPTS -classpath ""$CLASSPATH"" $CLASS ""$@""
+exec ""$JAVA"" -javaagent:/Users/Stack/checkouts/jcarder.jar=outputdir=/tmp/jc-@TIME@ $JAVA_HEAP_MAX $HBASE_OPTS -classpath ""$CLASSPATH"" $CLASS ""$@""
+#exec ""$JAVA"" $JAVA_HEAP_MAX $HBASE_OPTS -classpath ""$CLASSPATH"" $CLASS ""$@""
{code};;;","16/Apr/10 01:01;stack;At Todd's suggestion I used his version of jcarder because it does readwrite locks.  Its available here: http://github.com/toddlipcon/jcarder/tree/lockclasses  I ran a local test with 4 concurrent threads each loading 1M rows -- how I got the deadlock previously -- and then did analysis and it claimed no deadlocks:

{code}
stack:0.20_pre_durability Stack$ java -Xmx4G -jar ~/checkouts/jcarder/dist/jcarder.jar
Opening for reading: /Users/Stack/checkouts/0.20_pre_durability/jcarder_contexts.db
Opening for reading: /Users/Stack/checkouts/0.20_pre_durability/jcarder_events.db
Loaded from database files:
   Nodes: 166109
   Edges: 494196 (excluding 175530380 duplicated)

Cycle analysis result: 
   Cycles:          0
   Edges in cycles: 0
   Nodes in cycles: 0
   Max cycle depth: 0
   Max graph depth: 8

No cycles found!
{code}

I've been running multiple MR jobs over last day or so and we used deadlock reliably at 2% done or so.  I've exceeded this 2% many times since w/o deadlocking.

I'm going to say that this issue was fixed by hbase-2248.  Will open new issue if I see it again.;;;","16/Apr/10 01:02;stack;Fixed by hbase-2248.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Nit-pick about hbase-2279 shell fixup, if you do get with non-existant column family, throws lots of exceptions",HBASE-2313,12458885,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kovyrin,stack,stack,12/Mar/10 01:38,20/Nov/15 12:42,14/Jul/23 06:06,16/Mar/10 17:43,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"I believe the old shell would complain once only.. .or at least, it didn't print out so many fat exceptions.  New fixedup shell will retry a bunch of times with a fat stack trace each time.  Assigning Alexey 'cos he asked for it.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/10 22:54;kovyrin;HBASE-2313.2.patch;https://issues.apache.org/jira/secure/attachment/12438653/HBASE-2313.2.patch","12/Mar/10 23:00;kovyrin;HBASE-2313.3.patch;https://issues.apache.org/jira/secure/attachment/12438654/HBASE-2313.3.patch","12/Mar/10 23:04;kovyrin;HBASE-2313.4.patch;https://issues.apache.org/jira/secure/attachment/12438655/HBASE-2313.4.patch","16/Mar/10 16:54;kovyrin;HBASE-2313.5.patch;https://issues.apache.org/jira/secure/attachment/12438937/HBASE-2313.5.patch","12/Mar/10 22:12;kovyrin;HBASE-2313.patch;https://issues.apache.org/jira/secure/attachment/12438645/HBASE-2313.patch",,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26256,Reviewed,,,,Fri Nov 20 12:42:16 UTC 2015,,,,,,,,,,"0|i0hh7j:",100050,,,,,,,,,,,,,,,,,,,,,"12/Mar/10 22:12;kovyrin;Changed the way we handle exceptions in the shell by adding so called debug mode. By default debug mode is OFF which means that all errors will be displayed as one line comment:
{quote}
hbase(main):001:0> scan 'fooooo'
    
ERROR: Unknown table fooooo!
{quote}

When debug mode is ON, all errors would have a full backtrace displayed next to them:

{quote}
hbase(main):003:0> scan 'fooooo'
    
ERROR: Unknown table fooooo!
Backtrace: /Users/scoundrel/work/git/hbase/bin/../bin/../core/src/main/ruby/shell/commands.rb:46:in `check_table'
           /Users/scoundrel/work/git/hbase/bin/../bin/../core/src/main/ruby/shell/commands/scan.rb:26:in `command'
           /Users/scoundrel/work/git/hbase/bin/../bin/../core/src/main/ruby/shell/commands.rb:11:in `command_safe'
               /Users/scoundrel/work/git/hbase/bin/../bin/../core/src/main/ruby/shell.rb:84:in `command'
               (eval):2:in `scan'
               (hbase):4:in `irb_binding'
{quote}

To manage debug mode I've added two additional shell commands: debug and debug?:

{quote}
hbase(main):005:0> debug?
Debug mode is OFF

hbase(main):006:0> debug
Debug mode is ON

hbase(main):007:0> debug
Debug mode is OFF
{quote}

Additional change in this patch - now we check if a table exists before doing any operations on it.
;;;","12/Mar/10 22:13;kovyrin;Patch submitted, please review;;;","12/Mar/10 22:17;jdcryans;Patch looks good.

Is there a way to refactor check_table across all commands so that you don't have to call it everywhere?;;;","12/Mar/10 22:54;kovyrin;A better patch where we have a central place to translate hbase (and other java) errors to a messages for shell users. As an example I've moved table existence and column family existence checks there.;;;","12/Mar/10 23:00;kovyrin;A bit cleaner patch;;;","12/Mar/10 23:03;jdcryans;+1;;;","12/Mar/10 23:04;kovyrin;Removed formatting changes in describe.rb;;;","16/Mar/10 05:59;stack;Should 'debug' show in the help Alexey?  I don't see it.

I'm pretty sure I have the patch in place... but I'm seeing unexpected behaviors.

I did this:

{code}
pynchon-2:trunk stack$ ./bin/hbase shell
HBase Shell; enter 'help<RETURN>' for list of supported commands.
Type ""exit<RETURN>"" to leave the HBase Shell
Version: 0.21.0-SNAPSHOT, r923404, Mon Mar 15 22:38:16 PDT 2010

hbase(main):001:0> create 'y', 'y'
0 row(s) in 1.4720 seconds

hbase(main):002:0> put 'y', 'y', 'y:y', 'y'
0 row(s) in 0.0430 seconds

hbase(main):003:0> scan 'y'
ROW                          COLUMN+CELL                                                                      
 y                           column=y:y, timestamp=1268718421786, value=y                                     
1 row(s) in 0.0340 seconds

hbase(main):004:0> put 'y', 'y', 'z:z', 'y'
..
{code}

... and got a load of this:

{code}
java.util.concurrent.ExecutionException: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column family z does not exist in region y,,1268718407213 in table {NAME => 'y', FAMILIES => [{NAME => 'y', REPLICATION_SCOPE => '0', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}
        at org.apache.hadoop.hbase.regionserver.HRegion.checkFamily(HRegion.java:2438)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1276)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1243)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1714)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multiPut(HRegionServer.java:2448)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:576)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:919)

java.util.concurrent.ExecutionException: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column family z does not exist in region y,,1268718407213 in table {NAME => 'y', FAMILIES => [{NAME => 'y', REPLICATION_SCOPE => '0', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}
        at org.apache.hadoop.hbase.regionserver.HRegion.checkFamily(HRegion.java:2438)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1276)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1243)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1714)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multiPut(HRegionServer.java:2448)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:576)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:919)
..
{code}

Should I only see above if debug enabled?  it was a new shell.  I hadn't set debug.

It does the right thing nicely if I try to scan a nonexistent table.

Or, here if I try to create a new table on top of an extant one:

{code}
hbase(main):014:0> create 'x', 'x'         

ERROR: org.apache.hadoop.hbase.TableExistsException: org.apache.hadoop.hbase.TableExistsException: x
        at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:815)
        at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:780)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:576)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:919)

Here is some help for this command:
          Create table; pass table name, a dictionary of specifications per
          column family, and optionally a dictionary of table configuration.
          Dictionaries are described below in the GENERAL NOTES section.
          Examples:

          hbase> create 't1', {NAME => 'f1', VERSIONS => 5}
          hbase> create 't1', {NAME => 'f1'}, {NAME => 'f2'}, {NAME => 'f3'}
          hbase> # The above in shorthand would be the following:
          hbase> create 't1', 'f1', 'f2', 'f3'
          hbase> create 't1', {NAME => 'f1', VERSIONS => 1, TTL => 2592000,
                 BLOCKCACHE => true}
{code}

In the above case i'd not set debug?

;;;","16/Mar/10 16:54;kovyrin;Changes:
1) Rebased the patch against the latest trunk.
2) Added a nice message for TableExistsException errors
3) Reduced retries number for hbase clients (now we stop retrying after the first failure)

Regarding those retry exceptions, that's not something I could fix. There are two problems in there:
1) We suppress real exceptions from puts and throw this stupid RetriesExhaustedException - this makes it impossible for me to show a nice error mesage for puts
2) We have this System.out.println(e); in catch block of the processBatchOfPuts method - this makes output not so user friendly.
;;;","16/Mar/10 17:43;stack;Big improvement.  Thanks for the patch Alexey.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible data loss when RS goes into GC pause while rolling HLog,HBASE-2312,12458882,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,nspiegelberg,karthik.ranga,karthik.ranga,12/Mar/10 01:19,20/Nov/15 12:42,14/Jul/23 06:06,02/Nov/11 20:37,0.90.0,,,,,,,,,,,0.92.0,,master,regionserver,,,,0,,,"There is a very corner case when bad things could happen(ie data loss):

1)	RS #1 is going to roll its HLog - not yet created the new one, old one will get no more writes
2)	RS #1 enters GC Pause of Death
3)	Master lists HLog files of RS#1 that is has to split as RS#1 is dead, starts splitting
4)	RS #1 wakes up, created the new HLog (previous one was rolled) and appends an edit - which is lost

The following seems like a possible solution:

1)	Master detects RS#1 is dead
2)	The master renames the /hbase/.logs/<regionserver name>  directory to something else (say /hbase/.logs/<regionserver name>-dead)
3)	Add mkdir support (as opposed to mkdirs) to HDFS - so that a file create fails if the directory doesn't exist. Dhruba tells me this is very doable.
4)	RS#1 comes back up and is not able create the new hlog. It restarts itself.
",,chingshen,dhruba,kannanm,karthik.ranga,khemani,larsfrancke,larsh,mingma,ram_krish,tlipcon,wjiangwen,,,,,,,,,,,,,,,,,,,HBASE-2437,HDFS-1554,,,,,,,,HBASE-4744,HBASE-2593,,,,HBASE-4674,,,,,,,HADOOP-6886,HDFS-617,HADOOP-6840,,HBASE-2592,HDFS-1554,"28/Oct/11 02:39;phabricator@reviews.facebook.net;ASF.LICENSE.NOT.GRANTED--D99.1.patch;https://issues.apache.org/jira/secure/attachment/12501220/ASF.LICENSE.NOT.GRANTED--D99.1.patch","01/Nov/11 21:55;phabricator@reviews.facebook.net;ASF.LICENSE.NOT.GRANTED--D99.2.patch;https://issues.apache.org/jira/secure/attachment/12501846/ASF.LICENSE.NOT.GRANTED--D99.2.patch","01/Nov/11 22:09;phabricator@reviews.facebook.net;ASF.LICENSE.NOT.GRANTED--D99.3.patch;https://issues.apache.org/jira/secure/attachment/12501849/ASF.LICENSE.NOT.GRANTED--D99.3.patch","10/Nov/11 05:42;nspiegelberg;HBASE-2312.patch;https://issues.apache.org/jira/secure/attachment/12503169/HBASE-2312.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26255,,,,,Fri Nov 20 12:42:10 UTC 2015,,,,,,,,,,"0|i05i33:",30027,,,,,,,,,,,,,,,,,,,,,"12/Mar/10 01:23;tlipcon;Dhruba and I discussed this a little bit last night at the HBase meetup.

I agree that the proposed solution would work, but I don't really want to have to add another HDFS API here.

What about this idea:

1) RS #1 has written log.1, log.2, log.3
2) RS #1 is just about to write log.4 and enters gc pause before doing so
3) Master detects RS #1 dead
4) Master sees log.1, log.2, log.3. It then opens log.3 for append and also creates log.4 as a lock
5) RS #1 wakes up and isn't allowed to write to either log.3 or log.4 since HMaster holds both.

Alternatively, we discussed the idea of just adding a file like logs/.exclusive_lock in that directory. Whoever holds append lease on that file is allowed to write to those logs. Much simpler, no rolling, etc. What do you think?;;;","12/Mar/10 01:57;karthik.ranga;The .exclusive_lock will not work as the following two operations are not atomic:
-  Check if I own the lock
-  Create the file
The GC pause could hit between the two.

The other scheme - log.i - would work, but is not very clean. Right now the log names are of the format log.<timestamp>. In this scheme the master will have to list all the files, parse their names to find the current max i and then create the new file log.(i+1). Also, in your example, it is possible for the master to fail creating log.4 if the RS comes out of the GC pause - I guess it could create the next file, but doesn't feel like a clean solution. 

Something else we noticed when looking at the code is that HDFS would overwrite the existing file if another is created with the same name? If so, then the master can clobber log.4 created by the RS - not sure what would happen in this case.
Say in SequenceFile.java:838
fs.create(name, true, ...); // true is the overwrite flag

;;;","12/Mar/10 02:03;tlipcon;Right, I guess the problem with just using a file as a lock is that you also need to be appending/syncing to the file to know that you still have the lock.

bq. Something else we noticed when looking at the code is that HDFS would overwrite the existing file if another is created with the same name

Can't we modify the log code to not pass true for overwrite?

bq. Also, in your example, it is possible for the master to fail creating log.4 if the RS comes out of the GC pause

In that case, doesn't that mean the master should give up its recovery, since the RS is back to life?;;;","13/Mar/10 00:19;tlipcon;Just thought of another solution to this, which I think we had discussed a while back:

We can change the HLog rolling process to:

1) Write ""intend to roll HLog to new file hlog.N+1"" to hlog.N
2) Open hlog.N+1 for append
3) Write ""finished rolling"" to hlog.N
4) continue writing to hlog.N+1

This at step 3, both logs are open for append, and if the HMaster steals the write lease, the region server won't be able to successfully complete the log roll.;;;","15/Mar/10 19:02;karthik.ranga;Your last solution seems to be much better to implement - you are assuming that the HMaster opens the last log file first, right? This looks good to me. 

Will send out an email to hbase-dev to see if it sounds good everyone.

;;;","15/Mar/10 20:09;tlipcon;bq. you are assuming that the HMaster opens the last log file first

yea, and I think the HMaster needs to ""chase"" the regionserver - after it opens the last one, it looks for an ""intent to roll"" at the end, and if it finds it, opens the next one. It has to do this until it finds a log that doesn't end in ""intent to roll"" - otherwise we're susceptible to a really unlikely double-roll condition.;;;","15/Mar/10 21:06;karthik.ranga;A little confused about your comment. We have the follwing sequence of actions:

1) Write ""intend to roll HLog to new file hlog.N+1"" to hlog.N
2) Open hlog.N+1 for append
3) Write ""finished rolling"" to hlog.N
4) continue writing to hlog.N+1

If the GC pause hits before 2, no new log file is created. Master will take the append lease on log.N and step 3 will fail later. No edits could have gone into the new log.
If the GC pause hits after 3, the new log file is the one in effect, so no issues there.
If the GC pause hits after 2 but before 3, the master will always see the last log file (log.N+1) right? So master will try to take the append lease on log.N+1.
  - Master gets the append lease on log.N+1 in which case at the most RS does step 3 and fails on 4
  - Master does not get the lease on log.N+1, its still waiting for it, in which case the RS logs the edits to log.N+1 and then quits. Master does not lose the edits.

What is the scenario when the master chases the RS? The only thing I can think of is that step 2 takes a long time - but presumable the detection of the RS being dead takes longer?;;;","15/Mar/10 21:14;tlipcon;Yea, like I said I think it's really unlikely. What I'm thinking is something like:

|*RS*|*Master*|
|Writes ""intent to roll to hlog.2""|-|
|enters GC pause|-|
|-|detects RS dead, lists files, sees only up to hlog.1|
|comes back to life|-|
|opens hlog.2 for append|-|
|writes log.1 rolled, closes|-|
|-|sees intent to roll at end of hlog.1|
|writes some edits to hlog.2|-|
|writes ""intent to roll to hlog.3""|-|
|opens hlog.3 for append|-|
|writes ""rolled"" to hlog.2, closes|-|
|-|opens hlog.2 for append|
|-|*needs to check for intent to roll at end of hlog.2*|


Like I said this is very unlikely :);;;","15/Mar/10 21:17;tlipcon;Maybe we can actually avoid this situation. If the Master sees an ""intent to roll"" record that refers to a file it didn't see in the listStatus() it knows that the region server has actually made progress since it was originally detected dead. Thus, it can abort the whole recovery operation since the RS is known to be alive again.;;;","16/Mar/10 22:14;tlipcon;Having slept on it, I think the log chasing described above is actually avoidable. A couple different thoughts:

a) as above, I think we could be careful about using listStatus() to see whether the RS makes any progress
b) perhaps a very simple solution is to have the regionserver call ZK's sync() method before initiating metadata ops. This will limit the RS to a single metadata transition after it has been declared dead. This should be a useful pattern throughput HBase.

;;;","17/Mar/10 17:22;karthik.ranga;Just updating with the result of the discussion on hbase-dev: the consensus seems to be to use option #1.

See the thread here for the entire discussion:
http://www.mail-archive.com/hbase-dev@hadoop.apache.org/msg18210.html
;;;","25/Jun/10 00:51;nspiegelberg;need to port non-recursive create from 0.21 => 0.20-append to implement this fix;;;","26/Jun/10 08:08;kannanm;{quote}
1) Master detects RS#1 is dead
2) The master renames the /hbase/.logs/<regionserver name> directory to something else (say /hbase/.logs/<regionserver name>-dead)
3) Add mkdir support (as opposed to mkdirs) to HDFS - so that a file create fails if the directory doesn't exist. Dhruba tells me this is very doable.
4) RS#1 comes back up and is not able create the new hlog. It restarts itself.
{quote}

So in the proposed solution, what happens if the master dies after step #2? 

I suppose this is something the master failover (persisting master state) has to handle. But until then are we making things worse?;;;","28/Jul/10 05:51;nspiegelberg;Also needs HADOOP-6886 for local cluster testing to work correctly.;;;","28/Jul/10 06:17;hbasereviewboard;Message from: ""Nicolas"" <nspiegelberg@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/396/
-----------------------------------------------------------

Review request for hbase.


Summary
-------

There is a very corner case when bad things could happen(ie data loss):

1) RS #1 is going to roll its HLog - not yet created the new one, old one will get no more writes
2) RS #1 enters GC Pause of Death
3) Master lists HLog files of RS#1 that is has to split as RS#1 is dead, starts splitting
4) RS #1 wakes up, created the new HLog (previous one was rolled) and appends an edit - which is lost

Note that this fix requires a healthy dose of HDFS prerequisites: HDFS-617, HADOOP-6840, HADOOP-6886.  I encourage you to review those as well, give feedback, and hopefully give +1s so we can push the changes through.


This addresses bug HBASE-2312.
    http://issues.apache.org/jira/browse/HBASE-2312


Diffs
-----

  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 979953 
  trunk/src/main/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java 979953 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java 979953 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java 979953 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java 979953 

Diff: http://review.cloudera.org/r/396/diff


Testing
-------

mvn test;
bin/start-hbase.sh
bin/hbase shell < scan '.META.', get, put, etc


Thanks,

Nicolas


;;;","28/Jul/10 06:22;hbasereviewboard;Message from: ""Nicolas"" <nspiegelberg@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/396/#review510
-----------------------------------------------------------



trunk/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
<http://review.cloudera.org/r/396/#comment2052>

    will fix whitespace problems


- Nicolas



;;;","28/Jul/10 17:29;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/396/#review513
-----------------------------------------------------------


This looks good, except that it requires the HDFS patches to be in place to even compile. Although I think it's OK to not fix the bug in the case that we're on stock HDFS, I think we still need to be able to run, buggily.

Did we determine that all of the other solutions were flawed/too complicated?


trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.cloudera.org/r/396/#comment2061>

    Check return value of rename (this is one of those stupid APIs that returns false instead of throwing)



trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
<http://review.cloudera.org/r/396/#comment2062>

    rather than reassigning, just pass splitDir below?



trunk/src/main/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java
<http://review.cloudera.org/r/396/#comment2064>

    extract ""-splitting"" out to a constant?
    Or can we move some of this common code into HLog?



trunk/src/main/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java
<http://review.cloudera.org/r/396/#comment2063>

    maybe:
    assert !(logDirExists && splitDirExists) : ""Both "" + rsLogDir + "" and "" + rsSplitDir + "" exist"";
    so if the assertion failure happens it's more understandable


- Todd



;;;","28/Jul/10 21:21;hbasereviewboard;Message from: ""Nicolas"" <nspiegelberg@facebook.com>


bq.  On 2010-07-28 10:17:14, Todd Lipcon wrote:
bq.  > This looks good, except that it requires the HDFS patches to be in place to even compile. Although I think it's OK to not fix the bug in the case that we're on stock HDFS, I think we still need to be able to run, buggily.
bq.  > 
bq.  > Did we determine that all of the other solutions were flawed/too complicated?

HBASE-2312 had some lengthy discussions that ultimately led to this route.  For 0.21 + 0.22, only HADOOP-6840 is needed.  That is a very minor change consisting of only small API additions (so, no regression needed) that Dhruba & I think the lack of risk will make it easy to approve.  0.20-append needs a couple more JIRAs, but that should be even less flack.  As long as we ship with 0.20-append HDFS or newer, we'll be fine.

I understand that this diff is early and we will have to wait until we ship with the 0.20-append JAR before application.  Basically, I also wanted to show that the API change indeed fixes our issue painlessly.  Any +1s on HADOOP-6840 would be appreciated :)  Your comment is that we should also backward-support 0.20.3?


bq.  On 2010-07-28 10:17:14, Todd Lipcon wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java, line 650
bq.  > <http://review.cloudera.org/r/396/diff/1/?file=3523#file3523line650>
bq.  >
bq.  >     Check return value of rename (this is one of those stupid APIs that returns false instead of throwing)

will do


bq.  On 2010-07-28 10:17:14, Todd Lipcon wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java, line 651
bq.  > <http://review.cloudera.org/r/396/diff/1/?file=3523#file3523line651>
bq.  >
bq.  >     rather than reassigning, just pass splitDir below?

there are 2 cases:
#1 : the master hasn't already tried to split, hits this code
#2 : master tries to split, dies while splitting, backup master appears

In case #2, you don't need to rename the directory because an region cannot run on a directory of that name.  Therefore, the 'logDir' is correct from the get-go and doesn't need a rename.  So, you need to either have duplicate splitLog code or update 'logDir'


bq.  On 2010-07-28 10:17:14, Todd Lipcon wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java, line 297
bq.  > <http://review.cloudera.org/r/396/diff/1/?file=3524#file3524line297>
bq.  >
bq.  >     extract ""-splitting"" out to a constant?
bq.  >     Or can we move some of this common code into HLog?

sure.  will do


bq.  On 2010-07-28 10:17:14, Todd Lipcon wrote:
bq.  > trunk/src/main/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java, line 300
bq.  > <http://review.cloudera.org/r/396/diff/1/?file=3524#file3524line300>
bq.  >
bq.  >     maybe:
bq.  >     assert !(logDirExists && splitDirExists) : ""Both "" + rsLogDir + "" and "" + rsSplitDir + "" exist"";
bq.  >     so if the assertion failure happens it's more understandable

will do


- Nicolas


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/396/#review513
-----------------------------------------------------------



;;;","28/Jul/10 21:26;hbasereviewboard;Message from: ""Todd Lipcon"" <todd@cloudera.com>


bq.  On 2010-07-28 10:17:14, Todd Lipcon wrote:
bq.  > This looks good, except that it requires the HDFS patches to be in place to even compile. Although I think it's OK to not fix the bug in the case that we're on stock HDFS, I think we still need to be able to run, buggily.
bq.  > 
bq.  > Did we determine that all of the other solutions were flawed/too complicated?
bq.  
bq.  Nicolas wrote:
bq.      HBASE-2312 had some lengthy discussions that ultimately led to this route.  For 0.21 + 0.22, only HADOOP-6840 is needed.  That is a very minor change consisting of only small API additions (so, no regression needed) that Dhruba & I think the lack of risk will make it easy to approve.  0.20-append needs a couple more JIRAs, but that should be even less flack.  As long as we ship with 0.20-append HDFS or newer, we'll be fine.
bq.      
bq.      I understand that this diff is early and we will have to wait until we ship with the 0.20-append JAR before application.  Basically, I also wanted to show that the API change indeed fixes our issue painlessly.  Any +1s on HADOOP-6840 would be appreciated :)  Your comment is that we should also backward-support 0.20.3?

Yea, I think we decided at one point that we should be able to run against a vanilla apache cluster, just that it would be ""at your own risk"" - ie that the bug fixes wouldn't necessarily work. EG this is why we do the reflection to check for the syncFs() method and warn in the case when it's not there, but continue to function.

In this patch, it would actually fail to work at all, since the RPC for non-recursive create would get an error at the NN.


- Todd


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/396/#review513
-----------------------------------------------------------



;;;","28/Jul/10 21:40;hbasereviewboard;Message from: stack@duboce.net


bq.  On 2010-07-28 10:17:14, Todd Lipcon wrote:
bq.  > This looks good, except that it requires the HDFS patches to be in place to even compile. Although I think it's OK to not fix the bug in the case that we're on stock HDFS, I think we still need to be able to run, buggily.
bq.  > 
bq.  > Did we determine that all of the other solutions were flawed/too complicated?
bq.  
bq.  Nicolas wrote:
bq.      HBASE-2312 had some lengthy discussions that ultimately led to this route.  For 0.21 + 0.22, only HADOOP-6840 is needed.  That is a very minor change consisting of only small API additions (so, no regression needed) that Dhruba & I think the lack of risk will make it easy to approve.  0.20-append needs a couple more JIRAs, but that should be even less flack.  As long as we ship with 0.20-append HDFS or newer, we'll be fine.
bq.      
bq.      I understand that this diff is early and we will have to wait until we ship with the 0.20-append JAR before application.  Basically, I also wanted to show that the API change indeed fixes our issue painlessly.  Any +1s on HADOOP-6840 would be appreciated :)  Your comment is that we should also backward-support 0.20.3?
bq.  
bq.  Todd Lipcon wrote:
bq.      Yea, I think we decided at one point that we should be able to run against a vanilla apache cluster, just that it would be ""at your own risk"" - ie that the bug fixes wouldn't necessarily work. EG this is why we do the reflection to check for the syncFs() method and warn in the case when it's not there, but continue to function.
bq.      
bq.      In this patch, it would actually fail to work at all, since the RPC for non-recursive create would get an error at the NN.

We'll ship with a branch-0.20-append but yeah, current thinking is that we should be able to run on an hadoop 0.20.x that does not have a working sync.

We've been known to change our minds.  Start a discussion out on dev list if want to argue hbase 0.90.x requires a working sync.


- stack


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/396/#review513
-----------------------------------------------------------



;;;","28/Jul/10 22:04;hbasereviewboard;Message from: ""Nicolas"" <nspiegelberg@facebook.com>


bq.  On 2010-07-28 10:17:14, Todd Lipcon wrote:
bq.  > This looks good, except that it requires the HDFS patches to be in place to even compile. Although I think it's OK to not fix the bug in the case that we're on stock HDFS, I think we still need to be able to run, buggily.
bq.  > 
bq.  > Did we determine that all of the other solutions were flawed/too complicated?
bq.  
bq.  Nicolas wrote:
bq.      HBASE-2312 had some lengthy discussions that ultimately led to this route.  For 0.21 + 0.22, only HADOOP-6840 is needed.  That is a very minor change consisting of only small API additions (so, no regression needed) that Dhruba & I think the lack of risk will make it easy to approve.  0.20-append needs a couple more JIRAs, but that should be even less flack.  As long as we ship with 0.20-append HDFS or newer, we'll be fine.
bq.      
bq.      I understand that this diff is early and we will have to wait until we ship with the 0.20-append JAR before application.  Basically, I also wanted to show that the API change indeed fixes our issue painlessly.  Any +1s on HADOOP-6840 would be appreciated :)  Your comment is that we should also backward-support 0.20.3?
bq.  
bq.  Todd Lipcon wrote:
bq.      Yea, I think we decided at one point that we should be able to run against a vanilla apache cluster, just that it would be ""at your own risk"" - ie that the bug fixes wouldn't necessarily work. EG this is why we do the reflection to check for the syncFs() method and warn in the case when it's not there, but continue to function.
bq.      
bq.      In this patch, it would actually fail to work at all, since the RPC for non-recursive create would get an error at the NN.
bq.  
bq.  stack wrote:
bq.      We'll ship with a branch-0.20-append but yeah, current thinking is that we should be able to run on an hadoop 0.20.x that does not have a working sync.
bq.      
bq.      We've been known to change our minds.  Start a discussion out on dev list if want to argue hbase 0.90.x requires a working sync.

For this patch to work (or at least, not degrade) 0.20, we just need to switch on the SequenceFile() constructor.  I'll look into that.


- Nicolas


-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/396/#review513
-----------------------------------------------------------



;;;","05/Aug/10 22:09;hbasereviewboard;Message from: ""Nicolas"" <nspiegelberg@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/396/
-----------------------------------------------------------

(Updated 2010-08-05 14:55:17.886769)


Review request for hbase.


Changes
-------

updated per Todd's comments.  uses reflection for the new API, so now compiles properly with patched + unpatched clients


Summary
-------

There is a very corner case when bad things could happen(ie data loss):

1) RS #1 is going to roll its HLog - not yet created the new one, old one will get no more writes
2) RS #1 enters GC Pause of Death
3) Master lists HLog files of RS#1 that is has to split as RS#1 is dead, starts splitting
4) RS #1 wakes up, created the new HLog (previous one was rolled) and appends an edit - which is lost

Note that this fix requires a healthy dose of HDFS prerequisites: HDFS-617, HADOOP-6840, HADOOP-6886.  I encourage you to review those as well, give feedback, and hopefully give +1s so we can push the changes through.


This addresses bug HBASE-2312.
    http://issues.apache.org/jira/browse/HBASE-2312


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/HConstants.java 982788 
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 982788 
  trunk/src/main/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java 982788 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java 982788 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java 982788 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java 982788 

Diff: http://review.cloudera.org/r/396/diff


Testing
-------

mvn test;
bin/start-hbase.sh
bin/hbase shell < scan '.META.', get, put, etc


Thanks,

Nicolas


;;;","08/Aug/10 16:41;stack;Submitting patch; v2 is up on review.hbase.org.;;;","08/Aug/10 17:03;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/396/#review752
-----------------------------------------------------------

Ship it!


Looks good to me.

- stack



;;;","09/Aug/10 19:52;stack;When I apply this patch to trunk, unit tests hang.  Are we missing something from our hadoop Nicolas?  We have 0.20.3-append-r964955-1240 committed to trunk (Thats the svn revision and patch 1240 applied).

I see for example that the TestZookeeper is stuck doing this:

{code}

2010-08-09 12:45:23,964 WARN  [IPC Server handler 6 on 62023] namenode.FSNamesystem(1166): DIR* NameSystem.startFile: failed to create file /user/Stack/.logs/h135.sfo.stumble.net,62051,1281382952055-splitting/10.10.1.135%3A62051.1281382952241 for DFSClient_-1591590456 on client 127.0.0.1, because this file is already being created by DFSClient_hb_m_10.10.1.135:62037 on 127.0.0.1
2010-08-09 12:45:23,965 WARN  [master] util.FSUtils(631): Waited 151520ms for lease recovery on hdfs://localhost:62023/user/Stack/.logs/h135.sfo.stumble.net,62051,1281382952055-splitting/10.10.1.135%3A62051.1281382952241:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /user/Stack/.logs/h135.sfo.stumble.net,62051,1281382952055-splitting/10.10.1.135%3A62051.1281382952241 for DFSClient_-1591590456 on client 127.0.0.1, because this file is already being created by DFSClient_hb_m_10.10.1.135:62037 on 127.0.0.1
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1093)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:1181)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.append(NameNode.java:396)
        at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:961)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:957)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:955)
{code}

This test passes if I do not have /HBASE-2312-2.patch  in place.;;;","11/Nov/10 20:38;stack;Moving out of 0.90 after some discussion off-list between Nicolas and Jon.  Not having this issue in place means possible ""dataloss"" (Nicolas)... but Jon comes back ""It's data loss only in case where you have a GC pause of death (long enough that the RS is expired) and then I think there's one or two more edge cases that would have to be hit, and then could be data loss.""

Leaving the issue critical but not a blocker on 0.90.;;;","15/Dec/10 22:52;nspiegelberg;note that we hit this issue twice this week.  raised priority for us;;;","16/Dec/10 06:39;stack;Is patch ready to go Nicolas?;;;","22/Dec/10 03:08;hbasereviewboard;Message from: ""Nicolas"" <nspiegelberg@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/396/
-----------------------------------------------------------

(Updated 2010-12-21 19:06:32.166768)


Review request for hbase.


Changes
-------

Version for 0.90.  This version utilizes a new HDFS patch to forcibly recover a file lease (forthcoming).  TestZooKeeper will fail without this patch because it needs to wait until the soft lease expires otherwise.


Summary
-------

There is a very corner case when bad things could happen(ie data loss):

1) RS #1 is going to roll its HLog - not yet created the new one, old one will get no more writes
2) RS #1 enters GC Pause of Death
3) Master lists HLog files of RS#1 that is has to split as RS#1 is dead, starts splitting
4) RS #1 wakes up, created the new HLog (previous one was rolled) and appends an edit - which is lost

Note that this fix requires a healthy dose of HDFS prerequisites: HDFS-617, HADOOP-6840, HADOOP-6886.  I encourage you to review those as well, give feedback, and hopefully give +1s so we can push the changes through.


This addresses bug HBASE-2312.
    http://issues.apache.org/jira/browse/HBASE-2312


Diffs (updated)
-----

  trunk/src/main/java/org/apache/hadoop/hbase/HConstants.java 1051398 
  trunk/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java 1051398 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java 1051398 
  trunk/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java 1051398 
  trunk/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java 1051398 
  trunk/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java 1051398 

Diff: http://review.cloudera.org/r/396/diff


Testing
-------

mvn test;
bin/start-hbase.sh
bin/hbase shell < scan '.META.', get, put, etc


Thanks,

Nicolas


;;;","22/Dec/10 21:15;hbasereviewboard;Message from: stack@duboce.net

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/396/#review2146
-----------------------------------------------------------

Ship it!


Below looks good.  Doesn't work w/o the hadoop issues?  I still need to review those?

- stack



;;;","04/Mar/11 18:42;dhruba;hi folks, I think we would like this to get into the trunk and 0.90 release because it is a data-loss issue. Is that possible?;;;","04/Mar/11 18:54;stack;Looking... (I'm on HBASE-3285, which overlaps this patch).;;;","04/Mar/11 23:30;stack;I reviewed the patch again.  The necessary createWriter method is missing.  You want to apply the listed patches to 0.20-append Dhruba (HADOOP-6840, HADOOP-6886)?  They look pretty big but they each seem to have append branch versions.  Are you running them at your house?  Nicolas is using reflection to look for the features presence so we can apply it (The test his patch adds fails though because of the missing functionality -- I can fix that on commit).;;;","04/Mar/11 23:31;stack;Oh, HDFS-617 too.;;;","04/Mar/11 23:31;stack;And this patch causes TestReplication to fail.  I can look into that.;;;","24/Mar/11 05:14;stack;Moving to 0.92.  We don't have the API in HDFS to support Nicolas's patch.;;;","26/Mar/11 13:06;wjiangwen;the possible solution.
follow the process to roll log
1. create a new log file
2. check is alive on zookeeper
3. append edits;;;","24/Oct/11 23:16;stack;We don't have the hdfs support for this (0.20.205.0 or CDH).  Moving out to 0.94.0.;;;","24/Oct/11 23:24;nspiegelberg;@stack: since we have proper reflection, can we not apply this jira & mark the test as broken?  that way, customers can have this fix if they upgrade to the proper version of HDFS instead of requiring an HBase upgrade as well;;;","25/Oct/11 03:56;stack;Yes Nicolas.  Pulling it back in.  Let me try and apply latest version of the patch.;;;","25/Oct/11 04:05;nspiegelberg;tried to apply this onto our internal 92 branch and noticed that the Distributed Log Splitter broke this patch.  Will work and resubmit.  I think this is critical to get into 92 because of data loss issues, however.;;;","28/Oct/11 02:39;phabricator@reviews.facebook.net;nspiegelberg requested code review of ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".
Reviewers: JIRA

  HBASE-2312 Possible data loss when RS goes into GC pause while rolling HLog

  There is a very corner case when bad things could happen(ie data loss):

  1)	RS #1 is going to roll its HLog - not yet created the new one, old one will get no more writes
  2)	RS #1 enters GC Pause of Death
  3)	Master lists HLog files of RS#1 that is has to split as RS#1 is dead, starts splitting
  4)	RS #1 wakes up, created the new HLog (previous one was rolled) and appends an edit - which is lost

  The following seems like a possible solution:

  1)	Master detects RS#1 is dead
  2)	The master renames the /hbase/.logs/<regionserver name>  directory to something else (say /hbase/.logs/<regionserver name>-dead)
  3)	Add mkdir support (as opposed to mkdirs) to HDFS - so that a file create fails if the directory doesn't exist. Dhruba tells me this is very doable.
  4)	RS#1 comes back up and is not able create the new hlog. It restarts itself.

TEST PLAN
  EMPTY

REVISION DETAIL
  https://reviews.facebook.net/D99

AFFECTED FILES
  src/main/java/org/apache/hadoop/hbase/HConstants.java
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java
  src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
  src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java

MANAGE HERALD DIFFERENTIAL RULES
  https://reviews.facebook.net/herald/view/differential/

WHY DID I GET THIS EMAIL?
  https://reviews.facebook.net/herald/transcript/213/

Tip: use the X-Herald-Rules header to filter Herald messages in your client.
;;;","28/Oct/11 02:43;nspiegelberg;Have a review up that integrates distributed log splitting. Note that it also fixes that elusive bug in TestReplication, which was the original reason we delayed checking this patch in.  The problem was that ReplicationSource would check in the alive RS folder instead of the dead RS folder to verify that it should stall and wait for Log Splitting to finish and move to the OldLogs directory.  If JD could please verify.  I think Prakash has a little more work to do here for the ProcessServerDeath case, but this is an existing bug.  We should file another JIRA for that and get this one committed :);;;","28/Oct/11 04:16;phabricator@reviews.facebook.net;tedyu has commented on the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".

INLINE COMMENTS
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:232 Should this assignment be exchanged with line 231 ?
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:256 If fs.rename() failed, should this assignment be skipped if fs.exists(splitDir) returns false ?
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:275 Should read "" because of:""
  src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java:831 Where would this sentence come from ?

REVISION DETAIL
  https://reviews.facebook.net/D99
;;;","28/Oct/11 04:24;phabricator@reviews.facebook.net;nspiegelberg has commented on the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".

INLINE COMMENTS
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:232 231 basically resets the interrupt flag so the next blocking call will get an InterruptException.  The other acceptable way to handle this scenario is to throw an InterruptedIOException, but we don't need the split to finish, so an IOE really isn't necessary.
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:254 this needs an IOE.  patch porting problem
  src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java:831 from JUnit documentation

    @Ignore takes an optional default parameter if you want to record why a test is being ignored

REVISION DETAIL
  https://reviews.facebook.net/D99
;;;","28/Oct/11 04:32;phabricator@reviews.facebook.net;tedyu has commented on the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".

INLINE COMMENTS
  src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java:831 HBase TRUNK uses 0.20.205
  The @Ignore can be omitted, right ?

REVISION DETAIL
  https://reviews.facebook.net/D99
;;;","28/Oct/11 04:36;phabricator@reviews.facebook.net;stack has requested changes to the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".

  Looks good.  A few minor items.

INLINE COMMENTS
  src/main/java/org/apache/hadoop/hbase/HConstants.java:188 Should this define be up here?  Why not down in HLog?  (I favor keeping defines in classes they pertain to -- could be ugly though if we need to go cross packages to get at a define.  Master package would need to reach down into the wal package?  If so, I suppose, leave it up here in HConstants).
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:192 The former code bracketed the return.  Either put the return on same line as if or else add back the brackets I'd say.
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:223 Why not call server abort?  Thats the usual idiom.
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:227 Why this?  And 30 seconds is a long time (Should it be configurable)?
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:254 I suppose this not the end of the world.  A regionserver could add a new log I suppose but chances of failed rename and RS adding new WAL are probably fairly low.
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java:126 Superfluous logging I'd say.  Especially at info level.  Log when we DON'T have this feature (maybe do it debug; if every file open has one of these info logs, then its going to generate lots of queries up in mailing lists, etc.  At DEBUG it won't seem as critical)
  src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java:831 Nice

REVISION DETAIL
  https://reviews.facebook.net/D99
;;;","28/Oct/11 15:11;phabricator@reviews.facebook.net;Kannan has added CCs to the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".
Added CCs: Kannan

REVISION DETAIL
  https://reviews.facebook.net/D99
;;;","28/Oct/11 17:11;phabricator@reviews.facebook.net;nspiegelberg has commented on the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".

INLINE COMMENTS
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:192 will fix.  fyi : this got auto-formatted by our apache template
  src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java:831 @tedyu this patch is for 0.92.  please read the JIRA thread before this patch

REVISION DETAIL
  https://reviews.facebook.net/D99
;;;","28/Oct/11 17:19;yuzhihong@gmail.com;0.92 is using hadoop 0.20.205 to build.
{code}
zhihyu$ svn log pom.xml | head
------------------------------------------------------------------------
r1188404 | stack | 2011-10-24 14:57:04 -0700 (Mon, 24 Oct 2011) | 1 line

HBASE-4437 Update hadoop in 0.92 (0.20.205?)
{code};;;","28/Oct/11 17:34;yuzhihong@gmail.com;I tried to run TestHLogSplit#testLogRollAfterSplitStart after commenting out @Ignore directive under 0.92 and TRUNK.
I got same result:
{code}
Failed tests:   testLogRollAfterSplitStart(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit): rollWriter() did not throw any exception.

Tests run: 1, Failures: 1, Errors: 0, Skipped: 0
{code};;;","28/Oct/11 22:28;phabricator@reviews.facebook.net;nspiegelberg has commented on the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".

  sending a new diff.  note that Phabricator has a nifty diff-compare tool for you to make it easy to see my code changes in response to your commentary  :)

INLINE COMMENTS
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:223 Prakash did the Exception handling part of this code, so I'll let him follow up if you need more info.  Basically, we are still in the initialization phase (we are in the server abort thread right now), where server abort should be used during normal operation.

REVISION DETAIL
  https://reviews.facebook.net/D99
;;;","28/Oct/11 23:09;phabricator@reviews.facebook.net;khemani has commented on the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".

INLINE COMMENTS
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:223 Basically what Nicolas said ... setting the abort flag here will not have any immediate effect.

  From HMaster code
    status.setStatus(""Splitting logs after master startup"");
      this.fileSystemManager.
    splitLogAfterStartup(this.serverManager.getOnlineServers().keySet());
    // Make sure root and meta assigned before proceeding.
    assignRootAndMeta(status);

  the master thread will try to assignRootAndMeta() even if splitLogAfterStartup() returns with abort flag set. May be if HMaster.abort() had set the interrupt flag on at least the main master thread then it might have worked ...

REVISION DETAIL
  https://reviews.facebook.net/D99
;;;","28/Oct/11 23:27;phabricator@reviews.facebook.net;stack has commented on the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".

  I don't see the new diff Nicolas.  I presume you have not uploaded it yet?

REVISION DETAIL
  https://reviews.facebook.net/D99
;;;","01/Nov/11 19:34;nspiegelberg;It doesn't look like 0.20.0.205 has any of the HDFS/Hadoop patches that we need here.  That is supposed to be the append release, correct?  I worked with Suresh and some other HDFS committers to get this in a couple months ago.  Maybe something got lost in the notes?;;;","01/Nov/11 21:55;phabricator@reviews.facebook.net;nspiegelberg updated the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".
Reviewers: JIRA, stack

  Addressed Stack & Ted's change requests.  Integrated some fixes that Prakash found.

REVISION DETAIL
  https://reviews.facebook.net/D99

AFFECTED FILES
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
  src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java
  src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
  src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
;;;","01/Nov/11 22:09;phabricator@reviews.facebook.net;nspiegelberg updated the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".
Reviewers: JIRA, stack

  Minor touchups + reinsert @Ignore since 0.20.205 doesn't have the critical patches necessary.  205.1 should however

REVISION DETAIL
  https://reviews.facebook.net/D99

AFFECTED FILES
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
  src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java
  src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
  src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
;;;","02/Nov/11 05:34;phabricator@reviews.facebook.net;khemani has accepted the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".

  Looks good to me.

INLINE COMMENTS
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:201-205 In the original patch the -splitting wouldn't be stripped off over here.  With the current diff, a region-server which was earlier known to be dead will get another opportunity to be treated as alive. But I don't think that will happen and this is fine.

REVISION DETAIL
  https://reviews.facebook.net/D99
;;;","02/Nov/11 05:56;phabricator@reviews.facebook.net;stack has commented on the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".

  +1 on commit.  Minor comments that should not get in way of this patch.  Attach the patch Nicolas to JIRA and then submit the patch and let the patch-build run its course.  Good stuff.

INLINE COMMENTS
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:208 trivial nit -- not important: FYI, I prefer the operator at EOF rather than at start; when a hanging operator, its useful as signifier of more to come (Did you format this with your apache formatter?)
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:201 Next time, this messing w/ suffix belongs out in a method.
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:253 Should we throw only if ""hbase.hlog.split.skip.errors"" is NOT set?
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java:84 These gymnastics belong out in a dedicated method rather than inline here in init method.  Next time.

REVISION DETAIL
  https://reviews.facebook.net/D99
;;;","02/Nov/11 05:58;phabricator@reviews.facebook.net;stack has commented on the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".

  Hmm... how do I get the nice green border around my comment like Prakash got?  Do I 'Resign as reviewer'?

REVISION DETAIL
  https://reviews.facebook.net/D99
;;;","02/Nov/11 19:17;phabricator@reviews.facebook.net;nspiegelberg has commented on the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".

  @stack the green awesome border goes around comments where the action is ""Accept Diff"".  It's the equivalent of ""ship it"" on Review Board.

INLINE COMMENTS
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:201-205 I think that ServerName.parseServerName() will get a parse error unless we do this.  Will the server get a chance to be treated as alive?  I'm guessing something like your scenario could only happen if the same server rebooted and re-registered before we got here.  In that case, the start code will be different.
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:208 yes.  this was formatted with the apache thingy
  src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java:253 hmmm.  Interesting question.  We've never actually handled this case (even before distributed log splitting).  I think we should punt on this for right now and address it later.  Maybe a good idea would be to ""continue"" instead of throw an error

REVISION DETAIL
  https://reviews.facebook.net/D99
;;;","02/Nov/11 19:31;phabricator@reviews.facebook.net;nspiegelberg has committed the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".

REVISION DETAIL
  https://reviews.facebook.net/D99

COMMITS
  https://reviews.facebook.net/rHBASE1196773
  https://reviews.facebook.net/rHBASENINETWOBRANCH1196768
;;;","02/Nov/11 21:33;phabricator@reviews.facebook.net;stack has commented on the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".

  I don't seem to have the 'accept patch' option (Pardon my being dumb on this stuff)

REVISION DETAIL
  https://reviews.facebook.net/D99

COMMITS
  https://reviews.facebook.net/rHBASE1196773
  https://reviews.facebook.net/rHBASENINETWOBRANCH1196768
;;;","02/Nov/11 21:37;phabricator@reviews.facebook.net;nspiegelberg has commented on the revision ""HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling HLog"".

  @stack you don't have that option because the diff has already been accepted by Prakash.  Can only 'ship it' once.

REVISION DETAIL
  https://reviews.facebook.net/D99

COMMITS
  https://reviews.facebook.net/rHBASE1196773
  https://reviews.facebook.net/rHBASENINETWOBRANCH1196768
;;;","02/Nov/11 22:46;hudson;Integrated in HBase-TRUNK #2402 (See [https://builds.apache.org/job/HBase-TRUNK/2402/])
    HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling
HLog

Summary:
There is a very corner case when bad things could happen(ie data loss):

1)	RS #1 is going to roll its HLog - not yet created the new one, old one will
get no more writes
2)	RS #1 enters GC Pause of Death
3)	Master lists HLog files of RS#1 that is has to split as RS#1 is dead, starts
splitting
4)	RS #1 wakes up, created the new HLog (previous one was rolled) and appends an
edit - which is lost

The following seems like a possible solution:

1)	Master detects RS#1 is dead
2)	The master renames the /hbase/.logs/<regionserver name>  directory to
something else (say /hbase/.logs/<regionserver name>-dead)
3)	Add mkdir support (as opposed to mkdirs) to HDFS - so that a file create
fails if the directory doesn't exist. Dhruba tells me this is very doable.
4)	RS#1 comes back up and is not able create the new hlog. It restarts itself.

Test Plan: EMPTY
Reviewers: JIRA, stack, khemani
Reviewed By: khemani
CC: tedyu, nspiegelberg, stack, Kannan, khemani, jgray
Differential Revision: 99

nspiegelberg : 
Files : 
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java
* /hbase/trunk/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
* /hbase/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
;;;","03/Nov/11 02:11;hudson;Integrated in HBase-0.92 #102 (See [https://builds.apache.org/job/HBase-0.92/102/])
    HBASE-2312 [jira] Possible data loss when RS goes into GC pause while rolling
HLog

Summary:
There is a very corner case when bad things could happen(ie data loss):

1)	RS #1 is going to roll its HLog - not yet created the new one, old one will
get no more writes
2)	RS #1 enters GC Pause of Death
3)	Master lists HLog files of RS#1 that is has to split as RS#1 is dead, starts
splitting
4)	RS #1 wakes up, created the new HLog (previous one was rolled) and appends an
edit - which is lost

The following seems like a possible solution:

1)	Master detects RS#1 is dead
2)	The master renames the /hbase/.logs/<regionserver name>  directory to
something else (say /hbase/.logs/<regionserver name>-dead)
3)	Add mkdir support (as opposed to mkdirs) to HDFS - so that a file create
fails if the directory doesn't exist. Dhruba tells me this is very doable.
4)	RS#1 comes back up and is not able create the new hlog. It restarts itself.

Test Plan: EMPTY
Reviewers: JIRA, stack, khemani
Reviewed By: khemani
CC: tedyu, nspiegelberg, stack, Kannan, khemani, jgray
Differential Revision: 99

nspiegelberg : 
Files : 
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java
* /hbase/branches/0.92/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
* /hbase/branches/0.92/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
;;;","10/Nov/11 05:42;nspiegelberg;forgot to attach jira with the apache license grant;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not having assertions enabled causes index contrib tests to fail,HBASE-2311,12458880,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,12/Mar/10 01:05,20/Nov/15 12:43,14/Jul/23 06:06,12/Mar/10 01:08,,,,,,,,,,,,0.90.0,,,,,,,0,,,,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/10 01:06;stack;TestBitSet-fix.patch;https://issues.apache.org/jira/secure/attachment/12438563/TestBitSet-fix.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26254,Reviewed,,,,Fri Nov 20 12:43:02 UTC 2015,,,,,,,,,,"0|i0hh7b:",100049,,,,,,,,,,,,,,,,,,,,,"12/Mar/10 01:06;stack;If  no assertions enabled, print message but don't fail.;;;","12/Mar/10 01:08;stack;Committed to branch.  Hopefully fixes hudson breakage.;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix the bin/rename_table.rb script, make it work again",HBASE-2308,12458738,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,10/Mar/10 21:33,20/Nov/15 12:40,14/Jul/23 06:06,10/Mar/10 23:28,,,,,,,,,,,,0.90.0,,,,,,,0,,,bin/rename_table.rb script rotted.  Bring it back up to date (Someone asked nicely for it up on IRC).,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/10 21:36;stack;2308.patch;https://issues.apache.org/jira/secure/attachment/12438451/2308.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26253,,,,,Fri Nov 20 12:40:40 UTC 2015,,,,,,,,,,"0|i0hh6n:",100046,,,,,,,,,,,,,,,,,,,,,"10/Mar/10 21:36;stack;This version will work against a disabled table.  You don't have to shut down the cluster as you used to have to.

So, to use, first disable table.  In shell do this:

hbase> disable 'TABLE_NAME'

Then, run the script from the command-line:

./bin/hbase org.jruby.Main bin/rename_table.rb TABLE_NAME NEW_TABLE_NAME

Then reenable the table by doing below in shell:

hbase> enable 'NEW_TABLE_NAME';;;","10/Mar/10 23:28;stack;Applied branch and trunk.;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:40;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"hbase-2295 changed hregion size, testheapsize broke... fix it.",HBASE-2307,12458675,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,10/Mar/10 15:44,20/Nov/15 12:41,14/Jul/23 06:06,10/Mar/10 23:33,,,,,,,,,,,,0.90.0,,,,,,,0,,,Both trunk and branch are broke at mo.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/10 23:24;stack;size.patch;https://issues.apache.org/jira/secure/attachment/12438460/size.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26252,,,,,Fri Nov 20 12:41:05 UTC 2015,,,,,,,,,,"0|i0hh6f:",100045,,,,,,,,,,,,,,,,,,,,,"10/Mar/10 23:24;stack;Change HRegion size calculation to accomodate changes in data members (JGray helped).;;;","10/Mar/10 23:33;stack;Committed branch and trunk.;;;","12/May/10 23:53;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Client port for ZK has no default,HBASE-2305,12458570,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,jdcryans,jdcryans,09/Mar/10 18:27,20/Nov/15 12:42,14/Jul/23 06:06,15/Mar/10 18:15,,,,,,,,,,,,0.90.0,,,,,,,0,,,"From Doug Meil on the list:

{code}
config.set(""hbase.zookeeper.property.clientPort"",""2181"");

This is consistent with other references to the importance of ""hbase.zookeeper.quorum"" except that we did this testing with the intent of connecting using only parameters set in code to eliminate any classpath issues with XML files.

FYI... If the last parameter isn't set you'll get the following error....       could this be defaulted to a reasonable value?


10/03/09 10:04:55 ERROR zookeeper.ZooKeeperWrapper: no clientPort found in zoo.cfg
Exception in thread ""main"" java.io.IOException: Could not read quorum servers from zoo.cfg
     at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.<init>(ZooKeeperWrapper.java:85)
     at org.apache.hadoop.hbase.client.HConnectionManager$ClientZKWatcher.getZooKeeperWrapper(HConnectionManager.java:223)
     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getZooKeeperWrapper(HConnectionManager.java:932)
     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRootRegion(HConnectionManager.java:948)
     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:625)
{code}

It should be easy to set it to 2181 along with the other port settings in HQuorumPeer.",,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/10 02:56;svarma;HBASE-2305-update1.patch;https://issues.apache.org/jira/secure/attachment/12438733/HBASE-2305-update1.patch","13/Mar/10 23:07;svarma;HBASE-2305.patch;https://issues.apache.org/jira/secure/attachment/12438721/HBASE-2305.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26251,Reviewed,,,,Fri Nov 20 12:42:10 UTC 2015,,,,,,,,,,"0|i0hh5z:",100043,,,,,,,,,,,,,,,,,,,,,"13/Mar/10 23:07;svarma;Assigns a default zookeeper client port in case it is not specified in a configuration.;;;","13/Mar/10 23:43;jdcryans;Some comments:

- ZK_CFG_PROPERTY + ""clientPort"", should be made static final
- This should really be 1 line:

{code}
+      int clientPortValue = HQuorumPeer.DEFAULT_ZOOKEPER_CLIENT_PORT;
+      zkProperties.put(clientPortKey, Integer.valueOf(clientPortValue))
{code}
- assignDefaultsForMissingProperties is a bit misleading, put all the other missing properties in there if there's any. If not, don't create a new method just for that.
- No need to call Integer.valueOf on ints.

Thx for looking at this Suraj!;;;","14/Mar/10 02:56;svarma;Thanks for your suggestions - I have incorporated them in this updated patch.;;;","15/Mar/10 18:15;stack;Committed branch and trunk.  Thanks for the patch Suraj Varma.;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row locks may deadlock with themselves,HBASE-2295,12458313,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,dhruba,tlipcon,tlipcon,05/Mar/10 23:31,20/Nov/15 12:43,14/Jul/23 06:06,09/Mar/10 17:11,,,,,,,,,,,,0.90.0,,regionserver,,,,,0,,,"Row locks in HRegion are keyed by a int-sized hash of the row key. It's perfectly possible for two rows to hash to the same key. So, if any client tries to lock both rows, it will deadlock with itself. Switching to a 64-bit hash is an improvement but still sketchy.",,davelatham,dhruba,hammer,kaykay.unique,larsfrancke,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-3894,,,,,,,,,,,,,"08/Mar/10 08:00;dhruba;rowLockDeadlock.txt;https://issues.apache.org/jira/secure/attachment/12438164/rowLockDeadlock.txt","09/Mar/10 08:21;dhruba;rowLockDeadlock2.txt;https://issues.apache.org/jira/secure/attachment/12438269/rowLockDeadlock2.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26247,Reviewed,,,,Fri Nov 20 12:43:37 UTC 2015,,,,,,,,,,"0|i0hh3z:",100034,,,,,,,,,,,,,,,,,,,,,"07/Mar/10 19:09;dhruba;are you talking about the multiPut where multiple rows could be updated atomically by an app? ;;;","07/Mar/10 22:02;ryanobjc;The multiput does not promise nor do multi row atomic puts. It actually just
calls the array put API.

On Mar 7, 2010 11:11 AM, ""dhruba borthakur (JIRA)"" <jira@apache.org> wrote:


   [
https://issues.apache.org/jira/browse/HBASE-2295?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12842465#action_12842465]

dhruba borthakur commented on HBASE-2295:
-----------------------------------------

are you talking about the multiPut where multiple rows could be updated
atomically by an app?

perfectly possible for two rows to hash to the same key. So, if any client
tries to lock both rows, it will deadlock with itself. Switching to a 64-bit
hash is an improvement but still sketchy.

--
This message is automatically generated by JIRA.
-
You can reply to this email to add a comment to the issue online.
;;;","08/Mar/10 08:00;dhruba;1. Changed locksToRows to be a Set (instead of a Map)
2. Generate sequential lockids, and in the case of collision use a Random number to start from a new origin
;;;","08/Mar/10 08:17;dhruba;The patch I uploaded is for hbase 0.20.

BTW, when I downloaded the hbase trunk, I do not see any build.xml files in the source repo at all. Do I need to use something other than ""ant test""?;;;","08/Mar/10 19:16;tlipcon;Hey Dhruba,

I don't think this patch works right. You can't have a HashSet<byte[]> since byte[]'s hashcode is identity based, not content, right?;;;","08/Mar/10 20:01;dhruba;I agree, that is the case for all native items like byte[]. I will change it.;;;","08/Mar/10 23:03;stack;@Dhruba 

If you want to use a SortedSet and pass a comparator instead, so you can retain byte [] elements, there is a bytes comparator here: http://hadoop.apache.org/hbase/docs/r0.20.3/api/org/apache/hadoop/hbase/util/Bytes.html#BYTES_COMPARATOR  Otherwise, patch looks good.

Regards TRUNK, its been mavenized so to build it, you need maven -- see http://wiki.apache.org/hadoop/Hbase/MavenPrimer -- and then to build and run tests do something like mvn install
;;;","09/Mar/10 08:21;dhruba;This patch incorporates Todd's and Stack's comments. And is for hbase trunk.;;;","09/Mar/10 17:11;stack;Committed to trunk and branch.  Thanks for patch Dhruba.;;;","09/Mar/10 18:25;tlipcon;This already went in, but one quick note on the final patch: rather than checking locksToRows.size() > 0, it's probably faster to check !locksToRows.isEmpty()

Does this need to be a followup JIRA?;;;","09/Mar/10 20:23;stack;I agree isEmpty is better than size... I'll just put fix in under this issue.  Thanks Todd. 

I changed it to do this instead on trunk and branch:

{code}
      while (!this.lockedRows.isEmpty()) {
        if (LOG.isDebugEnabled()) {
          LOG.debug(""Waiting on "" + this.lockedRows.size() + "" row locks"");
        }
...
{code}
;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CME in RegionManager#isMetaServer,HBASE-2293,12458307,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,stack,stack,05/Mar/10 22:43,12/Oct/12 06:14,14/Jul/23 06:06,06/Mar/10 07:03,,,,,,,,,,,,0.20.4,0.90.0,,,,,,0,,,"{code}
2010-03-05 14:26:17,990 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 84 on 60000, call regionServerReport(address: 98.137.30.50:60020, startcode: 1267824847596, load: (requests=0, regions=163, usedHeap=3113, maxHeap=5983), [Lorg.apache.hadoop.hbase.HMsg;@5446315a, [Lorg.apache.hadoop.hbase.HRegionInfo;@4c5236ef) from 98.137.30.50:48101: error: java.io.IOException: java.util.ConcurrentModificationException
java.io.IOException: java.util.ConcurrentModificationException
       at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)
       at java.util.TreeMap$ValueIterator.next(TreeMap.java:1145)
       at org.apache.hadoop.hbase.master.RegionManager.isMetaServer(RegionManager.java:832)
       at org.apache.hadoop.hbase.master.RegionManager.regionsAwaitingAssignment(RegionManager.java:391)
       at org.apache.hadoop.hbase.master.RegionManager.assignRegions(RegionManager.java:196)
       at org.apache.hadoop.hbase.master.ServerManager.processMsgs(ServerManager.java:488)
       at org.apache.hadoop.hbase.master.ServerManager.processRegionServerAllsWell(ServerManager.java:414)
       at org.apache.hadoop.hbase.master.ServerManager.regionServerReport(ServerManager.java:323)
       at org.apache.hadoop.hbase.master.HMaster.regionServerReport(HMaster.java:724)
       at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
       at java.lang.reflect.Method.invoke(Method.java:597)
       at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
       at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/10 06:28;jdcryans;HBASE-2293-2.patch;https://issues.apache.org/jira/secure/attachment/12438084/HBASE-2293-2.patch","05/Mar/10 23:16;jdcryans;HBASE-2293.patch;https://issues.apache.org/jira/secure/attachment/12438062/HBASE-2293.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26246,Reviewed,,,,Sat Mar 06 07:03:23 UTC 2010,,,,,,,,,,"0|i08sv3:",49276,,,,,,,,,,,,,,,,,,,,,"05/Mar/10 22:55;jdcryans;Looks like we see this because all the RS report a lot more often.;;;","05/Mar/10 23:16;jdcryans;regionsiInTransition was missing some synchronized blocks, this patch for 0.20 adds them.;;;","06/Mar/10 06:21;stack;Sync blocks span more than they need to.  Suggest narrowing synchronization in new patch.;;;","06/Mar/10 06:28;jdcryans;Narrowed sync blocks.;;;","06/Mar/10 06:48;stack;+1;;;","06/Mar/10 07:03;jdcryans;Passes all tests, committed to branch and trunk. Thx for the review Stack!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Create Issue Ignore,HBASE-2291,12458300,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,05/Mar/10 21:56,11/Jun/22 23:08,14/Jul/23 06:06,05/Mar/10 22:11,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26245,,,,,Sat Mar 06 04:58:27 UTC 2010,,,,,,,,,,"0|i0hh3j:",100032,,,,,,,,,,,,,,,,,,,,,"05/Mar/10 22:03;ghelmling;Ignore this comment;;;","05/Mar/10 22:11;stack;ignore comment;;;","05/Mar/10 22:11;stack;resolving;;;","05/Mar/10 22:11;stack;reopening...;;;","05/Mar/10 22:11;stack;close;;;","06/Mar/10 04:42;hbasebuild;Does this comment go into the JIRA?

;;;","06/Mar/10 04:58;hbasebuild;Does this comment make it in?

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shell fails on alter,HBASE-2288,12458196,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,05/Mar/10 00:41,12/Oct/12 06:15,14/Jul/23 06:06,05/Mar/10 00:48,,,,,,,,,,,,0.20.4,,,,,,,0,,,"HBASE-2035 added this line to branch which doesn't make sense since it's related to a Trunk feature:

{code}
        if args[DEFERRED_LOG_FLUSH]
          htd.setDeferredLogFlush(JBoolean.valueOf(args[DEFERRED_LOG_FLUSH]))
        end
{code}

We just need to remove it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26242,,,,,Fri Mar 05 01:34:43 UTC 2010,,,,,,,,,,"0|i08sr3:",49258,,,,,,,,,,,,,,,,,,,,,"05/Mar/10 00:46;jdcryans;I committed this to branch:

{code}
Index: bin/HBase.rb
===================================================================
--- bin/HBase.rb	(revision 919248)
+++ bin/HBase.rb	(working copy)
@@ -231,9 +231,6 @@
         if args[MEMSTORE_FLUSHSIZE]
           htd.setMemStoreFlushSize(JLong.valueOf(args[MEMSTORE_FLUSHSIZE]))
         end
-        if args[DEFERRED_LOG_FLUSH]
-          htd.setDeferredLogFlush(JBoolean.valueOf(args[DEFERRED_LOG_FLUSH]))
-        end
         @admin.modifyTable(tableName.to_java_bytes, htd)
       else
         descriptor = hcd(args)
{code};;;","05/Mar/10 01:28;stack;Do I need to add this back after backport of group commit? HBASE-1939?;;;","05/Mar/10 01:34;jdcryans;Nope, not until HBASE-1944 is backported.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TypeError in shell,HBASE-2287,12458178,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kovyrin,jdcryans,jdcryans,04/Mar/10 21:42,20/Nov/15 12:42,14/Jul/23 06:06,11/Mar/10 23:46,,,,,,,,,,,,0.90.0,,,,,,,0,,,"In trunk I currently get:

{code}
hbase(main):003:0> get 't', 'r', 'f:allo'
TypeError: can't convert Class into String
	from /Users/jdcryans/Work/HBase/Trunk/bin/../bin/HBase.rb:526:in `get'
	from /Users/jdcryans/Work/HBase/Trunk/bin/../bin/hirb.rb:404:in `get'
	from (hbase):4
{code}

That part of the code didn't change but the JRuby version was switched from 1.2 to 1.4",,kovyrin,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2279,,,,,,,,,,,,,,,,,,,"07/Mar/10 17:42;kovyrin;HBASE-2287.patch;https://issues.apache.org/jira/secure/attachment/12438145/HBASE-2287.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26241,Reviewed,,,,Fri Nov 20 12:42:20 UTC 2015,,,,,,,,,,"0|i0hh2v:",100029,"Trivial patch attached, please review.",,,,,,,,,,,,,,,,,,,,"04/Mar/10 21:50;jdcryans;Assigning to Alexey by his request.;;;","07/Mar/10 17:42;kovyrin;Trivial patch. In HBASE-2279 I'll change the parsing code to make sure we support this kind of parameters (strings and arrays of strings instead of hashes only).;;;","11/Mar/10 23:46;stack;Thanks for the patch Alexey.  Applied to branch and trunk.;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fsWriteLatency metric may be incorrectly reported ,HBASE-2284,12458061,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,kannanm,kannanm,kannanm,04/Mar/10 01:06,10/Jun/19 02:26,14/Jul/23 06:06,04/Mar/10 05:21,,,,,,,,,,,,0.20.4,0.90.0,,,,,,0,,,"fsWriteLatency metric is computed by maintaining writeTime & writeOps in HLog. If an HLog.append() carries multiple edits, then ""writeTime"" is computed incorrectly for the subsequent edits because doWrite() is called for each of the edits with the same start time argument (""now"").

This also causes a lot of false WARN spews to the log. Only one of the edits might have taken a long time, but every edit after that in a given HLog.append() operation will also raise these warning messages.

{code}
2010-03-03 11:00:42,247 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 51 on 60020 took 1814ms appending an edit to hlog; editcount=302227
2010-03-03 11:00:42,247 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 51 on 60020 took 1814ms appending an edit to hlog; editcount=302228
2010-03-03 11:00:42,247 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 51 on 60020 took 1814ms appending an edit to hlog; editcount=302229
2010-03-03 11:00:42,247 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 51 on 60020 took 1814ms appending an edit to hlog; editcount=302230
2010-03-03 11:00:42,247 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 51 on 60020 took 1814ms appending an edit to hlog; editcount=302231
{code}

Will submit a patch shortly. ",,hudson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/10 02:08;kannanm;2284_0.20.patch;https://issues.apache.org/jira/secure/attachment/12437828/2284_0.20.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26239,Reviewed,,,,Mon Jun 10 02:26:14 UTC 2019,,,,,,,,,,"0|i08syf:",49291,,,,,,,,,,,,,,,,,,,,,"04/Mar/10 05:21;stack;Applied to branch and a version to trunk.  Thanks for patch Kannan.;;;","04/Mar/10 05:23;stack;Made Kannan a contributor.  Assigning him this issue.;;;","04/Mar/10 19:13;jdcryans;I just committed a fix for mdc_replication since it wasn't compiling anymore.;;;","10/Jun/19 02:26;hudson;Results for branch master
	[build #1113 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/master/1113/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/master/1113//General_Nightly_Build_Report/]




(x) {color:red}-1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1113//JDK8_Nightly_Build_Report_(Hadoop2)/]


(x) {color:red}-1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1113//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
row level atomicity ,HBASE-2283,12458055,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,kannanm,kannanm,kannanm,03/Mar/10 23:55,20/Nov/15 12:43,14/Jul/23 06:06,20/Mar/10 00:54,,,,,,,,,,,,0.90.0,,,,,,,0,,,"The flow during a HRegionServer.put() seems to be the following. [For now, let's just consider single row Put containing edits to multiple column families/columns.

HRegionServer.put() does a:

        HRegion.put();
       syncWal()  (the HDFS sync call).  /* this is assuming we have HDFS-200 */

HRegion.put() does a:
  for each column family 
  {
      HLog.append(all edits to the colum family);

      write all edits to Memstore;
  }

HLog.append() does a :
  foreach edit in a single column family {
    doWrite()
  }

doWrite() does a:
   this.writer.append().

There seems to be two related issues here that could result in inconsistencies.

Issue #1: A put() does a bunch of HLog.append() calls. These in turn do a bunch of ""write"" calls on the underlying DFS stream.  If we crash after having written out some append's to DFS, recovery will run and apply a partial transaction to memstore.  

Issue #2: The updates to memstore  should happen after the sync rather than before. Otherwise, there is the danger that the write to DFS (sync) fails for some reason & we return an error to the client, but we have already taken edits to the memstore. So subsequent reads will serve uncommitted data.
",,dhruba,hammer,kannanm,larsfrancke,streamy,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2294,,,,,,,,,,,,,"12/Mar/10 07:19;kannanm;rowLevelAtomicity_2283_v1.patch;https://issues.apache.org/jira/secure/attachment/12438586/rowLevelAtomicity_2283_v1.patch","17/Mar/10 18:41;kannanm;rowLevelAtomicity_2283_v2.patch;https://issues.apache.org/jira/secure/attachment/12439058/rowLevelAtomicity_2283_v2.patch","18/Mar/10 00:48;kannanm;rowLevelAtomicity_2283_v3.patch;https://issues.apache.org/jira/secure/attachment/12439104/rowLevelAtomicity_2283_v3.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26238,Reviewed,,,,Fri Nov 20 12:43:23 UTC 2015,,,,,,,,,,"0|i0hh1z:",100025,,,,,,,,,,,,,,,,,,,,,"03/Mar/10 23:58;kannanm;Followup discussions on the hbase-dev:

JD wrote: <<< Indeed. The syncWal was taken back up in HRS as a way to optimize batch Puts but the fact it's called after all the MemStore operations is indeed a problem. I think we need to fix both (#1) and (#2) by ensuring we do only a single append for whatever we have to put and then syncWAL once before processing the MemStore. But, the other problem here is that the row locks have to be taken out on all rows before everything else in the case of a Put[] else we aren't atomic. And then I think some checks are ran under HRegion that we would need to run before everything else.>>>

Ryan wrote: <<< Do we really need a single actual DFS atomic write operation?  If we had some kind of end-of-row marker, would that help instead?>>>

Yes, a marker or length-prefixed approach would suffice to recognize and ignore incomplete transactions during recovery.

Ryan wrote: <<< But as you said, what happens if hlog append fails?  The obvious thing would be to remove the additions from the memstore.  But how to accomplish this easily?>>>

Wouldn't moving all the memstore updates to happen after the sync suffice?

;;;","04/Mar/10 04:57;stack;So, we should pull WAL.append out of Store and instead do the WAL appending up in the HRegionServer, add all edits for a row to WAL before proceeding per column family to add edits per MemStore.;;;","04/Mar/10 05:28;stack;Moving into 0.20.4.;;;","04/Mar/10 14:40;dhruba; >Issue #2: The updates to memstore should happen after the sync rather than before.

A related question is : what should the region server do if a write/sync to the Hlog fails? A simple option would be to shut itself down. Another option would be to roll the logs?;;;","04/Mar/10 14:48;kannanm;If I recall correctly, on write failure, the log is indeed already rolled, and an exception is thrown to the client (for the failure of the current transaction). I would have to check what we do if sync fails. But in either case rolling the logs seems like a good option. Shutting down the server might be a more heavy handed option. 

Was the thought that if we went with ""shutting down the server"" option, we could punt on Issue #2? My guess is the refactoring required for Issue #1 will make it easy to fix #2 also as part of the changes.


;;;","04/Mar/10 14:52;kannanm;Regarding Issue #1: It seems like all edits for a row (including any start/end txn markers) should in fact happen in *one* call to WAL append; otherwise, you can have interleaved edits in the log coming from multiple transactions. Agreed?;;;","04/Mar/10 15:11;dhruba;If we say that we restart the region server when a write/sync to the region log fails, then we can defer a fix for #2. If we do that, then we do not need any refactoring of the code at all. We can solve #1 via putting a sync marker at the end of each transaction and making the HLog.reader handle this marker correctly (details not yet worked out).;;;","04/Mar/10 15:15;dhruba;Oh, actually we have to do the refactoring, other we could satisfy a read request from an uncommitted transaction. So, my above comment is not applicable.;;;","04/Mar/10 18:07;jdcryans;bq. Oh, actually we have to do the refactoring, other we could satisfy a read request from an uncommitted transaction.

WRT serving stuff that's not committed, I'd say it's more in the scope of HBASE-2248 since we don't serve from the WAL. In the case of a region server failure, then like you said if we don't meet the marker with HLog.Reader then those edits won't be replayed?;;;","09/Mar/10 18:52;kannanm;
Have coded the changes to make all the ""appends"" happen first, then ""sync"" and then all the memstore edits are done. 

Now, trying to work on collapsing all the appends into a single append operation. The marker can simply be the number of edits we expect to find in the append operation. 

Today, if a transaction contains 3 edits to c1, c2, c3 for row R. The HLog contains, has three log entries:

<logseq1-for-edit1>:<KeyValue-for-edit-c1>
<logseq2-for-edit2>:<KeyValue-for-edit-c2>
<logseq3-for-edit3>:<KeyValue-for-edit-c3>

In the new model, the plan is to have something like:

<logseq#-for-entire-txn>:<3, <Keyvalue-for-edit-c1>, <KeyValue-for-edit-c2>, <KeyValue-for-edit-c3>>

Implementing the change itself should be reasonably straightforward. 

But one quick question: In the new version, I presumably have to  worry about supporting both the old & the new log format, correct? (i.e. for cases when someone upgraded from an older version of HBase to this version)



;;;","09/Mar/10 20:05;stack;.bq In the new version, I presumably have to worry about supporting both the old & the new log format, correct? (i.e. for cases when someone upgraded from an older version of HBase to this version)

Well, a clean shutdown should have no outstanding wal logs since as part of the shutdown, we flush memstores.  Would it be hard reading both types?  I think that'd be the best thing to do if its not too hard.  Otherwise, you could fail the startup if you find old-style files (make it easy on yourself and give the newstyle different name format) and have admin run a small script to convert old-style to new?;;;","10/Mar/10 05:04;kannanm;Thanks for your input.

Have the code changes to support things upward compatibly.  The serialized format of a KeyValue starts with a ""int"" length. Overloading the length now for versioning. If the length is the special value -1, then will interpret the rest of the data in new format. Else, interpret the data in old format. 

A HLog entry could now either be:

<HLogKey>:<KeyValue>
or,
<HLogKey>:<-1 <# of edits, <KeyValue1>, <KeyValue2>, ...>>

I think we have the overall fix pretty much code complete. Will put up the patch after some basic testing (hopefully by tomorrow). And then will continue more detailed testing in parallel.



;;;","12/Mar/10 07:19;kannanm;Attaching patch for intial review.

* The order is now, append/sync/memstore for all edits (such as Put, checkAndPut, Delete, etc. operations).

* Also edits within a single Put (row), are appened as a single edit (KeyValueList) to the WAL.  This will ensure recovery doesn't apply partial tranactions.

* I still need to make one small change to recovery -- if we get an error while reading the WAL due to a partial txn at the end of the WAL-- (i.e. a incomplete HLogKey/KeyValueList) then ignore the error/partial txn. [This should happen, if at all, only for the last transaction in a WAL.]

* I manually tested killing region server, and ensured that edits were recovered correctly. 

* I still need to run other unit tests, and need to add unit tests for these cases.

(Thanks to Aravind/Karthik/Dhruba for discussions/suggestions on the issue & fix.)

;;;","12/Mar/10 20:50;stack;Here's some comments.

I think KeyValueList not the best name for a List that is purposed to appending to a WAL log.  KVL implies List<KeyValue> which it replaces in your code but in fact it does much more.  For one its a Writable.  It also takes care of your fancy new transaction start/end markings.  Rename it WALEdit or HLogEdit or something?  Should it also be moved to the wal package? (o.a.h.h.regionserver.wal).

Minor: Should it implement List<KeyValue>?  Would that help?  You wouldn't need to have a getList?

I love the class comment on KVL.

readNonLengthData should instead be an overloading of readFields, just add the length param?

Minor: The following if (kvlist.size() > 0) { is usually cheaper if done as !kvlist.isEmpty IIRC, especially if the list one of the concurrent implementations where size calc is expensive (probably not pertinent here).

Its interesting, our number-of-entries in hlog will change now, now you group up puts, etc.

Patch looks great Kannan. 



;;;","12/Mar/10 21:39;kannanm;Stack: thanks for the comments. I will take care of them and resubmit a patch along with a few other test changes I had to make.;;;","15/Mar/10 19:14;kannanm;Currently, with my patch, TestGetClosestAtOrBefore:testUsingMetaAndBinary() (in regionserver) is broken.

Debugged this a bit, and it seems that my change has somehow broken the interaction between the scanner & delete. What's the expected semantics when a delete happens in the middle of a scan, as the test does here:

{code}
    byte [] firstRowInC = HRegionInfo.createRegionName(Bytes.toBytes("""" + 'C'),
      HConstants.EMPTY_BYTE_ARRAY, HConstants.ZEROES);
    Scan scan = new Scan(firstRowInC);
    s = mr.getScanner(scan);
    try {
      List<KeyValue> keys = new ArrayList<KeyValue>();
      while (s.next(keys)) {
        mr.delete(new Delete(keys.get(0).getRow()), null, false);
        keys.clear();
      }
    } finally {
      s.close();
    }
{code}

Is the scanner expected to have snapshot semantics (i.e. not be affected by deletes that are happening)? With my patch, the scanner seems to be affected by deletes (need to debug why) -- but I was curious to hear if the old behavior is the expected one?


;;;","15/Mar/10 19:29;stack;What are you seeing? Yes, the delete above should have an effect such that the subsequent gets return nothing and the ongoing Scan should progress unaffected.;;;","15/Mar/10 19:56;tlipcon;Can we discuss the semantics questions in HBASE-2294? I really strongly believe we need to nail down these questions before we push forward with code that interacts so closely with them.;;;","15/Mar/10 21:34;kannanm;scatch my comment (@15/Mar/10 07:14 PM). I was misreading my debug output. The ongoing scan is indeed unaffected by the delete.  There is some other issue with this test + my patch that I am still trying to nail down.;;;","15/Mar/10 23:17;kannanm;Found the issue with TestGetClosestAtOrBefore:testUsingMetaAndBinary() failures. It was indeed due to my patch :(.

During a put(), updateKeys() is called to convert the special ""LATEST"" timestamp to ""now"". My patch had the inadvertent side effect of calling updateKeys() inside of the if (writeToWAL) check instead of doing so always. This particular test passes ""false"" for writeToWAL, and so the puts ended up having LATEST timestamps, as their timestamps never got adjusted. This causes subsequent deletes with ""now"" timestamps to become no-ops. 

 ;;;","15/Mar/10 23:20;kannanm;Stack: re: KeyValueList you wrote: <<Should it also be moved to the wal package? (o.a.h.h.regionserver.wal).>>>

I think the *.wal package is only in 0.21. Should I move it to o.a.h.h.regionserver instead for 0.20?

;;;","15/Mar/10 23:34;stack;Oh, yeah, wal is in TRUNK only... so yes, I'd suggest putting in o.a.h.h.regionserver for 0.20.;;;","16/Mar/10 02:01;kannanm;Stack:

btw, I think there might be preexisting issue with timestamps & WAL stuff for deletes that come in with LATEST timestamp.  Could you check the code and confirm?

Basically, in:

{code}
 delete(byte [] family, List<KeyValue> kvs, boolean writeToWAL)
{code}

the  ""kv.updateLatestStamp(byteNow);"" time stamp massaging happens *after* the WAL log.append() call. So the keyvalues written to the HLog does not have the massaged timestamp. On recovery, when these entries are replayed, we add them back to reconstructionCache but don't do anything with timestamps. 

;;;","16/Mar/10 17:57;stack;@Kannan I confirm you found an ugly bug.  Will I open an issue or you want to fix as part of this one.  Good on you.;;;","16/Mar/10 22:17;kannanm;Might be simpler to take care of this as part of my fix. Let me do that.
;;;","17/Mar/10 18:41;kannanm;V2 of the patch.  Addresses Stack's earlier comments. Also, fixes unit tests which were using some of the lower level APIs to use the modified APIs.

Unit tests pass now. Manually tested crashing RS and triggering recovery for single puts & multiputs. Also tried manual tests for delete (but that revealed some pre-existing issues with delete). The ""delete"" issue is being tracked in a separate JIRA (HBASE-2337).

Will create a separate JIRA for adding automated tests for these. That'll be my next task. Might need some pointers from Stack or others on this.
;;;","18/Mar/10 00:48;kannanm;A small one line improvement to v2 patch based on internal feedback. The change makes it so that if someone reuses an instance of WALEdit across multiple calls to the sequence file reader, the readFields() method cleans out any  old state of the object cleanly.;;;","19/Mar/10 00:22;stack;Patch looks good.  Applied to 0.20 branch.  Had to mess in transactional hbase to make it use new WALEdit rather than KeyValue as HLog value.   Working on forward port to TRUNK.;;;","20/Mar/10 00:54;stack;Committed to TRUNK after some masssage including disable of replication contrib building at J-D's request... he wants to update replication to match this patch himself.

Thanks for the patch Kannan.;;;","22/Mar/10 03:11;jdcryans;Looks like the patch for trunk disables deferred log flush, in HLog.append:

{code}
// sync txn to file system
this.sync(info.isMetaRegion());
{code}

Doesn't check info.getTableDesc().isDeferredLogFlush();;;","23/Mar/10 07:49;stack;Let me reopen so we get deferred log flush back in again.  J-D, what should be happening in append?  We check deferred flush flag and if set, do not call sync?  Thanks.;;;","23/Mar/10 16:38;jdcryans;Yeah check if info.isMetaRegion() || ! info.getTableDesc().isDeferredLogFlush()  
You want to force the sync if it's a catalog region or if deferred log flush is off. Actually I guess you could even get rid of the first check, unless we want to guard ourselves from a user setting .META. as deferred log flushed?;;;","23/Mar/10 17:11;kannanm;Is the ""deferred log flush"" trunk specific? What is its use case/intended semantics? [Are you ok with the memstore edits being done before a sync for this use case?]


;;;","23/Mar/10 17:20;jdcryans;This is from HBASE-1944 (see our use case there) and it is currently trunk-specific since it's a new feature that came along at the same time as group commit. It relies on the awaitNanos timer in HLog.LogSyncer.run to hflush entries that were appended but not flushed to the DNs. This is turned on by default in trunk (edits are less durable) after a vote came along in November and, if I remember correctly, Stack wasn't ok with the idea of much slower inserts out of the box compared to the 0.20 branch.;;;","12/May/10 23:53;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hbase Shell hcd() method is broken by the replication scope parameter,HBASE-2276,12457663,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kovyrin,kovyrin,kovyrin,28/Feb/10 07:54,20/Nov/15 12:42,14/Jul/23 06:06,28/Feb/10 10:53,0.90.0,,,,,,,,,,,0.90.0,,scripts,,,,,0,hbase,shell,"Since an additional HColumnDescriptor constructor parameter (scope) was introduced, hbase shell hcd() method fails to create a HColumnDescriptor object:

hbase(main):007:0> alter 'doc_total_stats', {NAME => 'country_views', VERSIONS => 1}
ArgumentError: wrong # of arguments(8 for 9)
","Hbase trunk, hadoop 0.21 branch",larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/10 07:57;kovyrin;HBASE-2276.patch;https://issues.apache.org/jira/secure/attachment/12437374/HBASE-2276.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26235,,,,,Fri Nov 20 12:42:44 UTC 2015,,,,,,,,,,"0|i0hh0n:",100019,Attached HBASE-2276.patch file contains a patch that adds REPLICATION_SCOPE support to hcd() method and to the shell itself by defining the REPLICATION_SCOPE constant,,,,,,,,,,,patch,,,,,,,,,"28/Feb/10 10:53;larsgeorge;Made Alexey a contributor so I could assign his name to the issue. Committed since this is a trivial fix.;;;","28/Feb/10 18:49;jdcryans;Thanks for figuring it and making a patch Alexey, I forgot to check the shell. ;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"PerformanceEvaluation ""--nomapred"" may assign duplicate random seed over multiple testing threads",HBASE-2269,12457468,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,,tatsuya6502,tatsuya6502,25/Feb/10 18:21,20/Nov/15 12:42,14/Jul/23 06:06,11/Mar/10 23:09,0.20.3,,,,,,,,,,,0.90.0,,test,,,,,0,,,"When you use PerformanceEvaluation with ""--nomapred"" option, you will end up having the same random seeds assigned over multiple testing threads. So you'll get inaccurate results from ""random~~"" tests.

{code:title=PerformanceEvaluation.java}
542:  abstract class Test {
543:     protected final Random rand = new Random(System.currentTimeMillis());
{code}

Milliseconds won't be sufficient; today's JVM is much faster to create multiple Test objects in one millisecond. You might want to use something like ""{{super.hashCode()}}"" instead. 
",Any operating system,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26232,Reviewed,,,,Fri Nov 20 12:42:43 UTC 2015,,,,,,,,,,"0|i0hgzr:",100015,,,,,,,,,,,,,,,,,,,,,"05/Mar/10 17:24;stack;@Tatsuya For sure super.hashCode() gives a different answer per instance of Test?  Thanks.;;;","06/Mar/10 23:20;tatsuya6502;@Stack At least Sun JDK, yes, we can expect {{super.hashCode() (java.lang.Object#hashCode() )}} to return different integers per instance of Test.

Quote from JDK 6 Javadoc: 
http://java.sun.com/javase/6/docs/api/java/lang/Object.html#hashCode()

{quote}
As much as is reasonably practical, the hashCode method defined by class Object does return distinct integers for distinct objects. (This is typically implemented by converting the internal address of the object into an integer, but this implementation technique is not required by the Java programming language.)
{quote}

There is no chance of different objects on the same Java VM getting the same internal address. 
There will be a very little chance of different objects on different Java VMs getting the same internal address. 


Or, if you'd like to avoid this implementation specific behavior, we could do something like: 

{code:title=In Test class}
private static final Random randomSeed = new Random(System.currentTimeMillis());
private static long nextRandomSeed() { return randomSeed.nextLong(); }
protected final Random rand = new Random(nextRandomSeed());
{code}

We can expect different map tasks to get different currentTimeMillis() and different threads to get different nextLong() values. 


Also, {{java.security.SecureRandom}} has {{getSeed(int numBytes)}} method, and Linux version will return perfectly random values based on environmental noise collected by OS. However, I don't recommend this because it uses Linux {{/dev/random}} as the source and gets blocked when {{/dev/random}} runs out the data in its entropy pool. ( http://en.wikipedia.org/wiki//dev/random )   We don't need this level of strength anyway. 

Thanks, 
;;;","11/Mar/10 23:09;stack;I committed your latter suggestion Tatsuya:

{code}
Index: src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java
===================================================================
--- src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java (revision 921459)
+++ src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java (working copy)
@@ -666,7 +666,15 @@
    * Subclass to particularize what happens per row.
    */
   static abstract class Test {
-    protected final Random rand = new Random(System.currentTimeMillis());
+    // Below is make it so when Tests are all running in the one
+    // jvm, that they each have a differently seeded Random. 
+    private static final Random randomSeed =
+      new Random(System.currentTimeMillis());
+    private static long nextRandomSeed() {
+      return randomSeed.nextLong();
+    }
+    protected final Random rand = new Random(nextRandomSeed());
+
     protected final int startRow;
     protected final int perClientRunRows;
     protected final int totalRows;
{code}

Thanks for the fix.;;;","12/May/10 23:53;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[stargate] ""Failed tests: warning(junit.framework.TestSuite$1)"" and DEBUG output is dumped to console since move to Mavenized build",HBASE-2268,12457454,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,apurtell,apurtell,apurtell,25/Feb/10 16:28,20/Nov/15 12:42,14/Jul/23 06:06,25/Feb/10 22:54,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Two problems with running unit tests since Stargate was moved to Maven, possibly related:

1) DEBUG output from tests is dumped to console.

2) ""Failed tests: warning(junit.framework.TestSuite$1)"" -- what?

{noformat}
Results :

Failed tests: 
  warning(junit.framework.TestSuite$1)

Tests run: 76, Failures: 1, Errors: 0, Skipped: 0
{noformat}


Someone who knows Maven, can you take a look at this?",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26231,,,,,Fri Nov 20 12:42:26 UTC 2015,,,,,,,,,,"0|i0hgzj:",100014,,,,,,,,,,,,,,,,,,,,,"25/Feb/10 20:34;apurtell;Up on irc tallpsmith suggests:

{code}
index 26cee50..7822272 100644
--- a/core/pom.xml
+++ b/core/pom.xml
-76,8 +76,6 @@
        <artifactId>maven-surefire-plugin</artifactId>
        <configuration>
           <forkMode>always</forkMode>
-          <argLine>-Xmx512m</argLine>
-          <redirectTestOutputToFile>true</redirectTestOutputToFile>
           <excludes>
             <exclude>**/SoftValueSortedMapTest.java</exclude>
             <exclude>**/*$*</exclude>
diff --git a/pom.xml b/pom.xml
index f5d4138..da82cc2 100644
--- a/pom.xml
+++ b/pom.xml
 -259,6 +259,10 @@
         <plugin>
           <artifactId>maven-surefire-plugin</artifactId>
           <version>2.5</version>
+            <configuration>
+                <argLine>-Xmx512m</argLine>
+                <redirectTestOutputToFile>true</redirectTestOutputToFile>                
+            </configuration>
         </plugin>
         <plugin>
           <artifactId>maven-javadoc-plugin</artifactId>
{code}

And to rename MiniClusterTestCase.java to something other with TestCase in the name. 
;;;","25/Feb/10 22:22;apurtell;{{[INFO] BUILD SUCCESSFUL}}

Will commit as soon as svn is back.
;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[stargate] missing MiniDFSCluster dependency,HBASE-2266,12457386,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,apurtell,apurtell,apurtell,25/Feb/10 07:34,20/Nov/15 12:43,14/Jul/23 06:06,25/Feb/10 07:59,,,,,,,,,,,,0.90.0,,,,,,,0,,,"This is the problem: java.lang.NoClassDefFoundError: org/apache/hadoop/net/StaticMapping
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:287)

Some dependency is missing from the sub project.

% mvn -DskipTests clean install
...
[INFO] BUILD SUCCESSFUL

% mvn -Dtest=Test00MiniCluster test
...
Tests run: 4, Failures: 0, Errors: 4, Skipped: 0

From the test log file:

testDFSMiniCluster(org.apache.hadoop.hbase.stargate.Test00MiniCluster)  Time elapsed: 1.263 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org/apache/hadoop/net/StaticMapping
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:287)
	at org.apache.hadoop.hbase.stargate.MiniClusterTestCase.startDFS(MiniClusterTestCase.java:81)
	at org.apache.hadoop.hbase.stargate.MiniClusterTestCase.startMiniCluster(MiniClusterTestCase.java:187)
	at org.apache.hadoop.hbase.stargate.MiniClusterTestCase.setUp(MiniClusterTestCase.java:225)
	at junit.framework.TestCase.runBare(TestCase.java:132)
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/10 07:44;apurtell;HBASE-2266.patch;https://issues.apache.org/jira/secure/attachment/12436963/HBASE-2266.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26230,,,,,Fri Nov 20 12:43:10 UTC 2015,,,,,,,,,,"0|i0hgz3:",100012,,,,,,,,,,,,,,,,,,,,,"25/Feb/10 07:59;apurtell;Attached patch to pom.xml fixes test failures. Committed.;;;","20/Nov/15 12:43;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StackOverflow in ExplicitColumnTracker when row has many columns,HBASE-2259,12457271,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,dlrozendaal,dlrozendaal,24/Feb/10 09:07,12/Oct/12 06:15,14/Jul/23 06:06,25/Feb/10 00:06,0.20.3,,,,,,,,,,,0.20.4,0.90.0,,,,,,0,,,"When doing a ""get"" on a row with many columns and where the ""get"" also contains many columns to get a stack overflow is thrown in the ExplicitColumnTracker:

java.io.IOException: java.io.IOException: java.lang.StackOverflowError
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:872)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:862)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1733)
        at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
Caused by: java.lang.StackOverflowError
        at org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.checkColumn(ExplicitColumnTracker.java:122)
        at org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.checkColumn(ExplicitColumnTracker.java:123)
        at org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.checkColumn(ExplicitColumnTracker.java:123)
[... repeats many times ...]
        at org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.checkColumn(ExplicitColumnTracker.java:123)
        at org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.checkColumn(ExplicitColumnTracker.java:123)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getRegionServerWithRetries(HConnectionManager.java:1048)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:417)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/10 09:42;ryanobjc;HBASE-2259-tail-recursive.patch;https://issues.apache.org/jira/secure/attachment/12436983/HBASE-2259-tail-recursive.patch","24/Feb/10 23:30;jdcryans;HBASE-2259.patch;https://issues.apache.org/jira/secure/attachment/12436925/HBASE-2259.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26229,,,,,Thu Feb 25 19:26:49 UTC 2010,,,,,,,,,,"0|i08smf:",49237,,,,,,,,,,,,,,,,,,,,,"24/Feb/10 16:34;jdcryans;A couple of recursive calls were added in 0.20.0 and they all seem to cause that kind of problem. I'll fix this one as I already fixed two others previously.;;;","24/Feb/10 23:30;jdcryans;Patch that adds a test to verify the stack overflow (fails on 0.20.3) along with the fix in ExplicitColumnTracker that works the same way as the other fixes I did previously.;;;","25/Feb/10 00:06;jdcryans;Committed to branch and trunk.;;;","25/Feb/10 09:24;ryanobjc;the variable 'recursive' should really be named something more accurate.  the continue at the end of the loop should get optimized out, but it looks like bad form.  also commented out code should not be in the patch, just remove it. 

since this call is also tail recursive, this should be solvable with a while(true) and return statements only, no boolean necessary. That would be a more straightforward port, so lets do that instead.;;;","25/Feb/10 09:42;ryanobjc;This is the kind of thing I'm talking about.  It should be recognizable to people as a tail recursive call, and the JIT should be optimize this better.;;;","25/Feb/10 19:26;jdcryans;I opened HBASE-2270 to take care of your comment for both WildcardColumnTracker and ExplicitColumnTracker at the same time.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The WhileMatchFilter doesn't delegate the call to filterRow(),HBASE-2258,12457263,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,dan.washusen,dan.washusen,24/Feb/10 07:54,12/Oct/12 06:14,14/Jul/23 06:06,24/Feb/10 17:11,0.20.3,,,,,,,,,,,0.20.4,0.90.0,Filters,,,,,0,,,"While testing the recent MemStoreScanner slowness I noticed that each scan in the randomSeekScan test takes about 19 seconds to complete.  The scan in question provides a startRow and a WhileMatchFilter containing a PageFilter that asks for 120 rows.  I would have expected this scan to return in roughly the same amount of time as a scan that specifies a startRow and stopRow that spans a similar number of rows.

As it turns out this is an issue with the WhileMatchFilter.  The WhileMatchFilter is not delegating the call the filterRow() down the the PageFilter.  As a result the PageFilter never increments the rowsAccepted counter.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/10 17:02;stack;HBASE-2258.patch;https://issues.apache.org/jira/secure/attachment/12436861/HBASE-2258.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26228,Reviewed,,,,Wed Feb 24 17:11:55 UTC 2010,,,,,,,,,,"0|i08swn:",49283,,,,,,,,,,,,,,,,,,,,,"24/Feb/10 10:19;dan.washusen;Added bonus of fixing this issue is that the PE randomSeekScan test becomes dramatically faster.

Running the following before and after the fix yields:
{quote}
hbase/bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation --rows=10 randomSeekScan 1
{quote}

Before: 198344ms at offset 0 for 10 rows
After: 238ms at offset 0 for 10 rows;;;","24/Feb/10 17:02;stack;Patch to fix issue Dan describes.;;;","24/Feb/10 17:11;stack;Committed on branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Delete row, followed quickly to put of the same row will sometimes fail.",HBASE-2256,12457251,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,clint.morgan,clint.morgan,24/Feb/10 03:00,11/Jun/22 23:07,14/Jul/23 06:06,11/Jun/22 23:07,0.20.3,,,,,,,,,,,,,,,,,,1,,,"Doing a Delete of a whole row, followed immediately by a put to that row will sometimes miss a cell. Attached is a test to provoke the issue.",,anoop.hbase,bharathv,davelatham,dhruba,evertot,gfeng,guertsen,heliangliang,hongyu.bi,jmspaggi,kevinpet,nkeywal,nvcook42,stack,tlipcon,tobe,tzolkin,xieliang007,yangzhe1991,,,,,,,,,,,,,,,,,,,,,HBASE-2406,HBASE-8927,,,HBASE-8770,,,,,,,,,,,,,"24/Feb/10 03:02;clint.morgan;hbase-2256.patch;https://issues.apache.org/jira/secure/attachment/12436808/hbase-2256.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26227,,,,,Sun Jul 30 12:02:28 UTC 2017,,,,,,,,,,"0|i02e07:",11870,,,,,,,,,,,,,,,,,,,,,"24/Feb/10 03:05;ryanobjc;isnt this due to the millisecond timestamp nature of our puts and delete markers? If the next put has the same millisecond TS as an existing delete record you will be masked by the previous delete.;;;","24/Feb/10 18:09;clint.morgan;Yes, I'm sure that the millisecond nature of timestamps comes in to play here. However, I'm not setting any timestamps, and was under the impression that hbase would always reflect the state of the last operation done. Is this not a valid assumption?

A related question. If I do two puts (w/latest timestamp), am I guaranteed to see the last one? I'm sure many users operate under this assumption.

So, for a given row, I'm doing a delete of an entire row, then a put of two cells in different families. Then I do a get.

Most times I'll see all of the latest put. Sometimes I see nothing for that row. Sometimes I see just one family/cell from the previous put. It seems with 0.20.2 I would either get all the row, or none. However with 0.20.3 I will see all or just one family. Returning just one family seems even more wrong.

So suppose I wish to wipe existing cells of a row, then write some new cells. Is the only way to reliably do this to put a 1 ms pause between the delete and the put? This would hurt my throughput...

;;;","24/Feb/10 18:15;jdcryans;bq. Yes, I'm sure that the millisecond nature of timestamps comes in to play here. However, I'm not setting any timestamps, and was under the impression that hbase would always reflect the state of the last operation done. Is this not a valid assumption?

Do you see this delete problem even on a fully distributed setup? In my experience, they only happen in unit tests where all components are in the same JVM whereas when network is involved some milliseconds will separate two consecutive operations. 

bq. A related question. If I do two puts (w/latest timestamp), am I guaranteed to see the last one? I'm sure many users operate under this assumption.

If they have the same timestamp, there's no guarantee.

bq. So, for a given row, I'm doing a delete of an entire row, then a put of two cells in different families. Then I do a get.

See my first comment, it's ok when not done in unit tests. Like the bigtable paper says, if you need a finer granularity than millisecond you may need to redefine the timestamps (using something like microseconds).;;;","24/Feb/10 18:49;clint.morgan;I have only noticed this in unit tests and a non-distributed setup. However, this Delete,Put happens in ITHbase's IndexRegion which means that even in a distributed setup the client for the put/delete and the regionserver handling them could be in the same JVM.

For the put, put case, it seems to me this could be a real issue. Even in distributed setup sequential puts could happen in the same ms no? However, I did a similar test for Put after Put and it seems to always work. If it did not, I'm sure users would have complained loudly by now.

From my point of view, it would be nice to have this behavior for Put after Delete as well. 

I'm not saying I need finer granularity tham ms. Just that when I'm never explicitly messing with timestamps, I always see a ""correct"" view that reflects my last operation. I just skimmed over the bigtable paper, and could not find an explication about what they do in this case..


;;;","26/Feb/10 08:17;ryanobjc;there is a millisecond resolution, and it might be difficult to get better without changing the storage format so we can get nanos in there.

but still, for most people, doing a put - delete - put all within 1 millisecond is a not common.  maybe it might be possible to change something so we dont have to run up against this issue?;;;","22/Apr/10 18:07;kevinpet;What if I could do something like this:

Put put1 = ...
HTable.put(put1);
Delete delete = new Delete(...).guaranteeAfter(put1);
HTable.delete(delete);
Put put2 = new Put(...).guaranteeAfter(delete);
HTable.put(put2)

It seems like the distributed case isn't a problem since it's so unlikely, but the delete then put seems more plausible. We could set the timestamp to 1ms after the delete if needed. The occasional write will get a timestamp a few ms in the future, which doesn't seem that bad. I think this solves Clint's requirement of seeing a correct view without explicitly messing with timestamps.;;;","22/Mar/11 23:00;nvcook42;We ran into this problem recently in our production code. A single hbase client needed to first clean up of several columns by deleting them and then put a subset those columns back in with new values. Frequently the delete and put call would happen in the same millisecond thus masking the put. For now we have implemented a fix on our side but would be nice to see a real fix for this, where the region servers handle this more gracefully.

Maybe we could log some warnings when this occurs for possible easier debugging? This was an extremely difficult problem to find.

Or maybe someone has a clever solution? 
 ;;;","24/Mar/11 23:09;stack;@Nathaniel This should be fixed as by-product of hbase-2856.;;;","07/Apr/11 20:24;yuzhihong@gmail.com;Currently timestamp for Put and Delete is in milliseconds.
If we can use System.nanoTime(), the chance of this issue happening would be very low.;;;","07/Apr/11 20:30;ryanobjc;Probably can't use nano time, it wraps around too frequently.
;;;","07/Apr/11 20:41;yuzhihong@gmail.com;From http://download.oracle.com/javase/1.5.0/docs/api/java/lang/System.html#nanoTime%28%29 :
Differences in successive calls that span greater than approximately 292 years (263  nanoseconds) will not accurately compute elapsed time due to numerical overflow. ;;;","07/Apr/11 20:44;tlipcon;more importantly, nanotime is elapsed time since some arbitrary system-local reference point, and has no meaning on an absolute scale.;;;","07/Apr/11 21:02;yuzhihong@gmail.com;{code}
	  long l = System.nanoTime();
	  long l2 = System.currentTimeMillis();
{code}
Looking at the values of l (1302209826865074000) and l2 (1302209826865), nanoTime is aligned with time in millis.
Assuming nano and milli timestamps correlate, we can devise (correction) mechanism in master and region servers such that (corrected) nano timestamp reflects the actual millisecond timestamp.;;;","07/Apr/11 21:09;streamy;I think this would be a hacky non-solution, regardless of whether it's epoch nanos or not.;;;","27/May/13 12:54;xieliang007;We(XiaoMi) fixed this issue with introducing a ScanDeleteTrackerWithMVCC.
My workmate [~fenghh] will upload a patch soon.;;;","09/Jun/13 10:08;xieliang007;Please refer to https://issues.apache.org/jira/browse/HBASE-8721 for our fix;;;","18/Nov/13 08:08;heliangliang;Also encountered similar problem. What about this solution?
{code}
public class IncrementingWallTimeEnvironmentEdge implements EnvironmentEdge {
  private long clock = -1;

  public IncrementingWallTimeEnvironmentEdge() {
  }

  @Override
  public long currentTimeMillis() {
    long wallTime = System.currentTimeMillis() << 10; // ~us, or any arbitrary scaling factor

    synchronized (this) {
        if (clock < wallTime) {
          clock = wallTime;
        }
        return clock++;
    }
  }
}
{code}
This would solve this problem and guarantee the timestamp aligned with the wall time clock in milliseconds as long as we set the scaling factor to a larger enough number (i.e. make sure the speed of logical clock is slower than System.currentTimeMillis()). Shift factor from 10 ~20 (1M-1G qps) is proper value for current server configuration which also will not introduce wrapping around concern (584M ~ 0.57M year).;;;","22/Nov/13 08:02;stack;[~heliangliang] I like this notion (It is related a bit to HBASE-8927).  The call to currentTimeMillis is done frequently.  I think we'd combine this change with an attempt at removing as many calls to currentTimeMillis as possible.  We might replace the currentTimeMillis calls that are for tiiming with nanotime calls instead and use this new class for Cell version).;;;","11/Nov/16 09:56;gfeng;It happened in HBase 1.2.3. My code is 
{code}
Delete del = new Delete(row.getBytes());
table.delete(del);

List<Put> puts = myPuts();
table.put(puts);
{code}

Most of time worked fine. But sometimes the data in HBase went crazy. So I don't trust the data  stored.;;;","11/Nov/16 17:34;stack;Please open a new issue [~gfeng] This one is a long time closed. World has changed a bunch since this issue too so probably different cause. Thanks.;;;","30/Jul/17 12:02;stack;HBASE-15968 has been resolved. It adds option to enable change in comparator which should fix this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
take trunk back to hadoop 0.20,HBASE-2255,12457241,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,23/Feb/10 23:33,20/Nov/15 12:41,14/Jul/23 06:06,22/Mar/10 23:40,,,,,,,,,,,,0.90.0,,,,,,,0,,,"revert the dependency on hadoop 0.21, back to hadoop 0.20 (we hardly knew ye)",,kannanm,kaykay.unique,kovyrin,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/10 23:34;ryanobjc;HBASE-2255.patch;https://issues.apache.org/jira/secure/attachment/12436784/HBASE-2255.patch","22/Mar/10 23:40;ryanobjc;hbase-2255.txt;https://issues.apache.org/jira/secure/attachment/12439526/hbase-2255.txt",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26226,Incompatible change,,,,Fri Nov 20 12:41:00 UTC 2015,,,,,,,,,,"0|i0hgxz:",100007,,,,,,,,,,,,,,,,,,,,,"23/Feb/10 23:44;larsfrancke;Could we wait a few days with this (until the dust around the maven change has settled) There also was HBASE-2233 which mentioned that both versions should be supported.

Maven supports ""classifiers""[1] which can perhaps be used for a ""dual"" build system: hbase-0.21.0-hadoop020 and hbase-0.21.0-hadoop021 or something like that. Though I'm not an expert with classifiers and it might not be easily possible at all. Perhaps Paul Smith has an idea?

[1] http://maven.apache.org/pom.html#POM_Relationships;;;","18/Mar/10 21:00;stack;+1;;;","20/Mar/10 15:24;stack;For trunk, we need our patched hadoop 0.20.2 put up somewhere in a maven repo -- or we can check it in as we've done for zk.

On the latter, maybe we don't want to check it in since zk is about to be published to a mvn repo on release of 3.3.0.  When thats done, we'll just have thrift in our lib dir (And Kay Kay has already done the work to make it possible to put this in a mvn repo).;;;","20/Mar/10 16:59;stack;This issue was discussed up on the hbase-dev list: http://mail-archives.apache.org/mod_mbox/hadoop-hbase-dev/201003.mbox/%3C7c962aed1003181128q6dee4d5fr93cc163ba8de2bee@mail.gmail.com%3E

Consensus was that yes, we should take TRUNK back to 0.20.x hadoop at least temporarily.;;;","21/Mar/10 00:10;ryanobjc;i have maven repos prepped for this on people.apache.org.  When I do the commit it will work in concert with published maven artifacts in my own personal space.;;;","21/Mar/10 00:34;kaykay.unique;what would it involve to upgrade zk 3.2.2 to zk 3.3.0 (on hbase trunk) since the mvn artifacts would be published by the zk team with 3.3.0 - which is anytime soon. 

The thrift pom.xml (for 0.2.0) on people.apache.org  seems to have no dependencies, which I believe is not true. ;;;","21/Mar/10 04:19;stack;@Kay Kay I'm trying out the zk 3.3.0 RC0.  Once release, then we should move TRUNK to pull zk from maven repo or are you thinking sooner?  The thrift we're pulling is http://people.apache.org/~psmith/hbase/repo.  Yeah, the pom is off because at a minimum we've discovered it requires sfl4j.  How you suggest we fix it?  Update the pom for thrift in paul's repo?;;;","21/Mar/10 04:34;kaykay.unique;| @Kay Kay I'm trying out the zk 3.3.0 RC0. Once release, then we should move TRUNK to pull zk from maven repo or are you thinking sooner?

sure, of course - we can wait for the 3.3.0 to be released and then try out the new version. I was just cross-linking information here, since the zk team seems to be very close to 3.3.0 , with release notes etc. prepared. So - we can leave it as it is and then revisit sometime later. 

| The thrift we're pulling is http://people.apache.org/~psmith/hbase/repo. Yeah, the pom is off because at a minimum we've discovered it requires sfl4j. How you suggest we fix it? Update the pom for thrift in paul's repo?

I think updating the pom in paul's repo seems like the right thing to do, so that everybody who need to use thrift can benefit from that.  Also , it would be useful to publish the artifacts under org.apache.hbase groupId to a snapshot repository so that it is more public. 

long term, anybody with good offices with the thrift team can gently nudge them towards publishing it themselves ;) . Even if they are reluctant to publishing artifacts, committing that patch alone would suffice since that will make it a lot more easy to publish artifacts ourselves, by using the source tree. 





;;;","21/Mar/10 04:42;ryanobjc;hey guys,

i have a snapshot on my own people.apache.org/~rawson directory. when
i commit this we will be pointing there

;;;","21/Mar/10 05:16;kaykay.unique;Great.  Here is the dependency chain to be inserted in the pom.xml  for thrift, which I assume was hand-coded. 

 <dependencies>
    <dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-api</artifactId>
      <version>1.5.8</version>
    </dependency>
    <dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-simple</artifactId>
      <version>1.5.8</version>
    </dependency>
    <dependency>
      <groupId>commons-lang</groupId>
      <artifactId>commons-lang</artifactId>
      <version>2.4</version>
    </dependency>
  </dependencies>

Good luck ! ;;;","22/Mar/10 22:20;ryanobjc;i've updated my thrift pom on people.apache.org including md5/sha1 sums;;;","22/Mar/10 22:56;kaykay.unique;Great. Thanks. ;;;","22/Mar/10 23:40;ryanobjc;the final patch applied to trunk;;;","20/Nov/15 12:41;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mapping a very big table kills region servers,HBASE-2252,12457199,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,23/Feb/10 19:22,12/Oct/12 06:14,14/Jul/23 06:06,05/Apr/10 20:05,,,,,,,,,,,,0.20.4,0.90.0,,,,,,0,,,"Currently TableInputFormat doesn't change the block caching behavior of scans and one of our table grew so big that using the defaults we kill a least one region server per job run (because of GCs even if we have a heap of 7GB). This doesn't scale well, we should set it by default to false.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/10 22:42;jdcryans;HBASE-2252.patch;https://issues.apache.org/jira/secure/attachment/12440290/HBASE-2252.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26225,Reviewed,,,,Mon Apr 05 20:05:04 UTC 2010,,,,,,,,,,"0|i08swv:",49284,,,,,,,,,,,,,,,,,,,,,"30/Mar/10 22:19;jdcryans;Patch that sets it false by default in both mapred and mapreduce.;;;","30/Mar/10 22:35;stack;Patch looks like it has pollution, unwanted changes.;;;","30/Mar/10 22:42;jdcryans;Well it's wanted but not in the scope of this jira, here's the correct patch.;;;","02/Apr/10 19:06;hbasebuild;+1 on patch

On Tue, Mar 30, 2010 at 3:42 PM, Jean-Daniel Cryans (JIRA)
;;;","02/Apr/10 21:28;streamy;+1;;;","05/Apr/10 20:05;jdcryans;Committed to branch and trunk, thanks for the review guys.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
typo in the maven pom,HBASE-2250,12457108,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,ryanobjc,ryanobjc,23/Feb/10 02:07,11/Jun/22 23:08,14/Jul/23 06:06,23/Feb/10 02:16,,,,,,,,,,,,,,,,,,,0,,,a simple typo in the maven pom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/10 02:08;ryanobjc;HBASE-X.patch;https://issues.apache.org/jira/secure/attachment/12436665/HBASE-X.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26224,,,,,Tue Feb 23 02:16:27 UTC 2010,,,,,,,,,,"0|i0hgx3:",100003,,,,,,,,,,,,,,,,,,,,,"23/Feb/10 02:16;ryanobjc;i committed this w/o review;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide new non-copy mechanism to assure atomic reads in get and scan,HBASE-2248,12457095,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,ryanobjc,davelatham,davelatham,22/Feb/10 23:35,12/Oct/12 06:14,14/Jul/23 06:06,14/Apr/10 21:57,0.20.3,,,,,,,,,,,0.20.4,,,,,,,0,,,"HBASE-2037 introduced a new MemStoreScanner which triggers a ConcurrentSkipListMap.buildFromSorted clone of the memstore and snapshot when starting a scan.

After upgrading to 0.20.3, we noticed a big slowdown in our use of short scans.  Some of our data repesent a time series.   The data is stored in time series order, MR jobs often insert/update new data at the end of the series, and queries usually have to pick up some or all of the series.  These are often scans of 0-100 rows at a time.  To load one page, we'll observe about 20 such scans being triggered concurrently, and they take 2 seconds to complete.  Doing a thread dump of a region server shows many threads in ConcurrentSkipListMap.biuldFromSorted which traverses the entire map of key values to copy it.  ",,davelatham,evertot,hammer,kannanm,kaykay.unique,streamy,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-2294,HBASE-2959,,,HBASE-2322,HBASE-2249,HBASE-2265,,,,,,,,,,,"15/Apr/10 21:49;ryanobjc;ASF.LICENSE.NOT.GRANTED--HBASE-2248-no-row-locks.txt;https://issues.apache.org/jira/secure/attachment/12441881/ASF.LICENSE.NOT.GRANTED--HBASE-2248-no-row-locks.txt","15/Apr/10 01:39;ryanobjc;ASF.LICENSE.NOT.GRANTED--HBASE-2248-rr-final1.txt;https://issues.apache.org/jira/secure/attachment/12441785/ASF.LICENSE.NOT.GRANTED--HBASE-2248-rr-final1.txt","14/Apr/10 04:47;ryanobjc;ASF.LICENSE.NOT.GRANTED--HBASE-2248-rr-pre-durability4.txt;https://issues.apache.org/jira/secure/attachment/12441688/ASF.LICENSE.NOT.GRANTED--HBASE-2248-rr-pre-durability4.txt","12/Apr/10 15:26;apurtell;ASF.LICENSE.NOT.GRANTED--profile.png;https://issues.apache.org/jira/secure/attachment/12441500/ASF.LICENSE.NOT.GRANTED--profile.png","12/Apr/10 15:26;apurtell;ASF.LICENSE.NOT.GRANTED--put_call_graph.png;https://issues.apache.org/jira/secure/attachment/12441501/ASF.LICENSE.NOT.GRANTED--put_call_graph.png","12/Mar/10 05:31;stack;HBASE-2248-GetsAsScans3.patch;https://issues.apache.org/jira/secure/attachment/12438577/HBASE-2248-GetsAsScans3.patch","24/Feb/10 17:48;stack;HBASE-2248-demonstrate-previous-impl-bugs.patch;https://issues.apache.org/jira/secure/attachment/12436876/HBASE-2248-demonstrate-previous-impl-bugs.patch","26/Feb/10 12:55;stack;HBASE-2248.patch;https://issues.apache.org/jira/secure/attachment/12437174/HBASE-2248.patch","23/Feb/10 18:39;stack;Screen shot 2010-02-23 at 10.33.38 AM.png;https://issues.apache.org/jira/secure/attachment/12436737/Screen+shot+2010-02-23+at+10.33.38+AM.png","23/Feb/10 18:09;davelatham;hbase-2248.gc;https://issues.apache.org/jira/secure/attachment/12436730/hbase-2248.gc","05/Mar/10 03:53;tlipcon;hbase-2248.txt;https://issues.apache.org/jira/secure/attachment/12437967/hbase-2248.txt","05/Mar/10 03:25;tlipcon;readownwrites-lost.2.patch;https://issues.apache.org/jira/secure/attachment/12437964/readownwrites-lost.2.patch","05/Mar/10 03:14;tlipcon;readownwrites-lost.patch;https://issues.apache.org/jira/secure/attachment/12437963/readownwrites-lost.patch","22/Feb/10 23:35;davelatham;threads.txt;https://issues.apache.org/jira/secure/attachment/12436646/threads.txt",,14.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26223,Incompatible change,Reviewed,,,Mon Apr 19 17:00:13 UTC 2010,,,,,,,,,,"0|i08srz:",49262,This patch changes the Get code path to instead be a Scan of one row.  This means than inserting cells out of timestamp order should work now (tests to verify to follow part of hbase-2294) but also that a delete at an explicit timestamp now overshadows EVEN if the effected cell is put after the delete (The old Get code path did early-out so a subsequent puts would not see the delete).,,,,,,,,,,,,,,,,,,,,"22/Feb/10 23:35;davelatham;Here's some example threads from a dump.;;;","22/Feb/10 23:42;davelatham;After doing a flush on the table, the scans are about 100x faster.;;;","23/Feb/10 00:02;dan.washusen;I notice the performance evaluation flushes the table after each test completes, as a result none of the read tests take the memstore into account.  Maybe the PerformanceEvaluation class could be changed to make the flush optional?;;;","23/Feb/10 00:48;dan.washusen;HBASE-2249 will allow changes to the MemStore to be performance tested.;;;","23/Feb/10 03:58;dan.washusen;K, with PerformanceEvaluation updates running ""hbase org.apache.hadoop.hbase.PerformanceEvaluation --rows=1000 scanRange100 10"" each scan takes on average 9ms to return a max of 100 rows (random data means they don't usually return 100 rows, average seemed to be around 70 rows).

The setup for that tests is as follows;
1 master
4 region servers (12GB heap)

1 million rows set up using:
hbase org.apache.hadoop.hbase.PerformanceEvaluation randomWrite 1

There were four regions all on one host.  Each region had roughly 40MB in the MemStore...;;;","23/Feb/10 04:03;jdcryans;bq. 1 million rows set up using:

With randomWrite you don't write 1M rows (more like ~700,000 IIRC) so that explains why your scans aren't always of 100 rows.;;;","23/Feb/10 04:13;dan.washusen;@JD: that would explain it...

With --nomapred (10 client threads in a single VM) each scan took 120-140ms...  

Also, the randomSeekScan test each scan seems VERY slow.  Each scan takes about 15 seconds...?  The scanRange100 uses a startRow and stopRow to get 100 rows back (well 70 rows).  The randomSeekScan using a ""scan.setFilter(new WhileMatchFilter(new PageFilter(120)));"".  What's up with that?

Oh, also, those tests are on the latest 0.20 branch (not on the 0.20.3 release)... ;;;","23/Feb/10 06:46;dan.washusen;Cloning the MemStore based on the scan.startRow and scan.stopRow drops the scan times from ~9ms per scan to ~3ms per scan on the above hardware...;;;","23/Feb/10 07:40;tlipcon;Can anyone shed light on why HBASE-2037 introduced this clone in the first place? Seems like a totally braindead thing for performance.;;;","23/Feb/10 07:55;ryanobjc;I had a look at the implementation of clone, and it is really not appropriate for what we are doing.

I would like to open up discussions to revert the original patch.  I would argue there has been too many lurking issues, and the additional functionality, while useful, doesnt justify crippling performance.;;;","23/Feb/10 08:41;dan.washusen;@Todd: I didn't author the change but it relates to the [tests|http://svn.apache.org/viewvc/hadoop/hbase/branches/0.20/src/test/org/apache/hadoop/hbase/regionserver/TestHRegion.java?p2=/hadoop/hbase/branches/0.20/src/test/org/apache/hadoop/hbase/regionserver/TestHRegion.java&p1=/hadoop/hbase/branches/0.20/src/test/org/apache/hadoop/hbase/regionserver/TestHRegion.java&r1=896138&r2=896137&view=diff&pathrev=896138] added with the change.

@Ryan: The tests added to PE as a result of HBASE-2249 seem to indicate that even with a fully loaded MemStore it takes 9ms to complete a scan for ~100 rows with 10 concurrent client VMs hitting a single region server.  That seems to contradict the 1-2 seconds seen by Dave.   The thread dump does seems to indicate the clone but maybe something else is coming into play as well?  Maybe the additional 4KB memory allocation is bringing GC into it?;;;","23/Feb/10 08:48;ryanobjc;could you please tell me where your 4k of memory quote is coming from?

the clone() is a deep/shallow clone.  The KeyValues arent being cloned, but in ever other way the clone is a deep clone - it copies all the nodes!  That could be literally a million nodes!  The number of nodes is dependent on your data size... 64MB memstore can accomodate 1.3m values if your KeyValue size is ~ 50 bytes.  Or even larger if you start kicking in the memstore multiplier during a pending snapshot, you could have 4m+ nodes in a snapshot and a oversized kvset.  Clone is not really viable, it needs to be rolled back.  Furthermore it doesnt provide atomic protection anyways.;;;","23/Feb/10 11:35;dan.washusen;Very good point...  

Even if the clone took the scan start and stop rows into account, there is still the possibility that only one or neither of them has been provided provided... ;;;","23/Feb/10 15:39;ykulbak;Ryan:
The 4K quote is my mistake, based on a non-typical HBASE usage (small memstore, large KVs).
Cloning is definitely bad. It's only benefit is that it allows the scan to be isolated from on-going writes; HRegion#newScannerLock takes care of writes not coming in while the scanner is created, so 0.20.3 unlike 0.20.2 does provide protection from 'partial puts' if this was what you're implying by 'atomic protection'. There is also a test added to TestHRegion which verifies that. 

I'm not sure that rollback is a viable option:  
The 0.20.2 Memstore was using the ConcurrentSkipListMap#tailMap for every row. tailMap incurs an O(log(n)) overhead when called on a ConcurrentSkipListMap so the total overhead of scanning the whole memstore in some cases, may be very close to the overhead of a complete sort of the KVs in memstore.
The 0.20.2 MemStore and MemStoreScanner are also functionally incorrect since  
- The scanner may observe a 'partial put' (not atomically protected) 
- The scanner scans incorrectly when a snapshot exists    

since we observed a considerable 'single scan' performance improvement using the new MemStore implementation could the performance hit stem from increased GC overhead on multiple concurrent scans?   
Note that with 0.20.2 we observed that MemStoreScanner is running slower than StoreFileScanner..  

Is it possible to avoid both 'partial puts' and cloning by 'timestamping' memstore records? e.g. each new KV in memstore gets a 'memstore timestamp' and when a scanner is created it grabs the current timestamp so that it knows to ignore KVs which entered the store after its creation?  Should probably use a counter and not currentTimeMillis to ensure a clear-cut. 

------------
About those ~50 byte KVs, according to my calcs:
KeyLength: 4 bytes
ValueLength: 4 bytes
rowLength: 2 bytes
FamilyLength: 1 byte
TimeStamp: 8 bytes
Type: 1 byte

There are 20 bytes of overhead to start with.
Adding an average of 10 bytes for the column and qualifier brings it to 40 bytes. 
This leaves 10 bytes (out of 50) for the row + value. Meaning 80% of the storage is overhead.
My point is that if ~50b KVs are the common use-case  some optimization needs to be made to the way things are stored.
Perhaps you meant 50b for row+value?

;;;","23/Feb/10 17:20;davelatham;Thanks, Dan, and others for looking into this issue.  The table where we were seeing these slow scans was definitely a tall, narrow table.  Each row has one cell, the column family and qualifier are each one byte.  The row varies, but is typically 8-20 bytes, and the value is usually 4 bytes or less.  Most common is probably row - 12 bytes, col fam - 1 byte, qualifier 1 byte, value - 3 bytes, giving 17 bytes plus overhead.

As I was trying to understand the discrepancy between the PE results you mentioned and what I've observed, I looked in to PerformanceEvaluation.  It looks like the timer only starts after the scanner is constructed which means that the MemStore clone isn't being timed as part of the test, so that would probably explain why the test seems fast.  Just reasoning, it seems hard to believe that ConcurrentSkipListMap.buildFromSorted could complete a million iterations that fast.;;;","23/Feb/10 17:52;stack;.bq Can anyone shed light on why HBASE-2037 introduced this clone in the first place? Seems like a totally braindead thing for performance. 

Mea Culpa. I should have caught this in review, the non-scalable, expensive full-copy.  Dumb.

I also should have run PE to catch degradation in performance before release though in this case, according to Dan, as PE is now, we'd not have caught the slowed-down memstore since we flush after each PE run and since the short-scan test is new with no history (Long time ago I wrote up a how-to-release: http://wiki.apache.org/hadoop/Hbase/HowToRelease.  It says PE required but I think I've not followed this receipe in a good while now).

.bq The 0.20.2 Memstore was using the ConcurrentSkipListMap#tailMap for every row. tailMap incurs an O(log) overhead when called on a ConcurrentSkipListMap so the total overhead of scanning the whole memstore in some cases, may be very close to the overhead of a complete sort of the KVs in memstore.

In the old implementation, we used to also make a copy of a row, everytime we called a next, to protect against the case where snapshot was removed out from under us.

.bq The scanner scans incorrectly when a snapshot exists

Why was this again?

.bq ... increased GC overhead on multiple concurrent scans

Dave, can you enable GC logging?  Even if this is the case, it needs to be addressed.

.bq Is it possible to avoid both 'partial puts' and cloning by 'timestamping' memstore records? e.g. each new KV in memstore gets a 'memstore timestamp' and when a scanner is created it grabs the current timestamp so that it knows to ignore KVs which entered the store after its creation? Should probably use a counter and not currentTimeMillis to ensure a clear-cut.

How would we snapshot such a thing?

We could add another ts/counter to KV.  We could do an AND on the type setting a bit if extra ts is present.  We then write out the KV as old style, dropping extra ts when we flush to hfile, or we just dump it all out.  System would need to be able to work with old-style KVs.  Comparator would be adjusted to accomodate new KV.   We'd do a tailset each time we made a scanner?  This would be a big change.  We should probably bump rpc version and require a restart of hbase cluster on upgrade.;;;","23/Feb/10 18:09;davelatham;I've got gc logging enabled.  Here's a snapshot of the regionserver for a few minutes during which I ran this test 5 or 6 times and generated 360 short scans.  Let me know if there's any other GC info that would be useful.

;;;","23/Feb/10 18:39;stack;There is a bunch of YG GC'ing going on... Might slow things some but not by much.  I've attached a screen shot.;;;","23/Feb/10 23:03;dan.washusen;@Dave: Could you have at HBASE-2249 and confirm that the call to HTable.getScanner(...) is now being timed?;;;","23/Feb/10 23:40;davelatham;@Dan: Took a read over the patch, though it seemed to be based in a different dir and didn't want to apply nicely.  From what I can see the ScanTest still does getScanner in testSetup before the timer is begun.  This may be fine, if the point of this test is to measure scan performance per-row and not setup/teardown time.  It just explains why the ScanTest doesn't exhibit this issue.  It does look like other tests, such as the RandomSeekScanTest and the new RandomScanWithRangeTest do test setup/teardown time as part of each ""testRow"" and should exhibit this issue, if run.;;;","23/Feb/10 23:44;ryanobjc;done properly, a timestamp oriented fix to version memstore should not require any RPC version bump, its all internal. ;;;","24/Feb/10 01:28;dan.washusen;@Dave: 

Correct you are.  I've added comments on HBASE-2249 as a result of your comments here...

It's worth noting that in the case of ScanTest the cost of setting up the ResultScanner is almost non-existent compared to the cost of scanning over the majority of table.  The ScanTest takes 23 seconds in total according to the log output (including opening the scanner etc).

Dave, the numbers I posted above (9ms) were from the RandomScanWithRangeTest.  As you mention, these tests include the cost of opening the scanner.  I was under the impression that this was closer to your use case (e.g. specify both a scan.startRow and scan.stopRow which returns a small number of rows)...?;;;","24/Feb/10 03:06;ykulbak;I did the following sanity check: I rolled back memstore to just before HBASE-2037 was applied [last commit on 21 Oct 2009]. 
[ To get things going I had to put back the MemStore#numKeyValues method and change the  MemStore#clearSnapshot   argument to SortedSet ]

I then ran TestHRegion and two tests failed:
- testFlushCacheWhileScanning - demonstrates the incorrect scans while a snapshot exists issue
- testWritesWhileScanning - demonstrates 'partial puts' being visible to the scanner
I also tried running TestMemStore but all the tests there have passed. I didn't try running the whole suite.

It took me a while to figure out what exactly goes wrong when a snapshot exists, the short (and vague) explanation is that the scanner may return keys in a 'non ordered' manner, meaning a KV with a higher row  may be returned before a KV with a lower row because the result list which aggregates results from both snapshot and kvset doesn't guarantee the KVs are added in a sorted order. I think there's a way to add a simple test to TestMemStore which will demonstrate that..   

;;;","24/Feb/10 17:48;stack;Patch that restores memstore to how it was.  With this in place run memstore unit tests to see how old implementation was broke.;;;","26/Feb/10 08:23;ryanobjc;i have a prototype implementation of how to fix the atomic read without using locking or copying.  I'll put up a patch within a few days.  It's a little subtle, but put simply it uses sequential ""Timestamps"" to internally version the memstore so people know to ignore half written rows.;;;","26/Feb/10 12:55;stack;Here is an attempt.  Tests pass.  Posting for review.  Need to do load tests yet.

""- Added a (transient) int updateId to KeyValue
- Memstore populates it on Adds and Deletes 
- When a MemstoreScanner is created it grabs the current id (actually increments  it to make sure no KV has that same id) and ignores records from kvset having an id greater than the one grabbed. Snapshots are scanned in full since they're not updated during the scanner's lifetime hence there's no risk of partial updates being visible.  There may be an issue with delete's becoming partly visible in this scheme, I'll check that later.""
;;;","26/Feb/10 16:33;tlipcon;bq. There may be an issue with delete's becoming partly visible in this scheme

I would think so - deletes in the memstore don't use tombstones, do they? Similarly for updates - if you update a row, its internal ts will update and the scanner will no longer see the old version either.;;;","26/Feb/10 22:37;ryanobjc;deletes use tombstones, but the current GET code might need... adjustment to make it work. I'm working on a base fix which I will post soon and I'll also check the get implementation. ;;;","27/Feb/10 12:52;stack;Yeah, if client adds new edit w/ exact same ts and the comparator used by memstore does not take sequenceid into consideration, we'll have issues Todd identifies.  Perhaps change the Comparator used by MemStore to consider sequenceid?   Also missing from patch is enforcement of the fact that on flush, the flush file has deletes that apply to older files only -- not to current flush file content.;;;","02/Mar/10 01:58;ryanobjc;I'm working on this, there is a general approach hammered out and code to be written.

The approach is like so:

- on read from memstore, for each row, we grab the 'read number' and ignore any keyvalues in the structure newer (ie: > value)
- on put to hregion/memstore, we start a write 'tx' and get a write-number, and put keyvalues with said write-number.  when we are finished, that write-number is 'commited' which causes the read number to be advanced most of the time.  under concurrent writes we have a little queue and slower puts may slightly hold up puts that come before it.  

this will need to be extensively tested to see how the performance profile changes. it will allow us to remove the newScannerLock.;;;","04/Mar/10 10:23;ryanobjc;Ok here is my proposal to fix this, hopefully once and for all.

The only thing that isn't covered is deletes:
- removing keyvalues wont ever be atomic
- we could stop deleting key values, but the get code would have to be checked
-- the flush would also need to prune out deleted key values to keep the delete invariant of 'get' going on.
;;;","04/Mar/10 10:28;ryanobjc;my patch passes all the new tests added by HBASE-2037 which focus on parallelism while doing scans.  ;;;","04/Mar/10 20:14;streamy;Might be time to turn gets into scans so we don't have a second read code path.;;;","04/Mar/10 22:46;ykulbak;Turning gets into scans will cause some minor functional changes. See for example the differences between gets and scans exposed in TestClient#testDeletes. IMHO eliminating the functional differences between gets and scans will be a change for the better but perhaps there are existing users which rely those subtle differences.;;;","04/Mar/10 22:56;tlipcon;bq. IMHO eliminating the functional differences between gets and scans will be a change for the better but perhaps there are existing users which rely those subtle differences

+1 for eliminating the differences. If people are relying on broken behavior, they should fix their applications ;-) HBase is not 1.0; let's pick sanity over compatibility.;;;","05/Mar/10 02:46;tlipcon;Hey Ryan

I looked over this patch a bit this afternoon. It's clever but I think it can result in loss of read-your-own-writes consistency for a single client. Consider this scenario:

|| Action || Read # || Write # || memstoreRead || memstoreWrite ||
| Client A begins a put on row R   | - | 1 | 0 | 1 |
| Client B begins a put on row S   | - | 2| 0 | 2 |
| Client B finishes a put on row S | - | - | 0 | 2 |
| Client B initiates a get on row S | 0 | - | 0 | 2 |

So, since client A's put #1 is still ongoing on a separate row, client B is unable to read version #2 of its row.

I think dropping consistency below read-your-own-writes is bad, even though it's rare that the above situation would occur. Under high throughput I think it's possible to occur, and it could be very very bad if people are relying on this level of consistency to implement transactions, etc.

One possible solution is that completeMemstoreInsert can spin until memstoreRead >= e.getWriteNumber(). Given that it only has to wait for other concurrent writers to finish, a spin on memstoreRead.get() should only go a few cycles and actually be reasonably efficient.

I'll think a bit about whether there are other possible solutions.;;;","05/Mar/10 03:14;tlipcon;Here's a test case patch (on top of yours) which should illustrate the issue. It fails every time for me on a dual core box:

Didnt read own writes expected:<395> but was:<394>
junit.framework.AssertionFailedError: Didnt read own writes expected:<395> but was:<394>
        at org.apache.hadoop.hbase.regionserver.TestMemStore$ReadOwnWritesTester.internalRun(TestMemStore.java:293)
        at org.apache.hadoop.hbase.regionserver.TestMemStore$ReadOwnWritesTester.run(TestMemStore.java:268)
;;;","05/Mar/10 03:25;tlipcon;Here's a slightly better test patch, much more sure to fail.

(this test could easily be written without multiple threads, but as an illustration of the client's view of the consistency, the threads are useful);;;","05/Mar/10 03:44;ryanobjc;I think your suggestion is a good one, the race condition is really small, and holding up a client for just a few more microseconds should be reasonable.  Once we restructure to not put logs appends between memstore puts, we are literally talking about the speed of adding a few dozen entries in an array.  There is no data copy involved, since KeyValue was already read in during RPC time, and we are talking inserting small objects into a data structure.

I originally thought of being speedy about returning, but read your own writes does make this be an issue.  I'll add in your suggestions and put this test in as well.

Thanks for the great test!;;;","05/Mar/10 03:53;tlipcon;Here's a patch on top of Ryan's which implements the spin-wait. The concurrency test for read-own-writes now passes.;;;","12/Mar/10 05:24;stack;@Ryan, your next patch picks up Todds spin-wait I believe?;;;","12/Mar/10 05:31;stack;Here is a different take for review and input on how to solve this issue.  

Get is now implemented using Scan. I deleted lots of get-related classes/code including the QueryMatcher. Deletes are no longer removing KV's from memstore.  The change so on flush we filter deleted KVs is not done in this patch -- can be done in another issue.  Maybe we don't want to filter deleted KVs on flush but rather on minor compactions, for instance (The axiom that a file hold only deletes that pertain to values held in storefiles that follow may not be necessary when gets are implemented using scan?).



Things left to do:
- Performance test
- More accurate heap size calculation for HRegion
- Discuss where/when deletes should be partially applied

Here is more detail on what this change includes:

M       src/contrib/indexed/src/java/org/apache/hadoop/hbase/regionserver/IdxRegion.java
 minor tweak due to Memstore#getScanners signature change

M       src/java/org/apache/hadoop/hbase/HConstants.java
 Appended EMPTY_KEY_VALUE_UPDATE_ID to stand for an unset update id

M       src/java/org/apache/hadoop/hbase/KeyValue.java
 Added a transient int updateId + accessors + heap size adjustment
 Added a createLastOnRow method (similar to create first on row) and
made sure the comparator treats this case symmetrically

M       src/java/org/apache/hadoop/hbase/client/Scan.java
 Added a constructor which accepts a Get and creates a matching scan +
a convenience method isGetScan

M       src/java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java
 Modified references to QueryMatcher to refer to ScanQueryMatcher

D       src/java/org/apache/hadoop/hbase/regionserver/DeleteCompare.java
M       src/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java
 QueryMatcher -> ScanQueryMatcher

M       src/java/org/apache/hadoop/hbase/regionserver/GetClosestRowBeforeTracker.java
 QueryMatcher -> ScanQueryMatcher

D       src/java/org/apache/hadoop/hbase/regionserver/GetDeleteTracker.java
M       src/java/org/apache/hadoop/hbase/regionserver/HRegion.java
 Added a member of type RegionUpdateTracker which is intialized both
in the constructor and every flush + heap size adjustment
 Scans are now paused while flushers prepare (e.g. snapshots are taken)
 Old gets replaced with new get implementation (which uses scans)
 #getClosestRowBefore is now using HRegion#get instead of Store#get
 #delete(Delete,Integer,boolean) no longer aquires a newScannerLock
and also tracks update ids using the update tracker
 #delete(byte[],List,boolean) protected changed to package since it's
used as an internal HRegion subroutine and accessed a few times by
tests. It's also no longer aquires the update lock
 #put no longer aquires newScannerLock also modified to track update ids
 RegionScanner stop-row logic was adjusted to support get scans. Also,
RegionUpdateTracker#UpdateIdValidator is now aquired and passed down
to store scanners

M       src/java/org/apache/hadoop/hbase/regionserver/KeyValueSkipListSet.java
 no longer Cloneable

M       src/java/org/apache/hadoop/hbase/regionserver/MemStore.java
 deleted lots of unneeded logic, mainly around deletes (very much
simplifed) and gets (no longer needed)

M       src/java/org/apache/hadoop/hbase/regionserver/MemStoreScanner.java
 Modified to consider UpdateIdValidator for kvset KeyValues. snapshot
kv's are reset to undefined update if for Store#updateColumnValue to
remain backward compatible

D       src/java/org/apache/hadoop/hbase/regionserver/QueryMatcher.java
A       src/java/org/apache/hadoop/hbase/regionserver/RegionUpdateTracker.java
 Trackes updates to HRegions. See javadoc.

M       src/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java
 Fixed to throw an exception as comment suggests

M       src/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java
 Merged with the deleted QueryMatcher. Added a slight variant for get
scans to use 'lastInRows'

M       src/java/org/apache/hadoop/hbase/regionserver/ScanWildcardColumnTracker.java
 QueryMatcher -> ScanQueryMatcher

M       src/java/org/apache/hadoop/hbase/regionserver/Store.java
 #getScanner now accepts an UpdateIdValidator

 #get deleted
 #updateColumnValue modified to use scans and not memstore#getWithCode
(which was deleted)
D       src/java/org/apache/hadoop/hbase/regionserver/StoreFileGetScan.java
M       src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
 QueryMatcher.MatchCode -> ScanQueryMatcher.MatchCode
 passing around of the UpdateIdValidator

D       src/java/org/apache/hadoop/hbase/regionserver/WildcardColumnTracker.java
M       src/test/org/apache/hadoop/hbase/TestKeyValue.java
M       src/test/org/apache/hadoop/hbase/client/TestClient.java
M       src/test/org/apache/hadoop/hbase/io/TestHeapSize.java
D       src/test/org/apache/hadoop/hbase/regionserver/TestDeleteCompare.java
M       src/test/org/apache/hadoop/hbase/regionserver/TestExplicitColumnTracker.java
D       src/test/org/apache/hadoop/hbase/regionserver/TestGetDeleteTracker.java
M       src/test/org/apache/hadoop/hbase/regionserver/TestHRegion.java
M       src/test/org/apache/hadoop/hbase/regionserver/TestMemStore.java
D       src/test/org/apache/hadoop/hbase/regionserver/TestQueryMatcher.java
A       src/test/org/apache/hadoop/hbase/regionserver/TestRegionUpdateTracker.java
M       src/test/org/apache/hadoop/hbase/regionserver/TestScanWildcardColumnTracker.java
M       src/test/org/apache/hadoop/hbase/regionserver/TestStore.java
D       src/test/org/apache/hadoop/hbase/regionserver/TestWildcardColumnTracker.java;;;","12/Mar/10 05:46;ryanobjc;Yes I have asked Todd and rolled up his patch. I have identified a small
race condition in scanning today and ill fix it soon and likely post on
Monday.

On Mar 12, 2010 12:25 AM, ""stack (JIRA)"" <jira@apache.org> wrote:


   [
https://issues.apache.org/jira/browse/HBASE-2248?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12844367#action_12844367]

stack commented on HBASE-2248:
------------------------------

@Ryan, your next patch picks up Todds spin-wait I believe?

HBASE-2248-ryan.patch, hbase-2248.gc, HBASE-2248.patch, hbase-2248.txt,
readownwrites-lost.2.patch, readownwrites-lost.patch, Screen shot 2010-02-23
at 10.33.38 AM.png, threads.txt
ConcurrentSkipListMap.buildFromSorted clone of the memstore and snapshot
when starting a scan.
scans.  Some of our data repesent a time series.   The data is stored in
time series order, MR jobs often insert/update new data at the end of the
series, and queries usually have to pick up some or all of the series.
 These are often scans of 0-100 rows at a time.  To load one page, we'll
observe about 20 such scans being triggered concurrently, and they take 2
seconds to complete.  Doing a thread dump of a region server shows many
threads in ConcurrentSkipListMap.biuldFromSorted which traverses the entire
map of key values to copy it.

--
This message is automatically generated by JIRA.
-
You can reply to this email to add a comment to the issue online.
;;;","15/Mar/10 06:56;tlipcon;I'm upgrading this to blocker - this patch fixes a ton of deadlock scenarios (see HBASE-2322);;;","15/Mar/10 21:30;tlipcon;Has this JIRA outgrown its scope? Should the reasonably small fix that Ryan and I did go in first, and then we can do the get->scan conversion separately?;;;","15/Mar/10 21:39;ryanobjc;Here is my patch to address the issue.
Some notes:
- we use a single forward pushing read point to accomplish atomicity.  Right now with appends and memstore puts mixed in, this causes writes to appear to be serialized. There is no sync block which prevents multiple puts from being done at the same time though.  When we restructure WAL append to happen in one go, and memstore put to happen after, this issue will go away.
- this patch does atomicity at a multi-family level. This means that if a write is going across multiple families, and we do a concurrent scan (AND if a concurrent flush also happens) we will only read the previously completely written row. No partial multi-family row reads. 
- Deletes are also atomic. By no longer removing KeyValues from memstore we make it so. Also adjusted Gets to use 1 row Scans, and put in a TODO for a bloomfilter hook.
- During a scan, we will ride over a snapshot and see the new data. We will also reset the read point between every row, so a scan will see new values as they are inserted _after_ it's current read point.  If a row is updated after a scan has already seen it, it will of course not see that value.

Some thanks:
- Todd for pointing out a case where read-your-own-writes might not happen
- Hints from the HBASE-2248-GetsAsScans patch for doing single row scans.
;;;","15/Mar/10 22:01;tlipcon;Hey Ryan.

Will try to take a look at your patch early this week. Regarding the new atomicity properties, can you please comment in HBASE-2294? I want to make sure that, if we add these properties, that (a) they are really user requirements and (b) we have documented them somewhere. If (a) isn't true, we should document that, in case we find efficiency improvements we can make by dropping some of them.;;;","16/Mar/10 00:15;ryanobjc;My patch doesn't add any new properties, it just makes the existing ones efficient and removes locks.  I already enumerated a number of properties we have and would like to have in HBASE-2294.  ;;;","16/Mar/10 02:40;apurtell;Some feedback based on initial tests:

- Heap sizes need to be adjusted for KeyValue (add a long), HRegion (add a reference), and MemStore (add a reference).

- Indexed contrib is unhappy:
-- o.a.h.h.TestIdxHBaseCluster fails.
-- o.a.h.h.regionserver.TestHRegionWithIdxRegion OOMEs after 376 put iterations. 

The OOME is concerning as might be catching a memory leak introduced in the change set. 

I'm going to put it up on EC2 and throw PE at it tomorrow. 





;;;","16/Mar/10 05:00;ykulbak;.bq 'm going to put it up on EC2 and throw PE at it tomorrow.

What are you going to compare it against?
May I suggest comparing against both the branch without the patch and the branch with GetsAsScans3 applied?;;;","16/Mar/10 05:54;stack;Yeah, I just tried to run test suite and ran into at least the TestHeapSize failures.

On a test up on cluster, something is up.  Its not deadlocked but its only making slow progress.  Regionservers are all waiting for something to do.  Will look in morning.

On the patch:

+ ""aka DNC"" ... whats DNC? (Democratic National Committee?)
+ In KV, it has ""+   * @deprecated""  Usually deprecated points helpfully to what should be used instead.  What should folks use instead of createFirstOnRow override?
+ +1 on this comment of yours ""+      // TODO the family and qualifier should be compared separately""
+ So, on flush of the MemStore, we don't need to clean out items that MemStore Deletes effect?  We now let go of the old axiom that Deletes in storefiles only apply to storefiles that follow and not to the current storefile?
+ I love all the stuff removed.

More review later.

What do we see as implications of removal of the special Get-code path?

+ Is it true that now, you can do inserts where timestamps are out of order? (If no deletes?)  If so, don't we need unit tests to prove this assertion?
+ What about performance?  Though the new Get-Scan does storefile accesses in parallel, if > N storefiles, if looking for latest version only, we'll be slower (at least until we add BFs?).;;;","16/Mar/10 06:37;ryanobjc;There is some profiling to be done to figure out what the problems might be.  I think running some Unit tests under a profiler will be illuminating.

DNC = do not care, it's a hardware engineering thing :-)

The problem with Get vs Scan, is the old code was not correct, so favoring a faster incorrect code is something I thought we agreed we would not do. But yes, we no longer get that assumed performance improvement.;;;","16/Mar/10 06:39;tlipcon;HBASE-2265 should really help with culling access to older regions (without the more complicated bloom filter solution);;;","19/Mar/10 19:26;apurtell;Commit of HBASE-2283 invalidates the patch on this issue.;;;","19/Mar/10 19:41;ryanobjc;ill update my patch to accomodate this commit.

it doesn't ""invalidate"" the patch - there is still a window of opportunity to see partial row updates.;;;","19/Mar/10 19:44;apurtell;Yeah, ok, imprecise word choice. Thanks for rebasing. ;;;","19/Mar/10 20:05;ryanobjc;ill try to update this today or this weekend!;;;","20/Mar/10 20:54;stack;+1 on updating the patch.  I just tried to do it and its a little involved so left it to the expert.

I've been doing a rough benchmarking of 0.20.2 hbase so I can measure roughly how this patch effects coarse performance (I didn't bother measuring 0.20.3.  It must be slower than 0.20.2).;;;","06/Apr/10 00:51;ryanobjc;Here is my update to my patch, this time I am using iterators to scan the memstore and snapshot.  There are a number of fixes to all sorts of fun race conditions, etc.

The best news: this is the fastest memstore scanner HBase has seen.  It is about 15x faster than the 0.20.3 version based on the microbenchmark included in the patch.  The old code takes about 400-500ms to scan 250k KeyValues in memstore, and this new patch takes 25-30ms.

I haven't run all the tests yet, but it passes the core TestMemStore and TestHRegion which contain all the hard tests that have concurrency.;;;","06/Apr/10 01:39;ryanobjc;a version that compiles and passes TestHeapSize;;;","08/Apr/10 06:29;stack;Can we have a version for 0.20_pre_durability branch Ryan?  There are a bunch of failures all in HRegion.  Some I can sort of make sense of but others would take me a while to figure.  You know the code so would probably take you short amount of time?;;;","08/Apr/10 19:10;stack;This is a big patch.  Can we have a bit more detail than what is given above on what it does to help w/ review?

Here's some comments so far:

In KV, this looks like a fix to the comparator:

{code}
+      if (rcolumnlength == 0 && rtype == Type.Minimum.getCode()) {
+        return -1;
+      }
{code}

If we had a getScan datamember flag -- true if this scan is a Get scan -- we could set it if the constructor that takes a Get is invoked and avoid comparing start and end rows.  If flag is not set, go ahead and do the compare.

Want to remove this?

{code}
+//    if (LOG.isDebugEnabled()) {
+//      LOG.debug(""compareResult="" + compareResult + "" "" + Bytes.toString(data, offset, length));
+//    }
{code}

This has to be public?

{code}
+  public ReadWriteConsistencyControl getRWCC() {
{code}

Is this unused?

{code}
+  @SuppressWarnings({""UnusedDeclaration""})
   public final static byte [] REGIONINFO_FILE_BYTES =
     Bytes.toBytes(REGIONINFO_FILE);
{code}

Remove it then?

Same here:

{code}
+  @SuppressWarnings({""UnusedDeclaration""})
   public long getRegionId() {
{code}

There are a bunch of them.

I'm about 1/5th done.  So far patch looks great.;;;","08/Apr/10 20:22;apurtell;When I run alpha3 on top of 0.20 head I see delete test failures in :

Testcase: testWeirdCacheBehaviour took 81.599 sec
Testcase: testFilterAcrossMutlipleRegions took 47.8 sec
Testcase: testSuperSimple took 18.15 sec
Testcase: testFilters took 15.781 sec
Testcase: testSimpleMissing took 16.448 sec
Testcase: testSingleRowMultipleFamily took 16.686 sec
Testcase: testNull took 21.443 sec
Testcase: testVersions took 18.169 sec
Testcase: testVersionLimits took 15.861 sec
Testcase: testDeletes took 15.837 sec
        Caused an ERROR
null
java.lang.NullPointerException
        at org.apache.hadoop.hbase.client.TestClient.testDeletes(TestClient.java
:1608)

Testcase: testJIRAs took 46.161 sec

Need more detail? 



      
;;;","08/Apr/10 21:21;stack;Why do this?

{code}
-      List<KeyValue> results = new ArrayList<KeyValue>();
-      store.get(get, null, results);
-      return new Result(results);
+      get.addFamily(family);
+      return get(get, null);
{code}

Is it because of this....up in HRegion:

{code}
+      // The reason why we set it up high is so that each HRegionScanner only
+      // has a single read point for all its sub-StoreScanners.
+      ReadWriteConsistencyControl.resetThreadReadPoint(rwcc);
{code}

This bit of the patch looks like its breaking the accumulation of qualifiers:

{code}
-            List<KeyValue> result = new ArrayList<KeyValue>(1);
-            Get g = new Get(kv.getRow());
-            g.setMaxVersions(count);
-            NavigableSet<byte []> qualifiers =
-              new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
-            qualifiers.add(qual);
-            get(store, g, qualifiers, result);
+            Get get = new Get(kv.getRow());
+            get.setMaxVersions(count);
+            get.addColumn(family, qual);
+
+            List<KeyValue> result = get(get);
+
{code}

Or is this no longer needed because we go back into Region.get rather than do a Store.get?

I don't like this if clause style:

+      if (w != null)
+      rwcc.completeMemstoreInsert(w);

I'd suggest you either wrap it in params or put it all on the one line. 

For example, undo changes like this I'd say:

{code}
-      if(lockid == null) releaseRowLock(lid);
+      if(lockid == null)
+        releaseRowLock(lid);

{code}

I think this needs a comment:

+    private int isScan;

or maybe where its assigned so its clear why it can't be a boolean though its named as though it were one... maybe change its name?

I don't get this:

{code}
+      // TODO call the proper GET API
       // Get the old value:
       Get get = new Get(row);
{code}

or this:

{code}
+    //noinspection SuspiciousMethodCalls
{code}

Why this change Ryan?

{code}
-class KeyValueSkipListSet implements NavigableSet<KeyValue>, Cloneable {
+class KeyValueSkipListSet implements NavigableSet<KeyValue> {
{code}

Its crazy how much code you've removed from MemStore around #195.

Ok, thats enough for now;;;","08/Apr/10 22:28;ryanobjc;this patch fixes the testDelete failure that people have been seeing. it should apply on both 0.20 and 0.20_pre_durability patches;;;","09/Apr/10 05:48;stack;Running unit tests, at least this one is failing for me:

    [junit] Test org.apache.hadoop.hbase.TestRegionRebalancing FAILED (timeout)

;;;","09/Apr/10 05:52;stack;I'm trying to run on cluster but its all hanging on me.  Its probably a config. messup on my part. Trying to figure it.

Meantime, this seems to run about 3-4 times slower than release against a standalone hbase:

$ ./bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation sequentialWrite 1;;;","09/Apr/10 06:08;apurtell;0.20_pre_durability branch plus HBASE-2248-rr-pre-durability2.txt passes rebalancing for me but fails TestIdxHBaseCluster consistently.

{noformat}
Testcase: testConcurrentReadWrite took 93.349 sec
	FAILED
nextCount=0, count=2, finalCount=2000
junit.framework.AssertionFailedError: nextCount=0, count=2, finalCount=2000
	at org.apache.hadoop.hbase.TestIdxHBaseCluster.testConcurrentReadWrite(TestIdxHBaseCluster.java:123)

Testcase: testHBaseCluster took 41.074 sec
{noformat}

Before the patch the indexed contrib tests pass on 0.20_pre_durability. ;;;","09/Apr/10 06:09;stack;In the above I'm trying to test branch of branch.   All core tests but above noted rebalancing passed.;;;","09/Apr/10 07:40;ryanobjc;unfortunately the way the branch and the branch-of-branch does things have diverged a lot.  specifically the locations of the update lock and the flush request. I rearranged things a bunch and have this new patch.  This might help with the PE slowness and other things as well.;;;","09/Apr/10 17:09;apurtell;On head of 0.20_pre_durability and patch rr-pre_durability3.txt I see this:

{noformat}
Testcase: testWritesWhileScanning took 0.155 sec
          FAILED
i=36 expected<1000> but was: <0>
junit.framework.AssertionFailedError: i=36 expected:<1000> but was: <0>
          at org.apache.hadoop.hbase.regionserver.TestHRegion.testWritesWhileScanning(TestHRegion.java:2014)
{noformat}

;;;","09/Apr/10 18:09;stack;I'm about 50% through HBASE-2248-rr-alpha3.txt:

ReadWriteConsistencyControl is missing a license (I like thename of this class and its nice and clean looking).

This could be final:

+    private long writeNumber;

This class doesn't have to be public: 

+  public static class WriteEntry {

Can you explain the below better please?

{code}
-    // The Get returns the latest value but then does not return the
-    // oldest, which was never deleted, ts[1]. 
-    
+
+    // It used to be due to the internal implementation of Get, that
+    // the Get() call would return ts[4] UNLIKE the Scan below. With
+    // the switch to using Scan for Get this is no longer the case.
     get = new Get(ROW);
     get.addFamily(FAMILIES[0]);
     get.setMaxVersions(Integer.MAX_VALUE);
     result = ht.get(get);
     assertNResult(result, ROW, FAMILIES[0], QUALIFIER, 
-        new long [] {ts[2], ts[3], ts[4]},
-        new byte[][] {VALUES[2], VALUES[3], VALUES[4]},
+        new long [] {ts[1], ts[2], ts[3]},
+        new byte[][] {VALUES[1], VALUES[2], VALUES[3]},
         0, 2);
{code}

Reading it, it would seem that we should be getting ts[4] because we just added it previous?

Why do this?

{code}
-    Scan scan = new Scan();
-    scan.setFilter(new RowFilter(CompareFilter.CompareOp.EQUAL,
-      new BinaryComparator(Bytes.toBytes(""row0""))));
+    Scan scan = new Scan(Bytes.toBytes(""row0""), Bytes.toBytes(""row1""));
+//    scan.setFilter(new RowFilter(CompareFilter.CompareOp.EQUAL,
+//      new BinaryComparator(Bytes.toBytes(""row0""))));
{code}

Otherwise, patch looks great.

This patch needs to be release noted describing how it changes how Get works.

;;;","10/Apr/10 19:59;stack;Testing I'm hanging when lots of concurrency at HLog.append at the synchronization on the updateLock inside the append method.  Its strange.  A bunch of threads are BLOCKED at this explicit line.  All but one say ""waiting to lock"".  A single thread is BLOCKED but it has 'locked' successfully, as though it should have moved on but it shows same location in stack trace (same line number) and there doesn't seem to be anything in the block that threads could contend over.  

Trying w/ different JVMs to see if I can get move info.;;;","10/Apr/10 21:33;stack;So, its not a lockup, rather, stuff is working but really, really slow.  It seems to be this patch because going back to a clean hadoop 0.20.2 and the current state of pre_durability branch, all runs fine again (until we do an actual deadlock, i.e. the known deadlock issue).  I'll spend more time trying to figure it but here is how it looks when you thread dump:

Most threads are 'WAITING', etc. and then a good few are like the below BLOCKED:

{code}
""IPC Server handler 36 on 60020"" daemon prio=10 tid=0x273e4400 nid=0x2888 waiting for monitor entry [0x257ad000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.hadoop.hbase.regionserver.HLog.append(HLog.java:646)
        - waiting to lock <0x3d5ec6a0> (a java.lang.Object)
{code}

Invariabley, there is one 'abnormal' BLOCKED that is the same as above -- excepting thread names etc. -- in all except 'waiting to lock' is instead 'locked' -- same line number and everything.

I'll keep digging.;;;","12/Apr/10 00:44;apurtell;I see the same results as Stack testing up on EC2, with 1.6.0_14 (64 bit). First thing I do is warm the cluster with PE --nomapred randomWrite N (usually N=10 or 15). Not a deadlock, but writes get really slow fast. Without the high write concurrency (N=1 or 3) it's better. ;;;","12/Apr/10 15:26;apurtell;We spend 76% of CPU time in ReadWriteConsistencyControl.completeMemstoreInsert. See attached 'profile.png'. Draining the write queue (for 42,175 puts?) explodes 42,166 calls to ReadWriteConsistencyControl.completeMemstoreInsert into 121,118,233 calls to AtomicLong.get and 121,143,574 calls to $WriteEntry.getWriteNumber, each arc represents 50% of the cumulative time there. See attached 'put_call_graph.png'.;;;","12/Apr/10 15:28;apurtell;From Ryan via email:
{quote}
There is a busy wait loop which attempts to ensure a write completes only when it is visible to others. With the log append as part of the ""transaction"" this is breaking down. The solution is to either forgo the busy wait loop (probably not a great idea) or restructure the code to do hlog appends first then memstore updates.

I'll talk to stack tomorrow and we can figure which route is better... Although I'd guess option #2
{quote};;;","12/Apr/10 18:23;stack;The release note for this issue needs to include note of how versioning and delete changes (I was going to say we should add new issue to add more extensive unit testing of our claim that versions will come out in right order now regardless of how they are put in, but we already have an issue for that -- the ACID spec tests issue).;;;","14/Apr/10 04:47;ryanobjc;ok here is a patch that addresses all the above issues:
- spin fixed by restructuring hlog append
- index test pass failure fixed
- test failures due to compaction
- all comments addressed

To accomplish the index hbase fix, I had to introduce a new notion of optional scanner creation atomicity along with pre-flush-commit work, so a sub-class can create an atomic section whereby some work is done (eg: switching out an index) and the flush commit (where the snapshot is removed and the hfile is introduced to open scanners) and this atomic section will be atomic relative to new scanner creation.  This was required to fix race conditions in indexed hbase, which also means that indexed hbase is not as fast as it can be, since it cannot create new scanners during this one critical phase of flush (which includes re-reading scanner blocks btw).

;;;","14/Apr/10 05:31;stack;This last patch is looking good.  The spin lock slowing writes is gone it looks like and most of the tests are passing.  Will report more in morning.  Will let tests run overnight.;;;","14/Apr/10 18:24;stack;All tests but this in 'indexed' pass:

    [junit] Test org.apache.hadoop.hbase.regionserver.TestIdxRegionIndexManager FAILED

My cluster test failed but for reasons other than would be attributable to this patch.  I did not deadlock.;;;","14/Apr/10 21:41;stack;I'm going to commit this.  All tests pass if I remove 'indexed'.  Patch looks good.  I have cluster issues but unrelated to this patch.  Logs for regionservers look good.;;;","14/Apr/10 21:57;stack;Thanks all who contributed to this issue: Todd, Dan, Yoram and in particular Ryan.;;;","15/Apr/10 01:39;ryanobjc;here is the updated version with fixes taken from the work on the 0.20_pre_durability but on plain old 0.20.;;;","15/Apr/10 21:49;ryanobjc;this removes row locks which are no longer necessary to ensure atomic reads;;;","15/Apr/10 22:10;streamy;ryan, can you explain more about removal of row locks?  seems like your patch just touches the simple get case that takes a row lock.  Are client-exposed row locks completely gone now?;;;","15/Apr/10 22:15;stack;@Jon No.. just the row lock around the Get.  The client-side row lock still in place.

I just tested it up on cluster and seems to run fine.  Going to commit.
;;;","19/Apr/10 14:46;ryanobjc;
   [[ Old comment, sent by email on Wed, 14 Apr 2010 14:43:49 -0700 ]]

the previous idx test break was a simple heap check break, so easy to
fix in the eventual destination. the core of the indexed stuff seemed
to work.


;;;","19/Apr/10 14:47;apurtell;
   [[ Old comment, sent by email on Wed, 14 Apr 2010 04:52:37 +0000 ]]

Thanks Ryan! Testing now.


;;;","19/Apr/10 17:00;stack;Hey Ryan, commit this to branch and TRUNK.  Testing over in branch-of-branch says its good.;;;",,,,,,,,,,,,,,,,,,,,
META gets inconsistent in a number of crash scenarios,HBASE-2244,12456875,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,Fixed,stack,kannanm,kannanm,20/Feb/10 02:08,12/Oct/12 06:15,14/Jul/23 06:06,04/Mar/10 18:24,,,,,,,,,,,,0.20.4,0.90.0,,,,,,0,,,"(Forking this issue off from HBASE-2235).

During load testing, in a number of failure scenarios (unexpected region server deaths) etc., we notice that META can get inconsistent. This primarily happens for regions which are in the process of being split. Manually running add_table.rb seems to fix the tables meta data just fine. 

But it would be good to do automatic cleansing (as part of META scanners work) and/or avoid these inconsistent states altogether.

For example, for a particular startkey, I see all these entries:

{code}
test1,1204765,1266569946560 column=info:regioninfo, timestamp=1266581302018, value=REGION => {NAME => 'test1,
                             1204765,1266569946560', STARTKEY => '1204765', ENDKEY => '1441091', ENCODED => 18
                             19368969, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'test1', FAMILIES =>
                              [{NAME => 'actions', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647'
                             , BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}
 test1,1204765,1266569946560 column=info:server, timestamp=1266570029133, value=10.129.68.212:60020
 test1,1204765,1266569946560 column=info:serverstartcode, timestamp=1266570029133, value=1266562597546
 test1,1204765,1266569946560 column=info:splitB, timestamp=1266581302018, value=\x00\x071441091\x00\x00\x00\x0
                             1\x26\xE6\x1F\xDF\x27\x1Btest1,1290703,1266581233447\x00\x071290703\x00\x00\x00\x
                             05\x05test1\x00\x00\x00\x00\x00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x
                             00\x00\x00\x07IS_META\x00\x00\x00\x05false\x00\x00\x00\x01\x07\x07actions\x00\x00
                             \x00\x07\x00\x00\x00\x0BBLOOMFILTER\x00\x00\x00\x05false\x00\x00\x00\x0BCOMPRESSI
                             ON\x00\x00\x00\x04NONE\x00\x00\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TT
                             L\x00\x00\x00\x0A2147483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00
                             \x00\x09IN_MEMORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04t
                             rueh\x0FQ\xCF
 test1,1204765,1266581233447 column=info:regioninfo, timestamp=1266609172177, value=REGION => {NAME => 'test1,
                             1204765,1266581233447', STARTKEY => '1204765', ENDKEY => '1290703', ENCODED => 13
                             73493090, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'test1', FAMILIES =>
                              [{NAME => 'actions', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647'
                             , BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}
 test1,1204765,1266581233447 column=info:server, timestamp=1266604768670, value=10.129.68.213:60020
 test1,1204765,1266581233447 column=info:serverstartcode, timestamp=1266604768670, value=1266562597511
 test1,1204765,1266581233447 column=info:splitA, timestamp=1266609172177, value=\x00\x071226169\x00\x00\x00\x0
                             1\x26\xE7\xCA,\x7D\x1Btest1,1204765,1266609171581\x00\x071204765\x00\x00\x00\x05\
                             x05test1\x00\x00\x00\x00\x00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x00\
                             x00\x00\x07IS_META\x00\x00\x00\x05false\x00\x00\x00\x01\x07\x07actions\x00\x00\x0
                             0\x07\x00\x00\x00\x0BBLOOMFILTER\x00\x00\x00\x05false\x00\x00\x00\x0BCOMPRESSION\
                             x00\x00\x00\x04NONE\x00\x00\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TTL\x
                             00\x00\x00\x0A2147483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00\x0
                             0\x09IN_MEMORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04true
                             \xB9\xBD\xFEO
 test1,1204765,1266581233447 column=info:splitB, timestamp=1266609172177, value=\x00\x071290703\x00\x00\x00\x0
                             1\x26\xE7\xCA,\x7D\x1Btest1,1226169,1266609171581\x00\x071226169\x00\x00\x00\x05\
                             x05test1\x00\x00\x00\x00\x00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x00\
                             x00\x00\x07IS_META\x00\x00\x00\x05false\x00\x00\x00\x01\x07\x07actions\x00\x00\x0
                             0\x07\x00\x00\x00\x0BBLOOMFILTER\x00\x00\x00\x05false\x00\x00\x00\x0BCOMPRESSION\
                             x00\x00\x00\x04NONE\x00\x00\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TTL\x
                             00\x00\x00\x0A2147483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00\x0
                             0\x09IN_MEMORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04true
                             \xE1\xDF\xF8p
 test1,1204765,1266609171581 column=info:regioninfo, timestamp=1266609172212, value=REGION => {NAME => 'test1,
                             1204765,1266609171581', STARTKEY => '1204765', ENDKEY => '1226169', ENCODED => 21
                             34878372, TABLE => {{NAME => 'test1', FAMILIES => [{NAME => 'actions', VERSIONS =
                             > '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMOR
                             Y => 'false', BLOCKCACHE => 'true'}]}}
{code}

",,clehene,hammer,karthik.ranga,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/10 07:19;stack;2244-v3.patch;https://issues.apache.org/jira/secure/attachment/12436826/2244-v3.patch","25/Feb/10 12:34;stack;2244-v5.patch;https://issues.apache.org/jira/secure/attachment/12436996/2244-v5.patch","02/Mar/10 21:05;stack;2244-v6.patch;https://issues.apache.org/jira/secure/attachment/12437642/2244-v6.patch","22/Feb/10 20:52;stack;2244.patch;https://issues.apache.org/jira/secure/attachment/12436626/2244.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26222,Reviewed,,,,Thu Mar 04 18:24:04 UTC 2010,,,,,,,,,,"0|i08sq7:",49254,,,,,,,,,,,,,,,,,,,,,"20/Feb/10 02:09;kannanm;Stack wrote <<<The report of split is not atomic; we are sending 3 separate puts. We don't have means of making a cross-row transaction out of this operation>>>.

Would keeping all region info for a table as columns under a single row key in .META. be such a crazy idea? With that you can do atomic mutations to a given table's region info. The cons would be that entire ""rowkey"" would be hosted on a single server... but for most tables that'd probably already be the case unless it has an exorbitant number of regions.

To which Stack responded with: <<<< For sure, lets do something in 0.20. I think metascanner when it runs can check an offlined region. If its not followed by two daughters - it has the necessary info as columns splitA and splitB - it can add them. Let me take a closer look. Will report back.

Having all in the one row is a bit radical. Gets and puts would each take out a row lock. I think this might slow down all .META. lookups. We also have a mechanism for getting the row closest to the asked for one. Its used for figuring out which region a row sits in. This would have to be recast if all was in the one row.>>>>

;;;","20/Feb/10 23:39;stack;I'm on it.  Patch soon.;;;","21/Feb/10 04:59;stack;Within the scope of this issue we should a couple of things for the 0.20 branch specifically.

First, add fixup to the metascanner for case where offlined parent but no daughters present because HRS crashed and didn't add daughters... or HRS carrying .META. crashed and we only recovered the parent offlining edit, but not the daughter additions.   This inconsistency is clean so easy to recognize.  Also, data needed to do the repair is in the offlined parent as columns splitA and splitB.

Here are a few notes.

+ On split, the HRS does three updates: 1. offline parent and add splitA and splitB columns that hold the HRegionInfo of daughter split regions, 2. add daughter A, and 3., add daughter B.  The updates are not done atomically.  Before we send the messages, the HRS has offlined (closed) the parent and created two new daughters.  The parent is already unavailable.
++ Reading code, there are issues to address in here.  If we crash after parent close, thats ok.  The parent will be assigned to a new HRS.  But subsequently, as the split goes forward, we do an open of the new daughter regions BEFORE we add them to the .META..  This seems like it could be avoided (speeding the split); only open once assigned in new location (Moving the location of where we do the split work should be all that is needed).  Also, if already a daughter region of same name in the FS, we'll fail the split rather than overwrite as it seems we should do (only reason for a pre-existing daughter is a split failed mid-way).  I can add a check of .META.  If daughter not there, its for sure a failed split.  Let me see if I can improve stuff in here in general for 0.20 as part of this patch.  I need to study  some more.
+ The HRS, after making updates in the .META., then sends a message to the master telling it about the split.  The master adds the new daughters to his assignment list and they are assigned out on next report-in by a cluster-member.  If this message is missed, the daughters are assigned the next time the metascanner runs.

In the .META. listing posted above, there are some interesting issues.  We still have a reference to a daughter, splitB, in the first offlined (row) region, yet the next row is a daughter that has been offlined itself.  There may be a race in here if we're splitting fast.  Let me check it out and see if a fix.

The other inconsistency is that there seems to be a row missing of the end, the splitB from test1,1204765,1266581233447.  Is that possible?;;;","21/Feb/10 16:17;kannanm;Stack: The splitB from test1,1204765,1266581233447, namely  ""test1,1226169,1266609171581"" was probably there. Not entirely sure. I should have cut-pasted more from the scan of .META. 

I just wanted to add that during this state, the client was receiving errors of the form:

{code}
org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 10.129.68.212:60020 for region\
 test1,1204765,1266581233447, row '1232762', but failed after 10 attempts. 
{code}

Note: key ""1232762"" would fall in the ""splitB"" range for the original test1,1204765,1266581233447.

(BTW, are the JIRA time zones always in GMT? Is there a way to configure it to local timezones?)

When I look at the same cluster's .META. now, the state for the related META regions is as follows:

{code}

 test1,1204765,1266616432091 column=info:regioninfo, timestamp=1266616432230, value=REGION => {NAME => 'test1,
                             1204765,1266616432091', STARTKEY => '1204765', ENDKEY => '1215466', ENCODED => 41
                             6976676, TABLE => {{NAME => 'test1', FAMILIES => [{NAME => 'actions', VERSIONS =>
                              '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY
                              => 'false', BLOCKCACHE => 'true'}]}}
 test1,1204765,1266616432091 column=info:server, timestamp=1266616433943, value=10.129.68.213:60020
 test1,1204765,1266616432091 column=info:serverstartcode, timestamp=1266616433943, value=1266562597511
 test1,1215466,1266616432091 column=info:regioninfo, timestamp=1266616432232, value=REGION => {NAME => 'test1,
                             1215466,1266616432091', STARTKEY => '1215466', ENDKEY => '1226169', ENCODED => 40
                             3995950, TABLE => {{NAME => 'test1', FAMILIES => [{NAME => 'actions', VERSIONS =>
                              '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY
                              => 'false', BLOCKCACHE => 'true'}]}}
 test1,1215466,1266616432091 column=info:server, timestamp=1266616434963, value=10.129.68.213:60020
 test1,1215466,1266616432091 column=info:serverstartcode, timestamp=1266616434963, value=1266562597511
 test1,1226169,1266609171581 column=info:regioninfo, timestamp=1266621116341, value=REGION => {NAME => 'test1,
                             1226169,1266609171581', STARTKEY => '1226169', ENDKEY => '1290703', ENCODED => 45
                             9318323, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'test1', FAMILIES =>
                             [{NAME => 'actions', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647',
                              BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}
 test1,1226169,1266609171581 column=info:server, timestamp=1266613546335, value=10.129.68.214:60020
 test1,1226169,1266609171581 column=info:serverstartcode, timestamp=1266613546335, value=1266562596451
 test1,1226169,1266609171581 column=info:splitA, timestamp=1266621116341, value=\x00\x0512790\x00\x00\x00\x01\
                             x26\xE8\x80l\xCD\x1Btest1,1226169,1266621115597\x00\x071226169\x00\x00\x00\x05\x0
                             5test1\x00\x00\x00\x00\x00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x00\x0
                             0\x00\x07IS_META\x00\x00\x00\x05false\x00\x00\x00\x01\x07\x07actions\x00\x00\x00\
                             x07\x00\x00\x00\x0BBLOOMFILTER\x00\x00\x00\x05false\x00\x00\x00\x0BCOMPRESSION\x0
                             0\x00\x00\x04NONE\x00\x00\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TTL\x00
                             \x00\x00\x0A2147483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00\x00\
                             x09IN_MEMORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04trueb\
                             xD0\x21\xBC
 test1,1226169,1266609171581 column=info:splitB, timestamp=1266621116341, value=\x00\x071290703\x00\x00\x00\x0
                             1\x26\xE8\x80l\xCD\x19test1,12790,1266621115597\x00\x0512790\x00\x00\x00\x05\x05t
                             est1\x00\x00\x00\x00\x00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x00\x00\
                             x00\x07IS_META\x00\x00\x00\x05false\x00\x00\x00\x01\x07\x07actions\x00\x00\x00\x0
                             7\x00\x00\x00\x0BBLOOMFILTER\x00\x00\x00\x05false\x00\x00\x00\x0BCOMPRESSION\x00\
                             x00\x00\x04NONE\x00\x00\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TTL\x00\x
                             00\x00\x0A2147483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00\x00\x0
                             9IN_MEMORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04true\x5B
                             \x7D\x1A1
 test1,1226169,1266621115597 column=info:regioninfo, timestamp=1266621116358, value=REGION => {NAME => 'test1,
                             1226169,1266621115597', STARTKEY => '1226169', ENDKEY => '12790', ENCODED => 1988
                             459280, TABLE => {{NAME => 'test1', FAMILIES => [{NAME => 'actions', VERSIONS =>
                             '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY
                             => 'false', BLOCKCACHE => 'true'}]}}
 test1,1226169,1266621115597 column=info:server, timestamp=1266768596714, value=10.129.68.212:60020
 test1,1226169,1266621115597 column=info:serverstartcode, timestamp=1266768596714, value=1266768408023
 test1,12790,1266621115597   column=info:regioninfo, timestamp=1266621116361, value=REGION => {NAME => 'test1,
                             12790,1266621115597', STARTKEY => '12790', ENDKEY => '1290703', ENCODED => 179081
                             1553, TABLE => {{NAME => 'test1', FAMILIES => [{NAME => 'actions', VERSIONS => '3
                             ', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY =>
                              'false', BLOCKCACHE => 'true'}]}}
 test1,12790,1266621115597   column=info:server, timestamp=1266768555610, value=10.129.68.214:60020
 test1,12790,1266621115597   column=info:serverstartcode, timestamp=1266768555610, value=1266768408015
{code}


Note: that both the daughters of  the parent region test1,1226169,1266609171581 have been installed. But the offlined parent row itself (for test1,1226169,1266609171581) is still present. Not sure if it is in these situations that the client starts seeing errors... but curious why the offline parent row hasn't been reaped yet from .META.

I repro'ed a similar problem yesterday. Might have more detailed logs and errors. Will share them shortly.


;;;","21/Feb/10 18:53;kannanm;Stack wrote: <<<< In the .META. listing posted above, there are some interesting issues. We still have a reference to a daughter, splitB, in the first offlined (row) region, yet the next row is a daughter that has been offlined itself. There may be a race in here if we're splitting fast. Let me check it out and see if a fix.>>>

Yes, I see several times that nested splits are happening, but the offlined parent row hasn't been reaped. But perhaps that in itself isn't an issue.  For example, corresponding to my first .META. snippet in this JIRA:

The split of test1,1204765,1266569946560 was announced @4:08:

{code}
2010-02-19 04:08:23,764 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_SPLIT: test1,1204765,1266569946560: Daughters; test1,1204765,1266581233447, test1,1290703,1266581233447 from test013.abcxyz.com,60020,1266562597546; 1 of 3
{code}

But reclaiming the offlined parent row from .META. took time. First we  detected one of the daughters no longer reference it @ about 11:53:
{code}
2010-02-19 11:53:46,673 DEBUG org.apache.hadoop.hbase.master.BaseScanner: test1,1204765,1266581233447/1373493090 no longer has references to test1,1204765,1266569946560
{code}

And the second daughter at about 14:01. It is only at this point we delete the offlined parent row:
{code}
2010-02-19 14:01:48,283 DEBUG org.apache.hadoop.hbase.master.BaseScanner: test1,1290703,1266581233447/580635726 no longer has references to test1,1204765,1266569946560
2010-02-19 14:01:48,299 INFO org.apache.hadoop.hbase.master.BaseScanner: Deleting region test1,1204765,1266569946560 (encoded=1819368969) because daughter splits no longer hold references
{code}

Naturally, given this wide window it is not uncommon to see rows corresponding to nested splits in .META. In most of these cases, eventually the .META. seems to fix itself. But it still seems odd to me that it takes so much time. 

During one of these situations, I saw the client get errors of the form:

10/02/19 09:09:37 INFO tests.MultiThreadedWriter: [22] Users = 1052116, mails = 1M, time = 10:10:53
org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 10.129.68.212:60020 for region\
 test1,1204765,1266581233447, row '1232785', but failed after 10 attempts.

and assumed that this was related to the .META. being in a wierd state (i.e. offlined parent not being deleted). But looking at the logs, these client errors happened during a smaller period (8:49 to 9:09). And were likely due to other load issues on the particular region server. I will post any findings from that RS'es logs shortly.




;;;","21/Feb/10 19:06;kannanm;(continued)

Looking at the logs for the concerned RS, yes, at 8:49 due to some DFS errors the region server shut itself down. Basically this confirms that META's state wasn't the cause for the client errors. It would still be good to understand why parent row deletion takes time and fix any known inconsistencies in this area. But the original snippet I posted of .META. in and of itself doesn't seem to indicate a problem.

But the comment Stack made about: <<<Also, if already a daughter region of same name in the FS, we'll fail the split rather than overwrite as it seems we should do (only reason for a pre-existing daughter is a split failed mid-way). I can add a check of .META. If daughter not there, its for sure a failed split. Let me see if I can improve stuff in here in general for 0.20 as part of this patch.>>>> seems would be worth addressing.
;;;","21/Feb/10 19:08;stack;The offlined parent is deleted only after daughters have let go of all references to the parent.  If you want a quick lesson on how references work just say.   So yes the offlined parent can stick around for a while.  It's the metascanner that does the parent removal. On the retried issue grep the region in the master logs to an idea of it's history.  What was happening during the time the client failed?;;;","21/Feb/10 19:10;jdcryans;bq. It would still be good to understand why parent row deletion takes time and fix any known inconsistencies in this area.

It's by design, we only clear the parent when the daughters no longer have references since we don't split the HFiles during the split process but after during compactions.;;;","22/Feb/10 00:48;kannanm;@stack, jdcryans:  The exact implication of the ""references"" -- i.e. the 
daughter regions still sharing HFile with the parent for a period of 
time -- hadn't sunk in.  But it makes sense now. I think we can close 
this issue as not a bug.

@stack: If you found other opportunities (as part of your recent code walk 
through) for making the general handling of splits more bullet proof,
perhaps we should file that as a separate issue so that it doesn't get
muddled with this non-issue.

@ What was happening during the time the client failed? 

In this particular case the RS was shutting down. The DNs were getting overwhelmed in my small cluster test case, and not able to keep up. It started with a lot of:

{code}
2010-02-19 08:48:57,462 WARN org.apache.hadoop.hdfs.DFSClient: NotReplicatedYetException sleeping /hbase-kannan1/test1/580635726/actions/133921297096924993\
7 retries left 1
{code}

Followed by:
{code}
2010-02-19 08:49:07,102 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_9144926768183088527_186431 bad datanode[0] nodes == null
2010-02-19 08:49:07,102 WARN org.apache.hadoop.hdfs.DFSClient: Could not get block locations. Source file ""/hbase-kannan1/test1/580635726/actions/133921297\
0969249937"" - Aborting...
2010-02-19 08:49:07,117 FATAL org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Replay of hlog required. Forcing server shutdown
{code}



;;;","22/Feb/10 20:52;stack;Patch that skips opening of new daughter regions post creation.  Its not necessary.  Should make splits run a little faster and take a little load off hdfs/nn.;;;","22/Feb/10 21:12;tlipcon;Kannan: have you applied HDFS-630 on your hadoop test cluster? There's a patch on that JIRA that's compatible with 0.20.
I also recommend HDFS-793, HDFS-101, and HDFS-927 which are in 0.20.2's release candidate but perhaps not in your version.;;;","22/Feb/10 21:14;stack;I took a look at our split code.  Its not as bad as I thought in that the fail I mention above should never happen (on open of a region, we clean up any old split working dirs so there should never be a clash creating daughter dirs -- if there is then something very seriously wrong is going on and we should fail).

@Kannan Do you think hdfs-826 would have helped wih ""Error Recovery for block blk_9144926768183088527_186431 bad datanode[0] nodes == null""?  Looks like we ran out of replicas?  Is that your take?

I'd like to keep this issue open for the fixup I'd like to add to .META. that will defend against case where we get the parent offlining edit but somehow the daughter additions don't go in.  Let me post a fuller patch soon.;;;","24/Feb/10 07:19;stack;This version adds to the above fixup in BaseScanner.  Currently BaseScanner, checks offlined parents.  If no references by daughters, then we let the parent go.   This patch adds a bit of code in here to check -- if there are references stil -- that the daughters of the split are in the metatable.  If not, it adds them and marks the parent row so we don't do expensive check each time through metascan.

Need to add a test and run on cluster yet.  Just posting what I have so far.;;;","24/Feb/10 08:18;kannanm;@Todd/@Stack: So my previous run didn't have HDFS-927. But we have now applied the fix for that. We are also in the process of applying/testing a fix for HBASE-2234 (which in turn depends on HDFS-836). Will look into your other recommened HDFS patches as well.

Stack wrote: <<< Do you think hdfs-826 would have helped wih ""Error Recovery for block blk_9144926768183088527_186431 bad datanode[0] nodes == null""? Looks like we ran out of replicas? Is that your take?>>>

Analyzed the various logs with Dhruba today.  Turns out this was happening because the region server started getting NotReplicatedYetException from HDFS for a particular block. When the client (region server) requests the HDFS namenode for a new block, the namenode first checks to see if datanodes for the penultimate block have sent in their ""blocks received"" confirmation. If not, the NameNode rejects the new block request with a NotReplicatedYetException.  The client retries a configurable number of times... (I think the default is 4), and in our case  eventually gave up after about 10 seconds.  The data nodes in question took about 30 seconds to send in their block received confirmation for the penultimate block. We don't yet have a good theory on why the data nodes were running slow. We have put in some more instrumentation on the HDFS side that might give us some clues if this happens again.


;;;","25/Feb/10 12:34;stack;Can't add unit test for this explicit case.  The actors are the BaseScanner up in Master and a failing split over in a RegionServer.  The split region needs to be completely coherent; that is, after split it has to have references in place else fixup won't cut in (that there are references and there is no entry for a daughter is how we identify this pathology).  Then we need to have regionserver fail at an explicit location in a cluster test, after offlining parent but but somewhere in the
middle of adding the daughters to meta.  Our current testing vocabulary doesn't allow us do this kind of detailed fail (Needs to be fixed but not as part of this patch).

So, I can see the repair cutting in if I doctor the RegionServer with the following patch to CompactSplitThread:

{code}
Index: src/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java       (revision 915869)
+++ src/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java       (working copy)
@@ -199,6 +199,13 @@
         Writables.getBytes(newRegions[1].getRegionInfo()));
     t.put(put);
     
+    // If we crash here, then the daughters will not be added and we'll have
+    // and offlined parent but no daughters to take up the slack.  hbase-2244
+    // adds fixup to the metascanners.
+
+    // REMOVE
+    if (true) throw new IOException(""Fail to add other regions and to send split to master"");
+    
     // Add new regions to META
     for (int i = 0; i < newRegions.length; i++) {
       put = new Put(newRegions[i].getRegionName());

{code}

Notice the throw IOE after parent has been offlined.  This makes it so meta does not get daughter entries and the master does not get split message.

In TestForceSplit, you'll see the following:

{code}
2010-02-24 19:45:16,685 ERROR [RegionServer:0.compactor] regionserver.CompactSplitThread(104): Compaction/Split failed for region test,,1267069500020
java.io.IOException: Fail to add other regions and to send split to master
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread.split(CompactSplitThread.java:207)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:95)
2010-02-24 19:45:17,101 DEBUG [WaitOnSplit] client.TestForceSplit$WaitOnSplit(72): Cycle waiting on split
2010-02-24 19:45:18,105 DEBUG [WaitOnSplit] client.TestForceSplit$WaitOnSplit(72): Cycle waiting on split
2010-02-24 19:45:19,111 DEBUG [WaitOnSplit] client.TestForceSplit$WaitOnSplit(72): Cycle waiting on split
2010-02-24 19:45:20,117 DEBUG [WaitOnSplit] client.TestForceSplit$WaitOnSplit(72): Cycle waiting on split
2010-02-24 19:45:21,122 DEBUG [WaitOnSplit] client.TestForceSplit$WaitOnSplit(72): Cycle waiting on split
2010-02-24 19:45:22,127 DEBUG [WaitOnSplit] client.TestForceSplit$WaitOnSplit(72): Cycle waiting on split
2010-02-24 19:45:23,133 DEBUG [WaitOnSplit] client.TestForceSplit$WaitOnSplit(72): Cycle waiting on split
2010-02-24 19:45:24,140 DEBUG [WaitOnSplit] client.TestForceSplit$WaitOnSplit(72): Cycle waiting on split
2010-02-24 19:45:24,655 INFO  [Thread-103] master.ServerManager$ServerMonitor(129): 1 region servers, 0 dead, average load 2.0
2010-02-24 19:45:24,920 INFO  [RegionManager.metaScanner] master.BaseScanner(163): RegionManager.metaScanner scanning meta region {server: 127.0.0.1:59090, regionname: .META.,,1, startKey: <>}
2010-02-24 19:45:24,921 INFO  [RegionManager.rootScanner] master.BaseScanner(163): RegionManager.rootScanner scanning meta region {server: 127.0.0.1:59090, regionname: -ROOT-,,0, startKey: <>}
2010-02-24 19:45:24,941 INFO  [RegionManager.rootScanner] master.BaseScanner(242): RegionManager.rootScanner scan of 1 row(s) of meta region {server: 127.0.0.1:59090, regionname: -ROOT-,,0, startKey: <>} complete
2010-02-24 19:45:24,942 WARN  [RegionManager.metaScanner] master.BaseScanner(379): Fixup broke split: Add missing split daughter to meta, daughter=REGION => {NAME => 'test,,1267069516539', STARTKEY => '', ENDKEY => 'lqg', ENCODED => 904299994, TABLE => {{NAME => 'test', FAMILIES => [{NAME => 'a', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}, parent=REGION => {NAME => 'test,,1267069500020', STARTKEY => '', ENDKEY => '', ENCODED => 970247455, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'test', FAMILIES => [{NAME => 'a', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}
2010-02-24 19:45:24,947 WARN  [RegionManager.metaScanner] master.BaseScanner(379): Fixup broke split: Add missing split daughter to meta, daughter=REGION => {NAME => 'test,lqg,1267069516539', STARTKEY => 'lqg', ENDKEY => '', ENCODED => 1813617937, TABLE => {{NAME => 'test', FAMILIES => [{NAME => 'a', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}, parent=REGION => {NAME => 'test,,1267069500020', STARTKEY => '', ENDKEY => '', ENCODED => 970247455, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'test', FAMILIES => [{NAME => 'a', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}
2010-02-24 19:45:24,948 INFO  [RegionManager.metaScanner] master.BaseScanner(242): RegionManager.metaScanner scan of 1 row(s) of meta region {server: 127.0.0.1:59090, regionname: .META.,,1, startKey: <>} complete
2010-02-24 19:45:24,949 INFO  [RegionManager.metaScanner] master.MetaScanner(132): All 1 .META. region(s) scanned
2010-02-24 19:45:25,145 DEBUG [WaitOnSplit] client.HConnectionManager$TableServers(780): Cache hit for row <> in tableName .META.: location server 127.0.0.1:59090, location region name .META.,,1
2010-02-24 19:45:25,149 DEBUG [WaitOnSplit] client.HTable$ClientScanner(1832): Creating scanner over .META. starting at key ''
2010-02-24 19:45:25,151 DEBUG [WaitOnSplit] client.HTable$ClientScanner(1928): Advancing internal scanner to startKey at ''
2010-02-24 19:45:25,151 DEBUG [WaitOnSplit] client.HConnectionManager$TableServers(780): Cache hit for row <> in tableName .META.: location server 127.0.0.1:59090, location region name .META.,,1
2010-02-24 19:45:25,157 INFO  [WaitOnSplit] client.TestForceSplit$WaitOnSplit(79): keyvalues={test,,1267069500020/info:regioninfo/1267069516682/Put/vlen=254, test,,1267069500020/info:server/1267069501165/Put/vlen=15, test,,1267069500020/info:serverstartcode/1267069501165/Put/vlen=8, test,,1267069500020/info:splitA/1267069516682/Put/vlen=257, test,,1267069500020/info:splitA_checked/1267069524944/Put/vlen=1, test,,1267069500020/info:splitB/1267069516682/Put/vlen=260, test,,1267069500020/info:splitB_checked/1267069524948/Put/vlen=1}
2010-02-24 19:45:25,158 INFO  [WaitOnSplit] client.TestForceSplit$WaitOnSplit(79): keyvalues={test,,1267069516539/info:regioninfo/1267069524943/Put/vlen=257}
2010-02-24 19:45:25,158 INFO  [WaitOnSplit] client.TestForceSplit$WaitOnSplit(79): keyvalues={test,lqg,1267069516539/info:regioninfo/1267069524948/Put/vlen=260}
2010-02-24 19:45:25,159 DEBUG [WaitOnSplit] client.HTable$ClientScanner(1915): Finished with scanning at REGION => {NAME => '.META.,,1', STARTKEY => '', ENDKEY => '', ENCODED => 1028785192, TABLE => {{NAME => '.META.', IS_META => 'true', MEMSTORE_FLUSHSIZE => '16384', FAMILIES => [{NAME => 'historian', VERSIONS => '2147483647', COMPRESSION => 'NONE', TTL => '604800', BLOCKSIZE => '8192', IN_MEMORY => 'false', BLOCKCACHE => 'false'}, {NAME => 'info', VERSIONS => '10', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '8192', IN_MEMORY => 'false', BLOCKCACHE => 'false'}]}}
2010-02-24 19:45:25,160 ERROR [IPC Server handler 4 on 59090] regionserver.HRegionServer(862): 
org.apache.hadoop.hbase.NotServingRegionException: test,,1267069500020 is closing=true or closed=true
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:1787)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1907)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
2010-02-24 19:45:25,175 DEBUG [main] client.HTable$ClientScanner(1928): Advancing internal scanner to startKey at 'aaa'
2010-02-24 19:45:25,177 ERROR [IPC Server handler 2 on 59090] regionserver.HRegionServer(864): Failed openScanner
org.apache.hadoop.hbase.NotServingRegionException: test,,1267069500020
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2279)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1858)
	at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
2010-02-24 19:45:30,186 DEBUG [main] client.HConnectionManager$TableServers(857): Removed test,,1267069500020 for tableName=test from cache because of aaa
2010-02-24 19:45:30,188 DEBUG [main] client.HConnectionManager$TableServers(734): locateRegionInMeta attempt 0 of 3 failed; retrying after sleep of 5000 because: No server address listed in .META. for region test,,1267069516539
2010-02-24 19:45:34,656 INFO  [Thread-103] master.ServerManager$ServerMonitor(129): 1 region servers, 0 dead, average load 2.0
2010-02-24 19:45:34,920 INFO  [RegionManager.rootScanner] master.BaseScanner(163): RegionManager.rootScanner scanning meta region {server: 127.0.0.1:59090, regionname: -ROOT-,,0, startKey: <>}
2010-02-24 19:45:34,920 INFO  [RegionManager.metaScanner] master.BaseScanner(163): RegionManager.metaScanner scanning meta region {server: 127.0.0.1:59090, regionname: .META.,,1, startKey: <>}
2010-02-24 19:45:34,932 DEBUG [RegionManager.metaScanner] master.BaseScanner(572): Current assignment of test,,1267069516539 is not valid;  serverAddress=, startCode=0 unknown.
2010-02-24 19:45:34,934 DEBUG [RegionManager.metaScanner] master.BaseScanner(572): Current assignment of test,lqg,1267069516539 is not valid;  serverAddress=, startCode=0 unknown.
2010-02-24 19:45:34,940 INFO  [RegionManager.rootScanner] master.BaseScanner(242): RegionManager.rootScanner scan of 1 row(s) of meta region {server: 127.0.0.1:59090, regionname: -ROOT-,,0, startKey: <>} complete
2010-02-24 19:45:34,941 INFO  [RegionManager.metaScanner] master.BaseScanner(242): RegionManager.metaScanner scan of 3 row(s) of meta region {server: 127.0.0.1:59090, regionname: .META.,,1, startKey: <>} complete
2010-02-24 19:45:34,941 INFO  [RegionManager.metaScanner] master.MetaScanner(132): All 1 .META. region(s) scanned
2010-02-24 19:45:35,189 DEBUG [main] client.HConnectionManager$TableServers(734): locateRegionInMeta attempt 1 of 3 failed; retrying after sleep of 5000 because: No server address listed in .META. for region test,,1267069516539
2010-02-24 19:45:35,315 INFO  [IPC Server handler 4 on 60000] master.RegionManager(337): Assigning region test,,1267069516539 to localhost,59090,1267069494948
2010-02-24 19:45:35,315 INFO  [IPC Server handler 4 on 60000] master.RegionManager(337): Assigning region test,lqg,1267069516539 to localhost,59090,1267069494948
2010-02-24 19:45:35,316 INFO  [RegionServer:0] regionserver.HRegionServer(499): MSG_REGION_OPEN: test,,1267069516539
2010-02-24 19:45:35,317 INFO  [RegionServer:0] regionserver.HRegionServer(499): MSG_REGION_OPEN: test,lqg,1267069516539
{code}

2010-02-24 19:26:50,203 and 2010-02-24 19:26:50,209 are the new stuff.  See how subsequently the daughter rows added by the fixup are assigned and the verification scan -- the test is to prove scans can ride across splits -- completes and the unit tests fails.

I'm going to commit the patch unless objection.

This patch adds some load on .META.: two new Gets after every split.  Shouldn't be too bad.  Splits are rare enough.;;;","25/Feb/10 12:35;stack;Marking patch available; review would be ok.  It ain't too big.  Mostly its comments.;;;","25/Feb/10 12:39;stack;@Kannan I've seen NotReplicatedYetException.  Adding instrumentation to figure why datanode is slow reporting sounds excellent.;;;","25/Feb/10 18:38;jdcryans;Patch itself looks good. Could Kannan try to destroy it before you commit?;;;","25/Feb/10 20:53;kannanm;@Stack, JD: I haven't had a chance to review the patch. I'll do that as well as try it out on my test setup. So perhaps just hold off on committing it for a few days...;;;","02/Mar/10 19:58;kannanm;Stack: The changes look very good. Like all the helpful comments as well.

Some small comments on the patch:

Previously, the first time hasReferences() finds that there are no references
from a daughter region, it would delete the corresponding ""info:splitA or B"" row
from the parent. On subsequent calls, hasReferences would bail early because ""split""
would be null.

Now, the deletion of the daughter column (info:splitA or B) in the parent row
happens in checkDaughter() (when it calls removeDaughterFromParent()). hasReferences()
will keep returning false for a daughter that no longer has references, and it seems
we'll unnecessarily be calling removeDaughterFromParent() on more than one
occasion. Delete of a non-existent column should be a no-op-- so this is probably
not a major issue, but it would be good to avoid introducing these wasteful deletes.

Minor/Cosmetic:
~~~~~~~~~~~~~~~

* HRegion.java:createDaughterDirInSplitDir()

Doesn't actually create the daughter dir, but only constructs a path
to the daughter dir, correct? Perhaps the function could be name
getSplitDirForDaughter() or something.

* typo: BaseScanner.java
 * the filesystem, then a daughters was not added __o__ .META. -- must have been

* typo: HRegion.java
      // __Crate__ a region instance and then move the splits into place under;;;","02/Mar/10 21:05;stack;@Kannan Sweet! Thanks for great review.  Attached patch has all your changes.  Your first observation is particularly egregious.  Thanks for pointing it out.  I'm running tests to make sure it still works as its supposed.... will commit if all pass.;;;","03/Mar/10 02:23;kannanm;There is one minor issue still with the patch that:

in hasReferences(), this condition:

      if (ps != null && ps.length > 0) {
        result = true;
        break;
      }

should be changed to:

      if (ps != null && ps.length > 0) {
        return true;
      }

We don't want to fall into removeDaughterFromParent() under the above. The older version of the code had a:

   if (result) {
      return result;
    }
    
to handle the same.

;;;","03/Mar/10 02:40;kannanm;Also,

{code}
    if (!verifyDaughterRowPresent(rowContent, qualifier, srvr, metaRegionName,
        hri, parent)) {
      // If we got here, we added a daughter region to metatable. Update
      // parent row that daughter has been verified present so we don't check
      // for it by doing a get each time through here.
      addDaughterRowChecked(metaRegionName, srvr, parent.getRegionName(), hri,
        qualifier);
    }
{code}

the comment above isn't strictly correct. We'll do a addDaughterRowChecked() even if the daughter region was already present in the meta table, correct?

Similarly, the description for the ""return"" param of verifyDaughterRowPresent() needs to be tweaked accordingly. The function returns TRUE if we have already checked this daughter and FALSE otherwise.


;;;","03/Mar/10 04:00;stack;@Kannan Really appreciate the review.  What I committed actually does as you suggest up in ""03/Mar/10 02:23 AM"" (My extra testing unveiled the condition you found by reading code).  I did what old code did to avoid return in middle of a method.  On the incorrect comment.  Fixing now.  Thanks.;;;","03/Mar/10 04:51;stack;@Kannan I committed below to address your last comment.  Let me know if insufficient.

{code}
Index: core/src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java
===================================================================
--- core/src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java  (revision 918324)
+++ core/src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java  (working copy)
@@ -342,9 +342,8 @@
     if (!references) return references;
     if (!verifyDaughterRowPresent(rowContent, qualifier, srvr, metaRegionName,
         hri, parent)) {
-      // If we got here, we added a daughter region to metatable. Update
-      // parent row that daughter has been verified present so we don't check
-      // for it by doing a get each time through here.
+      // If we got here, then the parent row does not yet have the
+      // ""daughter row verified present"" marker present. Add it.
       addDaughterRowChecked(metaRegionName, srvr, parent.getRegionName(), hri,
         qualifier);
     }
@@ -360,8 +359,8 @@
    * @param metaRegionName
    * @param daughterHRI
    * @throws IOException
-   * @return True, if the daughter row is present in meta.  If false, this
-   * method just added it to meta.
+   * @return True, if parent row has marker for ""daughter row verified present""
+   * else, false (and will do fixup adding daughter if daughter not present).
    */
   private boolean verifyDaughterRowPresent(final Result rowContent,
       final byte [] daughter, final HRegionInterface srvr,
{code};;;","03/Mar/10 05:48;kannanm;Looks good to me. Thanks. ;;;","03/Mar/10 06:02;apurtell;This has been committed, and I updated CHANGES.txt to reflect that.;;;","04/Mar/10 18:24;stack;Applied branch and trunk.  Resolving.  Please reopen if you think this issue not done (Kannan suggested it a non-issue a good while back)

That said, there is still good stuff/discussion up in this issue.  Lets open new issues for the still unresolve for stuff like figuring why notreplicatedyet is happening, etc.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change balancer sloppyness from 0.1 to 0.3,HBASE-2241,12456830,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,19/Feb/10 19:53,12/Oct/12 06:14,14/Jul/23 06:06,19/Feb/10 20:06,,,,,,,,,,,,0.20.4,0.90.0,,,,,,0,,,"This is a quick workaround until we do a better balancer.

Taking a region off line when cluster is under load is bad news.  Latency goes up as we wait on regions to come up in new locations.

The load balancer should only cut in if the cluster is way out of alignment.

I'd argue that 10% deviance from the avg. is not good enough reason moving regions around when cluster is under load.

Balancer already has a knack for cutting in at most inopportune moments: during cluster startup, when new node is added to a small cluster, or moving a region just after its been opened on a node.  We'll need to do a better balancer but meantime lets just allow that region loading can be sloppier, say 20% or 30% off the average before balancer cuts in.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/10 19:56;stack;2241.patch;https://issues.apache.org/jira/secure/attachment/12436362/2241.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26220,,,,,Fri Feb 19 20:06:24 UTC 2010,,,,,,,,,,"0|i08szr:",49297,,,,,,,,,,,,,,,,,,,,,"19/Feb/10 19:56;stack;Changes slop from .1 to .3.;;;","19/Feb/10 20:06;stack;Committed branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Region close needs to be  fast; e.g. if compacting, abandon it",HBASE-2228,12456291,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,nspiegelberg,stack,stack,15/Feb/10 05:32,20/Nov/15 12:42,14/Jul/23 06:06,05/Oct/10 23:56,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Over last week or so i've seen slow closes cause regions be off line for a good amount of time. Just now, i saw a big compaction go into effect because ""too many store files"".  This compaction took nearly two minutes on loaded server.  But during this time flushing was held up.  When the order to close came in (overloaded), we started the close -- so incoming writes were rejected -- but then we had to wait on the compaction to finish before the close went ahead... though incoming clients by now are being turned away.  Eventually the compaction completed and then the held-up flush was allowed run..... 91M in about 5 seconds.  Only now was the close allowed complete and the region deployed elsewhere.

Another time I saw the flush take a good long time because hdfs was running slow.  Probably not much we can do about this one but we should at least look into the above.  Interrupt an ongoing compaction and abandon it... or else keep region open while the compaction is going on and only when compete, then start up the close (Would require new state of CLOSING keeping up a progressable with the master).",,hammer,larsfrancke,streamy,tlipcon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HBASE-3043,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26213,,,,,Fri Nov 20 12:42:55 UTC 2015,,,,,,,,,,"0|i0hguv:",99993,,,,,,,,,,,,,,,,,,,,,"15/Feb/10 09:54;ryanobjc;do a close flush in it's own thread and not wait?;;;","05/Oct/10 19:32;nspiegelberg;With the addition of HBASE-3043, this should be a much easier fix;;;","05/Oct/10 23:55;nspiegelberg;After further investigation, this is indeed fixed due to HBASE-3043.  CloseRegionHandler.process() merely sets the ZK status and calls HRegion.close().  This section of code is the same section that handles compaction abort during system stops.  Might be nice to add a unit test a generic region close to make sure that we don't have a regression, but the functionality is in place.;;;","06/Oct/10 03:17;stack;Sorry N, what should the unit test do?  ;;;","06/Oct/10 21:37;nspiegelberg;@Stack

Currently: there is a unit test in TestCompaction that tests whether setting writesEnabled after HRegion.compact() will cause the compaction to abort.
Idea: Make a higher-level test that mocks the CompactSplitThread, initiates a compaction, calls HRegion.close() in thead #2, and verifies that thread #2 doesn't take long to exit.

Again, I would put this as a trivial test, because most of the code path is tested by what's currently there.  However, the 2nd test idea would be more end-to-end.;;;","20/Nov/15 12:42;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[IHBASE] Idx Expression functionality is incompatible with SingleColumnValueFilter  ,HBASE-2227,12456277,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,ykulbak,ykulbak,14/Feb/10 22:31,12/Oct/12 06:14,14/Jul/23 06:06,15/Feb/10 21:00,0.20.4,,,,,,,,,,,0.20.4,,,,,,,0,,,"Idx comparison expressions should have 1:1 mapping to SingleColumnValueFilter.
Without this mapping users can't fully use the index expressions to optimize their scan performance.
Currently there are two main features lacking:
- Support for a != (NEQ) operator 
- Support for the 'filterIfMissing' modifier  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/10 21:01;stack;HBASE-2227.patch;https://issues.apache.org/jira/secure/attachment/12435907/HBASE-2227.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26212,,,,,Mon Feb 15 21:01:47 UTC 2010,,,,,,,,,,"0|i08sz3:",49294,,,,,,,,,,,,,,,,,,,,,"15/Feb/10 21:00;stack;Committed to branch;;;","15/Feb/10 21:01;stack;Here is what was committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HQuorumPeerTest doesnt run because it doesnt start with the word Test,HBASE-2226,12456256,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,14/Feb/10 06:44,20/Nov/15 12:44,14/Jul/23 06:06,14/Feb/10 06:59,,,,,,,,,,,,0.90.0,,,,,,,0,,,"pretty simple, test runner looks for classes starting with the word ""Test"".",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26211,,,,,Fri Nov 20 12:44:02 UTC 2015,,,,,,,,,,"0|i0hgun:",99992,,,,,,,,,,,,,,,,,,,,,"14/Feb/10 06:45;ryanobjc;good news is that this is an outlier, no others that i can see.;;;","14/Feb/10 06:55;kaykay.unique;Would n't it be simpler to rename the test case to fit the junit include pattern ? ;;;","14/Feb/10 06:59;ryanobjc;committed this one to trunk;;;","20/Nov/15 12:44;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken build: TestGetRowVersions.testGetRowMultipleVersions ,HBASE-2224,12456211,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,13/Feb/10 03:52,20/Nov/15 13:01,14/Jul/23 06:06,13/Feb/10 04:06,,,,,,,,,,,,0.90.0,,,,,,,0,,,Hudson is broke.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26210,,,,,Fri Nov 20 13:01:40 UTC 2015,,,,,,,,,,"0|i0hgu7:",99990,,,,,,,,,,,,,,,,,,,,,"13/Feb/10 04:04;stack;So, things happen quick now.  On restart of the cluster in this test, can be zero servers when we check so do following:

{code}
Index: src/java/org/apache/hadoop/hbase/client/HTable.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/HTable.java (revision 909707)
+++ src/java/org/apache/hadoop/hbase/client/HTable.java (working copy)
@@ -141,6 +141,10 @@
     this.maxKeyValueSize = conf.getInt(""hbase.client.keyvalue.maxsize"", -1);
 
     int nrHRS = getCurrentNrHRS();
+    if (nrHRS == 0) {
+      // No servers running -- set default of 10 threads.
+      nrHRS = 10;
+    }
     int nrThreads = conf.getInt(""hbase.htable.threads.max"", nrHRS);
 
     // Unfortunately Executors.newCachedThreadPool does not allow us to
{code}
;;;","13/Feb/10 04:06;stack;Committed TRUNK;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stop using code mapping for method names in the RPC,HBASE-2219,12456125,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,ryanobjc,ryanobjc,ryanobjc,12/Feb/10 09:55,20/Nov/15 13:02,14/Jul/23 06:06,20/Mar/10 07:43,0.20.3,,,,,,,,,,,0.90.0,,IPC/RPC,,,,,0,,,"since we use a sorted mapping of method names -> codes and send that over the wire, even trivial changes, such as adding a new call, become wire-incompatible.  This means many features which could easily have gone into a minor update must wait for a major update.  Eg: 2066, 1845, etc.

This will increase on-wire overhead, but the compatibility is worth it I think.",,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/10 04:06;ryanobjc;HBASE-2219-2.patch;https://issues.apache.org/jira/secure/attachment/12435801/HBASE-2219-2.patch","12/Feb/10 09:56;ryanobjc;HBASE-2219.patch;https://issues.apache.org/jira/secure/attachment/12435681/HBASE-2219.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26209,Incompatible change,Reviewed,,,Fri Nov 20 13:02:08 UTC 2015,,,,,,,,,,"0|i0hgtj:",99987,,,,,,,,,,,,,,,,,,,,,"12/Feb/10 09:56;ryanobjc;one test passes, so things dont look horribly broken. I'll run the rest later, but I just wanted to get this up here for discussion...;;;","14/Feb/10 04:06;ryanobjc;here is a patch that actually works and all tests pass.;;;","14/Feb/10 08:55;ryanobjc;this is a pretty serious change, while it makes all previous RPCs incompatible, it also allows for minor changes to the interface without altering the version number.  

There may be a performance hit, we should consider running some microbenches as well as a larger test.;;;","15/Feb/10 08:59;ryanobjc;I did a PE (with no data, thus testing mostly RPC), and I came up with these:

with 2199:

10/02/15 00:41:08 INFO hbase.PerformanceEvaluation: Finished randomRead in 453339ms at offset 0 for 2560000 rows

without:

10/02/15 00:56:44 INFO hbase.PerformanceEvaluation: Finished randomRead in 452285ms at offset 0 for 2560000 rows



Not much of a difference here.;;;","15/Feb/10 09:11;ryanobjc;here is another test:

without:
10/02/15 01:06:24 INFO hbase.PerformanceEvaluation: Finished randomRead in 11106ms at offset 0 for 40000 rows

2199:
10/02/15 01:09:52 INFO hbase.PerformanceEvaluation: Finished randomRead in 12596ms at offset 0 for 40000 rows


This is 13% slower, which isnt really that great.  Maybe some profiling will help.;;;","15/Feb/10 09:14;ryanobjc;rerunning, might be facing system variance, eg: chrome messing with the run;;;","15/Feb/10 09:22;ryanobjc;ok, doing multiple runs shows quite a bit less variance, and in some cases with this patch is faster (?).

2219:
10/02/15 01:12:34 INFO hbase.PerformanceEvaluation: Finished randomRead in 23423ms at offset 0 for 100000 rows

10/02/15 01:13:20 INFO hbase.PerformanceEvaluation: Finished randomRead in 22861ms at offset 0 for 100000 rows

10/02/15 01:14:58 INFO hbase.PerformanceEvaluation: Finished randomRead in 22838ms at offset 0 for 100000 rows

10/02/15 01:15:39 INFO hbase.PerformanceEvaluation: Finished randomRead in 22851ms at offset 0 for 100000 rows

without:
10/02/15 01:19:27 INFO hbase.PerformanceEvaluation: Finished randomRead in 25250ms at offset 0 for 100000 rows

10/02/15 01:20:03 INFO hbase.PerformanceEvaluation: Finished randomRead in 23326ms at offset 0 for 100000 rows

10/02/15 01:20:35 INFO hbase.PerformanceEvaluation: Finished randomRead in 23010ms at offset 0 for 100000 rows

10/02/15 01:21:08 INFO hbase.PerformanceEvaluation: Finished randomRead in 23007ms at offset 0 for 100000 rows



;;;","15/Feb/10 18:26;stack;I think the difference is of no consequence; its small. Doing with no content was a good idea.  When we going to add this change that breaks RPC?;;;","15/Feb/10 18:45;jdcryans;WRT to the patch being sometimes faster, it's really a matter of timing of the splits and region reassignments.;;;","15/Feb/10 18:46;jdcryans;Doh forgot you didn't write data, scratch that.;;;","15/Feb/10 19:28;stack;Patch looks ok too.  I didn't try it.;;;","15/Feb/10 19:41;ryanobjc;I am going to commit this to trunk ASAP so it gets into 0.21 sooner than later.

Now, the question is, do we expect to maintain the 0.20.x branch longer, and if so are we willing to take a 1 time hit to get more expandability. I'm thinking ""no"".;;;","11/Mar/10 18:57;jdcryans;Bringing in 0.20.4 since we voted +1 on it and assigning Ryan.;;;","11/Mar/10 19:06;ryanobjc;I have a pending forward ported patch. Ill commit it soon.

On Mar 11, 2010 1:58 PM, ""Jean-Daniel Cryans (JIRA)"" <jira@apache.org>
wrote:


[
https://issues.apache.org/jira/browse/HBASE-2219?page=com.atlassian.jira.plugin.system.issue.
..
;;;","12/Mar/10 20:55;stack;Committed to branch.;;;","20/Mar/10 07:42;ryanobjc;apply to 0.20 branch;;;","20/Mar/10 07:43;ryanobjc;my bad already in there;;;","12/May/10 23:52;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 13:02;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in thrift deleteAll,HBASE-2210,12455869,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,larsfrancke,iranitov,iranitov,10/Feb/10 06:55,12/Oct/12 06:14,14/Jul/23 06:06,26/Apr/10 16:13,0.20.0,0.20.1,0.20.2,0.20.3,,,,,,,,0.20.4,,Thrift,,,,,0,,,Got a NPE when trying to delete all cells of an entire family.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/10 05:06;iranitov;HBASE-2210-patch;https://issues.apache.org/jira/secure/attachment/12435938/HBASE-2210-patch","10/Feb/10 07:00;iranitov;HBASE-2210.patch;https://issues.apache.org/jira/secure/attachment/12435418/HBASE-2210.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26207,Reviewed,,,,Mon Apr 26 16:13:04 UTC 2010,,,,,,,,,,"0|i08sxj:",49287,,,,,,,,,,,,,,,,,,,,,"16/Feb/10 05:08;iranitov;Tested on deleteAll, mutateRow and mutateRows.
(Please ignore/delete HBASE-2210.patch from 2010-02-10).
;;;","16/Feb/10 05:23;stack;Applied to branch.  Thanks for the patch Igor.

Will leave open assigned to Lars Francke.  Does this make sense in 0.21 context Lars?;;;","26/Apr/10 16:13;stack;Resolving against 0.20.4.  Lars, keep it in mind doing 0.21.... should I open a new issue for it?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[IHBASE] Index partial column values ,HBASE-2207,12455849,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,09/Feb/10 23:38,12/Oct/12 06:15,14/Jul/23 06:06,15/Feb/10 21:02,,,,,,,,,,,,0.20.4,,,,,,,0,,,"Index partial column values 
   - Modifies only IDX contrib. files
   - Allows extracting only part of the column value to be indexed. Can be useful if the column values are large since it allows to index only a small portion of the value and hence reduce the memory footprint",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/10 23:41;stack;5-index-partial-column-values;https://issues.apache.org/jira/secure/attachment/12435377/5-index-partial-column-values",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26206,,,,,Mon Feb 15 21:02:31 UTC 2010,,,,,,,,,,"0|i08sn3:",49240,,,,,,,,,,,,,,,,,,,,,"15/Feb/10 21:02;stack;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[IHBASE] Idx memory allocation fix,HBASE-2206,12455847,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,09/Feb/10 23:36,12/Oct/12 06:15,14/Jul/23 06:06,15/Feb/10 21:02,,,,,,,,,,,,0.20.4,,,,,,,0,,," Idx memory allocation fix
  - Includes a modification to core - MemStore was added with a numKeyValues query
  - It optimizes the number of memory allocation required to rebuild the index.  This is a major fix - it reduces the heap fragmentation and helped postpone/fix (not yet sure which of those) the region server crashing due to those long GC periods on a write-intensive setup",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/10 23:37;stack;4-idx-memory-allocation-fixes;https://issues.apache.org/jira/secure/attachment/12435376/4-idx-memory-allocation-fixes",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26205,,,,,Mon Feb 15 21:02:49 UTC 2010,,,,,,,,,,"0|i08sov:",49248,,,,,,,,,,,,,,,,,,,,,"15/Feb/10 21:02;stack;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[IHBASE] Updated Idx pacakge javadocs,HBASE-2205,12455846,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,09/Feb/10 23:33,12/Oct/12 06:15,14/Jul/23 06:06,15/Feb/10 21:03,,,,,,,,,,,,0.20.4,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/10 23:34;stack;3-updated-idx-package-javadoc;https://issues.apache.org/jira/secure/attachment/12435374/3-updated-idx-package-javadoc",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26204,,,,,Mon Feb 15 21:03:05 UTC 2010,,,,,,,,,,"0|i08snj:",49242,,,,,,,,,,,,,,,,,,,,,"15/Feb/10 21:03;stack;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[IHBASE] Index expression evaluation should fail with a DoNotRetryException in case of an invalid index specification,HBASE-2204,12455845,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,09/Feb/10 23:31,12/Oct/12 06:14,14/Jul/23 06:06,15/Feb/10 21:03,,,,,,,,,,,,0.20.4,,,,,,,0,,,"Index expression evaluation should fail with a DoNotRetryException in case of an invalid index specification:
  -  Modifies only IDX contrib. files",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/10 23:31;stack;2-the-index-expression-evaluator-throws-a-DoNotRetryException;https://issues.apache.org/jira/secure/attachment/12435373/2-the-index-expression-evaluator-throws-a-DoNotRetryException",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26203,,,,,Mon Feb 15 21:03:45 UTC 2010,,,,,,,,,,"0|i08svr:",49279,,,,,,,,,,,,,,,,,,,,,"15/Feb/10 21:03;stack;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[IHBase] Include only those columns required for indexed scan,HBASE-2203,12455844,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,09/Feb/10 23:27,12/Oct/12 06:15,14/Jul/23 06:06,15/Feb/10 21:03,,,,,,,,,,,,0.20.4,,,,,,,0,,,"Include only columns required for indexed scan:
  -  Modifies only IDX contrib. files
  -  Makes sure that an indexed rebuild scan would only include columns required to rebuild the index",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/10 23:27;stack;1-include-only-columns-required-for-indexed-scan;https://issues.apache.org/jira/secure/attachment/12435372/1-include-only-columns-required-for-indexed-scan",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26202,,,,,Mon Feb 15 21:03:59 UTC 2010,,,,,,,,,,"0|i08snb:",49241,,,,,,,,,,,,,,,,,,,,,"09/Feb/10 23:27;stack;Fix;;;","15/Feb/10 21:03;stack;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IdxRegion crash when binary characters,HBASE-2202,12455829,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,09/Feb/10 21:07,12/Oct/12 06:15,14/Jul/23 06:06,15/Feb/10 21:04,,,,,,,,,,,,0.20.4,,,,,,,0,,,If binary characters crash in IdxRegion code.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/10 21:07;stack;Fix_for_IdxRegion_crash_when_registering_a_MBean_that_contains_strange_chars.patch;https://issues.apache.org/jira/secure/attachment/12435341/Fix_for_IdxRegion_crash_when_registering_a_MBean_that_contains_strange_chars.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26201,Reviewed,,,,Mon Feb 15 21:04:18 UTC 2010,,,,,,,,,,"0|i08spr:",49252,,,,,,,,,,,,,,,,,,,,,"09/Feb/10 21:07;stack;Fix;;;","09/Feb/10 21:08;stack;Marking patch available;;;","15/Feb/10 21:04;stack;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"hbase.client.tableindexed.IndexSpecification, lines 72-73 should be reversed",HBASE-2199,12455760,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,,adipdia,adipdia,09/Feb/10 12:47,12/Oct/12 06:14,14/Jul/23 06:06,12/Feb/10 21:43,0.20.3,,,,,,,,,,,0.20.4,0.90.0,,,,,,0,,,"Error in constructor of  IndexSpecification, lines 72-73, where the order of ** this.makeAllColumns() and this.additionalColumns ** should be inverted. Otherwise by calling getAllColumns() only the indexed columns are returned.",,,,,,,,,,,,,,,,,,,,,,,300,300,,0%,300,300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/10 13:09;adipdia;HBASE-2199.patch;https://issues.apache.org/jira/secure/attachment/12435290/HBASE-2199.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26200,Reviewed,,,,Fri Feb 12 21:43:57 UTC 2010,,,,,,,,,,"0|i08ssn:",49265,,,,,,,,,,,,,,,,,,,,,"09/Feb/10 13:09;adipdia;Fix of the issue;;;","12/Feb/10 21:01;clint.morgan;looks good to me. Was never causing an issue for me because after serialization/deserialization then the additional columns gets correctly built...;;;","12/Feb/10 21:43;stack;Committed to branch and trunk.  Thanks for the patch Adrian.  Thanks for the review Clint.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Calling HTable.getTableDescriptor().* on a full cluster takes a long time,HBASE-2184,12455359,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,civascu,civascu,04/Feb/10 22:04,20/Nov/15 13:01,14/Jul/23 06:06,05/Feb/10 06:52,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"On a cluster with many tables, and consequently many regions, calling the getTableDescriptor() methods on a HTable takes a very long time, depending on the number of regions. For comparison, on a cluster with 7000 regions, getting a table descriptor ranged between 4 and 36 seconds, even when the queried table was empty.

The problem seems to lie in the HConnectionManager.getHTableDescriptor() method, which calls MetaScanner.scan() with an empty START_ROW. This means that even if we need the descriptor for a single region table, we still need to wait until the entire META is scanned. There is also a constructor for MetaScanner.scan() which takes the table name to lookup as a param.","CentOS 5.4, x86_64
MacOSX 10.6",civascu,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/10 22:07;civascu;HBASE_2184_0.21.0.patch;https://issues.apache.org/jira/secure/attachment/12434877/HBASE_2184_0.21.0.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26195,Reviewed,,,,Fri Nov 20 13:01:47 UTC 2015,,,,,,,,,,"0|i0hgov:",99966,,,,,,,,,,,,,,,,,,,,,"04/Feb/10 22:07;civascu;Attached proposed fix - the HConnectionManager.getHTableDescriptor(tableName) now calls MetaScanner.scan(*,*,tableName).

Observed results: time dropped from 5 seconds to 0.112 sec for a describe 'tableName' in the shell on the busy cluster;;;","05/Feb/10 06:52;stack;All tests passed for me.  Patch looks good.  Thanks Cristian.  Committed to TRUNK.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad random read performance from synchronizing hfile.fddatainputstream,HBASE-2180,12455120,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,ryanobjc,ryanobjc,02/Feb/10 22:11,12/Oct/12 06:14,14/Jul/23 06:06,26/Apr/10 16:17,,,,,,,,,,,,0.20.4,,,,,,,1,,,"deep in the HFile read path, there is this code:

    synchronized (in) {
      in.seek(pos);
      ret = in.read(b, off, n);
    }


this makes it so that only 1 read per file per thread is active. this prevents the OS and hardware from being able to do IO scheduling by optimizing lots of concurrent reads. 

We need to either use a reentrant API (pread may be partially reentrant according to Todd) or use multiple stream objects, 1 per scanner/thread.",,bandrews,chingshen,dlrozendaal,greggny3,hammer,kannanm,kaykay.unique,mbertozzi,streamy,tlipcon,,,,,,,,,,,,,,,,,,,,,,HBASE-1505,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/10 07:42;stack;2180-v2.patch;https://issues.apache.org/jira/secure/attachment/12434936/2180-v2.patch","05/Feb/10 00:29;stack;2180.patch;https://issues.apache.org/jira/secure/attachment/12434906/2180.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26193,Reviewed,,,,Mon Apr 26 16:17:16 UTC 2010,,,,,,,,,,"0|i08syn:",49292,,,,,,,,,,,,,,,,,,,,,"03/Feb/10 23:58;stack;Using pread -- its present in the code already, its just commented out on the line following the above cited by Ryan -- I see doubled throughput when 16 clients concurrently random reading out of a single regionserver so it helps.  i'll try and get some more numbers in here (I see 'wa' in top running at about the same for both cases but the regionserver is definetly working harder for the pread case using about double the CPU).

Numbers are not that good though -- about 50ms latency doing a random read when 16 concurrent clients.  This is a RS carrying 16M rows on 92 regions where there is 1 storefile only in the family and 4DNs under it.

Way back when we were looking at pread, it improved the random read latency by some small percentage IIRC, about 11%, but then scan speed slowed some... but these would have been for the case of low numbers of concurrent clients.

Its scanning 27k rows/second before the pread change using single client.  And 21k/second after.

Let me get some more numbers... up the concurrent client count and get some other points on how pread changes throughput.
;;;","04/Feb/10 02:31;stack;Link to issue that suggests we pread when doing random read and read when scanning.;;;","04/Feb/10 21:40;adragomir;We ran some testing that show improved performance by commenting in the change in BoundedFileInputStream, by replacing the synchronized statement with the one commented out, that uses the PositionalReader interface

{noformat}
    //synchronized (in) {
    //  in.seek(pos);
    //  ret = in.read(b, off, n);
    //}
    ret = in.read(pos, b, off, n);
{noformat};;;","05/Feb/10 00:29;stack;This patch has gets do preads fetching blocks and uses the old seek+read for scans.

Patch removes the old HFile.Reader.getScanner methods and replaces both with a getScanner that takes two arguments -- whether to cache blocks read and whether to use pread or not pulling in the block.  I got rid of the old getScanners to force all getScanners to be explicit about what they want regards caching and pread.

This patch does not include tests.  Its hard to test for this performance change.

A further improvement would recognize short scans -- i.e. scans that are < an hfile block size.  In this case, we'd want to pread rather than seek+scan (especially so when scan one row replaces get)

;;;","05/Feb/10 07:42;stack;This patch includes fixes for tests making them use new getScanner method and includes small PE fix when --rows is small (We would NPE).  I might need a v3.  A test is failing (TestGetDeleteTracker).  Need to investigate.

In testing on something that tries to resemble the yahoo papers testing -- ~20M rows per server, 116 regions on a RS and only one replica -- this patch seems to double the throughput if ~20 concurrent clients on a RS.  I tested scans and scan speeds are what they were w/ this patch in place.  They have not deterioated.

One thing I noticed was that scanning when the data is not local -- i.e. the data is in a DN on another machine -- there is added latency for sure.... taking maybe 25% as long again for the test to complete.  I need to see if same is true of random reads.  Cosmin suggested that the yahoo test with its single replica only might be doing lots of remote accessing and could be incurring the extra latency.;;;","06/Feb/10 22:59;ryanobjc;+1 thanks for doing this!;;;","07/Feb/10 05:29;stack;Committed branch and trunk.;;;","08/Feb/10 23:40;stack;Really commit to TRUNK.;;;","19/Feb/10 00:18;stack;Committed a while back.  Resolving.;;;","25/Feb/10 11:11;dlrozendaal;After applying this patch to 0.20.3 I got the following errors in my regionserver logs when doing high loads of gets and puts:

2010-02-25 11:44:08,243 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region inrdb_ticket,\x07R\x00\x00\x00\x00\x80\xFF\xFF\xFF\x7F\x00\x00\x00\x01,1267094341820 i
n 6sec
1177:java.net.BindException: Cannot assign requested address
        at sun.nio.ch.Net.connect(Native Method)
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:507)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:404)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.fetchBlockByteRange(DFSClient.java:1825)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1898)
        at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:46)
        at org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream.read(BoundedRangeFileInputStream.java:101)
        at org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream.read(BoundedRangeFileInputStream.java:88)
        at org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream.read(BoundedRangeFileInputStream.java:81)
        at org.apache.hadoop.io.compress.BlockDecompressorStream.rawReadInt(BlockDecompressorStream.java:121)
        at org.apache.hadoop.io.compress.BlockDecompressorStream.getCompressedData(BlockDecompressorStream.java:96)
        at org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:82)
        at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:74)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
        at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:100)
        at org.apache.hadoop.hbase.io.hfile.HFile$Reader.decompress(HFile.java:1018)
        at org.apache.hadoop.hbase.io.hfile.HFile$Reader.readBlock(HFile.java:966)
        at org.apache.hadoop.hbase.io.hfile.HFile$Reader$Scanner.next(HFile.java:1159)
        at org.apache.hadoop.hbase.regionserver.StoreFileGetScan.getStoreFile(StoreFileGetScan.java:108)
        at org.apache.hadoop.hbase.regionserver.StoreFileGetScan.get(StoreFileGetScan.java:65)
        at org.apache.hadoop.hbase.regionserver.Store.get(Store.java:1463)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2396)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2385)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1731)
        at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)

The DataNode logs are fine (no maximum xcievers exceeded errors). Turns out that the OS was running out of port numbers. netstat showed more than 20,000 connections in TIME_WAIT state. Reverting to the original hbase-0.20.3 jar solved the problem. Only very few (<10) TIME_WAIT connections even after running gets/puts for a while.

So it looks like this patch causes some network connection issues. Any ideas if that could be the case?

PS Running only gets seems to be fine, but I've mostly run tests with reads from the block cache.;;;","25/Feb/10 12:03;stack;Reopening to take a look.

I have a vague recollection of stuff not being closed down if not all is read out of the socket.  Thanks for reporting this Erik.;;;","25/Feb/10 16:22;stack;I was thinking of a very old issue, HADOOP-2341, but that was about CLOSE_WAIT, not TIME_WAIT.  Erik I presume the TIME_WAIT are on the datanode side?  I suppose there could be an issue here if many random reads in a short amount of time and the minimum segment lifetime (MSL) time is long in your tcp/ip implementation.  Do you know what it is?  2minutes seems default reading up on the internets so could be in TIME_WAIT for 4 minutes.  This what you are seeing you think Erik?   They go away after a while?  Whats the OS?  This would seem to be a new issue then.  We need pread that does keep-alive reusing sockets (Todd!).;;;","25/Feb/10 16:32;dlrozendaal;I saw both CLOSE_WAIT and TIME_WAIT. Maybe CLOSE_WAIT was in the majority. Connections were mostly to the data node.

$ uname -a
Linux inrdb-worker1.ripe.net 2.6.18-164.11.1.el5 #1 SMP Wed Jan 20 07:32:21 EST 2010 x86_64 x86_64 x86_64 GNU/Linux

They do go after a while, since after a few of the ""Cannot assign requested address"" exceptions the server starts working again.

Unfortunately I'll be away for the weekend and won't be able to investigate further. I wonder why so many connections are being opened so quickly that the server runs out of ports within a few minutes of starting the gets/puts?;;;","27/Feb/10 13:08;stack;.bq I wonder why so many connections are being opened so quickly that the server runs out of ports within a few minutes of starting the gets/puts?

Gets used hdfs pread.  pread opens a socket per access.  My guess is that high rate of gets soon overwhelms the time each socket takes to clean up after close.  What kinda rates are we talking here Erik?;;;","03/Mar/10 21:23;tlipcon;In the absence of reusing sockets, I think the TIME_WAIT issue could be dealt with on the system level by toggling /proc/sys/net/ipv4/tcp_tw_recycle;;;","26/Apr/10 16:17;stack;Resolving against 0.20.4.  I opened hbase-2492 to cover underlying new socket per pread.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add timestamping to gc logging options,HBASE-2177,12454991,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,01/Feb/10 23:29,12/Oct/12 06:14,14/Jul/23 06:06,12/Feb/10 00:15,,,,,,,,,,,,0.20.4,0.90.0,,,,,,0,,,http://forums.sun.com/thread.jspa?threadID=5165451,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26191,,,,,Fri Feb 12 00:16:32 UTC 2010,,,,,,,,,,"0|i08t0v:",49302,,,,,,,,,,,,,,,,,,,,,"11/Feb/10 23:20;stack;Use the below instead.  Includes application stopped time logging:

# export HBASE_OPTS=""$HBASE_OPTS -verbose:gc -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:$HBASE_HOME/logs/gc-hbase.log"";;;","11/Feb/10 23:38;ryanobjc;so one problem with this is the irb then logs all GC to stdout, which is ugly.  I do something like this in my scripts:

export HBASE_OPTS=""""
export HBASE_LOG_DIR=<somewhere>

export SERVER_GC_OPTS=""$HBASE_OPTS -verbose:gc -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:$HBASE_HOME/logs/gc-hbase.log""

export JMX_OPTS=""-Dcom.sun.management.jmxremote.authenticate=true -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.password.file=$HBASE_HOME/conf/jmxremote.password -Dcom.sun.management.jmxremote""


export HBASE_MASTER_OPTS=""$SERVER_GC_OPTS -Xloggc:$HBASE_LOG_DIR/logs/gc-master.log""
export HBASE_REGIONSERVER_OPTS=""$SERVER_GC_OPTS -Xloggc:$HBASE_LOG_DIR/gc-hbase.log -Dcom.sun.management.jmxremote.port=10102 $JMX_OPTS""
export HBASE_THRIFT_OPTS=""-Xmx1000m $SERVER_GC_OPTS -Xloggc:$HBASE_LOG_DIR/gc-hbase-thrift.log -Dcom.sun.management.jmxremote.port=10103 $JMX_OPTS""
export HBASE_ZOOKEEPER_OPTS=""-Xmx1000m $SERVER_GC_OPTS -Xloggc:$HBASE_LOG_DIR/gc-zk.log -Dcom.sun.management.jmxremote.port=10104 $JMX_OPTS""



now you get remote JMX with logging to whatever directory (we have to log to our large data partition since logs... can be big).  Also the shell doesnt log GC to stdout, and you can get separate GC logs for hmaster, hrs, thrift, zookeeper.

;;;","12/Feb/10 00:15;stack;Committed datestamp only.  The application paused logging can be obnoxious -- even though this is gc logging we're talking about here.;;;","12/Feb/10 00:16;stack;Committed TRUNK and branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New idx javadoc not included with the rest,HBASE-2173,12446866,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,jdcryans,jdcryans,27/Jan/10 23:42,12/Oct/12 06:14,14/Jul/23 06:06,09/Feb/10 23:10,,,,,,,,,,,,0.20.4,,,,,,,0,,,"I just figured that the new idx package javadoc isn't included in the normal build process. I think we should fix that for 0.20.4 but still regenerate the javadoc for 0.20.3 and add it to the website. It's out of the normal release process so please discuss, I'm obviously +1 on this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jan/10 21:46;stack;HBASE-2173.patch;https://issues.apache.org/jira/secure/attachment/12431822/HBASE-2173.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26188,Reviewed,,,,Tue Feb 09 23:10:36 UTC 2010,,,,,,,,,,"0|i08ssv:",49266,,,,,,,,,,,,,,,,,,,,,"29/Jan/10 21:46;stack;Adds indexed to the javadoc target.;;;","09/Feb/10 22:50;jdcryans;+1, please commit and publish to website.;;;","09/Feb/10 23:10;stack;Committed to TRUNK (Will publish to site when 0.20.4 goes out... which I don't think is too far out).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Alter statement in the hbase shell doesn't match documentation.,HBASE-2171,12446763,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,posix4e,posix4e,posix4e,27/Jan/10 03:17,11/Jun/22 23:05,14/Jul/23 06:06,19/Jul/14 00:01,0.20.2,0.20.3,,,,,,,,,,,,,,,,,0,,,"The documentation claims this should work. Perhaps this jira could be a starting point for a more detailed explanation of alter

HBASE SHELL COMMANDS:
 alter     Alter column family schema;  pass table name and a dictionary
           specifying new column family schema. Dictionaries are described
           below in the GENERAL NOTES section.  Dictionary must include name
           of column family to alter.  For example,

           To change or add the 'f1' column family in table 't1' from defaults
           to instead keep a maximum of 5 cell VERSIONS, do:
           hbase> alter 't1', {NAME => 'f1', VERSIONS => 5}

           To delete the 'f1' column family in table 't1', do:
           hbase> alter 't1', {NAME => 'f1', METHOD => 'delete'}

           You can also change table-scope attributes like MAX_FILESIZE
           MEMSTORE_FLUSHSIZE and READONLY.

           For example, to change the max size of a family to 128MB, do:
           hbase> alter 't1', {METHOD => 'table_att', MAX_FILESIZE => '134217728'}
....
ase Shell; enter 'help<RETURN>' for list of supported commands.
Version: 0.20.3, r902334, Mon Jan 25 13:13:08 PST 2010
hbase(main):001:0> drop 't3'
0 row(s) in 0.0060 seconds
0 row(s) in 0.0050 seconds
0 row(s) in 0.1560 seconds
hbase(main):002:0> create 't3'
0 row(s) in 2.1050 seconds
hbase(main):003:0> disable 't3'
0 row(s) in 2.0980 seconds
hbase(main):004:0> alter 't3', {NAME => 'f1', VERSIONS => 5}
NativeException: java.lang.NullPointerException: null
 
","linux 
java -version
java version ""1.6.0_16""
Java(TM) SE Runtime Environment (build 1.6.0_16-b01)
Java HotSpot(TM) 64-Bit Server VM (build 14.2-b01, mixed mode)
",bandrews,posix4e,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26187,,,,,Sat Jul 19 00:01:30 UTC 2014,,,,,,,,,,"0|i02dzz:",11869,,,,,,,,,,,,,,,,,,,,,"19/Jul/14 00:01;posix4e;This is fixed now 
call methods.


hbase(main):015:0> alter 't1', {NAME=>'f1', VERSIONS=>5}
Updating all regions with the new schema...
0/1 regions updated.
1/1 regions updated.
Done.
0 row(s) in 2.0810 seconds

hbase(main):016:0> 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZK dependencies - explicitly add them until ZK artifacts are published to mvn repository,HBASE-2163,12446434,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kaykay.unique,kaykay.unique,kaykay.unique,23/Jan/10 20:25,20/Nov/15 13:01,14/Jul/23 06:06,23/Jan/10 20:52,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Currently we include the binary of zookeeper but we need to add the dependencies explicitly as well ( similar to a recent issue , related to thrift ). 

zk depends on log4j / jline . 
log4j is already in. 

This patch adds jline to the dependencies explicitly. ",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/10 20:26;kaykay.unique;HBASE-2163.patch;https://issues.apache.org/jira/secure/attachment/12431222/HBASE-2163.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26184,Reviewed,,,,Fri Nov 20 13:01:50 UTC 2015,,,,,,,,,,"0|i0hgmn:",99956,,,,,,,,,,,,,,,,,,,,,"23/Jan/10 20:52;stack;Applied to TRUNK.  Thanks for patch Kay Kay.;;;","23/Jan/10 20:53;kaykay.unique;That is rapid fast. Thanks for taking the patch - stack . ;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't put with ts in shell,HBASE-2160,12446361,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,jdcryans,jdcryans,22/Jan/10 19:21,12/Oct/12 06:14,14/Jul/23 06:06,22/Jan/10 21:13,,,,,,,,,,,,0.20.3,0.90.0,,,,,,0,,,"On the latest branch I can't issue a put with a ts in the shell, it does this:

{code}
hbase(main):008:0> put 't', 'r', 'f:', 'test', 123123
NameError: no constructor with arguments matching
 [class org.jruby.java.proxies.ArrayJavaProxy, class org.jruby.RubyFixnum] on 
 object #<Java::OrgApacheHadoopHbaseClient::Put:0x49239780>
{code}

It works without a ts and delete/scan aren't affected by this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/10 21:12;stack;2160.patch;https://issues.apache.org/jira/secure/attachment/12431142/2160.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26181,,,,,Fri Jan 22 21:13:41 UTC 2010,,,,,,,,,,"0|i08tbr:",49351,,,,,,,,,,,,,,,,,,,,,"22/Jan/10 21:13;stack;Applied trunk and branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LATEST_TIMESTAMP not replaced by current timestamp in KeyValue,HBASE-2157,12446349,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,mluiten,mluiten,22/Jan/10 18:11,12/Oct/12 06:14,14/Jul/23 06:06,22/Jan/10 19:06,0.20.2,,,,,,,,,,,0.20.3,0.90.0,,,,,,0,,,"I was trying to bulk load using the new HFileOutputFormat. When using a MapReduce in which map generates {{KeyValue}}s and reduce is equal to KeyValueSortReducer, and using the constructor using (byte[] row, byte[] family, byte[] qualifier, byte[] value), the (undefined) timestamp was inserted as HConstants.LATEST_TIMESTAMP/Long.MAX_VALUE into HBase. This causes all kinds of troubles, but most importantly, while the records were in the table, other MapReduces (using TableInputFormat) and Hbase shell's 'get'-command did not fetch them. Guess there is some sort of filtering of future dates.

As I understood from St.Ack, the LASTEST_TIMESTAMP is supposed to be replaced by System.currentTimeMillis(), but I don't see this reflected in the code of KeyValue, and apparently it did not happen elsewhere; perhaps because there is no actual HBase connection?",Hadoop 0.20.0 - Hbase 0.20.2 - Java(TM) SE Runtime Environment (build 1.6.0_17-b04),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/10 19:03;stack;2157.patch;https://issues.apache.org/jira/secure/attachment/12431135/2157.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26180,,,,,Fri Jan 22 19:11:59 UTC 2010,,,,,,,,,,"0|i08tb3:",49348,,,,,,,,,,,,,,,,,,,,,"22/Jan/10 18:13;mluiten;BTW: Solved the issue by using the constructor including timestamp and passing System.currentTimeMillis() manually. Works fine.;;;","22/Jan/10 18:14;jdcryans;Right the replacement is done in the region server and in this case it gets bypassed. ;;;","22/Jan/10 18:20;mluiten;That explains it :) Cost me hours figuring it out :D Could this be solved by adding the check/replacement to HFileOutputFormat as well?;;;","22/Jan/10 18:33;jdcryans;Yes, I guess it would make sense.;;;","22/Jan/10 19:03;stack;I added note to the bulk load documentation that warns against creating KVs w/o explicit timestamp so others don't trip over Menno's issue.;;;","22/Jan/10 19:06;stack;Committed branch and trunk.;;;","22/Jan/10 19:11;stack;I don't think adding a check/replacement to HFOF will work since this is after the sort done up in the KeyValueSortReducer.  Rather, we could have it throw an exception if HFOF is passed a KV with LATEST_TIMESTAMP?  I'll open a new issue to do that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBASE-2037 broke Scan,HBASE-2156,12446290,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,jdcryans,jdcryans,jdcryans,22/Jan/10 07:41,12/Oct/12 06:14,14/Jul/23 06:06,19/Oct/10 21:49,0.20.3,,,,,,,,,,,0.20.3,0.90.0,,,,,,0,,,"Paul Ambrose wrote to the mailing list about some tests he has that doesn't pass on 0.20.3RC1-2. Looking into the issue it appears that this modification:

{code}
   public Scan addFamily(byte [] family) {
     familyMap.remove(family);
-    familyMap.put(family, null);
+    familyMap.put(family, EMPTY_NAVIGABLE_SET);
     return this;
   }
{code}

Makes it that when you use addColumn after that you put qualifiers into EMPTY_NAVIGABLE_SET which is static hence shared among all scanners after that like META scanners when calling tableExists.

This was introduced by HBASE-2037.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/10 21:07;jdcryans;HBASE-2156-test-fix.patch;https://issues.apache.org/jira/secure/attachment/12457595/HBASE-2156-test-fix.patch","22/Jan/10 08:00;jdcryans;HBASE-2156.patch;https://issues.apache.org/jira/secure/attachment/12431101/HBASE-2156.patch","11/Oct/10 20:20;larsfrancke;HBASE-2156.test-enabled.patch;https://issues.apache.org/jira/secure/attachment/12456888/HBASE-2156.test-enabled.patch",,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26179,Reviewed,,,,Tue Oct 19 21:49:41 UTC 2010,,,,,,,,,,"0|i08tdb:",49358,,,,,,,,,,,,,,,,,,,,,"22/Jan/10 08:00;jdcryans;Patch that fixes the bug by verifying if the set is the static one. Also adds a unit tests and re-indents the previous one in the class (which was horribly wrong).;;;","22/Jan/10 08:37;tsuna;+1, figured out the bug with JD and verified the patch.
The unit test it's adding could be more minimalist and straight-to-the-point but oh well...;;;","22/Jan/10 17:52;stack;+1;;;","22/Jan/10 18:41;jdcryans;I committed the fix to branch and a to-the-point test to branch and trunk per Benoit's comment.;;;","11/Oct/10 20:20;larsfrancke;The ""to-the-point"" test JD implemented unfortunately never ran due to a missing annotation.

And when I run it it fails:

{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hbase.client.TestFromClientSide.testScanVariableReuse(TestFromClientSide.java:3681)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:64)
{noformat};;;","16/Oct/10 20:21;stack;Reopening so J-D can take a look (This was marked a blocker after all).  Thanks for detective work Lars.;;;","19/Oct/10 21:07;jdcryans;This test was failing on trunk because HBASE-2037 never ended up getting applied there. I think the test is still useful to prevent that kind of bug, so here's a better/working version of it.;;;","19/Oct/10 21:45;stack;+1;;;","19/Oct/10 21:49;jdcryans;Thanks Stack, committed to trunk and re-closing the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Client#next(int) javadoc,HBASE-2154,12446272,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,22/Jan/10 00:11,12/Oct/12 06:14,14/Jul/23 06:06,22/Jan/10 18:06,,,,,,,,,,,,0.20.3,0.90.0,,,,,,0,,,"Its not clear what signifies scanner end and noobs probably think that batch size is how much we fetch in an RPC (thats different, thats Scan#setCaching).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/10 18:05;stack;jd-2.patch;https://issues.apache.org/jira/secure/attachment/12431126/jd-2.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26178,Reviewed,,,,Fri Jan 22 18:06:31 UTC 2010,,,,,,,,,,"0|i08ti7:",49380,,,,,,,,,,,,,,,,,,,,,"22/Jan/10 00:12;stack;This came up on IRC today. 

Applied to TRUNK.;;;","22/Jan/10 18:01;stack;Patch that fixes javadoc.;;;","22/Jan/10 18:05;stack;Here is what I applied to trunk yesterday.;;;","22/Jan/10 18:06;stack;Committed to branch too.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove onelab and include generated thrift classes in javadoc,HBASE-2151,12446183,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,larsfrancke,stack,stack,21/Jan/10 02:23,20/Nov/15 13:02,14/Jul/23 06:06,21/Jan/10 19:53,,,,,,,,,,,,0.90.0,,,,,,,0,,,Patch is actually over in hbase-1373 named javadoc.patch.  It was done by Lars Francke so I assigned him this issue.,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/10 13:08;larsfrancke;HBase-2151.patch;https://issues.apache.org/jira/secure/attachment/12431018/HBase-2151.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26177,Reviewed,,,,Fri Nov 20 13:02:03 UTC 2015,,,,,,,,,,"0|i0hglb:",99950,,,,,,,,,,,,,,,,,,,,,"21/Jan/10 02:26;stack;Hmm.  There's a bunch of javadoc warnings if I include thrift generated files.  Can these be fixed Lars?;;;","21/Jan/10 02:27;stack;Here is a sample:
{code}
 [javadoc] Building tree for all the packages and classes...
  [javadoc] /Users/stack/checkouts/hbase/trunk.old/src/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:49: warning - Parameter ""tableName"" is documented more than once.
  [javadoc] /Users/stack/checkouts/hbase/trunk.old/src/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:58: warning - Parameter ""tableName"" is documented more than once.
  [javadoc] /Users/stack/checkouts/hbase/trunk.old/src/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:66: warning - Parameter ""tableName"" is documented more than once.
  [javadoc] /Users/stack/checkouts/hbase/trunk.old/src/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:85: warning - Parameter ""tableName"" is documented more than once.
  [javadoc] /Users/stack/checkouts/hbase/trunk.old/src/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:94: warning - Parameter ""tableName"" is documented more than once.
  [javadoc] /Users/stack/checkouts/hbase/trunk.old/src/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:111: warning - Parameter ""tableName"" is documented more than once.
  [javadoc] /Users/stack/checkouts/hbase/trunk.old/src/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:111: warning - Parameter ""columnFamilies"" is documented more than once.
  [javadoc] /Users/stack/checkouts/hbase/trunk.old/src/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:121: warning - Parameter ""tableName"" is documented more than once.
  [javadoc] /Users/stack/checkouts/hbase/trunk.old/src/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:136: warning - Parameter ""tableName"" is documented more than once.
  [javadoc] /Users/stack/checkouts/hbase/trunk.old/src/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:136: warning - Parameter ""row"" is documented more than once.
  [javadoc] /Users/stack/checkouts/hbase/trunk.old/src/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java:136: warning - Parameter ""column"" is documented more than once.
..
{code};;;","21/Jan/10 03:05;larsfrancke;I've missed this, thanks. IDEA hid those warnings well. I learn something new about Thrift every day. I have a fix and I'll probably attach it tomorrow (seeing as I'm in CET time here ;-) ). 

It requires changes to Hbase.thrift and thus to the generated classes but nothing serious.

Just for reference how a function in Thrift needs to be documented.

{noformat}
  /**
   * Disables a table (takes it off-line) If it is being served, the master
   * will tell the servers to stop serving it.
   */
  void disableTable(
    /** name of the table */
    1:Bytes tableName
  ) throws (1:IOError io)
{noformat}

This is how it is in the current version of Hbase.thrift
{noformat}
  /**
   * Disables a table (takes it off-line) If it is being served, the master
   * will tell the servers to stop serving it.
   * @param tableName name of the table
   */
  void disableTable(1:Bytes tableName) throws (1:IOError io)
{noformat}

This makes the Hbase.thrift file a little bit more ugly but at least the documentation is correct.;;;","21/Jan/10 13:08;larsfrancke;This (in addition to the javadoc patch from HBASE-1373) fixes all Javadoc issues for me.

There are only documentation changes for the Hbase.thrift file but I had to regenerate Hbase.java for those changes to take effect.;;;","21/Jan/10 19:53;stack;Worked for me.  I can see the generated classes showing in javadoc now.  Thanks for the patch Lars Francke.;;;","20/Nov/15 13:02;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecated HBC(Configuration) constructor doesn't call this(),HBASE-2150,12446168,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,21/Jan/10 00:06,20/Nov/15 13:02,14/Jul/23 06:06,21/Jan/10 00:18,0.90.0,,,,,,,,,,,0.90.0,,,,,,,0,,,"While trying to port some 0.20 code, I found that HBC(Configuration) doesn't call the default constructor and thus never leads HBase ressources. This breaks compatibility.",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26176,,,,,Fri Nov 20 13:02:04 UTC 2015,,,,,,,,,,"0|i0hgl3:",99949,,,,,,,,,,,,,,,,,,,,,"21/Jan/10 00:18;jdcryans;Easy fix, committed this to trunk:

{code}
===================================================================
--- src/java/org/apache/hadoop/hbase/HBaseConfiguration.java	(revision 900938)
+++ src/java/org/apache/hadoop/hbase/HBaseConfiguration.java	(working copy)
@@ -53,11 +53,10 @@
   @Deprecated
   public HBaseConfiguration(final Configuration c) {
     //TODO:replace with private constructor
+    this();
     for (Entry<String, String>e: c) {
       set(e.getKey(), e.getValue());
     }
-    LOG.warn(""instantinating HBaseConfiguration() is deprecated. Please use "" +
-    		""HBaseConfiguration#create(conf) to construct a plain Configuration"");
   }

{code};;;","20/Nov/15 13:02;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hbase.regionserver.global.memstore.lowerLimit is too low,HBASE-2149,12446124,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,jdcryans,jdcryans,20/Jan/10 19:37,12/Oct/12 06:14,14/Jul/23 06:06,22/Jan/10 18:42,,,,,,,,,,,,0.20.3,0.90.0,,,,,,0,,,"The default value of hbase.regionserver.global.memstore.lowerLimit of 25% is very wrong and in almost all cases was problematic (I've seen this in at least 3 occurrences). The cost of flushing a memstore is fairly high and when the global size reaches 40% then ALL inserts are blocked. This means that with a heap of 1GB you could be flushing for 10-20 seconds or worse.

I suggest a default setting of 38% or even 40% so that only a region or two will be flushed (the biggest ones) for maximum availability.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/10 18:36;stack;2149.patch;https://issues.apache.org/jira/secure/attachment/12431131/2149.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26175,Reviewed,,,,Fri Jan 22 18:42:05 UTC 2010,,,,,,,,,,"0|i08t67:",49326,,,,,,,,,,,,,,,,,,,,,"20/Jan/10 19:48;stack;Chatting w/ J-D, it was set by me -- I think -- at 0.4 and 0.25 because of PE-itis, a condition that comes of loading only using the PE tool.  Symptoms are inability at being able to comprehend any other type of loading pattern.  This issue is prompted by a fella loading hbase with UUID keys would likely spread writes nice and evenly across the cluster requiring the flushing of many regions to go from 0.4 to 0.25.  0.4/.38?  We could try it.  Or 0.4 and 0.35?;;;","20/Jan/10 20:22;ryanobjc;putting the lower bound to high just means we take more pauses to do smaller flushes. maybe this might be an easy-to-get improvement...

maybe a better solution might be to have a soft limit where we start background flushing until we reach the lower limit. if we outrun that and hit the hard limit, the protection kicks in and pauses all inserts.;;;","20/Jan/10 20:35;jdcryans;I agree with Ryan. Could the MemStoreFlusher also check the state of the global size (if higher than the lower bound) when it doesn't have any region to flush?;;;","22/Jan/10 18:15;stack;For RC3, do we just want to change the hbase-default lower limit?;;;","22/Jan/10 18:26;jdcryans;Good idea, I vote for a conservative value of 0.35 like you commented.;;;","22/Jan/10 18:36;stack;Here is a patch making change to 0.35 from 0.25.  We can open another issue to do Ryans' suggestion (we should be more 'live' about taking on writes as he suggets).;;;","22/Jan/10 18:42;stack;Committed branch and trunk the change of low limit from 0.25 to 0.35.  Opened HBASE-2158 to implement Ryans' suggestion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
run zookeeper in the same jvm as master during non-distributed mode,HBASE-2147,12445966,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ryanobjc,ryanobjc,ryanobjc,19/Jan/10 21:19,20/Nov/15 13:01,14/Jul/23 06:06,20/Mar/10 07:23,,,,,,,,,,,,0.90.0,,,,,,,0,,,this will avoid needing to 'ssh localhost' to start hbase on a stand-alone non-distributed machine. We should run ZK in the same JVM and also change the scripts too.,,hammer,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/10 07:12;ryanobjc;HBASE-2147-0.20.txt;https://issues.apache.org/jira/secure/attachment/12439355/HBASE-2147-0.20.txt","19/Jan/10 23:11;ryanobjc;HBASE-2147-v2.patch;https://issues.apache.org/jira/secure/attachment/12430811/HBASE-2147-v2.patch","19/Jan/10 23:26;ryanobjc;HBASE-2147-v3.patch;https://issues.apache.org/jira/secure/attachment/12430813/HBASE-2147-v3.patch","19/Jan/10 21:25;ryanobjc;HBASE-2147.patch;https://issues.apache.org/jira/secure/attachment/12430798/HBASE-2147.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26174,,,,,Fri Nov 20 13:01:54 UTC 2015,,,,,,,,,,"0|i0hgkn:",99947,,,,,,,,,,,,,,,,,,,,,"19/Jan/10 21:42;apurtell;Looks ok to me. One minor issue:

{noformat}
+  // TODO: make this more configurable?
+  private static final int TICK_TIME = 2000;
{noformat}

All-localhost deployment may need timeout longer than 40 seconds? 

;;;","19/Jan/10 23:11;ryanobjc;new version that only starts 1 regionserver;;;","19/Jan/10 23:26;ryanobjc;tick time is now configurable;;;","19/Jan/10 23:59;apurtell;+1;;;","20/Jan/10 01:26;stack;+1 (if it works -- smile);;;","20/Mar/10 07:12;ryanobjc;for 0.20 branch;;;","20/Mar/10 07:12;ryanobjc;putting this to 0.20 branch;;;","20/Mar/10 07:23;ryanobjc;committed this to branch;;;","12/May/10 23:53;stack;Marking these as fixed against 0.21.0 rather than against 0.20.5.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RPC related metrics are missing in 0.20.3 since recent changes,HBASE-2146,12445928,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,ghelmling,larsgeorge,larsgeorge,19/Jan/10 13:02,12/Oct/12 06:14,14/Jul/23 06:06,22/Jan/10 18:13,0.20.3,,,,,,,,,,,0.20.3,0.90.0,,,,,,0,,,Since the recent change to the Metrics setup it seems that the RPC related stats have been completely dropped. See attached files for details.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/10 17:03;ghelmling;HBASE-2146_0.20.patch;https://issues.apache.org/jira/secure/attachment/12430902/HBASE-2146_0.20.patch","20/Jan/10 17:36;ghelmling;HBASE-2146_trunk.patch;https://issues.apache.org/jira/secure/attachment/12430904/HBASE-2146_trunk.patch","19/Jan/10 13:04;larsgeorge;jconsole-hbase-rpc.png;https://issues.apache.org/jira/secure/attachment/12430740/jconsole-hbase-rpc.png","19/Jan/10 13:04;larsgeorge;jmx-diffs.txt;https://issues.apache.org/jira/secure/attachment/12430741/jmx-diffs.txt",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26173,Reviewed,,,,Fri Jan 22 18:13:02 UTC 2010,,,,,,,,,,"0|i08tfj:",49368,,,,,,,,,,,,,,,,,,,,,"19/Jan/10 15:52;larsgeorge;Err, this is confusing. I cannot even find where these Metrics are exported in 0.20.2! Am I missing something? Do I have a special build that had those values? Why were they there and why were they removed? Bueller?;;;","19/Jan/10 18:11;stack;@LarsG For JMX do they have to be enabled in the hadoop-metrics.properties file (Maybe not?).;;;","19/Jan/10 19:24;larsgeorge;Hmm, I checked both cluster then they have the exact same hadoop-metrics, hbase-env and so on. All diff""ed by my a few times. So why is that cluster different. What is even weirder is that these are MetricsTimeVaryingRate instances so they must be declared and put into the MetricsRegistry before createMBean() is called. And I cannot find where that is done!;;;","19/Jan/10 20:30;ghelmling;Lars,

It looks like these extra metrics are dynamically created and added to the registry via the HBaseRPCMetrics.inc() and subsequently create() and get() methods.  The org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase class will update the MBean info if the underlying registry changes (see updateMbeanInfoIfMetricsListChanged()), but maybe this isn't being invoked everywhere it's needed?

The .inc() method is being called in HBaseRPC.Server.call() with the call method name.  So it looks like that's how they're getting populated.  -- As an aside it looks like rpcQueueTIme and rpcProcessingTime are incremented twice there is debug logging is enabled(!) --  I'll try to test this out as well a bit later.;;;","19/Jan/10 20:54;larsgeorge;Ah, thanks Gary, that makes sense. I guess the issue is that my second cluster is not busy and has not yet created the counters. This makes for a difficult discovery though then. Sure, you could run the discovery later, but it would be nice to get an authoritative list from somewhere. ;;;","19/Jan/10 21:02;larsgeorge;And Gary is right.

There is a duplicate call in the debug!

{code}
        if (LOG.isDebugEnabled()) {
          LOG.debug(""Served: "" + call.getMethodName() +
            "" queueTime= "" + qTime +
            "" procesingTime= "" + processingTime);
          rpcMetrics.rpcQueueTime.inc(qTime);
          rpcMetrics.rpcProcessingTime.inc(processingTime);
        }
        rpcMetrics.rpcQueueTime.inc(qTime);
        rpcMetrics.rpcProcessingTime.inc(processingTime);
        rpcMetrics.inc(call.getMethodName(), processingTime);
{code}

Also, the last line is what I could not see (forrest for the trees kind of thing). 

So we make this issue to patch the above duplicate inc and close it?;;;","20/Jan/10 15:47;ghelmling;Not having all the attributes initially available does seem a bit problematic if you have automated monitoring setup.  You're likely to get an AttributeNotFoundException if you have a process trying to retrieve it before it's there.

I'll take a shot at pre-registering those metrics.  I think we can do it with the code <-> method name map in HBaseRPC.  I'll post a patch for that (and remove the duplication) shortly.

;;;","20/Jan/10 17:03;ghelmling;Patch against 0.20 branch to pre-register per-RPC-method metrics, so that the full attribute list is initially available for JMX MBean creation.

This adds a default access HBaseRPC.getMappedMethodNames() static method to retrieve the values of the HBaseRPC.Invocation.CODE_TO_METHODNAMES map, which isn't super elegant.  But it seemed like the least intrusive way to get what we need.;;;","20/Jan/10 17:06;ghelmling;Lars, can you try this patch on your setup?  

I did a quick test with jconsole and all the rpc-method metrics seemed to show up on startup for me.

This patch also drops the duplicate incrementing of rpcQueueTime and rpcProcessingTime.;;;","20/Jan/10 17:36;ghelmling;Same patch as HBASE-2146_0.20.patch against trunk.;;;","20/Jan/10 18:51;larsgeorge;Will test first thing tomorrow, I'll let you know. Thanks!;;;","20/Jan/10 21:56;larsgeorge;Assigning to Gary who did the work.;;;","20/Jan/10 21:57;larsgeorge;Works great, will commit to trunk and hold of with branch until after RC2 has passed.;;;","22/Jan/10 18:13;stack;Applied branch.  Resolving.  Thanks for the patch Gary.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation --miniCluster randomRead 1 don't work,HBASE-2145,12445886,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,19/Jan/10 05:26,11/Jun/22 23:03,14/Jul/23 06:06,16/Jul/14 18:57,,,,,,,,,,,,,,,,,,,0,,,"I see this in the 0.20.3RC.  Its been there a while I'd guess.  Not enough to sink RC I'd say.  In fact, all PE args could do with a review.

{code}
...
10/01/18 21:25:31 DEBUG zookeeper.ZooKeeperWrapper: Failed to read: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/master
10/01/18 21:25:32 INFO zookeeper.ClientCnxn: Attempting connection to server localhost/fe80:0:0:0:0:0:0:1%1:2181
10/01/18 21:25:32 WARN zookeeper.ClientCnxn: Exception closing session 0x0 to sun.nio.ch.SelectionKeyImpl@7284aa02
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:933)
10/01/18 21:25:32 WARN zookeeper.ClientCnxn: Ignoring exception during shutdown input
java.nio.channels.ClosedChannelException
        at sun.nio.ch.SocketChannelImpl.shutdownInput(SocketChannelImpl.java:638)
        at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:360)
        at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:999)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:970)
10/01/18 21:25:32 WARN zookeeper.ClientCnxn: Ignoring exception during shutdown output
java.nio.channels.ClosedChannelException
        at sun.nio.ch.SocketChannelImpl.shutdownOutput(SocketChannelImpl.java:649)
        at sun.nio.ch.SocketAdaptor.shutdownOutput(SocketAdaptor.java:368)
        at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1004)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:970)
10/01/18 21:25:32 WARN zookeeper.ZooKeeperWrapper: Failed to create /hbase -- check quorum servers, currently=localhost:2181
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:608)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.ensureExists(ZooKeeperWrapper.java:405)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.ensureParentExists(ZooKeeperWrapper.java:428)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.writeMasterAddress(ZooKeeperWrapper.java:516)
        at org.apache.hadoop.hbase.master.HMaster.writeAddressToZooKeeper(HMaster.java:263)
        at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:245)
        at org.apache.hadoop.hbase.LocalHBaseCluster.<init>(LocalHBaseCluster.java:94)
        at org.apache.hadoop.hbase.MiniHBaseCluster.init(MiniHBaseCluster.java:61)
        at org.apache.hadoop.hbase.MiniHBaseCluster.<init>(MiniHBaseCluster.java:53)
        at org.apache.hadoop.hbase.PerformanceEvaluation.runTest(PerformanceEvaluation.java:871)
        at org.apache.hadoop.hbase.PerformanceEvaluation.doCommandLine(PerformanceEvaluation.java:981)
        at org.apache.hadoop.hbase.PerformanceEvaluation.main(PerformanceEvaluation.java:1001)
10/01/18 21:25:34 INFO zookeeper.ClientCnxn: Attempting connection to server localhost/0:0:0:0:0:0:0:1:2181
10/01/18 21:25:34 WARN zookeeper.ClientCnxn: Exception closing session 0x0 to sun.nio.ch.SelectionKeyImpl@52a34783
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:933)
10/01/18 21:25:34 WARN zookeeper.ClientCnxn: Ignoring exception during shutdown input
java.nio.channels.ClosedChannelException
        at sun.nio.ch.SocketChannelImpl.shutdownInput(SocketChannelImpl.java:638)
        at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:360)
        at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:999)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:970)
10/01/18 21:25:34 WARN zookeeper.ClientCnxn: Ignoring exception during shutdown output
java.nio.channels.ClosedChannelException
        at sun.nio.ch.SocketChannelImpl.shutdownOutput(SocketChannelImpl.java:649)
        at sun.nio.ch.SocketAdaptor.shutdownOutput(SocketAdaptor.java:368)
        at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1004)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:970)
...
{code}

Over and over...",,stack,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26172,,,,,Wed Jul 16 18:57:44 UTC 2014,,,,,,,,,,"0|i02dr3:",11829,,,,,,,,,,,,,,,,,,,,,"16/Jul/14 18:57;stack;This works now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[shell] Now does \x20 for spaces,HBASE-2144,12445880,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,19/Jan/10 02:26,12/Oct/12 06:14,14/Jul/23 06:06,22/Jan/10 21:28,0.20.2,,,,,,,,,,,0.20.3,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/10 21:16;stack;2144-v2.patch;https://issues.apache.org/jira/secure/attachment/12431143/2144-v2.patch","22/Jan/10 18:25;stack;2144.patch;https://issues.apache.org/jira/secure/attachment/12431129/2144.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26171,Reviewed,,,,Fri Jan 22 21:28:32 UTC 2010,,,,,,,,,,"0|i08tan:",49346,,,,,,,,,,,,,,,,,,,,,"19/Jan/10 02:27;stack;From IRC:
{code}
02:26 < javarants> looks like it does that for a lot of low-ascii characters that could be displayed
02:26 < javarants> kind of like reading url encoding
02:26 < javarants>  info:source                 timestamp=1259693316583, value=\x3Ca\x20href\x3D\x22http:\x2F\x2Ftwitterfeed.com\
02:26 < javarants>                              x22\x20rel\x3D\x22nofollow\x22\x3Etwitterfeed\x3C\x2Fa\x3E   
{code};;;","19/Jan/10 02:28;stack;This is in 0.20.3RC2;;;","19/Jan/10 02:29;stack;Putting into 0.20.4 bucket.  Lets pull it into 0.20.3RC if we do another.;;;","22/Jan/10 18:25;stack;Patch that lets through more 'binaries':

{code}
hbase(main):007:0> put 'y', 'y', 'y:y', '<a href=""http://one.two.three/four/five"">'
0 row(s) in 0.0030 seconds
hbase(main):008:0> scan 'y'
ROW                          COLUMN+CELL                                                                      
 y                           column=y:y, timestamp=1264184643032, value=<a href=\x22http://one.two.three/four/
                             five\x22>    
{code};;;","22/Jan/10 21:16;stack;Don't include \t as per Ryan suggestion.  We could add double-quote but maybe leave it out for now.;;;","22/Jan/10 21:23;ryanobjc;remote the \t and then commit.  \t may be copy-and-pasted as multiple spaces and could make it hard to copy-and-paste region names (the whole point of this exercise).;;;","22/Jan/10 21:28;stack;Committed branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
findbugs issues - 2 performance warnings as suggested by findbugs,HBASE-2140,12445756,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kaykay.unique,kaykay.unique,kaykay.unique,17/Jan/10 07:44,20/Nov/15 13:01,14/Jul/23 06:06,17/Jan/10 20:09,,,,,,,,,,,,0.90.0,,,,,,,0,,,"* Integer.valueOf  favored instead of new Integer() 
 

* map.entrySet() favored instead of map.keySet()  
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/10 07:45;kaykay.unique;HBASE-2140.patch;https://issues.apache.org/jira/secure/attachment/12430540/HBASE-2140.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26170,Reviewed,,,,Fri Nov 20 13:01:25 UTC 2015,,,,,,,,,,"0|i0hgjz:",99944,,,,,,,,,,,,,,,,,,,,,"17/Jan/10 20:09;stack;Committed.  Thanks for the patch Kay Kay.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unknown metrics type,HBASE-2138,12445744,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,jdcryans,jdcryans,16/Jan/10 21:08,12/Oct/12 06:14,14/Jul/23 06:06,17/Jan/10 00:32,,,,,,,,,,,,0.20.3,0.90.0,,,,,,0,,,"Since the recent metric commits I see this on the master and RS at boot:

{code}
2010-01-16 11:24:59,730 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processName=RegionServer, sessionId=regionserver/10.10.1.49:60020
2010-01-16 11:24:59,732 ERROR org.apache.hadoop.metrics.MetricsUtil: unknown metrics type: org.apache.hadoop.hbase.metrics.MetricsRate
2010-01-16 11:24:59,732 ERROR org.apache.hadoop.hbase.metrics: unknown metrics instance: org.apache.hadoop.metrics.util.MetricsTimeVaryingRate
2010-01-16 11:24:59,732 ERROR org.apache.hadoop.hbase.metrics: unknown metrics instance: org.apache.hadoop.metrics.util.MetricsTimeVaryingRate
2010-01-16 11:24:59,732 ERROR org.apache.hadoop.hbase.metrics: unknown metrics instance: org.apache.hadoop.metrics.util.MetricsTimeVaryingRate
2010-01-16 11:24:59,732 ERROR org.apache.hadoop.hbase.metrics: unknown metrics instance: org.apache.hadoop.metrics.util.MetricsTimeVaryingRate
{code}

We need to clean that for 0.20.3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/10 23:20;stack;2138.patch;https://issues.apache.org/jira/secure/attachment/12430525/2138.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26169,Reviewed,,,,Sun Jan 17 19:44:29 UTC 2010,,,,,,,,,,"0|i08t5r:",49324,,,,,,,,,,,,,,,,,,,,,"16/Jan/10 22:01;stack;I see these in .out files:

{code}
java.lang.ArithmeticException: / by zero
        at org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.inc(MetricsTimeVaryingRate.java:112)
        at org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics.doUpdates(RegionServerMetrics.java:175)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext.timerEvent(AbstractMetricsContext.java:286)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext.access$000(AbstractMetricsContext.java:52)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext$1.run(AbstractMetricsContext.java:251)
        at java.util.TimerThread.mainLoop(Timer.java:555)
        at java.util.TimerThread.run(Timer.java:505)
{code}
;;;","16/Jan/10 22:01;stack;They keep repeating.;;;","16/Jan/10 22:10;larsgeorge;This seems to be caused by HBASE-2068. The new MetricsMBeanBase is not handling the Hadoop class and the MetricsDynamicMBeanBase in the Hadoop class is reporting the MetricsRate from the HBase package to be unknown (see createMBeanInfo):

{code}
      } else {
        MetricsUtil.LOG.error(""unknown metrics type: "" + o.getClass().getName());
      }
{code}

;;;","16/Jan/10 22:15;larsgeorge;I also checked and even if these are logged as ERROR they indeed seem to work. I tested to query ""requests"" from a 0.20.3RC RegionServer and got  a valid ""0.0"". ;;;","16/Jan/10 22:18;larsgeorge;The above error you see seems to go deeper. It seems that if inc() is called without any real numOps (on idle cluster?) there is no check for ""0"" and the division must fail.

{code}
  public synchronized void inc(final int numOps, final long time) {
    currentData.numOperations += numOps;
    currentData.time += time;
    long timePerOps = time/numOps;
    minMax.update(timePerOps);
  }
{code}

;;;","16/Jan/10 23:20;stack;Fixes the two issues above.

1. It doesn't let hbase Metrics go up to hadoop.  It instead passes hadoop a copy absent the hbase Metrics.
2. If no writes on hfile or hlog, don't call increment to avoid the ArithmeticException

;;;","17/Jan/10 00:07;jdcryans;Tested the patch, it removes all the errors and jmx works well.

Setting up jmx on my laptop I found that the new documentation has some inconsistencies, will open a new issue.;;;","17/Jan/10 00:10;jdcryans;Scratch that, it's just that my html doc is old;;;","17/Jan/10 00:32;jdcryans;Committed to branch and trunk. I also committed the updated doc for JMX (this should have been done earlier). I can't do that for trunk because there's a missing file (cygwin.xml it seems).;;;","17/Jan/10 13:29;larsgeorge;Just a +1 after the fact. Updated my 0.20.3RC cluster with patch and tested. Logs are clean now and JMX attributes still work. Thanks!;;;","17/Jan/10 19:44;stack;@Lars Thanks for testing jmx still works.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
javadoc warnings from 'javadoc' target ,HBASE-2137,12445724,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kaykay.unique,kaykay.unique,kaykay.unique,16/Jan/10 07:09,20/Nov/15 13:01,14/Jul/23 06:06,17/Jan/10 06:11,,,,,,,,,,,,0.90.0,,documentation,,,,,0,,,"Some javadoc warnings: 

  [javadoc]  ../workspace/hbase/src/java/org/apache/hadoop/hbase/mapreduce/IndexConfiguration.java:149: warning - @return tag has no arguments.
  [javadoc] ../workspace/hbase/src/java/org/apache/hadoop/hbase/mapreduce/IndexConfiguration.java:149: warning - Tag @see: reference not found: Use #isAnalyze(String) for replacement.
  [javadoc] ../workspace/hbase/src/java/org/apache/hadoop/hbase/mapreduce/IndexConfiguration.java:159: warning - Tag @see: reference not found: Use #setAnalyze(String, boolean) for replacement.
  [javadoc] ../workspace/hbase/src/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java:104: warning - Tag @link: can't find getReader(org.apache.hadoop.fs.FileSystem,
  [javadoc]  org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HBaseConfiguration) in org.apache.hadoop.hbase.regionserver.wal.HLog
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/10 07:25;kaykay.unique;HBASE-2137.patch;https://issues.apache.org/jira/secure/attachment/12430493/HBASE-2137.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26168,,,,,Fri Nov 20 13:01:54 UTC 2015,,,,,,,,,,"0|i0hgjj:",99942,,,,,,,,,,,,,,,,,,,,,"17/Jan/10 06:11;stack;Committed to TRUNK.  Thanks for the patch Kay Kay.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant javadoc complains about missing classes ,HBASE-2135,12445695,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kaykay.unique,kaykay.unique,kaykay.unique,15/Jan/10 22:48,20/Nov/15 13:01,14/Jul/23 06:06,17/Jan/10 06:12,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Something to do with CP  - javadoc target not happy with CP ( in trunk). 

placeholder ticket to revisit it. ",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/10 07:08;kaykay.unique;HBASE-2135.patch;https://issues.apache.org/jira/secure/attachment/12430489/HBASE-2135.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26167,Reviewed,,,,Fri Nov 20 13:01:52 UTC 2015,,,,,,,,,,"0|i0hgj3:",99940,,,,,,,,,,,,,,,,,,,,,"16/Jan/10 07:08;kaykay.unique;javadoc cp errors are fixed. 

;;;","17/Jan/10 06:12;stack;Committed to TRUNK.  Thanks for the patch Kay Kay.;;;","17/Jan/10 06:18;kaykay.unique;Thanks stack. Just curious  - can you help double check if this patch is in trunk / different branch ? ;;;","17/Jan/10 20:14;stack;Its in TRUNK for me:

{code}
  <!-- Javadoc -->
  <target name=""javadoc"" description=""Generate javadoc"">
    <mkdir dir=""${build.javadoc}""/>
    <javadoc
      overview=""${src.dir}/overview.html""
      packagenames=""org.apache.hadoop.hbase.*""
      destdir=""${build.javadoc}""
      author=""true""
      version=""true""
      use=""true""
      windowtitle=""${Name} ${version} API""
      doctitle=""${Name} ${version} API""
      bottom=""Copyright &amp;copy; ${year} The Apache Software Foundation""
      >
      <packageset dir=""${src.dir}"">
          <include name=""org/apache/**""/>
          <exclude name=""org/onelab/**""/>
          <exclude name=""org/apache/hadoop/hbase/thrift/generated/**""/>
      </packageset>
        <link href=""${javadoc.link.java}""/>
        <classpath >
          <path refid=""classpath"" />
          <fileset dir=""${build.dir}/contrib"">
            <include name=""**/ivy/lib/common/*.jar"" />
          </fileset>
          <pathelement path=""${java.class.path}""/>
        </classpath>
      <packageset dir=""src/contrib/transactional/src/java""/>
      <packageset dir=""src/contrib/stargate/src/java""/>
    </javadoc>
  </target>
{code}

Above is quote from my current build.xml.

Whats up?;;;","17/Jan/10 21:10;kaykay.unique;{quote}
Its in TRUNK for me:
{quote}
My bad - Sorry . did not see build.xml in the patchset - hence was asking. please ignore. 

;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ivy nit regarding checking with latest snapshots ,HBASE-2134,12445673,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kaykay.unique,kaykay.unique,kaykay.unique,15/Jan/10 20:37,20/Nov/15 13:01,14/Jul/23 06:06,15/Jan/10 21:23,,,,,,,,,,,,0.90.0,,,,,,,0,,,"Currently - if a new jar gets published in one of the dependent m2 snapshots - ivy does not retrieve it unless the cache is cleared.  Add changing=""true"" to the dependency.  ( There has to be an alternate way to do it at the resolver level, but for now this works without a hitch). 
",,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/10 20:37;kaykay.unique;HBASE-2134.patch;https://issues.apache.org/jira/secure/attachment/12430433/HBASE-2134.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26166,,,,,Fri Nov 20 13:01:23 UTC 2015,,,,,,,,,,"0|i0hgiv:",99939,,,,,,,,,,,,,,,,,,,,,"15/Jan/10 20:37;kaykay.unique;Add changing=""true"" to all m2 snapshot dependencies ;;;","15/Jan/10 21:23;apurtell;Got it. Thanks Kay Kay.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant tar build broken since switch to Ivy,HBASE-2128,12445582,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kaykay.unique,larsfrancke,larsfrancke,15/Jan/10 01:50,20/Nov/15 13:01,14/Jul/23 06:06,13/Feb/10 04:10,0.90.0,,,,,,,,,,,0.90.0,,build,,,,,0,,,"Running ant tar produces a very small tar file because all .jar dependencies are missing. This happens since the switch to Ivy.

Adding common.ivy.lib.dir to the build.xml fixes some of it but some things still don't work:
{code:xml}
    <mkdir dir=""${dist.dir}/lib""/>
    <copy todir=""${dist.dir}/lib"">
      <fileset dir=""${build.lib}"" />
      <fileset dir=""${common.ivy.lib.dir}""/>
    </copy>
{code}

The jars for the contrib apps still seem to be missing. At the moment this is only stargate but the I've got the same problem for the new thrift contrib. I am afraid I don't know enough about Ant or Ivy to be of any further assistance.",,kaykay.unique,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/10 07:17;kaykay.unique;HBASE-2128.patch;https://issues.apache.org/jira/secure/attachment/12430490/HBASE-2128.patch","15/Jan/10 22:46;kaykay.unique;HBASE-2128.patch;https://issues.apache.org/jira/secure/attachment/12430450/HBASE-2128.patch","15/Jan/10 07:52;kaykay.unique;HBASE-2128.patch;https://issues.apache.org/jira/secure/attachment/12430374/HBASE-2128.patch","15/Jan/10 07:25;kaykay.unique;HBASE-2128.patch;https://issues.apache.org/jira/secure/attachment/12430369/HBASE-2128.patch",,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26165,Reviewed,,,,Fri Nov 20 13:01:46 UTC 2015,,,,,,,,,,"0|i0hgi7:",99936,,,,,,,,,,,,,,,,,,,,,"15/Jan/10 02:47;kaykay.unique;Will try to look further. 

But the tarball for a release is expected to be smaller after HBASE-1433, because the lib\*.jars are not supposed to be released , but will be retrieved on demand. 

zk and thrift are retained only because their artifacts have not yet been published. 
;;;","15/Jan/10 03:12;ryanobjc;Tar is supposed to be self contained because it is the basis of our packaging and release. We are not going to force our users to download all the deps on their prod machines.;;;","15/Jan/10 07:25;kaykay.unique;*) tar now maintains 3 different sets - lib/core/*.jar , lib/transactional/*.jar , lib/stargate/*.jar 
   
  Thrift + zookeeper , that are common to all of them is in lib only. 

*) Bug in init target due to space in libthrift* , not being copied

*) unnecessary classpath element in javadoc target removed since no such file exists ( contrib/**/*.jar) in the tree anymore. 


Having said that - the scripts need to be modified (or we need to identify a file-layout structure) since the scripts are meant for build/ivy/common  -  build/contrib/stargate/ivy/lib/common etc. , for a development environment. 

When we release - may be we need to revisit what structure we would want and refactor scripts accordingly. ;;;","15/Jan/10 07:52;kaykay.unique;next version of the same

In addition to the previous , 

- added lib/core/*.jar to CP in the hbase script as well- that should work with the release tarball. 

The CP entries are - 

lib/(thrift/zookeeper)
lib/core/*.jar

HBase team - feel free to swap the order if that does not sound right. 

Meanwhile - the CP jars of stargate / transactional are kept in a separate namespace altogether - not affecting the scripts. 

As the contrib packages increase - it would be useful to keep core and the launching script in a separate namespace . 
;;;","15/Jan/10 16:18;larsfrancke;bq. Meanwhile - the CP jars of stargate / transactional are kept in a separate namespace altogether - not affecting the scripts.

I don't know if you are doing this intentionally but now the core jars are duplicated in the lib/* folders. This not only doubles the size of the final tar (from 35 MB to 70 MB) but it seems to cause problems with SLF4J (which will have to be introduced as a new dependency for Thrift 0.2). The latter may be my fault though, I'm struggling to get everything running as it was before the switch to Ivy. I'll comment again if it turns out the problem was caused by me.

Either way the duplication should be unnecessary. At least I can't imagine why it would be required.

The src/contrib/stargate/lib folder is empty and can be deleted/does not need to be created at all.

Thanks for your quick fix!;;;","15/Jan/10 17:37;kaykay.unique;{quote}
I don't know if you are doing this intentionally but now the core jars are duplicated in the lib/* folders. 
{quote}
As I mentioned earlier - it was intentional to separate the CP namespace for core / transactional / .. <other contribs> . 

If that is too much of a complexity then - modify the tar patch such that all the target files  - dependency jars , from ivy, get copied to lib/*.jar directory. 

{quote}
The src/contrib/stargate/lib folder is empty and can be deleted/does not need to be created at all.
{quote}
true. thanks. 


{quote}
 I'm struggling to get everything running as it was before the switch to Ivy. I'll comment again if it turns out the problem was caused by me.
{quote}
Feel free to log tickets / add more details to the broken parts so that we can fix the same. ;;;","15/Jan/10 18:18;apurtell;bq. The src/contrib/stargate/lib folder is empty and can be deleted/does not need to be created at all.

I found if it was not there, then ivy would error out. 
;;;","15/Jan/10 18:29;stack;Maybe we need to add a mkdir somewhere?  mkdir after copy of stargate to build for compile?;;;","15/Jan/10 22:30;kaykay.unique;So - what is the consensus we have regarding the directory structure of release , as far as lib directory is concerned ? 

If we want to have all the jars in lib/*.jar as before - I am ok , but keep in mind as we add more contribs we may want to separate the CP namespaces for them to make thing easy for debugging. 

| The src/contrib/stargate/lib folder is empty and can be deleted/does not need to be created at all.

| Maybe we need to add a mkdir somewhere? mkdir after copy of stargate to build for compile?

I am ok with one way or another , but if it is distracting at this point - ok with maintaining current status quo while removing any CP reference in build.xml files to  contrib/**/lib/*.jar , since no such file exists / should be there in future. 
;;;","15/Jan/10 22:46;kaykay.unique;Patch that gets all jars in pre-Ivy stage - ( all jars in lib/*.jar ) . 
;;;","16/Jan/10 04:48;larsfrancke;My preferred way of doing this would be to keep only the extra dependencies for the contribs in their own folders.

So all core dependencies in lib/core all extra dependencies for stargate in lib/stargate but not the core dependencies. If I remember correctly this was the way it was done before. At least almost. If one decides to use stargate all you need to do is to add one directory to the classpath.

In all likelihood I'm missing something important here but I don't see why all jars would have to be duplicated. For 0.21 we'll probably have three contribs with dependencies (I'm not counting ec2): stargate, transactional and thrift. This would mean that all common jars would be included four times in the final tar.

All dependencies in one folder doesn't seem right either.

But I'm still very new to this project so feel free to disregard all I've said :);;;","16/Jan/10 05:10;kaykay.unique;{quote}
My preferred way of doing this would be to keep only the extra dependencies for the contribs in their own folders.
{quote}

That would be the ideal thing I guess. That would mean - revisiting build.xml / build-contrib.xml to reuse the files, to create only the 'diff' dependencies.  Given that 'tar' is broken - I am trying to look for something quick and immediate. 

{quote}
In all likelihood I'm missing something important here but I don't see why all jars would have to be duplicated
{quote}
Agree - that was just meant to have contrib-s as separated namespace. If the previous issue were resolved - then having core and contribs separate would be ideal. 

{quote}
All dependencies in one folder doesn't seem right either.
{quote}
Yup. that was my main concern. 

So - did you look at the most recent patch ( all in one lib/*.jar) minimally intrusive to the scripts - but need to be revisited in the long term though. 


;;;","16/Jan/10 05:22;ryanobjc;the contrib method in general needs to be reworked, i think its way out of scope here, so if we can do more or less what we were doing before, then i say it is fixed. I'm not sure how important it is to tease out the Classpaths, my initial thought it 'doesnt matter'. 

;;;","16/Jan/10 07:17;kaykay.unique;Do one at a time. Do not deal with javadoc issue here. See  HBASE-2135 for javadoc cp issue ;;;","16/Jan/10 09:23;apurtell;bq. For 0.21 we'll probably have three contribs with dependencies (I'm not counting ec2): stargate, transactional and thrift. 

Don't count out EC2. At some point the bash stuff will be deprecated and there will be a Python/libcloud based approach replacing it, like what Hadoop Core is doing. So the EC2 stuff is likely to pull dependencies. ;;;","16/Jan/10 09:31;ryanobjc;i think it would be better if ec2 wasnt part of the core build stuff. seems kind of weird...;;;","16/Jan/10 10:06;ryanobjc;while we are at it, we should turn stdout/stderr output buffering, since during an ivy build the 'download' does not actually print increment progress during 1 line.;;;","16/Jan/10 19:07;apurtell;bq. i think it would be better if ec2 wasnt part of the core build stuff. seems kind of weird... 

It's not. It's a contrib -- src/contrib/ec2/ ... Or did I miss the point? ;;;","16/Jan/10 21:11;kaykay.unique;{quote}
while we are at it, we should turn stdout/stderr output buffering, since during an ivy build the 'download' does not actually print increment progress during 1 line.
{quote}

makes sense. would it be ok to take it in a separate issue though ? ;;;","12/Feb/10 22:49;larsfrancke;Kay Kay's latest patch seems to work perfect in regards to the broken ant tar build. Is it ready to be committed?;;;","12/Feb/10 23:26;stack;I should commit this?;;;","13/Feb/10 00:02;ryanobjc;stack: do it, make it happen.;;;","13/Feb/10 04:10;stack;Applied to TRUNK.  Thanks for the patch Kay Kay.;;;","13/Feb/10 08:29;kaykay.unique;Thanks for taking this patch. When I submitted this patch - there were only 2 contribs in this - that I had hardcoded ( stargate / transactional ). But with the addition of mdc_replication now, an entry might need to be added for that as well. I can track it separately. 
;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
randomWrite mode of PerformanceEvaluation benchmark program writes only to a small range of keys,HBASE-2127,12445575,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,Fixed,,kannanm,kannanm,15/Jan/10 00:41,12/Oct/12 06:14,14/Jul/23 06:06,15/Jan/10 01:03,0.20.2,,,,,,,,,,,0.20.3,0.90.0,test,,,,,0,,,"""randomWrite"" mode of PerformanceEvaluation (PE), with nclients > 1, does random writes only within a small range rather than across all rows.

e.g, for:

./bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --rows=1000 randomWrite 10

[Note: As per convention in all modes of PE, --rows is the number of rows per client]

So for the above, with # of clients set to 10, currently all clients generate writes to keys in the 0..999 range instead of the 0..9999 range.

[Fix appears to be simple. Will provide a patch.]",not relevant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/10 00:46;kannanm;2127.0.20.patch;https://issues.apache.org/jira/secure/attachment/12430330/2127.0.20.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26164,Reviewed,,,,Fri Jan 15 01:03:40 UTC 2010,,,,,,,,,,"0|i08tef:",49363,,,,,,,,,,,,,,,,,,,,,"15/Jan/10 01:03;stack;Committed to branch and trunk.  Thanks for the patch Kannan.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix build break - ec2,HBASE-2126,12445571,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kaykay.unique,kaykay.unique,kaykay.unique,14/Jan/10 23:52,11/Jun/22 23:02,14/Jul/23 06:06,15/Jan/10 01:23,,,,,,,,,,,,,,,,,,,0,,,"Because all contrib/** reuses build-contrib.xml -, internally they reuse ivy-retrieve.xml and hence need the presence of an ivy.xml in ec2 directory to succeed. 

Temporary patch with no dependencies in . Ideal patch should be to refactor build\*.xml as appropriate. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/10 23:53;kaykay.unique;HBASE-2126.patch;https://issues.apache.org/jira/secure/attachment/12430315/HBASE-2126.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26163,Reviewed,,,,Fri Jan 15 01:23:40 UTC 2010,,,,,,,,,,"0|i0hghz:",99935,,,,,,,,,,,,,,,,,,,,,"14/Jan/10 23:53;kaykay.unique;no dependencies whatsoever. empty ivy.xml patch to fix build (contrib/ec2 ) ;;;","15/Jan/10 01:23;jdcryans;Thanks Kay Kay, committed to trunk right when I needed it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove 'master' command-line option from PE.,HBASE-2123,12445480,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,14/Jan/10 06:12,12/Oct/12 06:14,14/Jul/23 06:06,14/Jan/10 06:17,,,,,,,,,,,,0.20.3,0.90.0,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26161,,,,,Thu Jan 14 06:17:05 UTC 2010,,,,,,,,,,"0|i08t73:",49330,,,,,,,,,,,,,,,,,,,,,"14/Jan/10 06:17;stack;Committed branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Stargate] Initializing scanners column families doesn't work,HBASE-2122,12445474,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,greglu,greglu,greglu,14/Jan/10 05:01,12/Oct/12 06:14,14/Jul/23 06:06,14/Jan/10 10:42,0.20.2,,,,,,,,,,,0.20.3,0.90.0,,,,,,0,,,"Similar to HBASE-2120 the handling of column families with scanner does not work correctly.

The issue is in ScannerResultGenerator.java (line 62), and I'm attaching a patch for both trunk and the 0.20 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/10 05:02;greglu;HBASE-2122_0.20.3.patch;https://issues.apache.org/jira/secure/attachment/12430228/HBASE-2122_0.20.3.patch","14/Jan/10 05:02;greglu;HBASE-2122_trunk.patch;https://issues.apache.org/jira/secure/attachment/12430227/HBASE-2122_trunk.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26160,Reviewed,,,,Thu Jan 14 10:42:22 UTC 2010,,,,,,,,,,"0|i08t6n:",49328,,,,,,,,,,,,,,,,,,,,,"14/Jan/10 10:42;apurtell;Applied to trunk and 0.20 branch. Thanks for the fixes Greg!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Stargate] Unable to delete column families,HBASE-2120,12445463,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,greglu,greglu,greglu,14/Jan/10 00:44,12/Oct/12 06:14,14/Jul/23 06:06,14/Jan/10 01:39,0.20.2,,,,,,,,,,,0.20.3,0.90.0,,,,,,0,,,"When trying to delete a column family using Stargate, the following occurs (curl command + Stargate logging):

> curl http://localhost:8080/books/ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute/5426359469582345882 -X DELETE
---
10/01/13 18:57:38 DEBUG stargate.RowResource: DELETE http://localhost:8080/books/ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute/5426359469582345882
10/01/13 18:57:38 DEBUG stargate.RowResource: DELETE row=ff417a5b-c4d0-4a43-b1c7-94c356fe0b72, ts=9223372036854775807, families={(family=attribute, keyvalues=(ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute:/5426359469582345882/DeleteColumn/vlen=0)}
---

> curl http://localhost:8080/books/ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute:/5426359469582345882 -X DELETE
---
10/01/13 18:57:49 DEBUG stargate.RowResource: DELETE http://localhost:8080/books/ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute:/5426359469582345882
10/01/13 18:57:49 DEBUG stargate.RowResource: DELETE row=ff417a5b-c4d0-4a43-b1c7-94c356fe0b72, ts=9223372036854775807, families={(family=attribute, keyvalues=(ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute:/5426359469582345882/DeleteColumn/vlen=0)}
---

Both are attempting to delete columns instead of the intended action of deleting families. The problem occurs because RowResource.java (line 282) will always return a split of length 2, since RowSpec.java (line 122) appends a colon if it's missing.

I've patched it so that a check will occur if the second split's (split[1]) length is 0 and acts accordingly. I'll attach the patch after I've run it against the test suite.

P.S. It's my first time submitting a patch, so let me know if I screwed anything up.","Ubuntu, Java build 1.6.0_16-b01",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/10 01:11;greglu;HBASE-2120.patch;https://issues.apache.org/jira/secure/attachment/12430205/HBASE-2120.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26158,Reviewed,,,,Thu Jan 14 01:39:48 UTC 2010,,,,,,,,,,"0|i08t7j:",49332,,,,,,,,,,,,,,,,,,,,,"14/Jan/10 01:11;greglu;This patch is for the 0.20 branch. You could just apply it by hand to trunk.;;;","14/Jan/10 01:39;apurtell;Committed to trunk and 0.20 branch. Thanks for the diagnosis and patch Greg! ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix top-level NOTICES.txt file.  Its stale.,HBASE-2119,12445380,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,13/Jan/10 06:02,12/Oct/12 06:14,14/Jul/23 06:06,13/Jan/10 06:49,,,,,,,,,,,,0.20.3,0.90.0,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26157,,,,,Wed Jan 13 06:49:34 UTC 2010,,,,,,,,,,"0|i08t53:",49321,,,,,,,,,,,,,,,,,,,,,"13/Jan/10 06:49;stack;Made changes in branch and trunk (Removed one-lab from branch and removed it and the michael gottesman json stuff from trunk -- we use jackson now).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix our wikipedia page; says we're slow among other errors",HBASE-2118,12445319,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,12/Jan/10 17:25,11/Jun/22 23:02,14/Jul/23 06:06,04/Apr/11 19:30,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26156,,,,,Mon Apr 04 19:30:59 UTC 2011,,,,,,,,,,"0|i0hghj:",99933,,,,,,,,,,,,,,,,,,,,,"13/Jan/10 01:00;hammer;HDFS wikipedia page is ornery too. I think there are some frightened enterprise software companies looking to spread some FUD...;;;","13/Jan/10 01:05;stack;.bq ""I think there are some frightened enterprise software companies looking to spread some FUD....""

Thats funny.;;;","04/Apr/11 19:24;posix4e;Can we close this out?;;;","04/Apr/11 19:30;stack;Resolving.  Page is a bit better now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
./hbase shell would not launch due to missing jruby dependency,HBASE-2115,12445250,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kaykay.unique,kaykay.unique,kaykay.unique,12/Jan/10 01:56,11/Jun/22 23:19,14/Jul/23 06:06,12/Jan/10 02:04,,,,,,,,,,,,,,,,,,,0,,,"./hbase shell - gave some error due to missing org/jruby/.. etc. 


Add the following to ivy.xml - 
   <dependency org=""org.jruby"" name=""jruby-complete""
              rev=""${jruby.version}"" conf=""common->default"" />    

Also - in HBASE-2114 - there is an issue about missing log4j configuration as well. ( due to the fact that build/ivy/lib/common did not contain log4j , but build/ivy/lib/test did ). 

This patch addresses either of them. 

affects trunk  
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/10 02:02;kaykay.unique;HBASE-2115.patch;https://issues.apache.org/jira/secure/attachment/12429976/HBASE-2115.patch","12/Jan/10 01:57;kaykay.unique;HBASE-2115.patch;https://issues.apache.org/jira/secure/attachment/12429975/HBASE-2115.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26155,Reviewed,,,,Tue Jan 12 02:04:38 UTC 2010,,,,,,,,,,"0|i0hggv:",99930,,,,,,,,,,,,,,,,,,,,,"12/Jan/10 01:57;kaykay.unique;Artifact of HBASE-1433;;;","12/Jan/10 02:02;kaykay.unique;Revised patch after HBASE-2114.patch went in;;;","12/Jan/10 02:04;kaykay.unique;Sorry - my bad !! Did not check the scripts in ./bin/ at all. ;;;","12/Jan/10 02:04;jdcryans;Thanks Kay Kay! Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't start HBase in trunk,HBASE-2114,12445246,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,jdcryans,jdcryans,12/Jan/10 01:08,20/Nov/15 13:01,14/Jul/23 06:06,12/Jan/10 01:57,,,,,,,,,,,,0.90.0,,,,,,,0,,,"The new Configuration update HBASE-2036 didn't change the main methods in HRegionServer and HMaster, can't start hbase.",,kaykay.unique,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/10 01:44;kaykay.unique;HBASE-2114-ivy-log4j.patch;https://issues.apache.org/jira/secure/attachment/12429974/HBASE-2114-ivy-log4j.patch","12/Jan/10 01:21;jdcryans;HBASE-2114.patch;https://issues.apache.org/jira/secure/attachment/12429971/HBASE-2114.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26154,Reviewed,,,,Fri Nov 20 13:01:24 UTC 2015,,,,,,,,,,"0|i0hggn:",99929,,,,,,,,,,,,,,,,,,,,,"12/Jan/10 01:21;jdcryans;This patch fixes both HMaster and HRS. It uncovers what seems like a problem with Ivy:

{code}
Caused by: java.lang.NoClassDefFoundError: org/apache/log4j/Level
        at org.apache.hadoop.mapred.JobConf.<clinit>(JobConf.java:336)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:247)
        at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1070)
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:86)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:70)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:123)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1751)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:71)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:1780)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1768)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:195)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:103)
        at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:170)
{code}

I ant cleaned, rm -rf'ed my ivy cache too still doesn't fetch log4j or something.;;;","12/Jan/10 01:44;kaykay.unique;log4j moved from test config to common config to address issue mentioned. ;;;","12/Jan/10 01:46;kaykay.unique;Try the ivy patch attached. log4j was not available in lib/common , but in lib/test . (Not sure - from where is this being invoked - as a test case , from junit ). ;;;","12/Jan/10 01:57;jdcryans;Thanks Kay Kay! I committed both patches to trunk.;;;","20/Nov/15 13:01;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"For indexed contrib, fast-forward to next row if no more results left... big performance improvement",HBASE-2113,12445240,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,12/Jan/10 00:33,12/Oct/12 06:14,14/Jul/23 06:06,13/Jan/10 06:44,,,,,,,,,,,,0.20.3,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/10 00:35;stack;region-scanner.patch;https://issues.apache.org/jira/secure/attachment/12429964/region-scanner.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26153,,,,,Wed Jan 13 06:44:00 UTC 2010,,,,,,,,,,"0|i08t93:",49339,,,,,,,,,,,,,,,,,,,,,"12/Jan/10 00:35;stack;Adds jumping to next row if we know we'll find no more results in current row.  Needed to get the speeds up in 'indexed' hbase.  Adds a test too.;;;","13/Jan/10 06:44;stack;Applied to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New 'indexed' contrib is missing commons-lang.jar when packaged,HBASE-2112,12445239,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,12/Jan/10 00:30,12/Oct/12 06:14,14/Jul/23 06:06,13/Jan/10 06:41,,,,,,,,,,,,0.20.3,,,,,,,0,,,The packaged hbase is missing commons-lang in the new indexed contrib.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/10 00:31;stack;add-indexed-lib-to-tarball.patch;https://issues.apache.org/jira/secure/attachment/12429963/add-indexed-lib-to-tarball.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26152,,,,,Wed Jan 13 06:41:56 UTC 2010,,,,,,,,,,"0|i08t8n:",49337,,,,,,,,,,,,,,,,,,,,,"12/Jan/10 00:31;stack;Fix for broke packaging.;;;","13/Jan/10 06:41;stack;Applied to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Move to ivy broke our being able to run in-place; i.e. ./bin/start-hbase.sh in a checkout.",HBASE-2111,12445238,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,stack,stack,stack,12/Jan/10 00:19,11/Jun/22 23:19,14/Jul/23 06:06,12/Jan/10 00:24,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/10 00:20;stack;2111.patch;https://issues.apache.org/jira/secure/attachment/12429961/2111.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26151,,,,,Tue Jan 12 00:47:45 UTC 2010,,,,,,,,,,"0|i0hggf:",99928,,,,,,,,,,,,,,,,,,,,,"12/Jan/10 00:24;stack;Committed TRUNK.  Patch adds build/ivy/lib to CLASSPATH.;;;","12/Jan/10 00:47;kaykay.unique;Oops. Sorry. Also technically - we would be adding only build/ivy/common/*.jar  in deployments ( build/ivy/test/*.jar would  be used only by the junit classpath) . If there are exceptions - we can revisit the ivy.xml as appropriate. 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"status 'simple' should show total requests per second, also the requests/sec is wrong as is",HBASE-2109,12445232,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,ryanobjc,ryanobjc,11/Jan/10 23:08,11/Jun/22 23:19,14/Jul/23 06:06,20/Jan/10 02:06,0.20.3,0.90.0,,,,,,,,,,,,,,,,,0,,,"status 'simple' doesnt give us aggregate load, leaving the user to add up numbers by hand. 

Futhermore, the per-server requests numbers are off, too high by a factor of 3 - they are using the default toString() which assumes a 1 second report rate, when the shipping default is 3 seconds.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/10 00:02;ryanobjc;HBASE-2109-v2.patch;https://issues.apache.org/jira/secure/attachment/12430820/HBASE-2109-v2.patch","11/Jan/10 23:09;ryanobjc;HBASE-2109.patch;https://issues.apache.org/jira/secure/attachment/12429947/HBASE-2109.patch",,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26149,,,,,Wed Jan 20 02:06:48 UTC 2010,,,,,,,,,,"0|i0hgfz:",99926,,,,,,,,,,,,,,,,,,,,,"11/Jan/10 23:22;stack;+1

... though you might ask the configuration what the value for interval is, especially since its changed recently from 3 to 1 IIRC.;;;","13/Jan/10 00:07;ryanobjc;and right you are. i'll update it as such;;;","20/Jan/10 00:01;ryanobjc;here is a revised patch that doesnt do the 3 second thing. 

hopefully we can revamp the stats system to not be dependent on any tick time-like value.;;;","20/Jan/10 00:01;ryanobjc;here is a revised patch that doesnt do the 3 second thing. 

hopefully we can revamp the stats system to not be dependent on any tick time-like value.;;;","20/Jan/10 00:02;ryanobjc;here is a revised patch that doesnt do the 3 second thing. 

hopefully we can revamp the stats system to not be dependent on any tick time-like value.;;;","20/Jan/10 01:28;stack;+1;;;","20/Jan/10 02:06;ryanobjc;i committed this, only changing HBase.rb in the end for the summary.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KeyValueSortReducer collapses all values to last passed,HBASE-2101,12445115,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,,stack,stack,09/Jan/10 19:23,12/Oct/12 06:14,14/Jul/23 06:06,13/Jan/10 06:38,,,,,,,,,,,,0.20.3,0.90.0,,,,,,0,,,"From mailing list by Ioannis Konstantinou:

{code}The problem is in the class org.apache.hadoop.hbase.mapreduce.KeyValueSortReducer. 
When you add keyvalues to the treeset for sorting, you need to add keyvalue clones instead of just references. What happens now, is that in every iteration, the value that exists in the treeset gets replaced with the new value.


So, you need to replace line 41 ( map.add(kv);) 
with this line:   map.add(kv.clone())

in this case, the treeset populates correcty.{code}

I filed this against 0.20.3 so if we have to cut a new RC, we can include this fix too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/10 19:43;stack;2102.patch;https://issues.apache.org/jira/secure/attachment/12429828/2102.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26144,Reviewed,,,,Wed Jan 13 06:38:33 UTC 2010,,,,,,,,,,"0|i08tbj:",49350,,,,,,,,,,,,,,,,,,,,,"13/Jan/10 06:38;stack;Committed to trunk and branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock between HRegion.put and HRegion.close,HBASE-2097,12445000,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,stack,jdcryans,jdcryans,08/Jan/10 06:52,12/Oct/12 06:14,14/Jul/23 06:06,09/Jan/10 00:27,,,,,,,,,,,,0.20.3,,,,,,,0,,,"HBASE-2037 added a bunch of fixes but also a deadlock:

HRegion.put:
{code}
splitsAndClosesLock.readLock().lock();
newScannerLock.writeLock().lock();
{code}

HRegion.close
{code}
newScannerLock.writeLock().lock();
try {
  splitsAndClosesLock.writeLock().lock();
{code}

To recreate, start a PerformanceEvaluation on standalone and it happens roughly 75% of the time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/10 20:56;stack;2097.patch;https://issues.apache.org/jira/secure/attachment/12429785/2097.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26143,Reviewed,,,,Sat Jan 09 00:27:18 UTC 2010,,,,,,,,,,"0|i08tev:",49365,,,,,,,,,,,,,,,,,,,,,"08/Jan/10 20:56;stack;Locks being done in wrong order.   This patch should fix it.   Testing.;;;","08/Jan/10 23:38;stack;To reproduce -- from J-D -- run in standalone mode (I couldn't repro on cluster).  After reproducing on first run, after application of this patch, hang hasn't happened after trying sequentialWrite 4 times.;;;","09/Jan/10 00:27;jdcryans;I also tried the patch a couple of times and the deadlock is gone. I'm committing the patch to branch plus I removed some annoying debug (MemStoreScanner) that was also added in HBASE-2037.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hbase-2037 breaks mapreduce jobs going from 0.20.2 to 0.20.3,HBASE-2094,12444831,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Blocker,Fixed,,stack,stack,06/Jan/10 19:35,12/Oct/12 06:14,14/Jul/23 06:06,07/Jan/10 07:19,,,,,,,,,,,,0.20.3,,,,,,,0,,,hbase-2037 makes it so commons-lang is now needed on mr classpath.  Can't have fellas having to change their CLASSPATH on point release update.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/10 07:17;stack;HBASE-2094.patch;https://issues.apache.org/jira/secure/attachment/12429631/HBASE-2094.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26141,,,,,Thu Jan 07 07:19:26 UTC 2010,,,,,,,,,,"0|i08tfr:",49369,,,,,,,,,,,,,,,,,,,,,"06/Jan/10 21:52;stack;I ain't sure how to fix this.  The commons-lang addition is needed so Pair equals and hashcode works; as is its broke.  We need the fat commons-lang because it knows how to create hash codes for different types (Pair is generic so we need something that can work with its generaliity).  It then knows how to do the equals properly handling arrays of objects if passed.  I looked at pulling in pieces of commons-lang only but its hairball.  It looks like I'd end up pulling in all of commons-lang (internally they refer to each other).

Either we pull hbase-2037 -- a working comparable Pair seems pivotal to hbase-2037 -- or we expect that updating from 0.20.2 to 0.20.3, fellas need to update their MR CLASSPATHs.  I'm leaning toward the former.;;;","06/Jan/10 21:55;jdcryans;<idiotcomment> Could we just pull the needed class into our own source for 0.20?</idiotcomment>;;;","06/Jan/10 21:56;stack;Lets wait a bit on this.  Will take another look later.;;;","06/Jan/10 22:06;stack;@idiot Yeah, I was on that idiot-path for a good while but commons-lang is a hairball, at least where this hashcode making and equals is concerned.;;;","07/Jan/10 07:17;stack;Patch that removes commons-lang from core and adds it to src/contrib/indexed/lib.  Copies whats needed from commons-lang up into Pair -- about 100 lines.;;;","07/Jan/10 07:18;stack;All tests pass.  Committing.;;;","07/Jan/10 07:19;stack;Committed to branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[stargate] RowSpec parse bug,HBASE-2093,12444751,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,apurtell,apurtell,apurtell,05/Jan/10 21:58,12/Oct/12 06:14,14/Jul/23 06:06,08/Jan/10 05:25,,,,,,,,,,,,0.20.3,0.90.0,,,,,,0,,,"From mike anderson up on hbase-user@:

{quote}
Trying to create a row in hbase from ruby with row key ""http://www.google.com"" produces this exception

{noformat}
2010-01-05 11:40:53.972::WARN:  /cached_web_pages/http%3A%2F%
2Fwww.google.com%2F/
java.lang.IllegalArgumentException: java.lang.NumberFormatException: For
input string: ""www.google.com""
at org.apache.hadoop.hbase.stargate.RowSpec.parseTimestamp(RowSpec.java:171)
at org.apache.hadoop.hbase.stargate.RowSpec.(RowSpec.java:55)
at org.apache.hadoop.hbase.stargate.RowResource.(RowResource.java:62)
at org.apache.hadoop.hbase.stargate.TableResource.getRowResource(TableResource.java:117)
[...]
{noformat}

{quote}

Move to 0.20.4 if this gets in the way of the 0.20.3 release before I can look at it.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/10 05:01;apurtell;HBASE-2093.patch;https://issues.apache.org/jira/secure/attachment/12429717/HBASE-2093.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26140,Reviewed,,,,Wed Feb 23 07:34:34 UTC 2011,,,,,,,,,,"0|i08thb:",49376,,,,,,,,,,,,,,,,,,,,,"07/Jan/10 18:29;jdcryans;About to release 0.20.3 so punting to 0.20.4

Looking into the issue a bit, per http://wiki.apache.org/hadoop/Hbase/Stargate#A4, the path is parsing ""/"" and "":"" in a very special way. I guess one workaround would be to encode the row keys if they are known to be built like this. 
;;;","07/Jan/10 20:47;apurtell;bq. About to release 0.20.3 so punting to 0.20.4 

No problem J-D. I'm looking at this one today though, if that matters.

bq. I guess one workaround would be to encode the row keys if they are known to be built like this. 

Based on the example given by the reporter, the client is already hex encoding URL metacharacters. If a client submits a row with the key

{{http%3A%2F%2Fwww.google.com%2F}}

it should be passed through as-is to HBase, not URL-decoded somehow. I hope to fix this without resorting to requiring users to base64 encode their row and column specifiers. 
;;;","08/Jan/10 05:01;apurtell;Resolved partly by a documentation update, partly as a fix to row key handling. J-D, feel free to commit this to 0.20.3 at your option. I have already committed it on trunk.;;;","08/Jan/10 05:25;jdcryans;Committed to 0.20.3, thanks for doing it so fast Andrew!;;;","16/Feb/11 16:29;jeffgran;This bug is not resolved.  I am still seeing the described behavior under 0.20.6. The issue is that the `%2F` in the URL is getting parsed as if it were a literal slash, so even if you have a url like `http://localhost:8000/tablename/http%3A%2F%2Fwww.google.com%2F`, Stargate interprets the two `%2F`s as slash delimiters, and then the `www.google.com` part is interpreted as the timestamp, which causes the `NumberFormatException`. ;;;","23/Feb/11 07:34;stack;@Jeff Mind opening new issue to address the above?  My guess is that 0.90.x hbase has same issue?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
findbugs issues ,HBASE-2090,12444564,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,kaykay.unique,kaykay.unique,kaykay.unique,04/Jan/10 07:07,20/Nov/15 13:02,14/Jul/23 06:06,05/Jan/10 06:06,,,,,,,,,,,,0.90.0,,,,,,,0,,,Findbugs issues/ fixes for a subset of them. ,,larsfrancke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/10 07:07;kaykay.unique;HBASE-2090.patch;https://issues.apache.org/jira/secure/attachment/12429318/HBASE-2090.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26138,Reviewed,,,,Fri Nov 20 13:02:00 UTC 2015,,,,,,,,,,"0|i0hgdz:",99917,,,,,,,,,,,,,,,,,,,,,"04/Jan/10 07:08;kaykay.unique;Some of the issues addressed: 

* Boolean.valueOf , Long.valueOf as opposed to the ctors. 
* static inner classes as necessary 
* StringBuilder for concatentation of strings and not String concatentation, as the latter is immutable. ;;;","04/Jan/10 13:07;apurtell;+1;;;","05/Jan/10 06:06;stack;Committed TRUNK.  Thanks for patch Kay Kay.;;;","20/Nov/15 13:02;larsfrancke;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The wait on compaction because ""Too many store files"" holds up all flushing",HBASE-2087,12444468,Bug,Closed,HBASE,HBase,software,zhangduo,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,Fixed,jdcryans,stack,stack,01/Jan/10 00:26,12/Oct/12 06:14,14/Jul/23 06:06,02/Apr/10 00:55,,,,,,,,,,,,0.20.4,0.90.0,,,,,,0,,,"The method MemStoreFlusher#checkStoreFileCount is called from flushRegion.  flushRegion is called by MemStoreFlusher#run thread.  If the checkStoreFileCount finds too many store files, it'll stick around waiting on a compaction to happen.  While its hanging, the MemStoreFlusher#run is held up.  No other region can flush.  Meantime WALs will be rolling and memory will be accumulating writes.",,dhruba,kannanm,streamy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/10 23:55;jdcryans;HBASE-2087.patch;https://issues.apache.org/jira/secure/attachment/12440426/HBASE-2087.patch",,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,26137,Reviewed,,,,Fri Apr 02 00:55:44 UTC 2010,,,,,,,,,,"0|i08t0f:",49300,,,,,,,,,,,,,,,,,,,,,"01/Jan/10 01:02;jdcryans;So either we have too much WALs or too much store files right? Like I said in HBASE-2053, our WAL is set very small so that master splits fast and we don't lose data. In 0.21 we won't lose data so speeding up the spit time then set a higher/bigger WAL would solve this problem?;;;","01/Jan/10 01:07;jdcryans;Another thing to keep in mind, flushing incomplete memstores is highly inefficient. Let's say you want to drop the number of WALs by flushing 10 regions. Those are probably not full, maybe 2MB or 10MB big, but they still take time to flush and clogger HDFS with even more new files. Those files then have to be compacted, it's even worse if we hit the ""Too many store files"" problem and it's likely that one causes the other.;;;","03/Jan/10 22:51;stack;@J-D: ""...flushing incomplete memstores is highly inefficent..""  ... yeah but if the edit is old, its probably worth the flush if you take a systems view.  And this issue is about something else anyway, never holding up flushes.  Should we open a blanket issue in which we discuss undoing ""compensating"" changes now hdfs has a working sync; i.e.undo all the weird stuff we did to try and minimize losing edits when there was no working sync.;;;","04/Jan/10 01:09;apurtell;bq. Should we open a blanket issue in which we discuss undoing ""compensating"" changes now hdfs has a working sync

+1

Like we did with the compaction limiting thread and region server ""safe mode"" after the transition to 0.20.;;;","04/Jan/10 04:49;jdcryans;bq. And this issue is about something else anyway, never holding up flushes

As I said in my first comment, it's either too much WALs or too much store files. If we let all flushes go then we are overrun by store files. If we force flush memstores to be able to roll WALs then we easily create too much store files. We have seen stores that needed to compact 100 files and this is why we have a limit.

So, I question the feasibility of this jira.

In the particular case of WALs waiting on flushes waiting on too many store files, what I said is that it's by setting a very low number of WALs that we easily hit the limit. Setting it to a higher number means less chance of hitting this jira's problem, hence making it invalid?;;;","04/Jan/10 18:11;stack;The problem this issue covers is case where a regionserver has say 1k regions and it so happens that one of these is over the store file upper limit.  As is all flushing on the regionserver is held up because one region is over the limit.   Because no flushing we will block writes and so on;;;","04/Jan/10 18:38;jdcryans;Oh right I didn't see it like that. Yes we don't want to hold flushes for every region, just those concerned.;;;","15/Feb/10 05:26;stack;Moving into 0.20.4.;;;","19/Feb/10 00:20;stack;I was going to explore blocking the problematic store only by removing its flush request from the flush queue readding it later after the timer elapses (or after compaction completes);;;","31/Mar/10 23:55;jdcryans;Here's a patch that does what Stack describes. 

 - If the flush comes from flushSomeRegions, we will wait since that doesn't hold up the other regions.
 - If the flush comes from the main flushing thread, we check if there's too many store files. If so, we wait a bit and add it back to the queue.

I tried it on the randomWrite PE, works as advertised but it may be a bit chatty when the compactions are taking a long time. Could be improved by doing the ""triggered"" thing ensureStoreFileCount is doing.;;;","02/Apr/10 00:25;stack;+1 on patch.  Its a big improvement even though it means lots of ugly logs.   We can revisit the latter later.  I'd say change the 500ms to 1s sleep at least since unlikely compaction will complete in this time.  You wanted to change the name of checkStoreFileCount?  Change it on commit?;;;","02/Apr/10 00:55;jdcryans;Committed to branch and trunk with a 1000ms sleep, thanks for the review Stack!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
