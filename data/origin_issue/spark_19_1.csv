Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Cloners),Inward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
as_tibble was removed from Arrow R API,SPARK-28215,13242303,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,29/Jun/19 16:02,12/Dec/22 18:10,13/Jul/23 08:45,01/Jul/19 04:21,3.0.0,,,,,,,,3.0.0,,,,R,,,,,0,,,,,,"New R api of Arrow has removed `as_tibble`. Arrow optimized collect in R doesn't work now due to the change.

",,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29339,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 01 04:21:37 UTC 2019,,,,,,,,,,"0|z0481s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/19 04:21;gurwls223;Issue resolved by pull request 25012
[https://github.com/apache/spark/pull/25012];;;",,,,,,,,,,,,,,,,,,,,,
Flaky test: org.apache.spark.streaming.CheckpointSuite.basic rdd checkpoints + dstream graph checkpoint recovery,SPARK-28214,13242258,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,vanzin,vanzin,29/Jun/19 00:01,09/Sep/19 23:20,13/Jul/23 08:45,09/Sep/19 22:37,3.0.0,,,,,,,,3.0.0,,,,DStreams,Tests,,,,0,,,,,,"This test has failed a few times in some PRs. Example of a failure:

{noformat}
Error Message
org.scalatest.exceptions.TestFailedException: Map() was empty No checkpointed RDDs in state stream before first failure
Stacktrace
sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: Map() was empty No checkpointed RDDs in state stream before first failure
	at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528)
	at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501)
	at org.apache.spark.streaming.CheckpointSuite.$anonfun$new$3(CheckpointSuite.scala:266)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:149)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:56)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)
	at org.apache.spark.streaming.CheckpointSuite.org$scalatest$BeforeAndAfter$$super$runTest(CheckpointSuite.scala:209)
{noformat}

On top of that, when this failure happens, the test leaves a running {{SparkContext}} behind, which makes every single unit test run after it on that project fail.",,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 09 22:37:08 UTC 2019,,,,,,,,,,"0|z047rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/19 22:37;vanzin;Issue resolved by pull request 25731
[https://github.com/apache/spark/pull/25731];;;",,,,,,,,,,,,,,,,,,,,,
"""@pandas_udf"" in doctest is rendered as "":pandas_udf"" in html API doc",SPARK-28206,13242192,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,mengxr,mengxr,28/Jun/19 16:48,12/Dec/22 18:10,13/Jul/23 08:45,05/Jul/19 17:09,2.4.1,,,,,,,,3.0.0,,,,Documentation,PySpark,,,,0,,,,,,"Just noticed that in [pandas_udf API doc |https://spark.apache.org/docs/2.4.1/api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf], ""@pandas_udf"" is render as "":pandas_udf"".

cc: [~hyukjin.kwon] [~smilegator]",,mengxr,shivusondur@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/19 16:55;mengxr;Screen Shot 2019-06-28 at 9.55.13 AM.png;https://issues.apache.org/jira/secure/attachment/12973195/Screen+Shot+2019-06-28+at+9.55.13+AM.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 05 17:09:01 UTC 2019,,,,,,,,,,"0|z047dk:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,"05/Jul/19 09:56;gurwls223;This is a side effect by Epydoc doc plugin, which is a legacy in PySpark. I am not sure if I can get rid of it and replace all Epydoc specific syntax but let me try.;;;","05/Jul/19 17:09;mengxr;Issue resolved by pull request 25060
[https://github.com/apache/spark/pull/25060];;;",,,,,,,,,,,,,,,,,,,,
PythonRDD should respect SparkContext's conf when passing user confMap,SPARK-28203,13242142,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,advancedxy,advancedxy,advancedxy,28/Jun/19 10:27,12/Dec/22 18:11,13/Jul/23 08:45,15/Aug/19 01:41,2.4.3,,,,,,,,3.0.0,,,,PySpark,Spark Core,,,,0,,,,,,"PythonRDD have several API which accepts user configs from python side. The parameter is called confAsMap and it's intended to merge with RDD's hadoop configuration.


 However, the confAsMap is first mapped to Configuration then merged into SparkContext's hadoop configuration. The mapped Configuration will load default key values in core-default.xml etc., which may be updated in SparkContext's hadoop configuration. The default value will override updated value in the merge process.

I will submit a pr to fix this.",,advancedxy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 15 01:41:19 UTC 2019,,,,,,,,,,"0|z0472g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/19 01:41;gurwls223;Issue resolved by pull request 25002
[https://github.com/apache/spark/pull/25002];;;",,,,,,,,,,,,,,,,,,,,,
Revisit MakeDecimal behavior on overflow,SPARK-28201,13242112,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,mgaido,mgaido,28/Jun/19 08:13,16/Jul/19 15:41,13/Jul/23 08:45,01/Jul/19 03:56,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,1,,,,,,"As pointed out in https://github.com/apache/spark/pull/20350#issuecomment-505997469, in special cases of decimal aggregation we are using the `MakeDecimal` operator.

This operator has a not well defined behavior in case of overflow, namely what it does currently is:

 - if codegen is enabled it returns null;
 -  in interpreted mode it throws an `IllegalArgumentException`.

So we should make his behavior uniform with other similar cases and in particular we should honor the value of the conf introduced in SPARK-23179 and behave accordingly, ie.:

 - returning null if the flag is true;
 - throw an `ArithmeticException` if the flag is false.",,apachespark,cloud_fan,joshrosen,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23179,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 15 18:55:40 UTC 2019,,,,,,,,,,"0|z046vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/19 08:14;mgaido;I'll create a PR for this ASAP.;;;","01/Jul/19 03:56;cloud_fan;Issue resolved by pull request 25010
[https://github.com/apache/spark/pull/25010];;;","15/Jul/19 18:55;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/25165;;;",,,,,,,,,,,,,,,,,,,
Decimal overflow handling in ExpressionEncoder,SPARK-28200,13242110,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mickjermsurawong-stripe,mgaido,mgaido,28/Jun/19 08:07,05/Jul/19 14:17,13/Jul/23 08:45,05/Jul/19 14:09,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,1,,,,,,"As pointed out in https://github.com/apache/spark/pull/20350, we are currently not checking the overflow when serializing a java/scala `BigDecimal` in `ExpressionEncoder` / `ScalaReflection`.

We should add this check there too.",,cloud_fan,joshrosen,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23179,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 05 14:09:11 UTC 2019,,,,,,,,,,"0|z046vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/19 17:16;joshrosen;MickJermsurawong and I have a patch for this, including tests covering both ExpressionEncoder and RowEncoder; we'll submit a PR early next week.;;;","05/Jul/19 14:09;cloud_fan;Issue resolved by pull request 25016
[https://github.com/apache/spark/pull/25016];;;",,,,,,,,,,,,,,,,,,,,
Pyspark  - df.drop() is Case Sensitive when Referring to Upstream Tables,SPARK-28189,13242024,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Tonix517,lchu122,lchu122,27/Jun/19 20:35,21/Jul/19 07:12,13/Jul/23 08:45,07/Jul/19 04:45,2.4.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"Column names in general are case insensitive in Pyspark, and df.drop() in general is also case insensitive.

However, when referring to an upstream table, such as from a join, e.g.
{code:java}
vals1 = [('Pirate', 1),('Monkey', 2),('Ninja', 3),('Spaghetti', 4)]
df1 = spark.createDataFrame(vals1, ['KEY','field'])

vals2 = [('Rutabaga', 1),('Pirate', 2),('Ninja', 3),('Darth Vader', 4)]
df2 = spark.createDataFrame(vals2, ['KEY','CAPS'])


df_joined = df1.join(df2, df1['key'] == df2['key'], ""left"")
{code}
 

drop will become case sensitive. e.g.
{code:java}
# from above, df1 consists of columns ['KEY', 'field']
# from above, df2 consists of columns ['KEY', 'CAPS']

df_joined.select(df2['key']) # will give a result
df_joined.drop('caps') # will also give a result
{code}
however, note the following
{code:java}
df_joined.drop(df2['key']) # no-op
df_joined.drop(df2['caps']) # no-op

df_joined.drop(df2['KEY']) # will drop column as expected
df_joined.drop(df2['CAPS']) # will drop column as expected

{code}
 

 

so in summary, using df.drop(df2['col']) doesn't align with expected case insensitivity for column names, even though functions like select, join, and dropping a column generally are case insensitive.

 ",,dongjoon,lchu122,mgaido,Tonix517,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 07 05:05:21 UTC 2019,,,,,,,,,,"0|z046c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/19 17:42;Tonix517;Working on a fix now.;;;","05/Jul/19 01:14;dongjoon;Is this only for Spark 2.4? Could you check Spark 2.3, too?;;;","07/Jul/19 04:45;dongjoon;This is resolved via https://github.com/apache/spark/pull/25055;;;","07/Jul/19 04:45;dongjoon;hi, [~Tonix517]. You are added to `Apache Spark Contributor`. Welcome!;;;","07/Jul/19 05:05;Tonix517;[~dongjoon] thrilled. thanks for you help!;;;",,,,,,,,,,,,,,,,,
Trigger pandas iterator UDF closing stuff when iterator stop early,SPARK-28185,13241944,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,27/Jun/19 13:13,12/Dec/22 18:11,13/Jul/23 08:45,28/Jun/19 08:10,2.4.3,,,,,,,,3.0.0,,,,ML,SQL,,,,0,,,,,,"Fix the issue Pandas UDF closing stuff won't be triggered when iterator stop early.

 ",,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 28 08:10:49 UTC 2019,,,,,,,,,,"0|z045ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/19 08:10;gurwls223;Issue resolved by pull request 24986
[https://github.com/apache/spark/pull/24986];;;",,,,,,,,,,,,,,,,,,,,,
usage description does not match with shell scripts,SPARK-28164,13241450,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shivusondur@gmail.com,hannankan,hannankan,25/Jun/19 11:02,26/Jun/19 17:47,13/Jul/23 08:45,26/Jun/19 17:43,2.4.3,,,,,,,,2.3.4,2.4.4,3.0.0,,Project Infra,,,,,0,,,,,,"I found that ""spark/sbin/start-slave.sh"" may have some error. 

line 43 gives--- echo ""Usage: ./sbin/start-slave.sh [options] <master>""

but later this script,  I found line 59  MASTER=$1 

Is this a conflict?",,apachespark,hannankan,shivusondur@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,https://github.com/apache/spark/blob/master/sbin/start-slave.sh,,,,,,,,,,9223372036854775807,,,Wed Jun 26 17:43:47 UTC 2019,,,,,,,,,,"0|z042so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/19 17:38;shivusondur@gmail.com;[~hannankan]

 ./sbin/start-slave.sh  <master_URL>

it will start properly. Tested in master ;;;","26/Jun/19 02:36;hannankan;but if you add some options such as ""{{sbin/start-slave.sh -c $CORES_PER_WORKER -m 3G ${MASTER}}}""  it will not work properly. Because at the very beginning $1 is not master;;;","26/Jun/19 12:04;apachespark;User 'shivusondur' has created a pull request for this issue:
https://github.com/apache/spark/pull/24974;;;","26/Jun/19 12:06;shivusondur@gmail.com;[~hannankan]

i updated the usage message and raised the pull request;;;","26/Jun/19 17:43;srowen;Issue resolved by pull request 24974
[https://github.com/apache/spark/pull/24974];;;",,,,,,,,,,,,,,,,,
Kafka ignores user configuration on FETCH_OFFSET_NUM_RETRY and FETCH_OFFSET_RETRY_INTERVAL_MS,SPARK-28163,13241441,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,gsomogyi,gsomogyi,25/Jun/19 10:17,09/Aug/19 10:24,13/Jul/23 08:45,09/Aug/19 09:09,3.0.0,,,,,,,,3.0.0,,,,Structured Streaming,,,,,0,,,,,,"There are ""unsafe"" conversions in the Kafka connector.
CaseInsensitiveStringMap comes in which is then converted the following way:
{code:java}
...
options.asScala.toMap
...
{code}
The main problem with this is that such case it looses its case insensitive nature
(case insensitive map is converting the key to lower case when get/contains called).
",,cloud_fan,gsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 09 09:09:05 UTC 2019,,,,,,,,,,"0|z042qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/19 09:09;cloud_fan;Issue resolved by pull request 24967
[https://github.com/apache/spark/pull/24967];;;",,,,,,,,,,,,,,,,,,,,,
TransportClient.sendRpcSync may hang forever,SPARK-28160,13241436,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,25/Jun/19 09:51,30/Jun/19 20:20,13/Jul/23 08:45,30/Jun/19 20:19,2.3.3,2.4.3,3.0.0,,,,,,2.3.4,2.4.4,3.0.0,,Spark Core,,,,,0,,,,,,"This is very like [SPARK-26665|https://issues.apache.org/jira/browse/SPARK-26665]
`ByteBuffer.allocate` may throw OutOfMemoryError when the response is large but no enough memory is available. However, when this happens, TransportClient.sendRpcSync will just hang forever if the timeout set to unlimited.",,cltlfcjin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 30 20:19:39 UTC 2019,,,,,,,,,,"0|z042pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Jun/19 20:19;srowen;Issue resolved by pull request 24964
[https://github.com/apache/spark/pull/24964];;;",,,,,,,,,,,,,,,,,,,,,
Join plan sometimes does not use cached query,SPARK-28156,13241375,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,bersprockets,bersprockets,25/Jun/19 01:48,01/Aug/19 20:51,13/Jul/23 08:45,03/Jul/19 13:25,2.3.3,2.4.3,3.0.0,,,,,,2.3.4,2.4.4,3.0.0,,SQL,,,,,0,,,,,,"I came across a case where a cached query is referenced on both sides of a join, but the InMemoryRelation is inserted on only one side. This case occurs only when the cached query uses a (Hive-style) view.

Consider this example:
{noformat}
// create the data
val df1 = Seq.tabulate(10) { x => (x, x + 1, x + 2, x + 3) }.toDF(""a"", ""b"", ""c"", ""d"")
df1.write.mode(""overwrite"").format(""orc"").saveAsTable(""table1"")
sql(""drop view if exists table1_vw"")
sql(""create view table1_vw as select * from table1"")

// create the cached query
val cacheddataDf = sql(""""""
select a, b, c, d
from table1_vw
"""""")

import org.apache.spark.storage.StorageLevel.DISK_ONLY
cacheddataDf.createOrReplaceTempView(""cacheddata"")
cacheddataDf.persist(DISK_ONLY)

// main query
val queryDf = sql(s""""""
select leftside.a, leftside.b
from cacheddata leftside
join cacheddata rightside
on leftside.a = rightside.a
"""""")

queryDf.explain(true)
{noformat}
Note that the optimized plan does not use an InMemoryRelation for the right side, but instead just uses a Relation:
{noformat}
Project [a#45, b#46]
+- Join Inner, (a#45 = a#37)
   :- Project [a#45, b#46]
   :  +- Filter isnotnull(a#45)
   :     +- InMemoryRelation [a#45, b#46, c#47, d#48], StorageLevel(disk, 1 replicas)
   :           +- *(1) FileScan orc default.table1[a#37,b#38,c#39,d#40] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex[file:/Users/brobbins/github/spark_upstream/spark-warehouse/table1], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:int,b:int,c:int,d:int>
   +- Project [a#37]
      +- Filter isnotnull(a#37)
         +- Relation[a#37,b#38,c#39,d#40] orc

{noformat}
The fragment does not match the cached query because AliasViewChild adds an extra projection under the View on the right side (see #2 below).

AliasViewChild adds the extra projection because the exprIds in the View's output appears to have been renamed by Analyzer$ResolveReferences (#1 below). I have not yet looked at why.
{noformat}
-
-
-
   +- SubqueryAlias `rightside`
      +- SubqueryAlias `cacheddata`
         +- Project [a#73, b#74, c#75, d#76]
            +- SubqueryAlias `default`.`table1_vw`
(#1) ->        +- View (`default`.`table1_vw`, [a#73,b#74,c#75,d#76])
(#2) ->           +- Project [cast(a#45 as int) AS a#73, cast(b#46 as int) AS b#74, cast(c#47 as int) AS c#75, cast(d#48 as int) AS d#76]
                     +- Project [cast(a#37 as int) AS a#45, cast(b#38 as int) AS b#46, cast(c#39 as int) AS c#47, cast(d#40 as int) AS d#48]
                        +- Project [a#37, b#38, c#39, d#40]
                           +- SubqueryAlias `default`.`table1`
                              +- Relation[a#37,b#38,c#39,d#40] orc

{noformat}
In a larger query (where cachedata may be referred on either side only indirectly), this phenomenon can create certain oddities, as the fragment is not replaced with InMemoryRelation, and the fragment is present when the plan is optimized as a whole.

In Spark 2.1.3, Spark uses InMemoryRelation on both sides.",,bersprockets,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 03 13:25:45 UTC 2019,,,,,,,,,,"0|z042c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/19 13:25;cloud_fan;Issue resolved by pull request 24960
[https://github.com/apache/spark/pull/24960];;;",,,,,,,,,,,,,,,,,,,,,
do not leak SaveMode to file source v2,SPARK-28155,13241374,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,cloud_fan,manifoldQAQ,25/Jun/19 01:40,12/Dec/22 18:11,13/Jul/23 08:45,19/Feb/20 07:54,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"Currently there is a hack in `DataFrameWriter`, which passes `SaveMode` to file source v2. This should be removed and file source v2 should not accept SaveMode.",,cloud_fan,dongjoon,gsomogyi,idomi,Tonix517,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 19 07:54:22 UTC 2020,,,,,,,,,,"0|z042bs:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,"20/Jul/19 07:33;gurwls223;[~Tonix517] said:

Hi Wenchen, do you think it is viable solution mentioned below? 

Create a new V2WriteCommand case class and its Exec named maybe OverwriteByQueryId to replace WriteToDataSourceV2, which accepts a QueryId so that tests can pass.

Or should we keep WriteToDataSourceV2?;;;","20/Jul/19 07:39;gurwls223; SPARK-28155 was switched to SPARK-27815 due to an accident of a commit log.;;;","29/Jul/19 15:23;idomi;Hi,

Why was it reopened?

What's left to do?

 ;;;","19/Feb/20 07:54;cloud_fan;This is already done during a series of refactor for file source v2.;;;",,,,,,,,,,,,,,,,,,
Use AtomicReference at InputFileBlockHolder (to support input_file_name with Python UDF),SPARK-28153,13241370,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,25/Jun/19 01:01,12/Dec/22 18:10,13/Jul/23 08:45,01/Aug/19 05:20,2.3.3,2.4.3,3.0.0,,,,,,2.4.4,3.0.0,,,PySpark,,,,,0,,,,,,"{code}
from pyspark.sql.functions import udf, input_file_name
spark.range(10).write.mode(""overwrite"").parquet(""/tmp/foo"")
spark.read.parquet(""/tmp/foo"").select(udf(lambda x: x, ""long"")(""id""), input_file_name()).show()
{code}

{code}
+------------+-----------------+
|<lambda>(id)|input_file_name()|
+------------+-----------------+
|           8|                 |
|           5|                 |
|           0|                 |
|           9|                 |
|           6|                 |
|           2|                 |
|           3|                 |
|           4|                 |
|           7|                 |
|           1|                 |
+------------+-----------------+
{code}",,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27966,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 01 05:20:40 UTC 2019,,,,,,,,,,"0|z042aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/19 04:43;gurwls223;I am testing Python 3 compatibility of merigng script. Let me open and resolve. Please ignore this noise.;;;","01/Aug/19 05:20;gurwls223;Fixed in https://github.com/apache/spark/pull/24958;;;",,,,,,,,,,,,,,,,,,,,
Mapped ShortType to SMALLINT and FloatType to REAL for MsSqlServerDialect,SPARK-28152,13241349,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shivsood,shivsood,shivsood,24/Jun/19 22:08,12/Dec/22 18:10,13/Jul/23 08:45,15/Jul/19 19:17,2.4.3,3.0.0,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,," ShortType and FloatTypes are not correctly mapped to right JDBC types when using JDBC connector. This results in tables and spark data frame being created with unintended types. The issue was observed when validating against SQLServer.

Some example issue
 * Write from df with column type results in a SQL table of with column type as INTEGER as opposed to SMALLINT. Thus a larger table that expected.
 * read results in a dataframe with type INTEGER as opposed to ShortType 

FloatTypes have a issue with read path. In the write path Spark data type 'FloatType' is correctly mapped to JDBC equivalent data type 'Real'. But in the read path when JDBC data types need to be converted to Catalyst data types ( getCatalystType) 'Real' gets incorrectly gets mapped to 'DoubleType' rather than 'FloatType'.

 ",,dongjoon,shivsood,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28151,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 29 01:46:00 UTC 2019,,,,,,,,,,"0|z04268:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/19 21:42;shivsood;Resolved as part of https://issues.apache.org/jira/browse/SPARK-28151;;;","13/Jul/19 19:03;shivsood;Reopening this issue to submit this change as a separate PR for clarity. Earlier this change for made part of the ByteType PR ( 28151);;;","13/Jul/19 19:28;shivsood;Pull request for fix created. https://github.com/apache/spark/pull/25146;;;","15/Jul/19 18:30;dongjoon;Hi, [~shivsood]. Could you check `2.3.3` please, too?;;;","15/Jul/19 19:17;dongjoon;This is resolved via https://github.com/apache/spark/pull/25146;;;","25/Jul/19 02:36;dongjoon;This is merged to `branch-2.4` via https://github.com/apache/spark/pull/25248;;;","29/Nov/19 01:46;gurwls223;it was reverted in branch-2.4 at https://github.com/apache/spark/commit/00b61e36958118e98c6dbfa0515c11c8672a62ac;;;",,,,,,,,,,,,,,,
Failure to create multiple contexts in same JVM with Kerberos auth,SPARK-28150,13241322,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,24/Jun/19 18:39,15/Jul/19 17:37,13/Jul/23 08:45,27/Jun/19 20:30,3.0.0,,,,,,,,3.0.0,,,,Spark Core,,,,,0,,,,,,"Take the following small app that creates multiple contexts (not concurrently):

{code}
from pyspark.context import SparkContext
import time

for i in range(2):
  with SparkContext() as sc:
    pass
  time.sleep(5)
{code}

This fails when kerberos (without dt renewal) is being used:

{noformat}
19/06/24 11:33:58 ERROR spark.SparkContext: Error initializing SparkContext.
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.security.HBaseDelegationTokenProvider.obtainDelegationTokens(HBaseDelegationTokenProvider.scala:49)
Caused by: org.apache.hadoop.hbase.shaded.com.google.protobuf.ServiceException: Error calling method hbase.pb.AuthenticationService.GetAuthenticationToken
        at org.apache.hadoop.hbase.client.SyncCoprocessorRpcChannel.callBlockingMethod(SyncCoprocessorRpcChannel.java:71)
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.security.AccessDeniedException): org.apache.hadoop.hbase.security.AccessDeniedException: Token generation only allowed for Kerberos authenticated clients
        at org.apache.hadoop.hbase.security.token.TokenProvider.getAuthenticationToken(TokenProvider.java:126)
{noformat}

If you enable dt renewal things work since the codes takes a slightly different path when generating the initial delegation tokens.",,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 27 20:30:54 UTC 2019,,,,,,,,,,"0|z04208:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/19 20:30;vanzin;Issue resolved by pull request 24955
[https://github.com/apache/spark/pull/24955];;;",,,,,,,,,,,,,,,,,,,,,
KafkaContinuousStream ignores user configuration on CONSUMER_POLL_TIMEOUT,SPARK-28142,13241155,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,24/Jun/19 01:06,12/Dec/22 18:11,13/Jul/23 08:45,24/Jun/19 13:21,3.0.0,,,,,,,,3.0.0,,,,Structured Streaming,,,,,0,,,,,,"KafkaContinuousStream has a bug where {{pollTimeoutMs}} is always set to default value, as the value of {{KafkaSourceProvider.CONSUMER_POLL_TIMEOUT}} is {{kafkaConsumer.pollTimeoutMs}} which key-lowercased map has been provided as {{sourceOptions}}.

This is due to the missing spot, which Map should be passed as CaseInsensitiveStringMap as  SPARK-27106.",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 24 13:21:00 UTC 2019,,,,,,,,,,"0|z040zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/19 01:07;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/24942;;;","24/Jun/19 13:21;gurwls223;Issue resolved by pull request 24942
[https://github.com/apache/spark/pull/24942];;;",,,,,,,,,,,,,,,,,,,,
TRIM(type trimStr FROM str) returns incorrect result,SPARK-28109,13240452,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,19/Jun/19 15:59,24/Jun/19 02:38,13/Jul/23 08:45,19/Jun/19 19:48,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"SPARK-28093 introduced a new bug:

 
{noformat}
spark-sql> SELECT trim('yxTomxx', 'xyz'), trim(BOTH 'xyz' FROM 'yxTomxx');
Tom z
spark-sql> SELECT trim('xxxbarxxx', 'x'), trim(BOTH 'x' FROM 'xxxbarxxx');
bar
spark-sql> SELECT ltrim('zzzytest', 'xyz'), trim(LEADING 'xyz' FROM 'zzzytest');
test xyz
spark-sql> SELECT ltrim('zzzytestxyz', 'xyz'), trim(LEADING 'xyz' FROM 'zzzytestxyz');
testxyz
spark-sql> SELECT ltrim('xyxXxyLAST WORD', 'xy'), trim(LEADING 'xy' FROM 'xyxXxyLAST WORD');
XxyLAST WORD
spark-sql> SELECT rtrim('testxxzx', 'xyz'), trim(TRAILING 'xyz' FROM 'testxxzx');
test xy
spark-sql> SELECT rtrim('xyztestxxzx', 'xyz'), trim(TRAILING 'xyz' FROM 'xyztestxxzx');
xyztest
spark-sql> SELECT rtrim('TURNERyxXxy', 'xy'), trim(TRAILING 'xy' FROM 'TURNERyxXxy');
TURNERyxX{noformat}
{noformat}

postgres=# SELECT trim('yxTomxx', 'xyz'), trim(BOTH 'xyz' FROM 'yxTomxx');
btrim | btrim
-------+-------
Tom | Tom
(1 row)

postgres=# SELECT trim('xxxbarxxx', 'x'), trim(BOTH 'x' FROM 'xxxbarxxx');
btrim | btrim
-------+-------
bar | bar
(1 row)

postgres=# SELECT ltrim('zzzytest', 'xyz'), trim(LEADING 'xyz' FROM 'zzzytest');
ltrim | ltrim
-------+-------
test | test
(1 row)

postgres=# SELECT ltrim('zzzytestxyz', 'xyz'), trim(LEADING 'xyz' FROM 'zzzytestxyz');
ltrim | ltrim
---------+---------
testxyz | testxyz
(1 row)

postgres=# SELECT ltrim('xyxXxyLAST WORD', 'xy'), trim(LEADING 'xy' FROM 'xyxXxyLAST WORD');
ltrim | ltrim
--------------+--------------
XxyLAST WORD | XxyLAST WORD
(1 row)

postgres=# SELECT rtrim('testxxzx', 'xyz'), trim(TRAILING 'xyz' FROM 'testxxzx');
rtrim | rtrim
-------+-------
test | test
(1 row)

postgres=# SELECT rtrim('xyztestxxzx', 'xyz'), trim(TRAILING 'xyz' FROM 'xyztestxxzx');
rtrim | rtrim
---------+---------
xyztest | xyztest
(1 row)

postgres=# SELECT rtrim('TURNERyxXxy', 'xy'), trim(TRAILING 'xy' FROM 'TURNERyxXxy');
rtrim | rtrim
-----------+-----------
TURNERyxX | TURNERyxX
(1 row)

{noformat}
 ",,apachespark,dongjoon,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28093,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 24 02:38:39 UTC 2019,,,,,,,,,,"0|z03wns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/19 19:48;dongjoon;This is resolved via https://github.com/apache/spark/pull/24911;;;","20/Jun/19 19:38;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/24907;;;","21/Jun/19 01:20;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/24908;;;","21/Jun/19 01:21;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/24908;;;","24/Jun/19 02:35;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/24943;;;","24/Jun/19 02:38;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/24944;;;",,,,,,,,,,,,,,,,
Spark hangs when an execution plan has many projections on nested structs,SPARK-28090,13240120,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,yruslan2,yruslan2,18/Jun/19 06:56,16/Apr/22 20:19,13/Jul/23 08:45,03/Apr/22 07:49,2.4.3,,,,,,,,3.4.0,,,,SQL,,,,,3,,,,,,"This was already posted (#28016), but the provided example didn't always reproduce the error. This example consistently reproduces the issue.

Spark applications freeze on execution plan optimization stage (Catalyst) when a logical execution plan contains a lot of projections that operate on nested struct fields.

The code listed below demonstrates the issue.

To reproduce the Spark App does the following:
 * A small dataframe is created from a JSON example.
 * Several nested transformations (negation of a number) are applied on struct fields and each time a new struct field is created. 
 * Once more than 9 such transformations are applied the Catalyst optimizer freezes on optimizing the execution plan.
 * You can control the freezing by choosing different upper bound for the Range. E.g. it will work file if the upper bound is 5, but will hang is the bound is 10.

{code:java}
package com.example

import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{StructField, StructType}
import scala.collection.mutable.ListBuffer

object SparkApp1IssueSelfContained {

  // A sample data for a dataframe with nested structs
  val sample: List[String] =
    """""" { ""numerics"": {""num1"": 101, ""num2"": 102, ""num3"": 103, ""num4"": 104, ""num5"": 105, ""num6"": 106, ""num7"": 107, ""num8"": 108, ""num9"": 109, ""num10"": 110, ""num11"": 111, ""num12"": 112, ""num13"": 113, ""num14"": 114, ""num15"": 115} } """""" ::
    """""" { ""numerics"": {""num1"": 201, ""num2"": 202, ""num3"": 203, ""num4"": 204, ""num5"": 205, ""num6"": 206, ""num7"": 207, ""num8"": 208, ""num9"": 209, ""num10"": 210, ""num11"": 211, ""num12"": 212, ""num13"": 213, ""num14"": 214, ""num15"": 215} } """""" ::
    """""" { ""numerics"": {""num1"": 301, ""num2"": 302, ""num3"": 303, ""num4"": 304, ""num5"": 305, ""num6"": 306, ""num7"": 307, ""num8"": 308, ""num9"": 309, ""num10"": 310, ""num11"": 311, ""num12"": 312, ""num13"": 313, ""num14"": 314, ""num15"": 315} } """""" ::
    Nil

  /**
    * Transforms a column inside a nested struct. The transformed value will be put into a new field of that nested struct
    *
    * The output column name can omit the full path as the field will be created at the same level of nesting as the input column.
    *
    * @param inputColumnName  A column name for which to apply the transformation, e.g. `company.employee.firstName`.
    * @param outputColumnName The output column name. The path is optional, e.g. you can use `transformedName` instead of `company.employee.transformedName`.
    * @param expression       A function that applies a transformation to a column as a Spark expression.
    * @return A dataframe with a new field that contains transformed values.
    */
  def transformInsideNestedStruct(df: DataFrame,
                                  inputColumnName: String,
                                  outputColumnName: String,
                                  expression: Column => Column): DataFrame = {
    def mapStruct(schema: StructType, path: Seq[String], parentColumn: Option[Column] = None): Seq[Column] = {
      val mappedFields = new ListBuffer[Column]()

      def handleMatchedLeaf(field: StructField, curColumn: Column): Seq[Column] = {
        val newColumn = expression(curColumn).as(outputColumnName)
        mappedFields += newColumn
        Seq(curColumn)
      }

      def handleMatchedNonLeaf(field: StructField, curColumn: Column): Seq[Column] = {
        // Non-leaf columns need to be further processed recursively
        field.dataType match {
          case dt: StructType => Seq(struct(mapStruct(dt, path.tail, Some(curColumn)): _*).as(field.name))
          case _ => throw new IllegalArgumentException(s""Field '${field.name}' is not a struct type."")
        }
      }

      val fieldName = path.head
      val isLeaf = path.lengthCompare(2) < 0

      val newColumns = schema.fields.flatMap(field => {
        // This is the original column (struct field) we want to process
        val curColumn = parentColumn match {
          case None => new Column(field.name)
          case Some(col) => col.getField(field.name).as(field.name)
        }

        if (field.name.compareToIgnoreCase(fieldName) != 0) {
          // Copy unrelated fields as they were
          Seq(curColumn)
        } else {
          // We have found a match
          if (isLeaf) {
            handleMatchedLeaf(field, curColumn)
          } else {
            handleMatchedNonLeaf(field, curColumn)
          }
        }
      })

      newColumns ++ mappedFields
    }

    val schema = df.schema
    val path = inputColumnName.split('.')
    df.select(mapStruct(schema, path): _*)
  }

  /**
    * This Spark Job demonstrates an issue of execution plan freezing when there are a lot of projections
    * involving nested structs in an execution plan.
    *
    * The example works as follows:
    * - A small dataframe is created from a JSON example above
    * - A nested withColumn map transformation is used to apply a transformation on a struct field and create
    *   a new struct field.
    * - Once more than 9 such transformations are applied the Catalyst optimizer freezes on optimizing
    *   the execution plan
    *
    */
  def main(args: Array[String]): Unit = {

    val sparkBuilder = SparkSession.builder().appName(""Nested Projections Issue Example"")
    val spark = sparkBuilder
      .master(""local[4]"")
      .getOrCreate()

    import spark.implicits._

    val dfInput = spark.read.json(sample.toDS)

    // Apply several negations. You can change the number of negations by changing the upper bound of the range up to 16
    val dfOutput = Range(1,12).foldLeft(dfInput)( (df, i) => {
      transformInsideNestedStruct(df, s""numerics.num$i"", s""out_num$i"", c => -c)
    })

    dfOutput.printSchema()
    dfOutput.explain(true)
    dfOutput.show(false)
  }

}
{code}","Tried in
 * Spark 2.2.1, Spark 2.4.3 in local mode on Linux, MasOS and Windows
 * Spark 2.4.3 / Yarn on a Linux cluster",apachespark,Benedeki,dongjoon,iskenderunlu804,maropu,mgaido,viirya,yruslan2,Zejnilovic,,,,,,,,,,,,,,,,,,,,,SPARK-38712,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 03 07:49:13 UTC 2022,,,,,,,,,,"0|z03um0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/19 12:28;iskenderunlu804;I will try to work on this issue as my first contribution trial.;;;","02/Feb/22 16:12;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/35382;;;","03/Apr/22 07:49;dongjoon;This is resolved via https://github.com/apache/spark/pull/35382;;;",,,,,,,,,,,,,,,,,,,
LOAD DATA command resolving the partition column name considering case senstive manner ,SPARK-28084,13239985,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,S71955,S71955,S71955,17/Jun/19 16:01,03/Oct/19 17:03,13/Jul/23 08:45,03/Oct/19 08:12,2.4.3,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"LOAD DATA command resolving the partition column name considering case sensitive manner, where as insert command resolves case-insensitive manner.

Refer the snapshot for more details.

!image-2019-06-18-00-04-22-475.png!",,dongjoon,S71955,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/19 16:01;S71955;parition_casesensitive.PNG;https://issues.apache.org/jira/secure/attachment/12972000/parition_casesensitive.PNG",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 03 08:12:21 UTC 2019,,,,,,,,,,"0|z03tsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/19 16:37;S71955;insert command the partition column will be resolved using the resolver where the resovlver will resolve the names based on 

spark.sql.caseSensitive property. same logic can be applied for resolving the partition column names in LOAD COMMAND.

I am working on this, will raise a PR soon;;;","03/Oct/19 08:12;dongjoon;Issue resolved by pull request 24903
[https://github.com/apache/spark/pull/24903];;;",,,,,,,,,,,,,,,,,,,,
word2vec 'large' count value too low for very large corpora,SPARK-28081,13239947,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,17/Jun/19 13:59,19/Jun/19 01:31,13/Jul/23 08:45,19/Jun/19 01:29,2.4.3,,,,,,,,2.3.4,2.4.4,3.0.0,,ML,,,,,0,,,,,,"The word2vec implementation operates on word counts, and uses a hard-coded value of 1e9 to mean ""a very large count, larger than any actual count"". However this causes the logic to fail if, in fact, a large corpora has some words that really do occur more than this many times. We can probably improve the implementation to better handle very large counts in general.",,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 19 01:29:26 UTC 2019,,,,,,,,,,"0|z03tk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/19 01:29;srowen;Issue resolved by pull request 24893
[https://github.com/apache/spark/pull/24893];;;",,,,,,,,,,,,,,,,,,,,,
Incorrect results in decimal aggregation with whole-stage code gen enabled,SPARK-28067,13239824,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ksunitha,msirek,msirek,16/Jun/19 19:38,12/Dec/22 18:10,13/Jul/23 08:45,02/Jun/20 11:31,2.0.2,2.1.3,2.2.3,2.3.4,2.4.4,3.0.0,,,3.1.0,,,,SQL,,,,,0,correctness,,,,,"The following test case involving a join followed by a sum aggregation returns the wrong answer for the sum:

 
{code:java}
val df = Seq(
 (BigDecimal(""10000000000000000000""), 1),
 (BigDecimal(""10000000000000000000""), 1),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2)).toDF(""decNum"", ""intNum"")
val df2 = df.withColumnRenamed(""decNum"", ""decNum2"").join(df, ""intNum"").agg(sum(""decNum""))
scala> df2.show(40,false)
 ---------------------------------------

sum(decNum)
---------------------------------------

40000000000000000000.000000000000000000
---------------------------------------
 
{code}
 

The result should be 1040000000000000000000.0000000000000000.

It appears a partial sum is computed for each join key, as the result returned would be the answer for all rows matching intNum === 1.

If only the rows with intNum === 2 are included, the answer given is null:

 
{code:java}
scala> val df3 = df.filter($""intNum"" === lit(2))
 df3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [decNum: decimal(38,18), intNum: int]
scala> val df4 = df3.withColumnRenamed(""decNum"", ""decNum2"").join(df3, ""intNum"").agg(sum(""decNum""))
 df4: org.apache.spark.sql.DataFrame = [sum(decNum): decimal(38,18)]
scala> df4.show(40,false)
 -----------

sum(decNum)
-----------

null
-----------
 
{code}
 

The correct answer, 1000000000000000000000.0000000000000000, doesn't fit in the DataType picked for the result, decimal(38,18), so an overflow occurs, which Spark then converts to null.

The first example, which doesn't filter out the intNum === 1 values should also return null, indicating overflow, but it doesn't.  This may mislead the user to think a valid sum was computed.

If whole-stage code gen is turned off:

spark.conf.set(""spark.sql.codegen.wholeStage"", false)

... incorrect results are not returned because the overflow is caught as an exception:

java.lang.IllegalArgumentException: requirement failed: Decimal precision 39 exceeds max precision 38

 

 

 

 

 

 

 ",,anuragmantri,apachespark,cloud_fan,dongjoon,javier_ivanov,ksunitha,mgaido,msirek,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 08 00:00:43 UTC 2020,,,,,,,,,,"0|z03ssw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/19 15:07;mgaido;I cannot reproduce on master. It always returns null with whole stage codegen enabled.;;;","23/Jun/19 05:19;msirek;[~mgaido] Here is the physical plan I'm getting.  Maybe yours is different?  I tried on master this time...

 

 
{code:java}
msirek@skylake16:~/IdeaProjects/spark$ git status
On branch master
Your branch is up to date with 'origin/master'.
nothing to commit, working tree clean
msirek@skylake16:~/IdeaProjects/spark$ git log -5 --pretty=format:""%h%x09%an%x09%ad%x09%s""
870f972dcc Yuming Wang Sat Jun 22 09:15:07 2019 -0700 [SPARK-28104][SQL] Implement Spark's own GetColumnsOperation
5ad1053f3e Bryan Cutler Sat Jun 22 11:20:35 2019 +0900 [SPARK-28128][PYTHON][SQL] Pandas Grouped UDFs skip empty partitions
113f8c8d13 HyukjinKwon Fri Jun 21 10:47:54 2019 -0700 [SPARK-28132][PYTHON] Update document type conversion for Pandas UDFs (pyarrow 0.13.0, pandas 0.24.2, Python 3.7)
9b9d81b821 HyukjinKwon Fri Jun 21 10:27:18 2019 -0700 [SPARK-28131][PYTHON] Update document type conversion between Python data and SQL types in normal UDFs (Python 3.7)
54da3bbfb2 Yesheng Ma Thu Jun 20 19:45:59 2019 -0700 [SPARK-28127][SQL] Micro optimization on TreeNode's mapChildren method

msirek@skylake16:~/IdeaProjects/spark$ ./bin/spark-shell
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/msirek/IdeaProjects/spark/assembly/target/scala-2.12/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
19/06/22 22:13:39 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Spark context Web UI available at http://skylake16.home.colo:4041
Spark context available as 'sc' (master = local[*], app id = local-1561266819220).
Spark session available as 'spark'.
Welcome to
 ____ __
 / __/__ ___ _____/ /__
 _\ \/ _ \/ _ `/ __/ '_/
 /___/ .__/\_,_/_/ /_/\_\ version 3.0.0-SNAPSHOT
 /_/
 
Using Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_201)
Type in expressions to have them evaluated.
Type :help for more information.
scala> val df = Seq(
 | (BigDecimal(""10000000000000000000""), 1),
 | (BigDecimal(""10000000000000000000""), 1),
 | (BigDecimal(""10000000000000000000""), 2),
 | (BigDecimal(""10000000000000000000""), 2),
 | (BigDecimal(""10000000000000000000""), 2),
 | (BigDecimal(""10000000000000000000""), 2),
 | (BigDecimal(""10000000000000000000""), 2),
 | (BigDecimal(""10000000000000000000""), 2),
 | (BigDecimal(""10000000000000000000""), 2),
 | (BigDecimal(""10000000000000000000""), 2),
 | (BigDecimal(""10000000000000000000""), 2),
 | (BigDecimal(""10000000000000000000""), 2)).toDF(""decNum"", ""intNum"")
df: org.apache.spark.sql.DataFrame = [decNum: decimal(38,18), intNum: int]
scala> val df2 = df.withColumnRenamed(""decNum"", ""decNum2"").join(df, ""intNum"").agg(sum(""decNum""))
df2: org.apache.spark.sql.DataFrame = [sum(decNum): decimal(38,18)]
scala> df2.explain
== Physical Plan ==
*(2) HashAggregate(keys=[], functions=[sum(decNum#14)])
+- Exchange SinglePartition
 +- *(1) HashAggregate(keys=[], functions=[partial_sum(decNum#14)])
 +- *(1) Project [decNum#14]
 +- *(1) BroadcastHashJoin [intNum#8], [intNum#15], Inner, BuildLeft
 :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))
 : +- LocalTableScan [intNum#8]
 +- LocalTableScan [decNum#14, intNum#15]
 
scala> df2.show(40,false)
+---------------------------------------+
|sum(decNum) |
+---------------------------------------+
|40000000000000000000.000000000000000000|
+---------------------------------------+
{code}
 

 ;;;","24/Jun/19 07:21;mgaido;No, it is the same. Are you sure about your configs?

{code}
macmarco:spark mark9$ git log -5 --oneline
5ad1053f3e (HEAD, apache/master) [SPARK-28128][PYTHON][SQL] Pandas Grouped UDFs skip empty partitions
113f8c8d13 [SPARK-28132][PYTHON] Update document type conversion for Pandas UDFs (pyarrow 0.13.0, pandas 0.24.2, Python 3.7)
9b9d81b821 [SPARK-28131][PYTHON] Update document type conversion between Python data and SQL types in normal UDFs (Python 3.7)
54da3bbfb2 [SPARK-28127][SQL] Micro optimization on TreeNode's mapChildren method
47f54b1ec7 [SPARK-28118][CORE] Add `spark.eventLog.compression.codec` configuration
macmarco:spark mark9$ ./bin/spark-shell 
19/06/24 09:17:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://.....:4040
Spark context available as 'sc' (master = local[*], app id = local-1561360686725).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT
      /_/
         
Using Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_152)
Type in expressions to have them evaluated.
Type :help for more information.

scala> val df = Seq(
     |  (BigDecimal(""10000000000000000000""), 1),
     |  (BigDecimal(""10000000000000000000""), 1),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2)).toDF(""decNum"", ""intNum"")
df: org.apache.spark.sql.DataFrame = [decNum: decimal(38,18), intNum: int]

scala> val df2 = df.withColumnRenamed(""decNum"", ""decNum2"").join(df, ""intNum"").agg(sum(""decNum""))
df2: org.apache.spark.sql.DataFrame = [sum(decNum): decimal(38,18)]

scala> df2.explain
== Physical Plan ==
*(2) HashAggregate(keys=[], functions=[sum(decNum#14)])
+- Exchange SinglePartition
   +- *(1) HashAggregate(keys=[], functions=[partial_sum(decNum#14)])
      +- *(1) Project [decNum#14]
         +- *(1) BroadcastHashJoin [intNum#8], [intNum#15], Inner, BuildLeft
            :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))
            :  +- LocalTableScan [intNum#8]
            +- LocalTableScan [decNum#14, intNum#15]



scala> df2.show(40,false)
+-----------+                                                                   
|sum(decNum)|
+-----------+
|null       |
+-----------+
{code};;;","06/Jul/19 18:37;msirek;I tried the test on 4 different systems, all immediately after downloading Spark, and changing no settings, so they should all be the defaults.  None of the tests return null.  I'm not sure which config settings I should change.  I wouldn't think it's expected behavior for Spark to return an incorrect answer with a certain config setting, unless there's a setting which controls the hiding of overflows.;;;","08/Jul/19 20:53;mgaido;I cannot reproduce in 2.4.0 either:

{code}

spark-2.4.0-bin-hadoop2.7 xxx$ ./bin/spark-shell
2019-07-08 22:52:11 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://xxx:4040
Spark context available as 'sc' (master = local[*], app id = local-1562619141279).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.0
      /_/
         
Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_152)
Type in expressions to have them evaluated.
Type :help for more information.

scala> val df = Seq(
     |  (BigDecimal(""10000000000000000000""), 1),
     |  (BigDecimal(""10000000000000000000""), 1),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2),
     |  (BigDecimal(""10000000000000000000""), 2)).toDF(""decNum"", ""intNum"")
df: org.apache.spark.sql.DataFrame = [decNum: decimal(38,18), intNum: int]

scala> val df2 = df.withColumnRenamed(""decNum"", ""decNum2"").join(df, ""intNum"").agg(sum(""decNum""))
df2: org.apache.spark.sql.DataFrame = [sum(decNum): decimal(38,18)]

scala> df2.show(40,false)
+-----------+
|sum(decNum)|
+-----------+
|null       |
+-----------+
{code};;;","22/Jan/20 20:23;dongjoon;I reproduced this issue at 2.0.2, 2.1.3, 2.2.3, 2.3.4, 2.4.4, 3.0.0-preview2. As the description says, it give different results based on `spark.sql.codegen.wholeStage` value.
{code}
scala> :paste
// Entering paste mode (ctrl-D to finish)

val df = Seq(
 (BigDecimal(""10000000000000000000""), 1),
 (BigDecimal(""10000000000000000000""), 1),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2),
 (BigDecimal(""10000000000000000000""), 2)).toDF(""decNum"", ""intNum"")
val df2 = df.withColumnRenamed(""decNum"", ""decNum2"").join(df, ""intNum"").agg(sum(""decNum""))
df2.show(40,false)

// Exiting paste mode, now interpreting.

+---------------------------------------+
|sum(decNum)                            |
+---------------------------------------+
|40000000000000000000.000000000000000000|
+---------------------------------------+

df: org.apache.spark.sql.DataFrame = [decNum: decimal(38,18), intNum: int]
df2: org.apache.spark.sql.DataFrame = [sum(decNum): decimal(38,18)]

scala> spark.conf.set(""spark.sql.codegen.wholeStage"", false)

scala> df2.show(40,false)
20/01/22 20:18:10 ERROR Executor: Exception in task 1.0 in stage 4.0 (TID 20)
java.lang.IllegalArgumentException: requirement failed: Decimal precision 39 exceeds max precision 38
{code};;;","28/Jan/20 13:50;javier_ivanov;The expected result here should be an ArithmeticException for all cases of precision > DecimalType.MAX_PRECISION?;;;","10/Feb/20 02:48;gurwls223;I am lowering the priority given that {{spark.sql.codegen.wholeStage}} defaults to {{true}}.;;;","12/Feb/20 11:16;javier_ivanov;[~dongjoon], [~msirek]  what should be the expected result here in both cases a null or ArithmeticException instead?;;;","18/Feb/20 19:42;ksunitha;I looked into this issue and here are some of my notes. 

*Issue:*

Wrong results are returned for aggregate sum with decimals with whole stage codegen enabled 

*Repro:* 

Whole Stage codegen enabled -> Wrong results

Whole Stage codegen disabled -> Returns exception Decimal precision 39 exceeds max precision 38 

*Issues:* 

1: Wrong results are returned which is bad 

2: Inconsistency between whole stage enabled and disabled. 

 

*Cause:*

Sum does not take care of possibility of decimal overflow for the intermediate steps.  ie the updateExpressions and mergeExpressions.  

 

*Some ways to fix this:* 

+Approach 1:+  Do not return wrong results for this scenario, throw exception like whole stage enabled.  DB’s do similar, so there is precedence.  

Pros: 

- No wrong results

- Consistent behavior between wholestage enabled and disabled

- DB’s have similar existing behavior, there is precedence

 

+Approach 2:+ 

By default: Return null on overflow in the sum operation

But if you set spark.sql.ansi.enabled to true, and then it will throw exception. 

 

Pros:

- Maybe ok for users who can tolerate sum to be null on overflow. 

- Consistent with the spark.sql.ansi.enabled behavior

 

Cons:

- This will still keep inconsistency between codegen enabled and disabled. 

 

For those interested, there are some JIRA’s that were fixed for v3.0 which do the following: 
 * SPARK-23179, Throw null on overflow for decimal operations.   This does not kick in for sum for the use case above. 
 * SPARK-28224, that took care of decimal overflow for sum only partially for 2 values.   If you add another row into the dataset, it will return wrong results

 ------

That said, I think both Approach 1 and  Approach 2 will resolve the wrong results which is bad.  

 

Approach 1 is straightforward.   But in the pr’s related to overflow, I think the preference is to have it under a spark.sql.ansi.enabled flag which defaults to false and return null on overflow. 

I think Approach 2 is not as straightforward.  I have an implementation that will fix this. 

I can open 2 prs that implement each of the approach, and would like to get comments.  I have run the sql, catalyst and hive tests and they all pass.   

Please let me know your comments.   Thanks. 

 cc [~dongjoon], [~LI,Xiao], [~cloud_fan] [~hyukjin.kwon] [~hvanhovell] [~javier_ivanov] [~msirek];;;","18/Feb/20 22:26;ksunitha;I have submitted the two pr's for the two approaches I mention above in my comment. 

Approach 1:  Throw exception instead of returning wrong results: [https://github.com/apache/spark/pull/27629]

 

Approach 2:

Return null on decimal overflow for aggregate sum when spark.sql.ansi.enabled=false,  

Throw exception on decimal overflow for aggregate sum when spark.sql.ansi.enabled =true.

[https://github.com/apache/spark/pull/27627] (WIP);;;","02/Jun/20 11:31;cloud_fan;Issue resolved by pull request 27627
[https://github.com/apache/spark/pull/27627];;;","07/Jul/20 16:28;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29026;;;","07/Oct/20 23:33;anuragmantri;I just checked the issue exists in branch-2.4. Since this is a `correctness` issue, should we backport it to branch-2.4? 
cc: [~cloud_fan], [~dongjoon];;;","08/Oct/20 00:00;dongjoon;[~anuragmantri] For this one, this is not backported to 3.0.0, too.;;;",,,,,,,
HuberAggregator copies coefficients vector every time an instance is added,SPARK-28062,13239749,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Andrew-C,Andrew-C,Andrew-C,15/Jun/19 19:24,19/Jun/19 14:00,13/Jul/23 08:45,19/Jun/19 13:57,3.0.0,,,,,,,,3.0.0,,,,ML,,,,,0,,,,,,"Every time an instance is added to the HuberAggregator, a copy of the coefficients vector is created (see code snippet below). This causes a performance degradation, which is particularly severe when the instances have long sparse feature vectors.

{code:scala}
def add(instance: Instance): HuberAggregator = {
    instance match { case Instance(label, weight, features) =>
      require(numFeatures == features.size, s""Dimensions mismatch when adding new sample."" +
        s"" Expecting $numFeatures but got ${features.size}."")
      require(weight >= 0.0, s""instance weight, $weight has to be >= 0.0"")

      if (weight == 0.0) return this
      val localFeaturesStd = bcFeaturesStd.value
      val localCoefficients = bcParameters.value.toArray.slice(0, numFeatures)
val localGradientSumArray = gradientSumArray

// Snip

}
{code}

The LeastSquaresAggregator class avoids this performance issue via the use of transient lazy class variables to store such reused values. Applying a similar approach to HuberAggregator gives a significant speed boost. Running the script below locally on my machine gives the following timing results:

{noformat}
Current implementation: 
    Time(s): 540.1439919471741
    Iterations: 26
    Intercept: 0.518109382890512
    Coefficients: [0.0, -0.2516936902000245, 0.0, 0.0, -0.19633887469839809, 0.0, -0.39565545053893925, 0.0, -0.18617574426698882, 0.0478922416670529]

Modified implementation to match LeastSquaresAggregator:
    Time(s): 46.82946586608887
    Iterations: 26
    Intercept: 0.5181093828893774
    Coefficients: [0.0, -0.25169369020031357, 0.0, 0.0, -0.1963388746927919, 0.0, -0.3956554505389966, 0.0, -0.18617574426702874, 0.04789224166878518]
{noformat}




{code:python}
from random import random, randint, seed
import time

from pyspark.ml.feature import OneHotEncoder
from pyspark.ml.regression import LinearRegression
from pyspark.sql import SparkSession

seed(0)

spark = SparkSession.builder.appName('huber-speed-test').getOrCreate()
df = spark.createDataFrame([[randint(0, 100000), random()] for i in range(100000)],  [""category"", ""target""])
ohe = OneHotEncoder(inputCols=[""category""], outputCols=[""encoded_category""]).fit(df)
lr = LinearRegression(featuresCol=""encoded_category"", labelCol=""target"", loss=""huber"", regParam=1.0)

start = time.time()
model = lr.fit(ohe.transform(df))
end = time.time()

print(""Time(s): "" + str(end - start))
print(""Iterations: "" + str(model.summary.totalIterations))
print(""Intercept: "" + str(model.intercept))
print(""Coefficients: "" + str(list(model.coefficients)[0:10]))
{code}

",,Andrew-C,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 19 13:57:26 UTC 2019,,,,,,,,,,"0|z03sc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/19 13:57;srowen;Issue resolved by pull request 24880
[https://github.com/apache/spark/pull/24880];;;",,,,,,,,,,,,,,,,,,,,,
Reading csv with DROPMALFORMED sometimes doesn't drop malformed records,SPARK-28058,13239649,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,stwhit,stwhit,14/Jun/19 20:48,12/Dec/22 18:10,13/Jul/23 08:45,18/Jun/19 04:49,2.4.1,2.4.3,,,,,,,2.4.4,3.0.0,,,SQL,,,,,1,,,,,,"The spark sql csv reader is not dropping malformed records as expected.

Consider this file (fruit.csv).  Notice it contains a header record, 3 valid records, and one malformed record.

{noformat}
fruit,color,price,quantity
apple,red,1,3
banana,yellow,2,4
orange,orange,3,5
xxx
{noformat}

If I read this file using the spark sql csv reader as follows, everything looks good.  The malformed record is dropped.

{noformat}
scala> spark.read.option(""header"", ""true"").option(""mode"", ""DROPMALFORMED"").csv(""fruit.csv"").show(truncate=false)
+------+------+-----+--------+                                                  
|fruit |color |price|quantity|
+------+------+-----+--------+
|apple |red   |1    |3       |
|banana|yellow|2    |4       |
|orange|orange|3    |5       |
+------+------+-----+--------+
{noformat}

However, if I select a subset of the columns, the malformed record is not dropped.  The malformed data is placed in the first column, and the remaining column(s) are filled with nulls.

{noformat}
scala> spark.read.option(""header"", ""true"").option(""mode"", ""DROPMALFORMED"").csv(""fruit.csv"").select('fruit).show(truncate=false)
+------+
|fruit |
+------+
|apple |
|banana|
|orange|
|xxx   |
+------+

scala> spark.read.option(""header"", ""true"").option(""mode"", ""DROPMALFORMED"").csv(""fruit.csv"").select('fruit, 'color).show(truncate=false)
+------+------+
|fruit |color |
+------+------+
|apple |red   |
|banana|yellow|
|orange|orange|
|xxx   |null  |
+------+------+

scala> spark.read.option(""header"", ""true"").option(""mode"", ""DROPMALFORMED"").csv(""fruit.csv"").select('fruit, 'color, 'price).show(truncate=false)
+------+------+-----+
|fruit |color |price|
+------+------+-----+
|apple |red   |1    |
|banana|yellow|2    |
|orange|orange|3    |
|xxx   |null  |null |
+------+------+-----+
{noformat}

And finally, if I manually select all of the columns, the malformed record is once again dropped.

{noformat}
scala> spark.read.option(""header"", ""true"").option(""mode"", ""DROPMALFORMED"").csv(""fruit.csv"").select('fruit, 'color, 'price, 'quantity).show(truncate=false)
+------+------+-----+--------+
|fruit |color |price|quantity|
+------+------+-----+--------+
|apple |red   |1    |3       |
|banana|yellow|2    |4       |
|orange|orange|3    |5       |
+------+------+-----+--------+
{noformat}

I would expect the malformed record(s) to be dropped regardless of which columns are being selected from the file.
",,stwhit,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28082,SPARK-28079,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 18 04:49:30 UTC 2019,,,,,,,,,,"0|z03rq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/19 12:39;gurwls223;From a cursory look, seems like this behaviour was inherited from Univocity parser.;;;","17/Jun/19 14:24;viirya;This is due to CSV parser column pruning. You can disable it and the behavior would like you expect:
{code:java}
scala> spark.read.option(""header"", ""true"").option(""mode"", ""DROPMALFORMED"").csv(""fruit.csv"").select('fruit).show(truncate=false)
+------+
|fruit |
+------+
|apple |
|banana|
|orange|
|xxx   |
+------+

scala> spark.conf.set(""spark.sql.csv.parser.columnPruning.enabled"", false)

scala> spark.read.option(""header"", ""true"").option(""mode"", ""DROPMALFORMED"").csv(""fruit.csv"").select('fruit).show(truncate=false)
+------+
|fruit |
+------+
|apple |
|banana|
|orange|
+------+
{code};;;","17/Jun/19 14:42;viirya;Although this isn't a bug, I think it might be worth adding a note to current doc to explain/clarify it.;;;","17/Jun/19 15:07;stwhit;Thank you both for your responses.
 
I now see that at the [Spark SQL Upgrading Guide|https://spark.apache.org/docs/2.4.0/sql-migration-guide-upgrade.html], under the [Upgrading From Spark SQL 2.3 to 2.4|https://spark.apache.org/docs/2.4.0/sql-migration-guide-upgrade.html#upgrading-from-spark-sql-23-to-24] section, it states:

{noformat}
In version 2.3 and earlier, CSV rows are considered as malformed if at least one column 
value in the row is malformed. CSV parser dropped such rows in the DROPMALFORMED mode or
outputs an error in the FAILFAST mode. Since Spark 2.4, CSV row is considered as malformed
only when it contains malformed column values requested from CSV datasource, other values
can be ignored. As an example, CSV file contains the “id,name” header and one row “1234”.
In Spark 2.4, selection of the id column consists of a row with one column value 1234 but
in Spark 2.3 and earlier it is empty in the DROPMALFORMED mode. To restore the previous
behavior, set spark.sql.csv.parser.columnPruning.enabled to false.
{noformat}

I had not noticed that until you called the {{spark.sql.csv.parser.columnPruning.enabled}} option to my attention.

Thanks again for the help!;;;","17/Jun/19 15:28;viirya;[~stwhit] Thanks for letting us know that!

Although it is in the migration guide, for new users it should be good to have the note in {{DROPMALFORMED}} doc. I filed SPARK-28082 to track the doc improvement.

;;;","17/Jun/19 15:49;viirya;[~hyukjin.kwon] Do you mean this is suspect to be a bug:

{code}
scala> spark.read.option(""header"", ""true"").option(""mode"", ""DROPMALFORMED"").csv(""fruit.csv"").select('fruit, 'color).show(truncate=false)
+------+------+
|fruit |color |
+------+------+
|apple |red   |
|banana|yellow|
|orange|orange|
|xxx   |null  |
+------+------+
{code}

In this case, the reader should read two columns. But the corrupted record has only one column. Reasonably, it should be dropped as a malformed one. But we see the missing column is filled with null.

This seems to be inherited from Univocity parser, when we use {{CsvParserSettings.selectIndexes}} to do field selection. In above case, the parser returns two tokens where the second token is just null. I'm not sure if it is known behavior of Univocity parser, or it is a bug at Univocity parser.;;;","18/Jun/19 00:31;gurwls223;Yes, that was what I saw and I thought it's a bug in Unviocity. After another thought, yes, it looks an intended behaviour in Univocity (I guess). Thanks guys for clarification.;;;","18/Jun/19 04:49;gurwls223;Issue resolved by pull request 24894
[https://github.com/apache/spark/pull/24894];;;",,,,,,,,,,,,,,
Unable to insert partitioned table dynamically when partition name is upper case,SPARK-28054,13239484,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,514793425@qq.com,514793425@qq.com,14/Jun/19 08:45,12/Dec/22 18:11,13/Jul/23 08:45,24/Jun/19 00:45,2.4.3,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"{code:java}
-- create sql and column name is upper case
CREATE TABLE src (KEY STRING, VALUE STRING) PARTITIONED BY (DS STRING)

-- insert sql
INSERT INTO TABLE src PARTITION(ds) SELECT 'k' key, 'v' value, '1' ds
{code}

The error is:

{code:java}
Error in query: org.apache.hadoop.hive.ql.metadata.Table.ValidationFailureSemanticException: Partition spec {ds=, DS=1} contains non-partition columns;
{code}",,514793425@qq.com,sandeep.katta2007,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 08:36:43 UTC 2020,,,,,,,,,,"0|z03qpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jun/19 15:34;viirya;Is this query working on Hive?;;;","16/Jun/19 13:07;viirya;I tested on Hive, the query works. Btw, the issue is also reproducible on current master.;;;","24/Jun/19 00:45;gurwls223;Issue resolved by pull request 24886
[https://github.com/apache/spark/pull/24886];;;","27/May/20 13:18;sandeep.katta2007;[~hyukjin.kwon] is there any reason why this PR is not backported to branch2.4 ?;;;","28/May/20 08:36;gurwls223;Feel free to open a PR to backport.;;;",,,,,,,,,,,,,,,,,
Handle a corner case where there is no `Link` header,SPARK-28053,13239451,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dongjoon,dongjoon,dongjoon,14/Jun/19 07:14,12/Dec/22 18:10,13/Jul/23 08:45,14/Jun/19 07:34,3.0.0,,,,,,,,3.0.0,,,,Project Infra,,,,,0,,,,,,"Currently, `github_jira_sync.py` assumes that there is `Link` always. However, it will fail when the number of the open PR is less than 100 (the default paging number). It will not happen in Apache Spark, but we had better fix that because it happens during review process for `github_jira_sync.py` script.
{code}
Traceback (most recent call last):
  File ""dev/github_jira_sync.py"", line 139, in <module>
    jira_prs = get_jira_prs()
  File ""dev/github_jira_sync.py"", line 83, in get_jira_prs
    link_header = filter(lambda k: k.startswith(""Link""), page.info().headers)[0]
IndexError: list index out of range
{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 14 07:34:00 UTC 2019,,,,,,,,,,"0|z03qi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jun/19 07:34;gurwls223;Issue resolved by pull request 24874
[https://github.com/apache/spark/pull/24874];;;",,,,,,,,,,,,,,,,,,,,,
ArrayExists should follow the three-valued boolean logic.,SPARK-28052,13239450,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,14/Jun/19 07:05,15/Jun/19 23:01,13/Jul/23 08:45,15/Jun/19 17:48,2.4.3,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"Currently {{ArrayExists}} always returns boolean values (if the arguments are not null), but it should follow the three-valued boolean logic:
 - {{true}} if the predicate holds at least one {{true}}
 - otherwise, {{null}} if the predicate holds {{null}}
 - otherwise, {{false}}",,dongjoon,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 15 17:48:30 UTC 2019,,,,,,,,,,"0|z03qhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/19 17:48;dongjoon;Issue resolved by pull request 24873
[https://github.com/apache/spark/pull/24873];;;",,,,,,,,,,,,,,,,,,,,,
Binary file data source doesn't support space in file names,SPARK-28030,13239073,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,12/Jun/19 15:47,12/Jun/19 20:46,13/Jul/23 08:45,12/Jun/19 20:24,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"{code}
echo 123 > ""/tmp/test space.txt""


spark.read.format(""binaryFile"").load(""/tmp/test space.txt"").count()
{code}",,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 12 20:24:29 UTC 2019,,,,,,,,,,"0|z03o6w:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,"12/Jun/19 20:24;mengxr;Issue resolved by pull request 24855
[https://github.com/apache/spark/pull/24855];;;",,,,,,,,,,,,,,,,,,,,,
HDFSBackedStateStoreProvider should not leak .crc files ,SPARK-28025,13239002,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,gmaas,gmaas,12/Jun/19 10:39,08/Oct/20 08:12,13/Jul/23 08:45,23/Aug/19 06:12,2.4.3,,,,,,,,2.4.4,3.0.0,,,Structured Streaming,,,,,2,,,,,,"The HDFSBackedStateStoreProvider when using the default CheckpointFileManager is leaving '.crc' files behind. There's a .crc file created for each `atomicFile` operation of the CheckpointFileManager.

Over time, the number of files becomes very large. It makes the state store file system constantly increase in size and, in our case, deteriorates the file system performance.

Here's a sample of one of our spark storage volumes after 2 days of execution (4 stateful streaming jobs, each on a different sub-dir):
 # 
{noformat}
Total files in PVC (used for checkpoints and state store)
$find . | wc -l
431796

# .crc files
$find . -name ""*.crc"" | wc -l
418053{noformat}

With each .crc file taking one storage block, the used storage runs into the GBs of data.

These jobs are running on Kubernetes. Our shared storage provider, GlusterFS, shows serious performance deterioration with this large number of files:
{noformat}
DEBUG HDFSBackedStateStoreProvider: fetchFiles() took 29164ms{noformat}
 ","Spark 2.4.3

Kubernetes 1.11(?) (OpenShift)

StateStore storage on a mounted PVC. Viewed as a local filesystem by the `FileContextBasedCheckpointFileManager` : 
{noformat}
scala> glusterfm.isLocal
res17: Boolean = true{noformat}",Bartalos,dongjoon,gmaas,gsomogyi,kabhwan,skonto,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28712,SPARK-17475,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 08 08:12:33 UTC 2020,,,,,,,,,,"0|z03nr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/19 10:40;gmaas;Same problem. A different part of the code.;;;","12/Jun/19 10:50;gmaas;I reproduced  the issue in a  spark-shell session:
{code:java}
____ __
/ __/__ ___ _____/ /__
_\ \/ _ \/ _ `/ __/ '_/
/___/ .__/\_,_/_/ /_/\_\ version 2.4.3
/_/

scala> import org.apache.spark.sql.execution.streaming._
import org.apache.spark.sql.execution.streaming._

scala> val hadoopConf = spark.sparkContext.hadoopConfiguration

scala> import org.apache.spark.sql.internal.SQLConf
import org.apache.spark.sql.internal.SQLConf

scala> SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key
res1: String = spark.sql.streaming.checkpointFileManagerClass

scala> hadoopConf.getSQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key)
res2: String = null

// mount point for the shared PVC: /storage
scala> val glusterCpfm = new org.apache.hadoop.fs.Path(""/storage/crc-store"")
glusterCpfm: org.apache.hadoop.fs.Path = /storage/crc-store

scala> val glusterfm = CheckpointFileManager.create(glusterCpfm, hadoopConf)
glusterfm: org.apache.spark.sql.execution.streaming.CheckpointFileManager = org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager@28d00f54

scala> glusterfm.isLocal
res17: Boolean = true

scala> glusterfm.mkdirs(glusterCpfm)

scala> val atomicFile = glusterfm.createAtomic(new org.apache.hadoop.fs.Path(""/storage/crc-store/file.log""), false)
atomicFile: org.apache.spark.sql.execution.streaming.CheckpointFileManager.CancellableFSDataOutputStream = org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream@1c6e065

scala> atomicFile.writeChars(""Hello, World"")

scala> atomicFile.close

/**
* Inspect the file system
*
* $ cat file.log
* Hello, World
* $ ls -al
* total 5
* drwxr-sr-x. 2 jboss 2000 85 Jun 12 09:44 .
* drwxrwsr-x. 8 root 2000 4096 Jun 12 09:42 ..
* -rw-r--r--. 1 jboss 2000 12 Jun 12 09:44 ..file.log.c6f90863-77d2-494e-b1cc-0d0ed1344f74.tmp.crc
* -rw-r--r--. 1 jboss 2000 24 Jun 12 09:44 file.log
**/

// Delete the file -- simulate the operation done by the HDFSBackedStateStoreProvider#cleanup

scala> glusterfm.delete(new org.apache.hadoop.fs.Path(""/storage/crc-store/file.log""))

/**
* Inspect the file system -> .crc file left behind
* $ ls -al
* total 9
* drwxr-sr-x. 2 jboss 2000 4096 Jun 12 09:46 .
* drwxrwsr-x. 8 root 2000 4096 Jun 12 09:42 ..
* -rw-r--r--. 1 jboss 2000 12 Jun 12 09:44 ..file.log.c6f90863-77d2-494e-b1cc-0d0ed1344f74.tmp.crc
**/
{code};;;","12/Jun/19 12:36;kabhwan;[~gmaas]

Nice finding. Would you like to submit a PR for this? Thanks!

(If you would want to defer to someone, please ping me so that I could take this up.);;;","12/Jun/19 15:45;stevel@apache.org;looking at the previous patch, you don't need to call exists() Before the delete as delete is required to be a no-op if the source isnt' there. Saves the cost of a HEAD if you are using an object store as a destination.;;;","12/Jun/19 18:17;skonto;There is a workaround (avoid creating crc files if you dont want, in certain envs it is the default [https://cloud.google.com/blog/products/storage-data-transfer/new-file-checksum-feature-lets-you-validate-data-transfers-between-hdfs-and-cloud-storage]) by setting `--conf spark.hadoop.spark.sql.streaming.checkpointFileManagerClass=org.apache.spark.sql.execution.streaming.FileSystemBasedCheckpointFileManager` when using local fs

and modifying the FileSystemBasedCheckpointFileManager to run `fs.setWriteChecksum(false)` after fs is created.

Reason is the FileContextBasedCheckpointFileManager will use ChecksumFS ([https://github.com/apache/hadoop/blob/73746c5da76d5e39df131534a1ec35dfc5d2529b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFs.java]) under the hoods which will ignore ""

CreateOpts.checksumParam(ChecksumOpt.createDisabled())"" passed. These settings will only avoid creating checksums for the checksums themshelves and only if the underlying fs supports it. However, it will create the checksum file in any case.

FileSystemBasedCheckpointFileManager uses [https://github.com/apache/hadoop/blob/73746c5da76d5e39df131534a1ec35dfc5d2529b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java] which allows to avoid checksums when creating files if you set that flag.

Note that the crc is created when the tmp file is created not during rename or mv.

I will create a PR shortly. ;;;","12/Jun/19 20:47;kabhwan;Personally I would respect the reporter and encourage to submit a patch by theirselves so that we could have broader contributors, but I assume you two are colleague (same employer) and discussed to decide who to submit a PR.;;;","12/Jun/19 23:27;skonto;[~kabhwan] yes we discussed this internally when trying to solve the issue, otherwise I would not intervene. I fully respect people colleagues or not ;);;;","13/Jun/19 10:26;skonto;I just found out that the following config options would suffice to avoid creating crcs for the given case:

--conf spark.hadoop.spark.sql.streaming.checkpointFileManagerClass=org.apache.spark.sql.execution.streaming.FileSystemBasedCheckpointFileManager --conf spark.hadoop.fs.file.impl=org.apache.hadoop.fs.RawLocalFileSystem

So no need to make a PR to disable crc emission if user wants to for this case. I could make one to cover all cases to be able to enable/disable crcs if needed. However, the filesystem hierarchy in hadoop is a bit inconsistent. So the LocalFileSystem will use a a FilterFileSystem that has the flag setWriteChecksum but the DistributedFileSystem does not have it and it is controlled by property [https://github.com/apache/hadoop/blob/533138718cc05b78e0afe583d7a9bd30e8a48fdc/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/client/impl/TestBlockReaderLocal.java#L620]

`dfs.checksum.type`

Btw there is a nice summary on the topic in chapter 5 of the book: hadoop the definitive guide. 

Looking at the old PR, I am not sure it would work with the LocalFs as it seems that the crc file will be renamed by default as the underlying system supports checksums by default.  ;;;","25/Jul/19 15:21;Bartalos;I'm also affected by performance issue caused by .crc files leak in checkpoint directory. [~skonto] thank you for the workaround, it works.

Would it be possible to implement cleaning of crc files, when one needs the checksum ?;;;","18/Aug/19 08:01;kabhwan;I still think it should rename crc file as well (so that it can be removed altogether if we purge batches) or just remove the crc file when it renames tmp file. If we want to stick with workaround, at least the workaround should be added to the guide doc. It doesn't look like being resolved.;;;","30/Aug/19 11:52;skonto;@[~dongjoon] [~zsxwing] this needs to be re-opened. When using the workaround we recently hit this issue:

[https://github.com/broadinstitute/gatk/issues/1389]

which can be fixed easily with a derived class like in this PR: [https://github.com/broadinstitute/gatk/pull/1421/files]

but this is a bit of inconvenient. 

However, I believe as well that this should be fixed in Spark (less surprises) otherwise we need to document it as [~kabhwan] said above.;;;","30/Aug/19 12:48;kabhwan;[~skonto]

Please take a look at my PR as my PR didn't follow your workaround. We identified which Hadoop issue we are facing, and took a workaround as deleting crc file manually.;;;","30/Aug/19 12:53;stevel@apache.org;Has anyone considered enhancing org.apache.hadoop.fs.ChecksumFileSystem to say ""if ""file.bytes-per-checksum"" == 0 then checksums are disabled?

Currently it fails if bytes per CRC  <= 0, but you could make the 0 value a switch to say ""none"".;;;","30/Aug/19 14:06;skonto;[~kabhwan] cool I have a look.;;;","30/Aug/19 14:09;gsomogyi;[~skonto], this one: [https://github.com/apache/spark/pull/25488];;;","30/Aug/19 14:16;skonto;Thanks I will have a look :);;;","31/Aug/19 04:29;kabhwan;FYI, I just submitted a patch for HADOOP-16255. Hope we can get rid of workaround sooner.;;;","02/Sep/19 08:53;gsomogyi;[~stevel@apache.org] I think adding ""file.bytes-per-checksum = 0"" possibility would be a workaround (at least here). The whole point why one choose ChecksumFileSystem is to have checksum.;;;","08/Oct/20 08:12;kabhwan;[~stevel@apache.org]

Sorry to ping you on the old issue. According to the line we create a temp file, I guess we intend to ""disable"" creating CRC file, so actually it should never be an issue if thing works as intended, whereas it isn't.

[https://github.com/apache/spark/blob/5effa8ea261ba59214afedc2853d1b248b330ca6/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CheckpointFileManager.scala#L306-L311]
{code:java}
// fc = FileContext.getFileContext(...)

 fc.create(
  path, EnumSet.of(CREATE, OVERWRITE), CreateOpts.checksumParam(ChecksumOpt.createDisabled())){code}
Is this something we should fix in Hadoop side, or the option is incorrect and we should apply other option?;;;",,,
Check stringToDate() consumes entire input for the yyyy and yyyy-[m]m formats,SPARK-28015,13238956,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,yumwang,yumwang,12/Jun/19 07:07,11/Jul/19 08:48,13/Jul/23 08:45,11/Jul/19 01:13,1.6.3,2.0.2,2.1.3,2.2.3,2.3.3,2.4.3,,,2.4.4,3.0.0,,,SQL,,,,,0,correctness,,,,,"Invalid date formats should throw an exception:
{code:sql}
SELECT date '1999 08 01'
1999-01-01
{code}

Supported date formats:
https://github.com/apache/spark/blob/ab8710b57916a129fcb89464209361120d224535/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L365-L374

Since Spark 1.6.3 ~ 2.4.3, the behavior is the same.
{code}
spark-sql> SELECT CAST('1999 08 01' AS DATE);
1999-01-01
{code}

Hive returns NULL.",,dongjoon,iskenderunlu804,maxgekk,mgaido,shivusondur@gmail.com,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 11 01:13:31 UTC 2019,,,,,,,,,,"0|z03ngw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/19 19:51;iskenderunlu804;I will try to work on this issue as my first contribution trial.;;;","05/Jul/19 08:22;dongjoon;Thank you, [~iskenderunlu804]. Please file a PR on Apache Spark repo.;;;","10/Jul/19 09:24;maxgekk;I don't think the stringToDate() function should throw an exception in that case. Its return type is Option[SQLDate], so, it should just return None.

[~iskenderunlu804] Are you working on the issue? Please, let us know, if not I will prepare a PR.;;;","10/Jul/19 09:27;iskenderunlu804;Maxim Gekk, you might prepare a PR.;;;","10/Jul/19 17:07;dongjoon;I added `1.6~2.3` as `Affected Versions`, too.
{code}
scala> sql(""SELECT CAST('1999 08 01' AS DATE)"").show
+------------------------+
|CAST(1999 08 01 AS DATE)|
+------------------------+
|              1999-01-01|
+------------------------+
{code};;;","10/Jul/19 17:19;dongjoon;Hi, [~yumwang]. I updated the JIRA title according to the PR.
- We will throw exceptions for `date` prefix
- We will return NULL for CASTING.;;;","11/Jul/19 00:17;yumwang;Thank you [~dongjoon];;;","11/Jul/19 01:13;dongjoon;This is resolved via https://github.com/apache/spark/pull/25097;;;",,,,,,,,,,,,,,
SparkRackResolver should not log for resolving empty list,SPARK-28005,13238855,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,irashid,irashid,11/Jun/19 19:45,17/May/20 17:46,13/Jul/23 08:45,26/Jun/19 14:51,3.0.0,,,,,,,,3.0.0,,,,Scheduler,Spark Core,,,,0,,,,,,"After SPARK-13704, {{SparkRackResolver}} generates an INFO message everytime is called with 0 arguments:

https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/SparkRackResolver.scala#L73-L76

That actually happens every 1s when there are no active executors, because of the repeated offers that happen as part of delay scheduling:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L134-L139

while this is relatively benign, its a pretty annoying thing to be logging at INFO level every 1 second.

This is easy to reproduce -- in spark-shell, with dynamic allocation, set log level to info, see the logs appear every 1 second.  Then run something, see the msgs stop.  After the executors timeout, see the msgs reappear.

{noformat}
scala> :paste
// Entering paste mode (ctrl-D to finish)

sc.setLogLevel(""info"")
Thread.sleep(5000)
sc.parallelize(1 to 10).count()

// Exiting paste mode, now interpreting.

19/06/11 12:43:40 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
19/06/11 12:43:41 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
19/06/11 12:43:42 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
19/06/11 12:43:43 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
19/06/11 12:43:44 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
19/06/11 12:43:45 INFO spark.SparkContext: Starting job: count at <pastie>:28
19/06/11 12:43:45 INFO scheduler.DAGScheduler: Got job 0 (count at <pastie>:28) with 2 output partitions
19/06/11 12:43:45 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (count at <pastie>:28)
...
19/06/11 12:43:54 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
19/06/11 12:43:54 INFO scheduler.DAGScheduler: ResultStage 0 (count at <pastie>:28) finished in 9.548 s
19/06/11 12:43:54 INFO scheduler.DAGScheduler: Job 0 finished: count at <pastie>:28, took 9.613049 s
res2: Long = 10                                                                 

scala> 
...
19/06/11 12:44:56 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
19/06/11 12:44:57 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
19/06/11 12:44:58 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
19/06/11 12:44:59 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
...
{noformat}",,irashid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-13704,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 26 14:51:34 UTC 2019,,,,,,,,,,"0|z03mug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/19 19:46;irashid;cc [~cltlfcjin];;;","26/Jun/19 14:51;irashid;Issue resolved by pull request 24935
[https://github.com/apache/spark/pull/24935];;;",,,,,,,,,,,,,,,,,,,,
PySpark socket server should sync with JVM connection thread future,SPARK-27992,13238638,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,bryanc,bryanc,bryanc,10/Jun/19 21:46,12/Dec/22 18:10,13/Jul/23 08:45,26/Jun/19 20:07,2.4.0,2.4.1,2.4.2,2.4.3,3.0.0,,,,2.4.4,3.0.0,,,PySpark,,,,,0,correctness,,,,,"Both SPARK-27805 and SPARK-27548 identified an issue that errors in a Spark job are not propagated to Python. This is because toLocalIterator() and toPandas() with Arrow enabled run Spark jobs asynchronously in a background thread, after creating the socket connection info. The fix for these was to catch a SparkException if the job errored and then send the exception through the pyspark serializer.

A better fix would be to allow Python to await on the serving thread future and join the thread. That way if the serving thread throws an exception, it will be propagated on the call to awaitResult.",,bryanc,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27805,SPARK-27548,SPARK-28881,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 27 07:41:18 UTC 2019,,,,,,,,,,"0|z03li8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/19 20:07;bryanc;Issue resolved by pull request 24834
[https://github.com/apache/spark/pull/24834];;;","27/Aug/19 06:48;gurwls223;I am increasing the priority to blocker per SPARK-28881. cc [~dongjoon];;;","27/Aug/19 07:25;dongjoon;Thanks!;;;","27/Aug/19 07:41;dongjoon;I updated the issue type from `Improvement` to `Bug` since this is a blocker.;;;",,,,,,,,,,,,,,,,,,
ShuffleBlockFetcherIterator should take Netty constant-factor overheads into account when limiting number of simultaneous block fetches,SPARK-27991,13238597,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,joshrosen,joshrosen,10/Jun/19 17:24,20/May/21 04:28,13/Jul/23 08:45,20/May/21 04:27,2.4.0,,,,,,,,3.2.0,,,,Shuffle,Spark Core,,,,2,,,,,,"ShuffleBlockFetcherIterator has logic to limit the number of simultaneous block fetches. By default, this logic tries to keep the number of outstanding block fetches [beneath a data size limit|https://github.com/apache/spark/blob/v2.4.3/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L274] ({{maxBytesInFlight}}). However, this limiting does not take fixed overheads into account: even though a remote block might be, say, 4KB, there are certain fixed-size internal overheads due to Netty buffer sizes which may cause the actual space requirements to be larger.

As a result, if a map stage produces a huge number of extremely tiny blocks then we may see errors like
{code:java}
org.apache.spark.shuffle.FetchFailedException: failed to allocate 16777216 byte(s) of direct memory (used: 39325794304, max: 39325794304)
at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:554)
at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:485)
[...]
Caused by: io.netty.util.internal.OutOfDirectMemoryError: failed to allocate 16777216 byte(s) of direct memory (used: 39325794304, max: 39325794304)
at io.netty.util.internal.PlatformDependent.incrementMemoryCounter(PlatformDependent.java:640)
at io.netty.util.internal.PlatformDependent.allocateDirectNoCleaner(PlatformDependent.java:594)
at io.netty.buffer.PoolArena$DirectArena.allocateDirect(PoolArena.java:764)
at io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:740)
at io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:244)
at io.netty.buffer.PoolArena.allocate(PoolArena.java:226)
at io.netty.buffer.PoolArena.allocate(PoolArena.java:146)
at io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:324)
[...]{code}
SPARK-24989 is another report of this problem (but with a different proposed fix).

This problem can currently be mitigated by setting {{spark.reducer.maxReqsInFlight}} to some some non-IntMax value (SPARK-6166), but this additional manual configuration step is cumbersome.

Instead, I think that Spark should take these fixed overheads into account in the {{maxBytesInFlight}} calculation: instead of using blocks' actual sizes, use {{Math.min(blockSize, minimumNettyBufferSize)}}. There might be some tricky details involved to make this work on all configurations (e.g. to use a different minimum when direct buffers are disabled, etc.), but I think the core idea behind the fix is pretty simple.

This will improve Spark's stability and removes configuration / tuning burden from end users.",,1916038084@qq.com,ankurd,apachespark,cloud_fan,joshrosen,mauzhang,viirya,xiaojuwu,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24989,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 20 04:27:12 UTC 2021,,,,,,,,,,"0|z03l94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/19 00:27;joshrosen;I've tried to come up with a standalone reproduction of this issue, but so far I've been unable to find one that triggers this error. I've tried creating jobs which run 10000+ mappers shuffling tiny blocks to a single reducer, resulting in thousands of requests in flight, but this has failed to trigger the error posted above.

However, I _did_ manage to get a more complete backtrace from a different internal workload:
{code:java}
Caused by: io.netty.util.internal.OutOfDirectMemoryError: failed to allocate 16777216 byte(s) of direct memory (used: 7918845952, max: 7923040256)
	at io.netty.util.internal.PlatformDependent.incrementMemoryCounter(PlatformDependent.java:640)
	at io.netty.util.internal.PlatformDependent.allocateDirectNoCleaner(PlatformDependent.java:594)
	at io.netty.buffer.PoolArena$DirectArena.allocateDirect(PoolArena.java:764)
	at io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:740)
	at io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:244)
	at io.netty.buffer.PoolArena.allocate(PoolArena.java:226)
	at io.netty.buffer.PoolArena.allocate(PoolArena.java:146)
	at io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:324)
	at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:185)
	at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:176)
	at io.netty.buffer.AbstractByteBufAllocator.ioBuffer(AbstractByteBufAllocator.java:137)
	at io.netty.channel.DefaultMaxMessagesRecvByteBufAllocator$MaxMessageHandle.allocate(DefaultMaxMessagesRecvByteBufAllocator.java:80)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:122)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	... 1 more{code}
Something that jumps out to me is the {{DefaultMaxMessagesRecvByteBufAllocator}} (and the {{AdaptiveRecvByteBufAllocator}} in SPARK-24989): maybe there's something about these failing workloads which is leading to significant space wasting in receive buffers, causing tiny blocks to experience huge bloat in space requirements?;;;","22/Apr/21 05:50;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/32287;;;","22/Apr/21 05:50;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/32287;;;","20/May/21 04:27;cloud_fan;Issue resolved by pull request 32287
[https://github.com/apache/spark/pull/32287];;;",,,,,,,,,,,,,,,,,,
DataSourceV2Relation should not have refresh method,SPARK-27961,13237799,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Gengliang.Wang,jzhuge,jzhuge,05/Jun/19 19:35,09/Jun/19 23:48,13/Jul/23 08:45,08/Jun/19 18:00,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"The newly added `Refresh` method in [PR #24401|https://github.com/apache/spark/pull/24401] prevented me from moving DataSourceV2Relation into catalyst. It calls `case table: FileTable => table.fileIndex.refresh()` while `FileTable` belongs to sql/core.

More importantly, [~rdblue] pointed out DataSourceV2Relation is immutable by design, it should not have refresh method.",,dongjoon,Gengliang.Wang,jzhuge,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27504,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 08 18:00:37 UTC 2019,,,,,,,,,,"0|z03gc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/19 19:39;jzhuge;[~Gengliang.Wang] [~cloud_fan] Could you help?;;;","06/Jun/19 15:55;Gengliang.Wang;[~jzhuge][~rdblue] Make sense. I will remove it.;;;","06/Jun/19 18:04;Gengliang.Wang;So, will Spark support ""refresh table"" in DSV2 implementation?;;;","08/Jun/19 18:00;dongjoon;This is resolved via https://github.com/apache/spark/pull/24815 .
This issue reverted SPARK-27504 partially.;;;",,,,,,,,,,,,,,,,,,
Revert changes introduced as a part of Automatic namespace discovery [SPARK-24149],SPARK-27937,13237338,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Dhruve Ashar,Dhruve Ashar,Dhruve Ashar,03/Jun/19 21:31,20/Aug/19 20:41,13/Jul/23 08:45,20/Aug/19 19:43,2.4.3,,,,,,,,3.0.0,,,,Spark Core,,,,,0,release-notes,,,,,"Spark fails to launch for a valid deployment of HDFS while trying to get tokens for a logical nameservice instead of an actual namenode (with HDFS federation enabled). 

On inspecting the source code closely, it is unclear why we were doing it and based on the context from SPARK-24149, it solves a very specific use case of getting the tokens for only those namenodes which are configured for HDFS federation in the same cluster. IMHO these are better left to the user to specify explicitly.",,Dhruve Ashar,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,"In Spark 3.0, the behavior for automatic delegation token retrieval for file systems is the same as Spark 2.3. Users need to explicitly include the URIs they want to access in the spark.kerberos.access.hadoopFileSystems configuration. The automatic discovery added in Spark 2.4 (SPARK-24149) was removed.",false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 20 19:43:02 UTC 2019,,,,,,,,,,"0|z03dio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/19 21:27;Dhruve Ashar;The exception that we started encountering is while spark tries to create a path of the logic nameservice or nameservice id configured as a part of HDFS federation as a part of the code here:

https://github.com/apache/spark/blob/v2.4.3/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnSparkHadoopUtil.scala#L215

 
{code:java}
19/05/20 08:48:42 INFO SecurityManager: Changing modify acls groups to: 
19/05/20 08:48:42 INFO SecurityManager: SecurityManager: authentication enabled; ui acls enabled; users  with view permissions: Set(...); groups with view permissions: Set(....); users  with modify permissions: Set(....); groups with modify permissions: Set(.....)
19/05/20 08:48:43 INFO Client: Deleted staging directory hdfs://..........:8020/user/abc/.sparkStaging/application_123456_123456
Exception in thread ""main"" java.io.IOException: Cannot create proxy with unresolved address: abcabcabc-nn1:8020
        at org.apache.hadoop.hdfs.NameNodeProxiesClient.createNonHAProxyWithClientProtocol(NameNodeProxiesClient.java:345)
        at org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:133)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:351)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:285)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:160)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2821)
        at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:100)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2892)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2874)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
        at org.apache.spark.deploy.yarn.YarnSparkHadoopUtil$$anonfun$5$$anonfun$apply$2.apply(YarnSparkHadoopUtil.scala:215)
        at org.apache.spark.deploy.yarn.YarnSparkHadoopUtil$$anonfun$5$$anonfun$apply$2.apply(YarnSparkHadoopUtil.scala:214)
        at scala.Option.map(Option.scala:146)
        at org.apache.spark.deploy.yarn.YarnSparkHadoopUtil$$anonfun$5.apply(YarnSparkHadoopUtil.scala:214)
        at org.apache.spark.deploy.yarn.YarnSparkHadoopUtil$$anonfun$5.apply(YarnSparkHadoopUtil.scala:213)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
        at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
        at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)
        at org.apache.spark.deploy.yarn.YarnSparkHadoopUtil$.hadoopFSsToAccess(YarnSparkHadoopUtil.scala:213)
        at org.apache.spark.deploy.yarn.security.YARNHadoopDelegationTokenManager$$anonfun$1.apply(YARNHadoopDelegationTokenManager.scala:43)
        at org.apache.spark.deploy.yarn.security.YARNHadoopDelegationTokenManager$$anonfun$1.apply(YARNHadoopDelegationTokenManager.scala:43)
        at org.apache.spark.deploy.security.HadoopFSDelegationTokenProvider.obtainDelegationTokens(HadoopFSDelegationTokenProvider.scala:48)
{code}
 ;;;","20/Aug/19 19:43;vanzin;Issue resolved by pull request 24785
[https://github.com/apache/spark/pull/24785];;;",,,,,,,,,,,,,,,,,,,,
Semantic equals of CaseWhen is failing with case sensitivity of column Names,SPARK-27917,13237081,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandeep.katta2007,akashrn5,akashrn5,02/Jun/19 03:19,11/Jun/19 14:15,13/Jul/23 08:45,10/Jun/19 22:37,2.1.3,2.2.3,2.3.2,2.4.3,,,,,2.4.4,3.0.0,,,SQL,,,,,0,,,,,,Semantic equals of CaseWhen is failing with case sensitivity of column Names,,akashrn5,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 10 22:37:11 UTC 2019,,,,,,,,,,"0|z03byw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/19 03:28;sandeep.katta2007;This can be tested using the below UT code

val attrRef = AttributeReference(""ACCESS_CHECK"", StringType)()
val aliasAttrRef = attrRef.withName(""access_check"")
val caseWhenObj1 = CaseWhen(Seq((attrRef, Literal(""A""))))
val caseWhenObj2 = CaseWhen(Seq((aliasAttrRef, Literal(""A""))))
assert(caseWhenObj1.semanticEquals(caseWhenObj2));;;","03/Jun/19 03:33;dongjoon;Thank you for reporting. I also reproduced that in 2.4.3, too.;;;","03/Jun/19 12:39;akashrn5;Thank you [~dongjoon](y);;;","10/Jun/19 22:37;dongjoon;This is resolved via https://github.com/apache/spark/pull/24766 at 3.0.0.
We are going to backport to the older branches.;;;",,,,,,,,,,,,,,,,,,
HiveUDAF should return NULL in case of 0 rows,SPARK-27907,13236951,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ajithshetty,ajithshetty,ajithshetty,31/May/19 19:08,02/Mar/20 21:18,13/Jul/23 08:45,02/Jun/19 17:56,2.3.4,2.4.3,,,,,,,2.3.4,2.4.4,3.0.0,,SQL,,,,,0,correctness,,,,,"When query returns zero rows, the HiveUDAFFunction throws NPE

CASE 1:
create table abc(a int)
select histogram_numeric(a,2) from abc // NPE

Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 0, localhost, executor driver): java.lang.NullPointerException
	at org.apache.spark.sql.hive.HiveUDAFFunction.eval(hiveUDFs.scala:471)
	at org.apache.spark.sql.hive.HiveUDAFFunction.eval(hiveUDFs.scala:315)
	at org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:543)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:231)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.outputForEmptyGroupingKeyWithoutInput(ObjectAggregationIterator.scala:97)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:132)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:107)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:839)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:839)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:327)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:291)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:327)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:291)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:327)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:291)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:122)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:425)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1350)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:428)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


CASE 2:

create table abc(a int)
insert into abc values (1)
select histogram_numeric(a,2) from abc where a=3 //NPE

Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 5, localhost, executor driver): java.lang.NullPointerException
	at org.apache.spark.sql.hive.HiveUDAFFunction.serialize(hiveUDFs.scala:477)
	at org.apache.spark.sql.hive.HiveUDAFFunction.serialize(hiveUDFs.scala:315)
	at org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.serializeAggregateBufferInPlace(interfaces.scala:570)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$6(AggregationIterator.scala:254)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.outputForEmptyGroupingKeyWithoutInput(ObjectAggregationIterator.scala:97)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:132)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:107)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:839)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:839)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:327)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:291)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:327)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:291)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:94)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:122)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:425)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1350)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:428)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)",,ajithshetty,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24935,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 02 18:34:24 UTC 2019,,,,,,,,,,"0|z03b68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/19 17:30;dongjoon;Hi, [~ajithshetty].

Did you see this in Spark 2.3.3, too? For me, it works like the following.
{code:java}
scala> spark.version
res0: String = 2.3.3

scala> sql(""create table abc(a int)"")

scala> sql(""select histogram_numeric(a,2) from abc"").show
+------------------------+
|histogram_numeric( a, 2)|
+------------------------+
| null|
+------------------------+{code};;;","02/Jun/19 17:56;dongjoon;I removed 2.3.x from the affected versions. And, this is resolved via [https://github.com/apache/spark/pull/24762] (master/branch-2.4).

 ;;;","02/Jun/19 18:32;dongjoon;SPARK-24935 seems to be the root cause of this regression.;;;","02/Jun/19 18:34;dongjoon;I checked that 2.3.4-SNAPSHOT has this issue because of  -SPARK-24935.- I backported this patch to `branch-2.3`, too.;;;",,,,,,,,,,,,,,,,,,
Fix definition of clustering silhouette coefficient for 1-element clusters,SPARK-27896,13236827,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,31/May/19 12:21,02/Jun/19 18:50,13/Jul/23 08:45,31/May/19 23:30,2.4.3,,,,,,,,3.0.0,,,,ML,,,,,0,,,,,,"Reported by Samuel Kubler via email:

In the code https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/evaluation/ClusteringEvaluator.scala, I think there is a little mistake in the class “Silhouette” when you calculate the Silhouette coefficient for a point. Indeed, according to the scientific paper of reference “Silhouettes:  a graphical aid to the interpretation and validation of cluster analysis” Peter J. ROUSSEEUW 1986, for the points which are alone in a cluster it is not the currentClusterDissimilarity  which is supposed to be equal to 0 like it is the case in your code (“val currentClusterDissimilarity = if (pointClusterNumOfPoints == 1) {0.0}” but the silhouette coefficient itself. Indeed, “When cluster A contains only a single object it is unclear how a(i) should be defined, and the we simply set s(i) equal to zero”.

The problem of defining the currentClusterDissimilarity to zero like you have done is that you can’t use the silhouette coefficient anymore as a criterion to determine the optimal value of the number of clusters in your clustering process because your algorithm will answer that the more clusters you have, the better will be your clustering algorithm. Indeed, in that case when the number of clustering classes increases, s(i) converges toward 1. (so your algorithm seems to be more efficient) I have, beside, check this result of my own clustering example.",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 31 23:30:22 UTC 2019,,,,,,,,,,"0|z03aew:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,"31/May/19 12:22;srowen;Copying follow up from email:

Yes the paper does say the silhouette is 0 in this case. That's an
argument to change it.

On the other hand, I am not sure if I agree with the paper here. If A
consists of one point, then that point's assignment is optimal in a
sense. Setting the silhouette to 0 indicates that assigning it to B,
which is a cluster of more distant points, is just as good. I don't
think that makes as much sense as 1, which it returns now.

You could argue that silhouette is specifically penalizing, in a way,
this type of assignment in a way that Euclidean distance does not.
Wikipedia's definition follows the paper:
https://en.wikipedia.org/wiki/Silhouette_(clustering)
It looks like sklearn also follows the paper's definition:
https://github.com/scikit-learn/scikit-learn/blob/7813f7efb/sklearn/metrics/cluster/unsupervised.py#L235;;;","31/May/19 23:30;dongjoon;This is resolved via https://github.com/apache/spark/pull/24756;;;",,,,,,,,,,,,,,,,,,,,
"Csv reader, adding a corrupt record column causes error if enforceSchema=false",SPARK-27873,13236330,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,mejran,mejran,29/May/19 20:16,12/Dec/22 18:10,13/Jul/23 08:45,03/Jun/19 02:11,2.4.3,,,,,,,,2.4.4,3.0.0,,,SQL,,,,,0,,,,,,"In the Spark CSV reader If you're using permissive mode with a column for storing corrupt records then you need to add a new schema column corresponding to columnNameOfCorruptRecord.

However, if you have a header row and enforceSchema=false the schema vs. header validation fails because there is an extra column corresponding to columnNameOfCorruptRecord.

Since, the FAILFAST mode doesn't print informative error messages on which rows failed to parse there is no way other to track down broken rows without setting a corrupt record column.",,mejran,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25669,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 03 02:11:23 UTC 2019,,,,,,,,,,"0|z037cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/May/19 05:34;gurwls223;Can you show a reproducer with output and expected output?;;;","30/May/19 14:46;viirya;I guess what Marcin meant is:

{code}
val schema = StructType.fromDDL(""a int, b date"")
val columnNameOfCorruptRecord = ""_unparsed""
val schemaWithCorrField1 = schema.add(columnNameOfCorruptRecord, StringType)
val df = spark
  .read
  .option(""mode"", ""Permissive"")
  .option(""header"", ""true"")
  .option(""enforceSchema"", false)
  .option(""columnNameOfCorruptRecord"", columnNameOfCorruptRecord)
  .schema(schemaWithCorrField1)
  .csv(testFile(valueMalformedWithHeaderFile))
{code}

If we want to keep corrupt record, we provide a new column into the schema. But this new column isn't in CSV header. So if enforceSchema is disable at the same time, CSVHeaderChecker throws a exception like:

{code}
[info]   Cause: java.lang.IllegalArgumentException: Number of column in CSV header is not equal to number of fields in the schema:                     
[info]  Header length: 2, schema size: 3       
{code}

It is because CSVHeaderChecker doesn't consider columnNameOfCorruptRecord for now.;;;","30/May/19 14:48;viirya;I can prepare a PR if Marcin or Hyukjin Kwon don't plan to do.;;;","03/Jun/19 02:11;gurwls223;Fixed in https://github.com/apache/spark/pull/24757;;;",,,,,,,,,,,,,,,,,,
Driver and executors use a different service account breaking pull secrets,SPARK-27872,13236226,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,skonto,skonto,skonto,29/May/19 12:19,12/Oct/20 03:38,13/Jul/23 08:45,09/Jun/19 21:29,2.4.3,3.0.0,,,,,,,2.4.8,3.0.0,,,Kubernetes,Spark Core,,,,0,,,,,,"Driver and executors use different service accounts in case the driver has one set up which is different than default: [https://gist.github.com/skonto/9beb5afa2ec4659ba563cbb0a8b9c4dd]

This makes the executor pods fail when the user links the driver service account with a pull secret: [https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account]. Executors will not use the driver's service account and will not be able to get the secret in order to pull the related image. 

I am not sure what is the assumption here for using the default account for executors, probably because of the fact that this account is limited (btw executors dont create resources)? This is an inconsistency that could be worked around with the pod template feature in Spark 3.0.0 but it breaks pull secrets and in general I think its a bug to have it. 

 ",,apachespark,eje,nssalian,skonto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 25 23:06:41 UTC 2020,,,,,,,,,,"0|z036pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/May/19 12:21;skonto;[~eje] [~liyinan926] I can proceed with a PR to fix it if we agree, thoughts?;;;","29/May/19 16:19;eje;[~skonto], executors were never given a service account (or, ""default"") mostly on the principle of least permissions, however I see no problem with providing them the same service account as the driver if it is required for some purpose. Definitely feel free to submit a PR for review.;;;","09/Jun/19 21:29;srowen;Issue resolved by pull request 24748
[https://github.com/apache/spark/pull/24748];;;","23/Sep/20 00:00;apachespark;User 'nssalian' has created a pull request for this issue:
https://github.com/apache/spark/pull/29844;;;","23/Sep/20 00:00;apachespark;User 'nssalian' has created a pull request for this issue:
https://github.com/apache/spark/pull/29844;;;","25/Sep/20 22:06;apachespark;User 'nssalian' has created a pull request for this issue:
https://github.com/apache/spark/pull/29876;;;","25/Sep/20 23:06;apachespark;User 'nssalian' has created a pull request for this issue:
https://github.com/apache/spark/pull/29877;;;","25/Sep/20 23:06;apachespark;User 'nssalian' has created a pull request for this issue:
https://github.com/apache/spark/pull/29877;;;",,,,,,,,,,,,,,
Redact sensitive information in System Properties from UI,SPARK-27869,13236123,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aaruna,aaruna,aaruna,28/May/19 22:51,29/May/19 23:49,13/Jul/23 08:45,29/May/19 17:59,2.3.3,2.4.3,,,,,,,2.3.4,2.4.4,3.0.0,,Web UI,,,,,0,,,,,,"Spark users may have to use System Properties for sensitive information. These system properties are being shown in the Spark UI without any redaction.

 

This JIRA is to redact the system properties in Spark UI as well, similar to how the other properties are redacted in SPARK-18535.",,aaruna,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 29 17:59:22 UTC 2019,,,,,,,,,,"0|z0362g:",9223372036854775807,,,,,dbtsai,,,,,,,,,,,,,,,,,,,"28/May/19 23:48;aaruna;https://github.com/apache/spark/pull/24733;;;","29/May/19 17:59;dongjoon;This is resolved via https://github.com/apache/spark/pull/24733;;;",,,,,,,,,,,,,,,,,,,,
Better document shuffle / RPC listen backlog,SPARK-27868,13236121,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,28/May/19 22:41,08/Jun/20 17:09,13/Jul/23 08:45,29/May/19 21:59,2.4.3,,,,,,,,3.0.0,,,,Documentation,Spark Core,,,,0,,,,,,"The option to control the listen socket backlog for RPC and shuffle servers is not documented in our public docs.

The only piece of documentation is in a Java class, and even that documentation is incorrect:

{code}
  /** Requested maximum length of the queue of incoming connections. Default -1 for no backlog. */
  public int backLog() { return conf.getInt(SPARK_NETWORK_IO_BACKLOG_KEY, -1); }
{code}

The default value actual causes the default value from the JRE to be used, which is 50 according to the docs.",,dongjoon,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 17 18:55:42 UTC 2020,,,,,,,,,,"0|z03620:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/May/19 21:59;dongjoon;This is resolved via https://github.com/apache/spark/pull/24732;;;","16/Jan/20 04:31;dongjoon;This is reverted from `branch-2.4` via https://github.com/apache/spark/commit/94b5d3fd64ea5498b19f9ea7aacb484a18496018;;;","16/Jan/20 04:34;dongjoon;Hi, [~vanzin]. Since there is no proper way to set `Fixed Version` for reverting, I removed `2.4.4` tag and add `release-note` for 2.4.5.
Please advise me if there is a better way~ Thanks.;;;","16/Jan/20 16:34;vanzin;You shoudn't have reverted the whole change. The documentation and extra logging are still really useful.;;;","17/Jan/20 01:14;dongjoon;Oh, do you want me revert back?
Or, I can make a follow-up PR to sync with master after the on-going PR is merged to the master.;;;","17/Jan/20 18:02;vanzin;It's ok for now, it's done. Hopefully 3.0 will come out soon and the ""main"" documentation on the site will have the info.;;;","17/Jan/20 18:55;dongjoon;I see. Thank you, [~vanzin].;;;",,,,,,,,,,,,,,,
Metadata files and temporary files should not be counted as data files,SPARK-27863,13235928,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,28/May/19 07:58,07/Jun/19 06:43,13/Jul/23 08:45,30/May/19 06:17,2.4.3,,,,,,,,2.4.4,3.0.0,,,SQL,,,,,0,,,,,,DataSourceUtils.isDataPath(path) should be DataSourceUtils.isDataPath(status.getPath),,dongjoon,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 30 06:17:04 UTC 2019,,,,,,,,,,"0|z034v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/May/19 06:17;dongjoon;This is resolved via the followings.
- https://github.com/apache/spark/pull/24725 (master)
- https://github.com/apache/spark/pull/24737 (branch-2.4);;;",,,,,,,,,,,,,,,,,,,,,
Fix for avro deserialization on union types with multiple non-null types,SPARK-27858,13235883,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gcmerz,dongjoon,dongjoon,28/May/19 00:09,28/May/19 15:39,13/Jul/23 08:45,28/May/19 03:23,2.4.3,3.0.0,,,,,,,2.4.4,3.0.0,,,SQL,,,,,0,,,,,,"This issue aims to fix a union avro type with more than one non-null value (for instance `[""string"", ""null"", ""int""]`) the deserialization to a DataFrame would throw a `java.lang.ArrayIndexOutOfBoundsException`. The issue was that the `fieldWriter` relied on the index from the avro schema before null were filtered out.
",,apachespark,dongjoon,lingesh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 28 03:23:27 UTC 2019,,,,,,,,,,"0|z034l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/May/19 00:13;apachespark;User 'gcmerz' has created a pull request for this issue:
https://github.com/apache/spark/pull/24722;;;","28/May/19 03:23;dongjoon;This is resolved via https://github.com/apache/spark/pull/24722;;;",,,,,,,,,,,,,,,,,,,,
Make rule EliminateResolvedHint idempotent,SPARK-27824,13235253,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maryannxue,maryannxue,maryannxue,23/May/19 22:58,25/May/19 01:15,13/Jul/23 08:45,24/May/19 18:25,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,This is make sure we get the correct outcome even if the {{EliminateResolveHint}} rule gets applied more than once.,,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-05-23 22:58:57.0,,,,,,,,,,"0|z030pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kubernetes client import non-daemon thread which block jvm exit.,SPARK-27812,13235056,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,igor.calabria,Andrew HUALI,Andrew HUALI,23/May/19 06:53,18/Mar/21 21:36,13/Jul/23 08:45,17/Oct/19 19:24,2.4.3,2.4.4,,,,,,,2.4.5,3.0.0,,,Kubernetes,Spark Core,,,,0,,,,,,"I try spark-submit to k8s with cluster mode. Driver pod failed to exit with An Okhttp Websocket Non-Daemon Thread.

 ",,Andrew HUALI,dongjoon,ebiemond,eje,igor.calabria,irashid,Kotlov,pclay,Qin Yao,runzhliu,sdehaes,skonto,Udbhav Agrawal,,,,,,,,,,,,,,,,,SPARK-28721,SPARK-34674,,,,,,,,,SPARK-26742,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 18 21:36:13 UTC 2021,,,,,,,,,,"0|z02zhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/May/19 20:47;igor.calabria;I believe this was introduced when `kubernetes-client` was updated in https://issues.apache.org/jira/browse/SPARK-26742 . I can confirm that reverting [https://github.com/apache/spark/commit/2d4e9cf84b85a5f8278276e8d8ff59f6f4b11c4c] fixes this issue. This is funny because it was already discussed here (2017): [https://github.com/apache-spark-on-k8s/spark/pull/216#discussion_r112641765 |https://github.com/apache-spark-on-k8s/spark/pull/216#discussion_r112641765.]

The previous solution was setting `.withWebsocketPingInterval({color:#6897bb}0{color})` option when instantiating the client. This prevented the user thread from being created. Maybe something changed with okhttp or kubernetes-client and this is no longer the case.;;;","30/May/19 22:04;dongjoon;Thank you for reporting, [~Andrew HUALI] and thank you for the investigation, [~igor.calabria].

Can we move forward to resolve the issue by upgrading the libraries?;;;","31/May/19 16:02;igor.calabria;[~dongjoon] Do you mean downgrading? Because the issue was introduced when kubernetes client was updated. I took a look at both OkHttp and fabric8's kubernetes client code between the upgraded tags and I couldn't find anything obvious that caused this. 

 

Maybe the right path for spark is to actually deal with ""rogue"" user threads on shutdown/exceptions instead of simply relying that they won't be created by libs or user code.;;;","31/May/19 17:13;dongjoon;What I meat was *upgrading*, [~igor.calabria]. Currently, the latest one is already 4.2.2.;;;","10/Jun/19 05:30;Andrew HUALI;In our private branch, I fix this and potential non-daemon thread introduced by other third party lib by adding SparkUncaughtExceptionHandler to KubernetesClusterSchedulerBackend

. [~dongjoon]

Driver Pod doesn't exit because there is a uncaught exception, kubernetes-client failed to call close method , and non-daemon thread block shutdownhook to get executed. With SparkUncaughtExceptionHandler, we can catch user/spark uncaught exception and Call System.exit which triggers shutdownhook to make things better.

How about this solution?

 ;;;","10/Jun/19 05:35;Andrew HUALI;According to Okhttp Committer *[swankjesse|https://github.com/swankjesse]'s* answer, they won't fix non-daemon thread when using websocket.

[https://github.com/square/okhttp/issues/3339]

[~dongjoon];;;","11/Jun/19 00:37;dongjoon;Thank you for update, [~Andrew HUALI].;;;","11/Jun/19 05:50;Andrew HUALI;So, any other idea to fix it? or I will try to make pr with the SparkUncaughtExceptionHandler solution.  [~dongjoon] ;;;","11/Jun/19 06:40;dongjoon;If SPARK-26742 is the root cause and there is no way, we need to ping there to get a proper help. [~Andrew HUALI], could you comment on the following PR and give the link to this issue, please?
- https://github.com/apache/spark/pull/23993;;;","12/Jun/19 03:45;Andrew HUALI;OK [~dongjoon];;;","12/Jul/19 11:09;skonto;There is an issue in general with setting a driver SparkUncaughtExceptionHandler as described in here: [https://github.com/apache/spark/pull/24796]. I am also in favor of a handler, where we can just sys.exit without running any shutdown hook logic though because of the deadlock issue. 

Btw I dont think we should downgrade we need to move forward and K8s moves fast so we need to do the same. The upgrade happened because the client was very old but jvm exception handling is a pita in general. ;;;","15/Jul/19 13:13;ebiemond;for SPARK-27927 indeed all is fine on spark 2.4.3 when I revert SPARK-26742 commit -> k8s back to kubernetes-client-3.0.0.jar & kubernetes-model-2.0.0.jar

I understand we need to do patching but spark 2.4.3 is maintenance release and this should never happen. 

especially If we not even patch jackson-core to 2.9.9 because of security risks 

 

This has to be done in 3.0.0 and have a proper fix for this blocking threads plus all the time to resolve this.;;;","16/Jul/19 18:14;irashid;as mentioned elsewhere, installing an uncaught exception handler on the driver is not great because it can mess with usercode in the driver.  Maybe we can make it configurable – by default, only install one if there is no uncaught exception handler already installed?  No great solution here ...;;;","24/Jul/19 17:39;eje;Agreed with [~skonto] that downgrading isn't a good option.  We need to keep abreast of K8s (and the api) over time. Invoking sys.exit in theory seems a bit heavy handed, BUT it's also better than just hanging, and I don't know how one would manage a controlled unwind of a deadlock.;;;","13/Sep/19 20:04;igor.calabria;Hi everyone, I have added a simple PR [https://github.com/apache/spark/pull/25785] that fixes this. The SparkUncaughtExceptionHandler PR is not enough because the driver hangs even if there's no exception.

I have no idea if it's viable to merge this, but I'm providing the patch so people may have a quick fix before a better solution is agreed upon.;;;","17/Oct/19 19:24;dongjoon;Issue resolved by pull request 26093
[https://github.com/apache/spark/pull/26093];;;","17/Mar/21 11:16;Kotlov;It seems, I have reproduced this bug in Spark 3.1.1.

If I don't call sparkContext.stop() explicitly, then a Spark driver process doesn't terminate even after its Main method has been completed.
 There are two non-daemon threads, if I don't call sparkContext.stop():
{code:java}
Thread[OkHttp kubernetes.default.svc,5,main]
Thread[OkHttp kubernetes.default.svc Writer,5,main]{code}
It looks like, it prevents the driver jvm process from terminating.

Spark app is started on Amazon EKS (Kubernetes version - 1.17) by _spark-on-k8s-operator: v1beta2-1.2.0-3.0.0_ [(https://github.com/GoogleCloudPlatform/spark-on-k8s-operator)|https://github.com/GoogleCloudPlatform/spark-on-k8s-operator]

Spark docker image is built from the official release of spark-3.1.1 hadoop3.2.;;;","18/Mar/21 21:22;dongjoon;Thank you for reporting, [~Kotlov].
1. Could you file a new JIRA because it might be related to K8s client version?
2. BTW, `sparkContext.stop()` or `spark.stop()` should be called by App. I don't think your use case is a legit Spark example although it might be a behavior change across Spark versions.;;;","18/Mar/21 21:24;dongjoon;Does your example work with any Spark versions before? Then, what is the latest Spark version?;;;","18/Mar/21 21:36;dongjoon;I found your JIRA, SPARK-34674 . Please comment on there because this issue seems irrelevant to yours technically.;;;",,
byName/byPosition should apply to struct fields as well,SPARK-27806,13234936,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,22/May/19 15:55,27/May/19 17:53,13/Jul/23 08:45,23/May/19 17:39,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,,,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 23 17:39:06 UTC 2019,,,,,,,,,,"0|z02yr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/May/19 17:39;dongjoon;This is resolved via https://github.com/apache/spark/pull/24678;;;",,,,,,,,,,,,,,,,,,,,,
fix column pruning for python UDF,SPARK-27803,13234820,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,22/May/19 07:22,12/Dec/22 18:10,13/Jul/23 08:45,27/May/19 12:40,2.4.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 27 12:40:31 UTC 2019,,,,,,,,,,"0|z02y1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/May/19 12:40;gurwls223;Issue resolved by pull request 24675
[https://github.com/apache/spark/pull/24675];;;",,,,,,,,,,,,,,,,,,,,,
ConvertToLocalRelation should tolerate expression reusing output object,SPARK-27798,13234738,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,viirya,ykmori15,ykmori15,21/May/19 22:25,12/Dec/22 18:10,13/Jul/23 08:45,08/Jun/19 19:10,1.6.3,2.0.2,2.1.3,2.2.3,2.4.3,,,,2.3.4,2.4.4,3.0.0,,SQL,,,,,1,correctness,,,,,"Steps to reproduce:

Create a local Dataset (at least two distinct rows) with a binary Avro field. Use the {{from_avro}} function to deserialize the binary into another column. Verify that all of the rows incorrectly have the same value.

Here's a concrete example (using Spark 2.4.3). All it does is converts a list of TestPayload objects into binary using the defined avro schema, then tries to deserialize using {{from_avro}} with that same schema:
{code:java}
import org.apache.avro.Schema
import org.apache.avro.generic.{GenericDatumWriter, GenericRecord, GenericRecordBuilder}
import org.apache.avro.io.EncoderFactory
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.avro.from_avro
import org.apache.spark.sql.functions.col

import java.io.ByteArrayOutputStream

object TestApp extends App {
  // Payload container
  case class TestEvent(payload: Array[Byte])
  // Deserialized Payload
  case class TestPayload(message: String)
  // Schema for Payload
  val simpleSchema =
    """"""
      |{
      |""type"": ""record"",
      |""name"" : ""Payload"",
      |""fields"" : [ {""name"" : ""message"", ""type"" : [ ""string"", ""null"" ] } ]
      |}
    """""".stripMargin
  // Convert TestPayload into avro binary
  def generateSimpleSchemaBinary(record: TestPayload, avsc: String): Array[Byte] = {
    val schema = new Schema.Parser().parse(avsc)
    val out = new ByteArrayOutputStream()
    val writer = new GenericDatumWriter[GenericRecord](schema)
    val encoder = EncoderFactory.get().binaryEncoder(out, null)
    val rootRecord = new GenericRecordBuilder(schema).set(""message"", record.message).build()
    writer.write(rootRecord, encoder)
    encoder.flush()
    out.toByteArray
  }

  val spark: SparkSession = SparkSession.builder().master(""local[*]"").getOrCreate()
  import spark.implicits._
  List(
    TestPayload(""one""),
    TestPayload(""two""),
    TestPayload(""three""),
    TestPayload(""four"")
  ).map(payload => TestEvent(generateSimpleSchemaBinary(payload, simpleSchema)))
    .toDS()
    .withColumn(""deserializedPayload"", from_avro(col(""payload""), simpleSchema))
    .show(truncate = false)
}
{code}
And here is what this program outputs:
{noformat}
+----------------------+-------------------+
|payload               |deserializedPayload|
+----------------------+-------------------+
|[00 06 6F 6E 65]      |[four]             |
|[00 06 74 77 6F]      |[four]             |
|[00 0A 74 68 72 65 65]|[four]             |
|[00 08 66 6F 75 72]   |[four]             |
+----------------------+-------------------+{noformat}
Here, we can see that the avro binary is correctly generated, but the deserialized version is a copy of the last row. I have not yet verified that this is an issue in cluster mode as well.

 

I dug into a bit more of the code and it seems like the resuse of {{result}} in {{AvroDataToCatalyst}} is overwriting the decoded values of previous rows. I set a breakpoint in {{LocalRelation}} and the {{data}} sequence seem to all point to the same address in memory - and therefore a mutation in one variable will cause all of it to mutate.

!Screen Shot 2019-05-21 at 2.39.27 PM.png!",,dongjoon,Gengliang.Wang,viirya,ykmori15,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27027,,,,,,,,,,,,,,"21/May/19 22:26;ykmori15;Screen Shot 2019-05-21 at 2.39.27 PM.png;https://issues.apache.org/jira/secure/attachment/12969324/Screen+Shot+2019-05-21+at+2.39.27+PM.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 08 19:10:57 UTC 2019,,,,,,,,,,"0|z02xj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/May/19 22:38;ykmori15;Someone else seems to have encountered this issue as well https://stackoverflow.com/questions/53623373/spark-2-4-0-to-avro-from-avro-deserialization-not-working-with-seq-todf;;;","22/May/19 03:21;gurwls223;Seems fixed in the current master:

{code}
+----------------------+-------------------+
|payload               |deserializedPayload|
+----------------------+-------------------+
|[00 06 6F 6E 65]      |[one]              |
|[00 06 74 77 6F]      |[two]              |
|[00 0A 74 68 72 65 65]|[three]            |
|[00 08 66 6F 75 72]   |[four]             |
+----------------------+-------------------+
{code}
;;;","30/May/19 06:55;dongjoon;Is this fixed in `branch-2.4`, too?;;;","30/May/19 07:02;dongjoon;Ur, this seems to exist in `branch-2.4` until today.
{code}
+----------------------+-------------------+
|payload               |deserializedPayload|
+----------------------+-------------------+
|[00 06 6F 6E 65]      |[four]             |
|[00 06 74 77 6F]      |[four]             |
|[00 0A 74 68 72 65 65]|[four]             |
|[00 08 66 6F 75 72]   |[four]             |
+----------------------+-------------------+

scala> spark.version
res1: String = 2.4.4-SNAPSHOT
{code}

cc [~dbtsai], [~smilegator], [~cloud_fan], [~Gengliang.Wang] since this is registered as a correctness issue.;;;","30/May/19 07:04;dongjoon;BTW, thank you for reporting, [~ykmori15].;;;","30/May/19 13:41;Gengliang.Wang;[~ykmori15]Thanks for reporting. I am looking into it.;;;","30/May/19 15:07;Gengliang.Wang;Turning off the rule ""ConvertToLocalRelation"" should work around the problem:
spark.conf.set(“spark.sql.optimizer.excludedRules”, “org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation”)
;;;","30/May/19 16:42;Gengliang.Wang;Also, the issue can be reproduced on latest master if we use spark-shell.;;;","30/May/19 19:25;dongjoon;Thank you so much for the investigation and update, [~Gengliang.Wang].;;;","05/Jun/19 05:36;viirya;Is anyone working one this? If none, I will probably send a PR to fix it.;;;","05/Jun/19 05:50;Gengliang.Wang;[~viirya] I am currently working on other issues.
Thanks for working on it!;;;","08/Jun/19 19:10;dongjoon;This is resolved via the following PRs.
- https://github.com/apache/spark/pull/24805 (master)
- https://github.com/apache/spark/pull/24822 (branch-2.4)
- https://github.com/apache/spark/pull/24823 (branch-2.3);;;",,,,,,,,,,
"SHA1, MD5, and Base64 expression codegen doesn't work when commons-codec is shaded",SPARK-27786,13234478,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,joshrosen,joshrosen,21/May/19 05:29,21/May/19 13:21,13/Jul/23 08:45,21/May/19 13:19,2.4.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"When running a custom build of Spark which shades {{commons-codec}}, the {{sha1Hex}} expression generates code which doesn't compile:

{code}
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 93: A method named ""sha1Hex"" is not declared in any enclosing class nor any supertype, nor through a static import
{code}

This is caused by an interaction between Spark's code generator and the shading: the current code generator embeds ""org.apache.commons.codec.digest.DigestUtils.sha1Hex"" into a larger codegen template, preventing JarJarLinks from being able to replace it with the shaded class's name. The generated code ends up using the unshaded name but the unshaded dependency isn't on our classpath, triggering the above compilation error.

To fix this problem and allow for proper shading, we can replace the hardcoded string literal with {{classof[DigestUtils].getName}}",,cloud_fan,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 21 13:19:47 UTC 2019,,,,,,,,,,"0|z02vxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/May/19 13:19;cloud_fan;Issue resolved by pull request 24655
[https://github.com/apache/spark/pull/24655];;;",,,,,,,,,,,,,,,,,,,,,
Use '#' to mark expression id embedded in the subquery name in the SubqueryExec operator.,SPARK-27782,13234378,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dkbiswal,dkbiswal,dkbiswal,20/May/19 18:39,27/May/19 04:18,13/Jul/23 08:45,27/May/19 03:48,2.4.3,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"Use ""#"" to mark the expression id in the name field of `SubqueryExec` operator. Currently in SQLQueryTestSuite we anonymize the expression ids in the output file for comparison purposes. ",,dkbiswal,dongjoon,Francis47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 27 03:48:56 UTC 2019,,,,,,,,,,"0|z02vb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/May/19 03:48;dongjoon;This is resolved via https://github.com/apache/spark/pull/24652;;;",,,,,,,,,,,,,,,,,,,,,
Tried to access method org.apache.avro.specific.SpecificData.<init>()V,SPARK-27781,13234377,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,heuermh,heuermh,20/May/19 18:27,11/Sep/19 19:47,13/Jul/23 08:45,11/Sep/19 19:47,2.4.3,,,,,,,,2.4.4,,,,Spark Core,,,,,0,,,,,,"It appears that there is a conflict in avro dependency versions at runtime when using Spark 2.4.3 and Scala 2.12 (spark-2.4.3-bin-without-hadoop-scala-2.12 binary distribution) and Hadoop 2.7.7.

 

Specifically, the Spark 2.4.3 binary distribution for Hadoop 2.7.x includes avro-1.8.2.jar

{{$ find spark-2.4.3-bin-hadoop2.7 *.jar | grep avro}}

{{jars/avro-1.8.2.jar}}

{{jars/avro-mapred-1.8.2-hadoop2.jar}}

{{jars/avro-ipc-1.8.2.jar}}

 

Whereas the Spark 2.4.3 binary distribution for Scala 2.12 without Hadoop does not

{{$ find spark-2.4.3-bin-without-hadoop-scala-2.12 *.jar | grep avro}}

{{jars/avro-mapred-1.8.2-hadoop2.jar}}

 

Including Hadoop 2.7.7 onto the classpath brings in avro-1.7.4.jar, which conflicts at runtime

{{$ find hadoop-2.7.7 -name *.jar | grep avro}}

{{share/hadoop/mapreduce/lib/avro-1.7.4.jar}}

{{share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/avro-1.7.4.jar}}

{{share/hadoop/tools/lib/avro-1.7.4.jar}}

{{share/hadoop/common/lib/avro-1.7.4.jar}}

{{hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/avro-1.7.4.jar}}

 

Issue filed downstream in

[https://github.com/bigdatagenomics/adam/issues/2151]

 

Attached a smaller reproducing test case.",,heuermh,slachiewicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/19 19:57;heuermh;reproduce.sh;https://issues.apache.org/jira/secure/attachment/12969195/reproduce.sh",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 11 19:46:26 UTC 2019,,,,,,,,,,"0|z02vaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/19 15:45;srowen;I think this duplicates several general ""Avro + Parquet versions = problems"" that aren't really resolvable on the Spark side. Longer term, it's a matter of these project becoming more compatible with each other and upgrading.;;;","16/Jul/19 15:52;heuermh;I believe I saw a fix for this specific issue, where the avro jars are now added to the Spark binary distribution without Hadoop.  Will look for the pull request.

I cannot let Spark off the hook as easily as you suggest though – Spark is the project that brings these dependencies together, as compile time dependencies and on the runtime classpath.  Spark needs to ensure those dependencies are compatible with each other.;;;","03/Sep/19 20:06;heuermh;-This is still an issue with the Spark 2.4.4 binary distribution for Scala 2.12 without Hadoop.-
 https://amplab.cs.berkeley.edu/jenkins//job/ADAM-prb/3047/;;;","11/Sep/19 19:46;heuermh;This issue has been fixed in Spark 2.4.4, and fixed in ADAM Jenkins CI

https://github.com/bigdatagenomics/adam/pull/2206;;;",,,,,,,,,,,,,,,,,,
toPandas with arrow enabled fails for DF with no partitions,SPARK-27778,13234321,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dvogelbacher,dvogelbacher,dvogelbacher,20/May/19 13:57,12/Dec/22 18:10,13/Jul/23 08:45,22/May/19 04:23,3.0.0,,,,,,,,3.0.0,,,,PySpark,SQL,,,,0,,,,,,"Calling to pandas with {{spark.sql.execution.arrow.enabled: true}} fails for dataframes with no partitions. The error is a {{EOFError}}. With {{spark.sql.execution.arrow.enabled: false}} the conversion.

Repro (on current master branch):
{noformat}
>>> from pyspark.sql.types import *
>>> schema = StructType([StructField(""field1"", StringType(), True)])
>>> df = spark.createDataFrame(sc.emptyRDD(), schema)
>>> spark.conf.set(""spark.sql.execution.arrow.enabled"", ""true"")
>>> df.toPandas()
/Users/dvogelbacher/git/spark/python/pyspark/sql/dataframe.py:2162: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.fallback.enabled' does not have an effect on failures in the middle of computation.

  warnings.warn(msg)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/dvogelbacher/git/spark/python/pyspark/sql/dataframe.py"", line 2143, in toPandas
    batches = self._collectAsArrow()
  File ""/Users/dvogelbacher/git/spark/python/pyspark/sql/dataframe.py"", line 2205, in _collectAsArrow
    results = list(_load_from_socket(sock_info, ArrowCollectSerializer()))
  File ""/Users/dvogelbacher/git/spark/python/pyspark/serializers.py"", line 210, in load_stream
    num = read_int(stream)
  File ""/Users/dvogelbacher/git/spark/python/pyspark/serializers.py"", line 810, in read_int
    raise EOFError
EOFError
>>> spark.conf.set(""spark.sql.execution.arrow.enabled"", ""false"")
>>> df.toPandas()
Empty DataFrame
Columns: [field1]
Index: []
{noformat}",,dvogelbacher,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 22 04:23:05 UTC 2019,,,,,,,,,,"0|z02uyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/May/19 13:58;dvogelbacher;I will make a pr for this shortly.;;;","22/May/19 04:23;gurwls223;Issue resolved by pull request 24650
[https://github.com/apache/spark/pull/24650];;;",,,,,,,,,,,,,,,,,,,,
Support user provided avro schema for writing fields with different ordering,SPARK-27762,13234048,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dbtsai,dbtsai,dbtsai,17/May/19 19:57,04/Feb/21 18:58,13/Jul/23 08:45,21/May/19 17:34,2.4.3,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,,,dbtsai,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34365,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 21 17:34:43 UTC 2019,,,,,,,,,,"0|z02t9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/May/19 17:34;dbtsai;Issue resolved by pull request 24635
[https://github.com/apache/spark/pull/24635];;;",,,,,,,,,,,,,,,,,,,,,
Interval string in upper case is not supported in Trigger,SPARK-27735,13233622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,15/May/19 22:37,16/May/19 21:24,13/Jul/23 08:45,16/May/19 21:24,2.4.3,,,,,,,,2.3.4,2.4.4,3.0.0,,Structured Streaming,,,,,0,,,,,,Some APIs in Structured Streaming requires the user to specify an interval. Right now these APIs don't accept upper-case strings.,,dongjoon,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 16 21:24:40 UTC 2019,,,,,,,,,,"0|z02qmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/May/19 21:24;dongjoon;This is resolved via https://github.com/apache/spark/pull/24619;;;",,,,,,,,,,,,,,,,,,,,,
SQL query details in UI dose not show in correct format.,SPARK-27715,13233382,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,uncleGen,uncleGen,uncleGen,15/May/19 06:05,27/Sep/19 04:07,13/Jul/23 08:45,27/Sep/19 04:07,3.0.0,,,,,,,,3.0.0,,,,SQL,Web UI,,,,0,,,,,,,,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 27 04:07:54 UTC 2019,,,,,,,,,,"0|z02p54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/19 04:07;srowen;Resolved by https://github.com/apache/spark/pull/24609;;;",,,,,,,,,,,,,,,,,,,,,
InputFileBlockHolder should be unset at the end of tasks,SPARK-27711,13233299,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joseph.torres,joseph.torres,joseph.torres,14/May/19 18:51,26/May/19 00:30,13/Jul/23 08:45,26/May/19 00:27,2.4.3,,,,,,,,2.4.4,3.0.0,,,PySpark,Spark Core,,,,0,,,,,,InputFileBlockHolder should be unset at the end of each task. Otherwise the value of input_file_name() can leak over to other tasks instead of beginning as empty string.,,dongjoon,joseph.torres,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 26 00:27:00 UTC 2019,,,,,,,,,,"0|z02omo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/May/19 00:27;dongjoon;This is resolved via 
- https://github.com/apache/spark/pull/24605 (master)
- https://github.com/apache/spark/pull/24690 (branch-2.4);;;",,,,,,,,,,,,,,,,,,,,,
InMemoryFileIndex should hard-fail on missing files instead of logging and continuing,SPARK-27676,13232728,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,10/May/19 19:16,12/Dec/22 18:11,13/Jul/23 08:45,26/Jun/19 00:11,2.4.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"Spark's {{InMemoryFileIndex}} contains two places where {{FileNotFound}} exceptions are caught and logged as warnings (during [directory listing|https://github.com/apache/spark/blob/bcd3b61c4be98565352491a108e6394670a0f413/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala#L274] and [block location lookup|https://github.com/apache/spark/blob/bcd3b61c4be98565352491a108e6394670a0f413/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala#L333]). I think that this is a dangerous default behavior and would prefer that Spark hard-fails by default (with the ignore-and-continue behavior guarded by a SQL session configuration).

In SPARK-17599 and SPARK-24364, logic was added to ignore missing files. Quoting from the PR for SPARK-17599:
{quote}The {{ListingFileCatalog}} lists files given a set of resolved paths. If a folder is deleted at any time between the paths were resolved and the file catalog can check for the folder, the Spark job fails. This may abruptly stop long running StructuredStreaming jobs for example.

Folders may be deleted by users or automatically by retention policies. These cases should not prevent jobs from successfully completing.
{quote}
Let's say that I'm *not* expecting to ever delete input files for my job. In that case, this behavior can mask bugs.

One straightforward masked bug class is accidental file deletion: if I'm never expecting to delete files then I'd prefer to fail my job if Spark sees deleted files.

A more subtle bug can occur when using a S3 filesystem. Say I'm running a Spark job against a partitioned Parquet dataset which is laid out like this:
{code:java}
data/
  date=1/
    region=west/
       0.parquet
       1.parquet
    region=east/
       0.parquet
       1.parquet{code}
If I do {{spark.read.parquet(""/data/date=1/"")}} then Spark needs to perform multiple rounds of file listing, first listing {{/data/date=1}} to discover the partitions for that date, then listing within each partition to discover the leaf files. Due to the eventual consistency of S3 ListObjects, it's possible that the first listing will show the {{region=west}} and {{region=east}} partitions existing and then the next-level listing fails to return any for some of the directories (e.g. {{/data/date=1/}} returns files but {{/data/date=1/region=west/}} throws a {{FileNotFoundException}} in S3A due to ListObjects inconsistency).

If Spark propagated the {{FileNotFoundException}} and hard-failed in this case then I'd be able to fail the job in this case where we _definitely_ know that the S3 listing is inconsistent (failing here doesn't guard against _all_ potential S3 list inconsistency issues (e.g. back-to-back listings which both return a subset of the true set of objects), but I think it's still an improvement to fail for the subset of cases that we _can_ detect even if that's not a surefire failsafe against the more general problem).

Finally, I'm unsure if the original patch will have the desired effect: if a file is deleted once a Spark job expects to read it then that can cause problems at multiple layers, both in the driver (multiple rounds of file listing) and in executors (if the deletion occurs after the construction of the catalog but before the scheduling of the read tasks); I think the original patch only resolved the problem for the driver (unless I'm missing similar executor-side code specific to the original streaming use-case).

Given all of these reasons, I think that the ""ignore potentially deleted files during file index listing"" behavior should be guarded behind a feature flag which defaults to {{false}}, consistent with the existing {{spark.files.ignoreMissingFiles}} and {{spark.sql.files.ignoreMissingFiles}} flags (which both default to false).",,joshrosen,marmbrus,siddarthasagar,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17599,SPARK-24364,,,,,,,,SPARK-31545,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 29 17:24:14 UTC 2019,,,,,,,,,,"0|z02l40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/May/19 19:37;marmbrus;I tend to agree that all cases where we chose to ignore missing files should be hidden behind the existing {{spark.sql.files.ignoreMissingFiles}} flag.;;;","21/May/19 10:59;stevel@apache.org;Bear in mind that there are multiple forms of S3 inconsistency which you can observe if you try hard enough

# newly created file still has a 404 returned due to some caching in load balancers before the file was created
# overwritten file still has old file returned on a HEAD or GET. Worse if if the file changes during a read. (HADOOP-15625 will defend against that through failure detection)
# newly deleted file is listed but 404 on open
# newly created file isn't listed. 

This JIRA seems to be looking at issue #3. Are the others also vulnerabilities on this codepath?

Can I highlight this is what S3Guard is intended to defend against by keeping the listing data in DDB up to date with changes made by clients to the store...

;;;","21/May/19 20:46;joshrosen;+1 [~stevel@apache.org]: I agree that this is by no means sufficient to make S3 listing safe (due to all of the potential anomalies you listed). My goal here is to fail fast when we can detect case (3) since that's a better default behavior than continuing and guaranteeing that we'll miss data. Failing loudly and quickly here increases the likelihood that a user will investigate and realize that they have race conditions (perhaps prompting them to use S3Guard, Delta Lake, manifest files, pre-computation of expected filenames (e.g. fixed number of output partitions with consistent filenaming scheme), etc.);;;","26/Jun/19 00:11;gurwls223;Issue resolved by pull request 24668
[https://github.com/apache/spark/pull/24668];;;","29/Oct/19 11:07;siddarthasagar;hi [~stevel@apache.org] can this fix be made available in spark 2.x. it would really help us running spark on aws until spark 3 is released.;;;","29/Oct/19 17:24;stevel@apache.org;
1. Well see if you can create a backport PR and get it through.
1. If you are working on AWS with a S3 store that does not have a consistency layer on top of it (consistent EMR, S3Guard) and you are relying on directory listings to enumerate the content of your tables -you are doomed. Seriously. In particular if you're using the commit-by-rename committers then task and job commit may miss newly created files.

Which means, if you are seeing the problem discussed here it may be a symptom of a larger issue.;;;",,,,,,,,,,,,,,,,
the hint should not be dropped after cache lookup,SPARK-27674,13232665,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,10/May/19 13:36,15/May/19 22:50,13/Jul/23 08:45,15/May/19 22:48,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-05-10 13:36:23.0,,,,,,,,,,"0|z02kq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix error when casting from a nested null in a struct,SPARK-27671,13232629,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,10/May/19 10:39,14/May/19 16:40,13/Jul/23 08:45,13/May/19 19:42,2.4.3,3.0.0,,,,,,,2.4.4,3.0.0,,,SQL,,,,,0,,,,,,"When a null in a nested field in struct, casting from the struct throws error, currently.

{code}
scala> sql(""select cast(struct(1, null) as struct<a:int,b:int>)"").show
scala.MatchError: NullType (of class org.apache.spark.sql.types.NullType$)                                                                             
  at org.apache.spark.sql.catalyst.expressions.Cast.castToInt(Cast.scala:447)                                                                          
  at org.apache.spark.sql.catalyst.expressions.Cast.cast(Cast.scala:635)                                                                               
  at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castStruct$1(Cast.scala:603)   
{code}

{code}
scala> sql(""select * FROM VALUES (('a', (10, null))), (('b', (10, 50))), (('c', null)) AS tab(x, y)"").show                                             
org.apache.spark.sql.AnalysisException: failed to evaluate expression named_struct('col1', 10, 'col2', NULL): NullType (of class org.apache.spark.sql.t
ypes.NullType$); line 1 pos 14                                                                                                                         
  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)                                                     
  at org.apache.spark.sql.catalyst.analysis.ResolveInlineTables.$anonfun$convert$6(ResolveInlineTables.scala:106)        
{code}",,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 14 16:40:03 UTC 2019,,,,,,,,,,"0|z02ki8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/May/19 16:39;dongjoon;Hi, [~viirya]. Could you test old Spark and update the `Affects Version/s` please?;;;","13/May/19 18:35;dongjoon;The patch can land on `master` and `branch-2.4`. From branch-2.3, we need more patches. So, it's not feasible to backport.;;;","13/May/19 19:42;dongjoon;This is resolved via https://github.com/apache/spark/pull/24576;;;","14/May/19 04:40;viirya;[~dongjoon] Thanks for test and updating `Affects Version/s`. Do we need a patch for branch-2.3? If needed, please let me know.;;;","14/May/19 16:40;dongjoon;Thanks, [~viirya]. For now, it looks okay to me.;;;",,,,,,,,,,,,,,,,,
ml.util.Instrumentation.logFailure doesn't log error message,SPARK-27657,13232283,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,darkcaller,mengxr,mengxr,08/May/19 17:27,28/May/19 14:32,13/Jul/23 08:45,28/May/19 14:31,2.4.3,3.0.0,,,,,,,2.4.4,3.0.0,,,ML,,,,,0,,,,,,It only gets the stack trace without the error message.,,darkcaller,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 28 14:31:00 UTC 2019,,,,,,,,,,"0|z02ieg:",9223372036854775807,,,,,,,,,,,,,2.4.4,3.0.0,,,,,,,,,,"08/May/19 17:41;mengxr;[~mrbago] Can you send a PR to fix it?;;;","08/May/19 17:42;mengxr;This is how JDK format an exception: https://github.com/openjdk-mirror/jdk7u-jdk/blob/master/src/share/classes/java/lang/Throwable.java#L654;;;","23/May/19 06:46;darkcaller;If you won't mind, i can send a PR to fix this issue.;;;","28/May/19 14:31;srowen;Issue resolved by pull request 24684
[https://github.com/apache/spark/pull/24684];;;",,,,,,,,,,,,,,,,,,
date format yyyy-M-dd string comparison not handled properly ,SPARK-27638,13231714,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pengbo,pengbo,pengbo,06/May/19 07:59,05/Mar/23 23:49,13/Jul/23 08:45,14/May/19 13:28,2.4.2,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"The below example works with both Mysql and Hive, however not with spark.

{code:java}
mysql> select * from date_test where date_col >= '2000-1-1';
+------------+
| date_col   |
+------------+
| 2000-01-01 |
+------------+
{code}

The reason is that Spark casts both sides to String type during date and string comparison for partial date support. Please find more details in https://issues.apache.org/jira/browse/SPARK-8420.

Based on some tests, the behavior of Date and String comparison in Hive and Mysql:
Hive: Cast to Date, partial date is not supported
Mysql: Cast to Date,  certain ""partial date"" is supported by defining certain date string parse rules. Check out {{str_to_datetime}} in https://github.com/mysql/mysql-server/blob/5.5/sql-common/my_time.c

Here's 2 proposals:
a. Follow Mysql parse rule, but some partial date string comparison cases won't be supported either. 
b. Cast String value to Date, if it passes use date.toString, original string otherwise.
",,cloud_fan,maxgekk,pengbo,roczei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-40610,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 14 13:28:55 UTC 2019,,,,,,,,,,"0|z02ew8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/May/19 08:16;pengbo;[~cloud_fan] [~srowen] Your opinion will be appreciated.;;;","06/May/19 15:43;srowen;CC [~maxgekk] but isn't the issue that your date isn't matching the default format yyyy-MM-dd? what about parsing the string with the format explicitly specified?;;;","06/May/19 15:56;maxgekk;[~srowen] The date literal should be casted to the date type by [stringToDate|https://github.com/apache/spark/blob/ab8710b57916a129fcb89464209361120d224535/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L376] that is able to parse the date by default, see supported patterns:
{code}
`yyyy`
`yyyy-[m]m`
`yyyy-[m]m-[d]d`
`yyyy-[m]m-[d]d `
`yyyy-[m]m-[d]d *`
`yyyy-[m]m-[d]dT*
{code}

 ;;;","06/May/19 16:06;maxgekk;It works with explicit to_date:
{code:scala}
scala> val ds = spark.range(1).selectExpr(""date '2000-01-01' as d"")
ds: org.apache.spark.sql.DataFrame = [d: date]

scala> ds.where(""d >= to_date('2000-1-1')"").show
+----------+
|         d|
+----------+
|2000-01-01|
+----------+
{code}
but with to_date, it compares strings:
{code}
scala> ds.where(""d >= '2000-1-1'"").explain(true)
== Parsed Logical Plan ==
'Filter ('d >= 2000-1-1)
+- Project [10957 AS d#51]
   +- Range (0, 1, step=1, splits=Some(8))

== Analyzed Logical Plan ==
d: date
Filter (cast(d#51 as string) >= 2000-1-1)
+- Project [10957 AS d#51]
   +- Range (0, 1, step=1, splits=Some(8))

== Optimized Logical Plan ==
LocalRelation <empty>, [d#51]

== Physical Plan ==
LocalTableScan <empty>, [d#51]
{code}

The same is for '2000-01-01', the date column is casted to string. ;;;","06/May/19 16:47;srowen;Are you saying that's intended behavior or should be changed?;;;","06/May/19 16:52;cloud_fan;I think it should be changed. When comapring string and int, we cast string to int. When comparing string and date, I think it's reasonable to cast string to date. We also need to think about some corner cases like `date_col > 'invalid_date_string'`.;;;","06/May/19 21:41;maxgekk;[~pengbo] Are you going to propose a PR for that? If not, I can fix the issue.;;;","07/May/19 02:14;pengbo;[~maxgekk] 

I'd love to propose a PR for this. However i am in the middle of something, I will try to do it by the end of this week if that's convenient for you as well.

Besides, what's your suggestion about corner cases like `date_col > 'invalid_date_string'` mentioned by [~cloud_fan] ? Switch back to string comparison?

Thanks;;;","14/May/19 13:28;cloud_fan;Issue resolved by pull request 24567
[https://github.com/apache/spark/pull/24567];;;",,,,,,,,,,,,,
Stage retry causes totalRunningTasks calculation to be negative,SPARK-27630,13231486,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dzcxzl,dzcxzl,dzcxzl,03/May/19 17:50,08/Jun/20 17:13,13/Jul/23 08:45,25/Jun/19 19:30,2.3.0,,,,,,,,3.0.0,,,,Spark Core,,,,,0,,,,,,"Track tasks separately for each stage attempt (instead of tracking by stage), and do NOT reset the numRunningTasks to 0 on StageCompleted.

In the case of stage retry, the {{taskEnd}} event from the zombie stage sometimes makes the number of {{totalRunningTasks}} negative, which will causes the job to get stuck.
 Similar problem also exists with {{stageIdToTaskIndices}} & {{stageIdToSpeculativeTaskIndices}}.
 If it is a failed {{taskEnd}} event of the zombie stage, this will cause {{stageIdToTaskIndices}} or {{stageIdToSpeculativeTaskIndices}} to remove the task index of the active stage, and the number of {{totalPendingTasks}} will increase unexpectedly.",,apachespark,dongjoon,dzcxzl,irashid,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11334,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 27 07:11:04 UTC 2020,,,,,,,,,,"0|z02dhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/May/19 17:53;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/24497;;;","03/May/19 17:53;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/24497;;;","25/Jun/19 19:30;irashid;Issue resolved by pull request 24497
[https://github.com/apache/spark/pull/24497];;;","27/Feb/20 07:11;smilegator;This change breaks the API. Need a release note.
{code:java}
ProblemFilters.exclude[DirectMissingMethodProblem](""org.apache.spark.scheduler.SparkListenerSpeculativeTaskSubmitted.apply""),
ProblemFilters.exclude[DirectMissingMethodProblem](""org.apache.spark.scheduler.SparkListenerSpeculativeTaskSubmitted.copy""),
ProblemFilters.exclude[DirectMissingMethodProblem](""org.apache.spark.scheduler.SparkListenerSpeculativeTaskSubmitted.this""),
ProblemFilters.exclude[MissingTypesProblem](""org.apache.spark.scheduler.SparkListenerSpeculativeTaskSubmitted$""),{code};;;",,,,,,,,,,,,,,,,,,
Prevent Unpickler from intervening each unpickling,SPARK-27629,13231471,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,03/May/19 16:03,12/Dec/22 18:10,13/Jul/23 08:45,04/May/19 04:22,3.0.0,,,,,,,,3.0.0,,,,PySpark,,,,,0,,,,,,"In SPARK-27612, one correctness issue was reported. When protocol 4 is used to pickle Python objects, we found that unpickled objects were wrong. A temporary fix was proposed by not using highest protocol.

It was found that Opcodes.MEMOIZE was appeared in the opcodes in protocol 4. It is suspect to this issue.

A deeper dive found that Opcodes.MEMOIZE stores objects into internal map of Unpickler object. We use single Unpickler object to unpickle serialized Python bytes. Stored objects intervenes next round of unpickling, if the map is not cleared.

We has two options:

1. Continues to reuse Unpickler, but calls its close after each unpickling.
2. Not to reuse Unpickler and create new Unpickler object in each unpickling.

Note: This issue is because internal object map in Pyrolite is not cleared after op code STOP. If we use protocol 4 to pickle Python objects, op code MEMOIZE will store objects in the map. We need to clear up it to make sure next unpickling works on clear map. For now, we can manually clear the map.",,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27612,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 04 04:22:06 UTC 2019,,,,,,,,,,"0|z02de8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/May/19 04:22;gurwls223;Issue resolved by pull request 24521
[https://github.com/apache/spark/pull/24521];;;",,,,,,,,,,,,,,,,,,,,,
Fix `docker-image-tool.sh` to be robust in non-bash shell env,SPARK-27626,13231362,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,02/May/19 21:47,17/May/20 18:25,13/Jul/23 08:45,03/May/19 17:17,3.0.0,,,,,,,,2.3.4,2.4.4,3.0.0,,Kubernetes,Spark Core,,,,0,,,,,,,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 03 17:17:55 UTC 2019,,,,,,,,,,"0|z02cq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/May/19 17:17;dongjoon;This is resolved via https://github.com/apache/spark/pull/24517;;;",,,,,,,,,,,,,,,,,,,,,
ScalaReflection.serializerFor fails for annotated types,SPARK-27625,13231358,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,pgrandjean,pgrandjean,02/May/19 21:27,10/May/19 14:51,13/Jul/23 08:46,10/May/19 14:50,2.3.2,2.4.2,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"ScalaRelfection.serializerFor fails for annotated type. Example:
{code:java}
case class Foo(
  field1: String,
  field2: Option[String] @Bar
)

val rdd: RDD[Foo] = ...
val ds = rdd.toDS // fails at runtime{code}
The stack trace:
{code:java}
// code placeholder
User class threw exception: scala.MatchError: scala.Option[String] @Bar (of class scala.reflect.internal.Types$AnnotatedType)
at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$org$apache$spark$sql$catalyst$ScalaReflection$$serializerFor$1.apply(ScalaReflection.scala:483)
at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$org$apache$spark$sql$catalyst$ScalaReflection$$serializerFor$1.apply(ScalaReflection.scala:445)
at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:56)
at org.apache.spark.sql.catalyst.ScalaReflection$class.cleanUpReflectionObjects(ScalaReflection.scala:824)
at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:39)
at org.apache.spark.sql.catalyst.ScalaReflection$.org$apache$spark$sql$catalyst$ScalaReflection$$serializerFor(ScalaReflection.scala:445)
at ...{code}
I believe that it would be safe to ignore the annotation.",,cloud_fan,pgrandjean,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 10 14:50:56 UTC 2019,,,,,,,,,,"0|z02cp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/May/19 14:50;cloud_fan;Issue resolved by pull request 24564
[https://github.com/apache/spark/pull/24564];;;",,,,,,,,,,,,,,,,,,,,,
Fix CalenderInterval to show an empty interval correctly,SPARK-27624,13231337,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,02/May/19 18:49,08/May/19 21:35,13/Jul/23 08:46,08/May/19 21:35,2.3.3,2.4.2,3.0.0,,,,,,2.3.4,2.4.4,3.0.0,,Spark Core,,,,,0,,,,,,"*BEFORE*
{code}
scala> spark.readStream.schema(""ts timestamp"").parquet(""/tmp/t"").withWatermark(""ts"", ""1 microsecond"").explain
== Physical Plan ==
EventTimeWatermark ts#0: timestamp, interval 1 microseconds
+- StreamingRelation FileSource[/tmp/t], [ts#0]

scala> spark.readStream.schema(""ts timestamp"").parquet(""/tmp/t"").withWatermark(""ts"", ""0 microsecond"").explain
== Physical Plan ==
EventTimeWatermark ts#3: timestamp, interval
+- StreamingRelation FileSource[/tmp/t], [ts#3]
{code}

*AFTER*
{code}
scala> spark.readStream.schema(""ts timestamp"").parquet(""/tmp/t"").withWatermark(""ts"", ""1 microsecond"").explain
== Physical Plan ==
EventTimeWatermark ts#0: timestamp, interval 1 microseconds
+- StreamingRelation FileSource[/tmp/t], [ts#0]

scala> spark.readStream.schema(""ts timestamp"").parquet(""/tmp/t"").withWatermark(""ts"", ""0 microsecond"").explain
== Physical Plan ==
EventTimeWatermark ts#3: timestamp, interval 0 microseconds
+- StreamingRelation FileSource[/tmp/t], [ts#3]
{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 08 21:35:43 UTC 2019,,,,,,,,,,"0|z02ckg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/May/19 21:35;dongjoon;This is resolved via https://github.com/apache/spark/pull/24516;;;",,,,,,,,,,,,,,,,,,,,,
Calling transform() method on a LinearRegressionModel throws NoSuchElementException,SPARK-27621,13231210,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ansarb,ansarb,ansarb,02/May/19 09:19,03/May/19 23:20,13/Jul/23 08:46,03/May/19 23:19,2.3.0,2.3.1,2.3.2,2.3.3,2.3.4,2.4.0,2.4.1,2.4.2,2.3.4,2.4.4,3.0.0,,ML,,,,,0,,,,,,"When transform(...) method is called on a LinearRegressionModel created directly with the coefficients and intercepts, the following exception is encountered.
{code:java}
java.util.NoSuchElementException: Failed to find a default value for loss at org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:780) at org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:780) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.ml.param.Params$class.getOrDefault(params.scala:779) at org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:42) at org.apache.spark.ml.param.Params$class.$(params.scala:786) at org.apache.spark.ml.PipelineStage.$(Pipeline.scala:42) at org.apache.spark.ml.regression.LinearRegressionParams$class.validateAndTransformSchema(LinearRegression.scala:111) at org.apache.spark.ml.regression.LinearRegressionModel.validateAndTransformSchema(LinearRegression.scala:637) at org.apache.spark.ml.PredictionModel.transformSchema(Predictor.scala:192) at org.apache.spark.ml.PipelineModel$$anonfun$transformSchema$5.apply(Pipeline.scala:311) at org.apache.spark.ml.PipelineModel$$anonfun$transformSchema$5.apply(Pipeline.scala:311) at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57) at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66) at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186) at org.apache.spark.ml.PipelineModel.transformSchema(Pipeline.scala:311) at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74) at org.apache.spark.ml.PipelineModel.transform(Pipeline.scala:305)
{code}
This is because validateAndTransformSchema() is called both during training and scoring phases, but the checks against the training related params like loss should really be performed during training phase only, I think, please correct me if I'm missing anything.

This issue was first reported for mleap ([combust/mleap#455|https://github.com/combust/mleap/issues/455]) because basically when we serialize the Spark transformers for mleap, we only serialize the params that are relevant for scoring. We do have the option to de-serialize the serialized transformers back into Spark for scoring again, but in that case, we no longer have all the training params.

Test to reproduce in PR: [https://github.com/apache/spark/pull/24509]

 ",,ansarb,apachespark,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 03 23:19:13 UTC 2019,,,,,,,,,,"0|z02bs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/May/19 09:23;apachespark;User 'ancasarb' has created a pull request for this issue:
https://github.com/apache/spark/pull/24509;;;","02/May/19 09:24;apachespark;User 'ancasarb' has created a pull request for this issue:
https://github.com/apache/spark/pull/24509;;;","03/May/19 23:19;srowen;Issue resolved by pull request 24509
[https://github.com/apache/spark/pull/24509];;;",,,,,,,,,,,,,,,,,,,
MapType should be prohibited in hash expressions,SPARK-27619,13231182,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,rakson,joshrosen,joshrosen,02/May/19 02:36,02/Mar/20 21:10,13/Jul/23 08:46,26/Feb/20 18:01,2.3.0,2.3.4,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,3.0.0,3.0.0,,,,SQL,,,,,0,correctness,,,,,"Spark currently allows MapType expressions to be used as input to hash expressions, but I think that this should be prohibited because Spark SQL does not support map equality.

Currently, Spark SQL's map hashcodes are sensitive to the insertion order of map elements:
{code:java}
val a = spark.createDataset(Map(1->1, 2->2) :: Nil)
val b = spark.createDataset(Map(2->2, 1->1) :: Nil)

// Demonstration of how Scala Map equality is unaffected by insertion order:
assert(Map(1->1, 2->2).hashCode() == Map(2->2, 1->1).hashCode())
assert(Map(1->1, 2->2) == Map(2->2, 1->1))
assert(a.first() == b.first())

// In contrast, this will print two different hashcodes:
println(Seq(a, b).map(_.selectExpr(""hash(*)"").first())){code}
This behavior might be surprising to Scala developers.

I think there's precedence for banning the use of MapType here because we already prohibit MapType in aggregation / joins / equality comparisons (SPARK-9415) and set operations (SPARK-19893).

If we decide that we want this to be an error then it might also be a good idea to add a {{spark.sql.legacy}} flag as an escape-hatch to re-enable the old and buggy behavior (in case applications were relying on it in cases where it just so happens to be safe-by-accident (e.g. maps which only have one entry)).

Alternatively, we could support hashing here if we implemented support for comparable map types (SPARK-18134).",,chakravarthi,dongjoon,javier_ivanov,joshrosen,rakson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 14 09:22:42 UTC 2020,,,,,,,,,,"0|z02bls:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,"22/Jan/20 07:31;dongjoon;I just checked the result on 3.0.0-preview2. This issue is still valid.
{code}
scala> :paste
// Entering paste mode (ctrl-D to finish)

val a = spark.createDataset(Map(1->1, 2->2) :: Nil)
val b = spark.createDataset(Map(2->2, 1->1) :: Nil)

// Demonstration of how Scala Map equality is unaffected by insertion order:
assert(Map(1->1, 2->2).hashCode() == Map(2->2, 1->1).hashCode())
assert(Map(1->1, 2->2) == Map(2->2, 1->1))
assert(a.first() == b.first())

// In contrast, this will print two different hashcodes:
println(Seq(a, b).map(_.selectExpr(""hash(*)"").first()))

// Exiting paste mode, now interpreting.

List([-125051660], [783998793])
a: org.apache.spark.sql.Dataset[scala.collection.immutable.Map[Int,Int]] = [value: map<int,int>]
b: org.apache.spark.sql.Dataset[scala.collection.immutable.Map[Int,Int]] = [value: map<int,int>]

scala> spark.version
res1: String = 3.0.0-preview2
{code};;;","14/Feb/20 09:22;rakson;I am working on this.;;;",,,,,,,,,,,,,,,,,,,,
Creating a DataFrame in PySpark with ArrayType produces some Rows with Arrays of None,SPARK-27612,13231019,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gurwls223,bryanc,bryanc,30/Apr/19 23:26,12/Dec/22 18:10,13/Jul/23 08:46,03/May/19 05:40,3.0.0,,,,,,,,3.0.0,,,,PySpark,SQL,,,,0,correctness,,,,,"This seems to only affect Python 3.

When creating a DataFrame with type {{ArrayType(IntegerType(), True)}} there ends up being rows that are filled with None.

 
{code:java}
In [1]: from pyspark.sql.types import ArrayType, IntegerType                                                                 

In [2]: df = spark.createDataFrame([[1, 2, 3, 4]] * 100, ArrayType(IntegerType(), True))                                     

In [3]: df.distinct().collect()                                                                                              
Out[3]: [Row(value=[None, None, None, None]), Row(value=[1, 2, 3, 4])]
{code}
 

From this example, it is consistently at elements 97, 98:
{code}
In [5]: df.collect()[-5:]                                                                                                    
Out[5]: 
[Row(value=[1, 2, 3, 4]),
 Row(value=[1, 2, 3, 4]),
 Row(value=[None, None, None, None]),
 Row(value=[None, None, None, None]),
 Row(value=[1, 2, 3, 4])]
{code}
This also happens with a type of {{ArrayType(ArrayType(IntegerType(), True))}}",,bryanc,dongjoon,mgaido,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27629,,,SPARK-18161,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 25 09:45:38 UTC 2020,,,,,,,,,,"0|z02als:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,"01/May/19 10:45;mgaido;I am not able to reproduce...

{code}

____ __
 / __/__ ___ _____/ /__
 _\ \/ _ \/ _ `/ __/ '_/
 /__ / .__/\_,_/_/ /_/\_\ version 3.0.0-SNAPSHOT
 /_/

Using Python version 2.7.10 (default, Oct 6 2017 22:29:07)
SparkSession available as 'spark'.
>>> from pyspark.sql.types import ArrayType, IntegerType 
>>> df = spark.createDataFrame([[1, 2, 3, 4]] * 100, ArrayType(IntegerType(), True)) 
>>> df.distinct().collect() 
[Row(value=[1, 2, 3, 4])] 
>>>

{code};;;","01/May/19 16:04;bryanc;Thanks [~mgaido], it seems like the problem does not happen for me with Python 2, so only my Python 3 environments. Would you be able to check with Python 3?;;;","01/May/19 16:10;mgaido;I don't have a python3 env, sorry...;;;","01/May/19 16:10;bryanc;Also cc [~viirya] [~hyukjin.kwon], this is a little strange.. I hope I'm not crazy;;;","02/May/19 12:48;gurwls223;haha, you're not crazy

{code}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT
      /_/

Using Python version 3.7.3 (default, Mar 27 2019 09:23:15)
SparkSession available as 'spark'.
>>> from pyspark.sql.types import ArrayType, IntegerType
>>> df = spark.createDataFrame([[1, 2, 3, 4]] * 100, ArrayType(IntegerType(), True))
>>> df.distinct().collect()
[Row(value=[None, None]), Row(value=[1, 2, 3, 4])]
{code};;;","02/May/19 12:54;viirya;Yup, I can reproduce it too. No worry [~bryanc]. :)
Will take some time to look into it.

;;;","02/May/19 12:57;gurwls223;Argh, seems to be a regression.

{code}
>>> from pyspark.sql.types import ArrayType, IntegerType
>>> df = spark.createDataFrame([[1, 2, 3, 4]] * 100, ArrayType(IntegerType(), True))
>>> df.distinct().collect()

[Row(value=[1, 2, 3, 4])]
{code}

Doesn't happen in Spark 2.4.1 and Spark 2.3.3;;;","02/May/19 13:33;gurwls223;Argh, this happens after we upgraded the cloudpickle to 0.6.2 https://github.com/apache/spark/commit/75ea89ad94ca76646e4697cf98c78d14c6e2695f#diff-19fd865e0dd0d7e6b04b3b1e047dcda7
Upgrading cloudpickle to 0.8.1 still doesn't solve the problem .. I think we should fix it in cloudpickle, I made a cloudpickle release and we port that change into Spark.;;;","02/May/19 15:01;viirya;yeah, seems the issue is happened when python object gets pickled...;;;","03/May/19 05:40;gurwls223;Issue resolved by pull request 24519
[https://github.com/apache/spark/pull/24519];;;","25/Jan/20 09:45;dongjoon;I also did double-check that this is not required in branch-2.4 still.
To distinguish this from the other correctness issue, I set `Target Version` as `3.0.0`.;;;",,,,,,,,,,,
from_json expects values of options dictionary to be ,SPARK-27609,13231004,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maxgekk,zmjjmz,zmjjmz,30/Apr/19 21:42,12/Dec/22 18:11,13/Jul/23 08:46,18/Jul/19 04:37,2.2.1,,,,,,,,3.0.0,,,,PySpark,,,,,2,,,,,,"When reading a column of a DataFrame that consists of serialized JSON, one of the options for inferring the schema and then parsing the JSON is to do a two step process consisting of:
 
{code}
# this results in a new dataframe where the top-level keys of the JSON # are columns
df_parsed_direct = spark.read.json(df.rdd.map(lambda row: row.json_col))

# this does that while preserving the rest of df
schema = df_parsed_direct.schema
df_parsed = df.withColumn('parsed', from_json(df.json_col, schema)
{code}


When I do this, I sometimes find myself passing in options. My understanding is, from the documentation [here|http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.from_json], that the nature of these options passed should be the same whether I do

{code}
spark.read.option('option',value)
{code}

or

{code}
from_json(df.json_col, schema, options={'option':value})
{code}

 
However, I've found that the latter expects value to be a string representation of the value that can be decoded by JSON. So, for example options=\{'multiLine':True} fails with 

{code}
java.lang.ClassCastException: java.lang.Boolean cannot be cast to java.lang.String
{code}

whereas {{options=\{'multiLine':'true'}}} works just fine. 

Notably, providing {{spark.read.option('multiLine',True)}} works fine!

The code for reproducing this issue as well as the stacktrace from hitting it are provided in [this gist|https://gist.github.com/zmjjmz/0af5cf9b059b4969951e825565e266aa]. 

I also noticed that from_json doesn't complain if you give it a garbage option key – but that seems separate.","I've found this issue on an AWS Glue development endpoint which is running Spark 2.2.1 and being given jobs through a SparkMagic Python 2 kernel, running through livy and all that. I don't know how much of that is important for reproduction, and can get more details if needed. ",maxgekk,zmjjmz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 18 04:37:32 UTC 2019,,,,,,,,,,"0|z02aig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/May/19 08:37;gurwls223;Okay, we can match to \{read.option(...)}.;;;","03/May/19 08:37;gurwls223;We can do from_json, from_csv too while we're here. Wanna PR for that?;;;","03/May/19 16:20;zmjjmz;I can try, but I have exactly 0 experience developing any Spark stuff, so it'll likely take me a while.;;;","03/May/19 23:46;gurwls223;It's fine. Take a look at [https://spark.apache.org/contributing.html] and similar PRs in the history via Github blame, and form the PR similarily. Don't forget to add a test.;;;","11/Jul/19 11:48;maxgekk;[~zmjjmz] Please, let us know if you are working on this. If not, I could prepare a PR.;;;","11/Jul/19 18:31;zmjjmz;Sorry, no I haven't had the time to do this and I don't anticipate being able to anytime soon.;;;","18/Jul/19 04:37;gurwls223;Issue resolved by pull request 25182
[https://github.com/apache/spark/pull/25182];;;",,,,,,,,,,,,,,,
The JDBC 'query' option doesn't work for Oracle database,SPARK-27596,13230731,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,smilegator,smilegator,29/Apr/19 16:26,16/Sep/19 01:12,13/Jul/23 08:46,06/May/19 05:06,2.4.2,,,,,,,,2.4.4,3.0.0,,,SQL,,,,,0,,,,,,"For the JDBC option `query`, we use the identifier name to start with underscore: s""(${subquery}) __SPARK_GEN_JDBC_SUBQUERY_NAME_${curId.getAndIncrement()}"". This is not supported by Oracle. 

The Oracle doesn't seem to support identifier name to start with non-alphabet character (unless it is quoted) and has length restrictions as well.
https://docs.oracle.com/cd/B19306_01/server.102/b14200/sql_elements008.htm

{code:java}
Nonquoted identifiers must begin with an alphabetic character from your database character set. Quoted identifiers can begin with any character as per below documentation - 
Nonquoted identifiers can contain only alphanumeric characters from your database character set and the underscore (_), dollar sign ($), and pound sign (#). Database links can also contain periods (.) and ""at"" signs (@). Oracle strongly discourages you from using $ and # in nonquoted identifiers.
{code}

The alias name '_SPARK_GEN_JDBC_SUBQUERY_NAME<int value>' should be fixed to remove ""__"" prefix ( or make it quoted.not sure if it may impact other sources) to make it work for Oracle. Also the length should be limited as it is hitting below error on removing the prefix.

{code:java}
java.sql.SQLSyntaxErrorException: ORA-00972: identifier is too long 
{code}

It can be verified using below sqlfiddle link.

http://www.sqlfiddle.com/#!4/9bbe9a/10050

",,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29085,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-04-29 16:26:51.0,,,,,,,,,,"0|z028ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A bug in UnivocityParser prevents using UDT,SPARK-27591,13230647,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,drKalko,drKalko,drKalko,29/Apr/19 09:30,12/Dec/22 18:10,13/Jul/23 08:46,30/Apr/19 23:29,2.4.2,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"I am trying to define a UserDefinedType based on String but different from StringType in Spark 2.4.1 but it looks like there is a bug in Spark or I am doing smth incorrectly.

I define my type as follows:
{code:java}
class MyType extends UserDefinedType[MyValue] {
  override def sqlType: DataType = StringType
  ...
}

@SQLUserDefinedType(udt = classOf[MyType])
case class MyValue
{code}
I expect it to be read and stored as String with just a custom SQL type. In fact Spark can't read the string at all:
{code:java}
java.lang.ClassCastException: org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$makeConverter$11 cannot be cast to org.apache.spark.unsafe.types.UTF8String
    at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.getUTF8String(rows.scala:46)
    at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.getUTF8String(rows.scala:195)
    at org.apache.spark.sql.catalyst.expressions.JoinedRow.getUTF8String(JoinedRow.scala:102)
{code}
the problem is with UnivocityParser.makeConverter that doesn't return (String => Any) function but (String => (String => Any)) in the case of UDT, see UnivocityParser:184
{code:java}
case udt: UserDefinedType[_] => (datum: String) =>
  makeConverter(name, udt.sqlType, nullable, options)

{code}",,drKalko,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 30 23:29:02 UTC 2019,,,,,,,,,,"0|z028b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/19 09:40;drKalko;I change the line UnivocityParser:184 to 

 
{code:java}
case udt: UserDefinedType[_] => 
  makeConverter(name, udt.sqlType, nullable, options)
{code}
and it works as I expected

 ;;;","29/Apr/19 15:13;viirya;Are you returning string in {{serialize}} in your {{UserDefinedType}}? If so, please creating a {{UTF8String}} based on the string.

And also in {{deserialize}}, the passed in data is {{UTF8String}}.;;;","29/Apr/19 15:34;drKalko;[~viirya], yes, I'm returning a string. I tried creating a UTF8String, but it didn't help. As I mentioned in the description, the method is returning String => makeConverter(...) instead of just calling makeConverter on udt.sqlType;;;","29/Apr/19 15:50;viirya;oh, you're right. I've misread the description. Want to submit a PR with your fix?;;;","29/Apr/19 15:59;drKalko;yes, I will later today;;;","30/Apr/19 23:29;gurwls223;Issue resolved by pull request 24496
[https://github.com/apache/spark/pull/24496];;;",,,,,,,,,,,,,,,,
do not consider skipped tasks when scheduling speculative tasks,SPARK-27590,13230622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,29/Apr/19 07:26,07/May/19 18:17,13/Jul/23 08:46,07/May/19 17:02,3.0.0,,,,,,,,3.0.0,,,,Spark Core,,,,,0,,,,,,,,cloud_fan,irashid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 07 17:02:43 UTC 2019,,,,,,,,,,"0|z0285k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/May/19 17:02;irashid;Issue resolved by pull request 24485
[https://github.com/apache/spark/pull/24485];;;",,,,,,,,,,,,,,,,,,,,,
"DataFrame countDistinct(""*"") fails with AnalysisException: ""Invalid usage of '*' in expression 'count'""",SPARK-27581,13230420,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,joshrosen,joshrosen,27/Apr/19 02:06,29/Apr/19 13:19,13/Jul/23 08:46,29/Apr/19 13:18,2.4.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"If I have a DataFrame then I can use {{count(""*"")}} as an expression, e.g.:
{code:java}
import org.apache.spark.sql.functions._
val df = sql(""select id % 100 from range(100000)"")
df.select(count(""*"")).first()
{code}
However, if I try to do the same thing with {{countDistinct}} I get an error:
{code:java}
import org.apache.spark.sql.functions._
val df = sql(""select id % 100 from range(100000)"")
df.select(countDistinct(""*"")).first()

org.apache.spark.sql.AnalysisException: Invalid usage of '*' in expression 'count';
{code}
As a workaround, I need to use {{expr}}, e.g.
{code:java}
import org.apache.spark.sql.functions._
val df = sql(""select id % 100 from range(100000)"")
df.select(expr(""count(distinct(*))"")).first()
{code}
You might be wondering ""why not just use {{df.count()}} or {{df.distinct().count()}}?"" but in my case I'd ultimately to compute both counts as part of the same aggregation, e.g.
{code:java}
val (cnt, distinctCnt) = df.select(count(""*""), countDistinct(""*)).as[(Long, Long)].first()
{code}
I'm reporting this because it's a minor usability annoyance / surprise for inexperienced Spark users.",,cloud_fan,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 29 13:18:34 UTC 2019,,,,,,,,,,"0|z026wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/19 13:18;cloud_fan;Issue resolved by pull request 24482
[https://github.com/apache/spark/pull/24482];;;",,,,,,,,,,,,,,,,,,,,,
Wrong thresholds selected by BinaryClassificationMetrics when downsampling,SPARK-27577,13230257,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shishaochen,shishaochen,shishaochen,26/Apr/19 09:08,07/May/19 13:46,13/Jul/23 08:46,07/May/19 13:46,2.3.0,2.3.1,2.3.2,2.3.3,2.4.0,2.4.1,2.4.2,3.0.0,2.3.4,2.4.4,3.0.0,,MLlib,,,,,0,,,,,,"In binary metrics, a threshold means any instance with a score >= threshold will be considered as positive.

However, in the existing implementation:
 # When `numBins` is set when creating a `BinaryClassificationMetrics` object, all records (ordered by scores in DESC) will be grouped into chunks.
 # In each chunk, statistics (in `BinaryLabelCounter`) of records are accumulated while the first record's score (also the largest) is selected as threshold.
 # All these generated/sampled records form a new smaller data set to calculate binary metrics.

At the second step, it brings the BUG that the score/threshold of a record is correlated with wrong values like larger `true positive`, smaller `false negative` when calculating `recallByThresholds`, `precisionByThresholds`, etc.

Thus, the BUG fix is straightfoward. Let's pick up the last records's core in all chunks as thresholds while statistics merged.",,shishaochen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 07 13:46:36 UTC 2019,,,,,,,,,,"0|z025wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/May/19 13:46;srowen;Resolved by https://github.com/apache/spark/pull/24470;;;",,,,,,,,,,,,,,,,,,,,,
Spark overwrites existing value of spark.yarn.dist.* instead of merging value,SPARK-27575,13230228,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,26/Apr/19 06:10,23/Dec/20 01:07,13/Jul/23 08:46,29/Apr/19 17:15,2.3.4,2.4.0,,,,,,,2.4.8,3.0.0,,,Spark Core,Spark Submit,YARN,,,0,,,,,,"If we specify `--files` arg when submitting app where configuration has ""spark.yarn.dist.files"", SparkSubmit overwrites the new files (files provided in arg) instead of merging existing value in configuration.

Same issue happens also on ""spark.yarn.dist.pyFiles"", ""spark.yarn.dist.jars"", ""spark.yarn.dist.archives"".

 

While I encountered the issue in Spark 2.4.0, I can see the issue from codebase in master branch and also branch-2.3 as well.",,apachespark,kabhwan,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 22 20:16:03 UTC 2020,,,,,,,,,,"0|z025q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/19 17:15;vanzin;Issue resolved by pull request 24465
[https://github.com/apache/spark/pull/24465];;;","22/Dec/20 20:15;apachespark;User 'anuragmantri' has created a pull request for this issue:
https://github.com/apache/spark/pull/30895;;;","22/Dec/20 20:16;apachespark;User 'anuragmantri' has created a pull request for this issue:
https://github.com/apache/spark/pull/30895;;;",,,,,,,,,,,,,,,,,,,
NPE in TaskCompletionListener due to Spark OOM in UnsafeExternalSorter causing tasks to hang,SPARK-27558,13229907,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ayudovin,abellina,abellina,24/Apr/19 15:41,12/Dec/22 18:11,13/Jul/23 08:46,19/Nov/19 06:06,2.3.3,2.4.2,,,,,,,2.4.5,3.0.0,,,Spark Core,,,,,0,,,,,,"We see an NPE in the UnsafeInMemorySorter.getMemoryUsage function (due to the array we are accessing there being null). This looks to be caused by a Spark OOM when UnsafeInMemorySorter is trying to spill.

This is likely a symptom of https://issues.apache.org/jira/browse/SPARK-21492. The real question for this ticket is, could we handle things more gracefully, rather than NPE. For example:

Remove this:

https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java#L182

so when this fails (and store the new array into a temporary):

https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java#L186

we don't end up with a null ""array"". This state is causing one of our jobs to hang infinitely (we think) due to the original allocation error.

Stack trace for reference

{noformat}
2019-04-23 08:57:14,989 [Executor task launch worker for task 46729] ERROR org.apache.spark.TaskContextImpl  - Error in TaskCompletionListener
java.lang.NullPointerException
	at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.getMemoryUsage(UnsafeInMemorySorter.java:208)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getMemoryUsage(UnsafeExternalSorter.java:249)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.updatePeakMemoryUsed(UnsafeExternalSorter.java:253)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.freeMemory(UnsafeExternalSorter.java:296)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.cleanupResources(UnsafeExternalSorter.java:328)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.lambda$new$0(UnsafeExternalSorter.java:178)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:118)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:118)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:131)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:129)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:129)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:117)
	at org.apache.spark.scheduler.Task.run(Task.scala:119)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


2019-04-23 08:57:15,069 [Executor task launch worker for task 46729] ERROR org.apache.spark.executor.Executor  - Exception in task 102.0 in stage 28.0 (TID 46729)
org.apache.spark.util.TaskCompletionListenerException: null

Previous exception in task: Unable to acquire 65536 bytes of memory, got 0
	org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)
	org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:98)
	org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.reset(UnsafeInMemorySorter.java:186)
	org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:229)
	org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:204)
	org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:283)
	org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:96)
	org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.growPointerArrayIfNecessary(UnsafeExternalSorter.java:348)
	org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:403)
	org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:135)
	org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage23.sort_addToSorter_0$(Unknown Source)
	org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage23.processNext(Unknown Source)
	org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
	org.apache.spark.sql.execution.window.WindowExec$$anonfun$11$$anon$1.fetchNextRow(WindowExec.scala:314)
	org.apache.spark.sql.execution.window.WindowExec$$anonfun$11$$anon$1.<init>(WindowExec.scala:323)
	org.apache.spark.sql.execution.window.WindowExec$$anonfun$11.apply(WindowExec.scala:303)
	org.apache.spark.sql.execution.window.WindowExec$$anonfun$11.apply(WindowExec.scala:302)
	org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	org.apache.spark.scheduler.Task.run(Task.scala:109)
	org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	java.lang.Thread.run(Thread.java:748)
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:139)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:117)
	at org.apache.spark.scheduler.Task.run(Task.scala:119)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{noformat}
",,abellina,jiangxb1987,joshrosen,xiaojuwu,,,,,,,,,,,,,,,,,,,,,,SPARK-21492,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 19 06:22:51 UTC 2019,,,,,,,,,,"0|z023qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/19 05:44;gurwls223;Please avoid to set Critical+ which is usually reserved for committers.;;;","15/Oct/19 19:02;joshrosen;I just ran into this as well: it looks like the problem is that {{UnsafeInMemorySorter.getMemoryUsage}} does not gracefully handle the case where {{array = null}}. I suspect that adding 
{code:java}
if (array != null) {
  current code 
} else {
  0L
} {code}
would be a correct, sufficient fix.;;;","19/Nov/19 06:07;jiangxb1987;Resolved by https://github.com/apache/spark/pull/26349;;;","19/Nov/19 06:22;jiangxb1987;Trying to clarify the behavior: did the task failed, or did it hang forever ?;;;",,,,,,,,,,,,,,,,,,
cannot create table by using the hive default fileformat in both hive-site.xml and spark-defaults.conf,SPARK-27555,13229807,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandeep.katta2007,Hui WANG,Hui WANG,24/Apr/19 08:40,12/Dec/22 18:11,13/Jul/23 08:46,04/May/19 00:03,2.3.2,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"*You can see details in attachment called Try.pdf.*

I already seen https://issues.apache.org/jira/browse/SPARK-17620 

and https://issues.apache.org/jira/browse/SPARK-18397

and I check source code of Spark for the change of  set ""spark.sql.hive.covertCTAS=true"" and then spark will use ""spark.sql.sources.default"" which is parquet as storage format in ""create table as select"" scenario.

But my case is just create table without select. When I set  hive.default.fileformat=parquet in hive-site.xml or set  spark.hadoop.hive.default.fileformat=parquet in spark-defaults.conf, after create a table, when i check the hive table, it still use textfile fileformat.

 

It seems HiveSerDe gets the value of the hive.default.fileformat parameter from SQLConf

The parameter values in SQLConf are copied from SparkContext's SparkConf at SparkSession initialization, while the configuration parameters in hive-site.xml are loaded into SparkContext's hadoopConfiguration parameters by SharedState, And all the config with ""spark.hadoop"" conf are setted to hadoopconfig, so the configuration does not take effect.

 

 

 ",,Hui WANG,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/19 06:27;Hui WANG;Try.pdf;https://issues.apache.org/jira/secure/attachment/12966986/Try.pdf",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 04 05:39:51 UTC 2019,,,,,,,,,,"0|z0234w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/19 05:47;gurwls223;can you post a self-contained reproducer please?;;;","25/Apr/19 06:29;Hui WANG;ok, I already attached the details of reproduce in the attachment area called Try.pdf.;;;","04/May/19 00:03;gurwls223;Issue resolved by pull request 24489
[https://github.com/apache/spark/pull/24489];;;","04/May/19 05:39;sandeep.katta2007;[~hyukjin.kwon] I think by mistake assignee name is incorrect, can you please update it ;;;",,,,,,,,,,,,,,,,,,
The configuration `hive.exec.stagingdir` is invalid on Windows OS,SPARK-27552,13229755,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,10110346,10110346,10110346,24/Apr/19 02:31,17/May/19 19:03,13/Jul/23 08:46,17/May/19 19:00,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"If we set _{{hive.exec.stagingdir=.test-staging\tmp}}_,
 But the staging directory is still _{{.hive-staging}}_ on Windows OS.",,10110346,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 17 19:00:39 UTC 2019,,,,,,,,,,"0|z022tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/May/19 19:00;srowen;Issue resolved by pull request 24446
[https://github.com/apache/spark/pull/24446];;;",,,,,,,,,,,,,,,,,,,,,
Fix `test-dependencies.sh` not to use `kafka-0-8` profile for Scala-2.12,SPARK-27550,13229721,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,23/Apr/19 22:04,24/Apr/19 14:31,13/Jul/23 08:46,24/Apr/19 14:31,2.4.3,,,,,,,,2.4.3,,,,Tests,,,,,0,,,,,,"Since SPARK-27274 deprecated Scala-2.11 at Spark 2.4.1, we need to test Scala-2.12 more.

Kakfa 0.8 doesn't have Scala-2.12 artifacts, e.g., `org.apache.kafka:kafka_2.12:jar:0.8.2.1`. This issue aims to fix `test-dependencies.sh` script to understand Scala binary version.
{code:java}
$ dev/change-scala-version.sh 2.12
$ dev/test-dependencies.sh
Using `mvn` from path: /usr/local/bin/mvn
Using `mvn` from path: /usr/local/bin/mvn
Performing Maven install for hadoop-2.6
Using `mvn` from path: /usr/local/bin/mvn
[ERROR] Failed to execute goal on project spark-streaming-kafka-0-8_2.12: Could not resolve dependencies for project org.apache.spark:spark-streaming-kafka-0-8_2.12:jar:spark-335572: Could not find artifact org.apache.kafka:kafka_2.12:jar:0.8.2.1 in central (https://repo.maven.apache.org/maven2) -> [Help 1]
{code}
Please note that this issue doesn't aim to update `spark-deps-hadoop-2.6` manifest here.",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 24 14:31:23 UTC 2019,,,,,,,,,,"0|z022ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/19 14:31;dongjoon;This is resolved via https://github.com/apache/spark/pull/24445;;;",,,,,,,,,,,,,,,,,,,,,
PySpark toLocalIterator does not raise errors from worker,SPARK-27548,13229685,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bryanc,bryanc,bryanc,23/Apr/19 18:23,10/Jun/19 21:47,13/Jul/23 08:46,07/May/19 21:49,2.4.1,,,,,,,,3.0.0,,,,PySpark,,,,,0,,,,,,"When using a PySpark RDD local iterator and an error occurs on the worker, it is not picked up by Py4J and raised in the Python driver process. So unless looking at logs, there is no way for the application to know the worker had an error. This is a test that should pass if the error is raised in the driver:
{code}
def test_to_local_iterator_error(self):

    def fail(_):
        raise RuntimeError(""local iterator error"")

    rdd = self.sc.parallelize(range(10)).map(fail)

    with self.assertRaisesRegexp(Exception, ""local iterator error""):
        for _ in rdd.toLocalIterator():
            pass{code}
but it does not raise an exception:
{noformat}
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/home/bryan/git/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 428, in main
    process()
  File ""/home/bryan/git/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 423, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/home/bryan/git/spark/python/lib/pyspark.zip/pyspark/serializers.py"", line 505, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File ""/home/bryan/git/spark/python/lib/pyspark.zip/pyspark/util.py"", line 99, in wrapper
    return f(*args, **kwargs)
  File ""/home/bryan/git/spark/python/pyspark/tests/test_rdd.py"", line 742, in fail
    raise RuntimeError(""local iterator error"")
RuntimeError: local iterator error

    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:453)
...
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
FAIL

======================================================================
FAIL: test_to_local_iterator_error (pyspark.tests.test_rdd.RDDTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/bryan/git/spark/python/pyspark/tests/test_rdd.py"", line 748, in test_to_local_iterator_error
    pass
AssertionError: Exception not raised{noformat}",,apachespark,bryanc,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23961,,,SPARK-27992,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 07 21:49:14 UTC 2019,,,,,,,,,,"0|z022ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/19 00:44;bryanc;This is not that easy to fix by itself. Since there is no call from Py4J after the initial socket is setup, when a task fails the Thread serving the iterator to python just terminates and the serializer stops. This ends up giving partial results that look fine because the error is never seen by the Python driver.

Because the serving thread is being run asynchronously, the exception would have to be caught and then be transferred to Python after being joined. This would require lots of modifications, but is pretty easy to do with the changes from SPARK-23961, so I will add the fix there.;;;","30/Apr/19 21:07;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/24070;;;","07/May/19 21:49;bryanc;Issue resolved by pull request 24070
[https://github.com/apache/spark/pull/24070];;;",,,,,,,,,,,,,,,,,,,
Fix Python test script to work on Scala-2.12 build,SPARK-27544,13229512,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,23/Apr/19 06:14,23/Apr/19 22:37,13/Jul/23 08:46,23/Apr/19 14:01,2.4.0,2.4.1,2.4.2,,,,,,2.4.3,,,,PySpark,Tests,,,,0,,,,,,"Since SPARK-27274 deprecated Scala-2.11 at Spark 2.4.1, we need to test Scala-2.12 more. This issue aims to fix the Python test script on Scala-2.12 build.

{code:java}
$ dev/change-scala-version.sh 2.12

$ build/sbt -Pscala-2.12 package

$ python/run-tests.py --python-executables python2.7 --modules pyspark-sql
Traceback (most recent call last):
File ""python/run-tests.py"", line 69, in <module>
raise Exception(""Cannot find assembly build directory, please build Spark first."")
Exception: Cannot find assembly build directory, please build Spark first.{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27274,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 23 14:01:04 UTC 2019,,,,,,,,,,"0|z021bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/19 14:01;dongjoon;This is resolved via https://github.com/apache/spark/pull/24439;;;",,,,,,,,,,,,,,,,,,,,,
Fix inaccurate aggregate outputRows estimation with column containing null values,SPARK-27539,13229337,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pengbo,pengbo,pengbo,22/Apr/19 08:16,19/Aug/19 03:54,13/Jul/23 08:46,23/May/19 20:42,3.0.0,,,,,,,,2.4.3,3.0.0,,,SQL,,,,,0,,,,,,"This issue is follow up of [https://github.com/apache/spark/pull/24286]. As [~smilegator] pointed out that column with null value is inaccurate as well.
{code:java}
> select key from test;
2
NULL
1
spark-sql> desc extended test key;
col_name key
data_type int
comment NULL
min 1
max 2
num_nulls 1
distinct_count 2{code}
The distinct count should be distinct_count + 1 when the column contains null value.",,dongjoon,pengbo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27351,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 23 03:32:16 UTC 2019,,,,,,,,,,"0|z0208o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/19 03:32;dongjoon;This is resolved via https://github.com/apache/spark/pull/24436.;;;",,,,,,,,,,,,,,,,,,,,,
"Correct the default value in the Documentation for ""spark.redaction.regex""",SPARK-27532,13229258,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shivusondur@gmail.com,shivusondur@gmail.com,shivusondur@gmail.com,21/Apr/19 05:35,12/Dec/22 18:11,13/Jul/23 08:46,21/Apr/19 07:57,3.0.0,,,,,,,,3.0.0,,,,Documentation,,,,,0,,,,,,"Correct the default value in the Documentation for ""spark.redaction.regex"".",,shivusondur@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 21 07:57:22 UTC 2019,,,,,,,,,,"0|z01zr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/19 07:57;gurwls223;Issue resolved by pull request 24428
[https://github.com/apache/spark/pull/24428];;;",,,,,,,,,,,,,,,,,,,,,
Pandas udf corrupting data,SPARK-27519,13229009,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,f7faf8ba36,f7faf8ba36,19/Apr/19 07:24,12/Dec/22 18:11,13/Jul/23 08:46,30/Apr/19 22:49,2.3.0,,,,,,,,3.0.0,,,,PySpark,,,,,0,,,,,,"While trying to use a pandas udf, i sent the udf 2 columns, a string and a list of a list of strings. The second argument structure for example: [['1'],['2'],['3']]

But when getting this same value in the udf, i receive something like this: [['1','2'],['3'],[]]

I checked and the same row in the table has the list with the correct structure, only in the udf did it change.

 

I don't know why this happens, but i do know it has something to do with the fact that that row was the 10,001th row and last row in it's partition. Pandas batch size is 10,000 so that row was sent as a second batch alone, and that's the only thing that seems to cause it, having 1 or 2 rows in a second batch of the partition. I was also able to get this with a second batch of 2 rows, the list wasn't changed except an empty list was added to the end. 

Hope you can help me understand what is going on, thanks!",,bryanc,f7faf8ba36,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/19 22:09;f7faf8ba36;Pandas UDF Bug.py;https://issues.apache.org/jira/secure/attachment/12967331/Pandas+UDF+Bug.py",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 30 23:28:06 UTC 2019,,,,,,,,,,"0|z01y9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/19 10:57;gurwls223;Can you post a self-contained reproducer and check if the issue exists in higher version of Spark?;;;","19/Apr/19 11:00;f7faf8ba36;Well, unfortunately I don't have access to a higher version of spark currently, I can write some python code that reproduces this issue, but I won't be able to run it.;;;","19/Apr/19 11:09;gurwls223;I can run if you post it in higher versions of Spark.;;;","19/Apr/19 15:02;f7faf8ba36;Ok, i will write a reproducer in python 2 and test it in my environment to make sure it works and then i will post it here, probably in 2 days. Thanks,for the help! ;;;","28/Apr/19 22:10;f7faf8ba36;[^Pandas UDF Bug.py]

 

^Hello! [~hyukjin.kwon] here is my self-contained reproducer, if there is any issue with the code please let me know.^

^In my testing, it replicated the issue consistently.^

^Thanks for the help!^;;;","30/Apr/19 22:47;bryanc;Thanks for the script [~f7faf8ba36], I was able to reproduce with Spark 2.3.0 using pyarrow 0.8.0 and 0.12.1. With master, I did not see the issue so it could have been fixed by another Jira and will be in 3.0.0. I did not try out on Spark 2.4.0 . I'm going to close this then, but please try master or use 3.0.0 when it is released.

I did notice something strange when running master though. I get rows with values of None for some reason, so if I run {{df.distinct().collect()}} then the output is {{[Row(value=[[None, None]]), Row(value=[[1, 2], [3, 4]])]}}. This does not seem related to the issue here, so I will open another JIRA.;;;","30/Apr/19 22:49;bryanc;Problem does not happen when running the latest master. Marking resolved.;;;","30/Apr/19 23:28;bryanc;I made SPARK-27612 for the problem with {{Row(value=[[None, None]])}};;;",,,,,,,,,,,,,,
Decimal parsing leads to unexpected type inference,SPARK-27512,13228926,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,koert,koert,18/Apr/19 18:42,12/Dec/22 18:11,13/Jul/23 08:46,24/Apr/19 07:22,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"{code:bash}
$ hadoop fs -text test.bsv
x|y
1|1,2
2|2,3
3|3,4
{code}

in spark 2.4.1:
{code:bash}
scala> val data = spark.read.format(""csv"").option(""header"", true).option(""delimiter"", ""|"").option(""inferSchema"", true).load(""test.bsv"")

scala> data.printSchema
root
 |-- x: integer (nullable = true)
 |-- y: string (nullable = true)

scala> data.show
+---+---+
|  x|  y|
+---+---+
|  1|1,2|
|  2|2,3|
|  3|3,4|
+---+---+
{code}

in spark 3.0.0-SNAPSHOT:
{code:bash}
scala> val data = spark.read.format(""csv"").option(""header"", true).option(""delimiter"", ""|"").option(""inferSchema"", true).load(""test.bsv"")

scala> data.printSchema
root
 |-- x: integer (nullable = true)
 |-- y: decimal(2,0) (nullable = true)

scala> data.show
+---+---+
|  x|  y|
+---+---+
|  1| 12|
|  2| 23|
|  3| 34|
+---+---+
{code}
","spark 3.0.0-SNAPSHOT from this commit:
{code:bash}
commit 3ab96d7acf870e53c9016b0b63d0b328eec23bed
Author: Dilip Biswal <dbiswal@us.ibm.com>
Date:   Mon Apr 15 21:26:45 2019 +0800
{code}",koert,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 24 07:22:49 UTC 2019,,,,,,,,,,"0|z01xqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/19 11:11;gurwls223;You can specify `locale` option from Spark 3.0.;;;","19/Apr/19 14:25;koert;{code:bash}
$ hadoop fs -cat test.bsv
x|y
1|1,2,3
2|4,5,6
3|7,8,9

scala> data.printSchema
root
 |-- x: integer (nullable = true)
 |-- y: decimal(3,0) (nullable = true)

scala> data.show
+---+---+
|  x|  y|
+---+---+
|  1|123|
|  2|456|
|  3|789|
+---+---+
{code}

its great we can provide locale, but that also means there is a default locale. somehow the default locale interprets  1,2,3 is a decimal. i cannot think of any locale where this would be true, and if it existed it should not be our default.
;;;","20/Apr/19 17:47;koert;default locale is US, which now has this logic for parsing :

{code}
  def getDecimalParser(locale: Locale): String => java.math.BigDecimal = {
    if (locale == Locale.US) { // Special handling the default locale for backward compatibility                                                                                                                    
      (s: String) => new java.math.BigDecimal(s.replaceAll("","", """"))
    } else {
{code}

but as i have shown above this is not backwards compatible, nor does the result make sense to me.;;;","21/Apr/19 23:02;koert;[~maxgekk] maxim do you know why getDecimalParser has that if condition for Locale US where it calls
{code:java}
 (s: String) => new java.math.BigDecimal(s.replaceAll("","", """")) {code}
i think it's that
{code:java}
s.replaceAll("","", """"){code}
that is causing my issues.
 i saw it was introduced in:
{code:java}
commit 7a83d71403edf7d24fa5efc0ef913f3ce76d88b8
Author: Maxim Gekk <max.gekk@gmail.com>
Date:   Thu Nov 29 22:15:12 2018 +0800

    [SPARK-26163][SQL] Parsing decimals from JSON using locale
{code};;;","21/Apr/19 23:23;koert;seems DecimalFormat.parse also simply ignores commas. still unclear to me why we have to do same in the ""special handling of default locale for backwards compatibility"" but i am guessing that has to do with json parsing backwards compatibility, not csv backwards compatibility.;;;","22/Apr/19 11:20;gurwls223;I see a behaviour change. Yes, looks for schema inference path, how to handle decimals looks changed. But I think the current change makes sense rather than having two difference decimal parser for schema inference and data parse.
Still the workaround is to set {{locale}}.;;;","22/Apr/19 13:58;koert;i agree it is better than having two different decimal parsers. note that i was not able to get the old behavior back by using a a locale, so that is not a workaround in sofar i can see.

i think this is just a change we won't fix.;;;","24/Apr/19 07:22;gurwls223;Issue resolved by pull request 24437
[https://github.com/apache/spark/pull/24437];;;",,,,,,,,,,,,,,
Master fall into dead loop while launching executor failed in Worker,SPARK-27510,13228884,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,18/Apr/19 15:23,11/Aug/20 13:17,13/Jul/23 08:46,03/May/19 22:49,1.6.0,2.0.0,2.1.0,2.2.0,2.3.0,2.4.0,,,3.0.0,,,,Spark Core,,,,19/Apr/19 00:00,0,,,,,,"In Standalone, while launching executor during ExecutorRunner.start() is always failed in Worker, Master will continue to launch new executor for the same Worker indefinitely.

The issue is easy to reproduce by running a unit test with local-cluster mode and set a wrong  spark.test.home(e.g. /tmp). Then, when running unit test, test would get stuck and we can see endless executor directories under /tmp/work/app/.",,apachespark,jiangxb1987,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 11 13:17:36 UTC 2020,,,,,,,,,,"0|z01xhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/May/19 22:49;jiangxb1987;Issue resolved by pull request 24408
[https://github.com/apache/spark/pull/24408];;;","11/Aug/20 13:17;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29408;;;",,,,,,,,,,,,,,,,,,,,
JobGenerator thread exit for some fatal errors but application keeps running,SPARK-27503,13228792,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,uncleGen,uncleGen,uncleGen,18/Apr/19 08:20,23/Apr/19 14:14,13/Jul/23 08:46,23/Apr/19 14:12,3.0.0,,,,,,,,3.0.0,,,,DStreams,,,,,0,,,,,,"JobGenerator thread (including some other EventLoop threads) may exit for some fatal error, like OOM, but Spark Streaming job keep running with no batch job generating. Currently, we only report any non-fatal error. 
{code}
override def run(): Unit = {
      try {
        while (!stopped.get) {
          val event = eventQueue.take()
          try {
            onReceive(event)
          } catch {
            case NonFatal(e) =>
              try {
                onError(e)
              } catch {
                case NonFatal(e) => logError(""Unexpected error in "" + name, e)
              }
          }
        }
      } catch {
        case ie: InterruptedException => // exit even if eventQueue is not empty
        case NonFatal(e) => logError(""Unexpected error in "" + name, e)
      }
    }
{code}

In some corner cases, these event threads may exit with OOM error, but driver thread can still keep running.",,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 23 14:12:11 UTC 2019,,,,,,,,,,"0|z01wxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/19 14:12;srowen;Issue resolved by pull request 24400
[https://github.com/apache/spark/pull/24400];;;",,,,,,,,,,,,,,,,,,,,,
Spark wipes out bucket spec in metastore when updating table stats,SPARK-27497,13228714,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,17/Apr/19 23:22,09/Feb/21 01:51,13/Jul/23 08:46,10/May/20 15:59,2.4.0,3.0.0,,,,,,,2.4.6,3.1.0,,,SQL,,,,,0,,,,,,"The bucket spec gets wiped out after Spark writes to a Hive-bucketed table that has the following characteristics:
 - table is created by Hive (or even Spark, if you use HQL DDL)
 - table is stored in Parquet format
 - table has at least one Hive-created data file already

Also, spark.sql.hive.convertMetastoreParquet has to be set to true (the default).

For example, do the following in Hive:
{noformat}
hive> create table sourcetable as select 1 a, 3 b, 7 c;
hive> drop table hivebucket1;
hive> create table hivebucket1 (a int, b int, c int) clustered by (a, b) sorted by (a, b asc) into 10 buckets stored as parquet;
hive> insert into hivebucket1 select * from sourcetable;
hive> show create table hivebucket1;
OK
CREATE TABLE `hivebucket1`(
  `a` int, 
  `b` int, 
  `c` int)
CLUSTERED BY ( 
  a, 
  b) 
SORTED BY ( 
  a ASC, 
  b ASC) 
INTO 10 BUCKETS
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION
  'file:/Users/brobbins/github/spark_upstream/spark-warehouse/hivebucket1'
TBLPROPERTIES (
  'COLUMN_STATS_ACCURATE'='true', 
  'numFiles'='1', 
  'numRows'='1', 
  'rawDataSize'='3', 
  'totalSize'='352', 
  'transient_lastDdlTime'='1555542971')
Time taken: 0.056 seconds, Fetched: 26 row(s)
hive> 
{noformat}
Then in spark-shell, do the following:
{noformat}
scala> sql(""insert into hivebucket1 select 1, 3, 7"")
19/04/17 10:49:30 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
res0: org.apache.spark.sql.DataFrame = []
{noformat}
Note: At this point, I would have expected Spark to throw an {{AnalysisException}} with the message ""Output Hive table `default`.`hivebucket1` is bucketed..."". However, I am ignoring that for now and may open a separate Jira (SPARK-27498).

Return to some Hive CLI and note that the bucket specification is gone from the table definition:
{noformat}
hive> show create table hivebucket1;
OK
CREATE TABLE `hivebucket1`(
  `a` int, 
  `b` int, 
  `c` int)
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION
  '<location>'
TBLPROPERTIES (
  'COLUMN_STATS_ACCURATE'='false', 
  'SORTBUCKETCOLSPREFIX'='TRUE', 
  'numFiles'='2', 
  'numRows'='-1', 
  'rawDataSize'='-1', 
  'totalSize'='1144', 
  'transient_lastDdlTime'='1555523374')
Time taken: 1.619 seconds, Fetched: 20 row(s)
hive> 
{noformat}
This information is lost when Spark attempts to update table stats. HiveClientImpl.toHiveTable drops the bucket specification. toHiveTable drops the bucket information because {{table.provider}} is None instead of ""hive"". {{table.provider}} is not ""hive"" because Spark bypassed the serdes and used the built-in parquet code path (by default, spark.sql.hive.convertMetastoreParquet is true).",,bersprockets,Kotomi,sandeep.katta2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 10 15:59:03 UTC 2020,,,,,,,,,,"0|z01wg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/May/20 15:59;bersprockets;This is fixed in recent versions of 2.4.x and 3.x. Closing.;;;",,,,,,,,,,,,,,,,,,,,,
RPC should send back the fatal errors,SPARK-27496,13228700,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,17/Apr/19 21:30,22/Apr/19 00:11,13/Jul/23 08:46,22/Apr/19 00:11,2.4.1,,,,,,,,2.3.4,2.4.3,3.0.0,,Spark Core,,,,,0,,,,,,"Right now, when a fatal error throws from ""receiveAndReply"", the sender will not be notified. We should try our best to send it back.",,dongjoon,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 22 00:11:06 UTC 2019,,,,,,,,,,"0|z01wcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/19 00:11;dongjoon;This is resolved via https://github.com/apache/spark/pull/24396;;;",,,,,,,,,,,,,,,,,,,,,
Null keys/values don't work in Kafka source v2,SPARK-27494,13228681,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,uncleGen,zsxwing,zsxwing,17/Apr/19 18:57,02/Mar/20 21:04,13/Jul/23 08:46,26/Apr/19 06:29,2.4.0,2.4.1,,,,,,,2.4.3,3.0.0,,,Structured Streaming,,,,,0,correctness,,,,,"Right now Kafka source v2 doesn't support null keys or values.
 * When processing a null key, all of the following keys in the same partition will be null. This is a correctness bug.
 * When processing a null value, it will throw NPE.

The workaround is setting sql conf ""spark.sql.streaming.disabledV2MicroBatchReaders"" to ""org.apache.spark.sql.kafka010.KafkaSourceProvider"" to use the v1 source.",,cloud_fan,shahid,viirya,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 26 06:29:18 UTC 2019,,,,,,,,,,"0|z01w8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/19 06:29;cloud_fan;Issue resolved by pull request 24441
[https://github.com/apache/spark/pull/24441];;;",,,,,,,,,,,,,,,,,,,,,
EnsureRequirements.reorder should handle duplicate expressions gracefully,SPARK-27485,13228498,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hvanhovell,babloo80,babloo80,17/Apr/19 05:02,12/Dec/22 18:11,13/Jul/23 08:46,16/Jul/19 09:15,2.4.0,,,,,,,,2.4.4,3.0.0,,,Optimizer,SQL,,,,0,,,,,,"Certain queries fail with
{noformat}
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:349)
	at scala.None$.get(Option.scala:347)
	at org.apache.spark.sql.execution.exchange.EnsureRequirements.$anonfun$reorder$1(EnsureRequirements.scala:238)
	at org.apache.spark.sql.execution.exchange.EnsureRequirements.$anonfun$reorder$1$adapted(EnsureRequirements.scala:233)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.apache.spark.sql.execution.exchange.EnsureRequirements.reorder(EnsureRequirements.scala:233)
	at org.apache.spark.sql.execution.exchange.EnsureRequirements.reorderJoinKeys(EnsureRequirements.scala:262)
	at org.apache.spark.sql.execution.exchange.EnsureRequirements.org$apache$spark$sql$execution$exchange$EnsureRequirements$$reorderJoinPredicates(EnsureRequirements.scala:289)
	at org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse(EnsureRequirements.scala:304)
	at org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse(EnsureRequirements.scala:296)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$4(TreeNode.scala:282)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:282)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:275)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:275)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:275)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:275)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:275)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)
	at org.apache.spark.sql.execution.exchange.EnsureRequirements.apply(EnsureRequirements.scala:296)
	at org.apache.spark.sql.execution.exchange.EnsureRequirements.apply(EnsureRequirements.scala:38)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$prepareForExecution$1(QueryExecution.scala:87)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
	at scala.collection.immutable.List.foldLeft(List.scala:85)
{noformat}

I don't have an exact query reproducer for this. But, I can try to frame one if this problem hasn't been reported in the past?",,babloo80,mgaido,shahid,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 19 11:51:04 UTC 2019,,,,,,,,,,"0|z01v40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/19 15:20;shahid;Could you please share test to reproduce this?;;;","19/Apr/19 11:43;gurwls223;Yes, please share the reproducer;;;","19/Apr/19 11:51;babloo80;Let me try to build a sql expression for this. What I currently have is a join with a large join query.
I am trying to make a plan to make up some sizable test data and sql query. ;;;",,,,,,,,,,,,,,,,,,,
Kafka token provider should have provided dependency on Spark,SPARK-27477,13228381,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,koert,koert,koert,16/Apr/19 16:23,26/Apr/19 18:54,13/Jul/23 08:46,26/Apr/19 18:52,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"currently the external module spark-token-provider-kafka-0-10 has a compile dependency on spark-core. this means spark-sql-kafka-0-10 also has a transitive compile dependency on spark-core.

since spark-sql-kafka-0-10 is not bundled with spark but instead has to be added to an application that runs on spark this dependency should be provided, not compile.","spark 3.0.0-SNAPSHOT

commit 38fc8e2484aa4971d1f2c115da61fc96f36e7868
Author: Sean Owen <sean.owen@databricks.com>
Date:   Sat Apr 13 22:27:25 2019 +0900

    [MINOR][DOCS] Fix some broken links in docs
",koert,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 26 18:52:34 UTC 2019,,,,,,,,,,"0|z01uds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/19 18:52;vanzin;Issue resolved by pull request 24384
[https://github.com/apache/spark/pull/24384];;;",,,,,,,,,,,,,,,,,,,,,
"""Storage Level"" in ""RDD Storage Page"" is not correct",SPARK-27468,13228166,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,zsxwing,zsxwing,15/Apr/19 17:22,07/Oct/19 21:12,13/Jul/23 08:46,07/Oct/19 21:12,2.4.1,2.4.2,2.4.3,2.4.4,3.0.0,,,,3.0.0,,,,Spark Core,,,,,0,,,,,,"I ran the following unit test and checked the UI.
{code}
    val conf = new SparkConf()
      .setAppName(""test"")
      .setMaster(""local-cluster[2,1,1024]"")
      .set(""spark.ui.enabled"", ""true"")
    sc = new SparkContext(conf)
    val rdd = sc.makeRDD(1 to 10, 1).persist(StorageLevel.MEMORY_ONLY_2)
    rdd.count()
    Thread.sleep(3600000)
{code}

The storage level is ""Memory Deserialized 1x Replicated"" in the RDD storage page.

I tried to debug and found this is because Spark emitted the following two events:
{code}
event: SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(1, 10.8.132.160, 65473, None),rdd_0_0,StorageLevel(memory, deserialized, 2 replicas),56,0))
event: SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(0, 10.8.132.160, 65474, None),rdd_0_0,StorageLevel(memory, deserialized, 1 replicas),56,0))
{code}

The storage level in the second event will overwrite the first one. ""1 replicas"" comes from this line: https://github.com/apache/spark/blob/3ab96d7acf870e53c9016b0b63d0b328eec23bed/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L1457

Maybe AppStatusListener should calculate the replicas from events?

Another fact we may need to think about is when replicas is 2, will two Spark events arrive in the same order? Currently, two RPCs from different executors can arrive in any order.

Credit goes to [~srfnmnk] who reported this issue originally.",,dongjoon,Gengliang.Wang,irashid,KevinPis,shahid,smilegator,srfnmnk,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/19 02:43;shahid;Screenshot from 2019-04-17 10-42-55.png;https://issues.apache.org/jira/secure/attachment/12966176/Screenshot+from+2019-04-17+10-42-55.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 07 21:12:16 UTC 2019,,,,,,,,,,"0|z01t2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/19 17:25;smilegator;cc [~Gengliang.Wang];;;","16/Apr/19 05:56;shahid;I would like to analyze the issue.;;;","16/Apr/19 10:35;srfnmnk;Excellent [~shahid], if you need any assistance replicating, let me know; I can recreate the issue but you should be able to as well.

 

Thanks;;;","16/Apr/19 16:25;Gengliang.Wang;[~shahid] Thanks;;;","17/Apr/19 02:44;shahid;Hi [~srfnmnk], I tried to reproduce in the master branch. The steps followed shown below
1) bin/spark-shell --master local[2]


{code:java}
scala> import org.apache.spark.storage.StorageLevel
scala> val rdd = sc.parallelize(1 to 10, 1).persist(StorageLevel.MEMORY_ONLY_2)
scala > rdd.count
{code}

Storage tab in the UI is shown below ,

 !Screenshot from 2019-04-17 10-42-55.png! 

So, it seems I am not able to reproduce the issue. Could you please tell me if the test steps are correct or I need to enable any configurations. Thank you
;;;","17/Apr/19 20:00;zsxwing;[~shahid] You need to use ""--master local-cluster[2,1,1024]"". The local mode has only one BlockManager.;;;","17/Apr/19 21:39;shahid;[~zsxwing] Thanks;;;","07/Oct/19 21:12;irashid;Resolved by https://github.com/apache/spark/pull/25779;;;",,,,,,,,,,,,,,
Spark image datasource fail when encounter some illegal images,SPARK-27454,13227807,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,12/Apr/19 22:28,15/Apr/19 18:58,13/Jul/23 08:46,15/Apr/19 18:57,2.4.1,,,,,,,,3.0.0,,,,ML,SQL,,,,0,,,,,,"Spark image datasource fail when encounter some illegal images. Such as exception following:
{code:java}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 132, 10.95.233.69, executor 0): java.lang.IllegalArgumentException: Numbers of source Raster bands and source color space components do not match:

at java.awt.image.ColorConvertOp.filter(ColorConvertOp.java:482) at com.sun.imageio.plugins.jpeg.JPEGImageReader.acceptPixels(JPEGImageReader.java:1280) at com.sun.imageio.plugins.jpeg.JPEGImageReader.readImage(Native Method) at com.sun.imageio.plugins.jpeg.JPEGImageReader.readInternal(JPEGImageReader.java:1247) at com.sun.imageio.plugins.jpeg.JPEGImageReader.read(JPEGImageReader.java:1050) at javax.imageio.ImageIO.read(ImageIO.java:1448) at javax.imageio.ImageIO.read(ImageIO.java:1352) at org.apache.spark.ml.image.ImageSchema$.decode(ImageSchema.scala:136) at org.apache.spark.ml.source.image.ImageFileFormat$$anonfun$buildReader$2.apply(ImageFileFormat.scala:84) at org.apache.spark.ml.source.image.ImageFileFormat$$anonfun$buildReader$2.apply(ImageFileFormat.scala:70) at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:147) at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:134) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:226) at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:196) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:338) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:196) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:638) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55) at org.apache.spark.scheduler.Task.doRunTask(Task.scala:139) at org.apache.spark.scheduler.Task.run(Task.scala:112) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:497) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1481) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:503) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748){code}
 ",,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-04-12 22:28:00.0,,,,,,,,,,"0|z01quo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrameWriter.partitionBy is Silently Dropped by DSV1,SPARK-27453,13227803,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,liwensun,marmbrus,marmbrus,12/Apr/19 21:45,03/Jun/19 21:45,13/Jul/23 08:46,16/Apr/19 22:39,1.4.1,1.5.2,1.6.3,2.0.2,2.1.3,2.2.3,2.4.1,,2.4.2,3.0.0,,,SQL,,,,,0,,,,,,"This is a long standing quirk of the interaction between {{DataFrameWriter}} and {{CreatableRelationProvider}} (and the other forms of the DSV1 API).  Users can specify columns in {{partitionBy}} and our internal data sources will use this information.  Unfortunately, for external systems, this data is silently dropped with no feedback given to the user.

In the long run, I think that DataSourceV2 is a better answer. However, I don't think we should wait for that API to stabilize before offering some kind of solution to developers of external data sources. I also do not think we should break binary compatibility of this API, but I do think that  small surgical fix could alleviate the issue.

I would propose that we could propagate partitioning information (when present) along with the other configuration options passed to the data source in the {{String, String}} map.

I think its very unlikely that there are both data sources that validate extra options and users who are using (no-op) partitioning with them, but out of an abundance of caution we should protect the behavior change behind a {{legacy}} flag that can be turned off.",,apachespark,kotime42@gmail.com,marmbrus,tdas,xinxianyin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 03 21:45:09 UTC 2019,,,,,,,,,,"0|z01qts:",9223372036854775807,,,,,marmbrus,,,,,,,,2.4.2,3.0.0,,,,,,,,,,"16/Apr/19 22:39;tdas;Issue resolved by pull request 24365
[https://github.com/apache/spark/pull/24365];;;","03/Jun/19 21:45;apachespark;User 'liwensun' has created a pull request for this issue:
https://github.com/apache/spark/pull/24784;;;",,,,,,,,,,,,,,,,,,,,
RBackend always uses default values for spark confs,SPARK-27446,13227546,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bago.amirbekian,bago.amirbekian,bago.amirbekian,11/Apr/19 20:41,12/Dec/22 18:10,13/Jul/23 08:46,14/Apr/19 08:10,2.4.1,,,,,,,,3.0.0,,,,SparkR,,,,,0,,,,,,The RBackend and RBackendHandler create new conf objects that don't pick up conf values from the existing SparkSession and therefore always use the default conf values instead of values specified by the user.,,bago.amirbekian,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 14 08:10:06 UTC 2019,,,,,,,,,,"0|z01p8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/19 08:10;gurwls223;Issue resolved by pull request 24353
[https://github.com/apache/spark/pull/24353];;;",,,,,,,,,,,,,,,,,,,,,
Update SQLQueryTestSuite to process files ending with `.sql`,SPARK-27445,13227504,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dkbiswal,dkbiswal,dkbiswal,11/Apr/19 16:24,11/Apr/19 22:56,13/Jul/23 08:46,11/Apr/19 21:52,2.4.1,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"While using vi or vim to edit the test files the .swp or .swo files are created and attempt to run the test suite in the presence of these files causes errors like below :

{code}
info] - subquery/exists-subquery/.exists-basic.sql.swp *** FAILED *** (117 milliseconds)
[info] java.io.FileNotFoundException: /Users/dbiswal/mygit/apache/spark/sql/core/target/scala-2.12/test-classes/sql-tests/results/subquery/exists-subquery/.exists-basic.sql.swp.out (No such file or directory)
[info] at java.io.FileInputStream.open0(Native Method)
[info] at java.io.FileInputStream.open(FileInputStream.java:195)
[info] at java.io.FileInputStream.<init>(FileInputStream.java:138)
[info] at org.apache.spark.sql.catalyst.util.package$.fileToString(package.scala:49)
[info] at org.apache.spark.sql.SQLQueryTestSuite.runQueries(SQLQueryTestSuite.scala:247)
[info] at org.apache.spark.sql.SQLQueryTestSuite.$anonfun$runTest$11(SQLQueryTestSuite.scala:192)
```
{code}

While computing the list of test files to process, only consider files with `.sql` extension. This makes sure the unwanted temp files created from various editors are ignored from processing.",,dkbiswal,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 11 21:52:01 UTC 2019,,,,,,,,,,"0|z01ozc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/19 21:52;dongjoon;This is resolved via https://github.com/apache/spark/pull/24333;;;",,,,,,,,,,,,,,,,,,,,,
multi-select can be used in subquery,SPARK-27444,13227475,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,11/Apr/19 14:19,24/May/19 03:26,13/Jul/23 08:46,12/Apr/19 12:58,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 12 12:58:20 UTC 2019,,,,,,,,,,"0|z01osw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/19 12:58;cloud_fan;Issue resolved by pull request 24348
[https://github.com/apache/spark/pull/24348];;;",,,,,,,,,,,,,,,,,,,,,
Explainging Dataset should show correct resolved plans,SPARK-27439,13227417,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,lemont,lemont,11/Apr/19 09:36,12/Dec/22 18:11,13/Jul/23 08:46,21/May/19 18:27,2.4.1,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"{code}
scala> spark.range(10).createOrReplaceTempView(""test"")
scala> spark.range(5).createOrReplaceTempView(""test2"")
scala> spark.sql(""select * from test"").createOrReplaceTempView(""tmp001"")
scala> val df = spark.sql(""select * from tmp001"")
scala> spark.sql(""select * from test2"").createOrReplaceTempView(""tmp001"")
scala> df.show
+---+
| id|
+---+
|  0|
|  1|
|  2|
|  3|
|  4|
|  5|
|  6|
|  7|
|  8|
|  9|
+---+
scala> df.explain
{code}

Before:
{code}
== Physical Plan ==
*(1) Range (0, 5, step=1, splits=12)
{code}

After:
{code}
== Physical Plan ==
*(1) Range (0, 10, step=1, splits=12)
{code}",,apachespark,dongjoon,huonw,lemont,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 21 03:55:08 UTC 2019,,,,,,,,,,"0|z01og8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/19 10:11;viirya;The review is resolved during analysis stage when we create the dataset. That said, it is replaced with {{select * from default.t1}} in the described example. Even you replace the temp view {{tmp001}} after that, the analyzed plan won't be changed in the dataset.

Although this is marked as bug, currently I'd think it is just the way Dataset works. The content of a view in a dataset, is determined when the dataset is created, not when it is evaluated.

;;;","19/Apr/19 12:06;gurwls223;I agree.;;;","19/Apr/19 15:02;viirya;One possible issue I'm aware of is, {{df.explain}} shows a inconsistent plan, compared with the evaluated result of the dataset. I think it is confused for users when they saw it. I made a small PR for that.;;;","21/Apr/19 17:26;dongjoon;This is resolved via https://github.com/apache/spark/pull/24415;;;","26/Apr/19 00:03;huonw;I think this (partially) broke {{df.explain(extended = true)}}, as the ""Parsed Logical Plan"" section is now the same as the analysed one:

Before (2.4):

{code:none}
scala> spark.range(100).select(col(""id"")).explain(true)
== Parsed Logical Plan ==
'Project [unresolvedalias('id, None)]
+- Range (0, 100, step=1, splits=Some(12))

== Analyzed Logical Plan ==
id: bigint
Project [id#113L]
+- Range (0, 100, step=1, splits=Some(12))

...
{code}

After (master):

{code:none}
== Parsed Logical Plan ==
Project [id#0L]
+- Range (0, 100, step=1, splits=Some(12))

== Analyzed Logical Plan ==
id: bigint
Project [id#0L]
+- Range (0, 100, step=1, splits=Some(12))

...
{code};;;","26/Apr/19 00:37;viirya;I will look into it. Thanks [~huonw];;;","26/Apr/19 01:34;dongjoon;My bad. I'll revert the patch. [~huonw], [~viirya], [~hyukjin.kwon];;;","26/Apr/19 01:38;dongjoon;One possible followup will be pass logical plan and analyzed plan together to `ExplainCommand`. It will cause many changes.;;;","06/May/19 06:21;dongjoon;This is resolved via https://github.com/apache/spark/pull/24464;;;","20/May/19 22:10;dongjoon;This is reverted again due to the regression like `explain on EXPLAIN statement`.;;;","21/May/19 03:55;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/24654;;;",,,,,,,,,,,
broadcast hint should be respected for broadcast nested loop join,SPARK-27430,13227177,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,10/Apr/19 12:52,17/Apr/19 11:32,13/Jul/23 08:46,17/Apr/19 11:32,2.4.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 17 11:32:13 UTC 2019,,,,,,,,,,"0|z01n08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/19 11:32;cloud_fan;Issue resolved by pull request 24376
[https://github.com/apache/spark/pull/24376];;;",,,,,,,,,,,,,,,,,,,,,
"Test ""metrics StatsD sink with Timer "" fails on BigEndian",SPARK-27428,13227148,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mundaym,Anuja,Anuja,10/Apr/19 10:58,12/Dec/22 18:10,13/Jul/23 08:46,06/Oct/20 13:31,2.3.2,2.3.3,2.3.4,,,,,,3.0.2,3.1.0,,,Spark Core,,,,,0,BigEndian,,,,,"Test case ""metrics StatsD sink with Timer *** FAILED ***"" fails with error

java.net.SocketTimeoutException: Receive timed out
 at java.net.AbstractPlainDatagramSocketImpl.receive(AbstractPlainDatagramSocketImpl.java:143)
 at java.net.DatagramSocket.receive(DatagramSocket.java:812)
 at org.apache.spark.metrics.sink.StatsdSinkSuite$$anonfun$4$$anonfun$apply$mcV$sp$4$$anonfun$apply$3.apply(StatsdSinkSuite.scala:155)
 at org.apache.spark.metrics.sink.StatsdSinkSuite$$anonfun$4$$anonfun$apply$mcV$sp$4$$anonfun$apply$3.apply(StatsdSinkSuite.scala:154)
 at scala.collection.immutable.Range.foreach(Range.scala:160)
 at org.apache.spark.metrics.sink.StatsdSinkSuite$$anonfun$4$$anonfun$apply$mcV$sp$4.apply(StatsdSinkSuite.scala:154)
 at org.apache.spark.metrics.sink.StatsdSinkSuite$$anonfun$4$$anonfun$apply$mcV$sp$4.apply(StatsdSinkSuite.scala:123)
 at org.apache.spark.metrics.sink.StatsdSinkSuite.org$apache$spark$metrics$sink$StatsdSinkSuite$$withSocketAndSink(StatsdSinkSuite.scala:51)
 at org.apache.spark.metrics.sink.StatsdSinkSuite$$anonfun$4.apply$mcV$sp(StatsdSinkSuite.scala:123)
 at org.apache.spark.metrics.sink.StatsdSinkSuite$$anonfun$4.apply(StatsdSinkSuite.scala:123)

On debugging observed that the last packet is not received at ""socket.receive(p)"". Hence the assert fails.  

 

Also I want to know, which feature of Apache Spark is tested in this this test.","Working on Ubuntu16.04, Linux 

Java versions : 

Eclipse OpenJ9 VM (build openj9-0.12.1, JRE 1.8.0 Linux s390x-64-Bit Compressed References 20190205_218 (JIT enabled, AOT enabled)

and

openjdk version ""1.8.0_191""
OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-2ubuntu0.18.04.1-b12)
OpenJDK 64-Bit Zero VM (build 25.191-b12, interpreted mode)",Anuja,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 06 13:31:47 UTC 2020,,,,,,,,,,"0|z01mts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/19 18:35;gurwls223;I am sure there are multiple BigEndian issues. Can you link related JIRAs so that we can track?;;;","21/Sep/20 14:28;apachespark;User 'mundaym' has created a pull request for this issue:
https://github.com/apache/spark/pull/29819;;;","06/Oct/20 13:31;srowen;Issue resolved by pull request 29819
[https://github.com/apache/spark/pull/29819];;;",,,,,,,,,,,,,,,,,,,
"When setting spark.executor.heartbeatInterval to a value less than 1 seconds, it will always fail",SPARK-27419,13227007,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,09/Apr/19 17:32,22/Apr/19 04:05,13/Jul/23 08:46,10/Apr/19 16:43,2.4.0,2.4.1,,,,,,,2.4.2,,,,Spark Core,,,,,0,release-notes,,,,,"When setting spark.executor.heartbeatInterval to a value less than 1 seconds in branch-2.4, it will always fail because the value will be converted to 0 and the heartbeat will always timeout and finally kill the executor.",,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27198,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-04-09 17:32:15.0,,,,,,,,,,"0|z01lyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - ExternalSorter and ExternalAppendOnlyMap should free shuffle memory in their stop() methods,SPARK-27417,13226979,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,yangpengyu,yangpengyu,09/Apr/19 14:53,09/Apr/19 14:54,13/Jul/23 08:46,09/Apr/19 14:54,1.3.1,1.4.1,1.5.1,1.6.0,,,,,1.6.0,,,,Spark Core,,,,,0,,,,,,"I discovered multiple leaks of shuffle memory while working on my memory manager consolidation patch, which added the ability to do strict memory leak detection for the bookkeeping that used to be performed by the ShuffleMemoryManager. This uncovered a handful of places where tasks can acquire execution/shuffle memory but never release it, starving themselves of memory.

Problems that I found:

* {{ExternalSorter.stop()}} should release the sorter's shuffle/execution memory.
* BlockStoreShuffleReader should call {{ExternalSorter.stop()}} using a {{CompletionIterator}}.
* {{ExternalAppendOnlyMap}} exposes no equivalent of {{stop()}} for freeing its resources.",,yangpengyu,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11293,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-04-09 14:53:05.0,,,,,,,,,,"0|z01ls8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsafeMapData & UnsafeArrayData Kryo serialization breaks when two machines have different Oops size,SPARK-27416,13226971,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pengbo,pengbo,pengbo,09/Apr/19 14:16,22/Jul/19 05:20,13/Jul/23 08:46,17/Apr/19 05:04,2.4.1,,,,,,,,2.4.4,3.0.0,,,SQL,,,,,0,,,,,,"Actually this's follow up for https://issues.apache.org/jira/browse/SPARK-27406, https://issues.apache.org/jira/browse/SPARK-10914

This issue is to fix the UnsafeMapData & UnsafeArrayData Kryo serialization issue when two machines have different Oops size.",,cloud_fan,dongjoon,joshrosen,pengbo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10914,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 22 05:20:10 UTC 2019,,,,,,,,,,"0|z01lqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/19 05:04;cloud_fan;Issue resolved by pull request 24357
[https://github.com/apache/spark/pull/24357];;;","16/Jul/19 22:55;joshrosen;I think that we should backport this for Spark 2.4.4 because the related SPARK-27406 was backported for Spark 2.4.3 and both bugs seem like potential correctness issues (they'll often result in crashes but might theoretically result in wrong results in some cases, something that we saw happen with the original UnsafeRow OOPs bug).

I spotted this while going through the full list of 2.4.0 -> 2.4.3 commits: I saw that the related ArrayData PR was backported but this followup for Maps was not.

I can open a backport PR later; if anyone else wants to pick this up then please feel free to.

 /cc [~smilegator];;;","18/Jul/19 16:28;cloud_fan;yea let's backport it!;;;","22/Jul/19 05:20;dongjoon;This is backported to `branch-2.4` via https://github.com/apache/spark/pull/25223 . Thank you, [~joshrosen] and [~cloud_fan].;;;",,,,,,,,,,,,,,,,,,
DataSourceV2Strategy should not eliminate subquery,SPARK-27411,13226857,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Francis47,Francis47,Francis47,09/Apr/19 06:15,09/Apr/19 14:25,13/Jul/23 08:46,09/Apr/19 13:46,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"In DataSourceV2Strategy, it seems we eliminate the subqueries by mistake after normalizing filters. Here is an example:
We have an sql with a scalar subquery:
{code:scala}
val plan = spark.sql(""select * from t2 where t2a > (select max(t1a) from t1)"")
plan.explain(true)
{code}
And we get the log info of DataSourceV2Strategy:
{noformat}
Pushing operators to csv:examples/src/main/resources/t2.txt
Pushed Filters: 
Post-Scan Filters: isnotnull(t2a#30)
Output: t2a#30, t2b#31
{noformat}
The `Post-Scan Filters` should contain the scalar subquery, but we eliminate it by mistake.
{noformat}
== Parsed Logical Plan ==
'Project [*]
+- 'Filter ('t2a > scalar-subquery#56 [])
   :  +- 'Project [unresolvedalias('max('t1a), None)]
   :     +- 'UnresolvedRelation `t1`
   +- 'UnresolvedRelation `t2`

== Analyzed Logical Plan ==
t2a: string, t2b: string
Project [t2a#30, t2b#31]
+- Filter (t2a#30 > scalar-subquery#56 [])
   :  +- Aggregate [max(t1a#13) AS max(t1a)#63]
   :     +- SubqueryAlias `t1`
   :        +- RelationV2[t1a#13, t1b#14] csv:examples/src/main/resources/t1.txt
   +- SubqueryAlias `t2`
      +- RelationV2[t2a#30, t2b#31] csv:examples/src/main/resources/t2.txt

== Optimized Logical Plan ==
Filter (isnotnull(t2a#30) && (t2a#30 > scalar-subquery#56 []))
:  +- Aggregate [max(t1a#13) AS max(t1a)#63]
:     +- Project [t1a#13]
:        +- RelationV2[t1a#13, t1b#14] csv:examples/src/main/resources/t1.txt
+- RelationV2[t2a#30, t2b#31] csv:examples/src/main/resources/t2.txt

== Physical Plan ==
*(1) Project [t2a#30, t2b#31]
+- *(1) Filter isnotnull(t2a#30)
   +- *(1) BatchScan[t2a#30, t2b#31] class org.apache.spark.sql.execution.datasources.v2.csv.CSVScan
{noformat}

",,cloud_fan,Francis47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 09 13:46:42 UTC 2019,,,,,,,,,,"0|z01l14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/19 13:46;cloud_fan;Issue resolved by pull request 24321
[https://github.com/apache/spark/pull/24321];;;",,,,,,,,,,,,,,,,,,,,,
UnsafeArrayData serialization breaks when two machines have different Oops size,SPARK-27406,13226619,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pengbo,pengbo,pengbo,08/Apr/19 05:52,02/Mar/20 21:02,13/Jul/23 08:46,10/Apr/19 09:07,2.4.0,2.4.1,,,,,,,2.4.2,3.0.0,,,SQL,,,,,0,correctness,,,,,"ApproxCountDistinctForIntervals holds the UnsafeArrayData data to initialize endpoints. When the UnsafeArrayData is serialized with Java serialization, the BYTE_ARRAY_OFFSET in memory can change if two machines have different pointer width (Oops in JVM).

It's similar to SPARK-10914.



{code:java}
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.aggregate.ApproxCountDistinctForIntervals$$anonfun$endpoints$1.apply(ApproxCountDistinctForIntervals.scala:69)
	at org.apache.spark.sql.catalyst.expressions.aggregate.ApproxCountDistinctForIntervals$$anonfun$endpoints$1.apply(ApproxCountDistinctForIntervals.scala:69)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.sql.catalyst.expressions.aggregate.ApproxCountDistinctForIntervals.endpoints$lzycompute(ApproxCountDistinctForIntervals.scala:69)
	at org.apache.spark.sql.catalyst.expressions.aggregate.ApproxCountDistinctForIntervals.endpoints(ApproxCountDistinctForIntervals.scala:66)
	at org.apache.spark.sql.catalyst.expressions.aggregate.ApproxCountDistinctForIntervals.org$apache$spark$sql$catalyst$expressions$aggregate$ApproxCountDistinctForIntervals$$hllppArray$lzycompute(ApproxCountDistinctForIntervals.scala:94)
	at org.apache.spark.sql.catalyst.expressions.aggregate.ApproxCountDistinctForIntervals.org$apache$spark$sql$catalyst$expressions$aggregate$ApproxCountDistinctForIntervals$$hllppArray(ApproxCountDistinctForIntervals.scala:93)
	at org.apache.spark.sql.catalyst.expressions.aggregate.ApproxCountDistinctForIntervals.org$apache$spark$sql$catalyst$expressions$aggregate$ApproxCountDistinctForIntervals$$numWordsPerHllpp$lzycompute(ApproxCountDistinctForIntervals.scala:104)
	at org.apache.spark.sql.catalyst.expressions.aggregate.ApproxCountDistinctForIntervals.org$apache$spark$sql$catalyst$expressions$aggregate$ApproxCountDistinctForIntervals$$numWordsPerHllpp(ApproxCountDistinctForIntervals.scala:104)
	at org.apache.spark.sql.catalyst.expressions.aggregate.ApproxCountDistinctForIntervals.totalNumWords$lzycompute(ApproxCountDistinctForIntervals.scala:106)
	at org.apache.spark.sql.catalyst.expressions.aggregate.ApproxCountDistinctForIntervals.totalNumWords(ApproxCountDistinctForIntervals.scala:106)
	at org.apache.spark.sql.catalyst.expressions.aggregate.ApproxCountDistinctForIntervals.createAggregationBuffer(ApproxCountDistinctForIntervals.scala:110)
	at org.apache.spark.sql.catalyst.expressions.aggregate.ApproxCountDistinctForIntervals.createAggregationBuffer(ApproxCountDistinctForIntervals.scala:44)
	at org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.initialize(interfaces.scala:528)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator$$anonfun$initAggregationBuffer$2.apply(ObjectAggregationIterator.scala:120)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator$$anonfun$initAggregationBuffer$2.apply(ObjectAggregationIterator.scala:120)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.initAggregationBuffer(ObjectAggregationIterator.scala:120)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.org$apache$spark$sql$execution$aggregate$ObjectAggregationIterator$$createNewAggregationBuffer(ObjectAggregationIterator.scala:112)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.getAggregationBufferByKey(ObjectAggregationIterator.scala:128)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:150)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:78)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2.apply(ObjectHashAggregateExec.scala:114)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2.apply(ObjectHashAggregateExec.scala:105)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
{code}




",,joshrosen,pengbo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10914,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 16 22:50:59 UTC 2019,,,,,,,,,,"0|z01jkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/19 10:10;sandeep.katta2007;[~pengbo] thanks for raising this issue, soon I will raise PR for this;;;","08/Apr/19 12:10;pengbo;[~sandeep.katta2007] 

Actually, I have already submitted PR for this, can you please review it?

https://github.com/apache/spark/pull/24317/files;;;","16/Jul/19 22:50;joshrosen;Adding the 'correctness' label to this fixed issue because the related UnsafeRow serialization bug was a correctness issue so it seems likely that this is as well. In the example stacktrace this resulted in a crash, but in principle it could also result in wrong answers.;;;",,,,,,,,,,,,,,,,,,,
Fix `updateTableStats` to update table stats always with new stats or None,SPARK-27403,13226575,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,S71955,S71955,S71955,07/Apr/19 18:19,17/Apr/19 16:26,13/Jul/23 08:46,11/Apr/19 15:55,2.3.0,2.3.1,2.3.2,2.3.3,2.4.0,2.4.1,,,2.4.2,3.0.0,,,SQL,,,,,0,,,,,,"system shall update the table stats automatically if user set spark.sql.statistics.size.autoUpdate.enabled as true, currently this property is not having any significance even if it is enabled or disabled. This feature is similar to Hives auto-gather feature where statistics are automatically computed by default if this feature is enabled.

Reference:

[https://cwiki.apache.org/confluence/display/Hive/StatsDev]

Reproducing steps:

scala> spark.sql(""create table table1 (name string,age int) stored as parquet"")

scala> spark.sql(""insert into table1 select 'a',29"")
 res2: org.apache.spark.sql.DataFrame = []

scala> spark.sql(""desc extended table1"").show(false)
 +-------------------------------+-----------------------------------------------------------++-------
|col_name|data_type|comment|

+-------------------------------+-----------------------------------------------------------++-------
|name|string|null|
|age|int|null|
| | | |
| # Detailed Table Information| | |
|Database|default| |
|Table|table1| |
|Owner|Administrator| |
|Created Time|Sun Apr 07 23:41:56 IST 2019| |
|Last Access|Thu Jan 01 05:30:00 IST 1970| |
|Created By|Spark 2.4.1| |
|Type|MANAGED| |
|Provider|hive| |
|Table Properties|[transient_lastDdlTime=1554660716]| |
|Location|file:/D:/spark-2.4.1-bin-hadoop2.7/bin/spark-warehouse/table1| |
|Serde Library|org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe| |
|InputFormat|org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat| |
|OutputFormat|org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat| |
|Storage Properties|[serialization.format=1]| |
|Partition Provider|Catalog| |

+-------------------------------+-----------------------------------------------------------++-------",,dongjoon,S71955,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 11 15:55:07 UTC 2019,,,,,,,,,,"0|z01jb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/19 18:30;S71955;if user set spark.sql.statistics.size.autoUpdate.enabled as true, then system shall calculate the table size and record the same in metastore, 

On describe command the statistics shall be displayed

I will analyze further and raise a PR for handling the issue. please let me know for any suggestions. thanks;;;","07/Apr/19 20:22;dongjoon;Hi, [~S71955]. Could you check Spark 2.3.x behavior and update the affected versions? The existing code lands at 2.3.0 by SPARK-21237 .;;;","08/Apr/19 04:27;S71955;This has impact in the previous version also, will update the JIRA;;;","11/Apr/19 15:55;dongjoon;This is resolved via https://github.com/apache/spark/pull/24315;;;",,,,,,,,,,,,,,,,,,
The staleness of UI may last minutes or hours when no tasks start or finish,SPARK-27394,13226261,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,04/Apr/19 21:38,10/Apr/19 22:18,13/Jul/23 08:46,09/Apr/19 15:33,2.4.0,2.4.1,,,,,,,2.4.2,3.0.0,,,Web UI,,,,,0,,,,,,"Run the following codes on a cluster that has at least 2 cores.
{code}
sc.makeRDD(1 to 1000, 1000).foreach { i =>
  Thread.sleep(300000)
}
{code}

The jobs page will just show one running task.

This is because when the second task event calls ""AppStatusListener.maybeUpdate"" for a job, it will just ignore since the gap between two events is smaller than `spark.ui.liveUpdate.period`.

After the second task event, in the above case, because there won't be any other task events, the Spark UI will be always stale until the next task event gets fired (after 300 seconds).",,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-04-04 21:38:30.0,,,,,,,,,,"0|z01he0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deadlock in ContinuousExecution unit tests,SPARK-27391,13226252,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joseph.torres,joseph.torres,joseph.torres,04/Apr/19 21:10,06/Apr/19 04:16,13/Jul/23 08:46,05/Apr/19 19:57,2.4.0,,,,,,,,3.0.0,,,,Structured Streaming,,,,,0,,,,,,"ContinuousExecution (in the final query execution phrase) holds the lazy val lock of its IncrementalExecution for the entire duration of the (indefinite length) job. This can cause deadlocks in unit tests, which hook into internal APIs and try to instantiate other lazy vals.

 

(Note that this should not be able to affect production.)",,joseph.torres,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 05 19:57:54 UTC 2019,,,,,,,,,,"0|z01hc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/19 19:57;joseph.torres;Issue resolved by pull request 24301
[https://github.com/apache/spark/pull/24301];;;",,,,,,,,,,,,,,,,,,,,,
Fix package name mismatch,SPARK-27390,13226247,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dongjoon,dongjoon,dongjoon,04/Apr/19 20:52,05/Apr/19 19:00,13/Jul/23 08:46,05/Apr/19 19:00,3.0.0,,,,,,,,2.4.2,3.0.0,,,Spark Core,SQL,Tests,,,0,,,,,,,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 05 19:00:14 UTC 2019,,,,,,,,,,"0|z01haw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/19 19:00;dongjoon;This is fixed via https://github.com/apache/spark/pull/24300;;;",,,,,,,,,,,,,,,,,,,,,
Replace sqlutils assertPandasEqual with Pandas assert_frame_equal in tests,SPARK-27387,13226221,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bryanc,bryanc,bryanc,04/Apr/19 18:50,12/Dec/22 18:10,13/Jul/23 08:46,09/Apr/19 22:55,2.4.1,,,,,,,,3.0.0,,,,PySpark,Tests,,,,0,,,,,,"In PySpark unit tests, sqlutils ReusedSQLTestCase.assertPandasEqual is meant to check if 2 pandas.DataFrames are equal but it seems for later versions of Pandas, this can fail if the DataFrame has an array column. This method can be replaced by {{assert_frame_equal}} from pandas.util.testing.  This is what it is meant for and it will give a better assertion message as well.

The test failure I have seen is:

 {noformat}
======================================================================
ERROR: test_supported_types (pyspark.sql.tests.test_pandas_udf_grouped_map.GroupedMapPandasUDFTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/bryan/git/spark/python/pyspark/sql/tests/test_pandas_udf_grouped_map.py"", line 128, in test_supported_types
    self.assertPandasEqual(expected1, result1)
  File ""/home/bryan/git/spark/python/pyspark/testing/sqlutils.py"", line 268, in assertPandasEqual
    self.assertTrue(expected.equals(result), msg=msg)
  File ""/home/bryan/miniconda2/envs/pa012/lib/python3.6/site-packages/pandas

...
  File ""pandas/_libs/lib.pyx"", line 523, in pandas._libs.lib.array_equivalent_object
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
 {noformat}",,bryanc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27276,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 09 22:55:28 UTC 2019,,,,,,,,,,"0|z01h54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/19 18:51;bryanc;This can be done after the upgrade of pyarrow version to avoid conflicts.;;;","04/Apr/19 18:52;bryanc;I can work on this;;;","09/Apr/19 22:55;gurwls223;Fixed in https://github.com/apache/spark/pull/24306;;;",,,,,,,,,,,,,,,,,,,
Wrong outputRows estimation after AggregateEstimation with only null value column,SPARK-27351,13225711,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pengbo,pengbo,pengbo,03/Apr/19 06:58,23/May/19 20:41,13/Jul/23 08:46,23/May/19 20:41,2.4.1,,,,,,,,2.4.2,3.0.0,,,SQL,,,,,0,,,,,,"The upper bound of group-by columns row number is to multiply distinct counts of group-by columns. However, column with only null value will cause the output row number to be 0 which is incorrect.

Ex:
col1 (distinct: 2, rowCount 2)
col2 (distinct: 0, rowCount 2)

group by col1, col2
Actual: output rows: 0
Expected: output rows: 2 

{code:java}
var outputRows: BigInt = agg.groupingExpressions.foldLeft(BigInt(1))(
        (res, expr) => res * childStats.attributeStats(expr.asInstanceOf[Attribute]).distinctCount)
{code}
",,dongjoon,pengbo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27539,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 15 22:46:08 UTC 2019,,,,,,,,,,"0|z01e9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/19 22:46;dongjoon;This is resolved via https://github.com/apache/spark/pull/24286;;;",,,,,,,,,,,,,,,,,,,,,
HeartbeatReceiver doesn't remove lost executors from CoarseGrainedSchedulerBackend,SPARK-27348,13225603,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,zsxwing,zsxwing,02/Apr/19 18:35,02/Apr/20 00:42,13/Jul/23 08:46,30/Dec/19 05:58,2.4.0,,,,,,,,3.0.0,,,,Spark Core,,,,,1,,,,,,"When a heartbeat timeout happens in HeartbeatReceiver, it doesn't remove lost executors from CoarseGrainedSchedulerBackend. When a connection of an executor is not gracefully shut down, CoarseGrainedSchedulerBackend may not receive a disconnect event. In this case, CoarseGrainedSchedulerBackend still thinks a lost executor is still alive. CoarseGrainedSchedulerBackend may ask TaskScheduler to run tasks on this lost executor. This task will never finish and the job will hang forever.",,cloud_fan,sandeep.katta2007,sergey.mazin,shivusondur@gmail.com,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30297,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 30 05:58:17 UTC 2019,,,,,,,,,,"0|z01dm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/19 12:38;sandeep.katta2007;[~zsxwing] do you have any test code or scenario which can suffice your statement ?;;;","08/Apr/19 17:53;zsxwing;[~sandeep.katta2007] I cannot reproduce this locally. Ideally, when we decide to remove an executor, we should remove it from all places rather than counting on a TCP disconnect event which may not happen sometimes. ;;;","30/Dec/19 05:58;cloud_fan;Issue resolved by pull request 26980
[https://github.com/apache/spark/pull/26980];;;",,,,,,,,,,,,,,,,,,,
Fix supervised driver retry logic when agent crashes/restarts,SPARK-27347,13225564,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,stran,stran,stran,02/Apr/19 15:31,10/May/19 17:58,13/Jul/23 08:46,10/May/19 17:56,2.2.1,2.3.2,2.4.0,,,,,,2.3.4,2.4.4,3.0.0,,Mesos,,,,,0,,,,,,"Ran into scenarios where {{--supervised}} Spark jobs were retried multiple times when an agent would crash, come back, and re-register even when those jobs had already relaunched on a different agent.

That is:
 * supervised driver is running on agent1
 * agent1 crashes
 * driver is relaunched on another agent as `<task-id>-retry-1`
 * agent1 comes back online and re-registers with scheduler
 * spark relaunches the same job as `<task-id>-retry-2`
 * now there are two jobs running simultaneously and the first retry job is effectively orphaned within Zookeeper

This is because when an agent comes back and re-registers, it sends a status update {{TASK_FAILED}} for its old driver-task. Previous logic would indiscriminately remove the {{submissionId from Zookeeper's launchedDrivers}} node and add it to {{retryList}} node.

Then, when a new offer came in, it would relaunch another -retry task even though one was previously running.

Sample log looks something like this: 
{code:java}
19/01/15 19:21:38 TRACE MesosClusterScheduler: Received offers from Mesos: 
... [offers] ...
19/01/15 19:21:39 TRACE MesosClusterScheduler: Using offer 5d421001-0630-4214-9ecb-d5838a2ec149-O2532 to launch driver driver-20190115192138-0001 with taskId: value: ""driver-20190115192138-0001""
...
19/01/15 19:21:42 INFO MesosClusterScheduler: Received status update: taskId=driver-20190115192138-0001 state=TASK_STARTING message=''
19/01/15 19:21:43 INFO MesosClusterScheduler: Received status update: taskId=driver-20190115192138-0001 state=TASK_RUNNING message=''
...
19/01/15 19:29:12 INFO MesosClusterScheduler: Received status update: taskId=driver-20190115192138-0001 state=TASK_LOST message='health check timed out' reason=REASON_SLAVE_REMOVED
...
19/01/15 19:31:12 TRACE MesosClusterScheduler: Using offer 5d421001-0630-4214-9ecb-d5838a2ec149-O2681 to launch driver driver-20190115192138-0001 with taskId: value: ""driver-20190115192138-0001-retry-1""
...
19/01/15 19:31:15 INFO MesosClusterScheduler: Received status update: taskId=driver-20190115192138-0001-retry-1 state=TASK_STARTING message=''
19/01/15 19:31:16 INFO MesosClusterScheduler: Received status update: taskId=driver-20190115192138-0001-retry-1 state=TASK_RUNNING message=''
...
19/01/15 19:33:45 INFO MesosClusterScheduler: Received status update: taskId=driver-20190115192138-0001 state=TASK_FAILED message='Unreachable agent re-reregistered'
...
19/01/15 19:33:45 INFO MesosClusterScheduler: Received status update: taskId=driver-20190115192138-0001 state=TASK_FAILED message='Abnormal executor termination: unknown container' reason=REASON_EXECUTOR_TERMINATED
19/01/15 19:33:45 ERROR MesosClusterScheduler: Unable to find driver with driver-20190115192138-0001 in status update
...
19/01/15 19:33:47 TRACE MesosClusterScheduler: Using offer 5d421001-0630-4214-9ecb-d5838a2ec149-O2729 to launch driver driver-20190115192138-0001 with taskId: value: ""driver-20190115192138-0001-retry-2""
...
19/01/15 19:33:50 INFO MesosClusterScheduler: Received status update: taskId=driver-20190115192138-0001-retry-2 state=TASK_STARTING message=''
19/01/15 19:33:51 INFO MesosClusterScheduler: Received status update: taskId=driver-20190115192138-0001-retry-2 state=TASK_RUNNING message=''{code}",,dongjoon,stran,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 10 17:56:43 UTC 2019,,,,,,,,,,"0|z01ddc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/May/19 17:56;dongjoon;This is resolved via https://github.com/apache/spark/pull/24276;;;",,,,,,,,,,,,,,,,,,,,,
Alias on TimeWIndow expression may cause watermark metadata lost ,SPARK-27340,13225418,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,KevinZwx,KevinZwx,02/Apr/19 03:55,30/Apr/20 16:23,13/Jul/23 08:46,27/Apr/20 22:09,2.4.0,,,,,,,,3.0.0,,,,SQL,Structured Streaming,,,,0,,,,,,"When we use data api to write a structured streaming query job we usually specify a watermark on event time column. If we define a window on the event time column, the delayKey metadata of the event time column is supposed to be propagated to the new column generated by time window expression. But if we add additional alias on the time window column, the delayKey metadata is lost.

Currently I only find the bug will affect stream-stream join with equal window join keys. In terms of aggregation, the gourping expression can be trimed(in CleanupAliases rule) so additional alias are removed and the metadata is kept.

Here is an example:
{code:scala}
  val sparkSession = SparkSession
    .builder()
    .master(""local"")
    .getOrCreate()
  val rateStream = sparkSession.readStream
    .format(""rate"")
    .option(""rowsPerSecond"", 10)
    .load()
    val fooStream = rateStream
      .select(
        col(""value"").as(""fooId""),
        col(""timestamp"").as(""fooTime"")
      )
      .withWatermark(""fooTime"", ""2 seconds"")
      .select($""fooId"", $""fooTime"", window($""fooTime"", ""2 seconds"").alias(""fooWindow""))

    val barStream = rateStream
      .where(col(""value"") % 2 === 0)
      .select(
        col(""value"").as(""barId""),
        col(""timestamp"").as(""barTime"")
      )
      .withWatermark(""barTime"", ""2 seconds"")
      .select($""barId"", $""barTime"", window($""barTime"", ""2 seconds"").alias(""barWindow""))

    val joinedDf = fooStream
      .join(
        barStream,
        $""fooId"" === $""barId"" &&
          fooStream.col(""fooWindow"") === barStream.col(""barWindow""),
        joinType = ""LeftOuter""
      )

      val query = joinedDf
      .writeStream
      .format(""console"")
      .option(""truncate"", 100)
      .trigger(Trigger.ProcessingTime(""5 seconds""))
      .start()

    query.awaitTermination()
{code}

this program will end with an exception, and from the analyzed plan we can see there is no delayKey metadata on 'fooWindow'

{code:java}
org.apache.spark.sql.AnalysisException: Stream-stream outer join between two streaming DataFrame/Datasets is not supported without a watermark in the join keys, or a watermark on the nullable side and an appropriate range condition;;
Join LeftOuter, ((fooId#4L = barId#14L) && (fooWindow#9 = barWindow#19))
:- Project [fooId#4L, fooTime#5-T2000ms, window#10-T2000ms AS fooWindow#9]
:  +- Filter isnotnull(fooTime#5-T2000ms)
:     +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(fooTime#5-T2000ms, TimestampType, LongType) - 0) as double) / cast(2000000 as double))) as double) = (cast((precisetimestampconversion(fooTime#5-T2000ms, TimestampType, LongType) - 0) as double) / cast(2000000 as double))) THEN (CEIL((cast((precisetimestampconversion(fooTime#5-T2000ms, TimestampType, LongType) - 0) as double) / cast(2000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(fooTime#5-T2000ms, TimestampType, LongType) - 0) as double) / cast(2000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 2000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(fooTime#5-T2000ms, TimestampType, LongType) - 0) as double) / cast(2000000 as double))) as double) = (cast((precisetimestampconversion(fooTime#5-T2000ms, TimestampType, LongType) - 0) as double) / cast(2000000 as double))) THEN (CEIL((cast((precisetimestampconversion(fooTime#5-T2000ms, TimestampType, LongType) - 0) as double) / cast(2000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(fooTime#5-T2000ms, TimestampType, LongType) - 0) as double) / cast(2000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 2000000) + 0) + 2000000), LongType, TimestampType)) AS window#10-T2000ms, fooId#4L, fooTime#5-T2000ms]
:        +- EventTimeWatermark fooTime#5: timestamp, interval 2 seconds
:           +- Project [value#1L AS fooId#4L, timestamp#0 AS fooTime#5]
:              +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2cae5fa7, rate, Map(numPartitions -> 1, rowsPerSecond -> 1), [timestamp#0, value#1L]
+- Project [barId#14L, barTime#15-T2000ms, window#20-T2000ms AS barWindow#19]
   +- Filter isnotnull(barTime#15-T2000ms)
      +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(barTime#15-T2000ms, TimestampType, LongType) - 0) as double) / cast(2000000 as double))) as double) = (cast((precisetimestampconversion(barTime#15-T2000ms, TimestampType, LongType) - 0) as double) / cast(2000000 as double))) THEN (CEIL((cast((precisetimestampconversion(barTime#15-T2000ms, TimestampType, LongType) - 0) as double) / cast(2000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(barTime#15-T2000ms, TimestampType, LongType) - 0) as double) / cast(2000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 2000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(barTime#15-T2000ms, TimestampType, LongType) - 0) as double) / cast(2000000 as double))) as double) = (cast((precisetimestampconversion(barTime#15-T2000ms, TimestampType, LongType) - 0) as double) / cast(2000000 as double))) THEN (CEIL((cast((precisetimestampconversion(barTime#15-T2000ms, TimestampType, LongType) - 0) as double) / cast(2000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(barTime#15-T2000ms, TimestampType, LongType) - 0) as double) / cast(2000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 2000000) + 0) + 2000000), LongType, TimestampType)) AS window#20-T2000ms, barId#14L, barTime#15-T2000ms]
         +- EventTimeWatermark barTime#15: timestamp, interval 2 seconds
            +- Project [value#1L AS barId#14L, timestamp#0 AS barTime#15]
               +- Filter ((value#1L % cast(2 as bigint)) = cast(0 as bigint))
                  +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2cae5fa7, rate, Map(numPartitions -> 1, rowsPerSecond -> 1), [timestamp#0, value#1L]
{code}
",,apachespark,dongjoon,KevinZwx,Liangchang Zhu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 30 16:23:16 UTC 2020,,,,,,,,,,"0|z01cgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/20 22:09;dongjoon;Issue resolved by pull request 28326
[https://github.com/apache/spark/pull/28326];;;","30/Apr/20 06:10;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/28390;;;","30/Apr/20 06:11;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/28390;;;","30/Apr/20 16:23;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/28377;;;",,,,,,,,,,,,,,,,,,
Deadlock between TaskMemoryManager and UnsafeExternalSorter$SpillableIterator,SPARK-27338,13225316,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vsowrirajan,vsowrirajan,vsowrirajan,01/Apr/19 16:17,04/Apr/19 14:52,13/Jul/23 08:46,04/Apr/19 02:05,2.4.0,,,,,,,,2.3.4,2.4.2,3.0.0,,Spark Core,,,,,0,,,,,,"We saw similar deadlock like this https://issues.apache.org/jira/browse/SPARK-26265 happening between TaskMemoryManager and UnsafeExternalSorted$SpillableIterator

Jstack output:

jstack information as follow:


{code:java}
Found one Java-level deadlock:
=============================
""stdout writer for /usr/lib/envs/env-1923-ver-1755-a-4.2.9-py-3.5.3/bin/python"":
  waiting to lock monitor 0x00007fce56409088 (object 0x00000005700a2f98, a org.apache.spark.memory.TaskMemoryManager),
  which is held by ""Executor task launch worker for task 2203""
""Executor task launch worker for task 2203"":
  waiting to lock monitor 0x00000000007cd878 (object 0x00000005701a0eb0, a org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator),
  which is held by ""stdout writer for /usr/lib/envs/env-1923-ver-1755-a-4.2.9-py-3.5.3/bin/python""

Java stack information for the threads listed above:
===================================================
""stdout writer for /usr/lib/envs/env-1923-ver-1755-a-4.2.9-py-3.5.3/bin/python"":
	at org.apache.spark.memory.TaskMemoryManager.freePage(TaskMemoryManager.java:334)
	- waiting to lock <0x00000005700a2f98> (a org.apache.spark.memory.TaskMemoryManager)
	at org.apache.spark.memory.MemoryConsumer.freePage(MemoryConsumer.java:130)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.access$1100(UnsafeExternalSorter.java:48)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:583)
	- locked <0x00000005701a0eb0> (a org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.next(UnsafeExternalRowSorter.java:187)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.next(UnsafeExternalRowSorter.java:174)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$2.hasNext(WholeStageCodegenExec.scala:638)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)
	at scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)
	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1127)
	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2067)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)
""Executor task launch worker for task 2203"":
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.spill(UnsafeExternalSorter.java:525)
	- waiting to lock <0x00000005701a0eb0> (a org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:200)
	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:177)
	- locked <0x00000005700a2f98> (a org.apache.spark.memory.TaskMemoryManager)
	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:285)
	at org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:383)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:407)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:135)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:217)
	at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:108)
	at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:101)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1473)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Found 1 deadlock.
{code}
",,cloud_fan,ekoifman,vsowrirajan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 04 02:05:56 UTC 2019,,,,,,,,,,"0|z01buw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/19 16:17;vsowrirajan;We found this issue in one of the customer workload. Will file a PR for this issue shortly.;;;","04/Apr/19 02:05;cloud_fan;Issue resolved by pull request 24265
[https://github.com/apache/spark/pull/24265];;;",,,,,,,,,,,,,,,,,,,,
ForeachWriter is not being closed once a batch is aborted,SPARK-27330,13225040,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eyalzit,eyalzit,eyalzit,31/Mar/19 08:53,12/Dec/22 18:11,13/Jul/23 08:46,23/Aug/19 14:44,2.4.0,,,,,,,,2.4.4,3.0.0,,,Structured Streaming,,,,,0,,,,,,"in cases where a micro batch is being killed (interrupted), not during actual processing done by the {{ForeachDataWriter}} (when iterating the iterator), {{DataWritingSparkTask}} will handle the interruption and call {{dataWriter.abort()}}

the problem is that {{ForeachDataWriter}} has an empty implementation for the abort method.

due to that, I have tasks which uses the foreach writer and according to the [documentation|https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreach] they are opening connections in the ""open"" method and closing the connections on the ""close"" method but since the ""close"" is never called, the connections are never closed

this wasn't the behavior pre spark 2.4

my suggestion is to call {{ForeachWriter.abort()}} when {{DataWriter.abort()}} is called,  in order to notify the foreach writer that this task has failed

 
{code:java}
stack trace from the exception i have encountered:
 org.apache.spark.TaskKilledException: null
 at org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:149)
 at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)
 at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
 at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:117)
 at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)
 at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
 at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)
 at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)
 at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)
{code}
 ",,dongjoon,eyalzit,gsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 23 14:53:14 UTC 2019,,,,,,,,,,"0|z01a5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/19 09:05;eyalzit;guys, let me know if my fix suggestion is make sense and i will provide a PR;;;","01/Apr/19 10:45;gsomogyi;If processing throws any kind of exception then ForeachWriter never closed.
Just for my understanding what kind of problems have you faced when you've written the following?
{quote}i have encountered issues in connections{quote}
A little bit more detail would be good.;;;","01/Apr/19 11:13;eyalzit;I have updated the description, eventually i have tasks which are being killed without calling the ""close"" method and due to that i have connections which are never closed which lead to a leak in my connection pool.

from the docs:

{code}
streamingDatasetOfString.writeStream.foreach( new ForeachWriter[String] {
  def open(partitionId: Long, version: Long): Boolean = {
    // Open connection
  }

  def process(record: String): Unit = {
    // Write string to connection
  }

  def close(errorOrNull: Throwable): Unit = {
    // Close the connection
  }}).start()
{code}

 ;;;","01/Apr/19 11:19;gsomogyi;Now I see the point and makes sense from my perspective. A good example when CommitDeniedException has been thrown.
This can be properly unit tested. The tricky part is to make sure the close not called multiple times.;;;","02/Apr/19 00:31;gurwls223;(I think {{close()}} should be idempotent and documented as so);;;","15/Apr/19 11:59;gsomogyi;[~eyalzit] are you working on this? Happy to file a PR if don't have time.;;;","15/Apr/19 12:17;eyalzit;[~gsomogyi] yes, almost done with it;;;","15/Apr/19 12:30;gsomogyi;Cool, ping me if you need review...;;;","16/Apr/19 14:12;eyalzit;[~gsomogyi] i've submitted a PR;;;","23/Aug/19 14:53;dongjoon;Thank you, [~eyalzit]. You are added to the Apache Spark contributor group.;;;",,,,,,,,,,,,
DStreamCheckpointData failed to clean up because it's fileSystem cached,SPARK-27301,13224508,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,28/Mar/19 07:51,30/Mar/19 07:38,13/Jul/23 08:46,30/Mar/19 07:37,2.1.2,,,,,,,,2.3.4,2.4.2,3.0.0,,DStreams,Spark Core,,,,0,,,,,,"The cached FileSystem's token will expire if no tokens explicitly are add into it.
{code:java}
19/03/28 13:40:16 INFO storage.BlockManager: Removing RDD 83189
19/03/28 13:40:16 INFO rdd.MapPartitionsRDD: Removing RDD 82860 from persistence list
19/03/28 13:40:16 INFO spark.ContextCleaner: Cleaned shuffle 6005
19/03/28 13:40:16 INFO storage.BlockManager: Removing RDD 82860
19/03/28 13:40:16 INFO scheduler.ReceivedBlockTracker: Deleting batches:
19/03/28 13:40:16 INFO scheduler.InputInfoTracker: remove old batch metadata: 1553750250000 ms
19/03/28 13:40:17 WARN security.UserGroupInformation: PriviledgedActionException as:urs@HADOOP.HZ.NETEASE.COM (auth:KERBEROS) cause:org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 53240500 for urs) is expired, current time: 2019-03-28 13:40:17,010+0800 expected renewal time: 2019-03-28 13:39:48,523+0800
19/03/28 13:40:17 WARN ipc.Client: Exception encountered while connecting to the server : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 53240500 for urs) is expired, current time: 2019-03-28 13:40:17,010+0800 expected renewal time: 2019-03-28 13:39:48,523+0800
19/03/28 13:40:17 WARN security.UserGroupInformation: PriviledgedActionException as:urs@HADOOP.HZ.NETEASE.COM (auth:KERBEROS) cause:org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 53240500 for urs) is expired, current time: 2019-03-28 13:40:17,010+0800 expected renewal time: 2019-03-28 13:39:48,523+0800
19/03/28 13:40:17 WARN hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_-1396157959_1] for 53 seconds. Will retry shortly ...
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 53240500 for urs) is expired, current time: 2019-03-28 13:40:17,010+0800 expected renewal time: 2019-03-28 13:39:48,523+0800
at org.apache.hadoop.ipc.Client.call(Client.java:1468)
at org.apache.hadoop.ipc.Client.call(Client.java:1399)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
at com.sun.proxy.$Proxy11.renewLease(Unknown Source)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:571)
at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
at com.sun.proxy.$Proxy12.renewLease(Unknown Source)
at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:878)
at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
at java.lang.Thread.run(Thread.java:748)
{code}",,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 30 07:37:08 UTC 2019,,,,,,,,,,"0|z016wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/19 07:37;srowen;Issue resolved by pull request 24235
[https://github.com/apache/spark/pull/24235];;;",,,,,,,,,,,,,,,,,,,,,
Dataset except operation gives different results(dataset count) on Spark 2.3.0 Windows and Spark 2.3.0 Linux environment,SPARK-27298,13224484,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,Mahima,Mahima,28/Mar/19 05:25,12/Dec/22 18:11,13/Jul/23 08:46,07/Feb/20 23:24,2.3.0,2.4.2,,,,,,,2.4.4,,,,SQL,,,,,0,data-loss,,,,,"{code:java}
// package com.verifyfilter.example;

import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.Column;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SaveMode;

public class ExcludeInTesting {

public static void main(String[] args) {
SparkSession spark = SparkSession.builder()
.appName(""ExcludeInTesting"")
.config(""spark.some.config.option"", ""some-value"")
.getOrCreate();

Dataset<Row> dataReadFromCSV = spark.read().format(""com.databricks.spark.csv"")
.option(""header"", ""true"")
.option(""delimiter"", ""|"")
.option(""inferSchema"", ""true"")
//.load(""E:/resources/customer.csv""); local //below path for VM
.load(""/home/myproject/bda/home/bin/customer.csv"");
dataReadFromCSV.printSchema();
dataReadFromCSV.show();
//Adding an extra step of saving to db and then loading it again
dataReadFromCSV.write().mode(SaveMode.Overwrite).saveAsTable(""customer"");

Dataset<Row> dataLoaded = spark.sql(""select * from customer"");
//Gender EQ M
Column genderCol = dataLoaded.col(""Gender"");
Dataset<Row> onlyMaleDS = dataLoaded.where(genderCol.equalTo(""M""));
//Dataset<Row> onlyMaleDS = spark.sql(""select count(*) from customer where Gender='M'"");
onlyMaleDS.show();
System.out.println(""The count of Male customers is :""+ onlyMaleDS.count());
System.out.println(""*************************************"");
// Income in the list
Object[] valuesArray = new Object[5];
valuesArray[0]=503.65;
valuesArray[1]=495.54;
valuesArray[2]=486.82;
valuesArray[3]=481.28;
valuesArray[4]=479.79;

Column incomeCol = dataLoaded.col(""Income"");
Dataset<Row> incomeMatchingSet = dataLoaded.where(incomeCol.isin((Object[]) valuesArray));
System.out.println(""The count of customers satisfaying Income is :""+ incomeMatchingSet.count());
System.out.println(""*************************************"");

Dataset<Row> maleExcptIncomeMatch = onlyMaleDS.except(incomeMatchingSet);
System.out.println(""The count of final customers is :""+ maleExcptIncomeMatch.count());
System.out.println(""*************************************"");

}

}

{code}
 When the above code is executed on Spark 2.3.0 ,it gives below different results:

*Windows* :  The code gives correct count of dataset 148237,

*Linux :*         The code gives different {color:#172b4d}count of dataset 129532 {color}

 

{color:#172b4d}Some more info related to this bug:{color}

{color:#172b4d}1. Application Code (attached)
2. CSV file used(attached)
3. Windows spec 
          Windows 10- 64 bit OS 
4. Linux spec (Running on Oracle VM virtual box)
      Specifications: \{as captured from Vbox.log}
        00:00:26.112908 VMMDev: Guest Additions information report: Version 5.0.32 r112930          '5.0.32_Ubuntu'
        00:00:26.112996 VMMDev: Guest Additions information report: Interface = 0x00010004         osType = 0x00053100 (Linux >= 2.6, 64-bit)
5. Snapshots of output in both cases (attached){color}",,dongjoon,ksunitha,Mahima,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/19 05:33;Mahima;Console-Result-Windows.txt;https://issues.apache.org/jira/secure/attachment/12963982/Console-Result-Windows.txt","06/Feb/20 06:52;Mahima;Linux-spark-2.3.0_result.txt;https://issues.apache.org/jira/secure/attachment/12992757/Linux-spark-2.3.0_result.txt","06/Feb/20 06:52;Mahima;Linux-spark-2.4.4_result.txt;https://issues.apache.org/jira/secure/attachment/12992758/Linux-spark-2.4.4_result.txt","04/Apr/19 05:59;Mahima;console-reslt-2.3.3-linux.txt;https://issues.apache.org/jira/secure/attachment/12964815/console-reslt-2.3.3-linux.txt","04/Apr/19 05:59;Mahima;console-result-2.3.3-windows.txt;https://issues.apache.org/jira/secure/attachment/12964816/console-result-2.3.3-windows.txt","28/Mar/19 05:35;Mahima;console-result-LinuxonVM.txt;https://issues.apache.org/jira/secure/attachment/12963983/console-result-LinuxonVM.txt","30/Apr/19 06:35;Mahima;console-result-spark-2.4.2-linux;https://issues.apache.org/jira/secure/attachment/12967433/console-result-spark-2.4.2-linux","30/Apr/19 06:35;Mahima;console-result-spark-2.4.2-windows;https://issues.apache.org/jira/secure/attachment/12967434/console-result-spark-2.4.2-windows","28/Mar/19 05:26;Mahima;customer.csv;https://issues.apache.org/jira/secure/attachment/12963980/customer.csv","28/Mar/19 05:37;Mahima;pom.xml;https://issues.apache.org/jira/secure/attachment/12963984/pom.xml",,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 07 23:24:16 UTC 2020,,,,,,,,,,"0|z016rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/19 00:29;gurwls223;Please avoid to set Critical+ which is usually reserved for committers.;;;","29/Mar/19 19:03;srowen;I think your data has newlines issues, causing it to be parsed differently on different envs, where the newline default is different. Check your data vs your parser settings.;;;","01/Apr/19 06:01;Mahima;[~srowen] , There are no newline issues for sure.Since after the csv is saved to Spark-warehouse, the  dataset count in both the cases is same i.e. 259874 You can refer to the code and console output files for reference.;;;","01/Apr/19 13:49;srowen;I take it back, that's not the issue. 
There are a few odd things here like using floating-point equality, and that you are effectively executing 'except distinct', but I don't know of a reason it would vary across envs.
You would have to reproduce this on a newer release than 2.3.0 though, as even 2.3.3 is about to be EOL. I would try a build from 'master' as it's possible it's something that was fixed long ago. At least, try 2.3.3.;;;","04/Apr/19 03:23;Mahima;Sure , I will try the code with 2.3.3 and check;;;","04/Apr/19 06:05;Mahima;I downloaded ""spark-2.3.3-bin-hadoop2.7"" and tested and found that the results are different on the two OS even now.

Attached the console output as txt  files.;;;","22/Apr/19 10:19;gurwls223;Will you be able to test it against Spark 2.4.1 too?;;;","22/Apr/19 11:14;Mahima;Yes,I can test this .Will surely let you know the results.

 ;;;","30/Apr/19 06:35;Mahima;I have tested the code with Spark-2.4.2 version.But the behavior is still the same. The count is still different. Attached the console outputs for the testing done.;;;","29/Jan/20 01:33;ksunitha;1) Would it be possible to run your program with explain true to see the query plan in both the setups.  For e.g: 

maleExcptIncomeMatch.explain(true);

2) Also can you add this query in your application and get the output.

spark.sql(""select count(*) from customer where Income is null and Gender='M'"").show()

fwiw, I do not have the exact env that you have, but just wanted to add that I tried to run your repro on my mac and with spark 3.0 preview2 and the count shows up  as 148237.  

Interestingly, I observed that the difference that you are seeing in the count, actually matches the rows that have income null for Gender= 'M', which is 18705.  ;;;","05/Feb/20 03:38;Mahima;As of now I do not have the environment setup to test this .

Can I request you to submit the attached code jar to spark which is running on Linux machine and see the count in the result.

Since the problem I am talking about is varied result on different OS, for you the count on ""Mac"" is same as I am getting on ""windows"".

Hence it is very important for you to reproduce it first on Linux and see the count. ;;;","05/Feb/20 19:36;ksunitha;I tried on linux as well with the spark 3.0.0.0 preview2 and I cannot reproduce the behavior you observe.  I also quickly tried on linux with spark 2.4.2 but couldn't repro. I'm using the default spark distribution [spark-2.4.2-bin-hadoop2.7.tgz|https://archive.apache.org/dist/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz]  I am not sure what the differences are with your env.  

{{fwiw, here is some info on the linux env where I tried it out:}}
{quote}cat /etc/os-release

NAME=""Ubuntu""

VERSION=""18.04.3 LTS (Bionic Beaver)""

ID=ubuntu

ID_LIKE=debian

PRETTY_NAME=""Ubuntu 18.04.3 LTS""

VERSION_ID=""18.04"" 

 .....

 

uname -a

Linux xyz.com 4.15.0-66-generic #75-Ubuntu SMP Tue Oct 1 05:24:09 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
{quote}
 ;;;","05/Feb/20 19:42;ksunitha;If you get a chance to repro the issue again, it would be good to obtain the explain and the other query as I mentioned in my earlier comment.  Thanks. ;;;","06/Feb/20 06:52;Mahima;@Sunita ,We tested the bug with ""*spark-2.4.4-bin-hadoop2.7*"" and it shows the correct count.

*The bug is fixed with this version*.

==================================================

The count of Male customers is :148240
*************************************
The count of customers satisfaying Income is :5
*************************************
The count of final customers is :148237
*************************************

===============================================

We also tested again with spark-2.3.0 and it showed the wrong count.

It shows clearly there was a bug. 

The detailed console logs are attached.[*Linux-spark-2.3.0_result, Linux-spark-2.4.4_result*];;;","06/Feb/20 21:18;ksunitha;Thanks for trying it out in your env. That is good to know, that you are getting the right result on spark-2.4.4 and not on Spark-2.3.0.

Based on that, I ran this test on spark 2.3.0 in my linux environment and I can see the wrong count. I generated the explain to debug this and the plan is optimized to 

Spark 2.3.0

 
{code:java}
== Optimized Logical Plan ==
Aggregate [CustID#92, DOB#93, Gender#94, HouseholdID#95, Income#96, Initials#97, Occupation#98, Surname#99, Telephone#100L, Title#101], [CustID#92, DOB#93, Gender#94, HouseholdID#95, Income#96, Initials#97, Occupation#98, Surname#99, Telephone#100L, Title#101]
+- Filter ((isnotnull(Gender#94) && (Gender#94 = M)) && NOT Income#96 IN (503.65,495.54,486.82,481.28,479.79))
   +- Relation[CustID#92,DOB#93,Gender#94,HouseholdID#95,Income#96,Initials#97,Occupation#98,Surname#99,Telephone#100L,Title#101] parquet
{code}
 

 

With Spark 2.3.3, where it generates the correct count, I see the optimized plan.

 
{code:java}
== Optimized Logical Plan ==
Aggregate [CustID#92, DOB#93, Gender#94, HouseholdID#95, Income#96, Initials#97, Occupation#98, Surname#99, Telephone#100L, Title#101], [CustID#92, DOB#93, Gender#94, HouseholdID#95, Income#96, Initials#97, Occupation#98, Surname#99, Telephone#100L, Title#101]
+- Filter ((isnotnull(Gender#94) && (Gender#94 = M)) && NOT coalesce(Income#96 IN (503.65,495.54,486.82,481.28,479.79), false))
   +- Relation[CustID#92,DOB#93,Gender#94,HouseholdID#95,Income#96,Initials#97,Occupation#98,Surname#99,Telephone#100L,Title#101] parquet
{code}
 

 

Observations:

This issue is caused by the nulls in Income rows that were being filtered out incorrectly.  This is coming from the optimizer rule 'ReplaceExceptWithFilter'.  

This bug was fixed in SPARK-26366 and back ported and fixed in spark 2.3.3. 

---------

There doesn't seem to be anything specific to OS in this fix, so I am not sure why you were seeing the correct count on windows with  Spark 2.3.0(that has the bug).  For this, will need to get hold of the explain and also the count on how many rows were null for income column for gender=M on windows (as mentioned in my earlier comments).;;;","07/Feb/20 23:23;dongjoon;[~ksunitha]. SPARK-26366 is backported to 2.4.1 and the attached `console-result-spark-2.4.2-windows` shows a wrong result on windows. I'm not sure about that SPARK-26366 is related to this. BTW, since [~Mahima] verified that this is fixed at 2.4.4, I'll update the JIRA according to that information. We can add more information later when we find out what JIRA exactly fixed this.;;;","07/Feb/20 23:24;dongjoon;Remove `Target Version: 3.0.0` and set `Fix Version: 2.4.4`.;;;",,,,,
Efficient User Defined Aggregators ,SPARK-27296,13224451,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eje,eje,eje,27/Mar/19 23:12,15/Jun/20 21:24,13/Jul/23 08:46,12/Jan/20 07:38,2.3.3,2.4.0,3.0.0,,,,,,3.0.0,,,,Spark Core,SQL,Structured Streaming,,,0,performance,usability,,,,"Spark's UDAFs appear to be serializing and de-serializing to/from the MutableAggregationBuffer for each row.  This gist shows a small reproducing UDAF and a spark shell session:

[https://gist.github.com/erikerlandson/3c4d8c6345d1521d89e0d894a423046f]

The UDAF and its compantion UDT are designed to count the number of times that ser/de is invoked for the aggregator.  The spark shell session demonstrates that it is executing ser/de on every row of the data frame.

Note, Spark's pre-defined aggregators do not have this problem, as they are based on an internal aggregating trait that does the correct thing and only calls ser/de at points such as partition boundaries, presenting final results, etc.

This is a major problem for UDAFs, as it means that every UDAF is doing a massive amount of unnecessary work per row, including but not limited to Row object allocations. For a more realistic UDAF having its own non trivial internal structure it is obviously that much worse.",,chitralverma,cloud_fan,Cording,eje,ekoifman,ksunitha,mauzhang,ozars,reynoldsm88,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30423,,,SPARK-30423,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Wed Apr 15 08:21:07 UTC 2020,,,,,,,,,,"0|z016k0:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,"28/Mar/19 18:23;eje;My initial proposal would be to alter the logic underneath
{code:java}
register(name: String, udaf: UserDefinedAggregateFunction){code}
so that the UDAF gets hooked to a TypedImperativeAggregate, and registered in the same way that objects like CountMinSketchAgg are.;;;","03/Jul/19 00:04;eje;The basic approach as described above appears to be working (see the linked PR). To obtain the desired behavior I had to create a new API, which is fairly similar to UDAF, but inherits from TypedImperativeAggregate. This new API supports UDT and Column instantiation, and so I believe it offers feature parity with the original UDAF, with substantial performance improvements.;;;","06/Jul/19 20:30;eje;I wrote up my benchmarking results [here|https://github.com/apache/spark/pull/25024#issue-293548866]. For aggregators having a non-trivial serde cost, the performance improvement can be two orders of magnitude. For aggregators with more simple serde, the improvement is correspondingly smaller.;;;","19/Oct/19 16:45;eje;This started with the goal of fixing the performance bug in UDAF, but ultimately is a new variation on user defined aggregation, so I'm no longer sure if this Jira should be categorized as ""bug"" or ""feature"";;;","12/Jan/20 07:38;cloud_fan;Issue resolved by pull request 25024
[https://github.com/apache/spark/pull/25024];;;","14/Apr/20 10:40;Cording;I've been trying this out, and I have a couple of questions.

First, here's a snippet showing what I did.
{code:java}
class LongestRunUdaf extends UserDefinedAggregateFunction {
  // ...
}

class LongestRunAggregator extends Aggregator[String, LongestRunBuffer, Option[Run]] {
  // ...
}

// This is to get a reference. It is slow.
val lrUdaf = new LongestRunUdaf
val result = df.select(lrUdaf(col(""value"")))

// This is many times faster, but `res` is now a `DataFrame`
val longestRunAggregator = udaf(new LongestRunAggregator, Encoders.STRING)
val res = dataset.select(longestRunAggregator(col(""value"")))
res.show()

// This creates a `Dataset[Option[Run]]` as needed, but now it is as slow as the old UDAF
res.as(Encoders.kryo[Option[Run]])

// Furthermore, this is still as slow as before
val longestRunAggregator = (new LongestRunAggregator).toColumn
dataset.select(longestRunAggregator).first{code}
I am confused by the fact that the performance improvement only is visible when using UDAFs, but in order to get there, I need to define an Aggregator, which is still slow if I use it directly. Is it not possible to also achieve the improvement for the type-safe cases where Aggregators are used directly?

Also, wrapping an Aggregator where the output is of a non-standard type leaves you with a DataFrame with binary data. Converting it to a Dataset seems to negate the performance improvement. Is this intended behaviour? In my example, it should just be one row that should be deserialized.

Or am I just using this in the wrong way?

My full experiment is here: [https://github.com/patrickcording/udaf-benchmark];;;","15/Apr/20 08:21;cloud_fan;This feature is to speed up UDAF by using Aggregator, but not to improve Aggregator performance.;;;",,,,,,,,,,,,,,,
Reuse subquery should compare child node only,SPARK-27279,13223941,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,adrian-wang,adrian-wang,26/Mar/19 03:25,04/Apr/19 21:28,13/Jul/23 08:46,27/Mar/19 15:46,2.4.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"For now, `ReuseSubquery` in Spark compares two subqueries at `SubqueryExec` level, which invalidates the `ReuseSubquery` rule.",,adrian-wang,jonathak,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-03-26 03:25:38.0,,,,,,,,,,"0|z013fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential corruption in EncryptedMessage.transferTo,SPARK-27275,13223883,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,25/Mar/19 22:32,02/Mar/20 20:54,13/Jul/23 08:46,26/Mar/19 22:59,2.2.0,2.3.0,2.4.0,,,,,,2.3.4,2.4.2,3.0.0,,Spark Core,,,,,0,correctness,,,,,"`EncryptedMessage.transferTo` has a potential corruption issue. When the underlying buffer has more than `1024 * 32` bytes (this should be rare but it could happen in error messages that send over the wire), it may just send a partial message as `EncryptedMessage.count` becomes less than `transferred`. This will cause the client hang forever (or timeout) as it will wait until receiving expected length of bytes,  or weird errors (such as corruption or silent correctness issue) if the channel is reused by other messages.",,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-03-25 22:32:09.0,,,,,,,,,,"0|z0132o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snappy 1.1.7.1 fails when decompressing empty serialized data,SPARK-27267,13223720,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,max2049,max2049,max2049,25/Mar/19 09:57,31/Mar/19 02:30,13/Jul/23 08:46,30/Mar/19 05:01,2.4.0,,,,,,,,2.4.2,3.0.0,,,Block Manager,Spark Core,,,,0,,,,,,"I use pyspark  like that

```

from pyspark.storagelevel import StorageLevel
df=spark.sql(""select * from xzn.person"")
df.persist(StorageLevel(False, True, False, False))
df.count()

```

table person is a simple table stored as orc files and some orc files is empty. When I run the query, it throw the error : 

```

19/03/22 21:46:31 INFO MemoryStore:54 - Block rdd_2_1 stored as values in memory (estimated size 0.0 B, free 1662.6 MB)
19/03/22 21:46:31 INFO FileScanRDD:54 - Reading File path: viewfs://name/xzn.db/person/part-00011, range: 0-49, partition values: [empty row]
19/03/22 21:46:31 INFO FileScanRDD:54 - Reading File path: viewfs://name/xzn.db/person/part-00011_copy_1, range: 0-49, partition values: [empty row]
19/03/22 21:46:31 INFO FileScanRDD:54 - Reading File path: viewfs://name/xzn.db/person/part-00012, range: 0-49, partition values: [empty row]
19/03/22 21:46:31 INFO FileScanRDD:54 - Reading File path: viewfs://name/xzn.db/person/part-00012_copy_1, range: 0-49, partition values: [empty row]
19/03/22 21:46:31 INFO FileScanRDD:54 - Reading File path: viewfs://name/xzn.db/person/part-00013, range: 0-49, partition values: [empty row]
19/03/22 21:46:31 ERROR Executor:91 - Exception in task 1.0 in stage 0.0 (TID 1)
org.xerial.snappy.SnappyIOException: [EMPTY_INPUT] Cannot decompress empty stream
 at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:94)
 at org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:59)
 at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:164)
 at org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:163)
 at org.apache.spark.serializer.SerializerManager.dataDeserializeStream(SerializerManager.scala:209)
 at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:596)
 at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:886)
 at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
 at org.apache.spark.scheduler.Task.run(Task.scala:121)
 at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
 at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
```

After I search it, I find that 1.1.7.x  snappy-java 's behavior is different from 1.1.2.x (that  spark 2.0.2 use this version). SnappyOutputStream in 1.1.2.x version always writes a snappy header whether or not to write a value,  but  SnappyOutputStream in 1.1.7.x don't generate header if u don't write value into it, so in spark 2.4 if RDD cache a empty value, memoryStore will not cache any bytes ( no snappy header ),  then it will throw the empty error. 

 

Maybe we can change SnappyOutputStream to fix it in 1.1.7.x snappy-java, there is my SnappyOutputStream method compressInput code 

```

protected void compressInput()
 throws IOException
 {
 // generate header 
 if (!headerWritten) {
 outputCursor = writeHeader();
 headerWritten = true;
 }

 if (inputCursor <= 0) {
 return; // no need to dump
 }

// if (!headerWritten) {
// outputCursor = writeHeader();
// headerWritten = true;
// }

 // Compress and dump the buffer content
 if (!hasSufficientOutputBufferFor(inputCursor)) {
 dumpOutput();
 }

 writeBlockPreemble();

 int compressedSize = Snappy.compress(inputBuffer, 0, inputCursor, outputBuffer, outputCursor + 4);
 // Write compressed data size
 writeInt(outputBuffer, outputCursor, compressedSize);
 outputCursor += 4 + compressedSize;
 inputCursor = 0;
 }

```

 

 

 

 ","spark.rdd.compress=true

spark.io.compression.codec =snappy

spark 2.4 in hadoop 2.6 with hive",dongjoon,findepi,max2049,taroleo,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 30 04:55:45 UTC 2019,,,,,,,,,,"0|z0122w:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,"26/Mar/19 01:54;max2049;I has committed this fix to snappy-java ([https://github.com/xerial/snappy-java)] .After merge the commit, maybe spark should update its snappy-java. This is the commit [https://github.com/xerial/snappy-java/pull/229]  ;;;","26/Mar/19 02:21;yumwang;[~max2049] (y);;;","26/Mar/19 04:39;taroleo;Just released snappy-java 1.1.7.3 with this hot fix. https://github.com/xerial/snappy-java;;;","26/Mar/19 05:34;yumwang;Thank you [~taroleo].

@[~max2049] Could you create a PR to upgrade snappy-java to 1.1.7.3?;;;","29/Mar/19 18:08;srowen;Good call, looks like it has a Java 9+ fix too, which we may need eventually. There are no other changes that would even affect Spark, so seems safe.;;;","29/Mar/19 18:33;taroleo;[~srowen] Actually this java9 support is for SnappyFramedStream, which is not used in Spark. I'm now testing snappy-java using jdk11 [https://github.com/xerial/snappy-java/pull/230];;;","29/Mar/19 18:52;srowen;That's fine, can't hurt, but won't do anything now.;;;","30/Mar/19 04:55;max2049;[~yumwang] Sorry for this late reply. [~srowen] has create a PR ( [https://github.com/apache/spark/pull/24242]  ) to fix it.  Thank you. ;;;",,,,,,,,,,,,,,
Allow setting -1 as split size for InputFileBlock,SPARK-27259,13223523,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,praneetsharma,Simon_poortman@icloud.com,Simon_poortman@icloud.com,23/Mar/19 17:35,12/Dec/22 18:10,13/Jul/23 08:46,16/Oct/19 07:25,2.2.1,2.2.2,2.2.3,2.3.0,2.3.1,2.3.2,2.3.3,2.4.0,3.0.0,,,,Spark Core,,,,,0,,,,,," 

From spark 2.2.x versions, when spark job processing any compressed HDFS files with custom input file format then spark jobs are failing with error ""java.lang.IllegalArgumentException: requirement failed: length (-1) cannot be negative"", the custom input file format will return the number of bytes length value as -1 for compressed file formats due to the compressed HDFS file are non splitable, so for compressed input file format the split will be offset as 0 and number of bytes length as -1, spark should consider the bytes length value -1 as valid split for the compressed file formats.

 

We observed that earlier versions of spark doesn’t have this validation, and found that from spark 2.2.x new validation got introduced in the class InputFileBlockHolder, so spark should accept the number of bytes length value -1 as valid length for input splits from spark 2.2.x as well.

 

+Below is the stack trace.+

 Caused by: java.lang.IllegalArgumentException: requirement failed: length (-1) cannot be negative

  at scala.Predef$.require(Predef.scala:224)

  at org.apache.spark.rdd.InputFileBlockHolder$.set(InputFileBlockHolder.scala:70)

  at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:226)

  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:214)

  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)

  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)

  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)

  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)

  at org.apache.spark.scheduler.Task.run(Task.scala:109)

  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)

  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)

  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)

  at java.lang.Thread.run(Thread.java:748)

 

+Below is the code snippet which caused this issue.+

   **    {color:#ff0000}require(length >= 0, s""length ($length) cannot be negative""){color} // This validation caused the issue. 

 
{code:java}
// code placeholder

 org.apache.spark.rdd.InputFileBlockHolder - spark-core

 

def set(filePath: String, startOffset: Long, length: Long): Unit = {

    require(filePath != null, ""filePath cannot be null"")

    require(startOffset >= 0, s""startOffset ($startOffset) cannot be negative"")

    require(length >= 0, s""length ($length) cannot be negative"")  

    inputBlock.set(new FileBlock(UTF8String.fromString(filePath), startOffset, length))

  }
{code}
 

+Steps to reproduce the issue.+

 Please refer the below code to reproduce the issue.  
{code:java}
// code placeholder

import org.apache.hadoop.mapred.JobConf

val hadoopConf = new JobConf()

import org.apache.hadoop.mapred.FileInputFormat

import org.apache.hadoop.fs.Path

FileInputFormat.setInputPaths(hadoopConf, new Path(""/output656/part-r-00000.gz""))    

val records = sc.hadoopRDD(hadoopConf,classOf[com.platform.custom.storagehandler.INFAInputFormat], classOf[org.apache.hadoop.io.LongWritable], classOf[org.apache.hadoop.io.Writable]) 

records.count()
{code}
 ",,dongjoon,praneetsharma,rkinthali,Simon_poortman@icloud.com,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27239,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 16 07:25:23 UTC 2019,,,,,,,,,,"0|z010vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/19 01:38;gurwls223;Please avoid to set Ciritical+ which is usually reserved for committers.;;;","15/Oct/19 05:07;praneetsharma;Submitted a PR for this issue: [https://github.com/apache/spark/pull/26123];;;","15/Oct/19 17:48;dongjoon;Hi, [~praneetsharma]. Thank you for making a JIRA, but we cannot reproduce your problem due to the following.
{code}
com.platform.custom.storagehandler.INFAInputFormat
{code};;;","16/Oct/19 07:25;praneetsharma;Fixed with [https://github.com/apache/spark/pull/26123];;;",,,,,,,,,,,,,,,,,,
SparkSession clone discards SQLConf overrides in favor of SparkConf defaults,SPARK-27253,13223443,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chakravarthi,joseph.torres,joseph.torres,22/Mar/19 20:55,12/Dec/22 18:11,13/Jul/23 08:46,01/Apr/19 00:33,2.4.0,,,,,,,,3.0.0,,,,Structured Streaming,,,,,0,,,,,,"SparkSession.cloneSession() is normally supposed to create a child session which inherits all the SQLConf values of its parent session. But when a SQL conf is given a global default through the SparkConf, this does not happen; the child session will receive the SparkConf default rather than its parent's SQLConf override.

 

This is particularly impactful in structured streaming, as the microbatches run in a cloned child session.",,chakravarthi,gsomogyi,joseph.torres,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 01 00:33:52 UTC 2019,,,,,,,,,,"0|z010ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/19 08:35;chakravarthi;Thanks for reporting ,will be working on this .;;;","01/Apr/19 00:33;gurwls223;Issue resolved by pull request 24189
[https://github.com/apache/spark/pull/24189];;;",,,,,,,,,,,,,,,,,,,,
@volatile var cannot be defined in case class in Scala 2.11,SPARK-27251,13223433,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,jzhuge,jzhuge,22/Mar/19 20:12,24/Mar/19 05:55,13/Jul/23 08:46,24/Mar/19 05:55,3.0.0,,,,,,,,3.0.0,,,,Build,SQL,,,,0,,,,,,"https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7-ubuntu-scala-2.11/507/consoleFull

{noformat}
[info] Compiling 371 Scala sources and 102 Java sources to /home/jenkins/workspace/spark-master-test-maven-hadoop-2.7-ubuntu-scala-2.11/sql/core/target/scala-2.11/classes...
[error] /home/jenkins/workspace/spark-master-test-maven-hadoop-2.7-ubuntu-scala-2.11/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala:162: values cannot be volatile
[error]     @volatile var statsOfPlanToCache: Statistics)
{noformat}
",,dongjoon,jzhuge,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25196,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 24 05:55:00 UTC 2019,,,,,,,,,,"0|z010bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/19 00:47;maropu;@volatile var cannot be defined in case class in Scala 2.11: https://github.com/scala/scala/pull/5294

I'm fixing in https://github.com/apache/spark/pull/24178;;;","24/Mar/19 05:55;dongjoon;This is resolved via https://github.com/apache/spark/pull/24178;;;",,,,,,,,,,,,,,,,,,,,
Scala 2.11 maven compile should target Java 1.8,SPARK-27250,13223429,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jzhuge,jzhuge,jzhuge,22/Mar/19 19:59,24/Mar/19 14:08,13/Jul/23 08:46,24/Mar/19 14:05,3.0.0,,,,,,,,3.0.0,,,,Build,,,,,0,,,,,,"Discovered by https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7-ubuntu-scala-2.11/509/console:
{noformat}
[error] /home/jenkins/workspace/spark-master-test-maven-hadoop-2.7-ubuntu-scala-2.11/sql/catalyst/src/main/scala/org/apache/spark/sql/catalog/v2/LookupCatalog.scala:40: Static methods in interface require -target:jvm-1.8
 [error] (None, Identifier.of(Array.empty, name))
 [error] ^
 [error] /home/jenkins/workspace/spark-master-test-maven-hadoop-2.7-ubuntu-scala-2.11/sql/catalyst/src/main/scala/org/apache/spark/sql/catalog/v2/LookupCatalog.scala:44: Static methods in interface require -target:jvm-1.8
 [error] (Some(catalog), Identifier.of(tail.init.toArray, tail.last))
 [error] ^
 [error] /home/jenkins/workspace/spark-master-test-maven-hadoop-2.7-ubuntu-scala-2.11/sql/catalyst/src/main/scala/org/apache/spark/sql/catalog/v2/LookupCatalog.scala:47: Static methods in interface require -target:jvm-1.8
 [error] (None, Identifier.of(parts.init.toArray, parts.last))
 [error] ^
 [error] three errors found
 [error] Compile failed at Mar 22, 2019 2:10:52 AM [42.688s]
{noformat}",,jzhuge,Simon_poortman@icloud.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 24 14:05:52 UTC 2019,,,,,,,,,,"0|z010ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/19 14:05;srowen;Issue resolved by pull request 24184
[https://github.com/apache/spark/pull/24184];;;",,,,,,,,,,,,,,,,,,,,,
REFRESH TABLE should recreate cache with same cache name and storage level,SPARK-27248,13223394,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william1104,william1104,william1104,22/Mar/19 16:41,12/Dec/22 18:10,13/Jul/23 08:46,21/May/19 18:44,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"If we refresh a cached table, the table cache will be first uncached and then recache (lazily). Currently, the logic is embedded in CatalogImpl.refreshTable method.

The current implementation does not preserve the cache name and storage level. As a result, cache name and cache level could be changed after a REFERSH. IMHO, it is not what a user would expect.

I would like to fix this behavior by first save the cache name and storage level for recaching the table.",,dongjoon,maropu,william1104,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27062,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 21 18:44:46 UTC 2019,,,,,,,,,,"0|z01034:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/19 01:52;gurwls223;[~william1104], did you make a PR? I think I saw but looks I can't find the PR;;;","26/Mar/19 13:35;william1104;https://github.com/apache/spark/pull/24221

Hi @Hyukjin, just created a PR. Hope it is good enough. If not, please let me know and I will fix it. Many thanks. Best regards, William;;;","21/May/19 18:44;dongjoon;This is resolved via https://github.com/apache/spark/pull/24221;;;",,,,,,,,,,,,,,,,,,,
scalar  subquery with no columns throws exception in spark.shell,SPARK-27246,13223319,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sandeep.katta2007,chakravarthi,chakravarthi,22/Mar/19 12:07,12/Dec/22 18:10,13/Jul/23 08:46,26/Mar/19 00:27,2.3.2,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"scala> val exp = ScalarSubquery(LocalRelation()).as('a)
java.util.NoSuchElementException: next on empty iterator
  at scala.collection.Iterator$$anon$2.next(Iterator.scala:39)
  at scala.collection.Iterator$$anon$2.next(Iterator.scala:37)
  at scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63)
  at scala.collection.IterableLike$class.head(IterableLike.scala:107)
  at scala.collection.mutable.ArrayOps$ofRef.scala$collection$IndexedSeqOptimized$$super$head(ArrayOps.scala:186)
  at scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:126)
  at scala.collection.mutable.ArrayOps$ofRef.head(ArrayOps.scala:186)
  at org.apache.spark.sql.catalyst.expressions.ScalarSubquery.dataType(subquery.scala:251)
  at org.apache.spark.sql.catalyst.expressions.Alias.dataType(namedExpressions.scala:147)
  at org.apache.spark.sql.catalyst.expressions.NamedExpression$class.typeSuffix(namedExpressions.scala:89)
  at org.apache.spark.sql.catalyst.expressions.Alias.typeSuffix(namedExpressions.scala:129)
  at org.apache.spark.sql.catalyst.expressions.Alias.toString(namedExpressions.scala:176)
  at scala.runtime.ScalaRunTime$.scala$runtime$ScalaRunTime$$inner$1(ScalaRunTime.scala:332)
  at scala.runtime.ScalaRunTime$.stringOf(ScalaRunTime.scala:337)
  at scala.runtime.ScalaRunTime$.replStringOf(ScalaRunTime.scala:345)
  at .$print$lzycompute(<console>:10)
  at .$print(<console>:6)
  at $print(<console>)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:786)
  at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047)
  at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638)
  at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637)
  at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
  at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
  at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:637)
  at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:569)
  at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:565)
  at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:807)
  at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:681)
  at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:395)
  at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:415)
  at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:923)
  at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)
  at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)
  at scala.reflect.internal.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:97)
  at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:909)
  at org.apache.spark.repl.Main$.doMain(Main.scala:76)
  at org.apache.spark.repl.Main$.main(Main.scala:56)
  at org.apache.spark.repl.Main.main(Main.scala)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
  at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:925)
  at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:201)
  at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:231)
  at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:140)
  at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
",,chakravarthi,Simon_poortman@icloud.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 26 03:59:32 UTC 2019,,,,,,,,,,"0|z00zmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/19 00:27;gurwls223;Fixed in https://github.com/apache/spark/pull/24182;;;","26/Mar/19 03:53;sandeep.katta2007;[~hyukjin.kwon]I think by mistake assignee name is incorrect, it should assigned to me;;;","26/Mar/19 03:59;gurwls223;Thanks.;;;",,,,,,,,,,,,,,,,,,,
Redact Passwords While Using Option logConf=true,SPARK-27244,13223303,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,needna78,needna78,needna78,22/Mar/19 10:49,31/Mar/19 07:08,13/Jul/23 08:46,29/Mar/19 19:27,2.3.0,2.3.2,2.3.3,2.4.0,,,,,2.3.4,2.4.2,3.0.0,,Spark Core,,,,,0,,,,,,"When logConf is set to true and any sensitive information like passwords is set using spark-submit command then the passwords are logged in clear text from conf. Ideally, these passwords should be redacted as it does on UI and then printed to log for debugging purpose.",,chakravarthi,kabhwan,needna78,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 29 19:27:00 UTC 2019,,,,,,,,,,"0|z00ziw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/19 12:12;chakravarthi;I will try to analyze this.;;;","22/Mar/19 12:39;kabhwan;[~chakravarthi] Are you working on it, or plan to work on it? Because I recently dealt with similar task and looks like it's just couple of lines change, so if you didn't start I'd like to just make it done.;;;","22/Mar/19 15:49;chakravarthi;[~kabhwan] working on this,will submit a PR soon.;;;","23/Mar/19 01:24;needna78;[~kabhwan] [~chakravarthi]

I already have the code changes, please let me submit the PR, this will be my first contribution to Spark.;;;","23/Mar/19 05:41;chakravarthi;[~needna78] ok;;;","23/Mar/19 05:44;kabhwan;[~needna78] Sure, go ahead. That's awesome you're starting your first contribution to Spark :);;;","29/Mar/19 19:27;srowen;Issue resolved by pull request 24196
[https://github.com/apache/spark/pull/24196];;;",,,,,,,,,,,,,,,
RuleExecutor throws exception when dumping time spent with no rule executed,SPARK-27243,13223295,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,mgaido,mgaido,mgaido,22/Mar/19 10:06,12/Dec/22 18:10,13/Jul/23 08:46,23/Mar/19 00:49,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"RuleExecutor can dump the time spent in the analyzer/optimizer rules. When no rule is executed or the RuleExecutor has just been reset this results in an exception, rather than returning an empty summary, which should be the result of this operation.",,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 23 00:49:50 UTC 2019,,,,,,,,,,"0|z00zh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/19 00:49;gurwls223;Issue resolved by pull request 24180
[https://github.com/apache/spark/pull/24180];;;",,,,,,,,,,,,,,,,,,,,,
Continuous Streaming does not support python UDFs,SPARK-27234,13223156,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,mhamilton,mhamilton,21/Mar/19 17:58,12/Dec/22 18:11,13/Jul/23 08:46,24/Jul/19 01:00,2.4.0,,,,,,,,2.4.4,3.0.0,,,Structured Streaming,,,,,1,,,,,,"Heres a repro:
{code:java}
from pyspark.sql.functions import col, udf
fooUDF = udf(lambda p: ""foo"")

spark \
.readStream \
.format(""rate"") \
.load()\
.withColumn(""foo"", fooUDF(col(""value"")))\
.writeStream\
.format(""console"")\
.trigger(continuous=""1 second"").start() {code}
Error Message (All that Azure Databricks prints):

 
{code:java}
at org.apache.spark.sql.execution.streaming.continuous.WriteToContinuousDataSourceExec.doExecute(WriteToContinuousDataSourceExec.scala:62) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:143) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:183) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:180) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:131) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:114) at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution$$anonfun$runContinuous$3$$anonfun$apply$1.apply(ContinuousExecution.scala:273) at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution$$anonfun$runContinuous$3$$anonfun$apply$1.apply(ContinuousExecution.scala:269) at org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:92) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:233) at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:86) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:163) at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution$$anonfun$runContinuous$3.apply(ContinuousExecution.scala:269) at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution$$anonfun$runContinuous$3.apply(ContinuousExecution.scala:269) at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:251) at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:61) at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution.runContinuous(ContinuousExecution.scala:267) at org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution.runActivatedStream(ContinuousExecution.scala:93) at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:295) at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:205) Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 4 times, most recent failure: Lost task 4.3 in stage 0.0 (TID 9, 10.139.64.4, executor 0): org.apache.spark.sql.execution.streaming.continuous.ContinuousTaskRetryException: Continuous execution does not support task retry at org.apache.spark.sql.execution.streaming.continuous.ContinuousDataSourceRDD.compute(ContinuousDataSourceRDD.scala:68) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340) at org.apache.spark.rdd.RDD.iterator(RDD.scala:304) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340) at org.apache.spark.rdd.RDD.iterator(RDD.scala:304) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340) at org.apache.spark.rdd.RDD.iterator(RDD.scala:304) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340) at org.apache.spark.rdd.RDD.iterator(RDD.scala:304) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60) at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD$$anonfun$compute$1.apply$mcV$sp(ContinuousWriteRDD.scala:52) at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD$$anonfun$compute$1.apply(ContinuousWriteRDD.scala:51) at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD$$anonfun$compute$1.apply(ContinuousWriteRDD.scala:51) at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1466) at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD.compute(ContinuousWriteRDD.scala:76) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340) at org.apache.spark.rdd.RDD.iterator(RDD.scala:304) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.doRunTask(Task.scala:139) at org.apache.spark.scheduler.Task.run(Task.scala:112) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:496) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1432) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:502) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2098) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2086) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2085) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2085) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1076) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1076) at scala.Option.foreach(Option.scala:257) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1076) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2317) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2265) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2253) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:873) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2251) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2273) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2292) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2317) at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:961) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:379) at org.apache.spark.rdd.RDD.collect(RDD.scala:960) at org.apache.spark.sql.execution.streaming.continuous.WriteToContinuousDataSourceExec.doExecute(WriteToContinuousDataSourceExec.scala:53) ... 22 more Caused by: org.apache.spark.sql.execution.streaming.continuous.ContinuousTaskRetryException: Continuous execution does not support task retry at org.apache.spark.sql.execution.streaming.continuous.ContinuousDataSourceRDD.compute(ContinuousDataSourceRDD.scala:68) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340) at org.apache.spark.rdd.RDD.iterator(RDD.scala:304) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340) at org.apache.spark.rdd.RDD.iterator(RDD.scala:304) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340) at org.apache.spark.rdd.RDD.iterator(RDD.scala:304) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340) at org.apache.spark.rdd.RDD.iterator(RDD.scala:304) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60) at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD$$anonfun$compute$1.apply$mcV$sp(ContinuousWriteRDD.scala:52) at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD$$anonfun$compute$1.apply(ContinuousWriteRDD.scala:51) at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD$$anonfun$compute$1.apply(ContinuousWriteRDD.scala:51) at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1466) at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD.compute(ContinuousWriteRDD.scala:76) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340) at org.apache.spark.rdd.RDD.iterator(RDD.scala:304) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.doRunTask(Task.scala:139) at org.apache.spark.scheduler.Task.run(Task.scala:112) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:496) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1432) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:502) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)
{code}
 ",Azure Databricks 5.1,kabhwan,mhamilton,venkirao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 24 01:00:06 UTC 2019,,,,,,,,,,"0|z00ymg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/19 02:55;gurwls223;What's {{makeReply}} and output from the codes?;;;","22/Mar/19 03:27;kabhwan;Could you please provide reproducer for Apache vanilla version of Spark? Is it still broken if you remove ""makeReply"" from the code?

If it's specific to ""makeReply"", you may have to contact Azure / Databricks support.;;;","25/Mar/19 01:43;gurwls223;ping [~mhamilton];;;","25/Mar/19 15:47;mhamilton;Thank you for the quick reply [~hyukjin.kwon] and [~kabhwan]. Sorry for those leftovers in the code, I updated the repro to remove those pieces and make it smaller. Also added the error message;;;","25/Mar/19 23:49;gurwls223;Cool, thanks :D;;;","04/Apr/19 14:43;venkirao;FWIW i had a similar issue with python UDF not working w/ Continuous Streaming, except for my test, my kafka output topic didnt get any data; but neither did i see any errors. Simply switching the mode to regular microbatch processing, allowed the data to flow through.;;;","24/Jul/19 01:00;gurwls223;Issue resolved by pull request 24946
[https://github.com/apache/spark/pull/24946];;;",,,,,,,,,,,,,,,
当我在hive2.3.4中创建一个表后，然后使用pyspark调用hivecontext无法使用表中数据，同时报错,SPARK-27230,13223092,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,Vitamin_C,Vitamin_C,21/Mar/19 14:40,21/Mar/19 15:36,13/Jul/23 08:46,21/Mar/19 15:35,2.3.3,,,,,,,,,,,,PySpark,SQL,,,,0,,,,,,"我在hive中建表mdw.t_sd_mobile_user_log

然后使用pyspark执行查询

from pyspark.sql import HiveContext
 sq= HiveContext(sc)
 sq.sql('show databases').show()
 sq.sql('use mdw').show()
 sq.sql('show tables').show()
 sq.sql('select * from mdw.t_sd_mobile_user_log').show()

然后报错

Traceback (most recent call last): File ""/usr/local/spark-2.3.3/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 350:undefined, in show print(self._jdf.showString(n, 20, vertical)) File ""/usr/local/spark-2.3.3/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__ answer, self.gateway_client, self.target_id, self.name) File ""/usr/local/spark-2.3.3/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco return f(*a, **kw) File ""/usr/local/spark-2.3.3/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py"", line 328, in get_return_value format(target_id, ""."", name), value) Py4JJavaError: An error occurred while calling o280.showString. : java.lang.AssertionError: assertion failed: No plan for HiveTableRelation `mdw`.`t_sd_mobile_user_log`, org.apache.hadoop.hive.serde2.OpenCSVSerde, [imei#272, start_time#273, end_time#274, type1#275, jizhan_num#276, platform#277, app_type#278, app_name#279, sz_ll#280, xz_ll#281|#272, start_time#273, end_time#274, type1#275, jizhan_num#276, platform#277, app_type#278, app_name#279, sz_ll#280, xz_ll#281], [statis_day#282|#282] at scala.Predef$.assert(Predef.scala:170) at 

 

 

但是 我在spark-sql中建表然后可以在pyspark中以同样的方式，可以读取。","ubuntu 16.04

hadoop-2.8.5

spark-2.3.3

hive-2.3.4",Vitamin_C,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,"hive执行建表语句
CREATE TABLE IF NOT EXISTS mdw.t_sd_mobile_user_log(
    imei string,
    start_time string,
    end_time string,
    type1 string,
    jizhan_num string,
    platform int,
    app_type string,
    app_name string,
    sz_ll int,
    xz_ll int
    )
    partitioned by (statis_day int)
pyspark 调用hivecontext查询报错
Traceback (most recent call last): File ""/usr/local/spark-2.3.3/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 350:undefined, in show print(self._jdf.showString(n, 20, vertical)) File ""/usr/local/spark-2.3.3/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__ answer, self.gateway_client, self.target_id, self.name) File ""/usr/local/spark-2.3.3/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco return f(*a, **kw) File ""/usr/local/spark-2.3.3/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py"", line 328, in get_return_value format(target_id, ""."", name), value) Py4JJavaError: An error occurred while calling o280.showString. : java.lang.AssertionError: assertion failed: No plan for HiveTableRelation `mdw`.`t_sd_mobile_user_log`, org.apache.hadoop.hive.serde2.OpenCSVSerde, [imei#272, start_time#273, end_time#274, type1#275, jizhan_num#276, platform#277, app_type#278, app_name#279, sz_ll#280, xz_ll#281], [statis_day#282] at scala.Predef$.assert(Predef.scala:170) at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93) at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:78) at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:75) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336) at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:75) at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:67) at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93) at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:78) at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:75) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336) at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:75) at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:67) at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93) at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:78) at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:75) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336) at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:75) at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:67) at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93) at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72) at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68) at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77) at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3254) at org.apache.spark.sql.Dataset.head(Dataset.scala:2489) at org.apache.spark.sql.Dataset.take(Dataset.scala:2703) at org.apache.spark.sql.Dataset.showString(Dataset.scala:254) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:282) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:238) at java.lang.Thread.run(Thread.java:748) 

spark-sql 执行的建表语句
hive执行建表语句
CREATE TABLE IF NOT EXISTS mdw.t_sd_mobile_user_log_2(
    imei string,
    start_time string,
    end_time string,
    type1 string,
    jizhan_num string,
    platform int,
    app_type string,
    app_name string,
    sz_ll int,
    xz_ll int
    )
    partitioned by (statis_day int)
pyspark调用hivecontext查询不报错",false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 21 15:35:03 UTC 2019,,,,,,,,,,"0|z00y88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/19 15:24;sandeep.katta2007;can you please explain the problem in english ?;;;","21/Mar/19 15:29;Vitamin_C;https://issues.apache.org/jira/browse/SPARK-27230?focusedCommentId=16798187&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16798187

I have found the cause of the problem, which is not caused by spark, but Hue. The problem will occur when the parameter is passed.;;;","21/Mar/19 15:31;Vitamin_C;This is my first question, I want to know how to cancel this question;;;","21/Mar/19 15:33;sandeep.katta2007;just click on resolve issue and close it as invalid;;;","21/Mar/19 15:35;Vitamin_C;https://issues.apache.org/jira/browse/SPARK-27230?focusedCommentId=16798201&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16798201

thanks

 ;;;",,,,,,,,,,,,,,,,,
Remove private methods that skip conversion when passing user schemas for constructing a DataFrame,SPARK-27223,13222945,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maryannxue,maryannxue,maryannxue,20/Mar/19 21:12,12/Dec/22 18:10,13/Jul/23 08:46,21/Mar/19 02:14,2.4.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"When passing in a user schema to create a DataFrame, there might be mismatched nullability between the user schema and the the actual data. All related public interfaces now perform catalyst conversion using the user provided schema, which catches such mismatches to avoid runtime errors later on. However, there're private methods which allow this conversion to be skipped, so we need to remove these private methods which may lead to confusion and potential issues.",,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 21 02:14:04 UTC 2019,,,,,,,,,,"0|z00xbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/19 02:14;gurwls223;Issue resolved by pull request 24162
[https://github.com/apache/spark/pull/24162];;;",,,,,,,,,,,,,,,,,,,,,
Upgrade RoaringBitmap to 0.7.45 to fix Kryo unsafe ser/dser issue,SPARK-27216,13222836,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,20/Mar/19 13:07,16/Oct/21 15:43,13/Jul/23 08:46,04/Apr/19 01:11,2.0.0,2.1.0,2.2.0,2.3.3,2.4.0,3.0.0,,,2.3.4,2.4.2,3.0.0,,Spark Core,,,,,0,correctness,,,,,"HighlyCompressedMapStatus uses RoaringBitmap to record the empty blocks. But RoaringBitmap-0.5.11 couldn't be ser/deser with unsafe KryoSerializer.

We can use below UT to reproduce:
{code}
  test(""kryo serialization with RoaringBitmap"") {
    val bitmap = new RoaringBitmap
    bitmap.add(1787)

    val safeSer = new KryoSerializer(conf).newInstance()
    val bitmap2 : RoaringBitmap = safeSer.deserialize(safeSer.serialize(bitmap))
    assert(bitmap2.equals(bitmap))

    conf.set(""spark.kryo.unsafe"", ""true"")
    val unsafeSer = new KryoSerializer(conf).newInstance()
    val bitmap3 : RoaringBitmap = unsafeSer.deserialize(unsafeSer.serialize(bitmap))
    assert(bitmap3.equals(bitmap)) // this will fail
  }
{code}
Upgrade to latest version 0.7.45 to fix it",,aallen,cltlfcjin,irashid,joshrosen,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27530,,,,,,,SPARK-23178,,,SPARK-24160,SPARK-27367,SPARK-27530,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 25 06:44:14 UTC 2019,,,,,,,,,,"0|z00wnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/19 23:38;aallen;Thanks for this bug report, [~cltlfcjin]. We we running into ""FetchFailedException: Received a zero-size buffer for block shuffle_0_0_332 from BlockManagerId"" and this bug report gave us enough of the hint to try {{config.set(""spark.kryo.unsafe"", ""false"")}} which worked-around the issue.;;;","01/Apr/19 14:23;cltlfcjin;Thanks for reporting this [~aallen]. It's a bug of RoaringBitmap-0.5.11. I've filed PR #24264 to fix it. I believe it could be worked-around by upgrading the RoaringBitmap jar to latest version manually if you still need spark.kryo.unsafe=true;;;","01/Apr/19 20:54;irashid;Any chance you know what the issue in roaring bitmap is?  Just wondering as it is a pretty large version bump.

BTW, 0.7.45 also seems to have some big performance improvements (https://github.com/RoaringBitmap/RoaringBitmap/pull/320) for deserialization, it might be worth considering changing spark to take advantage of that (definitely a separate issue, might not even matter for the way spark uses roaring bitmap anyway ...);;;","02/Apr/19 04:18;cltlfcjin;[~irashid] Not yet. It's petty challenging to find out it without JIRA This fixing is much important than performance since it causes job failed (2.4 and above) or data quality problem (2.1~2.3 AFAIK) when using unsafe KryoSerializer. I don't know which version is the proper one, so I simply use latest version. Anyway, Spark is not a heavy user on this lib. Why not replace it is due to compatibility consideration.;;;","02/Apr/19 13:15;irashid;I think its probably fine to upgrade it, I just wanted to check if we knew what the issue was.

Though spark only uses this library in one spot, its a pretty important one -- managing MapStatus is a major source of pressure on the driver on large clusters.;;;","04/Apr/19 01:11;irashid;Fixed by https://github.com/apache/spark/pull/24264 in master / 3.0.0;;;","25/Apr/19 06:38;joshrosen;I've added the {{correctness}} label to this ticket because it sounds like it can cause query correctness issues in pre-2.4 versions of Spark (for example, I think SPARK-23178 might be a report of this).

Note for future readers: this problem only occurs in a non-default configuration (the default is {{spark.kryo.unsafe == false}}).

/cc [~smilegator];;;","25/Apr/19 06:44;smilegator;Thanks, Josh! We will add it to the release note. [~cloud_fan];;;",,,,,,,,,,,,,,
Correct the kryo configurations,SPARK-27215,13222832,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,20/Mar/19 12:51,20/Mar/19 21:29,13/Jul/23 08:46,20/Mar/19 21:27,3.0.0,,,,,,,,3.0.0,,,,Spark Core,,,,,0,,,,,,"{code}
  val KRYO_USE_UNSAFE = ConfigBuilder(""spark.kyro.unsafe"")
    .booleanConf
    .createWithDefault(false)

  val KRYO_USE_POOL = ConfigBuilder(""spark.kyro.pool"")
    .booleanConf
    .createWithDefault(true)
{code}
kyro should be kryo",,cltlfcjin,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 20 21:27:28 UTC 2019,,,,,,,,,,"0|z00wmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/19 21:27;vanzin;Issue resolved by pull request 24156
[https://github.com/apache/spark/pull/24156];;;",,,,,,,,,,,,,,,,,,,,,
spark-shell with packages option fails to load transitive dependencies even ivy successfully pulls jars,SPARK-27205,13222592,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,19/Mar/19 14:09,25/Mar/19 00:08,13/Jul/23 08:46,20/Mar/19 22:56,3.0.0,,,,,,,,3.0.0,,,,Spark Core,,,,,0,,,,,,"I found this bug while testing my patch regarding Spark SQL Kafka module - I tend to open spark-shell and link kafka module via `–packages`.

When we run
{code:java}
./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0{code}
we should be able to import ""org.apache.kafka"" in spark-shell, but it doesn't work for current master branch.

There's not enough evidence as well as I have no idea what's happening here even with `–verbose` option, so I had to spend couple of hours dealing with git bisect.

Turned out the commit introducing the bug was SPARK-26977 ([81dd21fda99da48ed76adb739a07d1dabf1ffb51|https://github.com/apache/spark/commit/81dd21fda99da48ed76adb739a07d1dabf1ffb51]).

 ",,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 20 22:56:31 UTC 2019,,,,,,,,,,"0|z00v54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/19 14:09;kabhwan;I have a patch now: it's not just a rollback of commit. Will submit it shortly.;;;","20/Mar/19 22:56;srowen;Issue resolved by pull request 24147
[https://github.com/apache/spark/pull/24147];;;",,,,,,,,,,,,,,,,,,,,
History Environment tab must sort Configurations/Properties by default,SPARK-27200,13222532,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ajithshetty,ajithshetty,ajithshetty,19/Mar/19 10:05,12/Dec/22 18:10,13/Jul/23 08:46,20/Mar/19 11:17,3.0.0,,,,,,,,3.0.0,,,,Web UI,,,,,0,,,,,,"Environment Page in SparkUI have all the configuration sorted by key. But this is not the case in History server case, to keep UX same, we can have it sorted in history server too",,ajithshetty,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 20 11:17:23 UTC 2019,,,,,,,,,,"0|z00urs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/19 11:17;gurwls223;Issue resolved by pull request 24143
[https://github.com/apache/spark/pull/24143];;;",,,,,,,,,,,,,,,,,,,,,
Heartbeat interval mismatch in driver and executor,SPARK-27198,13222482,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ajithshetty,ajithshetty,ajithshetty,19/Mar/19 04:33,21/Apr/19 22:37,13/Jul/23 08:46,25/Mar/19 20:38,2.3.3,2.4.0,,,,,,,2.4.1,,,,Spark Core,,,,,0,release-notes,,,,,"When heartbeat interval is configured via *spark.executor.heartbeatInterval* without specifying units, we have time mismatched between driver(considers in seconds) and executor(considers as milliseconds)

 [https://github.com/apache/spark/blob/v2.4.1-rc8/core/src/main/scala/org/apache/spark/SparkConf.scala#L613]

vs

[https://github.com/apache/spark/blob/v2.4.1-rc8/core/src/main/scala/org/apache/spark/executor/Executor.scala#L858]

 

 ",,ajithshetty,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27419,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 31 13:47:14 UTC 2019,,,,,,,,,,"0|z00ugo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/19 04:33;ajithshetty;will be working on this;;;","25/Mar/19 20:38;srowen;Issue resolved by pull request 24140
[https://github.com/apache/spark/pull/24140];;;","31/Mar/19 06:19;smilegator;[~srowen][~dbtsai] This requires a release note since it changes the behavior. ;;;","31/Mar/19 13:47;srowen;[~smilegator] see PR -- I am not sure it needs a release note as I don't believe (working) behavior changed. I proposed text there if you feel strongly about it, which can just be added to the Docs text here.;;;",,,,,,,,,,,,,,,,,,
Job failures when task attempts do not clean up spark-staging parquet files,SPARK-27194,13222381,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,duripeng,rezasafi,rezasafi,18/Mar/19 17:43,25/Nov/20 14:40,13/Jul/23 08:46,25/Nov/20 12:50,2.3.1,2.3.2,2.3.3,,,,,,3.1.0,,,,Spark Core,SQL,,,,3,,,,,,"When a container fails for some reason (for example when killed by yarn for exceeding memory limits), the subsequent task attempts for the tasks that were running on that container all fail with a FileAlreadyExistsException. The original task attempt does not seem to successfully call abortTask (or at least its ""best effort"" delete is unsuccessful) and clean up the parquet file it was writing to, so when later task attempts try to write to the same spark-staging directory using the same file name, the job fails.

Here is what transpires in the logs:

The container where task 200.0 is running is killed and the task is lost:
{code}
19/02/20 09:33:25 ERROR cluster.YarnClusterScheduler: Lost executor y on t.y.z.com: Container killed by YARN for exceeding memory limits. 8.1 GB of 8 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
 19/02/20 09:33:25 WARN scheduler.TaskSetManager: Lost task 200.0 in stage 0.0 (TID xxx, t.y.z.com, executor 93): ExecutorLostFailure (executor 93 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 8.1 GB of 8 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
{code}

The task is re-attempted on a different executor and fails because the part-00200-blah-blah.c000.snappy.parquet file from the first task attempt already exists:
{code}
19/02/20 09:35:01 WARN scheduler.TaskSetManager: Lost task 200.1 in stage 0.0 (TID 594, tn.y.z.com, executor 70): org.apache.spark.SparkException: Task failed while writing rows.
 at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
 at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
 at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
 at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
 at org.apache.spark.scheduler.Task.run(Task.scala:109)
 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 at java.lang.Thread.run(Thread.java:745)
 Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: /user/hive/warehouse/tmp_supply_feb1/.spark-staging-blah-blah-blah/dt=2019-02-17/part-00200-blah-blah.c000.snappy.parquet for client a.b.c.d already exists
{code}

The job fails when the the configured task attempts (spark.task.maxFailures) have failed with the same error:
{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 200 in stage 0.0 failed 20 times, most recent failure: Lost task 284.19 in stage 0.0 (TID yyy, tm.y.z.com, executor 16): org.apache.spark.SparkException: Task failed while writing rows.
 at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
 ...
 Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: /user/hive/warehouse/tmp_supply_feb1/.spark-staging-blah-blah-blah/dt=2019-02-17/part-00200-blah-blah.c000.snappy.parquet for client i.p.a.d already exists
{code}

SPARK-26682 wasn't the root cause here, since there wasn't any stage reattempt.

This issue seems to happen when spark.sql.sources.partitionOverwriteMode=dynamic. 

 ",,ajithshetty,apachespark,cloud_fan,dongjoon,elkhand,hzfeiwang,jonathak,koert,kzhao,LennonChin,mauzhang,NathanKan,ramkrish1489,rezasafi,viirya,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,PARQUET-1615,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 25 12:50:56 UTC 2020,,,,,,,,,,"0|z00tu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/19 02:37;ajithshetty;Currently looks like from logs the file name for task 200.0 and 200.1(reattempt) expected file name to be, part-00200-blah-blah.c000.snappy.parquet. (refer org.apache.spark.internal.io.HadoopMapReduceCommitProtocol#getFilename)

May be we should have taskId_attemptId in the part file name so that rerun tasks do not conflict with older failed tasks.

cc [~srowen] [~cloud_fan] [~dongjoon] any thoughts.?;;;","19/Mar/19 19:06;dongjoon;Hi, [~ajithshetty]. Just out of curiosity, according to the `Affected Versions`, do you observe this in 2.3.1 and 2.3.2? Is it possible to use 2.3.3 in your cluster since that's already released?;;;","20/Mar/19 14:33;ajithshetty;[~dongjoon] Yes i tried with spark 2.3.3, and the issue persist. Here is the operation i performed
{code:java}
spark.sql.sources.partitionOverwriteMode=DYNAMIC{code}
{code:java}
create table t1 (i int, part1 int, part2 int) using parquet partitioned by (part1, part2)
insert into t1 partition(part1=1, part2=1) select 1
insert overwrite table t1 partition(part1=1, part2=1) select 2
insert overwrite table t1 partition(part1=2, part2) select 2, 2   // here the exec is killed and task respawns{code}
and here is the full stacktrace as per 2.3.3
{code:java}
2019-03-20 19:58:06 WARN TaskSetManager:66 - Lost task 0.1 in stage 2.0 (TID 3, QWERTY, executor 2): org.apache.spark.SparkException: Task failed while writing rows.
at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
at org.apache.spark.scheduler.Task.run(Task.scala:109)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: /user/hive/warehouse/t2/.spark-staging-1f1efbfd-7e20-4e0f-a49c-a7fa3eae4cb1/part1=2/part2=2/part-00000-1f1efbfd-7e20-4e0f-a49c-a7fa3eae4cb1.c000.snappy.parquet for client 127.0.0.1 already exists
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2578)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2465)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2349)
at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:624)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:398)
at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2213)

at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1653)
at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1689)
at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1624)
at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:448)
at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:444)
at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:459)
at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:387)
at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:236)
at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask.org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(FileFormatWriter.scala:511)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask$$anonfun$execute$5.apply(FileFormatWriter.scala:546)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask$$anonfun$execute$5.apply(FileFormatWriter.scala:527)
at scala.collection.Iterator$class.foreach(Iterator.scala:893)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask.execute(FileFormatWriter.scala:527)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1415)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
... 8 more
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.FileAlreadyExistsException): /user/hive/warehouse/t2/.spark-staging-1f1efbfd-7e20-4e0f-a49c-a7fa3eae4cb1/part1=2/part2=2/part-00000-1f1efbfd-7e20-4e0f-a49c-a7fa3eae4cb1.c000.snappy.parquet for client 127.0.0.1 already exists
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2578)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2465)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2349)
at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:624)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:398)
at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2213)

at org.apache.hadoop.ipc.Client.call(Client.java:1475)
at org.apache.hadoop.ipc.Client.call(Client.java:1412)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
at com.sun.proxy.$Proxy15.create(Unknown Source)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:296)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
at com.sun.proxy.$Proxy16.create(Unknown Source)
at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1648)
... 32 more
{code}
 

 

 ;;;","20/Mar/19 14:54;ajithshetty;Hi [~dongjoon] , i have some analysis here : [https://github.com/apache/spark/pull/24142#issuecomment-474866759] Please let me know your views;;;","20/Mar/19 22:55;dongjoon;Thank you for checking that, [~ajithshetty].;;;","18/Nov/19 15:42;ramkrish1489;[~ajithshetty] is this being worked upon;;;","03/Jul/20 03:06;apachespark;User 'turboFei' has created a pull request for this issue:
https://github.com/apache/spark/pull/28989;;;","03/Jul/20 03:07;apachespark;User 'turboFei' has created a pull request for this issue:
https://github.com/apache/spark/pull/28989;;;","05/Jul/20 20:02;apachespark;User 'WinkerDu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29000;;;","27/Jul/20 11:43;apachespark;User 'WinkerDu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29260;;;","25/Nov/20 12:50;cloud_fan;Issue resolved by pull request 29000
[https://github.com/apache/spark/pull/29000];;;",,,,,,,,,,,
k8s test failing due to missing nss library in dockerfile,SPARK-27178,13222015,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shaneknapp,shaneknapp,shaneknapp,15/Mar/19 22:31,17/May/20 18:24,13/Jul/23 08:46,18/Mar/19 23:52,2.4.0,3.0.0,,,,,,,2.4.1,3.0.0,,,Build,jenkins,Kubernetes,Spark Core,,0,,,,,,"while performing some tests on our existing minikube and k8s infrastructure, i noticed that the integration tests were failing.  i dug in and discovered the following message buried at the end of the stacktrace:


{noformat}
  Caused by: java.io.FileNotFoundException: /usr/lib/libnss3.so
  	at sun.security.pkcs11.Secmod.initialize(Secmod.java:193)
  	at sun.security.pkcs11.SunPKCS11.<init>(SunPKCS11.java:218)
  	... 81 more
{noformat}

after i added the 'nss' package to resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile, everything worked.

i will also check and see if this is failing on 2.4...

tbh, i have no idea why this literally started failing today and not earlier.  the only recent change to this file that i can find is https://issues.apache.org/jira/browse/SPARK-26995",,apachespark,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 18 23:54:13 UTC 2019,,,,,,,,,,"0|z00rkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/19 23:01;shaneknapp;argh!  this also looks like it's failing on 2.4...  from my local testing (full stacktrace, pertinent error at the very end):


{noformat}
19/03/15 22:58:32 INFO SparkContext: Added JAR file:///opt/spark/examples/jars/spark-examples_2.11-2.4.2-SNAPSHOT.jar at spark://spark-test-app-1552690707070-driver-svc.900b14edc7d244ad99957f813e680625.svc:7078/jars/spark-examples_2.11-2.4.2-SNAPSHOT.jar with timestamp 1552690712498
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at sun.security.ssl.SSLSessionImpl.<init>(SSLSessionImpl.java:188)
	at sun.security.ssl.SSLSessionImpl.<init>(SSLSessionImpl.java:152)
	at sun.security.ssl.SSLSessionImpl.<clinit>(SSLSessionImpl.java:79)
	at sun.security.ssl.SSLSocketImpl.init(SSLSocketImpl.java:598)
	at sun.security.ssl.SSLSocketImpl.<init>(SSLSocketImpl.java:566)
	at sun.security.ssl.SSLSocketFactoryImpl.createSocket(SSLSocketFactoryImpl.java:110)
	at okhttp3.internal.connection.RealConnection.connectTls(RealConnection.java:270)
	at okhttp3.internal.connection.RealConnection.establishProtocol(RealConnection.java:251)
	at okhttp3.internal.connection.RealConnection.connect(RealConnection.java:151)
	at okhttp3.internal.connection.StreamAllocation.findConnection(StreamAllocation.java:195)
	at okhttp3.internal.connection.StreamAllocation.findHealthyConnection(StreamAllocation.java:121)
	at okhttp3.internal.connection.StreamAllocation.newStream(StreamAllocation.java:100)
	at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:42)
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)
	at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93)
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)
	at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
	at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:120)
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)
	at io.fabric8.kubernetes.client.utils.BackwardsCompatibilityInterceptor.intercept(BackwardsCompatibilityInterceptor.java:119)
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)
	at io.fabric8.kubernetes.client.utils.ImpersonatorInterceptor.intercept(ImpersonatorInterceptor.java:68)
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)
	at io.fabric8.kubernetes.client.utils.HttpClientUtils$2.intercept(HttpClientUtils.java:107)
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)
	at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:185)
	at okhttp3.RealCall.execute(RealCall.java:69)
	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:379)
	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:344)
	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleGet(OperationSupport.java:313)
	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleGet(OperationSupport.java:296)
	at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleGet(BaseOperation.java:801)
	at io.fabric8.kubernetes.client.dsl.base.BaseOperation.getMandatory(BaseOperation.java:218)
	at io.fabric8.kubernetes.client.dsl.base.BaseOperation.get(BaseOperation.java:185)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator$$anonfun$1.apply(ExecutorPodsAllocator.scala:57)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator$$anonfun$1.apply(ExecutorPodsAllocator.scala:55)
	at scala.Option.map(Option.scala:146)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.<init>(ExecutorPodsAllocator.scala:55)
	at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterManager.createSchedulerBackend(KubernetesClusterManager.scala:89)
	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2788)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:493)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:935)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:926)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)
	at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:31)
	at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.security.ProviderException: Could not initialize NSS
	at sun.security.pkcs11.SunPKCS11.<init>(SunPKCS11.java:223)
	at sun.security.pkcs11.SunPKCS11.<init>(SunPKCS11.java:103)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at sun.security.jca.ProviderConfig$2.run(ProviderConfig.java:224)
	at sun.security.jca.ProviderConfig$2.run(ProviderConfig.java:206)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.security.jca.ProviderConfig.doLoadProvider(ProviderConfig.java:206)
	at sun.security.jca.ProviderConfig.getProvider(ProviderConfig.java:187)
	at sun.security.jca.ProviderList.getProvider(ProviderList.java:233)
	at sun.security.jca.ProviderList.getIndex(ProviderList.java:263)
	at sun.security.jca.ProviderList.getProviderConfig(ProviderList.java:247)
	at sun.security.jca.ProviderList.getProvider(ProviderList.java:253)
	at java.security.Security.getProvider(Security.java:503)
	at sun.security.ssl.SignatureAndHashAlgorithm.<clinit>(SignatureAndHashAlgorithm.java:415)
	... 67 more
Caused by: java.io.FileNotFoundException: /usr/lib/libnss3.so
	at sun.security.pkcs11.Secmod.initialize(Secmod.java:193)
	at sun.security.pkcs11.SunPKCS11.<init>(SunPKCS11.java:218)
	... 83 more
{noformat}

(╯°□°)╯︵ ┻━┻;;;","18/Mar/19 21:37;apachespark;User 'shaneknapp' has created a pull request for this issue:
https://github.com/apache/spark/pull/24137;;;","18/Mar/19 23:44;shaneknapp;https://github.com/apache/spark/pull/24111 merged to master.  holding off on the 2.4 fix.;;;","18/Mar/19 23:54;shaneknapp;[~vanzin] just merged https://github.com/apache/spark/pull/24137

we should be g2g for now.  i'll create a new jira to discuss the potential pinning of these dockerfiles to a specific image version.;;;",,,,,,,,,,,,,,,,,,
Update jenkins locale to en_US.UTF-8,SPARK-27177,13221930,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shaneknapp,yumwang,yumwang,15/Mar/19 15:15,12/Dec/22 18:10,13/Jul/23 08:46,16/Oct/19 17:42,3.0.0,,,,,,,,3.0.0,,,,Build,jenkins,,,,0,,,,,,"Two test cases will failed on our jenkins since HADOOP-12045(Hadoop-2.8.0). I'd like to update our jenkins locale to en_US.UTF-8 to workaround this issue.
 How to reproduce:
{code:java}
export LANG=
git clone https://github.com/apache/spark.git && cd spark && git checkout v2.4.0
build/sbt ""hive/testOnly *.HiveDDLSuite"" -Phive -Phadoop-2.7 -Dhadoop.version=2.8.0
{code}
Stack trace:
{noformat}
Caused by: sbt.ForkMain$ForkError: java.nio.file.InvalidPathException: Malformed input or input contains unmappable characters: /home/jenkins/workspace/SparkPullRequestBuilder@2/target/tmp/warehouse-15474fdf-0808-40ab-946d-1309fb05bf26/DaTaBaSe_I.db/tab_ı
	at sun.nio.fs.UnixPath.encode(UnixPath.java:147)
	at sun.nio.fs.UnixPath.<init>(UnixPath.java:71)
	at sun.nio.fs.UnixFileSystem.getPath(UnixFileSystem.java:281)
	at java.io.File.toPath(File.java:2234)
	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getLastAccessTime(RawLocalFileSystem.java:683)
	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.<init>(RawLocalFileSystem.java:694)
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:664)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:454)
	at org.apache.hadoop.hive.metastore.Warehouse.isDir(Warehouse.java:520)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1436)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1503)
{noformat}
Workaround:
{code:java}
export LANG=en_US.UTF-8
build/sbt ""hive/testOnly *.HiveDDLSuite"" -Phive -Phadoop-2.7 -Dhadoop.version=2.8.0
{code}
More details: 
https://issues.apache.org/jira/browse/HADOOP-16180
https://github.com/apache/spark/pull/24044/commits/4c1ec25d3bc64bf358edf1380a7c863596722362",,dongjoon,shaneknapp,toopt4,yumwang,,,,,,,,,,,,,,,,,,,,,,,SPARK-27262,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 16 23:02:43 UTC 2019,,,,,,,,,,"0|z00r20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/19 15:19;yumwang;cc [~shaneknapp] [~srowen];;;","15/Mar/19 15:27;srowen;Seems OK to me.;;;","15/Mar/19 16:10;shaneknapp;are we proposing to do this for all spark builds, PRB only, or PRB + sbt builds?;;;","15/Mar/19 23:42;yumwang;I’d prefer to PRB + sbt builds.;;;","21/Mar/19 00:49;gurwls223;Yes, I think we can unblock https://github.com/apache/spark/pull/23823 as well ;;;","21/Mar/19 18:02;shaneknapp;done for the PRB.  will get to the SBT builds later today or tomorrow.;;;","22/Mar/19 13:00;yumwang;It works. Thank you [~shaneknapp];;;","25/Mar/19 18:05;shaneknapp;SBT builds to be updated in the next day or so...;;;","23/Jun/19 19:14;shaneknapp;i guess we're needing this now for maven builds?  see:  https://issues.apache.org/jira/browse/SPARK-28114?focusedCommentId=16870381&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16870381;;;","24/Jun/19 00:47;gurwls223;Yea, looks like it;;;","22/Sep/19 02:20;dongjoon;Hi, All.
I moved this out from SPARK-23710 since this is not related to `Hadoop-3.2` profile.;;;","16/Oct/19 17:42;shaneknapp;this is done!;;;","16/Oct/19 23:02;dongjoon;Oh. Thanks, [~shaneknapp]!;;;",,,,,,,,,
Upgrade Apache ORC to 1.5.5,SPARK-27165,13221765,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,14/Mar/19 22:20,23/Mar/19 17:40,13/Jul/23 08:46,15/Mar/19 03:15,2.4.1,3.0.0,,,,,,,2.4.1,3.0.0,,,Build,,,,,0,,,,,,"This issue aims to update Apache ORC dependency to fix SPARK-27107 .
{code:java}
[ORC-452] Support converting MAP column from JSON to ORC
Improvement
[ORC-447] Change the docker scripts to keep a persistent m2 cache
[ORC-463] Add `version` command
[ORC-475] ORC reader should lazily get filesystem
[ORC-476] Make SearchAgument kryo buffer size configurable{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27107,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 15 03:15:52 UTC 2019,,,,,,,,,,"0|z00q1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/19 03:15;dongjoon;This is resolved via [https://github.com/apache/spark/pull/24096] and [https://github.com/apache/spark/pull/24097] .;;;",,,,,,,,,,,,,,,,,,,,,
RDD.countApprox on empty RDDs schedules jobs which never complete ,SPARK-27164,13221758,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ajithshetty,rmoore,rmoore,14/Mar/19 20:47,17/Mar/19 17:59,13/Jul/23 08:46,17/Mar/19 17:57,2.2.3,2.4.0,,,,,,,3.0.0,,,,Spark Core,,,,,0,,,,,,"When calling `countApprox` on an RDD which has no partitions (such as those created by `sparkContext.emptyRDD`) a job is scheduled with 0 stages and 0 tasks. That job appears under the ""Active Jobs"" in the Spark UI until it is either killed or the Spark context is shut down.

 
{code:java}
Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 11.0.1)
Type in expressions to have them evaluated.
Type :help for more information.

scala> val ints = sc.makeRDD(Seq(1))
ints: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at makeRDD at <console>:24

scala> ints.countApprox(1000)
res0: org.apache.spark.partial.PartialResult[org.apache.spark.partial.BoundedDouble] = (final: [1.000, 1.000])
// PartialResult is returned, Scheduled job completed

scala> ints.filter(_ => false).countApprox(1000)
res1: org.apache.spark.partial.PartialResult[org.apache.spark.partial.BoundedDouble] = (final: [0.000, 0.000])
// PartialResult is returned, Scheduled job completed

scala> sc.emptyRDD[Int].countApprox(1000)
res5: org.apache.spark.partial.PartialResult[org.apache.spark.partial.BoundedDouble] = (final: [0.000, 0.000])
// PartialResult is returned, Scheduled job is ACTIVE but never completes

scala> sc.union(Nil : Seq[org.apache.spark.rdd.RDD[Int]]).countApprox(1000)
res16: org.apache.spark.partial.PartialResult[org.apache.spark.partial.BoundedDouble] = (final: [0.000, 0.000])
// PartialResult is returned, Scheduled job is ACTIVE but never completes


{code}","macOS, Spark-2.4.0 with Hadoop 2.7 running on Java 11.0.1

Also observed on:

macOS, Spark-2.2.3 with Hadoop 2.7 running on Java 1.8.0_151",ajithshetty,rmoore,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/19 20:49;rmoore;Screen Shot 2019-03-14 at 1.49.19 PM.png;https://issues.apache.org/jira/secure/attachment/12962533/Screen+Shot+2019-03-14+at+1.49.19+PM.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 17 17:57:02 UTC 2019,,,,,,,,,,"0|z00pzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/19 01:17;ajithshetty;i will be working on this;;;","17/Mar/19 17:57;srowen;Issue resolved by pull request 24100
[https://github.com/apache/spark/pull/24100];;;",,,,,,,,,,,,,,,,,,,,
Incorrect Literal Casting of DecimalType in OrcFilters,SPARK-27160,13221664,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sadhen,sadhen,sadhen,14/Mar/19 13:27,02/Mar/20 20:41,13/Jul/23 08:46,20/Mar/19 19:32,2.3.0,2.4.0,,,,,,,2.3.4,2.4.1,3.0.0,,SQL,,,,,0,correctness,,,,,"DecimalType Literal should not be casted to Long.

eg. For `df.filter(""x < 3.14"")`, assuming df (x in DecimalType) reads from a ORC table and uses the native ORC reader with predicate push down enabled, we will push down the `x < 3.14` predicate to the ORC reader via a SearchArgument.

OrcFilters will construct the SearchArgument, but not handle the DecimalType correctly.

The previous impl will construct `x < 3` from `x < 3.14`.",,dongjoon,sadhen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 20 03:38:02 UTC 2019,,,,,,,,,,"0|z00pf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/19 03:38;dongjoon;Hi, [~sadhen]. Thank you so much for this contribution. Next time, please don't set the `Fix Versions` and don't use `Blocker` priority. That is correct for this issue, but there is a guide line for contribution in Apache Spark community. In general, we had better respect the guideline.
- https://spark.apache.org/contributing.html;;;",,,,,,,,,,,,,,,,,,,,,
Update MsSqlServer dialect handling of BLOB type,SPARK-27159,13221637,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lipzhu,lipzhu,lipzhu,14/Mar/19 10:07,24/Jul/19 02:13,13/Jul/23 08:46,16/Mar/19 01:22,2.4.3,3.0.0,,,,,,,2.4.4,3.0.0,,,SQL,,,,,0,,,,,,"In the current Spark codes, the `MsSqlServerDialect` didn't handle correct for binary type, it use default blob type. According to the MSDN, the correct type should be varbinary(max) to replace the blob type.

!image-2019-03-14-18-06-48-842.png!",,lipzhu,Simon_poortman@icloud.com,,,,,,,,,,,,,,,,,,,,,,,SPARK-27168,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 16 01:22:21 UTC 2019,,,,,,,,,,"0|z00p94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/19 10:11;lipzhu;I am working on this.;;;","16/Mar/19 01:22;srowen;Issue resolved by pull request 24091
[https://github.com/apache/spark/pull/24091];;;",,,,,,,,,,,,,,,,,,,,
Docker Oracle XE image docker image has been removed by DockerHub ,SPARK-27155,13221580,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lipzhu,lipzhu,lipzhu,14/Mar/19 02:59,02/May/19 19:46,13/Jul/23 08:46,25/Mar/19 20:18,2.4.0,,,,,,,,3.0.0,,,,Build,,,,,0,,,,,,"Since 2019-Feb-13(the Valentine's day eve) this docker image has been removed by DockerHub due to the Docker DMCA Takedown Notice from the Copyright owner which is the Oracle.

 

!image-2019-03-14-11-00-05-498.png!",,apachespark,lipzhu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/19 03:00;lipzhu;image-2019-03-14-11-00-05-498.png;https://issues.apache.org/jira/secure/attachment/12962426/image-2019-03-14-11-00-05-498.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 25 20:18:04 UTC 2019,,,,,,,,,,"0|z00owg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/19 03:05;apachespark;User 'lipzhu' has created a pull request for this issue:
https://github.com/apache/spark/pull/24086;;;","14/Mar/19 03:05;apachespark;User 'lipzhu' has created a pull request for this issue:
https://github.com/apache/spark/pull/24086;;;","25/Mar/19 20:18;srowen;Issue resolved by pull request 24086
[https://github.com/apache/spark/pull/24086];;;",,,,,,,,,,,,,,,,,,,
array_distinct function does not work correctly with columns containing array of array,SPARK-27134,13221079,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,m1ke,m1ke,12/Mar/19 10:39,17/Jul/19 00:44,13/Jul/23 08:46,16/Mar/19 19:33,2.4.0,,,,,,,,2.4.1,3.0.0,,,SQL,,,,,0,correctness,,,,,"The array_distinct function introduced in spark 2.4 is producing strange results when used on an array column which contains a nested array. The resulting output can still contain duplicate values, and furthermore, previously distinct values may be removed.

This is easily repeatable, e.g. with this code:

val df = Seq(
 Seq(Seq(1, 2), Seq(1, 2), Seq(1, 2), Seq(3, 4), Seq(4, 5))
 ).toDF(""Number_Combinations"")

val dfWithDistinct = df.withColumn(""distinct_combinations"",
 array_distinct(col(""Number_Combinations"")))

 

The initial 'df' DataFrame contains one row, where column 'Number_Combinations' contains the following values:

[[1, 2], [1, 2], [1, 2], [3, 4], [4, 5]]

 

The array_distinct function run on this column produces a new column containing the following values:

[[1, 2], [1, 2], [1, 2]]

 

As you can see, this contains three occurrences of the same value (1, 2), and furthermore, the distinct values (3, 4), (4, 5) have been removed.

 

 ","Spark 2.4, scala 2.11.11",m1ke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23912,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 16 19:33:35 UTC 2019,,,,,,,,,,"0|z00lso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/19 19:33;srowen;Resolved by https://github.com/apache/spark/pull/24073;;;",,,,,,,,,,,,,,,,,,,,,
current_date/current_timestamp should be reserved keywords in ansi parser mode,SPARK-27117,13220622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,09/Mar/19 13:19,24/Jan/20 22:50,13/Jul/23 08:46,12/Mar/19 02:56,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,,,cloud_fan,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 24 22:50:46 UTC 2020,,,,,,,,,,"0|z00izs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/19 02:56;cloud_fan;Issue resolved by pull request 24039
[https://github.com/apache/spark/pull/24039];;;","24/Jan/20 22:50;rxin;I changed the title to make it more clear to end users what's happening.

 ;;;",,,,,,,,,,,,,,,,,,,,
Environment tab must sort Hadoop Configuration by default,SPARK-27116,13220617,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ajithshetty,ajithshetty,ajithshetty,09/Mar/19 11:43,11/Mar/19 13:46,13/Jul/23 08:46,11/Mar/19 13:44,3.0.0,,,,,,,,3.0.0,,,,Web UI,,,,,0,,,,,,"Environment tab in SparkUI do not have Hadoop Configuration sorted. All other tables in the same page like Spark Configrations, System Configuration etc are sorted by keys by default",,ajithshetty,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 11 13:44:04 UTC 2019,,,,,,,,,,"0|z00iyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/19 13:44;srowen;Issue resolved by pull request 24038
[https://github.com/apache/spark/pull/24038];;;",,,,,,,,,,,,,,,,,,,,,
Spark Scheduler encounters two independent Deadlocks when trying to kill executors either due to dynamic allocation or blacklisting ,SPARK-27112,13220569,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pgandhi,pgandhi,pgandhi,08/Mar/19 23:34,23/Mar/19 17:41,13/Jul/23 08:46,18/Mar/19 15:35,2.4.0,3.0.0,,,,,,,2.3.4,2.4.1,3.0.0,,Scheduler,Spark Core,,,,0,,,,,,"Recently, a few spark users in the organization have reported that their jobs were getting stuck. On further analysis, it was found out that there exist two independent deadlocks and either of them occur under different circumstances. The screenshots for these two deadlocks are attached here. 

We were able to reproduce the deadlocks with the following piece of code:

 
{code:java}
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}

import org.apache.spark._
import org.apache.spark.TaskContext

// Simple example of Word Count in Scala
object ScalaWordCount {
def main(args: Array[String]) {

if (args.length < 2) {
System.err.println(""Usage: ScalaWordCount <inputFilesURI> <outputFilesUri>"")
System.exit(1)
}

val conf = new SparkConf().setAppName(""Scala Word Count"")
val sc = new SparkContext(conf)

// get the input file uri
val inputFilesUri = args(0)

// get the output file uri
val outputFilesUri = args(1)

while (true) {
val textFile = sc.textFile(inputFilesUri)
val counts = textFile.flatMap(line => line.split("" ""))
.map(word => {if (TaskContext.get.partitionId == 5 && TaskContext.get.attemptNumber == 0) throw new Exception(""Fail for blacklisting"") else (word, 1)})
.reduceByKey(_ + _)
counts.saveAsTextFile(outputFilesUri)
val conf: Configuration = new Configuration()
val path: Path = new Path(outputFilesUri)
val hdfs: FileSystem = FileSystem.get(conf)
hdfs.delete(path, true)
}

sc.stop()
}
}
{code}
 

Additionally, to ensure that the deadlock surfaces up soon enough, I also added a small delay in the Spark code here:

[https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala#L256]

 
{code:java}
executorIdToFailureList.remove(exec)
updateNextExpiryTime()
Thread.sleep(2000)
killBlacklistedExecutor(exec)
{code}
 

Also make sure that the following configs are set when launching the above spark job:
*spark.blacklist.enabled=true*
*spark.blacklist.killBlacklistedExecutors=true*
*spark.blacklist.application.maxFailedTasksPerExecutor=1*",,ajithshetty,Dhruve Ashar,irashid,pgandhi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/19 23:36;pgandhi;Screen Shot 2019-02-26 at 4.10.26 PM.png;https://issues.apache.org/jira/secure/attachment/12961771/Screen+Shot+2019-02-26+at+4.10.26+PM.png","08/Mar/19 23:36;pgandhi;Screen Shot 2019-02-26 at 4.10.48 PM.png;https://issues.apache.org/jira/secure/attachment/12961772/Screen+Shot+2019-02-26+at+4.10.48+PM.png","08/Mar/19 23:36;pgandhi;Screen Shot 2019-02-26 at 4.11.11 PM.png;https://issues.apache.org/jira/secure/attachment/12961773/Screen+Shot+2019-02-26+at+4.11.11+PM.png","08/Mar/19 23:36;pgandhi;Screen Shot 2019-02-26 at 4.11.26 PM.png;https://issues.apache.org/jira/secure/attachment/12961774/Screen+Shot+2019-02-26+at+4.11.26+PM.png",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 20 16:24:12 UTC 2019,,,,,,,,,,"0|z00io0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/19 15:35;irashid;Fixed by https://github.com/apache/spark/pull/24072;;;","19/Mar/19 21:24;irashid;Fixed in 2.4 & 2.3 by https://github.com/apache/spark/pull/24134;;;","20/Mar/19 01:43;irashid;I revised the fix version for 2.4 to 2.4.2, as I just realized rc8 was cut before this was committed.  If there is another rc, we should adjust the fix version.;;;","20/Mar/19 15:09;Dhruve Ashar;[~irashid] - I think this is a critical bug and since it is resolved we should include it in the rc8.;;;","20/Mar/19 16:24;irashid;[~Dhruve Ashar] -- rc8 is already defined, there is nothing I (or anybody else) can do to change that.  I simply updated the jira to reflect that.  However, you might request that rc8 does not become 2.4.1, and instead we roll an rc9 with this this fix.  You should respond to the VOTE thread for rc8 on the dev list with your concerns, that's the right forum for this (thanks for the reminder btw, I will mention it there as well).;;;",,,,,,,,,,,,,,,,,
A continuous query may fail with InterruptedException when kafka consumer temporally 0 partitions temporally,SPARK-27111,13220568,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,08/Mar/19 23:30,23/Mar/19 17:41,13/Jul/23 08:46,09/Mar/19 23:22,2.3.0,2.3.1,2.3.2,2.3.3,2.4.0,2.4.1,,,2.3.4,2.4.1,3.0.0,,Structured Streaming,,,,,0,,,,,,"Before a Kafka consumer gets assigned with partitions, its offset will contain 0 partitions. However, runContinuous will still run and launch a Spark job having 0 partitions. In this case, there is a race that epoch may interrupt the query execution thread after `lastExecution.toRdd`, and either `epochEndpoint.askSync[Unit](StopContinuousExecutionWrites)` or the next `runContinuous` will get interrupted unintentionally.",,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-03-08 23:30:53.0,,,,,,,,,,"0|z00ins:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL Job failing because of Kryo buffer overflow with ORC,SPARK-27107,13220497,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,Dhruve Ashar,Dhruve Ashar,08/Mar/19 16:33,23/Mar/19 17:41,13/Jul/23 08:46,15/Mar/19 03:17,2.3.2,2.4.0,,,,,,,2.4.1,3.0.0,,,SQL,,,,,0,,,,,,"The issue occurs while trying to read ORC data and setting the SearchArgument.
{code:java}
 Caused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 9
Serialization trace:
literalList (org.apache.orc.storage.ql.io.sarg.SearchArgumentImpl$PredicateLeafImpl)
leaves (org.apache.orc.storage.ql.io.sarg.SearchArgumentImpl)
	at com.esotericsoftware.kryo.io.Output.require(Output.java:163)
	at com.esotericsoftware.kryo.io.Output.writeVarLong(Output.java:614)
	at com.esotericsoftware.kryo.io.Output.writeLong(Output.java:538)
	at com.esotericsoftware.kryo.serializers.DefaultSerializers$LongSerializer.write(DefaultSerializers.java:147)
	at com.esotericsoftware.kryo.serializers.DefaultSerializers$LongSerializer.write(DefaultSerializers.java:141)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552)
	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552)
	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:534)
	at org.apache.orc.mapred.OrcInputFormat.setSearchArgument(OrcInputFormat.java:96)
	at org.apache.orc.mapreduce.OrcInputFormat.setSearchArgument(OrcInputFormat.java:57)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(OrcFileFormat.scala:159)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(OrcFileFormat.scala:156)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.buildReaderWithPartitionValues(OrcFileFormat.scala:156)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:297)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:295)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:315)
	at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:121)
	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:605)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.python.EvalPythonExec.doExecute(EvalPythonExec.scala:89)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:371)
	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:150)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:605)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	... 52 more
{code}
This happens only with the new apache orc based implementation and doesn't happen with the hive based implementation. 

 

Reason:

Hive implementation (1.2) sets the default buffer size to 4K (edit: corrected from 4M to 4K) and max buffer size to 10M.

[https://github.com/apache/hive/blob/branch-1.2/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java#L998]

Orc implementation on the other hand, sets the size to 100K.
 [https://github.com/apache/orc/blob/master/java/mapreduce/src/java/org/apache/orc/mapred/OrcInputFormat.java#L93]
  

We need to fix this in the ORC library and update the version in spark to resolve the issue.

 ",,apachespark,Dhruve Ashar,dongjoon,roczei,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ORC-476,SPARK-27165,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 15 03:17:27 UTC 2019,,,,,,,,,,"0|z00i80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/19 16:37;Dhruve Ashar;[~dongjoon] can you review the PR for ORC to fix this issue? [https://github.com/apache/orc/pull/372]

Once this is merged, we can fix the issue in spark as well. Until then the only workaround is to use the hive based implementation.;;;","10/Mar/19 23:46;dongjoon;Hi, [~Dhruve Ashar]. Could you provide a reproducible test case here?
BTW, the Hive default is 4K instead of 4M, isn't it?;;;","10/Mar/19 23:51;dongjoon;Technically, Hive implementation also fails when it exceeds the limitation because it's a non-configurable parameter issue.
bq. This happens only with the new apache orc based implementation and doesn't happen with the hive based implementation. ;;;","11/Mar/19 14:57;Dhruve Ashar;This is something we can consistently reproduce every single time. I do not have an example with company details stripped off that I can share for this use case at this point. Is there anything specific that you are looking for? I can try to come up with a reproducible case, but it seems to be difficult to reproduce as this is dependent on user query and the parameters that are being passed to filter the data.
{quote} Could you provide a reproducible test case here?
{quote}
The Hive default is 4K and not 4M (that was a typo). Thanks for correcting that.
{quote}BTW, the Hive default is 4K instead of 4M, isn't it?
{quote}
 Yes. The hive implementation should fail when it exceeds the 10M limit for a SArg and the PR that I have against the Orc implementation tries to make this configurable so that spark can control the buffer size if we hit a buffer overflow error.
{quote}Technically, Hive implementation also fails when it exceeds the limitation because it's a non-configurable parameter issue.
{quote};;;","12/Mar/19 14:33;Dhruve Ashar;Update: The PR was merged in the orc repository. My understanding is that we should update our pom once a new orc release is cut out.;;;","12/Mar/19 15:45;dongjoon;Yep. [~Dhruve Ashar]. I already tested and voted for that ORC RC vote yesterday. I'll do if the vote passes and the artifacts are uploaded. And, we need a test case in Spark side. Without a test case, it's just a dependency upgrade. Also, users can simply replace their ORC jar files without waiting Spark releases. Apache Spark is not a fat assembly jar file for this reason.;;;","12/Mar/19 16:44;dongjoon;[~Dhruve Ashar]. Please vote on ORC RC1 after testing your environment. :);;;","12/Mar/19 19:33;Dhruve Ashar;I verified the changes and we are no longer seeing the issue. Thanks for testing+voting the ORC RC. I think I am not on the ORC mailing list, so I might have missed the voting. ;;;","12/Mar/19 22:50;dongjoon;Thank you for confirmation, [~Dhruve Ashar].;;;","14/Mar/19 22:17;dongjoon;The vote passed. I'm preparing the PRs.;;;","14/Mar/19 22:24;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/24096;;;","14/Mar/19 22:32;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/24097;;;","15/Mar/19 03:17;dongjoon;This is resolved via [https://github.com/apache/spark/pull/24096] and [https://github.com/apache/spark/pull/24097] .;;;",,,,,,,,,
Use `Array` instead of `Seq` in `FilePartition` to prevent StackOverflowError,SPARK-27100,13220370,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,parthc,KaiXu,KaiXu,08/Mar/19 03:21,12/Dec/22 18:10,13/Jul/23 08:46,26/Jun/19 07:49,2.1.3,2.3.3,,,,,,,2.4.4,,,,SQL,,,,,0,,,,,,"ALS in Spark MLlib causes StackOverflow:

 /opt/sparkml/spark213/bin/spark-submit  --properties-file /opt/HiBench/report/als/spark/conf/sparkbench/spark.conf --class com.intel.hibench.sparkbench.ml.ALSExample --master yarn-client --num-executors 3 --executor-memory 322g /opt/HiBench/sparkbench/assembly/target/sparkbench-assembly-7.1-SNAPSHOT-dist.jar --numUsers 40000 --numProducts 60000 --rank 100 --numRecommends 20 --numIterations 100 --kryo false --implicitPrefs true --numProductBlocks -1 --numUserBlocks -1 --lambda 1.0 hdfs://bdw-slave20:8020/HiBench/ALS/Input

 

Exception in thread ""dag-scheduler-event-loop"" java.lang.StackOverflowError
 at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1534)
 at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
 at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
 at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
 at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
 at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
 at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
 at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
 at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
 at scala.collection.immutable.List$SerializationProxy.writeObject(List.scala:468)
 at sun.reflect.GeneratedMethodAccessor27.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
 at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
 at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
 at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
 at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
 at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
 at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
 at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
 at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
 at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
 at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
 at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
 at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
 at scala.collection.immutable.List$SerializationProxy.writeObject(List.scala:468)
 at sun.reflect.GeneratedMethodAccessor27.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
 at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
 at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
 at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
 at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
 at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
 at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
 at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
 at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
 at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)",,apachespark,dbtsai,dongjoon,KaiXu,parthc,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4838,,,,"13/Jun/19 18:47;parthc;SPARK-27100-Overflow.txt;https://issues.apache.org/jira/secure/attachment/12971716/SPARK-27100-Overflow.txt","08/Mar/19 07:34;KaiXu;stderr;https://issues.apache.org/jira/secure/attachment/12961681/stderr",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 24 05:09:26 UTC 2019,,,,,,,,,,"0|z00hfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/19 04:47;yumwang;Could you try the latest Spark version please?;;;","08/Mar/19 06:02;KaiXu;Hi, [~yumwang], I tried Spark2.3.3, it also has the similar issue.

Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.StackOverflowError
java.lang.StackOverflowError
 at java.lang.Exception.<init>(Exception.java:102)
 at java.lang.ReflectiveOperationException.<init>(ReflectiveOperationException.java:89)
 at java.lang.reflect.InvocationTargetException.<init>(InvocationTargetException.java:72)
 at sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
 at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
 at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
 at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
 at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
 at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
 at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
 at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
 at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
 at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
 at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
 at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
 at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
 at scala.collection.immutable.List$SerializationProxy.writeObject(List.scala:468)
 at sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
 at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
 at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
 at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
 at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
 at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
 at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
 at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
 at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
 at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
 at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
 at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
 at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
 at scala.collection.immutable.List$SerializationProxy.writeObject(List.scala:468)
 at sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source);;;","08/Mar/19 07:27;KaiXu;I have checked stderr log file(50M:() of the task,  the stack trace is repeated as above but does not show any stack information about Spark, the issue seems exists since Spark1.1 ;;;","08/Mar/19 10:10;gurwls223;Hm, can you show reproducible steps including codes? I think it should be checked if there's the same issue in the current master branch or not.;;;","15/Mar/19 01:40;KaiXu;hi [~hyukjin.kwon], the workload I'm running is ALS from Hibench, the code can be obtained from [here|https://github.com/intel-hadoop/HiBench/blob/master/sparkbench/ml/src/main/scala/com/intel/sparkbench/ml/ALSExample.scala], and here is the [doc |https://github.com/intel-hadoop/HiBench/blob/master/docs/run-sparkbench.md] on how to build and run.

Steps to reproduce:
 # Follow above doc to config the Hibench based on your cluster.
 # Edit \{HIBENCH_HOME}/conf/benchmarks.lst, keep ml.als in this file to run ALS only.
 # Edit \{HIBENCH_HOME}/conf/hibench.conf, change the value of hibench.scale.profile to gigantic.
 # Edit \{HIBENCH_HOME}/conf/workloads/ml/al.conf, change the value of hibench.als.rank to 200, hibench.als.numIterations to 100
 # \{HIBENCH_HOME}/conf/run_all.sh, to start the test.
 # Wait to about 30 iterations, it will fail with StackOverflowError;;;","20/Mar/19 11:43;gurwls223;Would you be interested in narrowing down the problem, so that people can test it against the master branch?;;;","13/Jun/19 18:47;parthc;The stack overflow is due to serialization of a ShuffleMapTask (see attached file with complete stack) [^SPARK-27100-Overflow.txt] . 

ShuffleMapTask.partition is a FilePartition and FilePartition.files is a Stream which is essentially a linked list. It is therefore serialized recursively.
If the number of files in each partition is, say, 10000 files, recursing into a linked list of length 10000 causes a stack overflow. This is a general problem with serialization of Scala streams (and other collections that are lazily initialized) that is fixed in 2.13 (https://github.com/scala/scala/pull/6676). 

The problem is only in Bucketed partitions. The corresponding implementation for non Bucketed partitions uses a StreamBuffer. 
 
Partial expansion of ShuffleMapTask just before the stack overflow -

{code:java}
obj = \{ShuffleMapTask@16639} Method threw 'java.lang.StackOverflowError' exception. Cannot evaluate org.apache.spark.scheduler.ShuffleMapTask.toString()
 taskBinary = \{TorrentBroadcast@17216} Method threw 'java.lang.StackOverflowError' exception. Cannot evaluate org.apache.spark.broadcast.TorrentBroadcast.toString()
 partition = \{FilePartition@17217} Method threw 'java.lang.StackOverflowError' exception. Cannot evaluate org.apache.spark.sql.execution.datasources.FilePartition.toString()
 index = 0
 files = \{Stream$Cons@17244} Method threw 'java.lang.StackOverflowError' exception. Cannot evaluate scala.collection.immutable.Stream$Cons.toString()
 hd = \{PartitionedFile@17246} Method threw 'java.lang.StackOverflowError' exception. Cannot evaluate org.apache.spark.sql.execution.datasources.PartitionedFile.toString()
 partitionValues = \{GenericInternalRow@17259} Method threw 'java.lang.StackOverflowError' exception. Cannot evaluate org.apache.spark.sql.catalyst.expressions.GenericInternalRow.toString()
 filePath = ""hdfs://path/a.db/master/version=1/rangeid=000/part-00039-295e3ac1-760c-482e-8640-5e5d1539c2c9_00000.c000.gz.parquet""
 start = 0
 length = 225781388
 locations = \{String[3]@17261} 
 tlVal = \{Stream$Cons@16687} Method threw 'java.lang.StackOverflowError' exception. Cannot evaluate scala.collection.immutable.Stream$Cons.toString()
 hd = \{PartitionedFile@17249} Method threw 'java.lang.StackOverflowError' exception. Cannot evaluate org.apache.spark.sql.execution.datasources.PartitionedFile.toString()
 partitionValues = \{GenericInternalRow@17255} Method threw 'java.lang.StackOverflowError' exception. Cannot evaluate org.apache.spark.sql.catalyst.expressions.GenericInternalRow.toString()
 filePath = ""hdfs://path/a.db/master/version=1/rangeid=001/part-00061-0346437e-7f8f-44ac-8739-94d1ee285c0b_00000.c000.gz.parquet""
 start = 0
 length = 431239612
 locations = \{String[3]@17257} 
 tlVal = \{Stream$Cons@16812} Method threw 'java.lang.StackOverflowError' exception. Cannot evaluate scala.collection.immutable.Stream$Cons.toString()
 hd = \{PartitionedFile@17264} Method threw 'java.lang.StackOverflowError' exception. Cannot evaluate org.apache.spark.sql.execution.datasources.PartitionedFile.toString()
 partitionValues = \{GenericInternalRow@17268} Method threw 'java.lang.StackOverflowError' exception. Cannot evaluate org.apache.spark.sql.catalyst.expressions.GenericInternalRow.toString()
 filePath = ""hdfs://path/a.db/master/version=1/rangeid=002/part-00058-b3a99b18-140e-43ed-838e-276eaa45a5f3_00000.c000.gz.parquet""
 start = 0
 length = 219930113
 locations = \{String[3]@17270} 
 tlVal = \{Stream$Cons@17265} Method threw 'java.lang.StackOverflowError' exception. Cannot evaluate scala.collection.immutable.Stream$Cons.toString()
 hd = \{PartitionedFile@17273} Method threw 'java.lang.StackOverflowError' exception. Cannot evaluate org.apache.spark.sql.execution.datasources.PartitionedFile.toString()
 partitionValues = \{GenericInternalRow@17277} Method threw 'java.lang.StackOverflowError' exception. Cannot evaluate org.apache.spark.sql.catalyst.expressions.GenericInternalRow.toString()
 filePath = ""hdfs://path/a.db/master/version=1/rangeid=003/part-00051-58be3faa-0611-49de-8546-1656b6086934_00000.c000.gz.parquet""
 start = 0
 length = 219503110
 locations = \{String[3]@17279} 
 tlVal = \{Stream$Cons@17274} Method threw 'java.lang.StackOverflowError' exception. Cannot evaluate scala.collection.immutable.Stream$Cons.toString()
 tlGen = null
 tlGen = null
 tlGen = null
 tlGen = null
{code}
;;;","13/Jun/19 18:59;parthc;Opened a PR with a fix and a test to reproduce the issue. https://github.com/apache/spark/pull/24865.
Thanks to [~dbtsai] [~dongjoon] for offline help with this one. ;;;","25/Jun/19 04:05;apachespark;User 'parthchandra' has created a pull request for this issue:
https://github.com/apache/spark/pull/24957;;;","26/Jun/19 07:49;dbtsai;Issue resolved by pull request 24957
[https://github.com/apache/spark/pull/24957];;;","24/Jul/19 05:09;dongjoon;Hi, [~parthc]. I added you to the Apache Spark contributor group and assigned this issue to you.
Thank you for your contribution.;;;",,,,,,,,,,,
Avoid embedding platform-dependent offsets literally in whole-stage generated code,SPARK-27097,13220351,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,rednaxelafx,smilegator,smilegator,08/Mar/19 00:22,21/Mar/20 13:23,13/Jul/23 08:46,10/Mar/19 06:00,2.0.0,2.1.3,2.2.3,2.3.4,2.4.0,,,,2.4.1,,,,SQL,,,,,0,correctness,,,,,"Avoid embedding platform-dependent offsets literally in whole-stage generated code.

Spark SQL performs whole-stage code generation to speed up query execution. There are two steps to it:

Java source code is generated from the physical query plan on the driver. A single version of the source code is generated from a query plan, and sent to all executors.
It's compiled to bytecode on the driver to catch compilation errors before sending to executors, but currently only the generated source code gets sent to the executors. The bytecode compilation is for fail-fast only.
Executors receive the generated source code and compile to bytecode, then the query runs like a hand-written Java program.

In this model, there's an implicit assumption about the driver and executors being run on similar platforms. Some code paths accidentally embedded platform-dependent object layout information into the generated code, such as:


{code:java}
Platform.putLong(buffer, /* offset */ 24, /* value */ 1);

{code}
This code expects a field to be at offset +24 of the buffer object, and sets a value to that field.
But whole-stage code generation generally uses platform-dependent information from the driver. If the object layout is significantly different on the driver and executors, the generated code can be reading/writing to wrong offsets on the executors, causing all kinds of data corruption.

One code pattern that leads to such problem is the use of Platform.XXX constants in generated code, e.g. Platform.BYTE_ARRAY_OFFSET.

Bad:

{code:java}

val baseOffset = Platform.BYTE_ARRAY_OFFSET
// codegen template:
s""Platform.putLong($buffer, $baseOffset, $value);""
This will embed the value of Platform.BYTE_ARRAY_OFFSET on the driver into the generated code.
{code}


Good:

{code:java}

val baseOffset = ""Platform.BYTE_ARRAY_OFFSET""
// codegen template:
s""Platform.putLong($buffer, $baseOffset, $value);""
This will generate the offset symbolically -- Platform.putLong(buffer, Platform.BYTE_ARRAY_OFFSET, value), which will be able to pick up the correct value on the executors.
{code}


Caveat: these offset constants are declared as runtime-initialized static final in Java, so they're not compile-time constants from the Java language's perspective. It does lead to a slightly increased size of the generated code, but this is necessary for correctness.

NOTE: there can be other patterns that generate platform-dependent code on the driver which is invalid on the executors. e.g. if the endianness is different between the driver and the executors, and if some generated code makes strong assumption about endianness, it would also be problematic.

",,angerszhuuu,dbtsai,irashid,smilegator,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 21 13:23:24 UTC 2020,,,,,,,,,,"0|z00hbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/19 00:25;smilegator;The fix will be pushed soon. ;;;","08/Mar/19 00:25;smilegator;This is not a regression but a long-standing issue. ;;;","08/Mar/19 23:14;irashid;I'm kind of amazed Spark works at all on different Platforms.  As you note, endianness probably cannot be different.  What kind of platform difference results in this issue?  Is it different versions of the JVM?  I'd also be amazed if that worked properly.

I'm not saying we shouldn't fix this if its easy, but maybe we should clarify how different the ""platform"" can be between containers in a spark app?;;;","10/Mar/19 06:00;dbtsai;Issue resolved by pull request 24032
[https://github.com/apache/spark/pull/24032];;;","10/Mar/19 08:23;dbtsai;[~irashid] Initially, I was thinking this fix is for the platform difference of endianness. By looking at the test, this bug can happen when both executors and driver are x86, but `UseCompressedOops` is turned off in the executors to access more than 32GB of the heap while the driver uses the default JVM option with `UseCompressedOops` on with less memory. Thus, in driver, the references will be 32-bit in 64-bit JVM resulting different byte array offset. ;;;","21/Mar/20 13:23;angerszhuuu;[~irashid] to be honest, I meet this problem these days.

 

[~dbtsai] I have some question. 
We start a self-developed thrift server program  and use spark as compute engine with below javaOptions parameter
 
{color:#e14141}-Xmx64g {color}
{color:#e14141}-Djava.library.path=/home/hadoop/hadoop/lib/native {color}
{color:#e14141}-Djavax.security.auth.useSubjectCredsOnly=false {color}
{color:#e14141}-Dcom.sun.management.jmxremote.port=9021 {color}
{color:#e14141}-Dcom.sun.management.jmxremote.authenticate=false {color}
{color:#e14141}-Dcom.sun.management.jmxremote.ssl=false {color}
{color:#e14141}-XX:MaxPermSize=1024m -XX:PermSize=256m -XX:MaxDirectMemorySize=8192m -XX:-TraceClassUnloading {color}
{color:#e14141}-XX:+UseCompressedOops -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 -XX:+CMSParallelRemarkEnabled -XX:+DisableExplicitGC -XX:+PrintTenuringDistribution -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=75 -Xnoclassgc -XX:+PrintGCDetails -XX:+PrintGCDateStamps {color}
{color:#e14141} {color}
{color:#e14141} {color}
Then the {color:#347eec}Platform{color}{color:#e14141}.{color} {color:#347eec}BYTE_ARRAY_OFFSET{color} will be 24, when we start a normal spark thrift server, the value will be 16, this problem cause strange data corruption. 
After few days check, I located the problem because of spark  *codegen*， and  this pr can fix our problem , but I can’t find  evidence why 
Platform.BYTE_ARRAY_OFFSET will be 24 in above parameter. Since I test in local that when we set  {color:#e14141} -XX:+ UseCompressedOops,  {color} using pointer compression it's going to be 16.
{color:#e14141} -XX:- UseCompressedOops,  {color} not using pointer compression it's going to be 24. This is easy to understand why the offset is not same.
But I don’t know why above parameter will be 24 since I am not a professor  about java compiler and  Basic computer knowledge.
 
Can you give me some advisor or information about how to understand and find the root cause.
 ;;;",,,,,,,,,,,,,,,,
Reconcile the join type support between data frame and sql interface,SPARK-27096,13220350,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dkbiswal,dkbiswal,dkbiswal,08/Mar/19 00:21,16/Mar/19 04:55,13/Jul/23 08:46,11/Mar/19 07:12,2.4.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"Currently in the grammar file, we have the joinType rule defined as following :
{code:java}
joinType
 : INNER?
 ....
 ....
 | LEFT SEMI
 | LEFT? ANTI
 ;
{code:java}
 {code}
The keyword LEFT is optional for ANTI join even though its not optional for SEMI join. When
 using data frame interface join type ""anti"" is not allowed. The allowed types are ""left_anti"" or 
 ""leftanti"" for anti joins. We should also make LEFT optional for SEMI join and allow ""semi"" and ""anti"" join types from data frame.

 ",,apachespark,cloud_fan,dkbiswal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 11 07:12:45 UTC 2019,,,,,,,,,,"0|z00hbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/19 00:26;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/23982;;;","08/Mar/19 00:27;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/23982;;;","11/Mar/19 07:12;cloud_fan;Issue resolved by pull request 23982
[https://github.com/apache/spark/pull/23982];;;",,,,,,,,,,,,,,,,,,,
Thread interrupt being swallowed while launching executors in YarnAllocator,SPARK-27094,13220332,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,07/Mar/19 22:27,17/May/20 18:13,13/Jul/23 08:46,20/Mar/19 18:48,2.4.0,,,,,,,,2.4.1,3.0.0,,,Spark Core,YARN,,,,0,,,,,,"When shutting down a SparkContext, the YarnAllocator thread is interrupted. If the interrupt happens just at the wrong time, you'll see something like this:

{noformat}
19/03/05 07:04:20 WARN ScriptBasedMapping: Exception running blah
java.io.IOException: java.lang.InterruptedException
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:578)
	at org.apache.hadoop.util.Shell.run(Shell.java:478)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:766)
	at org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping.runResolveCommand(ScriptBasedMapping.java:251)
	at org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping.resolve(ScriptBasedMapping.java:188)
	at org.apache.hadoop.net.CachedDNSToSwitchMapping.resolve(CachedDNSToSwitchMapping.java:119)
	at org.apache.hadoop.yarn.util.RackResolver.coreResolve(RackResolver.java:101)
	at org.apache.hadoop.yarn.util.RackResolver.resolve(RackResolver.java:81)
	at org.apache.spark.deploy.yarn.SparkRackResolver.resolve(SparkRackResolver.scala:37)
	at org.apache.spark.deploy.yarn.YarnAllocator$$anonfun$handleAllocatedContainers$2.apply(YarnAllocator.scala:431)
	at org.apache.spark.deploy.yarn.YarnAllocator$$anonfun$handleAllocatedContainers$2.apply(YarnAllocator.scala:430)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.deploy.yarn.YarnAllocator.handleAllocatedContainers(YarnAllocator.scala:430)
	at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:281)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:556)
{noformat}

That means the YARN code being called ({{RackResolver}}) is swallowing the interrupt , so the Spark allocator thread never exits. In this particular app, the allocator was in the middle of allocating a very large number of executors, so it seemed like the application was hung, and there were a lot of executor coming up even though the context was being shut down.",,dongjoon,vanzin,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 20 18:48:36 UTC 2019,,,,,,,,,,"0|z00h7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/19 18:48;vanzin;Issue resolved by pull request 24017
[https://github.com/apache/spark/pull/24017];;;",,,,,,,,,,,,,,,,,,,,,
Read parquet file with merging metastore schema should compare schema field in uniform case.,SPARK-27080,13220102,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,Borui,Borui,07/Mar/19 05:44,09/Mar/19 13:34,13/Jul/23 08:46,09/Mar/19 13:34,2.3.2,2.3.3,2.4.0,,,,,,2.3.4,2.4.1,3.0.0,,SQL,,,,,0,,,,,,"In our product environment, when we upgrade spark from version 2.1 to 2.3, the job failed with an exception as below:

---ERROR stack trace –

Exception occur when running Job, 

org.apache.spark.SparkException: Detected conflicting schemas when merging the schema obtained from the Hive

 Metastore with the one inferred from the file format. Metastore schema:

{

  ""type"" : ""struct"",

  ""fields"" : [

......

}

Inferred schema:

{

  ""type"" : ""struct"",

  ""fields"" : [

......

}

at org.apache.spark.sql.hive.HiveMetastoreCatalog$.mergeWithMetastoreSchema(HiveMetastoreCatalog.scala:295)

at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$11.apply(HiveMetastoreCatalog.scala:243)

at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$11.apply(HiveMetastoreCatalog.scala:243)

at scala.Option.map(Option.scala:146)

at org.apache.spark.sql.hive.HiveMetastoreCatalog.org$apache$spark$sql$hive$HiveMetastoreCatalog$$inferIfNeeded(HiveMetastoreCatalog.scala:243)

at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$4$$anonfun$5.apply(HiveMetastoreCatalog.scala:167)

at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$4$$anonfun$5.apply(HiveMetastoreCatalog.scala:156)

at scala.Option.getOrElse(Option.scala:121)

at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$4.apply(HiveMetastoreCatalog.scala:156)

at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$4.apply(HiveMetastoreCatalog.scala:148)

at org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:54)

at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:148)

at org.apache.spark.sql.hive.RelationConversions.org$apache$spark$sql$hive$RelationConversions$$convert(HiveStrategies.scala:195)

at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:226)

at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:215)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)

at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)

at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)

at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:215)

at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:180)

 

The following case can trigger the exception, so we think it's a bug in spark2.3
{code:java}
// Parquet schema is subset of metaStore schema and has uppercase field name
assertResult(
  StructType(Seq(
    StructField(""UPPERCase"", DoubleType, nullable = true),
    StructField(""lowerCase"", BinaryType, nullable = true)))) {

  HiveMetastoreCatalog.mergeWithMetastoreSchema(
    StructType(Seq(
      StructField(""UPPERCase"", DoubleType, nullable = true),
      StructField(""lowerCase"", BinaryType, nullable = true))),

    StructType(Seq(
     StructField(""lowerCase"", BinaryType, nullable = true))))
}
{code}",,Borui,cloud_fan,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 09 13:34:45 UTC 2019,,,,,,,,,,"0|z00fsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/19 13:34;cloud_fan;Issue resolved by pull request 24001
[https://github.com/apache/spark/pull/24001];;;",,,,,,,,,,,,,,,,,,,,,
Read Hive materialized view throw MatchError,SPARK-27078,13220057,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,06/Mar/19 23:41,23/Mar/19 17:41,13/Jul/23 08:46,07/Mar/19 01:00,3.0.0,,,,,,,,2.4.1,3.0.0,,,SQL,,,,,0,,,,,,"How to reproduce:

Hive side:
{code:sql}
CREATE TABLE materialized_view_tbl (key INT);
CREATE MATERIALIZED VIEW view_1 AS SELECT * FROM materialized_view_tbl;  -- Hive 3.x
CREATE MATERIALIZED VIEW view_1 DISABLE REWRITE AS SELECT * FROM materialized_view_tbl;  -- Hive 2.3.x
{code}

Spark side(read from Hive 2.3.x):
{code:java}
bin/spark-sql --conf spark.sql.hive.metastore.version=2.3.4 --conf spark.sql.hive.metastore.jars=maven

spark-sql> select * from view_1;
19/03/06 16:33:44 ERROR SparkSQLDriver: Failed in [select * from view_1]
scala.MatchError: MATERIALIZED_VIEW (of class org.apache.hadoop.hive.metastore.TableType)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$3(HiveClientImpl.scala:434)
	at scala.Option.map(Option.scala:163)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$1(HiveClientImpl.scala:370)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:277)
{code}

Spark side(read from Hive 3.1.x):

{code:java}
bin/spark-sql --conf spark.sql.hive.metastore.version=3.1.1 --conf spark.sql.hive.metastore.jars=maven

spark-sql> select * from view_1;
19/03/05 19:55:37 ERROR SparkSQLDriver: Failed in [select * from view_1]
java.lang.NoSuchFieldError: INDEX_TABLE
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$3(HiveClientImpl.scala:438)
	at scala.Option.map(Option.scala:163)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$1(HiveClientImpl.scala:370)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:277)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:215)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:214)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:260)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getTableOption(HiveClientImpl.scala:368)
{code}
",,apachespark,dongjoon,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 07 01:00:56 UTC 2019,,,,,,,,,,"0|z00fig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/19 23:43;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/23984;;;","07/Mar/19 01:00;dongjoon;This is resolved via https://github.com/apache/spark/pull/23984;;;",,,,,,,,,,,,,,,,,,,,
Sorting table column in SQL WEBUI page throws 'IllegalArgumentException',SPARK-27075,13220029,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shahid,shahid,shahid,06/Mar/19 20:45,11/Mar/19 06:05,13/Jul/23 08:46,07/Mar/19 20:53,3.0.0,,,,,,,,3.0.0,,,,SQL,Web UI,,,,0,,,,,,"Test steps:
 1) bin/spark-sql
 2) run some queries
 3) Open SQL page in the webui
 4) Try to sort any column in the execution table.
",,shahid,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27115,,,,,,,,,,,,,,"06/Mar/19 21:07;shahid;image-2019-03-07-02-37-20-453.png;https://issues.apache.org/jira/secure/attachment/12961453/image-2019-03-07-02-37-20-453.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 07 20:53:08 UTC 2019,,,,,,,,,,"0|z00fc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/19 20:46;shahid;I will raise a PR;;;","07/Mar/19 20:53;vanzin;Issue resolved by pull request 23994
[https://github.com/apache/spark/pull/23994];;;",,,,,,,,,,,,,,,,,,,,
Fix a race condition when handling of IdleStateEvent,SPARK-27073,13219932,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,dzcxzl,dzcxzl,06/Mar/19 14:53,11/Mar/19 22:18,13/Jul/23 08:46,11/Mar/19 22:17,2.0.0,2.4.0,,,,,,,3.0.0,,,,Spark Core,,,,,0,,,,,,"When TransportChannelHandler processes IdleStateEvent, it first calculates whether the last request time has timed out.
At this time, TransportClient.sendRpc initiates a request.
TransportChannelHandler gets responseHandler.numOutstandingRequests() > 0, causing the normal connection to be closed.",,dzcxzl,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 11 22:17:00 UTC 2019,,,,,,,,,,"0|z00eqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/19 22:17;vanzin;Issue resolved by pull request 23989
[https://github.com/apache/spark/pull/23989];;;",,,,,,,,,,,,,,,,,,,,,
DefaultPartitionCoalescer can lock up driver for hours,SPARK-27070,13219845,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fitermay,fitermay,fitermay,06/Mar/19 08:59,17/Mar/19 16:49,13/Jul/23 08:46,17/Mar/19 16:47,2.3.1,2.3.2,2.4.0,,,,,,3.0.0,,,,Spark Core,,,,,0,,,,,,"We're running Spark on EMR reading large datasets from S3. When trying to coalesce a UnionRDD of two large FileScanRDDs (each with a few million partitions) into around 8k partitions the driver can stall for over an hour. 

 

Profiler shows that over 90% of the time is spent in TimSort which is invoked by `pickBin`. This seems like a very inefficient way to find the least occupied PartitionGroup. IMO a better way would just using the `min` method on the ArrayBuffer of `PartitionGroup`s",,dongjoon,fitermay,jonathak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 17 16:47:30 UTC 2019,,,,,,,,,,"0|z00e80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/19 09:20;fitermay;Patch:https://github.com/apache/spark/pull/23986;;;","15/Mar/19 01:13;srowen;Issue resolved by pull request 23986
[https://github.com/apache/spark/pull/23986];;;","15/Mar/19 21:58;dongjoon;This is reverted due to Scala-2.11 failure.
- https://github.com/apache/spark/commit/4bab69b22a50ae00b92ed6ab3b5120574dc3aa19;;;","17/Mar/19 16:47;srowen;Issue resolved by pull request 24116
[https://github.com/apache/spark/pull/24116];;;",,,,,,,,,,,,,,,,,,
avoid more than one active task set managers for a stage,SPARK-27065,13219606,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,05/Mar/19 16:56,17/May/20 17:46,13/Jul/23 08:46,06/Mar/19 18:02,2.3.3,2.4.0,,,,,,,2.3.4,2.4.1,3.0.0,,Scheduler,Spark Core,,,,0,,,,,,,,apachespark,cloud_fan,irashid,rajeshhadoop,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23433,SPARK-25250,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 06 18:02:28 UTC 2019,,,,,,,,,,"0|z00crs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/19 17:00;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/23927;;;","06/Mar/19 18:02;irashid;Issue resolved by pull request 23927
[https://github.com/apache/spark/pull/23927];;;",,,,,,,,,,,,,,,,,,,,
Bump Jackson version to 2.9.8,SPARK-27051,13219401,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanboliang,yanboliang,yanboliang,04/Mar/19 21:43,12/Dec/22 18:10,13/Jul/23 08:46,05/Mar/19 02:47,3.0.0,,,,,,,,3.0.0,,,,Spark Core,,,,,0,,,,,,"Fasterxml Jackson version before 2.9.8 is affected by multiple CVEs [[https://github.com/FasterXML/jackson-databind/issues/2186]], we need to fix bump the dependent Jackson to 2.9.8.",,apachespark,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 30 05:30:17 UTC 2019,,,,,,,,,,"0|z00bi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/19 02:47;gurwls223;Issue resolved by pull request 23965
[https://github.com/apache/spark/pull/23965];;;","30/Apr/19 05:29;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/24493;;;","30/Apr/19 05:30;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/24493;;;",,,,,,,,,,,,,,,,,,,
large partition data cause pyspark with python2.x oom,SPARK-27041,13219241,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,TigerYang414,TigerYang414,TigerYang414,04/Mar/19 09:02,12/Mar/19 15:25,13/Jul/23 08:46,12/Mar/19 15:24,2.4.0,,,,,,,,3.0.0,,,,PySpark,,,,,0,,,,,,"With large partition, pyspark may exceeds executor memory limit and trigger out of memory for python 2.7.
This is because map() is used. Unlike in python3.x, python 2.7 map() will generate a list and need to read all data into memory.

The proposed fix will use imap in python 2.7 and it has been verified.",,TigerYang414,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 12 15:24:15 UTC 2019,,,,,,,,,,"0|z00aj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/19 15:24;srowen;Issue resolved by pull request 23954
[https://github.com/apache/spark/pull/23954];;;",,,,,,,,,,,,,,,,,,,,,
"Even Broadcast thread is timed out, BroadCast Job is not aborted.",SPARK-27036,13219173,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jiangxb1987,Bjangir,Bjangir,03/Mar/19 19:07,16/May/19 07:43,13/Jul/23 08:46,15/May/19 21:48,2.3.2,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"During broadcast table job is execution if broadcast timeout (spark.sql.broadcastTimeout) happens ,broadcast Job still continue till completion whereas it should abort on broadcast timeout.

Exception is thrown in console  but Spark Job is still continue.

 

!image-2019-03-04-00-39-38-779.png!

!image-2019-03-04-00-39-12-210.png!

 

 wait for some time

!image-2019-03-04-00-38-52-401.png!

!image-2019-03-04-00-34-47-884.png!

 

How to Reproduce Issue

Option1 using SQL:- 
 create Table t1(Big Table,1M Records)
 val rdd1=spark.sparkContext.parallelize(1 to 1000000,100).map(x=> (""name_""+x,x%3,x))
 val df=rdd1.toDF.selectExpr(""_1 as name"",""_2 as age"",""_3 as sal"",""_1 as c1"",""_1 as c2"",""_1 as c3"",""_1 as c4"",""_1 as c5"",""_1 as c6"",""_1 as c7"",""_1 as c8"",""_1 as c9"",""_1 as c10"",""_1 as c11"",""_1 as c12"",""_1 as c13"",""_1 as c14"",""_1 as c15"",""_1 as c16"",""_1 as c17"",""_1 as c18"",""_1 as c19"",""_1 as c20"",""_1 as c21"",""_1 as c22"",""_1 as c23"",""_1 as c24"",""_1 as c25"",""_1 as c26"",""_1 as c27"",""_1 as c28"",""_1 as c29"",""_1 as c30"")
 df.write.csv(""D:/data/par1/t4"");
 spark.sql(""create table csv_2 using csv options('path'='D:/data/par1/t4')"");

create Table t2(Small Table,100K records)
 val rdd1=spark.sparkContext.parallelize(1 to 100000,100).map(x=> (""name_""+x,x%3,x))
 val df=rdd1.toDF.selectExpr(""_1 as name"",""_2 as age"",""_3 as sal"",""_1 as c1"",""_1 as c2"",""_1 as c3"",""_1 as c4"",""_1 as c5"",""_1 as c6"",""_1 as c7"",""_1 as c8"",""_1 as c9"",""_1 as c10"",""_1 as c11"",""_1 as c12"",""_1 as c13"",""_1 as c14"",""_1 as c15"",""_1 as c16"",""_1 as c17"",""_1 as c18"",""_1 as c19"",""_1 as c20"",""_1 as c21"",""_1 as c22"",""_1 as c23"",""_1 as c24"",""_1 as c25"",""_1 as c26"",""_1 as c27"",""_1 as c28"",""_1 as c29"",""_1 as c30"")
 df.write.csv(""D:/data/par1/t4"");
 spark.sql(""create table csv_2 using csv options('path'='D:/data/par1/t5')"");

spark.sql(""set spark.sql.autoBroadcastJoinThreshold=73400320"").show(false)
 spark.sql(""set spark.sql.broadcastTimeout=2"").show(false)
 Run Below Query 
 spark.sql(""create table s using parquet as select t1.* from csv_2 as t1,csv_1 as t2 where t1._c3=t2._c3"")

Option 2:- Use External DataSource and Add Delay in the #buildScan. and use datasource for query.",,apachespark,Bjangir,S71955,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20774,,,,,,,,,,,"03/Mar/19 19:08;Bjangir;image-2019-03-04-00-38-52-401.png;https://issues.apache.org/jira/secure/attachment/12960922/image-2019-03-04-00-38-52-401.png","03/Mar/19 19:09;Bjangir;image-2019-03-04-00-39-12-210.png;https://issues.apache.org/jira/secure/attachment/12960923/image-2019-03-04-00-39-12-210.png","03/Mar/19 19:09;Bjangir;image-2019-03-04-00-39-38-779.png;https://issues.apache.org/jira/secure/attachment/12960924/image-2019-03-04-00-39-38-779.png",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 14 22:48:56 UTC 2019,,,,,,,,,,"0|z00a40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/19 19:44;S71955;It seems to be the problem area is   BroadcastExchangeExec  in driver where  as part of Future a particular job will be fired and collected data will be broadcasted. 

The main problem is system will submit the job and its respective stage/tasks through DAGScheduler,  where the scheduler thread will schedule the respective events , In BroadcastExchangeExec when future time out happens respective exception will thrown but the jobs/task which is  scheduled by  the  DAGScheduler as part of the action called in future will not be cancelled, I think we shall cancel the respective job to avoid  running the same in  background even after Future time out exception, this can help to terminate the job promptly when TimeOutException happens, this will also save the additional resources getting utilized even after timeout exception thrown from driver. 

I want to give an attempt to handle this issue, Any comments suggestions are welcome.

cc [~blue@cloudera.com] [~hvanhovell] [~srowen];;;","14/May/19 22:48;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/24595;;;",,,,,,,,,,,,,,,,,,,,
Leaking Netty event loop group for shuffle chunk fetch requests,SPARK-27021,13218893,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,01/Mar/19 14:13,20/Dec/19 09:59,13/Jul/23 08:46,05/Mar/19 20:31,2.4.0,2.4.1,3.0.0,,,,,,3.0.0,,,,Spark Core,,,,,0,,,,,,The extra event loop group created for handling shuffle chunk fetch requests are never closed.,,attilapiros,roncenzhao,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/19 15:23;roncenzhao;image-2019-12-14-23-23-50-384.png;https://issues.apache.org/jira/secure/attachment/12988859/image-2019-12-14-23-23-50-384.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 20 09:59:39 UTC 2019,,,,,,,,,,"0|z008ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/19 14:13;attilapiros;I am already working on it.;;;","05/Mar/19 20:31;vanzin;Issue resolved by pull request 23930
[https://github.com/apache/spark/pull/23930];;;","13/Dec/19 03:49;roncenzhao;[~attilapiros]  [~vanzin] 

I hava two problems:
 # whether this bug also affect spark2.4.3 or not ? 
 # whether this bug cause the NodeManager OOM?

Look forward to your reply~ ;;;","13/Dec/19 10:48;attilapiros;
[~roncenzhao]

# yes
# I do not think so. This bug mostly effects the test system as test execution is the place where multiple TransportContext, NettyRpcEnv, etc ... are created and not closed correctly.;;;","14/Dec/19 15:25;roncenzhao;[~attilapiros] Thank you.

We have one problem about the memory leak of `StreamState` in `OneForOneStreamManager` which cause the `NodeManager` OOM. Most of the memory in NM is used by `StreamState`, like this:

!image-2019-12-14-23-23-50-384.png!

This may be caused by the shuffle service because we find the `StreamState` include some application which were already finished. Would you have any idea about this problem? Thanks~;;;","14/Dec/19 21:27;attilapiros;[~roncenzhao] it seems to me you bumped into https://issues.apache.org/jira/browse/SPARK-26418;;;","16/Dec/19 13:31;roncenzhao;[~attilapiros] Thanks. The issue is the same problem we have encountered.;;;","20/Dec/19 09:59;sandeep.katta2007;[~hyukjin.kwon] [~dongjoon] this issue is not back ported to branch-2.4. Please check it is required or not;;;",,,,,,,,,,,,,,
Spark UI's SQL tab shows inconsistent values,SPARK-27019,13218863,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shahid,peay,peay,01/Mar/19 11:08,25/Apr/20 11:57,13/Jul/23 08:46,06/Mar/19 22:09,2.4.0,,,,,,,,2.4.1,3.0.0,,,SQL,Web UI,,,,0,,,,,,"Since 2.4.0, I am frequently seeing broken outputs in the SQL tab of the Spark UI, where submitted/duration make no sense, description has the ID instead of the actual description.

Clicking on the link to open a query, the SQL plan is missing as well.

I have tried to increase `spark.scheduler.listenerbus.eventqueue.capacity` to very large values like 30k out of paranoia that we may have too many events, but to no avail. I have not identified anything particular that leads to that: it doesn't occur in all my jobs, but it does occur in a lot of them still.",,kabhwan,krish7919,peay,shahid,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31564,,,,"01/Mar/19 16:04;shahid;Screenshot from 2019-03-01 21-31-48.png;https://issues.apache.org/jira/secure/attachment/12960797/Screenshot+from+2019-03-01+21-31-48.png","01/Mar/19 14:33;peay;application_1550040445209_4748;https://issues.apache.org/jira/secure/attachment/12960789/application_1550040445209_4748","01/Mar/19 11:58;peay;query-1-details.png;https://issues.apache.org/jira/secure/attachment/12960770/query-1-details.png","01/Mar/19 11:58;peay;query-1-list.png;https://issues.apache.org/jira/secure/attachment/12960769/query-1-list.png","01/Mar/19 11:58;peay;query-job-1.png;https://issues.apache.org/jira/secure/attachment/12960771/query-job-1.png","01/Mar/19 11:09;peay;screenshot-spark-ui-details.png;https://issues.apache.org/jira/secure/attachment/12960764/screenshot-spark-ui-details.png","01/Mar/19 11:09;peay;screenshot-spark-ui-list.png;https://issues.apache.org/jira/secure/attachment/12960765/screenshot-spark-ui-list.png",,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 06 22:09:24 UTC 2019,,,,,,,,,,"0|z00874:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/19 11:09;peay;Seems like the screenshots did not embed, attaching them instead.;;;","01/Mar/19 11:19;peay;Also seeing this both for the Spark UI for live jobs, and when accessing past jobs through the Spark History Server.;;;","01/Mar/19 11:30;shahid;Hi [~peay] Could you please add test to reproduce it?;;;","01/Mar/19 11:39;peay;I don't really have a minimal example - this uses a bunch of python jobs and/or interactive work in notebooks. Is there a way I could collect debug information to help troubleshooting this?;;;","01/Mar/19 11:59;peay;OK, I can actually reproduce it pretty easily with pyspark:
{code:java}
df_test = spark.range(1024 * 1024 * 1024 * 10).toPandas(){code}
This makes the tasks fail because my executors don't have enough memory, which seems to be key to hitting the issue. Using only 1000 elements, the job succeeds and it does not trigger the issue.

!query-1-list.png!

 

!query-1-details.png!

!query-job-1.png!;;;","01/Mar/19 12:52;kabhwan;Submitted date feels me as ""-1"", as UNIX epoch 0 represents ""1970/01/01 00:00:00"".;;;","01/Mar/19 13:06;peay;Sure does.

I've done one more test, using no executors and running the same command:
{code:java}
spark.range(1024 * 1024 * 1024 * 10).toPandas()
{code}

 The SQL tab directly shows the wrong output, even though no task has started - i.e., my assumption above that this was because of failing tasks appears wrong. This actually seems consistent with the 'submitted date' somehow being wrong.;;;","01/Mar/19 13:29;shahid;seems event reordering has happened. Job start event came after sql execution end event, when the query failed. Could you please share spark eventLog for the application, if possible.;;;","01/Mar/19 14:33;peay;Attached: [^application_1550040445209_4748];;;","01/Mar/19 16:04;shahid;Yes. The issue happened because of the event reordering when the query failed. JobStart event came after the SQLExecutioEnd event, so the UI displayed weirdly. I will analyze the issue and send a patch.

 !Screenshot from 2019-03-01 21-31-48.png! ;;;","05/Mar/19 07:15;peay;Great! 

-Is that compatible with my second observation above? (I tested without any executors, and even without any task starting, the SQL tab had the wrong output). I can try to get an event log for that as well if that's helpful.- edit: I tried to reproduce that to export the event log, and could not. Seems like your patch should address the issue.;;;","05/Mar/19 07:26;shahid;Thanks [~peay] could you please share the event log for that too, if possible ;;;","05/Mar/19 07:56;shahid;Please upload the screenshot of the sql page of the second scenario. I don't think in that case it will display like that. The issue happens only when the new live execution data is overwritten by the existing one;;;","05/Mar/19 09:42;peay;Yes, I had edited my message above shortly after posting - cannot reproduce in the second scenario. Thanks!;;;","06/Mar/19 22:09;vanzin;Issue resolved by pull request 23939
[https://github.com/apache/spark/pull/23939];;;",,,,,,,
Checkpointed RDD deleted prematurely when using GBTClassifier,SPARK-27018,13218837,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,pkolaczk,pkolaczk,01/Mar/19 08:24,24/Jun/19 14:38,13/Jul/23 08:46,24/Jun/19 14:35,2.2.2,2.2.3,2.3.3,2.4.0,,,,,2.3.4,2.4.4,3.0.0,,ML,Spark Core,,,,0,,,,,,"Steps to reproduce:
{noformat}
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.classification.GBTClassifier

case class Row(features: org.apache.spark.ml.linalg.Vector, label: Int)

sc.setCheckpointDir(""/checkpoints"")
val trainingData = sc.parallelize(1 to 2426874, 256).map(x => Row(Vectors.dense(x, x + 1, x * 2 % 10), if (x % 5 == 0) 1 else 0)).toDF
val classifier = new GBTClassifier()
  .setLabelCol(""label"")
  .setFeaturesCol(""features"")
  .setProbabilityCol(""probability"")
  .setMaxIter(100)
  .setMaxDepth(10)
  .setCheckpointInterval(2)

classifier.fit(trainingData){noformat}
 

The last line fails with:
{noformat}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 10 times, most recent failure: Lost task 0.9 in stage 56.0 (TID 12058, 127.0.0.1, executor 0): java.io.FileNotFoundException: /checkpoints/191c9209-0955-440f-8c11-f042bdf7f804/rdd-51
at com.datastax.bdp.fs.hadoop.DseFileSystem$$anonfun$1.applyOrElse(DseFileSystem.scala:63)
at com.datastax.bdp.fs.hadoop.DseFileSystem$$anonfun$1.applyOrElse(DseFileSystem.scala:61)
at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
at com.datastax.bdp.fs.hadoop.DseFileSystem.com$datastax$bdp$fs$hadoop$DseFileSystem$$translateToHadoopExceptions(DseFileSystem.scala:70)
at com.datastax.bdp.fs.hadoop.DseFileSystem$$anonfun$6.apply(DseFileSystem.scala:264)
at com.datastax.bdp.fs.hadoop.DseFileSystem$$anonfun$6.apply(DseFileSystem.scala:264)
at com.datastax.bdp.fs.hadoop.DseFsInputStream.input(DseFsInputStream.scala:31)
at com.datastax.bdp.fs.hadoop.DseFsInputStream.openUnderlyingDataSource(DseFsInputStream.scala:39)
at com.datastax.bdp.fs.hadoop.DseFileSystem.open(DseFileSystem.scala:269)
at org.apache.spark.rdd.ReliableCheckpointRDD$.readCheckpointFile(ReliableCheckpointRDD.scala:292)
at org.apache.spark.rdd.ReliableCheckpointRDD.compute(ReliableCheckpointRDD.scala:100)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)
at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)
at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)
at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)
at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)
at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)
at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
at org.apache.spark.scheduler.Task.run(Task.scala:121)
at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748){noformat}
The problem happens as well when checkpointing directory is placed on the local file system, on a single-node setup. 

Debugging at the FS level showed that the driver requests to recursively delete the checkpointed rdd-51 soon before the exception gets thrown by the task.

 

 ","OS: Ubuntu Linux 18.10

Java: java version ""1.8.0_201""
Java(TM) SE Runtime Environment (build 1.8.0_201-b09)
Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)

Reproducible with a single-node Spark in standalone mode.

Reproducible with Zepellin or Spark shell.

 ",mgaido,pkolaczk,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/19 09:31;pkolaczk;Fix_check_if_the_next_checkpoint_exists_before_deleting_the_old_one.patch;https://issues.apache.org/jira/secure/attachment/12960748/Fix_check_if_the_next_checkpoint_exists_before_deleting_the_old_one.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 24 14:35:48 UTC 2019,,,,,,,,,,"0|z0081c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/19 09:04;pkolaczk;The checkpoint is deleted here (stacktrace for Spark 2.4.0):
{noformat}
removeCheckpointFile:186, PeriodicCheckpointer$ (org.apache.spark.util)
apply:175, PeriodicCheckpointer$$anonfun$removeCheckpointFile$1 (org.apache.spark.util)
apply:175, PeriodicCheckpointer$$anonfun$removeCheckpointFile$1 (org.apache.spark.util)
foreach:392, List (scala.collection.immutable)
removeCheckpointFile:174, PeriodicCheckpointer (org.apache.spark.util)
update:104, PeriodicCheckpointer (org.apache.spark.util)
boost:341, GradientBoostedTrees$ (org.apache.spark.ml.tree.impl)
run:55, GradientBoostedTrees$ (org.apache.spark.ml.tree.impl)
apply:206, GBTClassifier$$anonfun$train$1 (org.apache.spark.ml.classification)
apply:156, GBTClassifier$$anonfun$train$1 (org.apache.spark.ml.classification)
apply:183, Instrumentation$$anonfun$11 (org.apache.spark.ml.util)
apply:192, Try$ (scala.util)
instrumented:183, Instrumentation$ (org.apache.spark.ml.util)
train:156, GBTClassifier (org.apache.spark.ml.classification)
train:58, GBTClassifier (org.apache.spark.ml.classification)
fit:118, Predictor (org.apache.spark.ml){noformat}
 ;;;","01/Mar/19 09:16;pkolaczk;PeriodicCheckpointer:
{noformat}
// Remove checkpoints before the latest one.
var canDelete = true
while (checkpointQueue.size > 1 && canDelete) {
  // Delete the oldest checkpoint only if the next checkpoint exists.
  if (isCheckpointed(checkpointQueue.head)) {   
    removeCheckpointFile()
  } else {
    canDelete = false
  }
}{noformat}
PeriodicCheckpointer: 103:
{noformat}
if (isCheckpointed(checkpointQueue.head)){noformat}
This line doesn't do what the comment says it should do; it checks the CURRENT checkpoint to remove, not the NEXT checkpoint! And the debugger tells me the next checkpoint hasn't been materialized yet.

 

Patch coming.;;;","01/Mar/19 09:32;pkolaczk;Attached a patch for Spark 2.2.2. Should be compatible with all later versions.;;;","01/Mar/19 10:43;mgaido;[~pkolaczk] thanks for reporting the issue. Spark works with PRs, not patches. Can you please submit a PR for master branch? Thanks.;;;","01/Mar/19 12:57;pkolaczk;For master only? Why not for 2.2, 2.3, master?;;;","01/Mar/19 13:11;mgaido;First the issue is fixed on master and then it is backported to the other branches (if there are no conflicts, the committer will do it while merging to master). Anyway, 2.2 is EOL, so it will be eventually backported to 2.3/2.4.;;;","07/Mar/19 10:51;pkolaczk;The file does not exist on master branch. Where has that code been moved?;;;","07/Mar/19 17:02;mgaido;The PeriodicCheckpointer is still there in master, you can check it on github: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala. You can just check file histories in git in order to know what and how was changed. I'd recommend you, anyway, to test whether current master is still affected by this issue as it may have been fixed since 2.2.;;;","10/Jun/19 10:11;podongfeng;[~pkolaczk]  With the codes you provided, I reproduced this failure. 

moreover, I doubt that this bug may also affect the computation on distributed env.

I also encountered a similar case on a cluster, I will look into this.
{code:java}
java.io.FileNotFoundException: File does not exist: /tmp/sparkGBM/application_1551338088092_2518369/checkpoints/edbd13db-b61f-445a-8703-691acd595d62/rdd-46484/_partitioner
        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)
        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1929)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1900)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1803)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:604)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:388)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:624)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2094)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2090)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1803)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2090)

        at sun.reflect.GeneratedConstructorAccessor79.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
        at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1268)
        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1253)
        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1241)
        at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:303)
        at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:269)
        at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:261)
        at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1566)
        at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)
        at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)
        at org.apache.spark.rdd.ReliableCheckpointRDD$.org$apache$spark$rdd$ReliableCheckpointRDD$$readCheckpointedPartitionerFile(ReliableCheckpointRDD.scala:255)
        at org.apache.spark.rdd.ReliableCheckpointRDD$$anonfun$3.apply(ReliableCheckpointRDD.scala:59)
        at org.apache.spark.rdd.ReliableCheckpointRDD$$anonfun$3.apply(ReliableCheckpointRDD.scala:59)
        at scala.Option.orElse(Option.scala:289)
        at org.apache.spark.rdd.ReliableCheckpointRDD.<init>(ReliableCheckpointRDD.scala:58)
        at org.apache.spark.rdd.ReliableCheckpointRDD$.writeRDDToCheckpointDirectory(ReliableCheckpointRDD.scala:151)
        at org.apache.spark.rdd.ReliableRDDCheckpointData.doCheckpoint(ReliableRDDCheckpointData.scala:58)
        at org.apache.spark.rdd.RDDCheckpointData.checkpoint(RDDCheckpointData.scala:75)
        at org.apache.spark.rdd.RDD$$anonfun$doCheckpoint$1.apply$mcV$sp(RDD.scala:1734)
        at org.apache.spark.rdd.RDD$$anonfun$doCheckpoint$1.apply(RDD.scala:1724)
        at org.apache.spark.rdd.RDD$$anonfun$doCheckpoint$1.apply(RDD.scala:1724){code};;;","11/Jun/19 05:49;podongfeng;I test on both local env and a cluster env, and your patch works fine.

[~pkolaczk]  Could you plz create a PR to master branch? I think commiters can help backporting it to older versions.;;;","24/Jun/19 14:35;srowen;Issue resolved by pull request 24870
[https://github.com/apache/spark/pull/24870];;;",,,,,,,,,,,
spark-submit does not properly escape arguments sent to Mesos dispatcher,SPARK-27015,13218739,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mwlon,mwlon,mwlon,28/Feb/19 20:31,06/Mar/19 00:38,13/Jul/23 08:46,05/Mar/19 21:06,2.3.3,2.4.0,,,,,,,3.0.0,,,,Mesos,,,,,0,,,,,,"Arguments sent to the dispatcher must be escaped; for instance,

{noformat}spark-submit --master mesos://url:port my.jar --arg1 ""a b$c""{noformat}

fails, and instead must be submitted as
{noformat}spark-submit --master mesos://url:port my.jar --arg1 ""a\\ b\\$c""{noformat}",,mwlon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 05 01:00:10 UTC 2019,,,,,,,,,,"0|z007fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/19 20:56;srowen;Sure, open a PR to escape the args as needed.;;;","05/Mar/19 01:00;mwlon;Created a PR: https://github.com/apache/spark/pull/23967;;;",,,,,,,,,,,,,,,,,,,,
Storage tab shows rdd details even after executor ended,SPARK-27012,13218626,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ajithshetty,ajithshetty,ajithshetty,28/Feb/19 13:10,05/Mar/19 18:56,13/Jul/23 08:46,05/Mar/19 18:42,2.3.3,3.0.0,,,,,,,3.0.0,,,,Spark Core,Web UI,,,,0,,,,,," 

After we cache a table, we can see its details in Storage Tab of spark UI. If the executor has shutdown ( graceful shutdown/ Dynamic executor scenario) UI still shows the rdd as cached and when we click the link it throws error. This is because on executor remove event, we fail to adjust rdd partition details.",,ajithshetty,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23134,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 28 13:14:18 UTC 2019,,,,,,,,,,"0|z006qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/19 13:14;ajithshetty;I will be working on this;;;",,,,,,,,,,,,,,,,,,,,,
reset command fails after cache table,SPARK-27011,13218610,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ajithshetty,ajithshetty,ajithshetty,28/Feb/19 11:12,18/Mar/19 05:43,13/Jul/23 08:46,12/Mar/19 04:42,2.3.3,2.4.0,3.0.0,,,,,,3.0.0,,,,Spark Core,SQL,,,,0,,,,,,"Commands to reproduce 
{code:java}
spark-sql> create table abcde ( a int);

spark-sql> reset; // can work success

spark-sql> cache table abcde;

spark-sql> reset; //fails with exception
{code}
Below is the stack
{code:java}
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: makeCopy, tree:
ResetCommand$at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
 at org.apache.spark.sql.catalyst.trees.TreeNode.makeCopy(TreeNode.scala:379)
 at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:216)
 at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:211)
 at org.apache.spark.sql.catalyst.plans.QueryPlan.sameResult(QueryPlan.scala:259)
 at org.apache.spark.sql.execution.CacheManager.$anonfun$lookupCachedData$3(CacheManager.scala:236)
 at org.apache.spark.sql.execution.CacheManager.$anonfun$lookupCachedData$3$adapted(CacheManager.scala:236)
 at scala.collection.Iterator.find(Iterator.scala:993)
 at scala.collection.Iterator.find$(Iterator.scala:990)
 at scala.collection.AbstractIterator.find(Iterator.scala:1429)
 at scala.collection.IterableLike.find(IterableLike.scala:81)
 at scala.collection.IterableLike.find$(IterableLike.scala:80)
 at scala.collection.AbstractIterable.find(Iterable.scala:56)
 at org.apache.spark.sql.execution.CacheManager.$anonfun$lookupCachedData$2(CacheManager.scala:236)
 at org.apache.spark.sql.execution.CacheManager.readLock(CacheManager.scala:59)
 at org.apache.spark.sql.execution.CacheManager.lookupCachedData(CacheManager.scala:236) at org.apache.spark.sql.execution.CacheManager$$anonfun$1.applyOrElse(CacheManager.scala:250)
 at org.apache.spark.sql.execution.CacheManager$$anonfun$1.applyOrElse(CacheManager.scala:241)
 at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:258)
 at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:258)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:149)
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:147)
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
 at org.apache.spark.sql.execution.CacheManager.useCachedData(CacheManager.scala:241)
 at org.apache.spark.sql.execution.QueryExecution.withCachedData$lzycompute(QueryExecution.scala:68)
 at org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:65)
 at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:72)
 at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
 at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:72)
 at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:71)
 at org.apache.spark.sql.execution.QueryExecution.$anonfun$writePlans$4(QueryExecution.scala:139)
 at org.apache.spark.sql.catalyst.plans.QueryPlan$.append(QueryPlan.scala:316)
 at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$writePlans(QueryExecution.scala:139)
 at org.apache.spark.sql.execution.QueryExecution.toString(QueryExecution.scala:146)
 at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:82)
 at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:147)
 at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)
 at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3346)
 at org.apache.spark.sql.Dataset.<init>(Dataset.scala:203)
 at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
 at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:656)
 at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:685)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:372)
 at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:275)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
 at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:847)
 at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:162)
 at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:185)
 at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:87)
 at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:926)
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:935)
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.RuntimeException: No valid constructor for ResetCommand$
 at scala.sys.package$.error(package.scala:30)
 at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$makeCopy$1(TreeNode.scala:383)
 at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
 ... 60 more
{code}",,ajithshetty,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 12 04:42:38 UTC 2019,,,,,,,,,,"0|z006n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/19 11:13;ajithshetty;Will be working on this;;;","12/Mar/19 04:29;ajithshetty;@ [~cloud_fan] As [https://github.com/apache/spark/pull/23918] is merged, can we close this.?;;;","12/Mar/19 04:42;cloud_fan;Issue resolved by pull request 23918
[https://github.com/apache/spark/pull/23918];;;",,,,,,,,,,,,,,,,,,,
Code for https uri authentication in Spark Submit needs to be removed,SPARK-27004,13218351,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,skonto,skonto,27/Feb/19 14:00,11/Mar/19 19:30,13/Jul/23 08:46,11/Mar/19 19:27,3.0.0,,,,,,,,3.0.0,,,,Spark Core,,,,,0,,,,,,"The old code in Spark Submit used for uri verification according to the comments [here|https://github.com/apache/spark/pull/23546#issuecomment-463340476] and [here|https://github.com/apache/spark/pull/23546#issuecomment-463366075] needs to be removed or refactored otherwise it will cause failures with secure http uris.",,skonto,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 11 19:27:50 UTC 2019,,,,,,,,,,"0|z00520:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/19 19:27;vanzin;Issue resolved by pull request 24033
[https://github.com/apache/spark/pull/24033];;;",,,,,,,,,,,,,,,,,,,,,
Latest kafka delegation token not always picked up,SPARK-27002,13218309,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gsomogyi,gsomogyi,gsomogyi,27/Feb/19 10:43,27/Feb/19 18:10,13/Jul/23 08:46,27/Feb/19 18:07,3.0.0,,,,,,,,3.0.0,,,,Structured Streaming,,,,,0,,,,,,"Lately KAFKA-8004 has been discovered and this triggered me to do further tests with Spark.
Namely I've created long running Kafka to Kafka tests on 4 node cluster with randomly thrown artificial exceptions.

Test scenario:
* 4 node cluster
* Yarn
* Kafka broker version 2.1.0
* security.protocol = SASL_SSL
* sasl.mechanism = SCRAM-SHA-512

Kafka broker settings:
* delegation.token.expiry.time.ms=600000 (10 min)
* delegation.token.max.lifetime.ms=1200000 (20 min)
* delegation.token.expiry.check.interval.ms=300000 (5 min)

After each 7.5 minutes new delegation token obtained from Kafka broker (10 min * 0.75).
But when token expired after 10 minutes (Spark obtains new one and doesn't renew the old), the brokers expiring thread comes after each 5 minutes (invalidates expired tokens) and artificial exception has been thrown inside the Spark application (such case Spark closes connection), then the latest delegation token not always picked up.
",,gsomogyi,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 27 18:07:25 UTC 2019,,,,,,,,,,"0|z004sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/19 18:07;vanzin;Issue resolved by pull request 23906
[https://github.com/apache/spark/pull/23906];;;",,,,,,,,,,,,,,,,,,,,,
Global function that has the same name can't be overwritten in Python RDD API,SPARK-27000,13218225,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gurwls223,gurwls223,,27/Feb/19 02:56,12/Dec/22 18:10,13/Jul/23 08:46,27/Feb/19 17:33,3.0.0,,,,,,,,3.0.0,,,,PySpark,,,,,0,,,,,,"{code}
>>> def hey():
...     return ""Hi""
...
>>> spark.range(1).rdd.map(lambda _: hey()).collect()
['Hi']
>>> def hey():
...     return ""Yeah""
...
>>> spark.range(1).rdd.map(lambda _: hey()).collect()
['Hi']
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18161,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 27 17:33:47 UTC 2019,,,,,,,,,,"0|z004a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/19 17:33;gurwls223;Issue resolved by pull request 23904
[https://github.com/apache/spark/pull/23904];;;",,,,,,,,,,,,,,,,,,,,,
spark.ssl.keyStorePassword in plaintext on 'ps -ef' output of executor processes in Standalone mode,SPARK-26998,13218193,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,toopt4,toopt4,26/Feb/19 23:08,12/Dec/22 18:10,13/Jul/23 08:46,02/Apr/19 16:26,2.3.3,2.4.0,,,,,,,2.3.4,2.4.2,3.0.0,,Scheduler,Security,Spark Core,,,0,secur,security,Security,SECURITY,security-issue,"Run spark standalone mode, then start a spark-submit requiring at least 1 executor. Do a 'ps -ef' on linux (ie putty terminal) and you will be able to see  spark.ssl.keyStorePassword value in plaintext!

 

spark.ssl.keyStorePassword and  spark.ssl.keyPassword don't need to be passed to  CoarseGrainedExecutorBackend. Only  spark.ssl.trustStorePassword is used.

 

Can be resolved if below PR is merged:

[[Github] Pull Request #21514 (tooptoop4)|https://github.com/apache/spark/pull/21514]",,gsomogyi,kabhwan,toopt4,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22860,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 02 16:26:58 UTC 2019,,,,,,,,,,"0|z00434:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/19 05:27;gurwls223;can you reopen the PR and proceed?;;;","01/Mar/19 21:56;gsomogyi;How is this different from https://github.com/apache/spark/pull/23820?;;;","02/Mar/19 10:05;toopt4;[https://github.com/apache/spark/pull/23820] is only about hiding password from log file, SPARK-26998 is about hiding passwords from showing in 'ps -ef' process list;;;","05/Mar/19 12:51;gsomogyi;[~toopt4] thanks for the info. Are you working on this? If not I'm happy to push the solution forward.;;;","05/Mar/19 13:04;gsomogyi;{quote}
Can be resolved if below PR is merged:

[[Github] Pull Request #21514 (tooptoop4)|https://github.com/apache/spark/pull/21514]
{quote}
I think it's just not true. #21514 is solving a UI problem where an application 'name' urls point to http instead of https (even when ssl enabled).
Have I missed something?
;;;","05/Mar/19 13:07;gsomogyi;Ahaaa, I see now. 2 problems tried to be solved in one PR.;;;","05/Mar/19 13:33;kabhwan;If I understand correctly, the PR would mitigate the issue (remove some of unnecessary password parameters being passed) but not completely solve the issue, sine truststore password parameters will be still passed as it was.

To handle issue correctly we need to have secured storage to share the security information.;;;","05/Mar/19 19:03;toopt4;[~gsomogyi] please take it forward.

[~kabhwan] truststore password being shown is not much of a problem since truststore is often distributed to users anyway. But keystore password still being shown is the big no-no.;;;","06/Mar/19 06:49;kabhwan;[~toopt4]

Yeah I tend to agree that hiding more credential things are better so supportive on the change. Maybe I thought about the description of Jira issue your patch was originally landed.

Btw, are there any existing test or manual test to verify whether keystore password and key password are not used? Just curious, I honestly don't know about it.;;;","07/Mar/19 12:47;gsomogyi;I've tested the things through and see the issue. I think it's not only standalone mode problem but applies to all cases where sensitive information provided in command line.;;;","11/Mar/19 14:46;gsomogyi;Since the first part of the PR solved (http URLs in case of secure mode) continuing with the second issue.
In my view the problem can be mitigated to ask users to provide configuration parameters in configuration file (several commercial products does this)
* Either spark-defaults.conf
* or --properties-file

That way the command line options will show either nothing (spark-defaults.conf picked up by default) or something like ""... --properties-file my-secret-spark-properties.conf ..."".
As a side note this workaround is available at the moment but I would like to warn users for such situations.

The other approach what I've considered (and abandoned) is to open a pipe and send the password through this channel but since this approach is not really conform with Spark's configuration system
it would imply heavy changes and don't see the return of investment.

[~vanzin] what do you think since you have quite a bit experience with security?
;;;","11/Mar/19 17:05;vanzin;There are 3 ways to solve this: pipe, file, or env variable. Pick one.;;;","11/Mar/19 17:19;gsomogyi;Same understanding, chosen the file approach.;;;","02/Apr/19 16:26;vanzin;Issue resolved by pull request 24170
[https://github.com/apache/spark/pull/24170];;;",,,,,,,,
k8s integration tests failing after client upgraded to 4.1.2,SPARK-26997,13218140,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,vanzin,vanzin,26/Feb/19 17:39,17/May/20 18:24,13/Jul/23 08:46,01/Mar/19 19:25,3.0.0,,,,,,,,,,,,Kubernetes,Spark Core,,,,0,,,,,,"SPARK-26742 upgraded the client libs to version 4.1.2, and that doesn't seem to agree well with the minikube we're using in jenkins. My PRs are failing (minikube 0.25):

{noformat}
19/02/25 17:46:52.599 ScalaTest-main-running-KubernetesSuite INFO ProcessUtils: 19/02/25 17:46:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-3007689c-e3ca-48f5-a673-f3bad5c4774a
19/02/25 17:46:52.788 OkHttp https://192.168.39.69:8443/... ERROR ExecWebSocketListener: Exec Failure: HTTP:500. Message:container not found (""spark-kubernetes-driver"")
java.net.ProtocolException: Expected HTTP 101 response but was '500 Internal Server Error'
	at okhttp3.internal.ws.RealWebSocket.checkResponse(RealWebSocket.java:229)
	at okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:196)
	at okhttp3.RealCall$AsyncCall.execute(RealCall.java:206)
	at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
19/02/25 17:46:52.999 OkHttp https://192.168.39.69:8443/... ERROR ExecWebSocketListener: Exec Failure: HTTP:404. Message:404 page not found

java.net.ProtocolException: Expected HTTP 101 response but was '404 Not Found'
	at okhttp3.internal.ws.RealWebSocket.checkResponse(RealWebSocket.java:229)
	at okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:196)
	at okhttp3.RealCall$AsyncCall.execute(RealCall.java:206)
	at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{noformat}

Tests pass on my local minikube (0.34). Reverting that change makes them pass on jenkins (see https://github.com/apache/spark/pull/23893).

Not sure if this is a client bug or a compatibility issue.

[~shaneknapp] [~skonto]",,rvesse,shaneknapp,skonto,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 01 19:25:18 UTC 2019,,,,,,,,,,"0|z003rc:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,"26/Feb/19 17:43;shaneknapp;how exactly are you launching minikube when performing local testing?  in particular, what VM driver are you using?;;;","26/Feb/19 17:47;vanzin;I'm using virtualbox.;;;","26/Feb/19 17:50;shaneknapp;i bet it's a version incompatibility.

is there an open PR w/your failing changes that i can test against?;;;","26/Feb/19 17:54;vanzin;It's linked in the description. I can revert the revert that caused it to pass again.;;;","26/Feb/19 17:57;shaneknapp;
that'd be super.  :)

-- 
Shane Knapp
UC Berkeley EECS Research / RISELab Staff Technical Lead
https://rise.cs.berkeley.edu
;;;","26/Feb/19 20:32;skonto;[~vanzin] I run the tests successfully with the latest minikube version (v0.34.1) and k8s 1.13.3. It seems like a compatibility issue AFAIK. I have seen the failures in our intern ci as well. 

There is a compatibility matrix for the fabric8io client here: [https://github.com/fabric8io/kubernetes-client/blob/8b85c5f7259c86069a9ab591f31c91cd4fb88d86/README.md#compatibility-matrix] but does not contain version v4.1.2 yet.

[~shaneknapp] one problem though (just as a continuation of the discussion about testing) is that while in theory using something like ` --kubernetes-version=v1.11.7`  should work for targeting any k8s version it didnt. It failed for this case with: `Caused by: java.net.UnknownHostException: kubernetes.default.svc: for me. I am trying to make tests pass for [https://github.com/apache/spark/pull/23514] while also checking against versions that are still being patched or are still supported. Moreover, as a side note one reason I have also used `driver=none` is because on aws instances you cant run kvm, so it was not only for avoiding using a vm (of course security is a big issue with that but not if you are doing it on an isolated host).

 ;;;","27/Feb/19 18:17;shaneknapp;i just confirmed that k8s 1.10.0 is definitely incompatible w/the 4.1.2 client.  testing against [~vanzin]'s recent (and absolutely trivial) PR, i found that the integration tests would fail w/his changes reverted (using 4.1.2), and passed with his changes testing against 4.1.0.

this means we need a minimum viable version of k8s to test against.  how do we choose?  especially as it seems that backwards compatibility isn't ""a thing"".

putting on my build engineer hat:  i am extremely reluctant to tie spark testing infrastructure deps directly to some magical process that follows k8s releases.  it also appears that slight variations in local testing infrastructure (i use kvm2, [~vanzin] uses virtualbox, and [~skonto] none at all) cause failures (or successes) in different, and spectacular ways.

and i think we might be going about creating the pods and containers to run these tests the wrong way.  it might be better to use yaml instead of setting a bunch of scala vars and hitting an api.

this is already proven to be an absolute nightmare to debug, and for all these exact reasons.

discuss.  :);;;","27/Feb/19 23:23;shaneknapp;great news, everyone!  :)

i was able to get everything upgraded on my staging box to the latest-n-greatest, and all of the integration tests pass w/the 4.1.2 client.

here are the pertinent versions of all the things:

 
{noformat}
minikube + kvm2 driver: v0.34.1
k8s:  1.13.3
client:  4.1.2
{noformat}
 

 

TODO for each jenkins worker:

1) download + install latest minikube version

2) dist out a home-rolled kvm2 driver

3) change symlinks for minikube + docker-machine-driver-kvm2 to point to latest binaries

4) minikube delete, rm -rf .kube .minikube

 

TODO for the jenkins job config:

change the minikube start sequence to be the following:
{code:java}
$ minikube --vm-driver=kvm2 start --memory 6000 --cpus 8
$ kubectl create clusterrolebinding serviceaccounts-cluster-admin \
 --clusterrole=cluster-admin \
 --group=system:serviceaccounts{code}
 

only when these are done will we be able to re-merge [https://github.com/apache/spark/pull/23814] to master, as well as back-port to 2.4

 

TIMING:

i can stage this stuff on the jenkins workers, and mid-next week (once the dust literally settles) i can coordinate with whomever wants to merge/backport and make this work.

 

phew.  at least we now have some breathing room WRT deciding which k8s version/etc to test against.;;;","28/Feb/19 18:08;shaneknapp;ok...  everything is staged and all that remains to be done (in this order):

* deploy symlinks on the jenkins workers to point minikube and kvm2 drivers to v0.34.1, minikube delete + rm -rf stuff
* uncomment kubectl clusterrolebinding setup line in the k8s prb
* 4.1.2 client PR created
* see if build is happy
* merge PR

we can do this monday morning, or some time on wednesday.  definitely not today or tomorrow please.;;;","01/Mar/19 19:25;vanzin;I reverted the client upgrade and re-opened the original bug, so let's keep the discussion there.;;;",,,,,,,,,,,,
Running Spark in Docker image with Alpine Linux 3.9.0 throws errors when using snappy,SPARK-26995,13218119,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lucacanali,lucacanali,lucacanali,26/Feb/19 15:46,17/May/20 18:25,13/Jul/23 08:46,04/Mar/19 17:59,2.3.0,2.4.0,,,,,,,2.4.4,3.0.0,,,Kubernetes,Spark Core,,,,1,,,,,,"Running Spark in Docker image with Alpine Linux 3.9.0 throws errors when using snappy.  

The issue can be reproduced for example as follows: `Seq(1,2).toDF(""id"").write.format(""parquet"").save(""DELETEME1"")`  
The key part of the error stack is as follows `Caused by: java.lang.UnsatisfiedLinkError: /tmp/snappy-1.1.7-2b4872f1-7c41-4b84-bda1-dbcb8dd0ce4c-libsnappyjava.so: Error loading shared library ld-linux-x86-64.so.2: Noded by /tmp/snappy-1.1.7-2b4872f1-7c41-4b84-bda1-dbcb8dd0ce4c-libsnappyjava.so)`  

The source of the error appears to be due to the fact that libsnappyjava.so needs ld-linux-x86-64.so.2 and looks for it in /lib, while in Alpine Linux 3.9.0 with libc6-compat version 1.1.20-r3 ld-linux-x86-64.so.2 is located in /lib64.
Note: this issue is not present with Alpine Linux 3.8 and libc6-compat version 1.1.19-r10 ",,dongjoon,lucacanali,sdehaes,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27703,SPARK-28347,SPARK-28981,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 25 08:53:25 UTC 2019,,,,,,,,,,"0|z003mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/19 06:48;sdehaes;[~lucacanali] as a temporary fix I added the layer 

 
{code:java}
RUN ln -s /lib64/ld-linux-x86-64.so.2 /lib/ld-linux-x86-64.so.2{code}
to make the image usable

 ;;;","27/Feb/19 06:54;sdehaes;I see your PR did the same;;;","27/Feb/19 10:38;sdehaes;You can also add
{code:java}
ENV LD_LIBRARY_PATH /lib64{code};;;","04/Mar/19 17:59;vanzin;Issue resolved by pull request 23898
[https://github.com/apache/spark/pull/23898];;;","25/Jul/19 08:53;dongjoon;This is backported to `branch-2.4` via https://github.com/apache/spark/pull/25255 .;;;",,,,,,,,,,,,,,,,,
Fix STS scheduler pool correct delivery,SPARK-26992,13218022,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dzcxzl,dzcxzl,dzcxzl,26/Feb/19 07:19,10/Jun/19 07:30,13/Jul/23 08:46,06/Apr/19 22:14,2.0.0,2.4.0,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"The user sets the value of spark.sql.thriftserver.scheduler.pool.
 Spark thrift server saves this value in the LocalProperty of threadlocal type, but does not clean up after running, causing other sessions to run in the previously set pool name.

 

For example

The second session does not manually set the pool name. The default pool name should be used, but the pool name of the previous user's settings is used. This is incorrect.

!error_session.png!

 

!error_stage.png!

 ",,apachespark,dzcxzl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26914,,,,,,,,,,,"26/Feb/19 07:25;dzcxzl;error_session.png;https://issues.apache.org/jira/secure/attachment/12960140/error_session.png","26/Feb/19 07:25;dzcxzl;error_stage.png;https://issues.apache.org/jira/secure/attachment/12960141/error_stage.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 06 22:14:49 UTC 2019,,,,,,,,,,"0|z0031c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/19 07:20;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/23895;;;","06/Apr/19 22:14;srowen;Issue resolved by pull request 23895
[https://github.com/apache/spark/pull/23895];;;",,,,,,,,,,,,,,,,,,,,
Difference in handling of mixed-case partition column names after SPARK-26188,SPARK-26990,13217956,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,bersprockets,bersprockets,26/Feb/19 00:58,14/Mar/19 23:56,13/Jul/23 08:46,11/Mar/19 17:13,2.4.1,,,,,,,,2.4.1,3.0.0,,,SQL,,,,,0,,,,,,"I noticed that the [PR for SPARK-26188|https://github.com/apache/spark/pull/23165] changed how mixed-cased partition columns are handled when the user provides a schema.

Say I have this file structure (note that each instance of `pS` is mixed case):
{noformat}
bash-3.2$ find partitioned5 -type d
partitioned5
partitioned5/pi=2
partitioned5/pi=2/pS=foo
partitioned5/pi=2/pS=bar
partitioned5/pi=1
partitioned5/pi=1/pS=foo
partitioned5/pi=1/pS=bar
bash-3.2$
{noformat}
If I load the file with a user-provided schema in 2.4 (before the PR was committed) or 2.3, I see:
{noformat}
scala> val df = spark.read.schema(""intField int, pi int, ps string"").parquet(""partitioned5"")
df: org.apache.spark.sql.DataFrame = [intField: int, pi: int ... 1 more field]
scala> df.printSchema
root
 |-- intField: integer (nullable = true)
 |-- pi: integer (nullable = true)
 |-- ps: string (nullable = true)
scala>
{noformat}
However, using 2.4 after the PR was committed. I see:
{noformat}
scala> val df = spark.read.schema(""intField int, pi int, ps string"").parquet(""partitioned5"")
df: org.apache.spark.sql.DataFrame = [intField: int, pi: int ... 1 more field]
scala> df.printSchema
root
 |-- intField: integer (nullable = true)
 |-- pi: integer (nullable = true)
 |-- pS: string (nullable = true)
scala>
{noformat}
Spark is picking up the mixed-case column name {{pS}} from the directory name, not the lower-case {{ps}} from my specified schema.

In all tests, {{spark.sql.caseSensitive}} is set to the default (false).

Not sure is this is an bug, but it is a difference.",,apachespark,bersprockets,dongjoon,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26188,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 26 07:12:44 UTC 2019,,,,,,,,,,"0|z002mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/19 07:12;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/23894;;;",,,,,,,,,,,,,,,,,,,,,
Flaky test:DAGSchedulerSuite.Barrier task failures from the same stage attempt don't trigger multiple stage retries,SPARK-26989,13217950,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,vanzin,vanzin,26/Feb/19 00:08,18/Sep/19 22:40,13/Jul/23 08:46,11/Sep/19 17:25,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,3.0.0,,,2.4.5,3.0.0,,,Spark Core,Tests,,,,0,,,,,,"https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/102761/testReport/junit/org.apache.spark.scheduler/DAGSchedulerSuite/Barrier_task_failures_from_the_same_stage_attempt_don_t_trigger_multiple_stage_retries/

{noformat}
org.apache.spark.scheduler.DAGSchedulerSuite.Barrier task failures from the same stage attempt don't trigger multiple stage retries

Error Message
org.scalatest.exceptions.TestFailedException: ArrayBuffer() did not equal List(0)

Stacktrace
sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: ArrayBuffer() did not equal List(0)
	at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528)
	at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501)
	at org.apache.spark.scheduler.DAGSchedulerSuite.$anonfun$new$144(DAGSchedulerSuite.scala:2644)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:104)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.apache.spark.scheduler.DAGSchedulerSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(DAGSchedulerSuite.scala:122)
{noformat}

- https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/109303/consoleFull
{code}
- Barrier task failures from the same stage attempt don't trigger multiple stage retries *** FAILED ***
  ArrayBuffer(0) did not equal List(0) (DAGSchedulerSuite.scala:2656)
{code}",,dongjoon,kabhwan,smilegator,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24795,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 11 17:25:23 UTC 2019,,,,,,,,,,"0|z002lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/May/19 13:46;tgraves;seeing the same error intermittently, the weird thing is I sometimes also see it error with below, like its not converting the collection or maybe its still a timing issue, but I had increased the timeout from 10 seconds to 30 seconds as well.

org.scalatest.exceptions.TestFailedException: ArrayBuffer(0) did not equal List(0) at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528) at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527) at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560) at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501) at org.apache.spark.scheduler.DAGSchedulerSuite.$anonfun$new$144(DAGSchedulerSuite.scala:2656);;;","19/Aug/19 01:27;dongjoon;I also hit this issue today. It's the same with Thomas's situation `ArrayBuffer(0)`.;;;","05/Sep/19 23:39;kabhwan;[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/110202/testReport/]

Hit this again.

org.scalatest.exceptions.TestFailedException: ArrayBuffer() did not equal List(0)

 ;;;","11/Sep/19 17:25;vanzin;Issue resolved by pull request 25706
[https://github.com/apache/spark/pull/25706];;;",,,,,,,,,,,,,,,,,,
"Test ""access only some column of the all of columns "" fails on big endian",SPARK-26985,13217792,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ketan22584,Anuja,Anuja,25/Feb/19 10:43,12/Dec/22 18:10,13/Jul/23 08:46,25/Jun/19 13:35,2.3.2,,,,,,,,2.4.5,3.0.0,,,SQL,,,,,0,BigEndian,correctness,,,,"While running tests on Apache Spark v2.3.2 with AdoptJDK on big endian, I am observing test failures for 2 Suites of Project SQL.
 1. InMemoryColumnarQuerySuite
 2. DataFrameTungstenSuite
 In both the cases test ""access only some column of the all of columns"" fails due to mismatch in the final assert.

Observed that the data obtained after df.cache() is causing the error. Please find attached the log with the details. 

cache() works perfectly fine if double and  float values are not in picture.

Inside test !!!!!!- access only some column of the all of columns *** FAILED ***","Linux Ubuntu 16.04 

openjdk version ""1.8.0_202""
OpenJDK Runtime Environment (build 1.8.0_202-b08)
Eclipse OpenJ9 VM (build openj9-0.12.1, JRE 1.8.0 64-Bit Compressed References 20190205_218 (JIT enabled, AOT enabled)
OpenJ9 - 90dd8cb40
OMR - d2f4534b
JCL - d002501a90 based on jdk8u202-b08)

 ",Anuja,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31703,,,,,,,,"25/Feb/19 10:45;Anuja;DataFrameTungstenSuite.txt;https://issues.apache.org/jira/secure/attachment/12960003/DataFrameTungstenSuite.txt","25/Feb/19 10:45;Anuja;InMemoryColumnarQuerySuite.txt;https://issues.apache.org/jira/secure/attachment/12960004/InMemoryColumnarQuerySuite.txt","26/Feb/19 10:09;Anuja;access only some column of the all of columns.txt;https://issues.apache.org/jira/secure/attachment/12960165/access+only+some+column+of+the+all+of+columns.txt",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 25 13:35:53 UTC 2019,,,,,,,,,,"0|z001mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/19 05:24;gurwls223;[~Anuja], I am sure there're multiple JIRAs related to big endian. Mind adding links to the JIRAs in this JIRA?;;;","01/Mar/19 21:02;srowen;Same as SPARK-26940; until it's reproducible on a standard JDK, I don't think it's at all clear it's not due to this custom JDK implementation. I don't see evidence it has to do with endian-ness.;;;","11/Mar/19 10:50;Anuja;I did tried with OpenJDK. However same behavior is observed. 

The test fails with the same error. ;;;","11/Mar/19 10:51;Anuja;I did tried with OpenJDK. However same behavior is observed. 

The test fails with the same error. ;;;","19/Mar/19 10:11;Anuja;Hi [~srowen], [~hyukjin.kwon]

I have observed that after changing the ByteOrder in _*[OnHeapColumnVector.java|https://github.com/apache/spark/blob/v2.3.2/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java]*_ to  *ByteOrder.BIG_ENDIAN* the tests passes.

Because the float and double data is read properly. However in that case some tests of Paraquet Module fails. e.x: *ParquetIOSuite.*

Is there any specific reason why we are using ByteOrder format as LITTLE_ENDIAN even when the bigEndianPlatform is true?

Because the above fix however, doesn't work on all the test cases and the behavior of *ParquetIOSuite and DataFrameTungsten/InMemoryColumnarQuerySuite* compliment each other. 

*ParquetIOSuite*  passes only when  ByteOrder  is set to ByteOrder.LITTLE_ENDIAN and *DataFrameTungsten/InMemoryColumnarQuerySuite* passes only when ByteOrder  is set to ByteOrder.BIG_ENDIAN.

 ;;;","07/Jun/19 05:52;apachespark;User 'ketank-new' has created a pull request for this issue:
https://github.com/apache/spark/pull/24788;;;","25/Jun/19 13:35;srowen;Resolved by https://github.com/apache/spark/pull/24861;;;",,,,,,,,,,,,,,,
Warn against subclassing scala.App doesn't work,SPARK-26977,13217569,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mauzhang,mauzhang,mauzhang,23/Feb/19 03:18,02/Mar/19 08:33,13/Jul/23 08:46,01/Mar/19 23:38,2.4.0,,,,,,,,3.0.0,,,,Spark Submit,,,,,0,,,,,,"As per discussion in [PR#3497|https://github.com/apache/spark/pull/3497#discussion_r258412735], the warn against subclassing scala.App doesn't work. For example,


{code:scala}
object Test extends scala.App {
   // spark code
}
{code}

Scala will compile {{object Test}} into two Java classes, {{Test}} passed in by user and {{Test$}} subclassing {{scala.App}}. Currect code checks against {{Test}}  and thus there will be no warn when user's application subclassing {{scala.App}}",,mauzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 01 23:38:53 UTC 2019,,,,,,,,,,"0|z0009c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/19 03:29;srowen;Sure, would you like to make a pull request?;;;","23/Feb/19 03:51;mauzhang;I'd love to;;;","01/Mar/19 23:38;srowen;Issue resolved by pull request 23903
[https://github.com/apache/spark/pull/23903];;;",,,,,,,,,,,,,,,,,,,
[Spark] Using ODBC not able to see the data in table when datatype is decimal,SPARK-26969,13217400,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,S71955,abhishek.akg,abhishek.akg,22/Feb/19 09:58,13/Aug/19 13:10,13/Jul/23 08:46,12/Aug/19 22:49,2.4.0,,,,,,,,3.0.0,,,,Spark Shell,,,,,0,,,,,,"{code}
#  Using odbc rpm file install odbc 
 # connect to odbc using isql -v spark2xsingle
 # SQL> create table t1_t(id decimal(15,2));
 # SQL> insert into t1_t values(15);
 # 
SQL> select * from t1_t;
+-------------------------+
| id |
+-------------------------+
+-------------------------+  Actual output is empty
{code}

Note: When creating table of int data type select is giving result as below
{code}
SQL> create table test_t1(id int);
SQL> insert into test_t1 values(10);
SQL> select * from test_t1;
+------------+
| id         |
+------------+
| 10         |
+------------+
{code}

Needs to handle for decimal case.

",,abhishek.akg,dongjoon,S71955,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 12 22:49:03 UTC 2019,,,,,,,,,,"0|yi19k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/19 10:37;S71955;i will further analyze the issue and raise a PR if required. thanks;;;","01/Mar/19 14:39;abhishek.akg;ODBC INSTALLATION 
Follow Below Steps :
1. Install hiveodbc client in cluster 
2. create directory  /opt/odbc and download 
hiveODBC client into /opt/odbc
3. run the following command
rpm -ivh <hiveodbc_client> --nodeps --force 
4. Install unixODBC server in cluster
5. Download unixODBC-origin-2.3.6.tar.gz into /opt/odbc 

6. Compile unixODBC
 ./configure --enable-gui=no --enable-readline=no -prefix=/opt/odbc /unixODBC-2.3.6/unixodbc
 make && make install

7. Configure respective odbc.ini file
[spark2xsingle]
DRIVER=/usr/lib64/libodbchive.so
MODE=0
HOST=xxxx( Thriftserver IP)
PORT=xxxxxxxx( Thriftserver Port)
DATABASE=default
[PRINCIPAL=spark/hadoop@HADOOP.COM|mailto:PRINCIPAL=spark/hadoop@HADOOP.COM]
FRAMED=0
NAMESPACE=sparkthriftserver2x

8.Add the odbc jars to the classPath

9.Run the following command to launch hive client
 isql –v spark2xsingle

 

 ;;;","12/Aug/19 22:49;dongjoon;Issue resolved by pull request 23899
[https://github.com/apache/spark/pull/23899];;;",,,,,,,,,,,,,,,,,,,
Found Java-level deadlock in Spark Driver,SPARK-26961,13217321,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ajithshetty,sol401430,sol401430,21/Feb/19 23:25,12/Dec/22 18:11,13/Jul/23 08:46,26/Mar/19 00:09,2.3.0,,,,,,,,2.3.4,2.4.2,3.0.0,,Spark Submit,,,,,0,,,,,,"Our spark job usually will finish in minutes, however, we recently found it take days to run, and we can only kill it when this happened.

An investigation show all worker container could not connect drive after start, and driver is hanging, using jstack, we found a Java-level deadlock.

 

*Jstack output for deadlock part is showing below:*

 

Found one Java-level deadlock:
=============================
""SparkUI-907"":
 waiting to lock monitor 0x00007f387761b398 (object 0x00000005c0c1e5e0, a org.apache.hadoop.conf.Configuration),
 which is held by ""ForkJoinPool-1-worker-57""
""ForkJoinPool-1-worker-57"":
 waiting to lock monitor 0x00007f3860574298 (object 0x00000005b7991168, a org.apache.spark.util.MutableURLClassLoader),
 which is held by ""ForkJoinPool-1-worker-7""
""ForkJoinPool-1-worker-7"":
 waiting to lock monitor 0x00007f387761b398 (object 0x00000005c0c1e5e0, a org.apache.hadoop.conf.Configuration),
 which is held by ""ForkJoinPool-1-worker-57""

Java stack information for the threads listed above:
===================================================
""SparkUI-907"":
 at org.apache.hadoop.conf.Configuration.getOverlay(Configuration.java:1328)
 - waiting to lock <0x00000005c0c1e5e0> (a org.apache.hadoop.conf.Configuration)
 at org.apache.hadoop.conf.Configuration.handleDeprecation(Configuration.java:684)
 at org.apache.hadoop.conf.Configuration.get(Configuration.java:1088)
 at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1145)
 at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2363)
 at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2840)
 at org.apache.hadoop.fs.FsUrlStreamHandlerFactory.createURLStreamHandler(FsUrlStreamHandlerFactory.java:74)
 at java.net.URL.getURLStreamHandler(URL.java:1142)
 at java.net.URL.<init>(URL.java:599)
 at java.net.URL.<init>(URL.java:490)
 at java.net.URL.<init>(URL.java:439)
 at org.apache.spark.ui.JettyUtils$$anon$4.doRequest(JettyUtils.scala:176)
 at org.apache.spark.ui.JettyUtils$$anon$4.doGet(JettyUtils.scala:161)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
 at org.spark_project.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
 at org.spark_project.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)
 at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:171)
 at org.spark_project.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
 at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
 at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
 at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
 at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
 at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
 at org.spark_project.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
 at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
 at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
 at org.spark_project.jetty.server.Server.handle(Server.java:534)
 at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:320)
 at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
 at org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
 at org.spark_project.jetty.io.FillInterest.fillable(FillInterest.java:108)
 at org.spark_project.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
 at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
 at org.spark_project.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
 at java.lang.Thread.run(Thread.java:748)
""ForkJoinPool-1-worker-57"":
 at java.lang.ClassLoader.loadClass(ClassLoader.java:404)
 - waiting to lock <0x00000005b7991168> (a org.apache.spark.util.MutableURLClassLoader)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
 at org.apache.xerces.parsers.ObjectFactory.findProviderClass(Unknown Source)
 at org.apache.xerces.parsers.ObjectFactory.newInstance(Unknown Source)
 at org.apache.xerces.parsers.ObjectFactory.createObject(Unknown Source)
 at org.apache.xerces.parsers.ObjectFactory.createObject(Unknown Source)
 at org.apache.xerces.parsers.DOMParser.<init>(Unknown Source)
 at org.apache.xerces.parsers.DOMParser.<init>(Unknown Source)
 at org.apache.xerces.jaxp.DocumentBuilderImpl.<init>(Unknown Source)
 at org.apache.xerces.jaxp.DocumentBuilderFactoryImpl.newDocumentBuilder(Unknown Source)
 at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2737)
 at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2696)
 at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2579)
 - locked <0x00000005c0c1e5e0> (a org.apache.hadoop.conf.Configuration)
 at org.apache.hadoop.conf.Configuration.get(Configuration.java:1091)
 at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1145)
 at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2363)
 at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2840)
 at org.apache.hadoop.fs.FsUrlStreamHandlerFactory.createURLStreamHandler(FsUrlStreamHandlerFactory.java:74)
 at java.net.URL.getURLStreamHandler(URL.java:1142)
 at java.net.URL.<init>(URL.java:599)
 at java.net.URL.<init>(URL.java:490)
 at java.net.URL.<init>(URL.java:439)
 at java.net.JarURLConnection.parseSpecs(JarURLConnection.java:175)
 at java.net.JarURLConnection.<init>(JarURLConnection.java:158)
 at sun.net.www.protocol.jar.JarURLConnection.<init>(JarURLConnection.java:81)
 at sun.net.www.protocol.jar.Handler.openConnection(Handler.java:41)
 at java.net.URL.openConnection(URL.java:979)
 at java.net.URL.openStream(URL.java:1045)
 at java.util.ServiceLoader.parse(ServiceLoader.java:304)
 at java.util.ServiceLoader.access$200(ServiceLoader.java:185)
 at java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:357)
 at java.util.ServiceLoader$LazyIterator.hasNext(ServiceLoader.java:393)
 at java.util.ServiceLoader$1.hasNext(ServiceLoader.java:474)
 at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)
 at scala.collection.Iterator$class.foreach(Iterator.scala:893)
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
 at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
 at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
 at scala.collection.TraversableLike$class.filterImpl(TraversableLike.scala:247)
 at scala.collection.TraversableLike$class.filter(TraversableLike.scala:259)
 at scala.collection.AbstractTraversable.filter(Traversable.scala:104)
 at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:614)
 at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190)
 at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)
 at com.amazon.economics.spark.dq.io.SanjuroDataRegistryData$class.readInstance(SanjuroDataRegistryData.scala:16)
 at com.amazon.economics.spark.dq.metrics.DifferenceRatioMetricSuite.readInstance(DifferenceRatioMetricSuite.scala:19)
 at com.amazon.economics.spark.dq.metrics.ComparisonStandardProvider$class.loadComparisonStandardInstance(ComparisonStandardProvider.scala:21)
 at com.amazon.economics.spark.dq.metrics.DifferenceRatioMetricSuite.loadComparisonStandardInstance(DifferenceRatioMetricSuite.scala:19)
 at com.amazon.economics.spark.dq.metrics.DifferenceRatioMetricSuite.compute(DifferenceRatioMetricSuite.scala:35)
 at com.amazon.economics.spark.dq.MetricsTaskRunner$$anonfun$run$1$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2.apply(MetricsTaskRunner.scala:41)
 at com.amazon.economics.spark.dq.MetricsTaskRunner$$anonfun$run$1$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2.apply(MetricsTaskRunner.scala:39)
 at scala.util.Try$.apply(Try.scala:192)
 at com.amazon.economics.spark.dq.MetricsTaskRunner$$anonfun$run$1$$anonfun$1$$anonfun$apply$1.apply(MetricsTaskRunner.scala:39)
 at com.amazon.economics.spark.dq.MetricsTaskRunner$$anonfun$run$1$$anonfun$1$$anonfun$apply$1.apply(MetricsTaskRunner.scala:32)
 at scala.collection.parallel.AugmentedIterableIterator$class.map2combiner(RemainsIterator.scala:115)
 at scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:62)
 at scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1054)
 at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)
 at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
 at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
 at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)
 at scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1051)
 at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)
 at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)
 at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)
 at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
 at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
 at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
 at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
""ForkJoinPool-1-worker-7"":
 at org.apache.hadoop.conf.Configuration.getOverlay(Configuration.java:1328)
 - waiting to lock <0x00000005c0c1e5e0> (a org.apache.hadoop.conf.Configuration)
 at org.apache.hadoop.conf.Configuration.handleDeprecation(Configuration.java:684)
 at org.apache.hadoop.conf.Configuration.get(Configuration.java:1088)
 at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1145)
 at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2363)
 at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2840)
 at org.apache.hadoop.fs.FsUrlStreamHandlerFactory.createURLStreamHandler(FsUrlStreamHandlerFactory.java:74)
 at java.net.URL.getURLStreamHandler(URL.java:1142)
 at java.net.URL.<init>(URL.java:420)
 at sun.misc.URLClassPath$JarLoader.<init>(URLClassPath.java:812)
 at sun.misc.URLClassPath$JarLoader$3.run(URLClassPath.java:1094)
 at sun.misc.URLClassPath$JarLoader$3.run(URLClassPath.java:1091)
 at java.security.AccessController.doPrivileged(Native Method)
 at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1090)
 at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1050)
 at sun.misc.URLClassPath.getResource(URLClassPath.java:239)
 at java.net.URLClassLoader$1.run(URLClassLoader.java:365)
 at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
 at java.security.AccessController.doPrivileged(Native Method)
 at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
 - locked <0x00000005b7991168> (a org.apache.spark.util.MutableURLClassLoader)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
 at java.lang.Class.forName0(Native Method)
 at java.lang.Class.forName(Class.java:348)
 at scala.reflect.runtime.JavaMirrors$JavaMirror.javaClass(JavaMirrors.scala:555)
 at scala.reflect.runtime.JavaMirrors$JavaMirror.tryJavaClass(JavaMirrors.scala:559)
 at scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply(SymbolLoaders.scala:137)
 at scala.reflect.runtime.SymbolLoaders$PackageScope$$anonfun$lookupEntry$1.apply(SymbolLoaders.scala:126)
 at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)
 at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)
 at scala.reflect.runtime.SymbolLoaders$PackageScope.syncLockSynchronized(SymbolLoaders.scala:124)
 at scala.reflect.runtime.SymbolLoaders$PackageScope.lookupEntry(SymbolLoaders.scala:126)
 at scala.reflect.internal.Types$Type.findDecl(Types.scala:971)
 at scala.reflect.internal.Types$Type.decl(Types.scala:566)
 at scala.reflect.internal.SymbolTable.openPackageModule(SymbolTable.scala:335)
 at scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply$mcV$sp(SymbolLoaders.scala:74)
 at scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply(SymbolLoaders.scala:71)
 at scala.reflect.runtime.SymbolLoaders$LazyPackageType$$anonfun$complete$2.apply(SymbolLoaders.scala:71)
 at scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan(SymbolTable.scala:263)
 at scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete(SymbolLoaders.scala:71)
 at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1514)
 at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(SynchronizedSymbols.scala:174)
 at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply(SynchronizedSymbols.scala:127)
 at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply(SynchronizedSymbols.scala:127)
 at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)
 at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)
 at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe(SynchronizedSymbols.scala:123)
 at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.gilSynchronizedIfNotThreadsafe(SynchronizedSymbols.scala:174)
 at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info(SynchronizedSymbols.scala:127)
 at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.info(SynchronizedSymbols.scala:174)
 at scala.reflect.internal.Types$TypeRef.thisInfo(Types.scala:2194)
 at scala.reflect.internal.Types$TypeRef.baseClasses(Types.scala:2199)
 at scala.reflect.internal.tpe.FindMembers$FindMemberBase.<init>(FindMembers.scala:17)
 at scala.reflect.internal.tpe.FindMembers$FindMember.<init>(FindMembers.scala:219)
 at scala.reflect.internal.Types$Type.scala$reflect$internal$Types$Type$$findMemberInternal$1(Types.scala:1014)
 at scala.reflect.internal.Types$Type.findMember(Types.scala:1016)
 at scala.reflect.internal.Types$Type.memberBasedOnName(Types.scala:631)
 at scala.reflect.internal.Types$Type.member(Types.scala:600)
 at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:48)
 at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:45)
 at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:45)
 at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:45)
 at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:45)
 at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:66)
 at scala.reflect.internal.Mirrors$RootsBase.staticModuleOrClass(Mirrors.scala:77)
 at scala.reflect.internal.Mirrors$RootsBase.staticModule(Mirrors.scala:161)
 at scala.reflect.internal.Mirrors$RootsBase.staticModule(Mirrors.scala:22)
 at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$schemaFor$1$$typecreator48$1.apply(ScalaReflection.scala:726)
 at scala.reflect.api.TypeTags$WeakTypeTagImpl.tpe$lzycompute(TypeTags.scala:232)
 - locked <0x00000005c0c47770> (a scala.reflect.api.TypeTags$TypeTagImpl)
 at scala.reflect.api.TypeTags$WeakTypeTagImpl.tpe(TypeTags.scala:232)
 at org.apache.spark.sql.catalyst.ScalaReflection$class.localTypeOf(ScalaReflection.scala:839)
 at org.apache.spark.sql.catalyst.ScalaReflection$.localTypeOf(ScalaReflection.scala:39)
 at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$schemaFor$1.apply(ScalaReflection.scala:726)
 at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$schemaFor$1.apply(ScalaReflection.scala:715)
 at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:56)
 at org.apache.spark.sql.catalyst.ScalaReflection$class.cleanUpReflectionObjects(ScalaReflection.scala:824)
 at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:39)
 at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:714)
 at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:711)
 at org.apache.spark.sql.functions$.udf(functions.scala:3356)
 at com.amazon.economics.spark.stata.port.RowTotal$.rowtotalOf(RowTotal.scala:28)
 at com.amazon.economics.spark.stata.port.RowTotal$.rowtotalInt$lzycompute(RowTotal.scala:61)
 - locked <0x00000005c0c47858> (a com.amazon.economics.spark.stata.port.RowTotal$)
 at com.amazon.economics.spark.stata.port.RowTotal$.rowtotalInt(RowTotal.scala:61)
 at com.amazon.economics.spark.stata.port.DataFrameRowtotalOps.<init>(RowTotal.scala:71)
 at com.amazon.economics.spark.stata.port.DataFrameRowtotalOps$.toDataFrameRowTotalOps(RowTotal.scala:140)
 at com.amazon.economics.spark.dq.metrics.DifferenceRatioMetricSuite$$anonfun$1.apply(DifferenceRatioMetricSuite.scala:29)
 at com.amazon.economics.spark.dq.metrics.DifferenceRatioMetricSuite$$anonfun$1.apply(DifferenceRatioMetricSuite.scala:29)
 at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
 at scala.collection.immutable.List.foldLeft(List.scala:84)
 at com.amazon.economics.spark.dq.metrics.DifferenceRatioMetricSuite.compute(DifferenceRatioMetricSuite.scala:29)
 at com.amazon.economics.spark.dq.MetricsTaskRunner$$anonfun$run$1$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2.apply(MetricsTaskRunner.scala:41)
 at com.amazon.economics.spark.dq.MetricsTaskRunner$$anonfun$run$1$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2.apply(MetricsTaskRunner.scala:39)
 at scala.util.Try$.apply(Try.scala:192)
 at com.amazon.economics.spark.dq.MetricsTaskRunner$$anonfun$run$1$$anonfun$1$$anonfun$apply$1.apply(MetricsTaskRunner.scala:39)
 at com.amazon.economics.spark.dq.MetricsTaskRunner$$anonfun$run$1$$anonfun$1$$anonfun$apply$1.apply(MetricsTaskRunner.scala:32)
 at scala.collection.parallel.AugmentedIterableIterator$class.map2combiner(RemainsIterator.scala:115)
 at scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:62)
 at scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1054)
 at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)
 at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
 at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
 at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)
 at scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1051)
 at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)
 at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)
 at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)
 at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
 at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
 at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
 at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

Found 1 deadlock.

 ",,ajithshetty,kabhwan,maropu,sol401430,xsapphire,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26587,,,,,,,HADOOP-16159,,,SPARK-26587,SPARK-39094,,,"13/Mar/19 14:23;ajithshetty;image-2019-03-13-19-53-52-390.png;https://issues.apache.org/jira/secure/attachment/12962345/image-2019-03-13-19-53-52-390.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 26 17:29:33 UTC 2021,,,,,,,,,,"0|yi192w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/19 10:23;gurwls223;How did this happen? Would you be able to provide a reproducer and/or narrow down this further?;;;","28/Feb/19 13:49;ajithshetty;I think the root cause is [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala#L185] 

URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory())

 

This induces the thread lock, this may be bug in a non spark env also

Thread 1 : Does load class which will do below
 - waiting to lock <0x00000005c0c1e5e0> (a org.apache.hadoop.conf.Configuration)
at org.apache.hadoop.conf.Configuration.handleDeprecation(Configuration.java:684)
at org.apache.hadoop.conf.Configuration.get(Configuration.java:1088)
at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1145)
at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2363)
at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2840)
at org.apache.hadoop.fs.FsUrlStreamHandlerFactory.createURLStreamHandler(FsUrlStreamHandlerFactory.java:74)
at java.net.URL.getURLStreamHandler(URL.java:1142)
at java.net.URL.<init>(URL.java:420)
at sun.misc.URLClassPath$JarLoader.<init>(URLClassPath.java:812)
at sun.misc.URLClassPath$JarLoader$3.run(URLClassPath.java:1094)
at sun.misc.URLClassPath$JarLoader$3.run(URLClassPath.java:1091)
at java.security.AccessController.doPrivileged(Native Method)
at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1090)
at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1050)
at sun.misc.URLClassPath.getResource(URLClassPath.java:239)
at java.net.URLClassLoader$1.run(URLClassLoader.java:365)
at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
 - locked <0x00000005b7991168> (a org.apache.spark.util.MutableURLClassLoader)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)  

Thread 2 : Create new URL
 - waiting to lock <0x00000005b7991168> (a org.apache.spark.util.MutableURLClassLoader)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at org.apache.xerces.parsers.ObjectFactory.findProviderClass(Unknown Source)
at org.apache.xerces.parsers.ObjectFactory.newInstance(Unknown Source)
at org.apache.xerces.parsers.ObjectFactory.createObject(Unknown Source)
at org.apache.xerces.parsers.ObjectFactory.createObject(Unknown Source)
at org.apache.xerces.parsers.DOMParser.<init>(Unknown Source)
at org.apache.xerces.parsers.DOMParser.<init>(Unknown Source)
at org.apache.xerces.jaxp.DocumentBuilderImpl.<init>(Unknown Source)
at org.apache.xerces.jaxp.DocumentBuilderFactoryImpl.newDocumentBuilder(Unknown Source)
at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2737)
at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2696)
at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2579)
 - locked <0x00000005c0c1e5e0> (a org.apache.hadoop.conf.Configuration)
at org.apache.hadoop.conf.Configuration.get(Configuration.java:1091)
at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1145)
at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2363)
at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2840)
at org.apache.hadoop.fs.FsUrlStreamHandlerFactory.createURLStreamHandler(FsUrlStreamHandlerFactory.java:74)
at java.net.URL.getURLStreamHandler(URL.java:1142)
at java.net.URL.<init>(URL.java:599);;;","28/Feb/19 17:39;sol401430;Hi 

Thanks for the response, from our current investigation, we suspect the deadlock happened when 2 thread in drive process doing
 # one thread try to use DataSource' lookupDataSource() function. e.g. in the deadlock stack info:  org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:614)
 # another thread call java.lang.Class.forName().  e.g. in deadlock stack info:  locked <0x00000005b7991168> (a org.apache.spark.util.MutableURLClassLoader)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)

 

Both call need to lock MutableURLClassLoader and a org.apache.hadoop.conf.Configuration, but they are doing so in different order.;;;","01/Mar/19 08:31;kabhwan;Which distribution of Spark are you using? Specifically the Hadoop version since Configuration has been changed between 2.6 and 2.7.;;;","03/Mar/19 09:33;xsapphire;Hi there,

I'm one of Jialei's coworkers. By using prototype [https://gist.github.com/Fruiter/159f82f9e5ff554a348f1a21263f8748] I can consistently reproduce the problem. I have tested on single machine with env:

spark 2.3.0 + hadoop 2.8.1

spark 2.4.0 + hadoop 3.2.0

As mentioned by Ajith, the problem is caused by the global FsUrlStreamHandlerFactory instance. And there could be a loop such as ClassLoader -> FsUrlStreamHandlerFactory -> Configuration.;;;","04/Mar/19 10:44;ajithshetty;The problem is here org.apache.spark.util.MutableURLClassLoader (entire classloader) is getting locked.
{code:java}
at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
locked <0x00000005b7991168> (a org.apache.spark.util.MutableURLClassLoader)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348){code}
Checking javadoc of java.lang.ClassLoader#getClassLoadingLock we see that 
{code:java}
* If this ClassLoader object is registered as
* parallel capable, the method returns a dedicated object associated
* with the specified class name. Otherwise, the method returns this
* ClassLoader object{code}
Here we see for every class loading, a new object is created so it doesn't lock entire classloader itself if classloader is registered as ClassLoader.registerAsParallelCapable();

Even though MutableURLClassLoader/NonClosableMutableURLClassLoader/ChildFirstURLClassLoader are subclass of java.net.URLClassLoader, they will need to explicitly do {color:#FF0000}*ClassLoader.registerAsParallelCapable()*{color} (java.net.URLClassLoader does it in its static block)


May be this would avoid the deadlock by making locks more granular.? any thoughts.?;;;","04/Mar/19 19:50;xsapphire;Hi Ajith,

 

IMHO ClassLoader.registerAsParallelCapable() only helps to reduce the granularity of the lock. The lock will still be shared by loadClass calls with same ""className"". Theoretically deadlock can still be triggered in certain cases.;;;","11/Mar/19 05:53;ajithshetty;[~xsapphire] i agree with your opinion.

 

cc [~hyukjin.kwon] [~cloud_fan] [~srowen]

I do not see a clear way to fix this without changing org.apache.hadoop.fs.FsUrlStreamHandlerFactory.createURLStreamHandler ( or providing a spark's very own version of UrlStreamHandlerFactory where classloader can be locked before createURLStreamHandler)

What you suggest.?;;;","11/Mar/19 14:00;srowen;I think registerAsParallelCapable() sounds like it could resolve the actual issue in practice here. Let's do that, because these ClassLoader subclasses are called concurrently in some cases. We probably can't fix Hadoop here.

Looks like there are 4-5 implementations in Spark. I think we can address all of them. It would have to go in a companion object to these classes, in Scala. I don't see a downside here other than that the locking is potentially more expensive as it's finer-grained?;;;","11/Mar/19 18:04;xsapphire;I think the problem could be fixed from a more general prospect if there is a way to read Configuration without updating its content. That should help to ensure accessing Configuration object without triggering class loader. I'm neither familiar with Hadoop nor Spark. So I don't know if that's a practical solution or not.

I don't know how far will registerAsParallelCapable affect the whole system. Since it will cause storing one lock for each loaded class. I'm not sure how large will that overhead be.;;;","12/Mar/19 04:10;ajithshetty;[~srowen] Yes. I too have same opinion of fixing it via registerAsParallelCapable.  But Its not possible to do via Companion Object. I Tried and found a issue. Refer [https://github.com/scala/bug/issues/11429]

May be we need to move them to java implementation from scala to achieve this

[~xsapphire] i think these class loaders are child classloaders of LaunchAppClassLoader which already has classes for jar in class path. So overhead may not be of higher magnitude;;;","13/Mar/19 12:16;srowen;[~ajithshetty] I see, OK. It could happen in the class itself, in the constructor. The calls after the first would do nothing. ;;;","13/Mar/19 13:42;ajithshetty;[~srowen] That too will not work

Here is my custom classloader
{code:java}
class MYClassLoader(urls: Array[URL], parent: ClassLoader)
  extends URLClassLoader(urls, parent) {

  ClassLoader.registerAsParallelCapable()

  override def loadClass(name: String): Class[_] = {
    super.loadClass(name)
  }
}
{code}
If we see class initialization flow, we see that super constructor is called before ClassLoader.registerAsParallelCapable() line is hit, hence it doesn't take effect 
{code:java}
<init>:280, ClassLoader (java.lang)
<init>:316, ClassLoader (java.lang)
<init>:76, SecureClassLoader (java.security)
<init>:100, URLClassLoader (java.net)
<init>:23, MYClassLoader (org.apache.spark.util.ajith)
{code}
as per [https://github.com/scala/bug/issues/11429] scala 2.x do not have a pure static support yet. So moving classloader to a java based implementation may be only option we have;;;","13/Mar/19 14:11;srowen;When I run your class and print the result of registerAsParallelCapable, it returns true. Yes, parent initialization happens first, but URLClassLoader is also parallel capable.;;;","13/Mar/19 14:25;ajithshetty;1) Yes, the registerAsParallelCapable will return true, but if you inspect the classloader instance, parallelLockMap is still null as it was already initalized via super class constructor. so *it has no effect for already created instance*

!image-2019-03-13-19-53-52-390.png!

 

2) URLClassLoader is parallel capable as it does registration in static block which is before calling parent(ClassLoader) constructor. Also as per javadoc

[https://docs.oracle.com/javase/8/docs/api/java/lang/ClassLoader.html]
{code:java}
Note that the ClassLoader class is registered as parallel capable by default. However, its subclasses still need to register themselves if they are parallel capable. {code}
Hence MutableURLClassLoader lost its parallel capability by failing to register unlike URLClassLoader

 ;;;","16/Mar/19 16:17;srowen;Hm! OK, I believe it. That almost seems like a bug in the JDK. I don't see a way around it right now.;;;","17/Mar/19 03:20;xsapphire;copy or create a new Configuration every time in [https://github.com/facebookarchive/hadoop-20/blob/master/src/core/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java#L64] should also help. But that's part of hadoop. Not sure if it's a good fix.;;;","17/Mar/19 15:50;ajithshetty;[~xsapphire] nothing better if it can be fixed in hadoop library, but i doubt that.

[~srowen] can we have a java based classloader implementation for (MutableURLClassLoader/NonClosableMutableURLClassLoader/ChildFirstURLClassLoader) which can enable parallel capability and reduce the window of this deadlock from happening.?

 ;;;","17/Mar/19 16:43;srowen;Yes, I think implementing in Java is viable just for this reason. At least for MutableURLClassLoader.;;;","17/Mar/19 17:41;ajithshetty;[~srowen] ok, will raise a PR for this. Thanks;;;","26/Mar/19 00:09;srowen;Issue resolved by pull request 24126
[https://github.com/apache/spark/pull/24126];;;","26/Jan/21 17:29;ajithshetty;Scala issue : https://github.com/scala/bug/issues/11429;;;"
Make RandomDataGenerator use Float.NaN or Double.NaN for all NaN values,SPARK-26950,13217108,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,21/Feb/19 07:33,22/Feb/19 21:55,13/Jul/23 08:46,22/Feb/19 04:28,2.3.4,2.4.2,3.0.0,,,,,,2.3.4,2.4.1,3.0.0,,SQL,Tests,,,,0,,,,,,"Apache Spark uses the predefined `Float.NaN` and `Double.NaN` for NaN values, but there exists more NaN values with different binary presentations.

{code}
scala> java.nio.ByteBuffer.allocate(4).putFloat(Float.NaN).array
res1: Array[Byte] = Array(127, -64, 0, 0)

scala> val x = java.lang.Float.intBitsToFloat(-6966608)
x: Float = NaN

scala> java.nio.ByteBuffer.allocate(4).putFloat(x).array
res2: Array[Byte] = Array(-1, -107, -78, -80)
{code}

`RandomDataGenerator` generates these NaN values. It's good, but it causes `checkEvaluationWithUnsafeProjection` failures due to the difference between `UnsafeRow` binary presentation. The following is the UT failure instance. This issue aims to fix this flakiness.

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/102528/testReport/

{code}
Failed
org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct<col_0:decimal(16,11),col_1:float,col_2:decimal(38,0),col_3:decimal(38,0),col_4:string> with seed -81044812370056695
{code}",,cloud_fan,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 22 04:28:28 UTC 2019,,,,,,,,,,"0|yi17rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/19 04:28;cloud_fan;Issue resolved by pull request 23851
[https://github.com/apache/spark/pull/23851];;;",,,,,,,,,,,,,,,,,,,,,
Python streaming tests flaky while cleaning temp directories after StreamingQuery.stop,SPARK-26945,13216964,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,abellina,abellina,20/Feb/19 15:37,12/Dec/22 18:11,13/Jul/23 08:46,23/Feb/19 06:58,2.4.0,,,,,,,,3.0.0,,,,PySpark,,,,,0,,,,,,"From the test code, it seems like the `shmutil.rmtree` function is trying to delete a directory, but there's likely another thread adding entries to a directory, so when it gets to `os.rmdir(path)` it blows up. I think the test (and other streaming tests) should call `q.awaitTermination` after `q.stop`, before going on. I'll file a separate jira.
{noformat}
ERROR: test_query_manager_await_termination (pyspark.sql.tests.test_streaming.StreamingTests)
----------------------------------------------------------------------
Traceback (most recent call last):
 File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/sql/tests/test_streaming.py"", line 259, in test_query_manager_await_termination
 shutil.rmtree(tmpPath)
 File ""/home/anaconda/lib/python2.7/shutil.py"", line 256, in rmtree
 onerror(os.rmdir, path, sys.exc_info())
 File ""/home/anaconda/lib/python2.7/shutil.py"", line 254, in rmtree
 os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/home/jenkins/workspace/SparkPullRequestBuilder/python/target/072153bd-f981-47be-bda2-e2b657a16f65/tmp4WGp7n'{noformat}",,abellina,bryanc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 23 07:01:28 UTC 2019,,,,,,,,,,"0|yi16vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/19 13:15;abellina;[~hyukjin.kwon] thanks for taking a look. Seems like q.processAllAvailable is designed for this use case.;;;","23/Feb/19 06:58;gurwls223;Issue resolved by pull request 23870
[https://github.com/apache/spark/pull/23870];;;","23/Feb/19 07:01;gurwls223;Thanks for reporting this, [~abellina];;;",,,,,,,,,,,,,,,,,,,
incorrect computation of maxNumExecutorFailures in ApplicationMaster for streaming ,SPARK-26941,13216887,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,liupengcheng,liupengcheng,liupengcheng,20/Feb/19 11:26,17/Mar/19 00:47,13/Jul/23 08:46,17/Mar/19 00:45,2.1.0,2.4.0,,,,,,,3.0.0,,,,Spark Core,YARN,,,,0,,,,,,"Currently, when enabled streaming dynamic allocation for streaming applications, the maxNumExecutorFailures in ApplicationMaster is still computed with `spark.dynamicAllocation.maxExecutors`. 

Actually, we should consider `spark.streaming.dynamicAllocation.maxExecutors` instead.

Related codes:
{code:java}
private val maxNumExecutorFailures = {
  val effectiveNumExecutors =
    if (Utils.isStreamingDynamicAllocationEnabled(sparkConf)) {
      sparkConf.get(STREAMING_DYN_ALLOCATION_MAX_EXECUTORS)
    } else if (Utils.isDynamicAllocationEnabled(sparkConf)) {
      sparkConf.get(DYN_ALLOCATION_MAX_EXECUTORS)
    } else {
      sparkConf.get(EXECUTOR_INSTANCES).getOrElse(0)
    }
  // By default, effectiveNumExecutors is Int.MaxValue if dynamic allocation is enabled. We need
  // avoid the integer overflow here.
  val defaultMaxNumExecutorFailures = math.max(3,
    if (effectiveNumExecutors > Int.MaxValue / 2) Int.MaxValue else (2 * effectiveNumExecutors))

  sparkConf.get(MAX_EXECUTOR_FAILURES).getOrElse(defaultMaxNumExecutorFailures)
{code}",,liupengcheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 17 00:45:31 UTC 2019,,,,,,,,,,"0|yi16eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/19 00:45;srowen;Issue resolved by pull request 23845
[https://github.com/apache/spark/pull/23845];;;",,,,,,,,,,,,,,,,,,,,,
"On yarn-client mode, insert overwrite local directory can not create temporary path in local staging directory",SPARK-26936,13216796,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,beliefer,beliefer,beliefer,20/Feb/19 02:43,30/Apr/19 02:49,13/Jul/23 08:46,05/Apr/19 19:03,2.3.0,2.4.0,3.0.0,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"Let me introduce bug of  'insert overwrite local directory'.

If I execute the SQL mentioned before, a HiveException will appear as follows:
{code:java}
Driver stacktrace:
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
at scala.Option.foreach(Option.scala:257)
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2037)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:194)
... 36 more
Caused by: org.apache.spark.SparkException: Task failed while writing rows.
at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
at org.apache.spark.scheduler.Task.run(Task.scala:109)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Mkdirs failed to create file:/home/xitong/hive/stagingdir_hive_2019-02-19_17-31-00_678_1816816774691551856-1/-ext-10000/_temporary/0/_temporary/attempt_20190219173233_0002_m_000000_3 (exists=false, cwd=file:/data10/yarn/nm-local-dir/usercache/xitong/appcache/application_1543893582405_6126857/container_e124_1543893582405_6126857_01_000011)
at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:249)
at org.apache.spark.sql.hive.execution.HiveOutputWriter.<init>(HiveFileFormat.scala:123)
at org.apache.spark.sql.hive.execution.HiveFileFormat$$anon$1.newInstance(HiveFileFormat.scala:103)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
... 8 more
Caused by: java.io.IOException: Mkdirs failed to create file:/home/xitong/hive/stagingdir_hive_2019-02-19_17-31-00_678_1816816774691551856-1/-ext-10000/_temporary/0/_temporary/attempt_20190219173233_0002_m_000000_3 (exists=false, cwd=file:/data10/yarn/nm-local-dir/usercache/xitong/appcache/application_1543893582405_6126857/container_e124_1543893582405_6126857_01_000011)
at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:449)
at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:435)
at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:928)
at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:821)
at org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.getHiveRecordWriter(HiveIgnoreKeyTextOutputFormat.java:80)
at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:261)
at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:246)
... 16 more
{code}",,beliefer,Chopinxb,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27140,,,,,,,,,,SPARK-26596,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 05 19:03:49 UTC 2019,,,,,,,,,,"0|yi15ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/19 19:03;srowen;Issue resolved by pull request 23841
[https://github.com/apache/spark/pull/23841];;;",,,,,,,,,,,,,,,,,,,,,
Tests in ParquetFilterSuite don't verify filter class,SPARK-26930,13216603,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nkollar,nkollar,nkollar,19/Feb/19 11:21,12/Dec/22 18:11,13/Jul/23 08:46,22/Feb/19 06:09,2.4.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"While investigating Parquet predicate pushdown test cases, I noticed that several tests seems to be broken, they don't test what they were originally intended to. Most of the verification ends up in one of the overloaded checkFilterPredicate functions, which supposed to test if a given filter class is generated or not with this call: {{maybeFilter.exists(_.getClass === filterClass)}}, but on one side an assert is missing from here, on the other side, the filters are more complicated, for example equality is checked with an 'and' wrapping not null check along with an equality check for the given value. 'Exists' function call won't help with these compounds filters, since they are not collection instances.",,nkollar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 22 06:09:45 UTC 2019,,,,,,,,,,"0|yi14nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/19 06:55;gurwls223;Sorry but can you show some codes? It's difficult for me to parse the description :). Also, technically it's not broken if it doesn't test something. Please fix the JIRA title.;;;","20/Feb/19 09:15;gurwls223;Ah, gotya {{maybeFilter.exists(_.getClass === filterClass)}} doesn't check anything and {{assert}} should be added in {{ParquetFilterSuite.scala}} s 117 line;;;","20/Feb/19 09:28;nkollar;Thanks [~hyukjin.kwon] for taking a look at this Jira. That's correct, an assert is missing from there, but an other problem is, that if I put it there, the tests will fail. They won't fail because of a bug in the production code, but because the expression to test for are not just simple eq, lt etc. predicated, but more complex expressions like and(eq, lt). I guess the intent with that exists call was to find the relevant class in this expression, but it seems that optional's exists doesn't do this, one should manually iterate through the expression tree. And sorry about the ambiguous description! :) Anyway I don't think this is a serious issue, though it would be nice to improve these tests.;;;","20/Feb/19 09:32;gurwls223;Yea, {{IsNotNull}} will be inserted (see also https://github.com/apache/spark/commit/ef77003178eb5cdcb4fe519fc540917656c5d577). Looks we should fix anyway.;;;","20/Feb/19 09:44;nkollar;What do you think is the better approach? Test for the entire expression (verify for and(null check, filter)), or just simply search for the class in the expression tree (I think the relevant filter should be somewhere in the leaf nodes).;;;","20/Feb/19 09:45;gurwls223;I am not sure which way will be minimised and simple way. Since it's a test code, any minimised way would be preferred.;;;","20/Feb/19 09:48;nkollar;I don't know either, but I feel that the second option might be simpler to implement. I can give a shot.;;;","22/Feb/19 06:09;gurwls223;Issue resolved by pull request 23855
[https://github.com/apache/spark/pull/23855];;;",,,,,,,,,,,,,,
table owner should use user instead of principal when use kerberos,SPARK-26929,13216562,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hongdongdong,hongdongdong,hongdongdong,19/Feb/19 09:10,16/Sep/19 19:37,13/Jul/23 08:46,16/Sep/19 18:08,2.3.0,2.3.1,2.3.2,2.3.3,2.4.0,,,,3.0.0,,,,SQL,,,,,0,,,,,,"In kerberos cluster, when use spark-sql or beeline to create table,  the owner will be whole info of principal. the _issue_  was fixed in SPARK-19970 and modify by SPARK-22846, so it occur again. It will causes some problems when using role., and this time should resolved two issues together.

Use  org.apache.hadoop.hive.shims.Utils.getUGI  directly to get ugi.getShortUserName

instead of use  conf.getUser which return principal info.

Code change
{code:java}
private val userName: String = try {
val ugi = HiveUtils.getUGI
ugi.getShortUserName
} catch {
case e: LoginException => throw new IOException(e)
}
{code}
Berfore

{code}
scala> sql(""create table t(a int)"").show
 scala> sql(""desc formatted t"").show(false)
 ...
|Owner:|spark@EXAMPLE.COM| |
{code}

After:

{code}
 scala> sql(""create table t(a int)"").show
 scala> sql(""desc formatted t"").show(false)
 ...
|Owner:|spark| |
{code}
",,hongdongdong,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28559,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 16 18:08:18 UTC 2019,,,,,,,,,,"0|yi14eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/19 02:25;hongdongdong;cc [~dongjoon], could you help to review PR [#23952|https://github.com/apache/spark/pull/23952].;;;","16/Sep/19 18:08;vanzin;Issue resolved by pull request 23952
[https://github.com/apache/spark/pull/23952];;;",,,,,,,,,,,,,,,,,,,,
Race condition may cause dynamic allocation not working,SPARK-26927,13216559,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liupengcheng,liupengcheng,liupengcheng,19/Feb/19 09:00,23/Mar/19 17:42,13/Jul/23 08:46,12/Mar/19 21:27,2.1.0,2.4.0,,,,,,,2.3.4,2.4.1,3.0.0,,Spark Core,,,,,0,,,,,,"Recently, we catch a bug that caused our production spark thriftserver hangs:

There is a race condition in the ExecutorAllocationManager that the `SparkListenerExecutorRemoved` event is posted before the `SparkListenerTaskStart` event, which will cause the incorrect result of `executorIds`, then when some executor idles, the real executors will be removed even executor number is equal to `minNumExecutors` due to the incorrect computation of `newExecutorTotal`(may greater than the `minNumExecutors`), thus may finally causing zero available executors but a wrong number of executorIds was kept in memory.

What's more, even the `SparkListenerTaskEnd` event can not make the fake `executorIds` released, because later idle event for the fake executors can not cause the real removal of these executors, as they are already removed and they are not exist in the `executorDataMap`  of `CoaseGrainedSchedulerBackend`.

Logs:

!Selection_042.jpg!

!Selection_043.jpg!

!Selection_044.jpg!

!Selection_045.jpg!

!Selection_046.jpg!  

EventLogs(DisOrder of events):
{code:java}
{""Event"":""SparkListenerExecutorRemoved"",""Timestamp"":1549936077543,""Executor ID"":""131"",""Removed Reason"":""Container container_e28_1547530852233_236191_02_000180 exited from explicit termination request.""}

{""Event"":""SparkListenerTaskStart"",""Stage ID"":136689,""Stage Attempt ID"":0,""Task Info"":{""Task ID"":448048,""Index"":2,""Attempt"":0,""Launch Time"":1549936032872,""Executor ID"":""131"",""Host"":""mb2-hadoop-prc-st474.awsind"",""Locality"":""RACK_LOCAL"", ""Speculative"":false,""Getting Result Time"":0,""Finish Time"":1549936032906,""Failed"":false,""Killed"":false,""Accumulables"":[{""ID"":12923945,""Name"":""internal.metrics.executorDeserializeTime"",""Update"":10,""Value"":13,""Internal"":true,""Count Faile d Values"":true},{""ID"":12923946,""Name"":""internal.metrics.executorDeserializeCpuTime"",""Update"":2244016,""Value"":4286494,""Internal"":true,""Count Failed Values"":true},{""ID"":12923947,""Name"":""internal.metrics.executorRunTime"",""Update"":20,""Val ue"":39,""Internal"":true,""Count Failed Values"":true},{""ID"":12923948,""Name"":""internal.metrics.executorCpuTime"",""Update"":13412614,""Value"":26759061,""Internal"":true,""Count Failed Values"":true},{""ID"":12923949,""Name"":""internal.metrics.resultS ize"",""Update"":3578,""Value"":7156,""Internal"":true,""Count Failed Values"":true},{""ID"":12923954,""Name"":""internal.metrics.peakExecutionMemory"",""Update"":33816576,""Value"":67633152,""Internal"":true,""Count Failed Values"":true},{""ID"":12923962,""Na me"":""internal.metrics.shuffle.write.bytesWritten"",""Update"":1367,""Value"":2774,""Internal"":true,""Count Failed Values"":true},{""ID"":12923963,""Name"":""internal.metrics.shuffle.write.recordsWritten"",""Update"":23,""Value"":45,""Internal"":true,""Cou nt Failed Values"":true},{""ID"":12923964,""Name"":""internal.metrics.shuffle.write.writeTime"",""Update"":3259051,""Value"":6858121,""Internal"":true,""Count Failed Values"":true},{""ID"":12921550,""Name"":""number of output rows"",""Update"":""158"",""Value"" :""289"",""Internal"":true,""Count Failed Values"":true,""Metadata"":""sql""},{""ID"":12921546,""Name"":""number of output rows"",""Update"":""23"",""Value"":""45"",""Internal"":true,""Count Failed Values"":true,""Metadata"":""sql""},{""ID"":12921547,""Name"":""peak memo ry total (min, med, max)"",""Update"":""33816575"",""Value"":""67633149"",""Internal"":true,""Count Failed Values"":true,""Metadata"":""sql""},{""ID"":12921541,""Name"":""data size total (min, med, max)"",""Update"":""551"",""Value"":""1077"",""Internal"":true,""Count Failed Values"":true,""Metadata"":""sql""}]}}

{code}
 ",,liupengcheng,Ngone51,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/19 09:00;liupengcheng;Selection_042.jpg;https://issues.apache.org/jira/secure/attachment/12959215/Selection_042.jpg","19/Feb/19 09:01;liupengcheng;Selection_043.jpg;https://issues.apache.org/jira/secure/attachment/12959216/Selection_043.jpg","19/Feb/19 09:01;liupengcheng;Selection_044.jpg;https://issues.apache.org/jira/secure/attachment/12959217/Selection_044.jpg","19/Feb/19 09:01;liupengcheng;Selection_045.jpg;https://issues.apache.org/jira/secure/attachment/12959218/Selection_045.jpg","19/Feb/19 09:02;liupengcheng;Selection_046.jpg;https://issues.apache.org/jira/secure/attachment/12959219/Selection_046.jpg",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 12 21:27:41 UTC 2019,,,,,,,,,,"0|yi14ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/19 16:17;Ngone51;[~liupengcheng] I can not understand the issue clearly by your desc. Can you elaborate it with a more simple and concrete example ?;;;","26/Feb/19 11:36;liupengcheng;[~Ngone51]

Let's say we got the following dynamic allocation settings: 

min: 20 initial: 20 max 50
 # we finished 50 tasks on 50 executors, and no more task to execute, thus 50 executors will idle, then allocationManager will try to remove the 50 idle executors, if everything goes well, with the min number of executors guards(20), allocationManager will keep 20 executors not killed.
 # However, imagine such a case: when the `SparkListenerExecutorRemoved` comes before the `SparkListenerTaskStart`. – It's possible because the `SparkListenerTaskStart` event is posted by `DAGSchedulerEventLoop` thread, but the `SparkListenerExecutorRemoved` event is posted by `Netty` threads. 

          In this case, we might get a wrong number of `executorIds` due to the following logic: [https://github.com/apache/spark/blob/bc03c8b3faacd23edf40b8e75ffd9abb5881c50c/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala#L718]

         Explain:  allocationManager.executorIds does not contains executorId because it's removed in the onExecutorRemoved callback. so the removed executorId will be readded to the allocationManager.executorIds.

       3. Then, allocationManager now may think it has already got 21 or more executors, then we submit 20 tasks on 20 executors, then finish and idle. At this time, the allocationManager will not keep 20 min number of executors not removed, it remove 1 or more executors.

       4. so forth and back. .....

       5. Finally, there might be no alive executors, but allocationManager still think it has kept more than min number of executors. An extrame case is the wrong number is greater than the max number of executors, so allocationManager will never schedule more executors and the application will hangs forever.;;;","26/Feb/19 15:22;Ngone51;I got it, thank you.;;;","12/Mar/19 21:27;vanzin;Issue resolved by pull request 23842
[https://github.com/apache/spark/pull/23842];;;",,,,,,,,,,,,,,,,,,
ThriftServer scheduler pool may be unpredictably when using fair schedule mode,SPARK-26914,13216379,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cane,cane,cane,18/Feb/19 11:38,01/Apr/19 17:17,13/Jul/23 08:46,28/Mar/19 14:25,2.4.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"When using fair scheduler mode for thrift server, we may have unpredictable result.

{code:java}
val pool = sessionToActivePool.get(parentSession.getSessionHandle)
    if (pool != null) {
      sqlContext.sparkContext.setLocalProperty(SparkContext.SPARK_SCHEDULER_POOL, pool)
    }
{code}

Here is an example:
We have some query will use default pool, however it submit to 'normal' pool.
 !26914-02.png! 

I changed code and add some log.Got some strange result.
 !26914-01.png! 
 !26914-03.png! 

Then i found out that the localProperties of SparkContext may has unpredictable result when call setLocalProperty. And since thriftserver use thread pool to execute queries, it will trigger this bug sometimes.

{code:java}
/**
   * Set a local property that affects jobs submitted from this thread, such as the Spark fair
   * scheduler pool. User-defined properties may also be set here. These properties are propagated
   * through to worker tasks and can be accessed there via
   * [[org.apache.spark.TaskContext#getLocalProperty]].
   *
   * These properties are inherited by child threads spawned from this thread. This
   * may have unexpected consequences when working with thread pools. The standard java
   * implementation of thread pools have worker threads spawn other worker threads.
   * As a result, local properties may propagate unpredictably.
   */
  def setLocalProperty(key: String, value: String) {
    if (value == null) {
      localProperties.get.remove(key)
    } else {
      localProperties.get.setProperty(key, value)
    }
  }
{code}
   
",,cane,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26992,,,,,,,,,,,,,,"18/Feb/19 11:40;cane;26914-01.png;https://issues.apache.org/jira/secure/attachment/12959098/26914-01.png","18/Feb/19 11:40;cane;26914-02.png;https://issues.apache.org/jira/secure/attachment/12959099/26914-02.png","18/Feb/19 11:40;cane;26914-03.png;https://issues.apache.org/jira/secure/attachment/12959100/26914-03.png",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 28 14:25:18 UTC 2019,,,,,,,,,,"0|yi13ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/19 14:25;srowen;Resolved by https://github.com/apache/spark;;;",,,,,,,,,,,,,,,,,,,,,
use unsafeRow.hashCode() as hash value in HashAggregate,SPARK-26909,13216316,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yucai,yucai,yucai,18/Feb/19 08:00,19/Feb/19 05:11,13/Jul/23 08:46,19/Feb/19 05:11,2.4.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"This is a followup PR for #21149.

New way uses unsafeRow.hashCode() as hash value in HashAggregate.
The unsafe row has [null bit set] etc., the result should be different, so we don't need weird `48`.",,cloud_fan,yucai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 19 05:11:59 UTC 2019,,,,,,,,,,"0|yi12x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/19 05:11;cloud_fan;Issue resolved by pull request 23821
[https://github.com/apache/spark/pull/23821];;;",,,,,,,,,,,,,,,,,,,,,
Vectorized gapply should not prune columns,SPARK-26901,13216166,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,16/Feb/19 16:48,12/Dec/22 17:51,13/Jul/23 08:46,20/Feb/19 10:12,3.0.0,,,,,,,,3.0.0,,,,R,SQL,,,,0,,,,,,"Currently, if some columns can be pushed, it's being pushed through {{FlatMapGroupsInRWithArrow}}.

{code}
explain(count(gapply(df,
                     ""gear"",
                     function(key, group) {
                       data.frame(gear = key[[1]], disp = mean(group$disp))
                     },
                     structType(""gear double, disp double""))), TRUE)
{code}

{code}
*(4) HashAggregate(keys=[], functions=[count(1)], output=[count#64L])
+- Exchange SinglePartition
   +- *(3) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#67L])
      +- *(3) Project
         +- FlatMapGroupsInRWithArrow [...]
            +- *(2) Sort [gear#9 ASC NULLS FIRST], false, 0
               +- Exchange hashpartitioning(gear#9, 200)
                  +- *(1) Project [gear#9]
                     +- *(1) Scan ExistingRDD arrow[mpg#0,cyl#1,disp#2,hp#3,drat#4,wt#5,qsec#6,vs#7,am#8,gear#9,carb#10]
{code}

This causes to send corrupt values R workers when the R native functions are executed.

{code}
  c(5, 5, 5, 5, 5)
  c(7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 2.47032822920623e-323)
  c(0, 0, 0, 0, 2.05578399548861e-314)
  c(3.4483079184909e-313, 3.4483079184909e-313, 3.4483079184909e-313, 5.31146529464635e-315, 0)
  c(0, 0, 0, 0, -2.63230705887168e+228)
  c(5, 5, 5, 0, 2.47032822920623e-323)
  c(7.90505033345994e-323, 7.90505033345994e-323, 0, 0, 4.17777978645388e-314)
  c(0, 0, 0, 0, -2.18328530492023e+219)
  c(3.4483079184909e-313, 5.31146529464635e-315, 0, 0, -2.63230127529109e+228)
  c(0, 0, 0, 0, 2.47032822920623e-323)
  c(5, 0, 0, 0, 4.17777978645388e-314)
  c(3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3)
  c(7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 2.47032822920623e-323)
  c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2.05578399548861e-314)
  c(3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 5.30757980430645e-315, 0)
  c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2.73302088532611e+228)
  c(3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 2.47032822920623e-323)
  c(7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 7.90505033345994e-323, 0, 0, 4.17777978645388e-314)
  c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1.04669129845114e+219)
  c(3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 3.4482690635875e-313, 5.30757980430645e-315, 0, 0, -2.73301510174552e+228)
  c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2.47032822920623e-323)
  c(3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 4.17777978645388e-314)
{code}

which should be:

{code}

  c(21, 21, 22.8, 24.4, 22.8, 19.2, 17.8, 32.4, 30.4, 33.9, 27.3, 21.4)
  c(6, 6, 4, 4, 4, 6, 6, 4, 4, 4, 4, 4)
  c(160, 160, 108, 146.7, 140.8, 167.6, 167.6, 78.7, 75.7, 71.1, 79, 121)
  c(110, 110, 93, 62, 95, 123, 123, 66, 52, 65, 66, 109)
  c(3.9, 3.9, 3.85, 3.69, 3.92, 3.92, 3.92, 4.08, 4.93, 4.22, 4.08, 4.11)
  c(2.62, 2.875, 2.32, 3.19, 3.15, 3.44, 3.44, 2.2, 1.615, 1.835, 1.935, 2.78)
  c(16.46, 17.02, 18.61, 20, 22.9, 18.3, 18.9, 19.47, 18.52, 19.9, 18.9, 18.6)
  c(0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
  c(1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1)
  c(4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4)
  c(4, 4, 1, 2, 2, 4, 4, 1, 2, 1, 1, 2)
  c(26, 30.4, 15.8, 19.7, 15)
  c(4, 4, 8, 6, 8)
  c(120.3, 95.1, 351, 145, 301)
  c(91, 113, 264, 175, 335)
  c(4.43, 3.77, 4.22, 3.62, 3.54)
  c(2.14, 1.513, 3.17, 2.77, 3.57)
  c(16.7, 16.9, 14.5, 15.5, 14.6)
  c(0, 1, 0, 0, 0)
  c(1, 1, 1, 1, 1)
  c(5, 5, 5, 5, 5)
  c(2, 2, 4, 6, 8)
  c(21.4, 18.7, 18.1, 14.3, 16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 21.5, 15.5, 15.2, 13.3, 19.2)
  c(6, 8, 6, 8, 8, 8, 8, 8, 8, 8, 4, 8, 8, 8, 8)
  c(258, 360, 225, 360, 275.8, 275.8, 275.8, 472, 460, 440, 120.1, 318, 304, 350, 400)
  c(110, 175, 105, 245, 180, 180, 180, 205, 215, 230, 97, 150, 150, 245, 175)
  c(3.08, 3.15, 2.76, 3.21, 3.07, 3.07, 3.07, 2.93, 3, 3.23, 3.7, 2.76, 3.15, 3.73, 3.08)
  c(3.215, 3.44, 3.46, 3.57, 4.07, 3.73, 3.78, 5.25, 5.424, 5.345, 2.465, 3.52, 3.435, 3.84, 3.845)
  c(19.44, 17.02, 20.22, 15.84, 17.4, 17.6, 18, 17.98, 17.82, 17.42, 20.01, 16.87, 17.3, 15.41, 17.05)
  c(1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0)
  c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
  c(3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3)
  c(1, 2, 1, 4, 3, 3, 3, 4, 4, 4, 1, 2, 2, 4, 2)
{code}
",,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 20 10:12:45 UTC 2019,,,,,,,,,,"0|yi1200:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/19 10:12;cloud_fan;Issue resolved by pull request 23810
[https://github.com/apache/spark/pull/23810];;;",,,,,,,,,,,,,,,,,,,,,
"When running spark 2.3 as a proxy user (--proxy-user), SparkSubmit fails to resolve globs owned by target user",SPARK-26895,13216065,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,abellina,abellina,abellina,15/Feb/19 19:43,22/Aug/19 07:40,13/Jul/23 08:46,22/Feb/19 19:15,2.3.2,2.4.0,,,,,,,2.3.4,2.4.4,3.0.0,,Spark Core,,,,,0,,,,,,"We are resolving globs in SparkSubmit here (by way of prepareSubmitEnvironment) without first going into a doAs:

https://github.com/apache/spark/blob/6c18d8d8079ac4d2d6dc7539601ab83fc5b51760/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L143

Without first entering a doAs, as done here:

[https://github.com/apache/spark/blob/6c18d8d8079ac4d2d6dc7539601ab83fc5b51760/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L151]

So when running spark-submit with --proxy-user, and for example --archives, it will fail to launch unless the location of the archive is open to the user that executed spark-submit.",,abellina,krisden,roczei,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28417,,,,SPARK-21012,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 22 19:15:52 UTC 2019,,,,,,,,,,"0|yi11dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/19 19:59;abellina;I am working on this.;;;","22/Feb/19 19:15;vanzin;Issue resolved by pull request 23806
[https://github.com/apache/spark/pull/23806];;;",,,,,,,,,,,,,,,,,,,,
Fix Alias handling in AggregateEstimation,SPARK-26894,13216060,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vsowrirajan,vsowrirajan,vsowrirajan,15/Feb/19 19:38,21/Mar/19 14:49,13/Jul/23 08:46,21/Mar/19 02:24,2.4.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,Aliases are not handled separately in AggregateEstimation similar to ProjectEstimation due to which stats are not getting propagated when CBO is enabled.,,maropu,mgaido,vsowrirajan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 21 14:49:00 UTC 2019,,,,,,,,,,"0|yi11cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/19 21:41;vsowrirajan;Need to assign this Jira to myself. How can I do that?;;;","15/Feb/19 23:21;mgaido;It will be assigned once the PR is merged. Thanks.;;;","21/Mar/19 02:24;maropu;Resolved by https://github.com/apache/spark/pull/23803;;;","21/Mar/19 14:49;vsowrirajan;Thanks for merging the code, Takeshi Yamamuro

On Thu, Mar 21, 2019, 1:29 AM Takeshi Yamamuro (JIRA) <jira@apache.org>

;;;",,,,,,,,,,,,,,,,,,
saveAsTextFile throws NullPointerException  when null row present ,SPARK-26892,13215982,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liupengcheng,liupengcheng,liupengcheng,15/Feb/19 13:43,27/Feb/19 06:02,13/Jul/23 08:46,20/Feb/19 22:43,2.4.0,,,,,,,,3.0.0,,,,Spark Core,,,,,0,,,,,,"We encoutered this problem in our production cluster, it can be reproduced by the following code:
{code:java}
scala> sc.parallelize(Seq(1,null),1).saveAsTextFile(""/tmp/foobar.dat"")
19/02/15 21:39:17 ERROR Utils: Aborting task
java.lang.NullPointerException
at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$3(RDD.scala:1510)
at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:129)
at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1352)
at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:127)
at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
at org.apache.spark.scheduler.Task.run(Task.scala:121)
at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:425)
at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1318)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:428)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
{code}",,liupengcheng,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 20 22:43:17 UTC 2019,,,,,,,,,,"0|yi10vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/19 22:43;srowen;Issue resolved by pull request 23799
[https://github.com/apache/spark/pull/23799];;;",,,,,,,,,,,,,,,,,,,,,
"Flaky test:YarnSchedulerBackendSuite.""RequestExecutors reflects node blacklist and is serializable""",SPARK-26891,13215981,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,15/Feb/19 13:40,17/May/20 18:15,13/Jul/23 08:46,19/Feb/19 21:38,2.3.0,2.4.0,3.0.0,,,,,,2.4.3,3.0.0,,,Spark Core,YARN,,,,0,,,,,,"For even an unrelated change sometimes the YarnSchedulerBackendSuite#""RequestExecutors reflects node blacklist and is serializable"" test fails with the following error:
{noformat}
Error Message
org.mockito.exceptions.misusing.WrongTypeOfReturnValue:  EmptySet$ cannot be returned by resourceOffers() resourceOffers() should return Seq *** If you're unsure why you're getting above error read on. Due to the nature of the syntax above problem might occur because: 1. This exception *might* occur in wrongly written multi-threaded tests.    Please refer to Mockito FAQ on limitations of concurrency testing. 2. A spy is stubbed using when(spy.foo()).then() syntax. It is safer to stub spies -     - with doReturn|Throw() family of methods. More in javadocs for Mockito.spy() method. 
Stacktrace
sbt.ForkMain$ForkError: org.mockito.exceptions.misusing.WrongTypeOfReturnValue: 
EmptySet$ cannot be returned by resourceOffers()
resourceOffers() should return Seq
***
If you're unsure why you're getting above error read on.
Due to the nature of the syntax above problem might occur because:
1. This exception *might* occur in wrongly written multi-threaded tests.
   Please refer to Mockito FAQ on limitations of concurrency testing.
2. A spy is stubbed using when(spy.foo()).then() syntax. It is safer to stub spies - 
   - with doReturn|Throw() family of methods. More in javadocs for Mockito.spy() method.
{noformat}
 ",,attilapiros,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 19 21:38:26 UTC 2019,,,,,,,,,,"0|yi10v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/19 13:43;attilapiros;I am woking on it.;;;","19/Feb/19 21:38;vanzin;Issue resolved by pull request 23801
[https://github.com/apache/spark/pull/23801];;;",,,,,,,,,,,,,,,,,,,,
Fix timestamp type in Structured Streaming + Kafka Integration Guide,SPARK-26889,13215943,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,gsomogyi,gsomogyi,gsomogyi,15/Feb/19 10:06,12/Dec/22 18:10,13/Jul/23 08:46,18/Feb/19 09:22,3.0.0,,,,,,,,3.0.0,,,,Documentation,Structured Streaming,,,,0,,,,,,"{code:java}
$ spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:3.0.0-SNAPSHOT
...
scala> val df = spark.read.format(""kafka"").option(""kafka.bootstrap.servers"", ""foo"").option(""subscribe"", ""bar"").load().printSchema()
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

df: Unit = ()
{code}

In the doc timestamp type is long.
",,gsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 18 09:22:45 UTC 2019,,,,,,,,,,"0|yi10mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/19 09:22;gurwls223;Issue resolved by pull request 23796
[https://github.com/apache/spark/pull/23796];;;",,,,,,,,,,,,,,,,,,,,,
Create datetime.date directly instead of creating datetime64[ns] as intermediate data.,SPARK-26887,13215913,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,15/Feb/19 08:40,12/Dec/22 18:10,13/Jul/23 08:46,18/Feb/19 03:48,2.4.0,,,,,,,,3.0.0,,,,PySpark,,,,,0,,,,,,"Currently {{DataFrame.toPandas()}} with arrow enabled or {{ArrowStreamPandasSerializer}} for pandas UDF with pyarrow<0.12 creates {{datetime64[ns]}} type series as intermediate data and then convert to {{datetime.date}} series, but the intermediate {{datetime64[ns]}} might cause an overflow even if the date is valid.
{noformat}
>>> import datetime
>>>
>>> t  = [datetime.date(2262, 4, 12), datetime.date(2263, 4, 12)]
>>>
>>> df = spark.createDataFrame(t, 'date')
>>> df.show()
+----------+
|     value|
+----------+
|2262-04-12|
|2263-04-12|
+----------+

>>>
>>> spark.conf.set(""spark.sql.execution.arrow.enabled"", ""true"")
>>>
>>> df.toPandas()
        value
0  1677-09-21
1  1678-09-21
{noformat}
We should avoid creating such intermediate data and create {{datetime.date}} series directly instead.",,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 18 03:48:57 UTC 2019,,,,,,,,,,"0|yi10g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/19 03:48;gurwls223;Issue resolved by pull request 23795
[https://github.com/apache/spark/pull/23795];;;",,,,,,,,,,,,,,,,,,,,,
QueryTest.compare does not handle maps with array keys correctly,SPARK-26878,13215744,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ala.luszczak,ala.luszczak,ala.luszczak,14/Feb/19 13:34,18/Feb/19 03:08,13/Jul/23 08:46,18/Feb/19 02:56,2.4.0,,,,,,,,3.0.0,,,,SQL,Tests,,,,0,,,,,,"The current strategy for comparing Maps is sorting the (key, value) tuples by _.toString, zipping tuples from both maps together, and then comparing tuples within each of the pairs separately.

See: https://github.com/apache/spark/blob/ac9c0536bc518f173f2ff53bee42b7a89d28ee20/sql/core/src/test/scala/org/apache/spark/sql/QueryTest.scala#L344-L346

This is not ideal for byte arrays. The string representations of byte arrays looks like “[B@7d263ddc” and has nothing to do with values actually contained within the array.

Hence, if a map has byte array keys, then random values get compared with each other, which can result in false negatives.",,ala.luszczak,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 18 02:56:07 UTC 2019,,,,,,,,,,"0|yi0zeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/19 02:56;cloud_fan;Issue resolved by pull request 23789
[https://github.com/apache/spark/pull/23789];;;",,,,,,,,,,,,,,,,,,,,,
FileFormatWriter creates inconsistent MR job IDs,SPARK-26873,13215603,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,rdblue,rdblue,rdblue,13/Feb/19 21:47,02/Mar/20 20:31,13/Jul/23 08:46,14/Feb/19 16:27,2.1.0,2.2.0,2.2.3,2.3.2,2.4.0,,,,2.3.4,2.4.1,3.0.0,,SQL,,,,,0,correctness,,,,,"FileFormatWriter uses the current time to create a Job ID that is used when calling Hadoop committers. This ID is used to produce task and task attempt IDs used in commits.

The problem is that Spark [generates this Job ID|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala#L209] in {{executeTask}} for every task:
{code:lang=scala}
  /** Writes data out in a single Spark task. */
  private def executeTask(
      description: WriteJobDescription,
      sparkStageId: Int,
      sparkPartitionId: Int,
      sparkAttemptNumber: Int,
      committer: FileCommitProtocol,
      iterator: Iterator[InternalRow]): WriteTaskResult = {

    val jobId = SparkHadoopWriterUtils.createJobID(new Date, sparkStageId)
    val taskId = new TaskID(jobId, TaskType.MAP, sparkPartitionId)
    val taskAttemptId = new TaskAttemptID(taskId, sparkAttemptNumber)

...
{code}

Because this is called in each task, the Job ID used is not consistent across tasks, which violates the contract expected by Hadoop committers.

If a committer expects identical task IDs across attempts for correctness, this breaks correctness. For example, a Hadoop committer should be able to rename an output file to a path based on the task ID to ensure that only one copy is committed.

We hit this issue when preemption caused a task to die just after the commit operation. The commit coordinator authorized a second task commit because the first did not complete due to preemption.",,rdblue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-02-13 21:47:25.0,,,,,,,,,,"0|yi0yjk:",9223372036854775807,,,,,,,,,,,,,2.4.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java : Avro function to_avro and from_avro is undefined,SPARK-26870,13215493,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,baghelamit,baghelamit,13/Feb/19 12:14,12/Dec/22 18:10,13/Jul/23 08:46,15/Feb/19 02:25,2.4.0,,,,,,,,3.0.0,,,,Java API,SQL,,,,0,,,,,,"As given at *Apache Avro Data Source Guide* [to_avro() and from_avro() |https://spark.apache.org/docs/latest/sql-data-sources-avro.html#to_avro-and-from_avro], I tried this functions in Java code but it gives compiler error as ""The method to_avro(Column) is undefined"".",,baghelamit,dkbiswal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 15 02:25:12 UTC 2019,,,,,,,,,,"0|yi0xvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/19 01:29;gurwls223;Did you properly set `org.apache.spark:spark-avro_2.11:2.4.0`? Can you show your codes?;;;","14/Feb/19 03:08;baghelamit;Yes, I have added the spark-avro dependency in pom.xml and I am able to import classes from org.apache.spark.sql.avro.* package. I couldn't find to_avro and from_avro methods in any class of this package. I am using same java code given in the Apache Avro Data Source Guide.;;;","14/Feb/19 03:09;gurwls223;Yup, I made a PR to fix. Please take a look for that when you're available.;;;","15/Feb/19 02:25;gurwls223;Issue resolved by pull request 23784
[https://github.com/apache/spark/pull/23784];;;",,,,,,,,,,,,,,,,,,
DataSourceV2Strategy should push normalized filters,SPARK-26865,13215381,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,cloud_fan,cloud_fan,13/Feb/19 03:03,14/Feb/19 00:08,13/Jul/23 08:46,14/Feb/19 00:06,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"Although we designed `SupportsPushDownFilters` in the same way by using `Filter`. DSv1 and DSv2 passes different filters.
{code}
  /**
   * Pushes down filters, and returns filters that need to be evaluated after scanning.
   */
  Filter[] pushFilters(Filter[] filters);
{code}

Specifically, DSv2 doesn't guarantee that filter expressions match the underlying schema in terms of case-sensitivity.

{code}
buildReaderWithPartitionValues(..., filters: Seq[Filter], ...)
- IsNotNull(ID)

DataSourceV2Strategy.pushFilters
- IsNotNull(id)
{code}

steps to reproduce:
{code}
spark.range(10).write.orc(""/tmp/o1"")
spark.read.schema(""ID long"").orc(""/tmp/o1"").filter(""id > 5"").show

java.util.NoSuchElementException: key not found: id
  at scala.collection.immutable.Map$Map1.apply(Map.scala:114)
  at org.apache.spark.sql.execution.datasources.orc.OrcFilters$.createBuilder(OrcFilters.scala:263)
  at org.apache.spark.sql.execution.datasources.orc.OrcFilters$.buildSearchArgument(OrcFilters.scala:153)
  at org.apache.spark.sql.execution.datasources.orc.OrcFilters$.$anonfun$convertibleFilters$1(OrcFilters.scala:99)
  at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:244)
  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:39)
  at scala.collection.TraversableLike.flatMap(TraversableLike.scala:244)
  at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:241)
  at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
  at org.apache.spark.sql.execution.datasources.orc.OrcFilters$.convertibleFilters(OrcFilters.scala:98)
  at org.apache.spark.sql.execution.datasources.orc.OrcFilters$.createFilter(OrcFilters.scala:87)
  at org.apache.spark.sql.execution.datasources.v2.orc.OrcScanBuilder.pushFilters(OrcScanBuilder.scala:50)
{code}",,cloud_fan,dkbiswal,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23817,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 14 00:06:25 UTC 2019,,,,,,,,,,"0|yi0x6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/19 03:04;cloud_fan;cc [~dongjoon] [~Gengliang.Wang] [~LI,Xiao];;;","13/Feb/19 05:06;dongjoon;Thank you for pinging me. I'll take a look.;;;","13/Feb/19 05:10;dongjoon;Hi, [~cloud_fan]. The following is the result from the Apache Spark 2.4.0 and 2.3.2. Did you use additional configuration?

{code}
scala> spark.range(10).write.orc(""/tmp/o1"")

scala> spark.read.schema(""ID long"").orc(""/tmp/o1"").filter(""id > 5"").show
+---+
| ID|
+---+
|  8|
|  7|
|  6|
|  9|
+---+

scala> sc.version
res2: String = 2.4.0
{code}

{code}
scala> spark.range(10).write.mode(""overwrite"").orc(""/tmp/o1"")

scala> spark.read.schema(""ID long"").orc(""/tmp/o1"").filter(""id > 5"").show
+---+
| ID|
+---+
|  6|
|  9|
|  8|
|  7|
+---+


scala> sc.version
res3: String = 2.3.2
{code};;;","13/Feb/19 05:19;dongjoon;Oh, it's on master branch. I'm investigating it.;;;","13/Feb/19 05:33;dongjoon;This happens in DSv2 and is caused by SPARK-23817.;;;","13/Feb/19 05:53;dongjoon;[~cloud_fan]. I updated the issue description. The root cause seems to be the difference between DSv1 and DSv2. For me, DSv2 had better work in the same way in terms of filters. That would be a general solution. I didn't check the other data sources, but if we can fix this in ORCFilters, other data sources may hit this issue again. Shall I try to make a fix for DSv2 instead of ORC?;;;","13/Feb/19 06:08;dongjoon;I'll make a PR for this.;;;","14/Feb/19 00:06;dongjoon;This is resolved via https://github.com/apache/spark/pull/23770;;;",,,,,,,,,,,,,,
Query may return incorrect result when python udf is used as a join condition and the udf uses attributes from both legs of left semi join.,SPARK-26864,13215363,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,dkbiswal,dkbiswal,13/Feb/19 01:12,17/Jul/19 00:56,13/Jul/23 08:46,16/Feb/19 09:06,2.4.0,,,,,,,,2.4.1,3.0.0,,,SQL,,,,,0,correctness,,,,,"In SPARK-25314, we supported the scenario of having a python UDF that refers to attributes from both legs of a join condition by rewriting
the plan to convert an inner join or left semi join to a filter over a cross join. In case of left semi join, this transformation may
cause incorrect results when the right leg of join condition produces duplicate rows based on the join condition.",,dkbiswal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-02-13 01:12:28.0,,,,,,,,,,"0|yi0x2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix field writer index bug in non-vectorized ORC deserializer,SPARK-26859,13215208,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivan.vergiliev,ivan.vergiliev,ivan.vergiliev,12/Feb/19 10:47,08/Aug/19 21:56,13/Jul/23 08:46,20/Feb/19 14:05,2.3.0,,,,,,,,2.3.4,2.4.1,3.0.0,,SQL,,,,,0,correctness,,,,,"There is a bug in the ORC deserialization code that, when triggered, results in completely wrong data being read. I've marked this as a Blocker as per the docs in https://spark.apache.org/contributing.html as it's a data correctness issue.

The bug is triggered when the following set of conditions are all met:
- the non-vectorized ORC reader is being used;
- a schema is explicitly specified when reading the ORC file
- the provided schema has columns not present in the ORC file, and these columns are in the middle of the schema
- the ORC file being read contains null values in the columns after the ones added by the schema.

When all of these are met:
- the internal state of the ORC deserializer gets messed up, and, as a result
- the null values from the ORC file end up being set on wrong columns, not the one they're in, and
- the old values from the null columns don't get cleared from the previous record.

Here's a concrete example. Let's consider the following DataFrame:

{code:scala}
        val rdd = sparkContext.parallelize(Seq((1, 2, ""abc""), (4, 5, ""def""), (8, 9, null)))
        val df = rdd.toDF(""col1"", ""col2"", ""col3"")
{code}

and the following schema:

{code:scala}
col1 int, col4 int, col2 int, col3 string
{code}

Notice the `col4 int` added in the middle that doesn't exist in the dataframe.

Saving this dataframe to ORC and then reading it back with the specified schema should result in reading the same values, with nulls for `col4`. Instead, we get the following back:


{code:java}
[1,null,2,abc]
[4,null,5,def]
[8,null,null,def]
{code}

Notice how the `def` from the second record doesn't get properly cleared and ends up in the third record as well; also, instead of `col2 = 9` in the last record as expected, we get the null that should've been in column 3 instead.

*Impact*
When this issue is triggered, it results in completely wrong results being read from the ORC file. The set of conditions under which it gets triggered is somewhat narrow so the set of affected users is probably limited. There are possibly also people that are affected but haven't realized it because the conditions are so obscure.

*Bug details*
The issue is caused by calling `setNullAt` with a wrong index in `OrcDeserializer.scala:deserialize()`. I have a fix that I'll send out for review shortly.

*Workaround*
This bug is currently only triggered when new columns are added to the middle of the schema. This means that it can be worked around by only adding new columns at the end.",,cloud_fan,dongjoon,ivan.vergiliev,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 20 14:05:25 UTC 2019,,,,,,,,,,"0|yi0w48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/19 21:42;dongjoon;Thank you for reporting, [~ivan.vergiliev]. I understand this was marked as `Blocker` since it returns incorrect data, but it would be lower than that since this happens at user-given schema situation and non-vectorized path. Anyway, I'll review it tonight.
- https://spark.apache.org/contributing.html;;;","20/Feb/19 14:05;cloud_fan;Issue resolved by pull request 23766
[https://github.com/apache/spark/pull/23766];;;",,,,,,,,,,,,,,,,,,,,
CachedRDDBuilder only partially implements double-checked locking,SPARK-26851,13214695,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bersprockets,bersprockets,bersprockets,08/Feb/19 18:52,22/Feb/19 06:29,13/Jul/23 08:46,14/Feb/19 06:58,2.4.0,3.0.0,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"In CachedRDDBuilder, {{cachedColumnBuffers}} uses double-checked locking to lazily initialize {{_cachedColumnBuffers}}. Also, clearCache uses double-checked locking to likely avoid synchronization when {{_cachedColumnBuffers}} is still null.

However, the resource (in this case, {{_cachedColumnBuffers}}) is not declared as volatile, which could cause some visibility problems, particularly in {{clearCache}}, which may see null reference when actually there is an RDD.

From Java Concurrency in Practice by Brian Goetz et al:
{quote}Subsequent changes in the JMM (Java 5.0 and later) have enabled DCL to work if resource is made volatile, and the performance impact of this is small since volatile reads are usually only slightly more expensive than nonvolatile reads.
{quote}
There are comments in other documentation that volatile is not needed if the resource is immutable. While an RDD is immutable from a Spark user's point of view, it may not be from a JVM's point of view, since not all internal fields are final.

I've marked this as minor since the race conditions are highly unlikely.",,bersprockets,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 14 06:58:20 UTC 2019,,,,,,,,,,"0|yi0sy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/19 18:56;bersprockets;[~maropu] [~cloud_fan]

I will let this Jira marinate for a little while, since I might have missed something (misunderstanding of the code, improvements in the JMM since initial introduction). I will make a PR if needed, but I don't want to use up 4 hours of Jenkins resources for a nonsense PR.;;;","14/Feb/19 06:58;cloud_fan;Issue resolved by pull request 23768
[https://github.com/apache/spark/pull/23768];;;",,,,,,,,,,,,,,,,,,,,
Avoid cost-based join reorder in presence of join hints,SPARK-26840,13214302,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maryannxue,maryannxue,maryannxue,07/Feb/19 03:49,12/Dec/22 18:10,13/Jul/23 08:46,15/Feb/19 00:57,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"This is a fix for [https://github.com/apache/spark/pull/23524|https://github.com/apache/spark/pull/23524.], which did not stop cost-based join reorder when the {{CostBasedJoinReorder}} rule recurses down the tree and applies join reorder for nested joins with hints.",,maropu,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26065,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 09 06:47:47 UTC 2019,,,,,,,,,,"0|yi0qjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/19 06:47;gurwls223;[~maryannxue], can you fill the description of the JiRA?;;;",,,,,,,,,,,,,,,,,,,,,
Columns get switched in Spark SQL using Avro backed Hive table if schema evolves,SPARK-26836,13214170,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,attilapiros,treff7es,treff7es,06/Feb/19 14:54,12/Dec/22 18:10,13/Jul/23 08:46,05/Feb/21 18:56,2.3.1,2.4.0,3.0.1,3.1.0,3.2.0,,,,3.2.0,,,,SQL,,,,,2,correctness,,,,,"I have a hive avro table where the avro schema is stored on s3 next to the avro files. 

In the table definiton the avro.schema.url always points to the latest partition's _schema.avsc file which is always the lates schema. (Avro schemas are backward and forward compatible in a table)

When new data comes in, I always add a new partition where the avro.schema.url properties also set to the _schema.avsc which was used when it was added and of course I always update the table avro.schema.url property to the latest one.

Querying this table works fine until the schema evolves in a way that a new optional property is added in the middle. 

When this happens then after the spark sql query the columns in the old partition gets mixed up and it shows the wrong data for the columns.

If I query the table with Hive then everything is perfectly fine and it gives me back the correct columns for the partitions which were created the old schema and for the new which was created the evolved schema.

 

Here is how I could reproduce with the [doctors.avro|https://github.com/apache/spark/blob/master/sql/hive/src/test/resources/data/files/doctors.avro] example data in sql test suite.
 # I have created two partition folder:
{code:java}
[hadoop@ip-192-168-10-158 hadoop]$ hdfs dfs -ls s3://somelocation/doctors/*/
Found 2 items
-rw-rw-rw- 1 hadoop hadoop 418 2019-02-06 12:48 s3://somelocation/doctors
/dt=2019-02-05/_schema.avsc
-rw-rw-rw- 1 hadoop hadoop 521 2019-02-06 12:13 s3://somelocation/doctors
/dt=2019-02-05/doctors.avro
Found 2 items
-rw-rw-rw- 1 hadoop hadoop 580 2019-02-06 12:49 s3://somelocation/doctors
/dt=2019-02-06/_schema.avsc
-rw-rw-rw- 1 hadoop hadoop 577 2019-02-06 12:13 s3://somelocation/doctors
/dt=2019-02-06/doctors_evolved.avro{code}
Here the first partition had data which was created with the schema before evolving and the second one had the evolved one. (the evolved schema is the same as in your testcase except I moved the extra_field column to the last from the second and I generated two lines of avro data with the evolved schema.
 # I have created a hive table with the following command:

 
{code:java}
CREATE EXTERNAL TABLE `default.doctors`
 PARTITIONED BY (
 `dt` string
 )
 ROW FORMAT SERDE
 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
 WITH SERDEPROPERTIES (
 'avro.schema.url'='s3://somelocation/doctors/
/dt=2019-02-06/_schema.avsc')
 STORED AS INPUTFORMAT
 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
 OUTPUTFORMAT
 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
 LOCATION
 's3://somelocation/doctors/'
 TBLPROPERTIES (
 'transient_lastDdlTime'='1538130975'){code}
 

Here as you can see the table schema url points to the latest schema

3. I ran an msck _repair table_ to pick up all the partitions.

Fyi: If I run my select * query from here then everything is fine and no columns switch happening.

4. Then I changed the first partition's avro.schema.url url to points to the schema which is under the partition folder (non-evolved one -> s3://somelocation/doctors/
/dt=2019-02-05/_schema.avsc)

Then if you ran a _select * from default.spark_test_ then the columns will be mixed up (on the data below the first name column becomes the extra_field column. I guess because in the latest schema it is the second column):

 
{code:java}
number,extra_field,first_name,last_name,dt 
6,Colin,Baker,null,2019-02-05 
3,Jon,Pertwee,null,2019-02-05 
4,Tom,Baker,null,2019-02-05 
5,Peter,Davison,null,2019-02-05 
11,Matt,Smith,null,2019-02-05 
1,William,Hartnell,null,2019-02-05 
7,Sylvester,McCoy,null,2019-02-05 
8,Paul,McGann,null,2019-02-05 
2,Patrick,Troughton,null,2019-02-05 
9,Christopher,Eccleston,null,2019-02-05 
10,David,Tennant,null,2019-02-05 
21,fishfinger,Jim,Baker,2019-02-06 
24,fishfinger,Bean,Pertwee,2019-02-06

{code}
If I try the same query from Hive and not from spark sql then everything is fine and it never switches the columns.

 ",I tested with Hive and HCatalog which runs on version 2.3.4 and with Spark 2.3.1 and 2.4,apachespark,attilapiros,cloud_fan,david_ravet,dbtsai,dongjoon,Gengliang.Wang,haiboself,treff7es,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/19 15:42;treff7es;doctors.avro;https://issues.apache.org/jira/secure/attachment/12957798/doctors.avro","06/Feb/19 15:42;treff7es;doctors_evolved.avro;https://issues.apache.org/jira/secure/attachment/12957795/doctors_evolved.avro","06/Feb/19 15:42;treff7es;doctors_evolved.json;https://issues.apache.org/jira/secure/attachment/12957796/doctors_evolved.json","06/Feb/19 15:42;treff7es;original.avsc;https://issues.apache.org/jira/secure/attachment/12957797/original.avsc",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 05 18:56:58 UTC 2021,,,,,,,,,,"0|yi0pqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/19 08:08;dongjoon;Thank you for reporting, [~treff7es]. 
Did you see this issue before at Spark 2.3.x with Databricks Avro libraries?

cc [~Gengliang.Wang];;;","08/Feb/19 09:21;treff7es;I just tried with Spark 2.3.1 and the same issue. I did not specify any Avro library because it is a Hive Avro table originally. I guess it is using Hive's Avro serde for serialization/deserialization (or maybe I'm not right :)).;;;","08/Feb/19 14:13;treff7es;In the meantime I checked if using the same hive version in Spark would solve this issue by setting:

 
{code:java}
spark.sql.hive.metastore.version 2.3.3
spark.sql.hive.metastore.jars /etc/hadoop/conf:/usr/lib/hadoop/lib/*:/usr/lib/hadoop/.//*:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/*:/usr/lib/hadoop-hdfs/.//*:/usr/lib/hadoop-yarn/lib/*:/usr/lib/hadoop-yarn/.//*:/usr/lib/hadoop-mapreduce/lib/*:/usr/lib/hadoop-mapreduce/.//*::/etc/tez/conf:/usr/lib/tez/*:/usr/lib/tez/lib/*:/usr/lib/hadoop-lzo/lib/*:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar:/usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar:/usr/share/aws/emr/kinesis/lib/emr-kinesis-hadoop.jar:/usr/share/aws/emr/cloudwatch-sink/lib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/lib/hive/lib/* 
{code}
The issue still exists regardless the hive metastore version spark is using.;;;","11/Feb/19 07:47;Gengliang.Wang;[~dongjoon]I don't think the issue is related to spark-avro lib.
[~treff7es] Can you try reproducing the issue on Hive directly and see what the behavior is? ;;;","11/Feb/19 13:04;treff7es;On hive directly the query returns with the correct resultset regardless if _avro.schema.url_ is set for the partitions or not :
{code:java}
hive> select * from spark_test;
OK
6 fishfingers and custard Colin Baker 2019-02-05
3 fishfingers and custard Jon Pertwee 2019-02-05
4 fishfingers and custard Tom Baker 2019-02-05
5 fishfingers and custard Peter Davison 2019-02-05
11 fishfingers and custard Matt Smith 2019-02-05
1 fishfingers and custard William Hartnell 2019-02-05
7 fishfingers and custard Sylvester McCoy 2019-02-05
8 fishfingers and custard Paul McGann 2019-02-05
2 fishfingers and custard Patrick Troughton 2019-02-05
9 fishfingers and custard Christopher Eccleston 2019-02-05
10 fishfingers and custard David Tennant 2019-02-05
21 fishfinger Jim Baker 2019-02-06
24 fishfinger Bean Pertwee 2019-02-06
Time taken: 4.291 seconds, Fetched: 13 row(s){code};;;","11/Feb/19 13:43;treff7es;And one more thing.

I also got this warning which I did not get if I run on the table where the partitions do not contain the avro.schema.url property.
{code:java}
19/02/11 14:39:40 WARN AvroDeserializer: Received different schemas. Have to re-encode: {""type"":""record"",""name"":""doctors"",""namespace"":""testing.hive.avro.serde"",""fields"":[{""name"":""number"",""type"":""int"",""doc"":""Order of playing the role""},{""name"":""extra_field"",""type"":""string"",""default"":""fishfingers and custard"",""doc:"":""an extra field not in the original file""},{""name"":""first_name"",""type"":""string"",""doc"":""first name of actor playing role""},{""name"":""last_name"",""type"":""string"",""doc"":""last name of actor playing role""}]}
SIZE{-3ac2eea4:168dcc8e145:-8000=org.apache.hadoop.hive.serde2.avro.AvroDeserializer$SchemaReEncoder@1429dfec} ID -3ac2eea4:168dcc8e145:-8000{code};;;","20/Jun/19 10:03;david_ravet;Hi,

 

We encounter the same issue with Spark 2.2.0 when reading avro from a partition where avro.schema.url point to a new schema (forward compatibility) but stored avro files were in older schema.

Querying with Hive client is OK but with Spark Sql we get 
{code}
19/06/20 10:43:47 WARN avro.AvroDeserializer: Received different schemas.  Have to re-encode: {""type"":""record"",""name"":""KeyValuePair"",""namespace"":""org.apache.avro.mapreduce"",""doc"":""A key/value pair"",""fields"":[{""name"":""key"",""type"": .............
SIZE{4207b7ac:16b740e5ed1:-8000=org.apache.hadoop.hive.serde2.avro.AvroDeserializer$SchemaReEncoder@17173314} ID 4207b7ac:16b740e5ed1:-8000
19/06/20 10:43:47 ERROR executor.Executor: Exception in task 6.0 in stage 1.0 (TID 22)
java.lang.RuntimeException: Hive internal error: conversion of double to struct<unscaledamount:bigint,scale:int,currency:string,currencyalphacode:string>not supported yet.
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$StructConverter.<init>(ObjectInspectorConverters.java:380)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:155)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$StructConverter.<init>(ObjectInspectorConverters.java:374)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:155)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$ListConverter.convert(ObjectInspectorConverters.java:331)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$StructConverter.convert(ObjectInspectorConverters.java:396)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$StructConverter.convert(ObjectInspectorConverters.java:396)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:430)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:429)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code};;;","20/Jun/19 19:58;dongjoon;cc [~dbtsai];;;","20/Feb/20 11:01;cloud_fan;cc [~Gengliang.Wang];;;","28/Feb/20 02:59;gurwls223;I am lowering the priority to Critical as it's at least not a regression and doesn't look blocking Spark 3.0; however, indeed we should treat correctness issues at least Critical+.;;;","08/Jan/21 16:14;attilapiros;I am working on this and a PR can be expected this weekend / next week;;;","11/Jan/21 14:35;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/31133;;;","05/Feb/21 18:56;dongjoon;Issue resolved by pull request 31133
[https://github.com/apache/spark/pull/31133];;;",,,,,,,,,
Streaming queries may store checkpoint data in a wrong directory,SPARK-26824,13213729,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,04/Feb/19 17:51,25/Feb/19 08:59,13/Jul/23 08:46,20/Feb/19 23:49,2.0.0,2.1.0,2.2.0,2.3.0,2.4.0,,,,3.0.0,,,,Structured Streaming,,,,,0,release-notes,,,,,"When a user specifies a checkpoint location containing special chars that need to be escaped in a path, the streaming query will store checkpoint in a wrong place. For example, if you use ""/chk chk"", the metadata will be stored in ""/chk%20chk"". File sink's ""_spark_metadata"" directory has the same issue. ",,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,"Earlier version of Spark incorrectly escaped paths when writing out checkpoints and ""_spark_metadata"" for structured streaming. Queries affected by this issue will fail when running in Spark 3.0. It will report an instruction about how to migrate your queries.",false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 04 17:57:40 UTC 2019,,,,,,,,,,"0|yi0n0o:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,"04/Feb/19 17:57;zsxwing;This will need a release note. After the fix, the paths to store metadata will be changed. The user needs to move the metadata files which are in a wrong place to the right location manually. Otherwise, their query will not pick up the old metadata.;;;",,,,,,,,,,,,,,,,,,,,,
Make MLEvents JSON ser/de safe,SPARK-26818,13213488,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,02/Feb/19 15:08,12/Dec/22 18:11,13/Jul/23 08:46,03/Feb/19 13:20,3.0.0,,,,,,,,3.0.0,,,,ML,,,,,0,,,,,,"Looks ML events are not JSON serializable. We can make it serialisable like:

{code}
@DeveloperApi
case class SparkListenerSQLExecutionEnd(executionId: Long, time: Long)
  extends SparkListenerEvent {

  // The name of the execution, e.g. `df.collect` will trigger a SQL execution with name ""collect"".
  @JsonIgnore private[sql] var executionName: Option[String] = None

  // The following 3 fields are only accessed when `executionName` is defined.

  // The duration of the SQL execution, in nanoseconds.
  @JsonIgnore private[sql] var duration: Long = 0L

  // The `QueryExecution` instance that represents the SQL execution
  @JsonIgnore private[sql] var qe: QueryExecution = null

  // The exception object that caused this execution to fail. None if the execution doesn't fail.
  @JsonIgnore private[sql] var executionFailure: Option[Exception] = None
}
{code}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 03 13:20:35 UTC 2019,,,,,,,,,,"0|yi0lj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/19 13:20;gurwls223;Issue resolved by pull request 23728
[https://github.com/apache/spark/pull/23728];;;",,,,,,,,,,,,,,,,,,,,,
PushProjectionThroughUnion nullability issue,SPARK-26812,13213441,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,bograd,bograd,02/Feb/19 01:43,06/Sep/21 08:49,13/Jul/23 08:46,01/Apr/19 14:24,2.0.2,2.1.3,2.2.3,2.3.4,2.4.0,,,,2.4.4,3.0.0,,,SQL,,,,,0,correctness,,,,,"Union output data types are the output data types of the first child.
However the other union children may have different values nullability.
This means that we can't always push down a project on the children.

To reproduce
{code}
Seq(Map(""foo"" -> ""bar"")).toDF(""a"").write.saveAsTable(""table1"")
sql(""SELECT 1 AS b"").write.saveAsTable(""table2"")
sql(""CREATE OR REPLACE VIEW test1 AS SELECT map() AS a FROM table2 UNION ALL SELECT a FROM table1"")
 sql(""select * from test1"").show
{code}

This fails becaus the plan is no longer resolved.
The plan is broken by the PushProjectionThroughUnion rule which pushed down a cast to map<string,string> with values nullability=true on a child with type map<string, string> with values nullability=false.",,bograd,cloud_fan,dongjoon,joshrosen,maropu,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27685,,,,,,,,,,SPARK-36673,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 14 14:30:14 UTC 2019,,,,,,,,,,"0|yi0l8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/19 14:24;cloud_fan;Issue resolved by pull request 23726
[https://github.com/apache/spark/pull/23726];;;","14/May/19 14:30;joshrosen;Based on https://issues.apache.org/jira/browse/SPARK-27685, which was just marked as a duplicate of this ticket, I think this might be a query correctness bug and therefore should be considered for 2.4.x. backport.

/cc [~smilegator];;;",,,,,,,,,,,,,,,,,,,,
"EventTimeStats.merge doesn't handle ""zero.merge(zero)"" correctly",SPARK-26806,13213211,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,lian cheng,zsxwing,01/Feb/19 00:00,10/Oct/19 23:24,13/Jul/23 08:46,01/Feb/19 19:30,2.2.1,2.2.2,2.2.3,2.3.0,2.3.1,2.3.2,2.3.3,2.4.0,2.2.4,2.3.3,2.4.1,3.0.0,Structured Streaming,,,,,0,,,,,,"Right now, EventTimeStats.merge doesn't handle ""zero.merge(zero)"". This will make ""avg"" become ""NaN"". And whatever gets merged with the result of ""zero.merge(zero)"", ""avg"" will still be ""NaN"". Then finally, ""NaN"".toLong will return ""0"" and the user will see the following incorrect report:
{code:java}
""eventTime"" : {
    ""avg"" : ""1970-01-01T00:00:00.000Z"",
    ""max"" : ""2019-01-31T12:57:00.000Z"",
    ""min"" : ""2019-01-30T18:44:04.000Z"",
    ""watermark"" : ""1970-01-01T00:00:00.000Z""
  }
{code}",,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21597,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-02-01 00:00:15.0,,,,,,,,,,"0|yi0jtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CVE-2018-11760: Apache Spark local privilege escalation vulnerability,SPARK-26802,13213137,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lucacanali,irashid,irashid,31/Jan/19 17:35,31/Jan/19 17:39,13/Jul/23 08:46,31/Jan/19 17:35,1.6.3,2.0.2,2.1.3,2.2.2,,,,,2.2.3,2.3.2,2.4.0,,PySpark,Security,,,,0,,,,,,"Severity: Important

Vendor: The Apache Software Foundation

Versions affected:
All Spark 1.x, Spark 2.0.x, and Spark 2.1.x versions
Spark 2.2.0 to 2.2.2
Spark 2.3.0 to 2.3.1

Description:
When using PySpark , it's possible for a different local user to connect to the Spark application and impersonate the user running the Spark application.  This affects versions 1.x, 2.0.x, 2.1.x, 2.2.0 to 2.2.2, and 2.3.0 to 2.3.1.

Mitigation:
1.x, 2.0.x, 2.1.x, and 2.2.x users should upgrade to 2.2.3 or newer
2.3.x users should upgrade to 2.3.2 or newer
Otherwise, affected users should avoid using PySpark in multi-user environments.

Credit:
This issue was reported by Luca Canali and Jose Carlos Luna Duran from CERN.

References:
https://spark.apache.org/security.html

This was fixed by
master / 2.4:
https://github.com/apache/spark/commit/15fc2372269159ea2556b028d4eb8860c4108650
https://github.com/apache/spark/commit/0df6bf882907d7d76572f513168a144067d0e0ec

branch-2.3
https://github.com/apache/spark/commit/8080c937d3752aee2fd36f0045a057f7130f6fe

branch-2.2
https://github.com/apache/spark/commit/a5624c7ae29d6d49117dd78642879bf978212d30 

branch-2.1 (note that this does not exist in any release)
https://github.com/apache/spark/commit/b2e0f68f615cbe2cf74f9813ece76c311fe8e911",,irashid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-01-31 17:35:26.0,,,,,,,,,,"0|yi0jd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make ANTLR v4 version consistent between Maven and SBT,SPARK-26799,13213115,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,seancxmao,seancxmao,seancxmao,31/Jan/19 15:50,31/Jan/19 22:50,13/Jul/23 08:46,31/Jan/19 22:40,2.4.0,,,,,,,,3.0.0,,,,Build,,,,,0,,,,,,"Currently ANTLR v4 versions used by Maven and SBT are slightly different. Maven uses 4.7.1 while SBT uses 4.7.
 * Maven(pom.xml): <antlr4.version>4.7.1</antlr4.version>
 * SBT(project/SparkBuild): antlr4Version in Antlr4 := ""4.7""

We should make Maven and SBT use a single version.",,seancxmao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-01-31 15:50:10.0,,,,,,,,,,"0|yi0j88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSession enableHiveSupport does not point to hive but in-memory while the SparkContext exists,SPARK-26794,13213010,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,31/Jan/19 08:10,14/Feb/19 10:26,13/Jul/23 08:46,14/Feb/19 07:09,2.3.2,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"{code:java}

public class SqlDemo
{
    public static void main(final String[] args) throws Exception {
     
        SparkConf conf = new SparkConf().setAppName(""spark-sql-demo"");
        JavaSparkContext sc = new JavaSparkContext(conf);
        SparkSession ss = SparkSession.builder().enableHiveSupport().getOrCreate();
        ss.sql(""show databases"").show();
        }
        
    }
}
{code}
Before SPARK-20946, the demo above point to the right hive metastore if the hive-site.xml is present. But now it can only point to the default in-memory one.

Catalog is now as a variable shared across SparkSessions, it is instantiated with SparkContext's conf. After SPARK-20946, Session level configs are not pass to SparkContext's conf anymore, so the enableHiveSupport API takes no affect on the catalog instance.

You can set spark.sql.catalogImplementation=hive application wide to solve the problem, or never create a sc before you call SparkSession.builder().enableHiveSupport().getOrCreate()

",,cloud_fan,Qin Yao,toopt4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 14 07:09:07 UTC 2019,,,,,,,,,,"0|yi0il4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/19 07:09;cloud_fan;Issue resolved by pull request 23709
[https://github.com/apache/spark/pull/23709];;;",,,,,,,,,,,,,,,,,,,,,
Reduce Py4J communication cost in PySpark's execution barrier check,SPARK-26776,13212700,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,30/Jan/19 02:42,12/Dec/22 17:50,13/Jul/23 08:46,30/Jan/19 04:25,2.4.0,3.0.0,,,,,,,3.0.0,,,,PySpark,,,,,0,,,,,,"I am investigating flaky tests. I realised that:

{code}
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/rdd.py"", line 2512, in __init__
        self.is_barrier = prev._is_barrier() or isFromBarrier
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/rdd.py"", line 2412, in _is_barrier
        return self._jrdd.rdd().isBarrier()
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py"", line 1286, in __call__
        answer, self.gateway_client, self.target_id, self.name)
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py"", line 342, in get_return_value
        return OUTPUT_CONVERTER[type](answer[2:], gateway_client)
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py"", line 2492, in <lambda>
        lambda target_id, gateway_client: JavaObject(target_id, gateway_client))
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py"", line 1324, in __init__
        ThreadSafeFinalizer.add_finalizer(key, value)
      File ""/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.10.8.1-src.zip/py4j/finalizer.py"", line 43, in add_finalizer
        cls.finalizers[id] = weak_ref
      File ""/usr/lib64/pypy-2.5.1/lib-python/2.7/threading.py"", line 216, in __exit__
        self.release()
      File ""/usr/lib64/pypy-2.5.1/lib-python/2.7/threading.py"", line 208, in release
        self.__block.release()
    error: release unlocked lock
{code}

I assume it might not be directly related with the test itself but I noticed that it prev._is_barrier() attempts to access via Py4J.

Accessing via Py4J is expensive and IMHO it makes it flaky.",,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24822,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 30 04:25:16 UTC 2019,,,,,,,,,,"0|yi0gog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/19 02:45;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/23690;;;","30/Jan/19 02:46;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/23690;;;","30/Jan/19 04:25;cloud_fan;Issue resolved by pull request 23690
[https://github.com/apache/spark/pull/23690];;;",,,,,,,,,,,,,,,,,,,
Idle Executors are not getting killed after spark.dynamicAllocation.executorIdleTimeout value,SPARK-26758,13212402,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandeep.katta2007,abhishek.akg,abhishek.akg,29/Jan/19 05:51,12/Dec/22 18:10,13/Jul/23 08:46,05/Feb/19 04:18,2.4.0,,,,,,,,2.3.4,2.4.1,3.0.0,,Spark Core,YARN,,,,0,,,,,,"Steps:

1. Submit Spark shell with below initial Executor 3, minimum Executor=0 and executorIdleTimeout=60s

{code}
bin/spark-shell --master yarn --conf spark.dynamicAllocation.enabled=true \
  --conf spark.dynamicAllocation.initialExecutors=3 \
  --conf spark.dynamicAllocation.minExecutors=0 \
  --conf spark.dynamicAllocation.executorIdleTimeout=60s
{code}

2. Launch Spark UI and check under Executor Tab

Observation:

Initial 3 Executors assigned. After 60s( executorIdleTimeout) , number of active executor remains same.

Expected:

Apart from AM container, all other executors should be dead.

 

 ",Spark Version:2.4,abhishek.akg,zhangjianhua,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26588,,,,,,,,,,,,,,"29/Jan/19 06:33;abhishek.akg;SPARK-26758.png;https://issues.apache.org/jira/secure/attachment/12956658/SPARK-26758.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 05 04:18:33 UTC 2019,,,,,,,,,,"0|yi0euo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Jan/19 05:59;gurwls223;Can you include UI screenshot to explain the issue in the JIRA description?
;;;","29/Jan/19 06:19;sandeep.katta2007;I would like to check this issue and fix if applicable;;;","30/Jan/19 13:07;sandeep.katta2007;I am able to reproduce this issue and soon will be providing patch for this

Driver Logs


2019-01-30 14:16:39,134 | INFO | spark-dynamic-executor-allocation | *Request to remove executorIds: 2, 1* | org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
2019-01-30 14:16:39,135 | DEBUG | spark-dynamic-executor-allocation | *Not removing idle executor 2 because there are only 3 executor(s) left* (number of executor target 3) | org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)
2019-01-30 14:16:39,135 | DEBUG | spark-dynamic-executor-allocation | Not removing idle executor 2 because there are only 3 executor(s) left (number of executor target 3) | org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)
2019-01-30 14:16:39,135 | DEBUG | spark-dynamic-executor-allocation | Not removing idle executor 1 because there are only 3 executor(s) left (number of executor target 3) | org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)
2019-01-30 14:16:39,135 | DEBUG | spark-dynamic-executor-allocation | Not removing idle executor 1 because there are only 3 executor(s) left (number of executor target 3) | org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)
2019-01-30 14:16:39,241 | DEBUG | spark-dynamic-executor-allocation | Lowering target number of executors to 0 (previously 3) because not all requested executors are actually needed | org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)
2019-01-30 14:16:39,241 | DEBUG | spark-dynamic-executor-allocation | Lowering target number of executors to 0 (previously 3) because not all requested executors are actually needed | org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58);;;","05/Feb/19 04:18;srowen;Issue resolved by pull request 23697
[https://github.com/apache/spark/pull/23697];;;",,,,,,,,,,,,,,,,,,
GraphX EdgeRDDImpl and VertexRDDImpl `count` method cannot handle empty RDDs,SPARK-26757,13212385,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,huonw,huonw,huonw,29/Jan/19 03:32,06/Feb/19 16:49,13/Jul/23 08:46,31/Jan/19 23:28,2.3.1,2.3.2,2.4.0,,,,,,2.3.3,2.4.1,3.0.0,,GraphX,,,,,0,,,,,,"The {{EdgeRDDImpl}} and {{VertexRDDImpl}} types provided by {{GraphX}} throw an {{java.lang.UnsupportedOperationException: empty collection}} exception if {{count}} is called on an empty instance, when they should return 0.

{code:scala}
import org.apache.spark.graphx.{Graph, Edge}
val graph = Graph.fromEdges(sc.emptyRDD[Edge[Unit]], 0)
graph.vertices.count
graph.edges.count
{code}

Running that code in a spark-shell:

{code:none}
scala> import org.apache.spark.graphx.{Graph, Edge}
import org.apache.spark.graphx.{Graph, Edge}

scala> val graph = Graph.fromEdges(sc.emptyRDD[Edge[Unit]], 0)
graph: org.apache.spark.graphx.Graph[Int,Unit] = org.apache.spark.graphx.impl.GraphImpl@6879e983

scala> graph.vertices.count
java.lang.UnsupportedOperationException: empty collection
  at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1031)
  at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1031)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1031)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.reduce(RDD.scala:1011)
  at org.apache.spark.graphx.impl.VertexRDDImpl.count(VertexRDDImpl.scala:90)
  ... 49 elided

scala> graph.edges.count
java.lang.UnsupportedOperationException: empty collection
  at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1031)
  at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1031)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1031)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.reduce(RDD.scala:1011)
  at org.apache.spark.graphx.impl.EdgeRDDImpl.count(EdgeRDDImpl.scala:90)
  ... 49 elided
{code}",,huonw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 31 23:28:44 UTC 2019,,,,,,,,,,"0|yi0eqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/19 23:28;srowen;Issue resolved by pull request 23681
[https://github.com/apache/spark/pull/23681];;;",,,,,,,,,,,,,,,,,,,,,
Log4j customization not working for spark-shell,SPARK-26753,13212305,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankur.gupta,ankur.gupta,ankur.gupta,28/Jan/19 19:40,30/Jan/19 19:05,13/Jul/23 08:46,30/Jan/19 19:03,3.0.0,,,,,,,,3.0.0,,,,Spark Core,,,,,0,,,,,,"It's pretty common to add log4j entries to customize the level of specific loggers. e.g. adding the following to log4j.properties:


{code:java}
log4j.logger.org.apache.spark.deploy.security=DEBUG
{code}


This works fine on previous releases but not for the current build for spark-shell. This is probably caused by SPARK-25118.",,ankur.gupta,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 30 19:03:26 UTC 2019,,,,,,,,,,"0|yi0ea8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/19 19:03;vanzin;Issue resolved by pull request 23675
[https://github.com/apache/spark/pull/23675];;;",,,,,,,,,,,,,,,,,,,,,
HiveSessionImpl might have memory leak since Operation do not close properly,SPARK-26751,13212214,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cane,cane,cane,28/Jan/19 11:31,01/Mar/19 21:47,13/Jul/23 08:46,03/Feb/19 14:47,2.4.0,,,,,,,,2.3.3,2.4.1,3.0.0,,SQL,,,,,0,,,,,,"When we run in background and we get exception which is not HiveSQLException,
we may encounter memory leak since handleToOperation will not removed correctly.
The reason is below:
1. when calling operation.run we throw an exception which is not HiveSQLException
2. then opHandleSet will not add the opHandle, and operationManager.closeOperation(opHandle); will not be called
{code:java}
 private OperationHandle executeStatementInternal(String statement, Map<String, String> confOverlay, boolean runAsync) throws HiveSQLException {
        this.acquire(true);
        OperationManager operationManager = this.getOperationManager();
        ExecuteStatementOperation operation = operationManager.newExecuteStatementOperation(this.getSession(), statement, confOverlay, runAsync);
        OperationHandle opHandle = operation.getHandle();

        OperationHandle e;
        try {
            operation.run();
            this.opHandleSet.add(opHandle);
            e = opHandle;
        } catch (HiveSQLException var11) {
            operationManager.closeOperation(opHandle);
            throw var11;
        } finally {
            this.release(true);
        }

        return e;
    }


      try {
        // This submit blocks if no background threads are available to run this operation
        val backgroundHandle =
          parentSession.getSessionManager().submitBackgroundOperation(backgroundOperation)
        setBackgroundHandle(backgroundHandle)
      } catch {
        case rejected: RejectedExecutionException =>
          setState(OperationState.ERROR)
          throw new HiveSQLException(""The background threadpool cannot accept"" +
            "" new task for execution, please retry the operation"", rejected)
        case NonFatal(e) =>
          logError(s""Error executing query in background"", e)
          setState(OperationState.ERROR)
          throw e
      }
    }
{code}
3. when we close the session we will also call operationManager.closeOperation(opHandle),since we did not add this opHandle into the opHandleSet.
{code}
public void close() throws HiveSQLException {
        try {
            this.acquire(true);
            Iterator ioe = this.opHandleSet.iterator();

            while(ioe.hasNext()) {
                OperationHandle opHandle = (OperationHandle)ioe.next();
                this.operationManager.closeOperation(opHandle);
            }

            this.opHandleSet.clear();
            this.cleanupSessionLogDir();
            this.cleanupPipeoutFile();
            HiveHistory ioe1 = this.sessionState.getHiveHistory();
            if(null != ioe1) {
                ioe1.closeStream();
            }

            try {
                this.sessionState.close();
            } finally {
                this.sessionState = null;
            }
        } catch (IOException var17) {
            throw new HiveSQLException(""Failure to close"", var17);
        } finally {
            if(this.sessionState != null) {
                try {
                    this.sessionState.close();
                } catch (Throwable var15) {
                    LOG.warn(""Error closing session"", var15);
                }

                this.sessionState = null;
            }

            this.release(true);
        }

    }
{code}
4. however, the opHandle will added into handleToOperation for each statement
{code}
val handleToOperation = ReflectionUtils
    .getSuperField[JMap[OperationHandle, Operation]](this, ""handleToOperation"")

  val sessionToActivePool = new ConcurrentHashMap[SessionHandle, String]()
  val sessionToContexts = new ConcurrentHashMap[SessionHandle, SQLContext]()

  override def newExecuteStatementOperation(
      parentSession: HiveSession,
      statement: String,
      confOverlay: JMap[String, String],
      async: Boolean): ExecuteStatementOperation = synchronized {
    val sqlContext = sessionToContexts.get(parentSession.getSessionHandle)
    require(sqlContext != null, s""Session handle: ${parentSession.getSessionHandle} has not been"" +
      s"" initialized or had already closed."")
    val conf = sqlContext.sessionState.conf
    val hiveSessionState = parentSession.getSessionState
    setConfMap(conf, hiveSessionState.getOverriddenConfigurations)
    setConfMap(conf, hiveSessionState.getHiveVariables)
    val runInBackground = async && conf.getConf(HiveUtils.HIVE_THRIFT_SERVER_ASYNC)
    val operation = new SparkExecuteStatementOperation(parentSession, statement, confOverlay,
      runInBackground)(sqlContext, sessionToActivePool)
    handleToOperation.put(operation.getHandle, operation)
    logDebug(s""Created Operation for $statement with session=$parentSession, "" +
      s""runInBackground=$runInBackground"")
    operation
  }
{code}

Below is an example which has memory leak:
 !26751.png! ",,cane,toopt4,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26701,,,,,,,,,,,,,,"28/Jan/19 11:35;cane;26751.png;https://issues.apache.org/jira/secure/attachment/12956569/26751.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 03 14:47:23 UTC 2019,,,,,,,,,,"0|yi0dq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/19 14:47;srowen;Issue resolved by pull request 23673
[https://github.com/apache/spark/pull/23673];;;",,,,,,,,,,,,,,,,,,,,,
Non-parsing Dataset.count() optimization causes inconsistent results for JSON inputs with empty lines,SPARK-26745,13212125,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gurwls223,sumitsu,sumitsu,28/Jan/19 00:39,12/Dec/22 18:11,13/Jul/23 08:46,31/Jan/19 06:34,2.4.0,3.0.0,,,,,,,2.4.1,3.0.0,,,SQL,,,,,0,correctness,,,,,"The optimization introduced by [SPARK-24959|https://issues.apache.org/jira/browse/SPARK-24959] (improving performance of {{{color:#0000FF}count(){color}}} for DataFrames read from non-multiline JSON in {{{color:#0000FF}PERMISSIVE{color}}} mode) appears to cause {{{color:#0000FF}count(){color}}} to erroneously include empty lines in its result total if run prior to JSON parsing taking place.

For the following input:

{code:json}
{ ""a"" : 1 , ""b"" : 2 , ""c"" : 3 }

        { ""a"" : 4 , ""b"" : 5 , ""c"" : 6 }
     
{ ""a"" : 7 , ""b"" : 8 , ""c"" : 9 }


{code}

*+Spark 2.3:+*

{code:scala}
scala> val df = spark.read.json(""sql/core/src/test/resources/test-data/with-empty-line.json"")
df: org.apache.spark.sql.DataFrame = [a: bigint, b: bigint ... 1 more field]

scala> df.count
res0: Long = 3

scala> df.cache.count
res3: Long = 3
{code}

*+Spark 2.4:+*

{code:scala}
scala> val df = spark.read.json(""sql/core/src/test/resources/test-data/with-empty-line.json"")
df: org.apache.spark.sql.DataFrame = [a: bigint, b: bigint ... 1 more field]

scala> df.count
res0: Long = 7

scala> df.cache.count
res1: Long = 3
{code}

Since the count is apparently updated and cached when the Jackson parser runs, the optimization also causes the count to appear to be unstable upon cache/persist operations, as shown above.

CSV inputs, also optimized via [SPARK-24959|https://issues.apache.org/jira/browse/SPARK-24959], do not appear to be impacted by this effect.",,maropu,sumitsu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24959,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 31 06:34:32 UTC 2019,,,,,,,,,,"0|yi0d68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Jan/19 00:54;sumitsu;opened PR with proposed resolution: https://github.com/apache/spark/pull/23665;;;","28/Jan/19 01:33;maropu;I checked I could reproduce this in master/branch-2.4, so I added correctness in the label.;;;","31/Jan/19 06:34;gurwls223;Fixed in https://github.com/apache/spark/pull/23667;;;",,,,,,,,,,,,,,,,,,,
Statistics for date and timestamp columns depend on system time zone,SPARK-26740,13212027,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,26/Jan/19 18:50,28/Feb/19 01:12,13/Jul/23 08:46,12/Feb/19 02:59,2.4.0,,,,,,,,2.4.1,3.0.0,,,SQL,,,,,1,,,,,,"While saving statistics for timestamp/date columns, default time zone is used in conversion of internal type (microseconds or days since epoch) to textual representation. The textual representation doesn't contain time zone. So, when it is converted back to internal types (Long for TimestampType or DateType), the Timestamp.valueOf and Date.valueOf are used in conversions. The methods use current system time zone.
If system time zone is different while saving and retrieving statistics for timestamp/date columns, restored microseconds/days since epoch will be different.",,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26654,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 12 02:59:40 UTC 2019,,,,,,,,,,"0|yi0ckg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/19 02:59;cloud_fan;Issue resolved by pull request 23662
[https://github.com/apache/spark/pull/23662];;;",,,,,,,,,,,,,,,,,,,,,
Executor/Task STDERR & STDOUT log urls are not correct in Yarn deployment mode,SPARK-26737,13211987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,devaraj,devaraj,26/Jan/19 07:05,17/May/20 18:14,13/Jul/23 08:46,30/Jan/19 19:55,3.0.0,,,,,,,,3.0.0,,,,Spark Core,Web UI,YARN,,,0,,,,,,"

Base of the STDERR & STDOUT log urls are generating like these which is also including key,

{code}
http://ip:8042/node/containerlogs/container_1544212645385_0252_01_000001/(SPARK_USER, devaraj)
{code}


{code}
http://ip:8042/node/containerlogs/container_1544212645385_0252_01_000001/(USER, devaraj)
{code}

Instead of {code}http://ip:8042/node/containerlogs/container_1544212645385_0251_01_000002/devaraj {code}
",,devaraj,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22404,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 30 19:55:58 UTC 2019,,,,,,,,,,"0|yi0cbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/19 19:55;vanzin;The patch for SPARK-26311 ended up fixing this issue too.;;;",,,,,,,,,,,,,,,,,,,,,
StackOverflowError on WAL serialization caused by large receivedBlockQueue,SPARK-26734,13211966,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eddardstark,eddardstark,eddardstark,26/Jan/19 00:19,17/May/20 18:21,13/Jul/23 08:46,06/Feb/19 16:45,2.3.1,2.3.2,2.4.0,,,,,,2.3.4,2.4.1,3.0.0,,Block Manager,DStreams,Spark Core,,,0,,,,,,"We encountered an intermittent StackOverflowError with a stack trace similar to:

 
{noformat}
Exception in thread ""JobGenerator"" java.lang.StackOverflowError
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509){noformat}
The name of the thread has been seen to be either ""JobGenerator"" or ""streaming-start"", depending on when in the lifecycle of the job the problem occurs.  It appears to only occur in streaming jobs with checkpointing and WAL enabled; this has prevented us from upgrading to v2.4.0.

 

Via debugging, we tracked this down to allocateBlocksToBatch in ReceivedBlockTracker:
{code:java}
/**
 * Allocate all unallocated blocks to the given batch.
 * This event will get written to the write ahead log (if enabled).
 */
def allocateBlocksToBatch(batchTime: Time): Unit = synchronized {
  if (lastAllocatedBatchTime == null || batchTime > lastAllocatedBatchTime) {
    val streamIdToBlocks = streamIds.map { streamId =>
      (streamId, getReceivedBlockQueue(streamId).clone())
    }.toMap
    val allocatedBlocks = AllocatedBlocks(streamIdToBlocks)
    if (writeToLog(BatchAllocationEvent(batchTime, allocatedBlocks))) {
      streamIds.foreach(getReceivedBlockQueue(_).clear())
      timeToAllocatedBlocks.put(batchTime, allocatedBlocks)
      lastAllocatedBatchTime = batchTime
    } else {
      logInfo(s""Possibly processed batch $batchTime needs to be processed again in WAL recovery"")
    }
  } else {
    // This situation occurs when:
    // 1. WAL is ended with BatchAllocationEvent, but without BatchCleanupEvent,
    // possibly processed batch job or half-processed batch job need to be processed again,
    // so the batchTime will be equal to lastAllocatedBatchTime.
    // 2. Slow checkpointing makes recovered batch time older than WAL recovered
    // lastAllocatedBatchTime.
    // This situation will only occurs in recovery time.
    logInfo(s""Possibly processed batch $batchTime needs to be processed again in WAL recovery"")
  }
}
{code}
Prior to 2.3.1, this code did
{code:java}
getReceivedBlockQueue(streamId).dequeueAll(x => true){code}
but it was changed as part of SPARK-23991 to
{code:java}
getReceivedBlockQueue(streamId).clone(){code}
We've not been able to reproduce this in a test of the actual above method, but we've been able to produce a test that reproduces it by putting a lot of values into the queue:

 
{code:java}
class SerializationFailureTest extends FunSpec {

  private val logger = LoggerFactory.getLogger(getClass)

  private type ReceivedBlockQueue = mutable.Queue[ReceivedBlockInfo]

  describe(""Queue"") {
    it(""should be serializable"") {
      runTest(1062)
    }
    it(""should not be serializable"") {
      runTest(1063)
    }
    it(""should DEFINITELY not be serializable"") {
      runTest(199952)
    }
  }

  private def runTest(mx: Int): Array[Byte] = {
    try {
      val random = new scala.util.Random()
      val queue = new ReceivedBlockQueue()
      for (_ <- 0 until mx) {
        queue += ReceivedBlockInfo(
          streamId = 0,
          numRecords = Some(random.nextInt(5)),
          metadataOption = None,
          blockStoreResult = WriteAheadLogBasedStoreResult(
            blockId = StreamBlockId(0, random.nextInt()),
            numRecords = Some(random.nextInt(5)),
            walRecordHandle = FileBasedWriteAheadLogSegment(
              path = s""""""hdfs://foo.bar.com:8080/spark/streaming/BAZ/00007/receivedData/0/log-${random.nextInt()}-${random.nextInt()}"""""",
              offset = random.nextLong(),
              length = random.nextInt()
            )
          )
        )
      }
      val record = BatchAllocationEvent(
        Time(1548320400000L), AllocatedBlocks(
          Map(
            0 -> queue
          )
        )
      )
      Utils.serialize(record)
    } catch {
      case t: Throwable =>
        fail(t)
    }
  }
}

{code}
In my tests it seemed like the serialization would fail if there were ~1064 elements in the queue.  I'm _assuming_ that this is actually a scala bug, though I haven't tried reproducing it without the involvement of the spark objects.

I expect this could be solved by transforming the cloned queue into a different type of Seq.

 ","spark 2.4.0 streaming job

java 1.8

scala 2.11.12",eddardstark,Spearsberg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 31 18:33:45 UTC 2019,,,,,,,,,,"0|yi0c6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/19 16:45;srowen;Issue resolved by pull request 23716
[https://github.com/apache/spark/pull/23716];;;","31/Dec/19 18:33;Spearsberg;Is this bug fixed? I still see the same error at Spark 2.4.3;;;",,,,,,,,,,,,,,,,,,,,
Flaky test: SparkContextInfoSuite.getRDDStorageInfo only reports on RDDs that actually persist data,SPARK-26732,13211945,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,25/Jan/19 22:13,30/Jan/19 15:14,13/Jul/23 08:46,30/Jan/19 15:14,2.3.2,2.4.0,3.0.0,,,,,,2.3.3,2.4.1,3.0.0,,Tests,,,,,0,,,,,,"From https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7/5437/testReport/junit/org.apache.spark/SparkContextInfoSuite/getRDDStorageInfo_only_reports_on_RDDs_that_actually_persist_data/:

{noformat}
Error Message
org.scalatest.exceptions.TestFailedException: 0 did not equal 1
Stacktrace
sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: 0 did not equal 1
	at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528)
	at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501)
	at org.apache.spark.SparkContextInfoSuite.$anonfun$new$3(SparkContextInfoSuite.scala:63)
{noformat}",,dongjoon,maropu,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 30 15:14:15 UTC 2019,,,,,,,,,,"0|yi0c28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Jan/19 21:54;dongjoon;cc [~maropu].;;;","30/Jan/19 00:38;maropu;Thanks for pinging me, dongjoon!;;;","30/Jan/19 15:14;maropu;Resolved by https://github.com/apache/spark/pull/23654;;;",,,,,,,,,,,,,,,,,,,
  Synchronize the amount of memory used by the broadcast variable to the UI display,SPARK-26726,13211762,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hantiantian,hantiantian,hantiantian,25/Jan/19 08:51,01/Feb/19 01:21,13/Jul/23 08:46,31/Jan/19 17:22,2.4.0,,,,,,,,2.3.3,2.4.1,3.0.0,,Spark Core,,,,,0,,,,,,"The amount of memory used by the broadcast variable is not synchronized to the UI display，

spark-sql>  select /*+ broadcast(a)*/ a.id,b.id from a join b on a.id = b.id;

View the app's driver log：

2019-01-25 16:45:23,726 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.43.xx.xx:33907 (size: 6.6 KB, free: 2.5 GB)
 2019-01-25 16:45:23,727 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.43.xx.xx:38399 (size: 6.6 KB, free: 2.5 GB)
 2019-01-25 16:45:23,745 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.43.xx.xx:33907 (size: 32.1 KB, free: 2.5 GB)
 2019-01-25 16:45:23,749 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.43.xx.xx:38399 (size: 32.1 KB, free: 2.5 GB)
 2019-01-25 16:45:23,838 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.43.xx.xx:38399 (size: 147.0 B, free: 2.5 GB)
 2019-01-25 16:45:23,840 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.43.xx.xx:33907 (size: 147.0 B, free: 2.5 GB)

 

Web UI does not have the use of memory，
||Executor ID||Address||Status||RDD Blocks||Storage Memory||Disk Used||Cores||Active Tasks||Failed Tasks||Complete Tasks||Total Tasks||Task Time (GC Time)||Input||Shuffle Read||Shuffle Write||Logs||Thread Dump||
|0|xxx:38399|Active|0|0.0 B / 2.7 GB|0.0 B|1|0|0|2|2|4 s (0.4 s)|8 B|0.0 B|0.0 B| | |
|driver|xxx:47936|Active|0|0.0 B / 384.1 MB|0.0 B|0|0|0|0|0|0.0 ms (0.0 ms)|0.0 B|0.0 B|0.0 B| | |
|1|xxx:47414|Active|0|0.0 B / 2.7 GB|0.0 B|1|0|0|0|0|0.0 ms (0.0 ms)|0.0 B|0.0 B|0.0 B| | |
|2|xxx:33907|Active|0|0.0 B / 2.7 GB|0.0 B|1|0|0|2|2|4 s (0.2 s)|4 B|0.0 B|0.0 B| | |

 

 ",,hantiantian,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 31 17:22:04 UTC 2019,,,,,,,,,,"0|yi0axk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/19 17:22;vanzin;Issue resolved by pull request 23649
[https://github.com/apache/spark/pull/23649];;;",,,,,,,,,,,,,,,,,,,,,
Bug in feature importance calculation in GBM (and possibly other decision tree classifiers),SPARK-26721,13211667,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,danjump,danjump,24/Jan/19 23:11,26/Oct/19 23:11,13/Jul/23 08:46,16/Feb/19 22:51,2.4.0,,,,,,,,3.0.0,,,,ML,,,,,0,,,,,,"The feature importance calculation in org.apache.spark.ml.classification.GBTClassificationModel.featureImportances follows a flawed implementation from scikit-learn resulting in incorrect importance values. This error was recently discovered and updated in scikit-learn version 0.20.0. This error is inherited in the spark implementation and needs to be fixed here as well.

As described in the scikit-learn release notes ([https://scikit-learn.org/stable/whats_new.html#version-0-20-0]):
{quote}Fix Fixed a bug in ensemble.GradientBoostingRegressor and ensemble.GradientBoostingClassifier to have feature importances summed and then normalized, rather than normalizing on a per-tree basis. The previous behavior over-weighted the Gini importance of features that appear in later stages. This issue only affected feature importances. #11176 by Gil Forsyth.
{quote}
Full discussion of this error and debate ultimately validating the correctness of the change can be found in the comment thread of the scikit-learn pull request: [https://github.com/scikit-learn/scikit-learn/pull/11176] 

 

I believe the main change required would be to the featureImportances function in mllib/src/main/scala/org/apache/spark/ml/tree/treeModels.scala , however, I do not have the experience to make this change myself.",,danjump,maropu,shahid,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28222,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 16 22:51:19 UTC 2019,,,,,,,,,,"0|yi0acg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/19 23:17;danjump;updated the priority to Blocker as this is a correctness issue.;;;","26/Jan/19 00:47;maropu;Unset the priority tag for now cuz its generally reserved for committers.
[~srowen] [~cloud_fan] [~mengxr] Anyone can check this? (I'm not familiar with this part..);;;","26/Jan/19 02:12;srowen;I would not call it a Blocker, but I'd welcome a pull request to evaluate it.;;;","16/Feb/19 22:51;srowen;Issue resolved by pull request 23773
[https://github.com/apache/spark/pull/23773];;;",,,,,,,,,,,,,,,,,,
Fixed integer overflow in SS kafka rateLimit calculation,SPARK-26718,13211637,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,linehrr,linehrr,linehrr,24/Jan/19 19:15,30/Jan/19 19:14,13/Jul/23 08:46,29/Jan/19 19:01,2.4.0,,,,,,,,2.3.3,2.4.1,3.0.0,,Structured Streaming,,,,,1,,,,,,"when running spark structured streaming using lib: `""org.apache.spark"" %% ""spark-sql-kafka-0-10"" % ""2.4.0""`, we keep getting error regarding current offset fetching:
{code:java}
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, qa2-hdp-4.acuityads.org, executor 2): java.lang.AssertionError: assertion failed: latest offs
et -9223372036854775808 does not equal -1
at scala.Predef$.assert(Predef.scala:170)
at org.apache.spark.sql.kafka010.KafkaMicroBatchInputPartitionReader.resolveRange(KafkaMicroBatchReader.scala:371)
at org.apache.spark.sql.kafka010.KafkaMicroBatchInputPartitionReader.<init>(KafkaMicroBatchReader.scala:329)
at org.apache.spark.sql.kafka010.KafkaMicroBatchInputPartition.createPartitionReader(KafkaMicroBatchReader.scala:314)
at org.apache.spark.sql.execution.datasources.v2.DataSourceRDD.compute(DataSourceRDD.scala:42)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
at org.apache.spark.scheduler.Task.run(Task.scala:121)
at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
{code}
for some reason, looks like fetchLatestOffset returned a Long.MIN_VALUE for one of the partitions. I checked the structured streaming checkpoint, that was correct, it's the currentAvailableOffset was set to Long.MIN_VALUE.

kafka broker version: 1.1.0.
lib we used:

{\{libraryDependencies += ""org.apache.spark"" %% ""spark-sql-kafka-0-10"" % ""2.4.0"" }}

how to reproduce:
basically we started a structured streamer and subscribed a topic of 4 partitions. then produced some messages into topic, job crashed and logged the stacktrace like above.

also the committed offsets seem fine as we see in the logs: 
{code:java}
=== Streaming Query ===
Identifier: [id = c46c67ee-3514-4788-8370-a696837b21b1, runId = 31878627-d473-4ee8-955d-d4d3f3f45eb9]
Current Committed Offsets: {KafkaV2[Subscribe[REVENUEEVENT]]: {""REVENUEEVENT"":{""0"":1}}}
Current Available Offsets: {KafkaV2[Subscribe[REVENUEEVENT]]: {""REVENUEEVENT"":{""0"":-9223372036854775808}}}
{code}
so spark streaming recorded the correct value for partition: 0, but the current available offsets returned from kafka is showing Long.MIN_VALUE. ",,apachespark,dongjoon,gsomogyi,kabhwan,linehrr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17813,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 29 19:01:59 UTC 2019,,,,,,,,,,"0|yi0a5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/19 22:02;linehrr;found the issue, it's the rateLimit calculation. if anyone set the `maxOffsetsPerTrigger` to Long.MaxValue. then this could happen.

in class KafkaMicroBatchReader.scala: 
{code:java}
private def rateLimit(
limit: Long,
from: PartitionOffsetMap,
until: PartitionOffsetMap): PartitionOffsetMap = {
val fromNew = kafkaOffsetReader.fetchEarliestOffsets(until.keySet.diff(from.keySet).toSeq)
val sizes = until.flatMap {
case (tp, end) =>
// If begin isn't defined, something's wrong, but let alert logic in getBatch handle it
from.get(tp).orElse(fromNew.get(tp)).flatMap { begin =>
val size = end - begin
logDebug(s""rateLimit $tp size is $size"")
if (size > 0) Some(tp -> size) else None
}
}
val total = sizes.values.sum.toDouble
if (total < 1) {
until
} else {
until.map {
case (tp, end) =>
tp -> sizes.get(tp).map { size =>
val begin = from.get(tp).getOrElse(fromNew(tp))
val prorate = limit * (size / total)
// Don't completely starve small topicpartitions
val off = begin + (if (prorate < 1) Math.ceil(prorate) else Math.floor(prorate)).toLong
// Paranoia, make sure not to return an offset that's past end
Math.min(end, off)
}.getOrElse(end)
}
}
}
{code}
this rateLimit function is the trouble where if limit is set to Long.MaxValue, it could have integer overflow, showing in below example: 
{code:java}
val begin = 100
val limit = Long.MaxValue
val size = 5933L
val total = 5933L.toDouble

val prorate = limit * (size / total)
// Don't completely starve small topicpartitions
val off = begin + (if (prorate < 1) Math.ceil(prorate) else Math.floor(prorate)).toLong

println(off)

// prints -9223372036854775709{code}
root cause is `limit * (size/total)', it would lose precision due to Double type and then `begin + Math.floor(prorate)` will overflow.  

 ;;;","24/Jan/19 22:05;linehrr;A simple fix would be a if statement to check if the integer overflowed, and then throw some exception to notify the user. ;;;","24/Jan/19 23:22;kabhwan;[~linehrr]
Thanks for the analysis. I think allowing Long.MaxValue to end users sounds convenient and end users already use it, so ensuring 'off' to not being overflowed would be ideal. This is simply ensured via modifying the code as:

{code}
val prorate = limit * (size / total)
val prorateLong = (if (prorate < 1) Math.ceil(prorate) else Math.floor(prorate)).toLong
val off = if (prorateLong > Long.MaxValue - begin) Long.MaxValue else begin + propateLong
Math.min(end, off)
{code}

Please submit a patch if you would like to. If you wouldn't submit a patch please let me know that I can take this up. Thanks!;;;","24/Jan/19 23:52;gsomogyi;+1 on [~kabhwan] suggestion;;;","25/Jan/19 15:28;linehrr;[~kabhwan] cool, I will make a PR soon. thank you. ;;;","25/Jan/19 16:47;linehrr;[~kabhwan], [~gsomogyi]

PR submitted : https://github.com/apache/spark/pull/23652;;;","25/Jan/19 17:24;apachespark;User 'linehrr' has created a pull request for this issue:
https://github.com/apache/spark/pull/23652;;;","29/Jan/19 19:01;dongjoon;This is resolved via https://github.com/apache/spark/pull/23666;;;",,,,,,,,,,,,,,
The job whose partiton num is zero not shown in WebUI,SPARK-26714,13211485,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,deshanxiao,deshanxiao,deshanxiao,24/Jan/19 08:36,02/Feb/19 00:41,13/Jul/23 08:46,02/Feb/19 00:38,2.3.1,2.4.0,,,,,,,3.0.0,,,,Web UI,,,,,0,,,,,,"When the job's partiton is zero, it will still get a jobid but not shown in ui.I think it's strange.

Example:

mkdir /home/test/testdir

sc.textFile(""/home/test/testdir"")",,deshanxiao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 02 00:38:51 UTC 2019,,,,,,,,,,"0|yi0980:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/19 00:38;srowen;Issue resolved by pull request 23637
[https://github.com/apache/spark/pull/23637];;;",,,,,,,,,,,,,,,,,,,,,
PipedRDD may holds stdin writer and stdout read threads even if the task is finished,SPARK-26713,13211481,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,advancedxy,advancedxy,advancedxy,24/Jan/19 07:56,18/Sep/19 16:06,13/Jul/23 08:46,28/Jan/19 16:54,2.1.3,2.2.3,2.3.0,2.3.1,2.3.2,2.4.0,,,2.4.5,3.0.0,,,Spark Core,,,,,0,,,,,,"During an investigation of OOM of one internal production job, I found that PipedRDD leaks memory. After some digging, the problem lies down to the fact that PipedRDD doesn't release stdin writer and stdout threads even if the task is finished.

 

PipedRDD creates two threads: stdin writer and stdout reader. If we are lucky and the task is finished normally, these two threads exit normally. If the subprocess(pipe command) is failed, the task will be marked failed, however the stdin writer will be still running until it consumes its parent RDD's iterator. There is even a race condition with ShuffledRDD + PipedRDD: the ShuffleBlockFetchIterator is cleaned up at task completion and hangs stdin writer thread, which leaks memory. ",,advancedxy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 28 16:54:40 UTC 2019,,,,,,,,,,"0|yi0974:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/19 07:57;advancedxy;I have fixed and tested this issue in our internal cluster, will submit a PR soon.;;;","28/Jan/19 16:54;srowen;Issue resolved by pull request 23638
[https://github.com/apache/spark/pull/23638];;;",,,,,,,,,,,,,,,,,,,,
JSON Schema inference takes 15 times longer,SPARK-26711,13211440,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,24/Jan/19 03:13,12/Dec/22 18:10,13/Jul/23 08:46,26/Jan/19 00:16,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"I noticed that the first benchmark/case of JSONBenchmark (""JSON schema inferring"", ""No encoding"") was taking an hour to run, when it used to run in 4-5 minutes.

The culprit seems to be this commit: [https://github.com/apache/spark/commit/d72571e51d]

A quick look using a profiler, and it seems to be spending 99% of its time doing some kind of exception handling in JsonInferSchema.scala.

You can reproduce in the spark-shell by recreating the data used by the benchmark
{noformat}
scala> :paste
val rowsNum = 100 * 1000 * 1000
spark.sparkContext.range(0, rowsNum, 1)
        .map(_ => ""a"")
        .toDF(""fieldA"")
        .write
        .option(""encoding"", ""UTF-8"")
        .json(""utf8.json"")

// Entering paste mode (ctrl-D to finish)

// Exiting paste mode, now interpreting.
rowsNum: Int = 100000000
scala> 
{noformat}
Then you can run the test by hand starting spark-shell as so (emulating SqlBasedBenchmark):
{noformat}
 bin/spark-shell --driver-memory 8g \
  --conf ""spark.sql.autoBroadcastJoinThreshold=1"" \
  --conf ""spark.sql.shuffle.partitions=1"" --master ""local[1]""
{noformat}
On commit d72571e51d:
{noformat}
scala> val start = System.currentTimeMillis; spark.read.json(""utf8.json"");  System.currentTimeMillis-start
start: Long = 1548297682225
res0: Long = 815978 <== 13.6 minutes
scala>
{noformat}
On the previous commit (86100df54b):
{noformat}
scala> val start = System.currentTimeMillis; spark.read.json(""utf8.json"");  System.currentTimeMillis-start
start: Long = 1548298927151
res0: Long = 50087 <= 50 seconds
scala> 
{noformat}
I also tried {{spark.read.option(""inferTimestamp"", false).json(""utf8.json"")}}, but that option didn't seem to make a difference in run time. Edit: {{inferTimestamp}} does, in fact, have an impact: It halves the run time. However, that means even with {{inferTimestamp}}, the run time is still 7 times slower than before.",,bersprockets,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 26 00:16:09 UTC 2019,,,,,,,,,,"0|yi08y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/19 04:22;bersprockets;ping [~maxgekk] [~hyukjin.kwon];;;","24/Jan/19 04:40;gurwls223;So how was the time if {{inferTimestamp}} was enable/disabled? It would be odd even if there's regression with {{inferTimestamp}} disabled. It just compares one if-else.;;;","24/Jan/19 05:46;bersprockets;[~hyukjin.kwon]
 inferTimestamp=<default>: ~13 min
 inferTimestamp=false: ~7 min

7 minutes is a lot better than 13 minutes, but still not as good as 50 seconds.

A quick look in the profiler shows that in the case where inferTimestamp is _disabled_, Spark is spending 96% of its time here:
{code:java}
val bigDecimal = decimalParser(field)
{code}
That line did change in the original commit.;;;","24/Jan/19 06:04;gurwls223;Hm, the results say something is wrong hm. 50 sec <> 7 mins sounds serious. ;;;","24/Jan/19 22:33;bersprockets;Re: 7 minutes vs. 50 seconds:

Looking at the code, it appears the difference is this:

Before the timestamp inference change, options.prefersDecimal was checked before attempting to convert the String to a BigDecimal. If options.prefersDecimal is disabled, we would not bother with the conversion.

After the timestamp inference change, we always attempt to convert the String to a BigDecimal regardless of the setting of options.prefersDecimal (we still use options.prefersDecimal to determine what type to return)

My guess is that attempting to convert every string to a BigDecimal is very expensive.;;;","25/Jan/19 09:56;gurwls223;Oh, right. Sounds a good lead to follow. We can just add `lazy val decimalTry` to that val in that case. Can you try and make a PR? ;;;","25/Jan/19 09:58;gurwls223;Just open a PR that replace one line after manually testing it. I don't think we should update the benchmark again since you're going to update it in https://github.com/apache/spark/pull/23336;;;","25/Jan/19 14:31;bersprockets;[~hyukjin.kwon] Ok, that worked. I had in my mind a more verbose fix. I will open a PR with the one line change.;;;","26/Jan/19 00:16;dongjoon;This is resolved via https://github.com/apache/spark/pull/23653;;;",,,,,,,,,,,,,
OptimizeMetadataOnlyQuery does not correctly handle the files with zero record,SPARK-26709,13211402,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,Gengliang.Wang,smilegator,smilegator,23/Jan/19 22:56,02/Feb/21 05:55,13/Jul/23 08:46,26/Jan/19 00:29,2.1.0,2.1.3,2.2.3,2.3.2,2.4.0,,,,2.3.3,2.4.1,3.0.0,,SQL,,,,,0,correctness,,,,,"{code:java}
import org.apache.spark.sql.functions.lit
withSQLConf(SQLConf.OPTIMIZER_METADATA_ONLY.key -> ""true"") {
  withTempPath { path =>
    val tabLocation = path.getAbsolutePath
    val partLocation = new Path(path.getAbsolutePath, ""partCol1=3"")
    val df = spark.emptyDataFrame.select(lit(1).as(""col1""))
    df.write.parquet(partLocation.toString)
    val readDF = spark.read.parquet(tabLocation)
    checkAnswer(readDF.selectExpr(""max(partCol1)""), Row(null))
    checkAnswer(readDF.selectExpr(""max(col1)""), Row(null))
  }
}
{code}

OptimizeMetadataOnlyQuery has a correctness bug to handle the file with the empty records for partitioned tables. The above test will fail in 2.4, which can generate an empty file, but the underlying issue in the read path still exists in 2.3, 2.2 and 2.1. ",,apachespark,Gengliang.Wang,maropu,smilegator,yuwang0917@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26996,,,,SPARK-15752,,,SPARK-34194,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 28 12:20:21 UTC 2019,,,,,,,,,,"0|yi08pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/19 00:59;maropu;Anyone is already working on this?;;;","24/Jan/19 04:40;Gengliang.Wang;[~maropu] I can take it. Are you working on it?;;;","24/Jan/19 05:11;maropu;I looked over the code though, not yet. plz do it ;);;;","25/Jan/19 07:05;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/23648;;;","25/Jan/19 07:06;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/23648;;;","28/Jan/19 12:20;maropu;Resolved by https://github.com/apache/spark/pull/23635;;;",,,,,,,,,,,,,,,,
Incorrect result caused by inconsistency between a SQL cache's cached RDD and its physical plan,SPARK-26708,13211373,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,maryannxue,smilegator,smilegator,23/Jan/19 20:54,06/Feb/19 00:41,13/Jul/23 08:46,29/Jan/19 12:36,2.4.0,,,,,,,,2.4.1,3.0.0,,,SQL,,,,,0,correctness,,,,,"When performing non-cascading cache invalidation, {{recache}} is called on the other cache entries which are dependent on the cache being invalidated. It leads to the the physical plans of those cache entries being re-compiled. For those cache entries, if the cache RDD has already been persisted, chances are there will be inconsistency between the data and the new plan. It can cause a correctness issue if the new plan's {{outputPartitioning}} or {{outputOrdering}} is different from the that of the actual data, and meanwhile the cache is used by another query that asks for specific {{outputPartitioning}} or {{outputOrdering}} which happens to match the new plan but not the actual data.",,bersprockets,dongjoon,maropu,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 05 23:01:07 UTC 2019,,,,,,,,,,"0|yi08j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/19 06:17;dongjoon;Hi, [~smilegator]. Is this only related to Spark 2.4.0?;;;","26/Jan/19 00:49;maropu;I also want to know that this is related to branch-2.3...;;;","28/Jan/19 19:43;smilegator;I do not think this affects 2.3;;;","29/Jan/19 01:48;maropu;Yea, thanks for the answer!;;;","29/Jan/19 12:36;maropu;Resolved by https://github.com/apache/spark/pull/23644;;;","05/Feb/19 23:01;bersprockets;How does one hit this issue?

Edit: Ah, never mind. I see there is a test.;;;",,,,,,,,,,,,,,,,
Console progress bar not showing in 3.0,SPARK-26694,13211120,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ankur.gupta,vanzin,vanzin,22/Jan/19 22:24,25/Jan/19 18:25,13/Jul/23 08:46,25/Jan/19 18:22,3.0.0,,,,,,,,3.0.0,,,,Spark Core,,,,,0,,,,,,"This is the code that initializes it:

{code}
    _progressBar =
      if (_conf.get(UI_SHOW_CONSOLE_PROGRESS) && !log.isInfoEnabled) {
        Some(new ConsoleProgressBar(this))
      } else {
        None
      }
{code}

SPARK-25118 changed the way the log system is initialized, and the code above is not initializing the progress bar anymore.

[~ankur.gupta]",,ankur.gupta,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 25 18:22:01 UTC 2019,,,,,,,,,,"0|yi06z4:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,"22/Jan/19 23:26;ankur.gupta;I am working on this;;;","25/Jan/19 18:22;vanzin;Issue resolved by pull request 23618
[https://github.com/apache/spark/pull/23618];;;",,,,,,,,,,,,,,,,,,,,
Checkpoints of Dataframes are not visible in the SQL UI,SPARK-26690,13210979,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tomvanbussel,tomvanbussel,tomvanbussel,22/Jan/19 13:46,24/Jan/19 15:46,13/Jul/23 08:46,24/Jan/19 15:46,2.4.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,Checkpoints and local checkpoints of dataframes do not show up in the SQL UI.,,tomvanbussel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-01-22 13:46:50.0,,,,,,,,,,"0|yi063s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task attempt ID collision causes lost data,SPARK-26682,13210863,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,rdblue,rdblue,rdblue,22/Jan/19 01:20,02/Mar/20 19:45,13/Jul/23 08:46,24/Jan/19 04:54,2.1.0,2.1.3,2.3.2,2.4.0,,,,,2.3.3,2.4.1,3.0.0,,SQL,,,,,0,data-loss,,,,,"We recently tracked missing data to a collision in the fake Hadoop task attempt ID created when using Hadoop OutputCommitters. This is similar to SPARK-24589.

A stage had one task fail to get one shard from a shuffle, causing a FetchFailedException and Spark resubmitted the stage. Because only one task was affected, the original stage attempt continued running tasks that had been resubmitted. Another task ran two attempts concurrently on the same executor, but had the same attempt number because they were from different stage attempts. Because the attempt number was the same, the task used the same temp locations. That caused one attempt to fail because a file path already existed, and that attempt then removed the shared temp location and deleted the other task's data. When the second attempt succeeded, it committed partial data.

The problem was that both attempts had the same partition and attempt numbers, despite being run in different stages, and that was used to create a Hadoop task attempt ID on which the temp location was based. The fix is to use Spark's global task attempt ID, which is a counter, instead of attempt number because attempt number is reused in stage attempts.",,cloud_fan,rdblue,rdub,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20213,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 24 22:32:07 UTC 2019,,,,,,,,,,"0|yi05e8:",9223372036854775807,,,,,,,,,,,,,2.3.3,2.4.1,,,,,,,,,,"23/Jan/19 22:37;zsxwing;IIUC, this issue will cause a file deletion (delete the temp file) and a file rename (move the temp file to the target file) happen at the same time. Could you clarify why this will cause a task committed partial data? I think the file rename should either move the whole file to the target file, or just fail, right?;;;","24/Jan/19 04:54;cloud_fan;Issue resolved by pull request 23608
[https://github.com/apache/spark/pull/23608];;;","24/Jan/19 22:32;zsxwing;For future reference, data loss could happen when one task modified the other task's temp output file.;;;",,,,,,,,,,,,,,,,,,,
StackOverflowError if Stream passed to groupBy,SPARK-26680,13210853,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,21/Jan/19 23:48,27/Oct/20 11:02,13/Jul/23 08:46,24/Jan/19 10:23,2.3.2,2.4.0,3.0.0,,,,,,2.3.3,2.4.1,3.0.0,,SQL,,,,,0,,,,,,"This Java code results in a StackOverflowError:
{code:java}
List<Column> groupByCols = new ArrayList<>();
groupByCols.add(new Column(""id1""));
scala.collection.Seq<Column> groupByColsSeq =
    JavaConverters.asScalaIteratorConverter(groupByCols.iterator())
        .asScala().toSeq();
df.groupBy(groupByColsSeq).max(""id2"").toDF(""id1"", ""id2"").show();
{code}
The {{toSeq}} method above produces a Stream. Passing a Stream to groupBy results in the StackOverflowError. In fact, the error can be produced more easily in spark-shell:
{noformat}
scala> val df = spark.read.schema(""id1 int, id2 int"").csv(""testinput.csv"")
df: org.apache.spark.sql.DataFrame = [id1: int, id2: int]
scala> val groupBySeq = Stream(col(""id1""))
groupBySeq: scala.collection.immutable.Stream[org.apache.spark.sql.Column] = Stream(id1, ?)
scala> df.groupBy(groupBySeq: _*).max(""id2"").toDF(""id1"", ""id2"").collect
java.lang.StackOverflowError
  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1161)
  at scala.collection.immutable.Stream.drop(Stream.scala:797)
  at scala.collection.immutable.Stream.drop(Stream.scala:204)
  at scala.collection.LinearSeqOptimized.apply(LinearSeqOptimized.scala:66)
  at scala.collection.LinearSeqOptimized.apply$(LinearSeqOptimized.scala:65)
  at scala.collection.immutable.Stream.apply(Stream.scala:204)
  at org.apache.spark.sql.catalyst.expressions.BoundReference.doGenCode(BoundAttribute.scala:45)
  at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:138)
  at scala.Option.getOrElse(Option.scala:138)
  at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:133)
  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$consume$3(WholeStageCodegenExec.scala:159)
  at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:418)
  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1171)
  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1161)
  at scala.collection.immutable.Stream.drop(Stream.scala:797)
  at scala.collection.immutable.Stream.drop(Stream.scala:204)
  at scala.collection.LinearSeqOptimized.apply(LinearSeqOptimized.scala:66)
  at scala.collection.LinearSeqOptimized.apply$(LinearSeqOptimized.scala:65)
  at scala.collection.immutable.Stream.apply(Stream.scala:204)
  at org.apache.spark.sql.catalyst.expressions.BoundReference.doGenCode(BoundAttribute.scala:45)
  at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:138)
  at scala.Option.getOrElse(Option.scala:138)
  at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:133)
  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$consume$3(WholeStageCodegenExec.scala:159)
...etc...
{noformat}
This is due to the lazy nature of Streams. The method {{consume}} in {{CodegenSupport}} assumes that a map function will be eagerly evaluated:
{code:java}
val inputVars =
        ctx.currentVars = null <== the closure cares about this
        ctx.INPUT_ROW = row
        output.zipWithIndex.map { case (attr, i) =>
          BoundReference(i, attr.dataType, attr.nullable).genCode(ctx)
-
-
-
    ctx.currentVars = inputVars
    ctx.INPUT_ROW = null
    ctx.freshNamePrefix = parent.variablePrefix
    val evaluated = evaluateRequiredVariables(output, inputVars, parent.usedInputs)
{code}
The closure passed to the map function assumes {{ctx.currentVars}} will be set to null. But due to lazy evaluation, {{ctx.currentVars}} is set to something else by the time the closure is actually called. Worse yet, {{ctx.currentVars}} is set to the yet-to-be evaluated inputVars stream. The closure uses {{ctx.currentVars}} (via the call {{genCode(ctx)}}), therefore it ends up using the data structure it is attempting to create.

You can recreate the problem is a vanilla Scala shell:
{code:java}
scala> var p1: Seq[Any] = null
p1: Seq[Any] = null
scala> val s = Stream(1, 2).zipWithIndex.map { case (x, i) => if (p1 != null) p1(i) else x }
s: scala.collection.immutable.Stream[Any] = Stream(1, ?)
scala> p1 = s
p1: Seq[Any] = Stream(1, ?)
scala> s.foreach(println)
1
java.lang.StackOverflowError
  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1166)
  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159)
  at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415)
  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169)
  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159)
... etc ...
{code}
Possible fixes:
 - In {{DataSet.groupBy}}, we could ensure the passed Seq is a List before passing it to RelationalGroupedDataset (simply by changing {{cols.map(_.expr)}} to {{cols.toList.map(_.expr)}}
 - In {{CodegenSupport.consume}}, we could ensure that the map function is eagerly evaluated (simply by moving the existing match statement to handle the result from either path of the if statement).
 - Something else that hasn't occurred to me (opinions welcome).",,apachespark,bersprockets,gsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33260,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 22 15:07:09 UTC 2019,,,,,,,,,,"0|yi05c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/19 23:52;bersprockets;I will make a PR for this, but I would like to hear any suggested solutions beyond the two that I proposed above.;;;","22/Jan/19 15:07;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/23617;;;",,,,,,,,,,,,,,,,,,,,
Incorrect results of not(eqNullSafe) when data read from Parquet file ,SPARK-26677,13210795,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,rdblue,kapalka,kapalka,21/Jan/19 15:25,12/Dec/22 18:10,13/Jul/23 08:46,02/Feb/19 17:19,2.4.0,,,,,,,,2.4.1,3.0.0,,,SQL,,,,,0,correctness,,,,,"Example code (spark-shell from Spark 2.4.0):
{code:java}
scala> Seq(""A"", ""A"", null).toDS.repartition(1).write.parquet(""t"")

scala> spark.read.parquet(""t"").where(not(col(""value"").eqNullSafe(""A""))).show
+-----+
|value|
+-----+
+-----+
{code}
Running the same with Spark 2.2.0 or 2.3.2 gives the correct result:
{code:java}
scala> spark.read.parquet(""t"").where(not(col(""value"").eqNullSafe(""A""))).show
+-----+
|value|
+-----+
| null|
+-----+

{code}
Also, with a different input sequence and Spark 2.4.0 we get the correct result:
{code:java}
scala> Seq(""A"", null).toDS.repartition(1).write.parquet(""t"")

scala> spark.read.parquet(""t"").where(not(col(""value"").eqNullSafe(""A""))).show
+-----+
|value|
+-----+
| null|
+-----+
{code}","Local installation of Spark on Linux (Java 1.8, Ubuntu 18.04).",aaruna,anandchinn,dongjoon,kapalka,maropu,rdblue,,,,,,,,,,,,,,,,,,,,,,,,,,,,PARQUET-1510,,,,,,PARQUET-1309,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 02 17:19:18 UTC 2019,,,,,,,,,,"0|yi04z4:",9223372036854775807,,,,,,,,,,,,,2.4.1,,,,,,,,,,,"22/Jan/19 16:21;anandchinn;I have done the analysis for this bug. Below are the initial analysis 

scala> Seq(""A"", ""B"", null).toDS.repartition(1).write.parquet(""t3"");

scala> spark.read.parquet(""t3"").where(not(col(""value"").eqNullSafe(""A""))).show;

+-----+
|value|

+-----+
|    B|
|null|

+-----+

When the issue happens only if the columns have duplicate row data. Will do further analysis ;;;","23/Jan/19 05:20;gurwls223;Im gonna open a PR soon.;;;","25/Jan/19 00:53;anandchinn;[~hyukjin.kwon] - Do you know exactly the issue is from ParquetFileReader, The file reader was an issue with override the duplicate row keys.

Let me know your thoughts. 

 ;;;","25/Jan/19 01:08;gurwls223;Yes, please read the linked PR above.;;;","25/Jan/19 04:28;dongjoon;Thank you, [~anandchinn] and [~hyukjin.kwon].
So, according to the PR and PARQUET-1309, only Parquet 1.10.0 (used in Spark 2.4.0) version has this issue.;;;","25/Jan/19 19:42;rdblue;To clarify [~dongjoon]'s comment: All recent versions of Parquet are affected by this {{not(eqNullSafe(...)}} bug. Only Parquet 1.10.0 is affected by PARQUET-1309.

This filter bug has been present since Parquet introduced dictionary filtering.;;;","25/Jan/19 19:55;dongjoon;Yep. Correct. There were two mixed issues. Thank you for clarifying.;;;","30/Jan/19 22:11;dongjoon;Hi, [~rdblue]. I moved `2.4.1` from `Fixed Versions` field to `Target Versions` since it's not merged to `branch-2.4` yet.;;;","30/Jan/19 23:01;rdblue;Thanks, sorry about the mistake.;;;","02/Feb/19 17:19;dongjoon;This is resolved via https://github.com/apache/spark/pull/23704;;;",,,,,,,,,,,,
BlockTransferService.fetchBlockSync may hang forever,SPARK-26665,13210473,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,18/Jan/19 21:36,06/Feb/19 16:52,13/Jul/23 08:46,22/Jan/19 17:02,2.3.0,2.3.1,2.3.2,2.4.0,,,,,2.3.3,2.4.1,3.0.0,,Spark Core,,,,,0,,,,,,"`ByteBuffer.allocate` may throw OutOfMemoryError when the block is large but no enough memory is available. However, when this happens, right now BlockTransferService.fetchBlockSync will just hang forever.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 18 22:01:16 UTC 2019,,,,,,,,,,"0|yi02zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/19 22:01;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/23590;;;",,,,,,,,,,,,,,,,,,,,,
Show actual class name of the writing command in CTAS explain,SPARK-26661,13210324,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,18/Jan/19 08:30,22/Jan/19 21:59,13/Jul/23 08:46,22/Jan/19 21:56,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"The explain output of the Hive CTAS command, regardless of whether it's actually writing via Hive's SerDe or converted into using Spark's data source, would always show that it's using {{InsertIntoHiveTable}} because it's hardcoded.

e.g.
{code:none}
Execute OptimizedCreateHiveTableAsSelectCommand [Database:default, TableName: foo, InsertIntoHiveTable]
{code}
This CTAS is converted into using Spark's data source, but it still says {{InsertIntoHiveTable}} in the explain output.

It's better to show the actual class name of the writing command used. For the example above, it'd be:
{code:none}
Execute OptimizedCreateHiveTableAsSelectCommand [Database:default, TableName: foo, InsertIntoHadoopFsRelationCommand]
{code}",,dongjoon,rednaxelafx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 22 21:56:31 UTC 2019,,,,,,,,,,"0|yi022o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/19 21:56;dongjoon;This is resolved via https://github.com/apache/spark/pull/23582;;;",,,,,,,,,,,,,,,,,,,,,
Duplicate cmd.nodeName in the explain output of DataWritingCommandExec,SPARK-26659,13210286,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,18/Jan/19 01:53,18/Jan/19 06:47,13/Jul/23 08:46,18/Jan/19 06:44,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"{{DataWritingCommandExec}} generates {{cmd.nodeName}} twice in its explain output, e.g. when running this query

{code:none}
spark.sql(""create table foo stored as parquet as select id, id % 10 as cat1, id % 20 as cat2 from range(10)"")
{code}

The query plan's explain output would be:

{code:none}
Execute OptimizedCreateHiveTableAsSelectCommand OptimizedCreateHiveTableAsSelectCommand [Database:default, TableName: foo, InsertIntoHiveTable]
+- *(1) Project [id#2L, (id#2L % 10) AS cat1#0L, (id#2L % 20) AS cat2#1L]
   +- *(1) Range (0, 10, step=1, splits=8)
{code}",,apachespark,rednaxelafx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 18 01:55:35 UTC 2019,,,,,,,,,,"0|yi01u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/19 01:55;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/23579;;;",,,,,,,,,,,,,,,,,,,,,
Yarn Client throws 'ClassNotFoundException: org.apache.hadoop.hbase.HBaseConfiguration',SPARK-26650,13210232,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,devaraj,devaraj,17/Jan/19 19:55,12/Dec/22 18:10,13/Jul/23 08:46,14/Feb/19 03:40,3.0.0,,,,,,,,3.0.0,,,,Build,Spark Core,YARN,,,0,,,,,,"{code:xml}
19/01/17 11:33:00 WARN security.HBaseDelegationTokenProvider: Fail to invoke HBaseConfiguration
java.lang.ClassNotFoundException: org.apache.hadoop.hbase.HBaseConfiguration
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at org.apache.spark.deploy.security.HBaseDelegationTokenProvider.hbaseConf(HBaseDelegationTokenProvider.scala:69)
        at org.apache.spark.deploy.security.HBaseDelegationTokenProvider.delegationTokensRequired(HBaseDelegationTokenProvider.scala:62)
        at org.apache.spark.deploy.security.HadoopDelegationTokenManager.$anonfun$obtainDelegationTokens$1(HadoopDelegationTokenManager.scala:134)
        at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:244)
        at scala.collection.Iterator.foreach(Iterator.scala:941)
        at scala.collection.Iterator.foreach$(Iterator.scala:941)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
        at scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:213)
        at scala.collection.TraversableLike.flatMap(TraversableLike.scala:244)
        at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:241)
        at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
        at org.apache.spark.deploy.security.HadoopDelegationTokenManager.obtainDelegationTokens(HadoopDelegationTokenManager.scala:133)
        at org.apache.spark.deploy.yarn.security.YARNHadoopDelegationTokenManager.obtainDelegationTokens(YARNHadoopDelegationTokenManager.scala:59)
        at org.apache.spark.deploy.yarn.Client.setupSecurityToken(Client.scala:305)
        at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:1014)
        at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:181)
        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:58)
        at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:184)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:509)
        at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2466)
        at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$5(SparkSession.scala:948)
        at scala.Option.getOrElse(Option.scala:138)
        at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
        at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:30)
        at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:853)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:168)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:196)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:87)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:932)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:941)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
19/01/17 11:33:00 INFO yarn.Client: Submitting application application_1544212645385_0197 to ResourceManager
19/01/17 11:33:00 INFO impl.YarnClientImpl: Submitted application application_1544212645385_0197
{code}",,devaraj,roczei,shahid,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 14 03:40:55 UTC 2019,,,,,,,,,,"0|yi01i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/19 19:28;shahid;Thanks. I am working on it.;;;","12/Feb/19 21:01;vanzin;So nothing is being *thrown* here. It's just an exception being logged.

Agree that it's unnecessarily noisy, but it's not broken.;;;","14/Feb/19 03:40;gurwls223;Issue resolved by pull request 23776
[https://github.com/apache/spark/pull/23776];;;",,,,,,,,,,,,,,,,,,,
"CSV infer schema bug infers decimal(9,-1)",SPARK-26645,13210164,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mgaido,uzadude,uzadude,17/Jan/19 14:12,12/Dec/22 18:10,13/Jul/23 08:46,20/Jan/19 09:46,2.3.0,2.4.7,,,,,,,2.4.8,3.0.0,,,SQL,,,,,0,,,,,,"we have a file /tmp/t1/file.txt that contains only one line ""1.18927098E9"".
running:
{code:python}
df = spark.read.csv('/tmp/t1', header=False, inferSchema=True, sep='\t')
print df.dtypes
{code}

causes:
{noformat}
ValueError: Could not parse datatype: decimal(9,-1)
{noformat}

I'm not sure where the bug is - inferSchema or dtypes?
I saw it is legal to have a decimal with negative scale in the code (CSVInferSchema.scala):
{code:python}
if (bigDecimal.scale <= 0) {
        // `DecimalType` conversion can fail when
        //   1. The precision is bigger than 38.
        //   2. scale is bigger than precision.
        DecimalType(bigDecimal.precision, bigDecimal.scale)
      } 
{code}
but what does it mean?",,apachespark,bullsoverbears,dongjoon,mgaido,uzadude,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33445,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 25 23:01:28 UTC 2020,,,,,,,,,,"0|yi013k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/19 14:36;mgaido;The error is on python side, I will submit a PR shortly, thanks for reporting this.;;;","20/Jan/19 09:46;gurwls223;Issue resolved by pull request 23575
[https://github.com/apache/spark/pull/23575];;;","19/Nov/20 21:42;dongjoon;Do you need this, [~bullsoverbears]?;;;","25/Nov/20 15:45;bullsoverbears;Hello [~dongjoon] If we can get this PR then this would be tremendously helpful.;;;","25/Nov/20 19:04;dongjoon;Okay. Let me make a backporting PR to branch-2.4, [~bullsoverbears].;;;","25/Nov/20 19:08;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30503;;;","25/Nov/20 23:01;dongjoon;This landed `branch-2.4` via https://github.com/apache/spark/pull/30503 .;;;",,,,,,,,,,,,,,,
Pyspark vector classes always return error for unary negation,SPARK-26638,13210027,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,srowen,srowen,16/Jan/19 23:07,06/Feb/19 16:52,13/Jul/23 08:46,17/Jan/19 20:25,2.3.2,2.4.0,,,,,,,2.3.3,2.4.1,3.0.0,,ML,PySpark,,,,0,,,,,,"It looks like the implementation of {{__neg__}} for Pyspark vector classes is wrong:

{code}
    def _delegate(op):
        def func(self, other):
            if isinstance(other, DenseVector):
                other = other.array
            return DenseVector(getattr(self.array, op)(other))
        return func

    __neg__ = _delegate(""__neg__"")
{code}

This delegation works for binary operators but not for unary, and indeed, it doesn't work at all:

{code}
from pyspark.ml.linalg import DenseVector
v = DenseVector([1,2,3])
-v
...
TypeError: func() missing 1 required positional argument: 'other'
{code}

This was spotted by static analyis on lgtm.com:
https://lgtm.com/projects/g/apache/spark/alerts/?mode=tree&lang=python&ruleFocus=7850093

Easy to fix and add a test for, as I presume we want this to be implemented.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 17 20:25:41 UTC 2019,,,,,,,,,,"0|yi009s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/19 20:25;srowen;Issue resolved by pull request 23570
[https://github.com/apache/spark/pull/23570];;;",,,,,,,,,,,,,,,,,,,,,
illegal hardware instruction,SPARK-26635,13209828,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,elch10,elch10,16/Jan/19 08:55,16/Jan/19 08:57,13/Jul/23 08:46,16/Jan/19 08:57,2.4.0,,,,,,,,,,,,PySpark,Spark Core,,,,0,,,,,,"I can't import pyarrow:
{code:java}
>>> import pyarrow as pa
[1]    31441 illegal hardware instruction (core dumped)  python3{code}
The environment is:

Python 3.6.7
 PySpark 2.4.0
 PyArrow: 0.11.1
 Pandas: 0.23.4
 NumPy: 1.15.4
 OS: Linux 4.15.0-43-generic #46-Ubuntu SMP Thu Dec 6 14:45:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux",,elch10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-01-16 08:55:38.0,,,,,,,,,,"0|y0022w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error with multiple file stream in a query + restart on a batch that has no data for one file stream,SPARK-26629,13209777,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,16/Jan/19 03:02,16/Jan/19 18:04,13/Jul/23 08:46,16/Jan/19 17:48,2.3.0,2.3.1,2.3.2,2.3.3,2.4.0,2.4.1,,,2.4.1,3.0.0,,,Structured Streaming,,,,,0,,,,,,"When a streaming query has multiple file streams, and there is a batch where one of the file streams dont have data in that batch, then if the query has to restart from that, it will throw the following error.

{code}
java.lang.IllegalStateException: batch 1 doesn't exist
	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog$.verifyBatchIds(HDFSMetadataLog.scala:300)
	at org.apache.spark.sql.execution.streaming.FileStreamSourceLog.get(FileStreamSourceLog.scala:120)
	at org.apache.spark.sql.execution.streaming.FileStreamSource.getBatch(FileStreamSource.scala:181)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$populateStartOffsets$2.apply(MicroBatchExecution.scala:294)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$populateStartOffsets$2.apply(MicroBatchExecution.scala:291)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at org.apache.spark.sql.execution.streaming.StreamProgress.foreach(StreamProgress.scala:25)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$populateStartOffsets(MicroBatchExecution.scala:291)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:178)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:175)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:175)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:251)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:61)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:175)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:169)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:295)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:205)
{code}

**Reason**
Existing {{HDFSMetadata.verifyBatchIds}} throws error whenever the batchIds list was empty. In the context of {{FileStreamSource.getBatch}} (where verify is called) and FileStreamSourceLog (subclass of HDFSMetadata), this is usually okay because, in a streaming query with one file stream, the batchIds can never be empty:

A batch is planned only when the FileStreamSourceLog has seen new offset (that is, there are new data files).
So FileStreamSource.getBatch will be called on X to Y where X will always be > Y. This calls internally {{HDFSMetadata.verifyBatchIds (X+1, Y)}} with X+1-Y ids.
For example, {{FileStreamSource.getBatch(4, 5)}} will call {{verify(batchIds = Seq(5), start = 5, end = 5)}}. However, the invariant of X > Y is not true when there are two file stream sources, as a batch may be planned even when only one of the file streams has data. So one of the file stream may not have data, which can call {{FileStreamSource.getBatch(X, X) -> verify(batchIds = Seq.empty, start = X+1, end = X) -> failure}}.

Note that FileStreamSource.getBatch(X, X) gets called only when restarting a query in a batch where a file source did not have data. This is because, in normal planning of batches, MicroBatchExecution avoids calling {{FileStreamSource.getBatch(X, X)}} when offset X has not changed. However, when restarting a stream at such a batch, {{MicroBatchExecution.populateStartOffsets()}} calls {{FileStreamSource.getBatch(X, X)}} (DataSource V1 hack to initialize the source with last known offsets) thus hitting this issue.


**Solution**
The minimum solution (that can be backported) here is to skip verification when FileStreamSource.getBatch(X, X).",,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-01-16 03:02:25.0,,,,,,,,,,"0|y001rc:",9223372036854775807,,,,,,,,,,,,,2.4.1,3.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.redaction.regex should include oauthToken,SPARK-26625,13209677,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,vinooganesh,vinooganesh,vinooganesh,15/Jan/19 15:43,21/Mar/21 07:04,13/Jul/23 08:46,16/Jan/19 19:44,2.4.0,,,,,,,,2.4.8,3.0.0,,,Kubernetes,Spark Core,,,,0,,,,,,The regex (spark.redaction.regex) that is used to decide which config properties or environment settings are sensitive should also include oauthToken to match  spark.kubernetes.authenticate.submission.oauthToken,,dongjoon,vinooganesh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 21 07:04:53 UTC 2021,,,,,,,,,,"0|y0015c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/19 15:44;vinooganesh;cc [~mcheah] [~vanzin];;;","15/Jan/19 15:47;vinooganesh;Will put up a PR shortly.;;;","21/Mar/21 07:04;dongjoon;I assigned this issue to [~vinooganesh] and backported this to branch-2.4 for Apache Spark 2.4.8.;;;",,,,,,,,,,,,,,,,,,,
Fixing transport server/client resource leaks in the core unittests ,SPARK-26615,13209450,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,14/Jan/19 15:58,16/Jan/19 15:03,13/Jul/23 08:46,16/Jan/19 15:01,2.4.0,3.0.0,,,,,,,2.4.1,3.0.0,,,Spark Core,,,,,0,,,,,,The testing of SPARK-24938 PR ([https://github.com/apache/spark/pull/22114)] always fails with OOM. Analysing this problem lead to identifying some resource leaks where TransportClient/TransportServer instances are nor closed properly.,,attilapiros,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 16 15:01:33 UTC 2019,,,,,,,,,,"0|u00tlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/19 16:01;attilapiros;I am working on it (soon a PR will be created);;;","16/Jan/19 15:01;srowen;Issue resolved by pull request 23540
[https://github.com/apache/spark/pull/23540];;;",,,,,,,,,,,,,,,,,,,,
Speculation kill might cause job failure,SPARK-26614,13209382,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,liupengcheng,liupengcheng,14/Jan/19 11:22,16/Jan/19 02:46,13/Jul/23 08:46,16/Jan/19 02:44,2.1.0,,,,,,,,2.2.2,2.3.1,2.4.0,,Scheduler,Spark Core,,,,0,,,,,,"This issue is similar to SPARK-26612

Some odd exceptions might be thrown in speculation kill, however, currently spark does not handle this case and will report failure to Driver. This exception will be counting towards MAX_TASK_FAILURES, and might result in the job failure.

 

I think we can check state of task to tell if we report failure or killed.",,liupengcheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 16 02:45:30 UTC 2019,,,,,,,,,,"0|u00t68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/19 02:45;liupengcheng;Already resolved by https://github.com/apache/spark/pull/20987;;;",,,,,,,,,,,,,,,,,,,,,
Speculation kill causing finished stage recomputed,SPARK-26612,13209380,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,liupengcheng,liupengcheng,14/Jan/19 11:14,16/Jan/19 02:46,13/Jul/23 08:46,16/Jan/19 02:44,2.1.0,,,,,,,,2.2.2,2.3.1,2.4.0,,Scheduler,Spark Core,,,,0,,,,,,"In our production spark cluster, we encoutered this issue.

A more detailed explaination:

Let's say we have two stage: stage0.0 and stage1.0, and stage 0 is a shuffleMapStage, and stage1 has dependency on stage0, and we enabled spark.speculation.

when task0.0 of stage1.0 finished, and is trying to kill task0.1(speculative) of stage1.0, task0.1 throws a wrapped FetchFailedException whose root cause is  java.nio.channels.ClosedByInterruptException(caused by speculation kill).

Exception stack:
{code:java}
at org.apache.spark.shuffle.BlockStoreShuffleReader$ProcessFetchFailedIterator$$anonfun$hasNext$1.apply$mcZ$sp(BlockStoreShuffleReader.scala:148)
at org.apache.spark.shuffle.BlockStoreShuffleReader$ProcessFetchFailedIterator$$anonfun$hasNext$1.apply(BlockStoreShuffleReader.scala:148)
at org.apache.spark.shuffle.BlockStoreShuffleReader$ProcessFetchFailedIterator$$anonfun$hasNext$1.apply(BlockStoreShuffleReader.scala:148)
at org.apache.spark.shuffle.BlockStoreShuffleReader$ProcessFetchFailedIterator.tryThrowFetchFailedException(BlockStoreShuffleReader.scala:127)
at org.apache.spark.shuffle.BlockStoreShuffleReader$ProcessFetchFailedIterator.hasNext(BlockStoreShuffleReader.scala:148)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
at org.apache.spark.scheduler.Task.run(Task.scala:99)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:308)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/home/work/hdd6/yarn/c3prc-hadoop/nodemanager/usercache/h_user_profile/appcache/application_1505730831071_76097/blockmgr-bb226ff8-dd5f-4296-b3cc-ce7ff5cc60cc/37/shuffle_1_1182_0.data, offset=17789166, length=35709}
at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:114)
at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:371)
... 26 more
Caused by: java.nio.channels.ClosedByInterruptException
at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:155)
at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
{code}
Seems in latest spark version, this problem still exists! FetchFailedException might be throwed in ShuffleBlockFetcherIterator.next, where the task is accessing local shuffle block or encountering a stream corruption. ",,liupengcheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 16 02:45:53 UTC 2019,,,,,,,,,,"0|u00t5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/19 02:45;liupengcheng;Already resolved by https://github.com/apache/spark/pull/20987;;;",,,,,,,,,,,,,,,,,,,,,
parameters passed in extraJavaOptions are not being picked up ,SPARK-26606,13209071,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,rrb441,rrb441,11/Jan/19 18:59,24/Mar/19 23:48,13/Jul/23 08:46,22/Mar/19 22:31,2.3.1,,,,,,,,2.3.4,2.4.1,3.0.0,,Spark Submit,,,,,1,java,spark,,,,"driver.extraJavaOptions and executor.extraJavaOptions are not being picked up . Even though I see the parameters are being passed fine in the spark launch command I do not see these parameters are being picked up for some unknown reason. My source code throws an error stating the java params are empty

 

This is my spark submit command: 

    output=`spark-submit \
 --class com.demo.myApp.App \
 --conf 'spark.executor.extraJavaOptions=-Dapp.env=dev -Dapp.country=US -Dapp.banner=ABC -Doracle.net.tns_admin=/work/artifacts/oracle/current -Djava.security.egd=[file:/dev/./urandom|file:///dev/urandom]' \
 --conf 'spark.driver.extraJavaOptions=-Dapp.env=dev -Dapp.country=US -Dapp.banner=ABC -Doracle.net.tns_admin=/work/artifacts/oracle/current -Djava.security.egd=[file:/dev/./urandom|file:///dev/urandom]' \
 --executor-memory ""$EXECUTOR_MEMORY"" \
 --executor-cores ""$EXECUTOR_CORES"" \
 --total-executor-cores ""$TOTAL_CORES"" \
 --driver-memory ""$DRIVER_MEMORY"" \
 --deploy-mode cluster \
 /home/spark/asm//current/myapp-*.jar 2>&1 &`

 

 

Is there any other way I can access the java params with out using extraJavaOptions. ",,m.kaczor,rrb441,toopt4,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 22 22:31:51 UTC 2019,,,,,,,,,,"0|u00r9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/19 19:04;vanzin;Define not working?

e.g. I see you have both single quotes and variable references. That won't work the way you're probably expecting it to.;;;","11/Jan/19 19:07;rrb441;I have tried removing the single quotes too and even hardcoded the reference variables. It does not work yet. You can also see the attached screenshot where it picks the reference variable values;;;","11/Jan/19 19:10;vanzin;Can you define what ""does not work"" means?

Because it works for me based on my expectations.

{noformat}
$ ./bin/spark-shell --conf ""spark.driver.extraJavaOptions=-Dprop1=$LOGNAME""
...
scala> sys.props(""prop1"")
res0: String = vanzin
{noformat}
;;;","11/Jan/19 19:16;rrb441;I can see the launch command which has the expected values. But the individual parts like  ""-Dapp.env=prod""  ""-Dapp.country=US"" ""-Dapp.banner=WMT"" are missing. I am suspecting this could be the reason why my source code is throwing an exception that the jvm params are null. 

 ;;;","11/Jan/19 19:22;vanzin;What master are you using?;;;","11/Jan/19 19:24;rrb441;10.36.67.188:7077 is what I am using. The rest of all the master links over there are just stand by's;;;","11/Jan/19 19:44;vanzin;So you're using standalone cluster mode. It might be an issue with that combination. That's not really my area.

bq. Is there any other way I can access the java params with out using extraJavaOptions. 

There's a thousand different ways. Use parameters to you class instead of system properties, config files, etc, etc.;;;","15/Jan/19 18:19;rrb441;any update on this

 ;;;","20/Mar/19 08:46;m.kaczor;I think I've come across similar issue (at least with the driver, works fine for executors in my case)

I've even posted a question on stackoverflow: [https://stackoverflow.com/questions/55244273/spark-2-4-0-submit-in-cluster-mode-why-is-rest-submission-server-required]

 

To sum up the problem:

Spark version 2.4.0, *standalone* cluster.

I'm submitting app using spark-submit, in all cases exactly the same script is used, just changing master port and deploy mode.

I want to pass some extraJavaOptions to driver hence I'm using spark.driver.extraJavaOptions property (-- conf ""spark.driver.extraJavaOptions=-Dfoo=BAR"")

I assume that variable was properly passed if it's listed in System Properties table in Environment tab of app UI (the one running on port 4040).

Here is what I've observed:

 
||Deploy mode||Deploy to 7077 (regular way)||Deploy to 6066 (via REST)||
|Client|Variables are passed correctly|N/A|
|Cluster|*{color:#ff0000}Variables are not passed{color}*|Variables are passed correctly|

 

All in all, it looks to me that if we want to pass system variables in cluster mode *we have to* deploy via REST.

I consider it a bug, please correct me if I'm wrong. 

 ;;;","22/Mar/19 22:31;vanzin;Issue resolved by pull request 24163
[https://github.com/apache/spark/pull/24163];;;",,,,,,,,,,,,
Fix HiveThriftServer2 set hiveconf and hivevar in every sql,SPARK-26598,13208913,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dzcxzl,wangtao93,wangtao93,11/Jan/19 03:31,11/Sep/19 02:30,13/Jul/23 08:46,10/Sep/19 05:10,3.0.0,,,,,,,,3.0.0,,,,SQL,,,,,0,,,,,,"[https://github.com/apache/spark/pull/17886,] this pr provide that hiveserver2 support --haveconf  and --hivevar。But it set hiveconf and hivevar in every sql in class SparkSQLOperationManager，i think this is not suitable。So i make a little modify to set --hiveconf and --hivevar in class SparkSQLSessionManager, it will only run once in open HiveServer2 session, instead of ervery sql to init --hiveconf and --hivevar",,wangtao93,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 10 05:10:07 UTC 2019,,,,,,,,,,"0|u00qag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/19 05:10;yumwang;Issue resolved by pull request 25722
https://github.com/apache/spark/pull/25722;;;",,,,,,,,,,,,,,,,,,,,,
Kafka delegation token doesn't support proxy user,SPARK-26592,13208841,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,gsomogyi,gsomogyi,10/Jan/19 18:33,21/May/19 22:13,13/Jul/23 08:46,15/Jan/19 18:00,3.0.0,,,,,,,,3.0.0,,,,Structured Streaming,,,,,0,,,,,,Kafka is not yet support to obtain delegation token with proxy user. It has to be turned off until https://issues.apache.org/jira/browse/KAFKA-6945 implemented.,,gsomogyi,vanzin,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-6945,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 15 18:00:52 UTC 2019,,,,,,,,,,"0|u00puw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/19 18:51;gsomogyi;Here is the KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-373%3A+Allow+users+to+create+delegation+tokens+for+other+users;;;","15/Jan/19 18:00;vanzin;Issue resolved by pull request 23511
[https://github.com/apache/spark/pull/23511];;;",,,,,,,,,,,,,,,,,,,,
Streaming queries should have isolated SparkSessions and confs,SPARK-26586,13208694,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mukulmurthy,mukulmurthy,mukulmurthy,10/Jan/19 05:18,11/Jan/19 21:26,13/Jul/23 08:46,11/Jan/19 19:47,2.2.0,2.2.1,2.2.2,2.3.0,2.3.1,2.3.2,2.4.0,,2.4.1,3.0.0,,,SQL,Structured Streaming,,,,0,,,,,,"When a stream is started, the stream's config is supposed to be frozen and all batches run with the config at start time. However, due to a race condition in creating streams, updating a conf value in the active spark session immediately after starting a stream can lead to the stream getting that updated value.

 

The problem is that when StreamingQueryManager creates a MicrobatchExecution (or ContinuousExecution), it passes in the shared spark session, and the spark session isn't cloned until StreamExecution.start() is called. DataStreamWriter.start() should not return until the SparkSession is cloned.",,mukulmurthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-01-10 05:18:37.0,,,,,,,,,,"0|u00oy8:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add `paranamer` dependency to `core` module,SPARK-26583,13208648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,09/Jan/19 22:48,08/Feb/19 08:47,13/Jul/23 08:46,10/Jan/19 08:45,2.4.0,3.0.0,,,,,,,2.4.1,3.0.0,,,Build,Spark Core,,,,0,,,,,,"With Scala-2.12 profile, Spark application fails while Spark is okay. For example, our documented `SimpleApp` example succeeds to compile but it fails at runtime because it doesn't use `paranamer 2.8` and hits SPARK-22128.

https://dist.apache.org/repos/dist/dev/spark/3.0.0-SNAPSHOT-2019_01_09_13_59-e853afb-docs/_site/quick-start.html

{code}
$ mvn dependency:tree -Dincludes=com.thoughtworks.paranamer
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ simple ---
[INFO] my.test:simple:jar:1.0-SNAPSHOT
[INFO] \- org.apache.spark:spark-sql_2.12:jar:3.0.0-SNAPSHOT:compile
[INFO]    \- org.apache.spark:spark-core_2.12:jar:3.0.0-SNAPSHOT:compile
[INFO]       \- org.apache.avro:avro:jar:1.8.2:compile
[INFO]          \- com.thoughtworks.paranamer:paranamer:jar:2.7:compile
{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26819,,,,,,,,,,SPARK-22128,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 10 08:45:28 UTC 2019,,,,,,,,,,"0|u00oog:",9223372036854775807,,,,,,,,,,,,,2.4.1,3.0.0,,,,,,,,,,"09/Jan/19 22:50;dongjoon;This is a minor issue, but this should be fixed before Spark 3.0.0 because this affects Spark applications .
For Spark 2.4.0 Scala 2.12 artifacts, the situation is the same. Of course, users can add `paranamer` dependency into their pom.;;;","10/Jan/19 08:45;dongjoon;This is resolved via https://github.com/apache/spark/pull/23502;;;",,,,,,,,,,,,,,,,,,,,
Broadcast hint not applied to partitioned table,SPARK-26576,13208453,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jzhuge,jzhuge,jzhuge,09/Jan/19 02:17,18/Feb/19 06:13,13/Jul/23 08:46,11/Jan/19 17:23,2.2.2,2.3.2,2.4.0,,,,,,2.4.1,3.0.0,,,SQL,,,,,0,,,,,,"Broadcast hint is not applied to partitioned Parquet table. Below ""SortMergeJoin"" is chosen incorrectly and ""ResolvedHit(broadcast)"" is removed in Optimized Plan.
{noformat}
scala> spark.sql(""CREATE TABLE jzhuge.parquet_with_part (val STRING) PARTITIONED BY (dateint INT) STORED AS parquet"")

scala> spark.conf.set(""spark.sql.autoBroadcastJoinThreshold"", ""-1"")

scala> Seq(spark.table(""jzhuge.parquet_with_part"")).map(df => df.join(broadcast(df), ""dateint"").explain(true))

== Parsed Logical Plan ==
'Join UsingJoin(Inner,List(dateint))
:- SubqueryAlias `jzhuge`.`parquet_with_part`
:  +- Relation[val#28,dateint#29] parquet
+- ResolvedHint (broadcast)
   +- SubqueryAlias `jzhuge`.`parquet_with_part`
      +- Relation[val#32,dateint#33] parquet

== Analyzed Logical Plan ==
dateint: int, val: string, val: string
Project [dateint#29, val#28, val#32]
+- Join Inner, (dateint#29 = dateint#33)
   :- SubqueryAlias `jzhuge`.`parquet_with_part`
   :  +- Relation[val#28,dateint#29] parquet
   +- ResolvedHint (broadcast)
      +- SubqueryAlias `jzhuge`.`parquet_with_part`
         +- Relation[val#32,dateint#33] parquet

== Optimized Logical Plan ==
Project [dateint#29, val#28, val#32]
+- Join Inner, (dateint#29 = dateint#33)
   :- Project [val#28, dateint#29]
   :  +- Filter isnotnull(dateint#29)
   :     +- Relation[val#28,dateint#29] parquet
   +- Project [val#32, dateint#33]
      +- Filter isnotnull(dateint#33)
         +- Relation[val#32,dateint#33] parquet

== Physical Plan ==
*(5) Project [dateint#29, val#28, val#32]
+- *(5) SortMergeJoin [dateint#29], [dateint#33], Inner
   :- *(2) Sort [dateint#29 ASC NULLS FIRST], false, 0
   :  +- Exchange(coordinator id: 55629191) hashpartitioning(dateint#29, 500), coordinator[target post-shuffle partition size: 67108864]
   :     +- *(1) FileScan parquet jzhuge.parquet_with_part[val#28,dateint#29] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[], PartitionCount: 0, PartitionFilters: [isnotnull(dateint#29)], PushedFilters: [], ReadSchema: struct<val:string>
   +- *(4) Sort [dateint#33 ASC NULLS FIRST], false, 0
      +- ReusedExchange [val#32, dateint#33], Exchange(coordinator id: 55629191) hashpartitioning(dateint#29, 500), coordinator[target post-shuffle partition size: 67108864]
{noformat}
Broadcast hint is applied to Parquet table without partition. Below ""BroadcastHashJoin"" is chosen as expected.
{noformat}
scala> spark.sql(""CREATE TABLE jzhuge.parquet_no_part (val STRING, dateint INT) STORED AS parquet"")

scala> spark.conf.set(""spark.sql.autoBroadcastJoinThreshold"", ""-1"")

scala> Seq(spark.table(""jzhuge.parquet_no_part"")).map(df => df.join(broadcast(df), ""dateint"").explain(true))

== Parsed Logical Plan ==
'Join UsingJoin(Inner,List(dateint))
:- SubqueryAlias `jzhuge`.`parquet_no_part`
:  +- Relation[val#44,dateint#45] parquet
+- ResolvedHint (broadcast)
   +- SubqueryAlias `jzhuge`.`parquet_no_part`
      +- Relation[val#50,dateint#51] parquet

== Analyzed Logical Plan ==
dateint: int, val: string, val: string
Project [dateint#45, val#44, val#50]
+- Join Inner, (dateint#45 = dateint#51)
   :- SubqueryAlias `jzhuge`.`parquet_no_part`
   :  +- Relation[val#44,dateint#45] parquet
   +- ResolvedHint (broadcast)
      +- SubqueryAlias `jzhuge`.`parquet_no_part`
         +- Relation[val#50,dateint#51] parquet

== Optimized Logical Plan ==
Project [dateint#45, val#44, val#50]
+- Join Inner, (dateint#45 = dateint#51)
   :- Filter isnotnull(dateint#45)
   :  +- Relation[val#44,dateint#45] parquet
   +- ResolvedHint (broadcast)
      +- Filter isnotnull(dateint#51)
         +- Relation[val#50,dateint#51] parquet

== Physical Plan ==
*(2) Project [dateint#45, val#44, val#50]
+- *(2) BroadcastHashJoin [dateint#45], [dateint#51], Inner, BuildRight
   :- *(2) Project [val#44, dateint#45]
   :  +- *(2) Filter isnotnull(dateint#45)
   :     +- *(2) FileScan parquet jzhuge.parquet_no_part[val#44,dateint#45] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [IsNotNull(dateint)], ReadSchema: struct<val:string,dateint:int>
   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint)))
      +- *(1) Project [val#50, dateint#51]
         +- *(1) Filter isnotnull(dateint#51)
            +- *(1) FileScan parquet jzhuge.parquet_no_part[val#50,dateint#51] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [IsNotNull(dateint)], ReadSchema: struct<val:string,dateint:int>
{noformat}
Observed similar issue with partitioned Orc table. SequenceFile is fine.",,cloud_fan,jzhuge,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26599,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 09 08:20:38 UTC 2019,,,,,,,,,,"0|u00nh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/19 02:28;jzhuge;The ""ResolvedHint"" node is removed by the following code introduced in SPARK-14581.
{code:java|title=PhysicalOperation.collectProjectsAndFilters}
case h: ResolvedHint =>
  collectProjectsAndFilters(h.child)
{code}
[~davies], [~cloud_fan], What scenario does the above code try to cover? Is there any unit test covering this code path?;;;","09/Jan/19 02:33;cloud_fan;can you reproduce it with the master branch? There is a major refactor on master branch about how to deal with `ResolvedHint`.;;;","09/Jan/19 08:20;jzhuge;No issue on the master branch. Please note ""rightHint=(broadcast)"" for the Join in Optimized Plan.
{noformat}
scala> Seq(spark.table(""jzhuge.parquet_with_part"")).map(df => df.join(broadcast(df), ""dateint"").explain(true))

== Parsed Logical Plan ==
'Join UsingJoin(Inner,List(dateint))
:- SubqueryAlias `jzhuge`.`parquet_with_part`
:  +- Relation[val#34,dateint#35] parquet
+- ResolvedHint (broadcast)
   +- SubqueryAlias `jzhuge`.`parquet_with_part`
      +- Relation[val#40,dateint#41] parquet

== Analyzed Logical Plan ==
dateint: int, val: string, val: string
Project [dateint#35, val#34, val#40]
+- Join Inner, (dateint#35 = dateint#41)
   :- SubqueryAlias `jzhuge`.`parquet_with_part`
   :  +- Relation[val#34,dateint#35] parquet
   +- ResolvedHint (broadcast)
      +- SubqueryAlias `jzhuge`.`parquet_with_part`
         +- Relation[val#40,dateint#41] parquet

== Optimized Logical Plan ==
Project [dateint#35, val#34, val#40]
+- Join Inner, (dateint#35 = dateint#41), rightHint=(broadcast)
   :- Project [val#34, dateint#35]
   :  +- Filter isnotnull(dateint#35)
   :     +- Relation[val#34,dateint#35] parquet
   +- Project [val#40, dateint#41]
      +- Filter isnotnull(dateint#41)
         +- Relation[val#40,dateint#41] parquet

== Physical Plan ==
*(2) Project [dateint#35, val#34, val#40]
+- *(2) BroadcastHashJoin [dateint#35], [dateint#41], Inner, BuildRight
   :- *(2) FileScan parquet jzhuge.parquet_with_part[val#34,dateint#35] Batched: true, DataFilters: [], Format: Parquet, Location: PrunedInMemoryFileIndex[], PartitionCount: 0, PartitionFilters: [isnotnull(dateint#35)], PushedFilters: [], ReadSchema: struct<val:string>
   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint)))
      +- *(1) FileScan parquet jzhuge.parquet_with_part[val#40,dateint#41] Batched: true, DataFilters: [], Format: Parquet, Location: PrunedInMemoryFileIndex[], PartitionCount: 0, PartitionFilters: [isnotnull(dateint#41)], PushedFilters: [], ReadSchema: struct<val:string>
{noformat}
From a quick look at the source, EliminateResolvedHint pulls broadcast hint into Join and eliminates the ResolvedHint node. It is called before PruneFileSourcePartitions so the above code in PhysicalOperation.collectProjectsAndFilters is never called on master branch for the few cases I tried.;;;",,,,,,,,,,,,,,,,,,,
Join on distinct column with monotonically_increasing_id produces wrong output,SPARK-26572,13208308,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,soerenreichardt,soerenreichardt,08/Jan/19 13:16,02/Mar/20 19:36,13/Jul/23 08:46,15/Feb/19 01:05,2.0.2,2.1.3,2.2.2,2.3.2,2.4.0,,,,2.3.4,2.4.1,3.0.0,,SQL,,,,,2,correctness,,,,,"When joining a table with projected monotonically_increasing_id column after calling distinct with another table the operators do not get executed in the right order. 

Here is a minimal example:
{code:java}
import org.apache.spark.sql.{DataFrame, SparkSession, functions}

object JoinBug extends App {

  // Spark session setup
  val session =  SparkSession.builder().master(""local[*]"").getOrCreate()
  import session.sqlContext.implicits._
  session.sparkContext.setLogLevel(""error"")

  // Bug in Spark: ""monotonically_increasing_id"" is pushed down when it shouldn't be. Push down only happens when the
  // DF containing the ""monotonically_increasing_id"" expression is on the left side of the join.
  val baseTable = Seq((1), (1)).toDF(""idx"")
  val distinctWithId = baseTable.distinct.withColumn(""id"", functions.monotonically_increasing_id())
  val monotonicallyOnRight: DataFrame = baseTable.join(distinctWithId, ""idx"")
  val monotonicallyOnLeft: DataFrame = distinctWithId.join(baseTable, ""idx"")

  monotonicallyOnLeft.show // Wrong
  monotonicallyOnRight.show // Ok in Spark 2.2.2 - also wrong in Spark 2.4.0

}

{code}
It produces the following output:
{code:java}
Wrong:
+---+------------+
|idx| id         |
+---+------------+
| 1|369367187456 |
| 1|369367187457 |
+---+------------+

Right:
+---+------------+
|idx| id         |
+---+------------+
| 1|369367187456 |
| 1|369367187456 |
+---+------------+
{code}
We assume that the join operator triggers a pushdown of expressions (monotonically_increasing_id in this case) which gets pushed down to be executed before distinct. This produces non-distinct rows with unique id's. However it seems like this behavior only appears if the table with the projected expression is on the left side of the join in Spark 2.2.2 (for version 2.4.0 it fails on both joins).",Running on Ubuntu 18.04LTS and Intellij 2018.2.5,dongjoon,maropu,mju,petertoth,roufique07,soerenreichardt,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 15 01:05:18 UTC 2019,,,,,,,,,,"0|u00mkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/19 11:34;maropu;I checked I could reproduce this below and I set ""correctness"" in the Label.
It seems HashAggregate hits a bug if it has stateful expressions, e.g., monotonically_increasing_id, rand, ...
{code:java}
scala> val baseTable = Seq((1), (1)).toDF(""idx"")
scala> val distinctWithId = baseTable.distinct.withColumn(""id"", functions.monotonically_increasing_id())
scala> baseTable.join(distinctWithId, ""idx"").show
+---+------------+                                                              
|idx|          id|
+---+------------+
|  1|369367187456|
|  1|369367187457|
+---+------------+

scala> sql(""SET spark.sql.codegen.wholeStage=false"")
scala> baseTable.join(distinctWithId, ""idx"").show
+---+------------+
|idx|          id|
+---+------------+
|  1|369367187456|
|  1|369367187456|
+---+------------+
{code}
This is pretty a corner case, so I didn't set a blocker.;;;","15/Feb/19 01:05;maropu;Resolved by https://github.com/apache/spark/pull/23731;;;",,,,,,,,,,,,,,,,,,,,
Update Hive Serde mapping with canonical name of Parquet and Orc FileFormat,SPARK-26571,13208299,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,08/Jan/19 12:41,09/Jan/19 03:11,13/Jul/23 08:46,09/Jan/19 03:11,3.0.0,,,,,,,,2.4.1,3.0.0,,,SQL,,,,,0,,,,,,"Currently the following queries will lead to wrong Hive Serde:
df.write.format(""org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat"").saveAsTable(..)

df.write.format(""org.apache.spark.sql.execution.datasources.orc.OrcFileFormat"").saveAsTable(..)

This minor PR is to fix the mapping.",,dongjoon,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 09 03:11:21 UTC 2019,,,,,,,,,,"0|u00miw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/19 03:11;dongjoon;This is resolved via https://github.com/apache/spark/pull/23491 
Thank you, [~Gengliang.Wang] and [~cloud_fan].;;;",,,,,,,,,,,,,,,,,,,,,
Fix wrong assertions and error messages for parameter checking,SPARK-26564,13208156,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sekikn,sekikn,sekikn,07/Jan/19 22:25,12/Jan/19 20:56,13/Jul/23 08:46,12/Jan/19 20:53,2.3.0,2.3.1,2.3.2,2.4.0,,,,,3.0.0,,,,MLlib,Spark Core,SQL,,,0,starter,,,,,"I mistakenly set an equivalent value with spark.network.timeout to spark.executor.heartbeatInterval and got the following error:

{code}
java.lang.IllegalArgumentException: requirement failed: The value of spark.network.timeout=120s must be no less than the value of spark.executor.heartbeatInterval=120s.
{code}

But it can be read as they could be equal. ""Greater than"" is more precise than ""no less than"".

----

In addition, the following assertions are inconsistent with their messages.

{code:title=mllib/src/main/scala/org/apache/spark/ml/optim/WeightedLeastSquares.scala}
 91   require(maxIter >= 0, s""maxIter must be a positive integer: $maxIter"")
{code}

{code:title=sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala}
416       require(capacity < 512000000, ""Cannot broadcast more than 512 millions rows"")
{code}",,sekikn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 12 20:53:50 UTC 2019,,,,,,,,,,"0|u00ln4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/19 20:53;srowen;Issue resolved by pull request 23488
[https://github.com/apache/spark/pull/23488];;;",,,,,,,,,,,,,,,,,,,,,
Repeating select on udf function throws analysis exception - function not registered,SPARK-26560,13207991,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,hari28,hari28,07/Jan/19 10:28,01/Apr/20 18:40,13/Jul/23 08:46,02/Jan/20 13:30,2.3.2,,,,,,,,2.4.5,3.0.0,,,SQL,,,,,1,,,,,,"In spark-shell,

1. Create the new function

sql(""CREATE FUNCTION last_day_test AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFLastDayTest' USING JAR 'hdfs:///tmp/two_udfs.jar"")

2. Perform select over the udf
{code:java}
scala> sql("" select last_day_test('2015-08-22')"")
 res1: org.apache.spark.sql.DataFrame = [default.last_day_test(2015-08-22): string]

scala> sql("" select last_day_test('2015-08-22')"")
 org.apache.spark.sql.AnalysisException: Undefined function: 'last_day_test'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 8
 at org.apache.spark.sql.catalyst.catalog.SessionCatalog.failFunctionLookup(SessionCatalog.scala:1167)
 at org.apache.spark.sql.hive.HiveSessionCatalog.lookupFunction0(HiveSessionCatalog.scala:145)
 at org.apache.spark.sql.hive.HiveSessionCatalog.lookupFunction(HiveSessionCatalog.scala:124)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$16$$anonfun$applyOrElse$6$$anonfun$applyOrElse$53.apply(Analyzer.scala:1244)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$16$$anonfun$applyOrElse$6$$anonfun$applyOrElse$53.apply(Analyzer.scala:1244)
 at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$16$$anonfun$applyOrElse$6.applyOrElse(Analyzer.scala:1243)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$16$$anonfun$applyOrElse$6.applyOrElse(Analyzer.scala:1227)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
 at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)
 at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:85)
 at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:85)
 at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)
 at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)
 at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
 at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)
 at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)
 at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)
 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
 at scala.collection.immutable.List.foreach(List.scala:381)
 at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
 at scala.collection.immutable.List.map(List.scala:285)
 at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)
 at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
 at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)
 at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:85)
 at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:76)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$16.applyOrElse(Analyzer.scala:1227)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$16.applyOrElse(Analyzer.scala:1225)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
 at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
 at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:1225)
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:1224)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)
 at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
 at scala.collection.immutable.List.foldLeft(List.scala:84)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)
 at scala.collection.immutable.List.foreach(List.scala:381)
 at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)
 at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:124)
 at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:118)
 at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:103)
 at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
 at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
 at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
 at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)
 at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
 ... 49 elided{code}",,cloud_fan,ebyhr,hari28,islandshinji,maropu,nivedeeta,sumitchauhan,,,,,,,,,,,,,,,,,,,,,,,SPARK-26624,SPARK-26602,SPARK-30260,,,,,SPARK-31312,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 02 13:30:59 UTC 2020,,,,,,,,,,"0|u00kmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/19 10:35;nivedeeta;I will be working on this issue.;;;","02/Jan/20 13:30;cloud_fan;Issue resolved by pull request 27075
[https://github.com/apache/spark/pull/27075];;;",,,,,,,,,,,,,,,,,,,,
ML image can't work with numpy versions prior to 1.9,SPARK-26559,13207983,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,07/Jan/19 09:34,12/Dec/22 18:10,13/Jul/23 08:46,07/Jan/19 10:39,3.0.0,,,,,,,,2.4.1,3.0.0,,,ML,PySpark,,,,0,,,,,,"ML image can't work with numpy version prior to 1.9 now.

Current pyspark test can show it:
{code:java}
test_read_images (pyspark.ml.tests.test_image.ImageReaderTest) ... ERROR                                                                              
test_read_images_multiple_times (pyspark.ml.tests.test_image.ImageReaderTest2) ... ok                                                                 
                                                                                                                                                      
======================================================================                                                                                
ERROR: test_read_images (pyspark.ml.tests.test_image.ImageReaderTest)                                                                                 
----------------------------------------------------------------------                                                                                
Traceback (most recent call last):
  File ""/Users/viirya/docker_tmp/repos/spark-1/python/pyspark/ml/tests/test_image.py"", line 36, in test_read_images                                   
    self.assertEqual(ImageSchema.toImage(array, origin=first_row[0]), first_row)                                                                      
  File ""/Users/viirya/docker_tmp/repos/spark-1/python/pyspark/ml/image.py"", line 193, in toImage                                                      
    data = bytearray(array.astype(dtype=np.uint8).ravel().tobytes())                                                                                  
AttributeError: 'numpy.ndarray' object has no attribute 'tobytes'                                                                                     
                                                                                                                                                      
----------------------------------------------------------------------
Ran 2 tests in 29.040s                                                                                                                                
                                                                                                                                                      
FAILED (errors=1)
{code}",,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 07 10:39:45 UTC 2019,,,,,,,,,,"0|u00kko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/19 10:39;gurwls223;Fixed in https://github.com/apache/spark/pull/23484;;;",,,,,,,,,,,,,,,,,,,,,
Thread safety issue causes createDataset to fail with misleading errors,SPARK-26555,13207928,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mwlon,mwlon,mwlon,07/Jan/19 00:36,12/Dec/22 18:10,13/Jul/23 08:46,19/Mar/19 10:23,2.4.0,,,,,,,,2.4.4,3.0.0,,,SQL,,,,,0,,,,,,"This can be replicated (~2% of the time) with

{code:scala}
import java.sql.Timestamp
import java.util.concurrent.{Executors, Future}

import org.apache.spark.sql.SparkSession

import scala.collection.mutable.ListBuffer
import scala.concurrent.ExecutionContext
import scala.util.Random

object Main {
  def main(args: Array[String]): Unit = {
    val sparkSession = SparkSession.builder
      .getOrCreate()
    import sparkSession.implicits._

    val executor = Executors.newFixedThreadPool(1)
    try {
      implicit val xc: ExecutionContext = ExecutionContext.fromExecutorService(executor)
      val futures = new ListBuffer[Future[_]]()

      for (i <- 1 to 3) {
        futures += executor.submit(new Runnable {
          override def run(): Unit = {
            val d = if (Random.nextInt(2) == 0) Some(""d value"") else None
            val e = if (Random.nextInt(2) == 0) Some(5.0) else None
            val f = if (Random.nextInt(2) == 0) Some(6.0) else None
            println(""DEBUG"", d, e, f)
            sparkSession.createDataset(Seq(
              MyClass(new Timestamp(1L), ""b"", ""c"", d, e, f)
            ))
          }
        })
      }

      futures.foreach(_.get())
    } finally {
      println(""SHUTDOWN"")
      executor.shutdown()
      sparkSession.stop()
    }
  }

  case class MyClass(
    a: Timestamp,
    b: String,
    c: String,
    d: Option[String],
    e: Option[Double],
    f: Option[Double]
  )
}
{code}

So it will usually come up during

{code:bash}
for i in $(seq 1 200); do
  echo $i
  spark-submit --master local[4] target/scala-2.11/spark-test_2.11-0.1.jar
done
{code}

causing a variety of possible errors, such as

{code}Exception in thread ""main"" java.util.concurrent.ExecutionException: scala.MatchError: scala.Option[String] (of class scala.reflect.internal.Types$ClassArgsTypeRef)
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
Caused by: scala.MatchError: scala.Option[String] (of class scala.reflect.internal.Types$ClassArgsTypeRef)
	at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$org$apache$spark$sql$catalyst$ScalaReflection$$deserializerFor$1.apply(ScalaReflection.scala:210){code}

or

{code}Exception in thread ""main"" java.util.concurrent.ExecutionException: java.lang.UnsupportedOperationException: Schema for type scala.Option[scala.Double] is not supported
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
Caused by: java.lang.UnsupportedOperationException: Schema for type scala.Option[scala.Double] is not supported
	at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$schemaFor$1.apply(ScalaReflection.scala:789){code}",,cloud_fan,joshrosen,maropu,mwlon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27150,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 20 02:05:08 UTC 2019,,,,,,,,,,"0|u00k8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/19 04:18;gurwls223;Critical+ is usually reserved for committers. Please avoid to set this.;;;","01/Mar/19 22:04;srowen;This doesn't look like a Spark bug. It comes up, I think, when your random data set has all ""None"" for a column. That's what the error indicates at least. That part of the code shouldn't have any shared state. Can you verify from your debug output?;;;","01/Mar/19 22:46;srowen;Hm, I might have that backwards; might be when all are not None? at least, I have a strong suspicious it's to do with the data that gets generated in some runs. Maybe fix that at one data set and see if you can reproduce?;;;","01/Mar/19 23:03;mwlon;Will try it out and report back;;;","02/Mar/19 01:24;mwlon;I was able to replicate with both all rows in all optional columns as `Some()` and all rows in all optional columns as `None`.;;;","02/Mar/19 02:50;mwlon;I can also replicate with different schemas containing Option.

When I remove all Option columns from the schema, the sporadic failure goes away. This also never happens when I remove the concurrency.;;;","03/Mar/19 18:30;srowen;To be clear, is there a data set that works only when not run in this concurrent loop? What I'm reading here is simply that you generate datasets sometimes that (correctly, I believe) can't have their schema inferred.;;;","03/Mar/19 18:55;mwlon;Yes - when I take away any randomness and use the same dataset every time (say, with Some(something) for each optional value), I still get this issue.

I've run this code in a couple different environments and obtained the same result, so you should be able to verify this as well.;;;","09/Mar/19 18:51;srowen;What is the fixed data set that reproduces this, to be clear?
And you mean that if you run it once it works, but fails in parallel?;;;","09/Mar/19 19:03;mwlon;You can literally try any dataset with Option's in the schema and replicate this issue. For example,

sparkSession.createDataset(Seq(
              MyClass(new Timestamp(1L), ""b"", ""c"", Some(""d""), Some(1.0), Some(2.0))
))

I think the code I left is pretty clear - it fails sometimes. Run it once, and it may or may not work. I don't run multiple spark-submit's in parallel.;;;","13/Mar/19 20:26;mwlon;Update: I have proved that the issue lies in reflection thread safety issues in org.apache.spark.sql.catalyst.ScalaReflection: https://stackoverflow.com/questions/55150590/thread-safety-in-scala-reflection-with-type-matching

Investigating whether this can be fixed with different usage of the reflection library, or whether this is a scala issue.;;;","14/Mar/19 01:44;mwlon;This is an existing issue with scala: https://github.com/scala/bug/issues/10766;;;","19/Mar/19 10:23;cloud_fan;Issue resolved by pull request 24085
[https://github.com/apache/spark/pull/24085];;;","06/May/19 17:22;joshrosen;[~cloud_fan] [~srowen], could we backport this to the 2.4.x series? It'd be nice to have an LTS fix for users who can't immediately upgrade to 3.0.;;;","06/May/19 17:45;srowen;I personally think it's OK to backport -- do you want to open a PR and go for it?;;;","06/May/19 21:18;joshrosen;I won't be able to tackle a backport for at least a week, so this is up for grabs in case someone else wants to do it.

If I do end up working on this then I'll loop back here to claim it.;;;","20/Jun/19 02:05;joshrosen;Backported for 2.4.4 in https://github.com/apache/spark/commit/ba7f61e25d58aa379f94a23b03503a25574529bc;;;",,,,,
Update `release-util.sh` to avoid GitBox fake 200 headers,SPARK-26554,13207918,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,06/Jan/19 22:59,08/Jan/19 01:56,13/Jul/23 08:46,07/Jan/19 04:01,2.4.1,3.0.0,,,,,,,2.4.1,3.0.0,,,Build,,,,,0,,,,,,"Unlike the previous Apache Git repository, new GitBox returns a fake HTTP 200 header instead of `404 Not Found` header. This makes release scripts out of order. This issue aims to fix it to handle the context message instead of the fake HTTP headers. This is a release blocker.

{code}
$ curl -s --head --fail ""https://gitbox.apache.org/repos/asf?p=spark.git;a=commit;h=v3.0.0""
HTTP/1.1 200 OK
Date: Sun, 06 Jan 2019 22:42:39 GMT
Server: Apache/2.4.18 (Ubuntu)
Vary: Accept-Encoding
Access-Control-Allow-Origin: *
Access-Control-Allow-Methods: POST, GET, OPTIONS
Access-Control-Allow-Headers: X-PINGOTHER
Access-Control-Max-Age: 1728000
Content-Type: text/html; charset=utf-8
{code}

*BEFORE*
{code}
$ ./do-release-docker.sh -d /tmp/test -n
Branch [branch-2.4]:
Current branch version is 2.4.1-SNAPSHOT.
Release [2.4.1]:
RC # [1]:
v2.4.1-rc1 already exists. Continue anyway [y/n]?
{code}

*AFTER*
{code}
$ ./do-release-docker.sh -d /tmp/test -n
Branch [branch-2.4]:
Current branch version is 2.4.1-SNAPSHOT.
Release [2.4.1]:
RC # [1]:
This is a dry run. Please confirm the ref that will be built for testing.
Ref [v2.4.1-rc1]:
{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 07 04:01:22 UTC 2019,,,,,,,,,,"0|u00k68:",9223372036854775807,,,,,,,,,,,,,2.4.1,3.0.0,,,,,,,,,,"07/Jan/19 04:01;dongjoon;This is resolved via https://github.com/apache/spark/pull/23476;;;",,,,,,,,,,,,,,,,,,,,,
NameError: global name '_exception_message' is not defined,SPARK-26553,13207891,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,dbirkman,dbirkman,06/Jan/19 16:25,12/Dec/22 18:10,13/Jul/23 08:46,07/Jan/19 04:01,2.3.2,,,,,,,,2.2.3,2.3.3,,,PySpark,,,,,0,,,,,,"java_gateway seems to be missing import for _exception_message:

 
{code:java}
$ pyspark
Python 2.7.13 (default, Apr 4 2017, 08:47:57)
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 5.3.0 -- An enhanced Interactive Python.
? -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
help -> Python's own help system.
object? -> Details about 'object', use 'object??' for extra details.
2019-01-06 18:16:56 WARN Utils:66 - Your hostname, TLVMACJ425FVH6 resolves to a loopback address: 127.0.0.1; using 10.196.0.12 instead (on interface en3)
2019-01-06 18:16:56 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2019-01-06 18:16:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
____ __
/ __/__ ___ _____/ /__
_\ \/ _ \/ _ `/ __/ '_/
/__ / .__/\_,_/_/ /_/\_\ version 2.3.2
/_/

Using Python version 2.7.13 (default, Apr 4 2017 08:47:57)
SparkSession available as 'spark'.

In [1]: from java_gateway import local_connect_and_auth

In [2]: local_connect_and_auth(80, ""xxx"")
---------------------------------------------------------------------------
NameError Traceback (most recent call last)
<ipython-input-2-bc1a1b76dc77> in <module>()
----> 1 local_connect_and_auth(80, ""xxx"")

/Users/dbirkman/projects/spark-2.3.2-with-pythonrdd-patch/spark/python/pyspark/java_gateway.pyc in local_connect_and_auth(port, auth_secret)
171 return (sockfile, sock)
172 except socket.error as e:
--> 173 emsg = _exception_message(e)
174 errors.append(""tried to connect to %s, but an error occured: %s"" % (sa, emsg))
175 sock.close()

NameError: global name '_exception_message' is not defined{code}",,dbirkman,dongjoon,irashid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 07 04:00:41 UTC 2019,,,,,,,,,,"0|u00k08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/19 04:00;gurwls223;I have backported the followup of SPARK-25253.;;;",,,,,,,,,,,,,,,,,,,,,
Selecting one complex field and having is null predicate on another complex field can cause error,SPARK-26551,13207858,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,06/Jan/19 03:58,11/Jan/19 19:26,13/Jul/23 08:46,11/Jan/19 19:24,2.4.0,3.0.0,,,,,,,2.4.1,3.0.0,,,SQL,,,,,0,,,,,,"The query below can cause error when doing schema pruning:

{code:java}
val query = sql(""select * from contacts"")
  .where(""name.middle is not null"")
  .select(
    ""id"",
    ""name.first"",
    ""name.middle"",
    ""name.last""
  )
  .where(""last = 'Jones'"")
  .select(count(""id"")
{code}
",,dbtsai,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 11 19:24:56 UTC 2019,,,,,,,,,,"0|u00jsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/19 19:24;dbtsai;Issue resolved by pull request 23474
[https://github.com/apache/spark/pull/23474];;;",,,,,,,,,,,,,,,,,,,,,
PySpark worker reuse take no effect for parallelize xrange,SPARK-26549,13207842,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,05/Jan/19 18:48,12/Dec/22 18:10,13/Jul/23 08:46,09/Jan/19 03:58,3.0.0,,,,,,,,3.0.0,,,,PySpark,,,,,0,,,,,,"During [the follow-up work|https://github.com/apache/spark/pull/23435#issuecomment-451079886] for PySpark worker reuse scenario, we found that the worker reuse takes no effect for `sc.parallelize(xrange(...))`.
It happened because of the specialize rdd.parallelize for xrange(SPARK-4398) generated data by xrange, which don't need to use the passed-in iterator. But this will break the end of stream checking in python worker and finally cause worker reuse takes no effect.


Relative code block and more details listing below:
Current specialize logic of xrange don't need the passed-in iterator, context.py:
{code:java}
if isinstance(c, xrange):
    ...
    def f(split, iterator):
        return xrange(getStart(split), getStart(split + 1), step)
    ...
    return self.parallelize([], numSlices).mapPartitionsWithIndex(f)
{code}
We got an unexpected value -1 which refers to END_OF_DATA_SECTION while check end of stream. See the code in worker.py:
{code:java}
# check end of stream
if read_int(infile) == SpecialLengths.END_OF_STREAM:
    write_int(SpecialLengths.END_OF_STREAM, outfile)
else:
    # write a different value to tell JVM to not reuse this worker
    write_int(SpecialLengths.END_OF_DATA_SECTION, outfile)
    sys.exit(-1)
{code}
The code works well for parallelize(range) because the END_OF_DATA_SECTION has been handled during load iterator from the socket stream, see the code in FramedSerializer:
{code:java}
def load_stream(self, stream):
    while True:
        try:
            yield self._read_with_length(stream)
        except EOFError:
            return
...
def _read_with_length(self, stream):
    length = read_int(stream)
    if length == SpecialLengths.END_OF_DATA_SECTION:
        raise EOFError #END_OF_DATA_SECTION raised EOF here and catched in load_stream
    elif length == SpecialLengths.NULL:
        return None
    obj = stream.read(length)
    if len(obj) < length:
        raise EOFError
    return self.loads(obj)
{code}",,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26573,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 09 03:58:59 UTC 2019,,,,,,,,,,"0|u00jpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/19 03:58;gurwls223;Issue resolved by pull request 23470
[https://github.com/apache/spark/pull/23470];;;",,,,,,,,,,,,,,,,,,,,,
Fix typo in EqualNullSafe's truth table comment,SPARK-26545,13207813,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,05/Jan/19 10:21,05/Jan/19 22:40,13/Jul/23 08:46,05/Jan/19 22:40,3.0.0,,,,,,,,2.2.3,2.3.3,2.4.1,3.0.0,SQL,,,,,0,,,,,,The truth table comment in {{EqualNullSafe}} incorrectly marked FALSE results as UNKNOWN,,rednaxelafx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-01-05 10:21:41.0,,,,,,,,,,"0|u00jiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Postgres numeric array support,SPARK-26538,13207747,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,alsh,alsh,alsh,04/Jan/19 20:42,15/Jan/19 07:57,13/Jul/23 08:46,12/Jan/19 19:09,2.2.2,2.3.2,2.4.1,,,,,,2.3.3,2.4.1,3.0.0,,SQL,,,,,0,,,,,,"Consider the following table definition:


{code:sql}
create table test1
(
   v  numeric[],
   d  numeric
);

insert into test1 values('{1111.222,2222.332}', 222.4555);
{code}

When reading the table into a Dataframe, I get the following schema:


{noformat}
root
 |-- v: array (nullable = true)
 |    |-- element: decimal(0,0) (containsNull = true)
 |-- d: decimal(38,18) (nullable = true){noformat}

Notice that for both columns precision and scale were not specified, but in case of the array element I got both set to 0, while in the other case defaults were set.

Later, when I try to read the Dataframe, I get the following error:


{noformat}
java.lang.IllegalArgumentException: requirement failed: Decimal precision 4 exceeds max precision 0
        at scala.Predef$.require(Predef.scala:224)
        at org.apache.spark.sql.types.Decimal.set(Decimal.scala:114)
        at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:453)
        at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(JdbcUtils.scala:474)
        ...{noformat}


I would expect to get array elements of type decimal(38,18) and no error when reading in this case.","PostgreSQL 10.4, 9.6.9.",alsh,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26540,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 12 19:09:41 UTC 2019,,,,,,,,,,"0|u00j48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/19 19:09;dongjoon;This is resolved via https://github.com/apache/spark/pull/23456;;;",,,,,,,,,,,,,,,,,,,,,
update the release scripts to point to gitbox,SPARK-26537,13207739,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shaneknapp,shaneknapp,shaneknapp,04/Jan/19 19:51,09/Jan/19 03:28,13/Jul/23 08:46,06/Jan/19 02:32,1.6.0,2.0.0,2.1.0,2.2.0,2.3.0,2.4.0,,,2.2.3,2.3.3,2.4.1,3.0.0,Build,,,,,0,,,,,,"we're seeing packaging build failures like this:  [https://amplab.cs.berkeley.edu/jenkins/view/Spark%20Packaging/job/spark-master-package/2179/console]

i did a quick skim through the repo, and found the offending urls to the old apache git repos:

 
{code:java}
(py35) ➜ spark git:(update-apache-repo) grep -r git-wip *
dev/create-release/release-tag.sh:ASF_SPARK_REPO=""git-wip-us.apache.org/repos/asf/spark.git""
dev/create-release/release-util.sh:ASF_REPO=""https://git-wip-us.apache.org/repos/asf/spark.git""
dev/create-release/release-util.sh:ASF_REPO_WEBUI=""https://git-wip-us.apache.org/repos/asf?p=spark.git""
pom.xml: <developerConnection>scm:git:https://git-wip-us.apache.org/repos/asf/spark.git</developerConnection>
{code}
this affects all versions of spark, so it will need to be backported to all released versions.

i'll put together a pull request later today.

 ",,dongjoon,maropu,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 06 02:33:51 UTC 2019,,,,,,,,,,"0|u00j2g:",9223372036854775807,,,,,,,,,,,,,2.2.3,2.3.3,2.4.1,3.0.0,,,,,,,,"04/Jan/19 20:07;dongjoon;Thank you for investigating this, [~shaneknapp].;;;","04/Jan/19 20:28;shaneknapp;[~srowen] i'm pretty sure that my change to the root pom is fine, but a second set of eyes would be great.  :);;;","05/Jan/19 23:00;dongjoon;I guess we can skip `branch-1.6/2.0/2.1` because those branches are EOL and the Jenkins jobs are stopped for a while.;;;","06/Jan/19 02:33;dongjoon;This is resolved via 
- https://github.com/apache/spark/pull/23454
- https://github.com/apache/spark/pull/23472
- https://github.com/apache/spark/pull/23473;;;",,,,,,,,,,,,,,,,,,
test case for SPARK-10316 is not valid any more,SPARK-26526,13207454,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liulinhong,liulinhong,liulinhong,03/Jan/19 11:09,04/Jan/19 05:57,13/Jul/23 08:46,04/Jan/19 02:52,2.4.0,,,,,,,,3.0.0,,,,SQL,Tests,,,,0,,,,,,"Test case in [SPARK-10316|https://github.com/apache/spark/pull/8486] is used to make sure non-deterministic `Filter` won't be pushed through `Project`
But in current code base this test case can't cover this purpose.
Change LogicalRDD to HadoopFsRelation can fix this issue.",,cloud_fan,liulinhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 04 02:52:17 UTC 2019,,,,,,,,,,"0|u00hb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/19 02:52;cloud_fan;Issue resolved by pull request 23440
[https://github.com/apache/spark/pull/23440];;;",,,,,,,,,,,,,,,,,,,,,
