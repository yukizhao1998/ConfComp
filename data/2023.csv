Issue Type,Issue key,Issue id,Summary,Description,Assignee,Reporter,Priority,Status,Resolution,Created,Updated
Bug,ZOOKEEPER-4671,13523166,Java classpath should contain libs about metrics providers,"When developing zookeeper, we often start or debug it by using the current source code instead of the built package. So it is good to add the classes about metrics providers to the classpath to avoid ClassNotFound exception. This ClassNotFound exception happens when we use the config {{metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider}} to start zookeeper.",,liguoxiong,Minor,Resolved,Fixed,06/Feb/23 07:29,17/Mar/23 07:01
Bug,ZOOKEEPER-4674,13524485,C client tests don't pass on CI,,ztzg,eolivelli,Blocker,Resolved,Fixed,13/Feb/23 14:18,05/Jul/23 16:40
Bug,ZOOKEEPER-4694,13536363,Persistent node loss,"When the client uses version 3.3.1 of the jar package, and the server uses version 3.4.6 of the application, when the scheduled task registration is implemented with Quartz, the created persistent node suddenly disappears, and a log is added before the delete method of the jar package, which is not printed, so it is speculated that it is not a client-initiated delete operation

This phenomenon is an occasional situation, it may not occur for a week, or it may occur twice a day, we tried to upgrade the client and server versions to 3.4.14, and it is still the same problem, is there anything wrong with Quartz",,zhaoxinxin,Major,Resolved,Fixed,16/May/23 06:15,19/May/23 01:50
Bug,ZEPPELIN-5881,13524325,can't compile spark 3.3,"on the master branch, when i build zeppelin/Dockerfile with

 

./mvnw -B package -DskipTests -Pbuild-distr -Pspark-3.3 -Pinclude-hadoop -Phadoop3 -Pspark-scala-2.12 -Pweb-angular -Pweb-dist

 

i get error

 

[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[687,11] cannot find symbol
[ERROR]   symbol:   class AuthenticationToken
[ERROR]   location: class org.apache.zeppelin.realm.kerberos.KerberosRealm
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[688,14] cannot find symbol
[INFO] Zeppelin: Markdown interpreter ..................... SUCCESS [  4.732 s]                                                                                                                    [89/1957]
[INFO] Zeppelin: MongoDB interpreter ...................... SUCCESS [  0.518 s]
[INFO] Zeppelin: Angular interpreter ...................... SUCCESS [  0.373 s]
[INFO] Zeppelin: Livy interpreter ......................... SUCCESS [ 53.694 s]
[INFO] Zeppelin: HBase interpreter ........................ SUCCESS [ 36.755 s]
[INFO] Zeppelin: JDBC interpreter ......................... SUCCESS [02:19 min]
[INFO] Zeppelin: File System Interpreters ................. SUCCESS [  1.858 s]
[INFO] Zeppelin: Flink Parent ............................. SUCCESS [  0.157 s]
[INFO] Zeppelin: Flink Shims .............................. SUCCESS [  1.803 s]
[INFO] Zeppelin: Flink1.13 Shims .......................... SUCCESS [01:05 min]
[INFO] Zeppelin: Flink1.14 Shims .......................... SUCCESS [ 53.029 s]
[INFO] Zeppelin: Flink1.15 Shims .......................... SUCCESS [01:02 min]
[INFO] Zeppelin: Flink1.16 Shims .......................... SUCCESS [ 38.489 s]
[INFO] Zeppelin: Flink Scala Parent ....................... SUCCESS [01:16 min]
[INFO] Zeppelin: Flink-Cmd interpreter .................... SUCCESS [  2.965 s]
[INFO] Zeppelin: InfluxDB interpreter ..................... SUCCESS [ 14.294 s]
[INFO] Zeppelin: Apache Cassandra interpreter ............. SUCCESS [01:25 min]
[INFO] Zeppelin: Elasticsearch interpreter ................ SUCCESS [ 21.956 s]
[INFO] Zeppelin: BigQuery interpreter ..................... SUCCESS [03:13 min]
[INFO] Zeppelin: Alluxio interpreter ...................... SUCCESS [02:29 min]
[INFO] Zeppelin: Neo4j interpreter ........................ SUCCESS [ 12.282 s]
[INFO] Zeppelin: Java interpreter ......................... SUCCESS [  1.133 s]
[INFO] Zeppelin: Sparql interpreter ....................... SUCCESS [ 21.923 s]
[INFO] Zeppelin: Client ................................... SUCCESS [  3.399 s]
[INFO] Zeppelin: Client Examples .......................... SUCCESS [  0.167 s]
[INFO] Zeppelin: Server ................................... FAILURE [ 38.227 s]
[INFO] Zeppelin: web Application .......................... SKIPPED
[INFO] Zeppelin: Plugins Parent ........................... SKIPPED
[INFO] Zeppelin: Plugin S3NotebookRepo .................... SKIPPED
[INFO] Zeppelin: Plugin GitHubNotebookRepo ................ SKIPPED
[INFO] Zeppelin: Plugin AzureNotebookRepo ................. SKIPPED
[INFO] Zeppelin: Plugin OSSNotebookRepo ................... SKIPPED                                                                                                                                [55/1957]
[INFO] Zeppelin: Plugin Kubernetes StandardLauncher ....... SKIPPED
[INFO] Zeppelin: Plugin Flink Launcher .................... SKIPPED
[INFO] Zeppelin: Plugin Docker Launcher ................... SKIPPED
[INFO] Zeppelin: Plugin Cluster Launcher .................. SKIPPED
[INFO] Zeppelin: Plugin Yarn Launcher ..................... SKIPPED
[INFO] Zeppelin: web angular Application .................. SKIPPED
[INFO] Zeppelin: Packaging distribution ................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  55:42 min
[INFO] Finished at: 2023-02-10T11:18:45Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.10.1:compile (default-compile) on project zeppelin-server: Compilation failure: Compilation failure:
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosAuthenticationFilter.java:[19,56] package org.apache.hadoop.security.authentication.server does not exi
st
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[20,30] package org.apache.hadoop.conf does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[21,34] package org.apache.hadoop.security does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[22,56] package org.apache.hadoop.security.authentication.client does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[23,56] package org.apache.hadoop.security.authentication.client does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[24,56] package org.apache.hadoop.security.authentication.client does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[25,56] package org.apache.hadoop.security.authentication.server does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[26,56] package org.apache.hadoop.security.authentication.server does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[28,1] package org.apache.hadoop.security.authentication.util does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/jwt/KnoxJwtRealm.java:[23,30] package org.apache.hadoop.conf does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/jwt/KnoxJwtRealm.java:[24,34] package org.apache.hadoop.security does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[184,18] cannot find symbol
[ERROR]   symbol:   class Signer
[ERROR]   location: class org.apache.zeppelin.realm.kerberos.KerberosRealm
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[185,11] cannot find symbol
[ERROR]   symbol:   class SignerSecretProvider
[ERROR]   symbol:   class AuthenticationException                                                                                                                                                   [0/1957]
[ERROR]   location: class org.apache.zeppelin.realm.kerberos.KerberosRealm
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[695,18] cannot find symbol
[ERROR]   symbol:   class AuthenticationToken
[ERROR]   location: class org.apache.zeppelin.realm.kerberos.KerberosRealm
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[696,14] cannot find symbol
[ERROR]   symbol:   class AuthenticationException
[ERROR]   location: class org.apache.zeppelin.realm.kerberos.KerberosRealm
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[787,44] cannot find symbol
[ERROR]   symbol:   class AuthenticationToken
[ERROR]   location: class org.apache.zeppelin.realm.kerberos.KerberosRealm
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/jwt/KnoxJwtRealm.java:[73,11] cannot find symbol
[ERROR]   symbol:   class Groups
[ERROR]   location: class org.apache.zeppelin.realm.jwt.KnoxJwtRealm
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/LdapRealm.java:[47,30] package org.apache.hadoop.conf does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/LdapRealm.java:[48,40] package org.apache.hadoop.security.alias does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/LdapRealm.java:[49,40] package org.apache.hadoop.security.alias does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosUtil.java:[20,37] package org.apache.hadoop.util does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosUtil.java:[20,1] static import only from classes and interfaces
[ERROR] -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] [http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException]
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :zeppelin-server
The command '/bin/sh -c echo ""unsafe-perm=true"" > ~/.npmrc &&     echo '\{ ""allow_root"": true }' > ~/.bowerrc &&     ./mvnw -B package -DskipTests -Pbuild-distr -Pspark-3.3 -Pinclude-hadoop -Phadoop3 -Pspa
rk-scala-2.12 {-}Pweb-angular -Pweb-dist &&     mv /workspace/zeppelin/zeppelin-distribution/target/zeppelin{-}{*}/zeppelin-{*} /opt/zeppelin/ &&     rm -rf ~/.m2 &&     rm -rf /workspace/zeppelin/*' returned a n
on-zero code: 1
Copy Snippet
Edit Snippet
 Wordwrap
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[687,11] cannot find symbol
[ERROR]   symbol:   class AuthenticationToken
[ERROR]   location: class org.apache.zeppelin.realm.kerberos.KerberosRealm
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[688,14] cannot find symbol
[INFO] Zeppelin: Markdown interpreter ..................... SUCCESS [  4.732 s]                                                                                                                    [89/1957]
[INFO] Zeppelin: MongoDB interpreter ...................... SUCCESS [  0.518 s]
[INFO] Zeppelin: Angular interpreter ...................... SUCCESS [  0.373 s]
[INFO] Zeppelin: Livy interpreter ......................... SUCCESS [ 53.694 s]
[INFO] Zeppelin: HBase interpreter ........................ SUCCESS [ 36.755 s]
[INFO] Zeppelin: JDBC interpreter ......................... SUCCESS [02:19 min]
[INFO] Zeppelin: File System Interpreters ................. SUCCESS [  1.858 s]
[INFO] Zeppelin: Flink Parent ............................. SUCCESS [  0.157 s]
[INFO] Zeppelin: Flink Shims .............................. SUCCESS [  1.803 s]
[INFO] Zeppelin: Flink1.13 Shims .......................... SUCCESS [01:05 min]
[INFO] Zeppelin: Flink1.14 Shims .......................... SUCCESS [ 53.029 s]
[INFO] Zeppelin: Flink1.15 Shims .......................... SUCCESS [01:02 min]
[INFO] Zeppelin: Flink1.16 Shims .......................... SUCCESS [ 38.489 s]
[INFO] Zeppelin: Flink Scala Parent ....................... SUCCESS [01:16 min]
[INFO] Zeppelin: Flink-Cmd interpreter .................... SUCCESS [  2.965 s]
[INFO] Zeppelin: InfluxDB interpreter ..................... SUCCESS [ 14.294 s]
[INFO] Zeppelin: Apache Cassandra interpreter ............. SUCCESS [01:25 min]
[INFO] Zeppelin: Elasticsearch interpreter ................ SUCCESS [ 21.956 s]
[INFO] Zeppelin: BigQuery interpreter ..................... SUCCESS [03:13 min]
[INFO] Zeppelin: Alluxio interpreter ...................... SUCCESS [02:29 min]
[INFO] Zeppelin: Neo4j interpreter ........................ SUCCESS [ 12.282 s]
[INFO] Zeppelin: Java interpreter ......................... SUCCESS [  1.133 s]
[INFO] Zeppelin: Sparql interpreter ....................... SUCCESS [ 21.923 s]
[INFO] Zeppelin: Client ................................... SUCCESS [  3.399 s]
[INFO] Zeppelin: Client Examples .......................... SUCCESS [  0.167 s]
[INFO] Zeppelin: Server ................................... FAILURE [ 38.227 s]
[INFO] Zeppelin: web Application .......................... SKIPPED
[INFO] Zeppelin: Plugins Parent ........................... SKIPPED
[INFO] Zeppelin: Plugin S3NotebookRepo .................... SKIPPED
[INFO] Zeppelin: Plugin GitHubNotebookRepo ................ SKIPPED
[INFO] Zeppelin: Plugin AzureNotebookRepo ................. SKIPPED
[INFO] Zeppelin: Plugin OSSNotebookRepo ................... SKIPPED                                                                                                                                [55/1957]
[INFO] Zeppelin: Plugin Kubernetes StandardLauncher ....... SKIPPED
[INFO] Zeppelin: Plugin Flink Launcher .................... SKIPPED
[INFO] Zeppelin: Plugin Docker Launcher ................... SKIPPED
[INFO] Zeppelin: Plugin Cluster Launcher .................. SKIPPED
[INFO] Zeppelin: Plugin Yarn Launcher ..................... SKIPPED
[INFO] Zeppelin: web angular Application .................. SKIPPED
[INFO] Zeppelin: Packaging distribution ................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  55:42 min
[INFO] Finished at: 2023-02-10T11:18:45Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.10.1:compile (default-compile) on project zeppelin-server: Compilation failure: Compilation failure:
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosAuthenticationFilter.java:[19,56] package org.apache.hadoop.security.authentication.server does not exi
st
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[20,30] package org.apache.hadoop.conf does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[21,34] package org.apache.hadoop.security does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[22,56] package org.apache.hadoop.security.authentication.client does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[23,56] package org.apache.hadoop.security.authentication.client does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[24,56] package org.apache.hadoop.security.authentication.client does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[25,56] package org.apache.hadoop.security.authentication.server does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[26,56] package org.apache.hadoop.security.authentication.server does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[28,1] package org.apache.hadoop.security.authentication.util does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/jwt/KnoxJwtRealm.java:[23,30] package org.apache.hadoop.conf does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/jwt/KnoxJwtRealm.java:[24,34] package org.apache.hadoop.security does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[184,18] cannot find symbol
[ERROR]   symbol:   class Signer
[ERROR]   location: class org.apache.zeppelin.realm.kerberos.KerberosRealm
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[185,11] cannot find symbol
[ERROR]   symbol:   class SignerSecretProvider
[ERROR]   symbol:   class AuthenticationException                                                                                                                                                   [0/1957]
[ERROR]   location: class org.apache.zeppelin.realm.kerberos.KerberosRealm
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[695,18] cannot find symbol
[ERROR]   symbol:   class AuthenticationToken
[ERROR]   location: class org.apache.zeppelin.realm.kerberos.KerberosRealm
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[696,14] cannot find symbol
[ERROR]   symbol:   class AuthenticationException
[ERROR]   location: class org.apache.zeppelin.realm.kerberos.KerberosRealm
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosRealm.java:[787,44] cannot find symbol
[ERROR]   symbol:   class AuthenticationToken
[ERROR]   location: class org.apache.zeppelin.realm.kerberos.KerberosRealm
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/jwt/KnoxJwtRealm.java:[73,11] cannot find symbol
[ERROR]   symbol:   class Groups
[ERROR]   location: class org.apache.zeppelin.realm.jwt.KnoxJwtRealm
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/LdapRealm.java:[47,30] package org.apache.hadoop.conf does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/LdapRealm.java:[48,40] package org.apache.hadoop.security.alias does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/LdapRealm.java:[49,40] package org.apache.hadoop.security.alias does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosUtil.java:[20,37] package org.apache.hadoop.util does not exist
[ERROR] /workspace/zeppelin/zeppelin-server/src/main/java/org/apache/zeppelin/realm/kerberos/KerberosUtil.java:[20,1] static import only from classes and interfaces
[ERROR] -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] [http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException]
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :zeppelin-server
The command '/bin/sh -c echo ""unsafe-perm=true"" > ~/.npmrc &&     echo '\{ ""allow_root"": true }' > ~/.bowerrc &&     ./mvnw -B package -DskipTests -Pbuild-distr -Pspark-3.3 -Pinclude-hadoop -Phadoop3 -Pspa
rk-scala-2.12 {-}Pweb-angular -Pweb-dist &&     mv /workspace/zeppelin/zeppelin-distribution/target/zeppelin{-}{*}/zeppelin-{*} /opt/zeppelin/ &&     rm -rf ~/.m2 &&     rm -rf /workspace/zeppelin/*' returned a n
on-zero code: 1",phongnt,comet,Major,Resolved,Fixed,11/Feb/23 13:50,01/Mar/23 16:41
Bug,ZEPPELIN-5885,13526610,IOException: Fail to clone note,"I have problems running clone notes concurrently.

The test code:
{code:java}
package com.test.zeppelin;import kong.unirest.HttpResponse;
import kong.unirest.JsonNode;
import kong.unirest.Unirest;
import kong.unirest.json.JSONObject;
import org.apache.commons.text.StringEscapeUtils;
import org.apache.zeppelin.client.ClientConfig;
import org.apache.zeppelin.client.ZeppelinClient;public class ZeppelinClientTest
{
    public static void main(String[] args) throws Exception
    {
        final ClientConfig config = new ClientConfig(""http://23.134.136.201:8080"");
        final ZeppelinClient zeppelinClient = new ZeppelinClient(config);        for (int i = 0; i < 10; i++)
        {
            new Thread(new Runnable() {
                @Override
                public void run() {
                    for (int j = 0; j < 1000; j++) {
                        try {
                            cloneNote(""2HSUP6HYY"", """", ""/test/cloneNote"" + System.currentTimeMillis());
                        } catch (Exception e) {
                            e.printStackTrace();
                        }
                    }
                }
            }).start();
        }
    }    /**
     * @param noteId 
     * @param userName
     * @param noteName
     * @throws Exception      */
    public static String cloneNote(String noteId, String userName, String noteName) throws Exception
    {
        JsonNode jsonNode = null;        try
        {
            JSONObject bodyObject = new JSONObject();
            bodyObject.put(""name"", noteName);
            HttpResponse<JsonNode> response = Unirest.post(""/notebook/{noteId}"").routeParam(""noteId"", noteId).body(bodyObject.toString()).asJson();
   
            jsonNode = response.getBody();
        }
        catch (Exception e)
        {
            throw new Exception(""clone note failed."", e);
        }        return jsonNode.getObject().getString(""body"");
    }
} {code}
 

The error is reported as follows
{code:java}
ERROR [2023-03-01 11:35:45,619] ({qtp398690014-16} WebApplicationExceptionMapper.java[toResponse]:49) - Error response
java.util.ConcurrentModificationException
    at java.util.HashMap$HashIterator.nextNode(HashMap.java:1442)
    at java.util.HashMap$EntryIterator.next(HashMap.java:1476)
    at java.util.HashMap$EntryIterator.next(HashMap.java:1474)
    at org.apache.zeppelin.notebook.NotebookAuthorizationInfoSaving.<init>(NotebookAuthorizationInfoSaving.java:39)
    at org.apache.zeppelin.notebook.AuthorizationService.saveNoteAuth(AuthorizationService.java:109)
    at org.apache.zeppelin.notebook.Notebook.createNote(Notebook.java:258)
    at org.apache.zeppelin.notebook.Notebook.createNote(Notebook.java:220)
    at org.apache.zeppelin.notebook.Notebook.cloneNote(Notebook.java:322)
    at org.apache.zeppelin.service.NotebookService.cloneNote(NotebookService.java:290)
    at org.apache.zeppelin.rest.NotebookRestApi.cloneNote(NotebookRestApi.java:454)
    at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
    at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
    at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:469)
    at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:391)
    at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:80)
    at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:253)
    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
    at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
    at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
    at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
    at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
    at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:232)
    at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
    at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
    at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
    at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
    at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
    at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
    at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:763)
    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1651)
    at org.apache.zeppelin.server.CorsFilter.doFilter(CorsFilter.java:64)
    at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1638)
    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:567)
    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
    at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
    at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
    at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1610)
    at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
    at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1377)
    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:507)
    at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1580)
    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
    at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1292)
    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
    at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
    at io.micrometer.core.instrument.binder.jetty.TimedHandler.handle(TimedHandler.java:120)
    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
    at org.eclipse.jetty.server.Server.handle(Server.java:501)
    at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
    at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:556)
    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
    at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
    at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
    at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
    at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
    at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
    at java.lang.Thread.run(Thread.java:748) {code}",,qinbo,Major,Closed,Fixed,01/Mar/23 05:48,08/Jun/23 10:31
Bug,ZEPPELIN-5897,13532346,Spark-Interpreter context change,"I have encountered some strange behaviour in the Spark interpreter. This problem occurs when several cron jobs are started in parallel.

The launch command looks quite good.
{code:java}
[INFO] Interpreter launch command: /opt/conda/lib/python3.9/site-packages/pyspark/bin/spark-submit --class org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer --driver-class-path /usr/share/java/*:/tmp/local-repo/spark_8g_8g/*:/opt/zeppelin/interpreter/spark/*:::/opt/zeppelin/interpreter/zeppelin-interpreter-shaded-0.11.0-SNAPSHOT.jar:/opt/zeppelin/interpreter/spark/spark-interpreter-0.11.0-SNAPSHOT.jar --driver-java-options   -Dfile.encoding=UTF-8 -Dlog4j.configuration=file:///opt/zeppelin/conf/log4j.properties -Dlog4j.configurationFile=file:///opt/zeppelin/conf/log4j2.properties -Dzeppelin.log.file=/opt/zeppelin/logs/zeppelin-interpreter-spark_8g_8g-isolated-2G8V2J18D-2023-04-11_00-00-00--spark8g8g-isolated-2g8v2j18d-2023-04-1100-00-00-upuren.log --conf spark.driver.maxResultSize=8g --conf spark.kubernetes.executor.request.cores=0. --conf spark.network.timeout=1800 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --verbose --conf spark.jars.ivySettings=/opt/spark/ivysettings.xml --proxy-user ejavaheri --conf spark.master=k8s://https://kubernetes.default.svc --conf spark.driver.memory=8g --conf spark.driver.cores=2 --conf spark.app.name=spark_8g_8g --conf spark.driver.host=spark8g8g-isolated-2g8v2j18d-2023-04-1100-00-00-upuren.spark.svc --conf spark.kubernetes.memoryOverheadFactor=0.4 --conf spark.webui.yarn.useProxy=false --conf spark.blockManager.port=22322 --conf spark.driver.port=22321 --conf spark.driver.bindAddress=0.0.0.0 --conf spark.kubernetes.namespace=spark --conf spark.kubernetes.driver.request.cores=200m --conf spark.kubernetes.driver.pod.name=spark8g8g-isolated-2g8v2j18d-2023-04-1100-00-00-upuren --conf spark.executor.instances=1 --conf spark.executor.memory=8g --conf spark.executor.cores=4 --conf spark.submit.deployMode=client --conf spark.kubernetes.container.image=harbor.mycompany.com/dap/zeppelin-executor:3.3 /opt/zeppelin/interpreter/spark/spark-interpreter-0.11.0-SNAPSHOT.jar zeppelin-server.spark.svc 12320 spark_8g_8g-isolated-2G8V2J18D-2023-04-11_00-00-00 12321:12321{code}
 

As you can see the config value `spark.driver.host` is `spark8g8g-isolated-2g8v2j18d-2023-04-1100-00-00-upuren.spark.svc`, which is correct

During start-up, the host seems to change. New name:
{code:java}
spark2g4g-isolated-2d8reueys-2023-04-1100-00-00-fbvrgw.spark.svc {code}

The new name is the host name of the other parallel running cron job. How is it possible that the spark driver host changes? Does Zeppelin even have the possibility to do this?

{code:java}

INFO [2023-04-11 00:00:04,288] ({RegisterThread} RemoteInterpreterServer.java[run]:620) - Start registration
INFO [2023-04-11 00:00:04,288] ({RemoteInterpreterServer-Thread} RemoteInterpreterServer.java[run]:200) - Launching ThriftServer at 10.129.4.191:12321
INFO [2023-04-11 00:00:05,409] ({RegisterThread} RemoteInterpreterServer.java[run]:634) - Registering interpreter process
INFO [2023-04-11 00:00:05,433] ({RegisterThread} RemoteInterpreterServer.java[run]:636) - Registered interpreter process
INFO [2023-04-11 00:00:05,433] ({RegisterThread} RemoteInterpreterServer.java[run]:657) - Registration finished
WARN [2023-04-11 00:00:05,517] ({pool-3-thread-1} ZeppelinConfiguration.java[<init>]:87) - Failed to load XML configuration, proceeding with a default,for a stacktrace activate the debug log
INFO [2023-04-11 00:00:05,522] ({pool-3-thread-1} ZeppelinConfiguration.java[create]:137) - Server Host: 127.0.0.1
INFO [2023-04-11 00:00:05,523] ({pool-3-thread-1} ZeppelinConfiguration.java[create]:144) - Zeppelin Version: 0.11.0-SNAPSHOT
INFO [2023-04-11 00:00:05,522] ({pool-3-thread-1} ZeppelinConfiguration.java[create]:141) - Server Port: 8080
INFO [2023-04-11 00:00:05,523] ({pool-3-thread-1} ZeppelinConfiguration.java[create]:143) - Context Path: /
INFO [2023-04-11 00:00:05,531] ({pool-3-thread-1} RemoteInterpreterServer.java[createLifecycleManager]:293) - Creating interpreter lifecycle manager: org.apache.zeppelin.interpreter.lifecycle.TimeoutLifecycleManager
INFO [2023-04-11 00:00:05,535] ({pool-3-thread-1} RemoteInterpreterServer.java[init]:236) - Creating RemoteInterpreterEventClient with connection pool size: 100
INFO [2023-04-11 00:00:05,535] ({pool-3-thread-1} TimeoutLifecycleManager.java[onInterpreterProcessStarted]:73) - Interpreter process: spark_8g_8g-isolated-2G8V2J18D-2023-04-11_00-00-00 is started
INFO [2023-04-11 00:00:05,535] ({pool-3-thread-1} TimeoutLifecycleManager.java[<init>]:67) - TimeoutLifecycleManager is started with checkInterval: 60000, timeoutThreshold: ¸3600000
INFO [2023-04-11 00:00:05,627] ({pool-3-thread-1} RemoteInterpreterServer.java[createInterpreter]:406) - Instantiate interpreter org.apache.zeppelin.spark.SparkInterpreter, isForceShutdown: true
INFO [2023-04-11 00:00:05,635] ({pool-3-thread-1} RemoteInterpreterServer.java[createInterpreter]:406) - Instantiate interpreter org.apache.zeppelin.spark.SparkSqlInterpreter, isForceShutdown: true
INFO [2023-04-11 00:00:05,645] ({pool-3-thread-1} RemoteInterpreterServer.java[createInterpreter]:406) - Instantiate interpreter org.apache.zeppelin.spark.PySparkInterpreter, isForceShutdown: true
INFO [2023-04-11 00:00:05,655] ({pool-3-thread-1} RemoteInterpreterServer.java[createInterpreter]:406) - Instantiate interpreter org.apache.zeppelin.spark.IPySparkInterpreter, isForceShutdown: true
INFO [2023-04-11 00:00:05,663] ({pool-3-thread-1} RemoteInterpreterServer.java[createInterpreter]:406) - Instantiate interpreter org.apache.zeppelin.spark.SparkRInterpreter, isForceShutdown: true
INFO [2023-04-11 00:00:05,670] ({pool-3-thread-1} RemoteInterpreterServer.java[createInterpreter]:406) - Instantiate interpreter org.apache.zeppelin.spark.SparkIRInterpreter, isForceShutdown: true
INFO [2023-04-11 00:00:05,679] ({pool-3-thread-1} RemoteInterpreterServer.java[createInterpreter]:406) - Instantiate interpreter org.apache.zeppelin.spark.SparkShinyInterpreter, isForceShutdown: true
INFO [2023-04-11 00:00:05,753] ({pool-3-thread-1} RemoteInterpreterServer.java[createInterpreter]:406) - Instantiate interpreter org.apache.zeppelin.spark.KotlinSparkInterpreter, isForceShutdown: true
INFO [2023-04-11 00:00:05,806] ({pool-3-thread-1} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_688737023
INFO [2023-04-11 00:00:05,806] ({pool-3-thread-1} SchedulerFactory.java[<init>]:56) - Scheduler Thread Pool Size: 100
INFO [2023-04-11 00:00:05,810] ({FIFOScheduler-interpreter_688737023-Worker-1} AbstractScheduler.java[runJob]:127) - Job 20210622-101638_112853005 started by scheduler interpreter_688737023
INFO [2023-04-11 00:00:05,818] ({pool-3-thread-2} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_839216362
INFO [2023-04-11 00:00:05,818] ({pool-3-thread-2} SchedulerFactory.java[createOrGetParallelScheduler]:88) - Create ParallelScheduler: org.apache.zeppelin.spark.SparkSqlInterpreter1135593921 with maxConcurrency: 10
INFO [2023-04-11 00:00:05,857] ({FIFOScheduler-interpreter_688737023-Worker-1} SparkInterpreter.java[extractScalaVersion]:279) - Using Scala: version 2.12.15
INFO [2023-04-11 00:00:05,881] ({FIFOScheduler-interpreter_688737023-Worker-1} SparkScala212Interpreter.scala[createSparkILoop]:182) - Scala shell repl output dir: /tmp/spark16004603505225443508
INFO [2023-04-11 00:00:06,113] ({FIFOScheduler-interpreter_688737023-Worker-1} SparkScala212Interpreter.scala[createSparkILoop]:191) - UserJars: file:/opt/zeppelin/interpreter/spark/spark-interpreter-0.11.0-SNAPSHOT.jar:/opt/zeppelin/interpreter/spark/scala-2.12/spark-scala-2.12-0.11.0-SNAPSHOT.jar
INFO [2023-04-11 00:00:11,260] ({FIFOScheduler-interpreter_688737023-Worker-1} HiveConf.java[findConfigFile]:187) - Found configuration file file:/opt/conda/lib/python3.9/site-packages/pyspark/conf/hive-site.xml
INFO [2023-04-11 00:00:11,438] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Running Spark version 3.3.0
INFO [2023-04-11 00:00:11,472] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - No custom resources configured for spark.driver.
INFO [2023-04-11 00:00:11,472] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - ==============================================================
INFO [2023-04-11 00:00:11,471] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - ==============================================================
INFO [2023-04-11 00:00:11,473] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Submitted application: spark_8g_8g
INFO [2023-04-11 00:00:11,500] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 8192, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
INFO [2023-04-11 00:00:11,512] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Limiting resource is cpus at 4 tasks per executor
INFO [2023-04-11 00:00:11,515] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Added ResourceProfile id: 0
INFO [2023-04-11 00:00:11,580] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Changing view acls to: zeppelin,ejavaheri
INFO [2023-04-11 00:00:11,580] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Changing modify acls to: zeppelin,ejavaheri
INFO [2023-04-11 00:00:11,581] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(zeppelin, ejavaheri); groups with view permissions: Set(); users  with modify permissions: Set(zeppelin, ejavaheri); groups with modify permissions: Set()
INFO [2023-04-11 00:00:11,581] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Changing modify acls groups to:
INFO [2023-04-11 00:00:11,581] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Changing view acls groups to:
INFO [2023-04-11 00:00:11,852] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Successfully started service 'sparkDriver' on port 22321.
INFO [2023-04-11 00:00:11,880] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Registering MapOutputTracker
INFO [2023-04-11 00:00:11,912] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Registering BlockManagerMaster
INFO [2023-04-11 00:00:11,946] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
INFO [2023-04-11 00:00:11,947] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - BlockManagerMasterEndpoint up
INFO [2023-04-11 00:00:11,950] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Registering BlockManagerMasterHeartbeat
INFO [2023-04-11 00:00:11,975] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Created local directory at /tmp/blockmgr-1903d257-be01-4cb7-954f-9a5c13ab0598
INFO [2023-04-11 00:00:11,993] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - MemoryStore started with capacity 4.6 GiB
INFO [2023-04-11 00:00:12,010] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Registering OutputCommitCoordinator
INFO [2023-04-11 00:00:12,079] ({FIFOScheduler-interpreter_688737023-Worker-1} Log.java[initialized]:170) - Logging initialized @9839ms to org.sparkproject.jetty.util.log.Slf4jLog
INFO [2023-04-11 00:00:12,193] ({FIFOScheduler-interpreter_688737023-Worker-1} Server.java[doStart]:375) - jetty-9.4.46.v20220331; built: 2022-03-31T16:38:08.030Z; git: bc17a0369a11ecf40bb92c839b9ef0a8ac50ea18; jvm 11.0.17+8-post-Ubuntu-1ubuntu220.04
INFO [2023-04-11 00:00:12,223] ({FIFOScheduler-interpreter_688737023-Worker-1} Server.java[doStart]:415) - Started @9983ms
INFO [2023-04-11 00:00:12,273] ({FIFOScheduler-interpreter_688737023-Worker-1} AbstractConnector.java[doStart]:333) - Started ServerConnector@325be8be{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
INFO [2023-04-11 00:00:12,274] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Successfully started service 'SparkUI' on port 4040.
INFO [2023-04-11 00:00:12,310] ({FIFOScheduler-interpreter_688737023-Worker-1} ContextHandler.java[doStart]:921) - Started o.s.j.s.ServletContextHandler@47745fce{/,null,AVAILABLE,@Spark}
INFO [2023-04-11 00:00:12,342] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Added JAR file:/opt/zeppelin/interpreter/spark/spark-interpreter-0.11.0-SNAPSHOT.jar at spark://spark2g4g-isolated-2d8reueys-2023-04-1100-00-00-fbvrgw.spark.svc:22321/jars/spark-interpreter-0.11.0-SNAPSHOT.jar with timestamp 1681164011433
INFO [2023-04-11 00:00:12,413] ({FIFOScheduler-interpreter_688737023-Worker-1} Logging.scala[logInfo]:61) - Auto-configuring K8S client using current context from users K8S config file
{code}",Reamer,Reamer,Major,Resolved,Fixed,12/Apr/23 13:01,26/May/23 11:37
Bug,ZEPPELIN-5898,13532874,down csv data error,"query data

!image-2023-04-17-19-56-48-675.png!

 

but down load csv data

!image-2023-04-17-19-57-40-741.png!

 

 

 ",,zhugezifang,Major,Resolved,Fixed,17/Apr/23 11:57,02/May/23 06:40
Bug,ZEPPELIN-5900,13533551,Fix RemoteSchedulerTest.testAbortOnPending,"During ZEPPELIN-5851 we have noticed that the test RemoteSchedulerTest.testAbortOnPending constantly fails.
The error is also reproducible in the local IDE.",Reamer,Reamer,Major,Resolved,Fixed,21/Apr/23 11:18,02/May/23 06:38
Bug,ZEPPELIN-5903,13534038,NoteCache still growing,"The NoteCache does not shrink so easily. In some cases, the NoteCache has grown to 650 entries, although the threshold is set at 50.
This can happen when Zeppelin is starting and a user starts a long running note during the Notebook Init Thread. The Notebook Init Thread now loads various Notes. However, the oldest note from the user cannot be removed because it is in use. The cache therefore grows very strongly above the threshold.",Reamer,Reamer,Major,Resolved,Fixed,25/Apr/23 18:05,02/May/23 06:39
Bug,ZEPPELIN-5910,13535599,zeppelin-interpreter has hamcrest-all as compile dependency,hamcrest-all should only be included as a test dependency.,Reamer,Reamer,Major,Resolved,Fixed,10/May/23 07:37,19/Jun/23 17:35
Bug,ZEPPELIN-5920,13538082,DeprecationWarning when using z.show with non Integer index in DataFrame,"If z.show is used with a Panda DataFrame that has a timestamp as index, then a DeprecationWarning appears for every row in the data frame.

*Steps to reproduce:*
 * Execute the following code with python interpreter:

{code:python}
import pandas as pd

idx = pd.date_range('20230530', periods=3, freq='D')
df = pd.DataFrame({'name':['a','b','c']}, index= idx)
z.show(df, show_index=True){code}
*Expected result:*
 * A table of the data frame
 * No warnings

*Observed result:*
 * A table of the data frame
 * A deprecation warning for every row in the data frame

 ",,matthias.koch,Minor,Resolved,Fixed,30/May/23 08:30,31/May/23 12:43
Bug,ZEPPELIN-5926,13539081,zeppelin_user_sockets and zeppelin_note_sockets are not decreased,"While searching for a memory leak I came across the metrics `zeppelin_note_sockets` and `zeppelin_user_sockets` which only drops back to 0 after a reboot.
 !image-2023-06-07-14-20-42-696.png! ",Reamer,Reamer,Major,Resolved,Fixed,07/Jun/23 12:23,12/Jun/23 09:25
Bug,ZEPPELIN-5927,13539367,Solve the concurrency calls to `saveNoteAuth`,"I have problems the concurrency calls to `saveNoteAuth`.
[related pull request |[https://github.com/apache/zeppelin/pull/4563],] this pull request migrates solve the concurrency problem caused by multiple concurrent calls to `org.apache.zeppelin.notebook.AuthorizationService#saveNoteAuth`, but this can result in concurrent modifications to `notebook authorization.json`, then throw java.nio.file.NoSuchFileException.
{code:java}
Caused by: java.nio.file.NoSuchFileException: /usr/local/zeppelin-0.10.1-bin-all/conf/notebook-authorization.json
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixCopyFile.move(UnixCopyFile.java:447)
    at sun.nio.fs.UnixFileSystemProvider.move(UnixFileSystemProvider.java:262)
    at java.nio.file.Files.move(Files.java:1395)
    at org.apache.zeppelin.util.FileUtils.atomicWriteToFile(FileUtils.java:60)
    at org.apache.zeppelin.util.FileUtils.atomicWriteToFile(FileUtils.java:71)
    at org.apache.zeppelin.storage.LocalConfigStorage.save(LocalConfigStorage.java:71)
    at org.apache.zeppelin.notebook.AuthorizationService.saveNoteAuth(AuthorizationService.java:109)
    at org.apache.zeppelin.notebook.Notebook.createNote(Notebook.java:258)
    at org.apache.zeppelin.service.NotebookService.createNote(NotebookService.java:168) {code}",,yousj,Major,Resolved,Fixed,09/Jun/23 04:44,12/Jun/23 09:24
Bug,ZEPPELIN-5929,13539440,Spark Basic Feature Tutorial notebook data link is broken,"Spark Tutorial > 2. Spark Basic Features notebook > Load data into table section load bank data link is broken. It look like, it is not available anymore.
{code:java}
// load bank data
val bankText = sc.parallelize(
    IOUtils.toString(
        new URL(""https://s3.amazonaws.com/apache-zeppelin/tutorial/bank/bank.csv""),
        Charset.forName(""utf8"")).split(""\n""))
{code}
{noformat}
java.io.IOException: Server returned HTTP response code: 403 for URL: https://s3.amazonaws.com/apache-zeppelin/tutorial/bank/bank.csv
{noformat}",jongyoul,holoda,Major,Closed,Fixed,09/Jun/23 13:15,29/Jun/23 08:06
Bug,SPARK-41848,13516236,Tasks are over-scheduled with TaskResourceProfile,"{code:java}
test(""SPARK-XXX"") {
  val conf = new SparkConf().setAppName(""test"").setMaster(""local-cluster[1,4,1024]"")
  sc = new SparkContext(conf)
  val req = new TaskResourceRequests().cpus(3)
  val rp = new ResourceProfileBuilder().require(req).build()

  val res = sc.parallelize(Seq(0, 1), 2).withResources(rp).map { x =>
    Thread.sleep(5000)
    x * 2
  }.collect()
  assert(res === Array(0, 2))
} {code}
In this test, tasks are supposed to be scheduled in order since each task requires 3 cores but the executor only has 4 cores. However, we noticed 2 tasks are launched concurrently from the logs.

It turns out that we used the TaskResourceProfile (taskCpus=3) of the taskset for task scheduling:
{code:java}
val rpId = taskSet.taskSet.resourceProfileId
val taskSetProf = sc.resourceProfileManager.resourceProfileFromId(rpId)
val taskCpus = ResourceProfile.getTaskCpusOrDefaultForProfile(taskSetProf, conf) {code}
but the ResourceProfile (taskCpus=1) of the executor for updating the free cores in ExecutorData:
{code:java}
val rpId = executorData.resourceProfileId
val prof = scheduler.sc.resourceProfileManager.resourceProfileFromId(rpId)
val taskCpus = ResourceProfile.getTaskCpusOrDefaultForProfile(prof, conf)
executorData.freeCores -= taskCpus {code}
which results in the inconsistency of the available cores.",ivoson,Ngone51,Blocker,Resolved,Fixed,03/Jan/23 02:44,09/Jan/23 20:03
Bug,SPARK-41858,13516291,Fix ORC reader perf regression due to DEFAULT value feature,A huge ORC reader perf regression is detected by SPARK-41782. The root cause was SPARK-39862.,dongjoon,dongjoon,Blocker,Resolved,Fixed,03/Jan/23 09:48,03/Jan/23 18:45
Bug,SPARK-41859,13516322,CreateHiveTableAsSelectCommand should set the overwrite flag correctly,,cloud_fan,cloud_fan,Major,Resolved,Fixed,03/Jan/23 12:33,04/Jan/23 05:09
Bug,SPARK-41894,13516596,sql/core module mvn clean failed,"run the following commands:
 # mvn clean install -pl sql/core -am -DskipTests
 # mvn test -pl sql/core 
 # mvn clean

 

then following error:

 
{code:java}
[INFO] Spark Project Parent POM ........................... SUCCESS [  0.133 s]
[INFO] Spark Project Tags ................................. SUCCESS [  0.008 s]
[INFO] Spark Project Sketch ............................... SUCCESS [  0.007 s]
[INFO] Spark Project Local DB ............................. SUCCESS [  0.008 s]
[INFO] Spark Project Networking ........................... SUCCESS [  0.015 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  0.020 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [  0.007 s]
[INFO] Spark Project Launcher ............................. SUCCESS [  0.008 s]
[INFO] Spark Project Core ................................. SUCCESS [  0.279 s]
[INFO] Spark Project ML Local Library ..................... SUCCESS [  0.010 s]
[INFO] Spark Project GraphX ............................... SUCCESS [  0.016 s]
[INFO] Spark Project Streaming ............................ SUCCESS [  0.039 s]
[INFO] Spark Project Catalyst ............................. SUCCESS [  0.262 s]
[INFO] Spark Project SQL .................................. FAILURE [  1.305 s]
[INFO] Spark Project ML Library ........................... SKIPPED
[INFO] Spark Project Tools ................................ SKIPPED
[INFO] Spark Project Hive ................................. SKIPPED
[INFO] Spark Project REPL ................................. SKIPPED
[INFO] Spark Project YARN Shuffle Service ................. SKIPPED
[INFO] Spark Project YARN ................................. SKIPPED
[INFO] Spark Project Mesos ................................ SKIPPED
[INFO] Spark Project Kubernetes ........................... SKIPPED
[INFO] Spark Project Hive Thrift Server ................... SKIPPED
[INFO] Spark Ganglia Integration .......................... SKIPPED
[INFO] Spark Project Hadoop Cloud Integration ............. SKIPPED
[INFO] Spark Project Assembly ............................. SKIPPED
[INFO] Kafka 0.10+ Token Provider for Streaming ........... SKIPPED
[INFO] Spark Integration for Kafka 0.10 ................... SKIPPED
[INFO] Kafka 0.10+ Source for Structured Streaming ........ SKIPPED
[INFO] Spark Kinesis Integration .......................... SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Integration for Kafka 0.10 Assembly .......... SKIPPED
[INFO] Spark Avro ......................................... SKIPPED
[INFO] Spark Project Connect Common ....................... SKIPPED
[INFO] Spark Project Connect Server ....................... SKIPPED
[INFO] Spark Project Connect Client ....................... SKIPPED
[INFO] Spark Protobuf ..................................... SKIPPED
[INFO] Spark Project Kinesis Assembly ..................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  2.896 s
[INFO] Finished at: 2023-01-05T15:15:57+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-clean-plugin:3.1.0:clean (default-clean) on project spark-sql_2.13: Failed to clean project: Failed to delete /${basedir}/sql/core/target/tmp/streaming.metadata-1b8b16d8-c9ba-4c38-9ac0-94a39f583082/commits/.0.crc -> [Help 1]
 {code}
 

 

run :
 * ll /${basedir}/sql/core/target/tmp/streaming.metadata-1b8b16d8-c9ba-4c38-9ac0-94a39f583082/commits/.0.crc

 

 
{code:java}
-rw-r--r-- 1 work work 12 Dec 28 16:06 /${basedir}/sql/core/target/tmp/streaming.metadata-1b8b16d8-c9ba-4c38-9ac0-94a39f583082/commits/.0.crc{code}
 

 

and current user(work) can't rm this file:
 * rm  /${basedir}/sql/core/target/tmp/streaming.metadata-1b8b16d8-c9ba-4c38-9ac0-94a39f583082/commits/.0.crc

 
{code:java}
rm: cannot remove `/${basedir}/sql/core/target/tmp/streaming.metadata-1b8b16d8-c9ba-4c38-9ac0-94a39f583082/commits/.0.crc': Permission denied {code}
need to use root to clean this file

 ",LuciferYang,LuciferYang,Major,Resolved,Fixed,05/Jan/23 07:28,09/Jan/23 02:22
Bug,SPARK-41896,13516619,Filtering by row_index always returns empty results,"Queries that include a filter with row_index currently always return an empty result. This is because we consider all metadata attributes constant per file [here|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala#L76] and the filter then always evaluates to false.

This should be fixed as a follow up to SPARK-41791",olaky,olaky,Critical,Resolved,Fixed,05/Jan/23 09:17,16/Jan/23 12:18
Bug,SPARK-41912,13516714,Subquery should not validate CTE,,amaliujia,amaliujia,Major,Resolved,Fixed,05/Jan/23 19:04,06/Jan/23 03:31
Bug,SPARK-41914,13516724,Sorting issue with partitioned-writing and planned write optimization disabled,"Spark 3.4.0 introduced option {{{}spark.sql.optimizer.plannedWrite.enabled{}}}, which is enabled by default. When disabled, partitioned writing loses in-partition order when spilling occurs.

This is related to SPARK-40885 where setting option {{spark.sql.optimizer.plannedWrite.enabled}} to {{true}} will remove the existing sort (for {{day}} and {{{}id{}}}) entirely.

Run this with 512m memory and one executor, e.g.:
{code}
spark-shell --driver-memory 512m --master ""local[1]""
{code}
{code:scala}
import org.apache.spark.sql.SaveMode

spark.conf.set(""spark.sql.optimizer.plannedWrite.enabled"", false)

val ids = 2000000
val days = 2
val parts = 2

val ds = spark.range(0, days, 1, parts).withColumnRenamed(""id"", ""day"").join(spark.range(0, ids, 1, parts))

ds.repartition($""day"")
  .sortWithinPartitions($""day"", $""id"")
  .write
  .partitionBy(""day"")
  .mode(SaveMode.Overwrite)
  .csv(""interleaved.csv"")
{code}
Check the written files are sorted (states OK when file is sorted):
{code:bash}
for file in interleaved.csv/day\=*/part-*
do
  echo ""$(sort -n ""$file"" | md5sum | cut -d "" "" -f 1)  $file""
done | md5sum -c
{code}
Files should look like this
{code}
0
1
2
...
1048576
1048577
1048578
...
{code}
But they look like
{code}
0
1048576
1
1048577
2
1048578
...
{code}
The cause issue is the same as in SPARK-40588. A sort (for {{{}day{}}}) is added on top of the existing sort (for {{day}} and {{{}id{}}}). Spilling interleaves the sorted spill files.

{code}
Sort [input[0, bigint, false] ASC NULLS FIRST], false, 0
+- AdaptiveSparkPlan isFinalPlan=false
   +- Sort [day#2L ASC NULLS FIRST, id#4L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(day#2L, 200), REPARTITION_BY_COL, [plan_id=30]
         +- BroadcastNestedLoopJoin BuildLeft, Inner
            :- BroadcastExchange IdentityBroadcastMode, [plan_id=28]
            :  +- Project [id#0L AS day#2L]
            :     +- Range (0, 2, step=1, splits=2)
            +- Range (0, 2000000, step=1, splits=2)
{code}
",EnricoMi,EnricoMi,Major,Resolved,Fixed,05/Jan/23 21:05,13/Jan/23 07:07
Bug,SPARK-41937,13517141,SparkR datetime column compare with Sys.time() throws error in R (>= 4.2.0),"Base R 4.2.0 introduced a change ([[Rd] R 4.2.0 is released|https://stat.ethz.ch/pipermail/r-announce/2022/000683.html]), ""{{{}Calling if() or while() with a condition of length greater than one gives an error rather than a warning.{}}}""

The below code is a reproducible example of the issue. If it is executed in R >=4.2.0 then it will generate an error, or else just a warning message. `{{{}Sys.time()`{}}} is a multi-class object in R, and throughout the Spark R repository '{{{}if{}}}' statement is used as: `{{{}if(class(x) == ""Column""){}}}` - this causes error in the latest R version >= 4.2.0. Note that R allows an object to have multiple '{{{}class{}}}' names as a character vector ([R: Object Classes|https://stat.ethz.ch/R-manual/R-devel/library/base/html/class.html]); hence this type of check itself was not a good idea in the first place.

The below chunks are executed on R version 4.1.3.
{code:java}
{
 SparkR::sparkR.session()
 t <- Sys.time()
 sdf <- SparkR::createDataFrame(data.frame(x = t + c(-1, 1, -1, 1, -1)))
 SparkR::collect(SparkR::filter(sdf, SparkR::column('x') > t))
}
#> Warning in if (class(e2) == 'Column') {: the condition has length > 1 
#> and only the first element will be used
#> x
#> 1 2023-01-07 20:40:20
#> 2 2023-01-07 20:40:20 

{code}
 

 
{code:java}
{
 Sys.setenv(`_R_CHECK_LENGTH_1_CONDITION_` = ""true"")
 SparkR::sparkR.session()
 t <- Sys.time()
 sdf <- SparkR::createDataFrame(data.frame(x = t + c(-1, 1, -1, 1, -1)))
 SparkR::collect(SparkR::filter(sdf, SparkR::column('x') > t))
}
#> Error in h(simpleError(msg, call)): error in evaluating the argument 'x' 
#> in selecting a method for function 'collect': error in evaluating the 
#> argument 'condition' in selecting a method for function 'filter': the
#> condition has length > 1 {code}
 

Similar issue is noted for these SparkR functions where {{Sys.time()}} type of multi-class data might be used: {{lit, fillna, when, otherwise, contains, ifelse }}

The suggested change is to add the `{{{}all{}}}` function (or `{{{}any{}}}`, as appropriate) while doing the check of whether `{{{}class(.){}}}` is `{{{}Column{}}}` or not: `{{{}if(all(class(.) == ""Column"")){}}}`. Or, better to use `{{{}base::inherits{}}}` for this check as `{{{}if(inherits(., ""Column"")){}}}`.",atalvivek,atalvivek,Minor,Resolved,Fixed,08/Jan/23 02:37,09/Jan/23 00:41
Bug,SPARK-41947,13517191,Update the contents of error class guidelines,"The error class guidelines for `core/src/main/resources/error/README.md` is out of date, we should update the guidelines to match the current behavior.",itholic,itholic,Major,Resolved,Fixed,09/Jan/23 07:23,09/Jan/23 20:24
Bug,SPARK-41948,13517202,Fix NPE for error classes: CANNOT_PARSE_JSON_FIELD,,panbingkun,panbingkun,Minor,Resolved,Fixed,09/Jan/23 08:34,23/Jan/23 12:16
Bug,SPARK-41952,13517298,Upgrade Parquet to fix off-heap memory leaks in Zstd codec,"Recently, native memory leak have been discovered in Parquet in conjunction of it using Zstd decompressor from luben/zstd-jni library (PARQUET-2160).

This is very problematic to a point where we can't use Parquet w/ Zstd due to pervasive OOMs taking down our executors and disrupting our jobs.

Luckily fix addressing this had already landed in Parquet:
[https://github.com/apache/parquet-mr/pull/982]

 

Now, we just need to
 # Updated version of Parquet is released in a timely manner
 # Spark is upgraded onto this new version in the upcoming release

 ",chengpan,alexey.kudinkin,Critical,Resolved,Fixed,09/Jan/23 23:48,20/Feb/23 17:45
Bug,SPARK-41958,13517367,Disallow arbitrary custom classpath with proxy user in cluster mode,To avoid arbitrary classpath in spark cluster.,Ngone51,Ngone51,Major,Resolved,Fixed,10/Jan/23 03:20,06/Jun/23 02:31
Bug,SPARK-41971,13517524,`toPandas` should support duplicate filed names when arrow-optimization is on,"toPandas support duplicate columns name, but for a struct column, it doesnot support duplicate field names.

{code:java}
In [27]: spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", False)

In [28]: spark.sql(""select 1 v, 1 v"").toPandas()
Out[28]: 
   v  v
0  1  1

In [29]: spark.sql(""select struct(1 v, 1 v)"").toPandas()
Out[29]: 
  struct(1 AS v, 1 AS v)
0                 (1, 1)

In [30]: spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", True)

In [31]: spark.sql(""select 1 v, 1 v"").toPandas()
Out[31]: 
   v  v
0  1  1

In [32]: spark.sql(""select struct(1 v, 1 v)"").toPandas()
/Users/ruifeng.zheng/Dev/spark/python/pyspark/sql/pandas/conversion.py:204: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an effect on failures in the middle of computation.
  Ran out of field metadata, likely malformed
  warn(msg)
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
Cell In[32], line 1
----> 1 spark.sql(""select struct(1 v, 1 v)"").toPandas()

File ~/Dev/spark/python/pyspark/sql/pandas/conversion.py:143, in PandasConversionMixin.toPandas(self)
    141 tmp_column_names = [""col_{}"".format(i) for i in range(len(self.columns))]
    142 self_destruct = jconf.arrowPySparkSelfDestructEnabled()
--> 143 batches = self.toDF(*tmp_column_names)._collect_as_arrow(
    144     split_batches=self_destruct
    145 )
    146 if len(batches) > 0:
    147     table = pyarrow.Table.from_batches(batches)

File ~/Dev/spark/python/pyspark/sql/pandas/conversion.py:358, in PandasConversionMixin._collect_as_arrow(self, split_batches)
    356             results.append(batch_or_indices)
    357     else:
--> 358         results = list(batch_stream)
    359 finally:
    360     # Join serving thread and raise any exceptions from collectAsArrowToPython
    361     jsocket_auth_server.getResult()

File ~/Dev/spark/python/pyspark/sql/pandas/serializers.py:55, in ArrowCollectSerializer.load_stream(self, stream)
     50 """"""
     51 Load a stream of un-ordered Arrow RecordBatches, where the last iteration yields
     52 a list of indices that can be used to put the RecordBatches in the correct order.
     53 """"""
     54 # load the batches
---> 55 for batch in self.serializer.load_stream(stream):
     56     yield batch
     58 # load the batch order indices or propagate any error that occurred in the JVM

File ~/Dev/spark/python/pyspark/sql/pandas/serializers.py:98, in ArrowStreamSerializer.load_stream(self, stream)
     95 import pyarrow as pa
     97 reader = pa.ipc.open_stream(stream)
---> 98 for batch in reader:
     99     yield batch

File ~/.dev/miniconda3/envs/spark_dev/lib/python3.9/site-packages/pyarrow/ipc.pxi:638, in __iter__()

File ~/.dev/miniconda3/envs/spark_dev/lib/python3.9/site-packages/pyarrow/ipc.pxi:674, in pyarrow.lib.RecordBatchReader.read_next_batch()

File ~/.dev/miniconda3/envs/spark_dev/lib/python3.9/site-packages/pyarrow/error.pxi:100, in pyarrow.lib.check_status()

ArrowInvalid: Ran out of field metadata, likely malformed

{code}
",ueshin,podongfeng,Minor,Resolved,Fixed,11/Jan/23 03:12,04/May/23 18:07
Bug,SPARK-41982,13517574,"When the inserted partition type is of string type, similar `dt=01` will be converted to `dt=1`","At present, during the process of upgrading Spark2.4 to Spark3.2, we carefully read the migration documentwe and found a kind of situation not involved:
{code:java}
create table if not exists test_90(a string, b string) partitioned by (dt string);
desc formatted test_90;
// case1
insert into table test_90 partition (dt=05) values(""1"",""2"");
// case2
insert into table test_90 partition (dt='05') values(""1"",""2"");
drop table test_90;{code}
in spark2.4.3, it will generate such a path:
{code:java}
// the path
hdfs://test5/user/hive/db1/test_90/dt=05 

//result
spark-sql> select * from test_90;
1       2       05
1       2       05
Time taken: 1.316 seconds, Fetched 2 row(s)

spark-sql> show partitions test_90; 
dt=05 
Time taken: 0.201 seconds, Fetched 1 row(s)

spark-sql> select * from test_90 where dt='05';
1       2       05
1       2       05
Time taken: 0.212 seconds, Fetched 2 row(s)

spark-sql> explain insert into table test_90 partition (dt=05) values(""1"",""2"");
== Physical Plan ==
Execute InsertIntoHiveTable InsertIntoHiveTable `db1`.`test_90`, org.apache.hadoop.hive.ql.io.orc.OrcSerde, Map(dt -> Some(05)), false, false, [a, b]
+- LocalTableScan [a#116, b#117]
Time taken: 1.145 seconds, Fetched 1 row(s){code}
in spark3.2.0, it will generate two path:
{code:java}
// the path
hdfs://test5/user/hive/db1/test_90/dt=05 
hdfs://test5/user/hive/db1/test_90/dt=5 

// result
spark-sql> select * from test_90;
1       2       05
1       2       5
Time taken: 2.119 seconds, Fetched 2 row(s)

spark-sql> show partitions test_90;
dt=05
dt=5
Time taken: 0.161 seconds, Fetched 2 row(s)

spark-sql> select * from test_90 where dt='05';
1       2       05
Time taken: 0.252 seconds, Fetched 1 row(s)

spark-sql> explain insert into table test_90 partition (dt=05) values(""1"",""2"");
plan
== Physical Plan ==
Execute InsertIntoHiveTable `db1`.`test_90`, org.apache.hadoop.hive.ql.io.orc.OrcSerde, [dt=Some(5)], false, false, [a, b]
+- LocalTableScan [a#109, b#110]{code}
This will cause problems in reading data after the user switches to spark3. The root cause is that in the process of partition field resolution, Spark3 has a process of strongly converting this string type, which will cause partition `05` to lose the previous `0`

So I think we have two solutions:

one is to record the risk clearly in the migration document, and the other is to repair this case, because we internally keep the partition of string type as string type, regardless of whether single or double quotation marks are added.

 

 ",zhongjingxiong,zhongjingxiong,Critical,Resolved,Fixed,11/Jan/23 09:12,17/Jan/23 04:47
Bug,SPARK-41985,13517600,Centralize more column resolution rules,,cloud_fan,cloud_fan,Major,Resolved,Fixed,11/Jan/23 11:31,03/Feb/23 05:07
Bug,SPARK-41989,13517649,PYARROW_IGNORE_TIMEZONE warning can break application logging setup,"in {code}python/pyspark/pandas/__init__.py{code}  there is currently a warning when {{PYARROW_IGNORE_TIMEZONE}} env var is not set (https://github.com/apache/spark/blob/187c4a9c66758e973633c5c309b551b1d9094e6e/python/pyspark/pandas/__init__.py#L44-L59):

{code:python}
    import logging

    logging.warning(
        ""'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to ""...
{code}

The {{logging.warning()}} call  will silently do a {{logging.basicConfig()}} call (at least in python 3.9, which I tried).
(FYI: Something like {{logging.getLogger(...).warning()}} would not do this silent call)


This has the following very hard to figure out side-effect:
importing `pyspark.pandas` (directly or indirectly somewhere)  might break your logging setup (if PYARROW_IGNORE_TIMEZONE is not set).

Very basic  example (assuming PYARROW_IGNORE_TIMEZONE is not set):

{code:python}
import logging
import pyspark.pandas

logging.basicConfig(level=logging.DEBUG)

logger = logging.getLogger(""test"")
logger.warning(""I warn you"")
logger.debug(""I debug you"")
{code}

Will only produce the warning, not the debug line.
By removing the {{import pyspark.pandas}}, the debug line is produced",soxofaan,soxofaan,Major,Resolved,Fixed,11/Jan/23 15:53,12/Jan/23 09:26
Bug,SPARK-41990,13517668,Filtering by composite field name like `field name` doesn't work with pushDownPredicate = true,"Suppose we have some table in postgresql with field `Last Name` The following code results in error

Dataset<Row> dataset = sparkSession.read()
.format(""jdbc"")
.option(""url"", myUrl)
.option(""dbtable"", ""myTable"")
.option(""user"", ""myUser"")
.option(""password"", ""muPassword"")
.load();

dataset.where(""`Last Name`='Tessel'"").show();    //error

 

 

Exception in thread ""main"" org.apache.spark.sql.catalyst.parser.ParseException: 
Syntax error at or near 'Name': extra input 'Name'(line 1, pos 5)

== SQL ==
Last Name
-----^^^

    at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseMultipartIdentifier(ParseDriver.scala:67)
    at org.apache.spark.sql.connector.expressions.LogicalExpressions$.parseReference(expressions.scala:40)
    at org.apache.spark.sql.connector.expressions.FieldReference$.apply(expressions.scala:368)
    at org.apache.spark.sql.sources.IsNotNull.toV2(filters.scala:262)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.$anonfun$unhandledFilters$1(JDBCRelation.scala:278)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.$anonfun$unhandledFilters$1$adapted(JDBCRelation.scala:278)

 

But if we set pushDownPredicate to false everything works fine.",huaxingao,mkrasilnikova,Major,Resolved,Fixed,11/Jan/23 19:08,13/Feb/23 06:46
Bug,SPARK-41991,13517671,Interpreted mode subexpression elimination can throw exception during insert,"Example:
{noformat}
drop table if exists tbl1;
create table tbl1 (a int, b int) using parquet;

set spark.sql.codegen.wholeStage=false;
set spark.sql.codegen.factoryMode=NO_CODEGEN;

insert into tbl1
select id as a, id as b
from range(1, 5);
{noformat}
This results in the following exception:
{noformat}
java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.ExpressionProxy cannot be cast to org.apache.spark.sql.catalyst.expressions.Cast
	at org.apache.spark.sql.catalyst.expressions.CheckOverflowInTableInsert.withNewChildInternal(Cast.scala:2514)
	at org.apache.spark.sql.catalyst.expressions.CheckOverflowInTableInsert.withNewChildInternal(Cast.scala:2512)
{noformat}
The query produces 2 bigint values, but the table's schema expects 2 int values, so Spark wraps each output field with a {{Cast}}.

Later, in {{InterpretedUnsafeProjection}}, {{prepareExpressions}} tries to wrap the two {{Cast}} expressions with an {{ExpressionProxy}}. However, the parent expression of each {{Cast}} is a {{CheckOverflowInTableInsert}} expression, which does not accept {{ExpressionProxy}} as a child.

",bersprockets,bersprockets,Major,Resolved,Fixed,11/Jan/23 19:16,23/Feb/23 19:13
Bug,SPARK-42034,13517904,"QueryExecutionListener and Observation API, df.observe do not work with `foreach` action.","Observation API, {{observe}} dataframe transformation, and custom QueryExecutionListener.
Do not work with {{foreach}} or {{foreachPartition actions.}}
{{This is due to }}QueryExecutionListener functions do not trigger on queries whose action is {{foreach}} or {{{}foreachPartition{}}}.
But the Spark GUI SQL tab sees this query as SQL query and shows its query plans and etc.


here is the code to reproduce it:
https://gist.github.com/GrigorievNick/e7cf9ec5584b417d9719e2812722e6d3",Zing,hryhoriev.nick,Major,Resolved,Fixed,12/Jan/23 17:28,13/Feb/23 05:10
Bug,SPARK-42046,13518038,Add `connect-client-jvm` to connect module,,dongjoon,dongjoon,Minor,Resolved,Fixed,13/Jan/23 06:12,27/Jan/23 03:52
Bug,SPARK-42057,13518357,Avoid losing exception info in Protobuf errors,"Protobuf connector related error handlers incorrectly report the exception. This is makes it hard for users to see actual issue. E.g. if there is a {{FileNotFoundException}} these error handlers use pass {{exception.getCause()}} rather than passing {{{}exception{}}}. As result, we lose the information that it was a {{FileNotFoundException}}",rangadi,rangadi,Major,Resolved,Fixed,13/Jan/23 18:03,14/Jan/23 04:30
Bug,SPARK-42059,13518458,Update ORC to 1.8.2,,william,william,Major,Resolved,Fixed,14/Jan/23 00:02,14/Jan/23 02:57
Bug,SPARK-42061,13518549,Mark Expressions that have state has stateful,,,cloud_fan,Major,Resolved,Fixed,14/Jan/23 03:03,18/Jan/23 01:54
Bug,SPARK-42066,13519167,The DATATYPE_MISMATCH error class contains inappropriate and duplicating subclasses,"subclass WRONG_NUM_ARGS (with suggestions) semantically does not belong into DATATYPE_MISMATCH and there is an error class with that same name.
We should rea the subclasses for this errorclass, which seems to have become a bit of a dumping ground...",itholic,srielau,Major,Resolved,Fixed,14/Jan/23 21:56,30/Jan/23 10:56
Bug,SPARK-42084,13519694,Avoid leaking the qualified-access-only restriction,,cloud_fan,cloud_fan,Major,Resolved,Fixed,16/Jan/23 05:38,18/Jan/23 10:59
Bug,SPARK-42088,13519711,Running python3 setup.py sdist on windows reports a permission error,"My system version is windows 10, and I can run setup.py with administrator permissions, so there will be no error. However, it may be troublesome for us to upgrade permissions with Windows Server, so we need to modify the code of setup.py to ensure no error. To avoid the hassle of compiling for the user, I suggest modifying the following code to enable the out-of-the-box effect
{code:python}
def _supports_symlinks():
    """"""Check if the system supports symlinks (e.g. *nix) or not.""""""
    return getattr(os, ""symlink"", None) is not None and ctypes.windll.shell32.IsUserAnAdmin() != 0 if sys.platform == ""win32"" else True
{code}",zheju_he,zheju_he,Minor,Resolved,Fixed,16/Jan/23 08:02,17/Jan/23 00:38
Bug,SPARK-42090,13519791,Introduce sasl retry count in RetryingBlockTransferor,"Previously a boolean variable, saslTimeoutSeen, was used in RetryingBlockTransferor. However, the boolean variable wouldn't cover the following scenario:

1. SaslTimeoutException
2. IOException
3. SaslTimeoutException
4. IOException

Even though IOException at #2 is retried (resulting in increment of retryCount), the retryCount would be cleared at step #4.
Since the intention of saslTimeoutSeen is to undo the increment due to retrying SaslTimeoutException, we should keep a counter for SaslTimeoutException retries and subtract the value of this counter from retryCount.",yuzhihong@gmail.com,yuzhihong@gmail.com,Major,Resolved,Fixed,16/Jan/23 14:00,24/Jan/23 18:24
Bug,SPARK-42094,13519830,Support `fill_value` for `ps.Series.add`,For pandas function parity: https://pandas.pydata.org/docs/reference/api/pandas.Series.add.html,itholic,itholic,Major,Resolved,Fixed,17/Jan/23 01:43,08/Feb/23 00:17
Bug,SPARK-42109,13520107,Upgrade Kafka to 3.3.2,,dongjoon,dongjoon,Major,Resolved,Fixed,18/Jan/23 20:06,18/Jan/23 22:38
Bug,SPARK-42112,13520153,Add null check before `ContinuousWriteRDD#compute` method close dataWriter,"Run 

 

mvn clean test -pl sql/core -Dtest=none -DwildcardSuites=org.apache.spark.sql.streaming.continuous.ContinuousSuite -am

 

NPE exists in the test log

 
{code:java}
- repeatedly restart
...
16:07:39.891 ERROR org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD: Writer for partition 1 is aborting.
16:07:39.891 ERROR org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD: Writer for partition 1 aborted.
16:07:39.892 WARN org.apache.spark.util.Utils: Suppressing exception in finally: null
java.lang.NullPointerException
    at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD.$anonfun$compute$7(ContinuousWriteRDD.scala:91)
    at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1558)
    at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD.compute(ContinuousWriteRDD.scala:91)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
    at org.apache.spark.scheduler.Task.run(Task.scala:139)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1502)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750) {code}
 

 

The test did not fail because Utils.tryWithSafeFinallyAndFailureCallbacks function suppressed Exception in finally block

 ",LuciferYang,LuciferYang,Minor,Resolved,Fixed,19/Jan/23 08:17,20/Jan/23 03:24
Bug,SPARK-42113,13520157,Upgrade pandas to 1.5.3,To support latest pandas.,itholic,itholic,Major,Resolved,Fixed,19/Jan/23 09:21,19/Jan/23 16:54
Bug,SPARK-42115,13520190,Push down limit through Python UDFs,"{code}
from pyspark.sql.functions import udf

spark.range(10).write.mode(""overwrite"").parquet(""/tmp/abc"")

@udf(returnType='string')
def my_udf(arg):
    return arg


df = spark.read.parquet(""/tmp/abc"")
df.limit(10).withColumn(""prediction"", my_udf(df[""id""])).explain()
{code}

As an example. since Python UDFs are executed asynchronously, so pushing limits benefit the performance.

{code}
== Physical Plan ==
CollectLimit 10
+- *(2) Project [id#3L, pythonUDF0#10 AS prediction#6]
   +- BatchEvalPython [my_udf(id#3L)#5], [pythonUDF0#10]
      +- *(1) ColumnarToRow
         +- FileScan parquet [id#3L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/abc], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>
{code}

This is a regression from Spark 3.3.1:

{code}
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [id#3L, pythonUDF0#10 AS prediction#6]
   +- BatchEvalPython [my_udf(id#3L)#5], [pythonUDF0#10]
      +- GlobalLimit 10
         +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=30]
            +- LocalLimit 10
               +- FileScan parquet [id#3L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/abc], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>
{code}",gurwls223,gurwls223,Major,Resolved,Fixed,19/Jan/23 11:52,02/Feb/23 01:01
Bug,SPARK-42134,13520766,Fix getPartitionFiltersAndDataFilters() to handle filters without referenced attributes,,petertoth,petertoth,Major,Resolved,Fixed,20/Jan/23 13:46,21/Jan/23 02:38
Bug,SPARK-42156,13520940,Support client-side retries in Spark Connect Python client,,grundprinzip-db,grundprinzip-db,Major,Resolved,Fixed,22/Jan/23 17:57,31/Jan/23 07:05
Bug,SPARK-42157,13520943,`spark.scheduler.mode=FAIR` should provide FAIR scheduler, !Screenshot 2023-01-22 at 2.39.34 PM.png! ,dongjoon,dongjoon,Major,Resolved,Fixed,22/Jan/23 22:39,24/Jan/23 07:48
Bug,SPARK-42162,13521035,Memory usage on executors increased drastically for a complex query with large number of addition operations,"With the [recent changes|https://github.com/apache/spark/pull/37851]  in the expression canonicalization, a complex query with a large number of Add operations ends up consuming 10x more memory on the executors.

The reason for this issue is that with the new changes the canonicalization process ends up generating lot of intermediate objects, especially for complex queries with a large number of commutative operators. In this specific case, a heap histogram analysis shows that a large number of Add objects use the extra memory.
This issue does not happen before PR [#37851.|https://github.com/apache/spark/pull/37851]

The high memory usage causes the executors to lose heartbeat signals and results in task failures.",scnakandala,scnakandala,Major,Resolved,Fixed,23/Jan/23 20:43,10/Feb/23 15:57
Bug,SPARK-42163,13521048,Schema pruning fails on non-foldable array index or map key,"Schema pruning tries to extract selected fields from struct extractors. It looks through GetArrayItem/GetMapItem, but when doing so, it ignores the index/key, which may itself be a struct field. If it is a struct field that is not otherwise selected, and some other field of the same attribute is selected, then pruning will drop the field, resulting in an optimizer error.",David Cashman,David Cashman,Major,Resolved,Fixed,23/Jan/23 23:08,31/Jan/23 02:17
Bug,SPARK-42168,13521114,CoGroup with window function returns incorrect result when partition keys differ in order,"The following example returns an incorrect result:
{code:java}
import pandas as pd

from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import col, lit, sum

spark = SparkSession \
    .builder \
    .getOrCreate()

ids = 1000
days = 1000
parts = 10

id_df = spark.range(ids)
day_df = spark.range(days).withColumnRenamed(""id"", ""day"")
id_day_df = id_df.join(day_df)
left_df = id_day_df.select(col(""id"").alias(""id""), col(""day"").alias(""day""), lit(""left"").alias(""side"")).repartition(parts).cache()
right_df = id_day_df.select(col(""id"").alias(""id""), col(""day"").alias(""day""), lit(""right"").alias(""side"")).repartition(parts).cache()  #.withColumnRenamed(""id"", ""id2"")

# note the column order is different to the groupBy(""id"", ""day"") column order below
window = Window.partitionBy(""day"", ""id"")

left_grouped_df = left_df.groupBy(""id"", ""day"")
right_grouped_df = right_df.withColumn(""day_sum"", sum(col(""day"")).over(window)).groupBy(""id"", ""day"")

def cogroup(left: pd.DataFrame, right: pd.DataFrame) -> pd.DataFrame:
    return pd.DataFrame([{
        ""id"": left[""id""][0] if not left.empty else (right[""id""][0] if not right.empty else None),
        ""day"": left[""day""][0] if not left.empty else (right[""day""][0] if not right.empty else None),
        ""lefts"": len(left.index),
        ""rights"": len(right.index)
    }])

df = left_grouped_df.cogroup(right_grouped_df) \
         .applyInPandas(cogroup, schema=""id long, day long, lefts integer, rights integer"")

df.explain()
df.show(5)
{code}
Output is
{code}
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- FlatMapCoGroupsInPandas [id#8L, day#9L], [id#29L, day#30L], cogroup(id#8L, day#9L, side#10, id#29L, day#30L, side#31, day_sum#54L), [id#64L, day#65L, lefts#66, rights#67]
   :- Sort [id#8L ASC NULLS FIRST, day#9L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(id#8L, day#9L, 200), ENSURE_REQUIREMENTS, [plan_id=117]
   :     +- ...
   +- Sort [id#29L ASC NULLS FIRST, day#30L ASC NULLS FIRST], false, 0
      +- Project [id#29L, day#30L, id#29L, day#30L, side#31, day_sum#54L]
         +- Window [sum(day#30L) windowspecdefinition(day#30L, id#29L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS day_sum#54L], [day#30L, id#29L]
            +- Sort [day#30L ASC NULLS FIRST, id#29L ASC NULLS FIRST], false, 0
               +- Exchange hashpartitioning(day#30L, id#29L, 200), ENSURE_REQUIREMENTS, [plan_id=112]
                  +- ...


+---+---+-----+------+
| id|day|lefts|rights|
+---+---+-----+------+
|  0|  3|    0|     1|
|  0|  4|    0|     1|
|  0| 13|    1|     0|
|  0| 27|    0|     1|
|  0| 31|    0|     1|
+---+---+-----+------+
only showing top 5 rows
{code}
The first child is hash-partitioned by {{id}} and {{{}day{}}}, while the second child is hash-partitioned by {{day}} and {{id}} (required by the window function). Therefore, rows end up in different partitions.

This has been fixed in Spark 3.3 by [#32875|https://github.com/apache/spark/pull/32875/files#diff-e938569a4ca4eba8f7e10fe473d4f9c306ea253df151405bcaba880a601f075fR75-R76]:
{code}
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- FlatMapCoGroupsInPandas [id#8L, day#9L], [id#29L, day#30L], cogroup(id#8L, day#9L, side#10, id#29L, day#30L, side#31, day_sum#54L)#63, [id#64L, day#65L, lefts#66, rights#67]
   :- Sort [id#8L ASC NULLS FIRST, day#9L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(id#8L, day#9L, 200), ENSURE_REQUIREMENTS, [plan_id=117]
   :     +- ...
   +- Sort [id#29L ASC NULLS FIRST, day#30L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(id#29L, day#30L, 200), ENSURE_REQUIREMENTS, [plan_id=118]
         +- Project [id#29L, day#30L, id#29L, day#30L, side#31, day_sum#54L]
            +- Window [sum(day#30L) windowspecdefinition(day#30L, id#29L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS day_sum#54L], [day#30L, id#29L]
               +- Sort [day#30L ASC NULLS FIRST, id#29L ASC NULLS FIRST], false, 0
                  +- Exchange hashpartitioning(day#30L, id#29L, 200), ENSURE_REQUIREMENTS, [plan_id=112]
                     +- ...

+---+---+-----+------+
| id|day|lefts|rights|
+---+---+-----+------+
|  0| 13|    1|     1|
|  0| 63|    1|     1|
|  0| 89|    1|     1|
|  0| 95|    1|     1|
|  0| 96|    1|     1|
+---+---+-----+------+
only showing top 5 rows
{code}

Only PySpark is to be affected ({{FlatMapCoGroupsInPandas }}), as Scala API uses {{CoGroup}}. {{FlatMapCoGroupsInPandas}} reports required child distribution {{ClusteredDistribution}}, while {{CoGroup}} reports {{HashClusteredDistribution}}. The {{EnsureRequirements}} rule correctly recognizes a {{HashClusteredDistribution(id, day)}} as not compatible with {{hashpartitioning(day, id)}}, while {{ClusteredDistribution(id, day)}} is compatible with {{hashpartitioning(day, id)}}.",EnricoMi,EnricoMi,Major,Resolved,Fixed,24/Jan/23 12:12,30/Jan/23 10:55
Bug,SPARK-42171,13521153,Fix `pyspark-errors` module and enable it in GitHub Action,,dongjoon,dongjoon,Blocker,Resolved,Fixed,24/Jan/23 18:15,24/Jan/23 21:08
Bug,SPARK-42174,13521185,Use scikit-learn instead of sklearn,,dongjoon,dongjoon,Major,Resolved,Fixed,24/Jan/23 22:49,25/Jan/23 00:29
Bug,SPARK-42176,13521193,Cast boolean to timestamp fails with ClassCastException,"When casting a boolean value to timestamp, the following error is thrown:
{code:java}
[info]   java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long
[info]   at scala.runtime.BoxesRunTime.unboxToLong(BoxesRunTime.java:107)
[info]   at org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getWriter$5(InternalRow.scala:178)
[info]   at org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getWriter$5$adapted(InternalRow.scala:178) {code}",ivan.sadikov,ivan.sadikov,Major,Resolved,Fixed,25/Jan/23 00:49,25/Jan/23 03:34
Bug,SPARK-42177,13521201,Change master to brach-3.4 in GitHub Actions,See https://github.com/apache/spark/actions/runs/4002380215/jobs/6869886029,gurwls223,gurwls223,Major,Resolved,Fixed,25/Jan/23 03:21,25/Jan/23 03:38
Bug,SPARK-42179,13521224,Upgrade ORC to 1.7.8,,dongjoon,dongjoon,Major,Resolved,Fixed,25/Jan/23 07:31,25/Jan/23 11:24
Bug,SPARK-42186,13521442,Make SparkR able to stop properly when the connection is timed-out,"{code}
./bin/sparkR --conf spark.r.backendConnectionTimeout=10
{code}

wait 10 secs.

{code}

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.4.0-SNAPSHOT
      /_/


SparkSession Web UI available at http://192.168.35.219:4040
SparkSession available as 'spark'(master = local[*], app id = local-1674649482367).
> 23/01/25 21:24:53 WARN RBackendHandler: Ignoring read timeout in RBackendHandler
spark.session()
Error in spark.session() : could not find function ""spark.session""
> spark.session.stop()
Error in spark.session.stop() :
  could not find function ""spark.session.stop""
{code}",gurwls223,gurwls223,Major,Resolved,Fixed,25/Jan/23 12:25,25/Jan/23 17:13
Bug,SPARK-42188,13521497,Force SBT protobuf version to match Maven on branch 3.2 and 3.3,"Update SparkBuild.scala to force SBT use of protobuf-java to match the Maven version.  The Maven dependencyManagement section forces protobuf-java to use 2.5.0, but SBT is using 3.14.0.

Snippet from Maven dependency tree

 
{noformat}
[INFO] +- com.google.crypto.tink:tink:jar:1.6.0:compile
[INFO] |  +- com.google.protobuf:protobuf-java:jar:2.5.0:compile    <--- 2.x
[INFO] |  \- com.google.code.gson:gson:jar:2.8.6:compile{noformat}
  Snippet from SBT dependency tree
{noformat}
[info]   +-com.google.crypto.tink:tink:1.6.0
[info]   | +-com.google.code.gson:gson:2.8.6
[info]   | +-com.google.protobuf:protobuf-java:3.14.0               <--- 3.x{noformat}
The fix is updating SparkBuild.scala just like SPARK-11538 did with guava.  In addition we should comment on the need to keep the top-level pom.xml and SparkBuild.scala in sync as was done in SPARK-41247

 ",svaughan,svaughan,Major,Resolved,Fixed,25/Jan/23 14:41,26/Jan/23 02:31
Bug,SPARK-42196,13521585,Typo in StreamingQuery.scala,"{color:#172b4d}/**{color}
{color:#172b4d} * Returns the unique id of this run of the query. That is, every start/restart of a query *will*{color}
{color:#172b4d} ** *generated* a unique runId. Therefore, every time a query is restarted from{color}
{color:#172b4d} * checkpoint, it will have the same [[id]] but different [[runId]]s.{color}
{color:#172b4d} */{color}
{color:#172b4d}def runId: UUID{color}",ganeshchand,ganeshchand,Major,Resolved,Fixed,26/Jan/23 05:40,30/Jan/23 05:43
Bug,SPARK-42201,13521661,`build/sbt` should allow SBT_OPTS to override JVM memory setting,,dongjoon,dongjoon,Major,Resolved,Fixed,26/Jan/23 20:16,27/Jan/23 00:39
Bug,SPARK-42214,13521765,Branch-3.4 daily test failed,https://github.com/apache/spark/actions/runs/4023012095/jobs/6913400923,yikunkero,LuciferYang,Major,Resolved,Fixed,27/Jan/23 14:27,30/Jan/23 12:26
Bug,SPARK-42228,13521912,connect-client-jvm module should shaded+relocation grpc,,LuciferYang,LuciferYang,Blocker,Resolved,Fixed,29/Jan/23 08:37,01/Feb/23 18:11
Bug,SPARK-42241,13522105, Correct the condition for `SparkConnectServerUtils#findSparkConnectJar` to find the correct connect server jar for maven,,LuciferYang,LuciferYang,Minor,Resolved,Fixed,30/Jan/23 17:52,31/Jan/23 00:12
Bug,SPARK-42242,13522132,Upgrade snappy-java to 1.1.9.1,,dongjoon,dongjoon,Major,Resolved,Fixed,30/Jan/23 23:28,01/Feb/23 06:14
Bug,SPARK-42250,13522150,predict_batch_udf with float fails when the batch size consists of single value,"{code}
import numpy as np
import pandas as pd
from pyspark.ml.functions import predict_batch_udf
from pyspark.sql.types import ArrayType, FloatType, StructType, StructField
from typing import Mapping

df = spark.createDataFrame([[[0.0, 1.0, 2.0, 3.0], [0.0, 1.0, 2.0]], [[4.0, 5.0, 6.0, 7.0], [4.0, 5.0, 6.0]]], schema=[""t1"", ""t2""])

def make_multi_sum_fn():
    def predict(x1: np.ndarray, x2: np.ndarray) -> np.ndarray:
        return np.sum(x1, axis=1) + np.sum(x2, axis=1)
    return predict

multi_sum_udf = predict_batch_udf(
    make_multi_sum_fn,
    return_type=FloatType(),
    batch_size=1,
    input_tensor_shapes=[[4], [3]],
)

df.select(multi_sum_udf(""t1"", ""t2"")).collect()
{code}

fails as below:

{code}
 File ""/.../spark/python/lib/pyspark.zip/pyspark/worker.py"", line 829, in main
    process()
  File ""/.../spark/python/lib/pyspark.zip/pyspark/worker.py"", line 821, in process
    serializer.dump_stream(out_iter, outfile)
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 345, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 86, in dump_stream
    for batch in iterator:
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 339, in init_stream_yield_batches
    batch = self._create_batch(series)
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 275, in _create_batch
    arrs.append(create_array(s, t))
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 245, in create_array
    raise e
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 233, in create_array
    array = pa.Array.from_pandas(s, mask=mask, type=t, safe=self._safecheck)
  File ""pyarrow/array.pxi"", line 1044, in pyarrow.lib.Array.from_pandas
  File ""pyarrow/array.pxi"", line 316, in pyarrow.lib.array
  File ""pyarrow/array.pxi"", line 83, in pyarrow.lib._ndarray_to_array
  File ""pyarrow/error.pxi"", line 100, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Could not convert array(569.) with type numpy.ndarray: tried to convert to float32

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:554)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:507)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:391)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1520)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

{code}",gurwls223,gurwls223,Major,Resolved,Fixed,31/Jan/23 03:31,31/Jan/23 10:42
Bug,SPARK-42259,13522278,ResolveGroupingAnalytics should take care of Python UDAF,,cloud_fan,cloud_fan,Major,Resolved,Fixed,31/Jan/23 17:49,01/Feb/23 09:42
Bug,SPARK-42274,13522421,Upgrade `compress-lzf` to 1.1.2,,dongjoon,dongjoon,Minor,Resolved,Fixed,01/Feb/23 07:56,01/Feb/23 10:12
Bug,SPARK-42276,13522433,Add ServicesResourceTransformer to connect server module  shade configuration,The contents of META-INF/services directory in the shaded connect-server jar have not been relocated.,LuciferYang,LuciferYang,Minor,Resolved,Fixed,01/Feb/23 08:38,10/Feb/23 02:58
Bug,SPARK-42286,13522606,Fix internal error for valid CASE WHEN expression with CAST when inserting into a table,"```

spark-sql> create or replace table es570639t1 as select x FROM values (1), (2), (3) as tab(x);
spark-sql> create or replace table es570639t2 (x Decimal(9, 0));
spark-sql> insert into es570639t2 select 0 - (case when x = 1 then 1 else x end) from es570639t1 where x = 1;

```

hits the following internal error
org.apache.spark.SparkException: [INTERNAL_ERROR] Child is not Cast or ExpressionProxy of Cast
 

Stack trace:
org.apache.spark.SparkException: [INTERNAL_ERROR] Child is not Cast or ExpressionProxy of Cast at org.apache.spark.SparkException$.internalError(SparkException.scala:78) at org.apache.spark.SparkException$.internalError(SparkException.scala:82) at org.apache.spark.sql.catalyst.expressions.CheckOverflowInTableInsert.checkChild(Cast.scala:2693) at org.apache.spark.sql.catalyst.expressions.CheckOverflowInTableInsert.withNewChildInternal(Cast.scala:2697) at org.apache.spark.sql.catalyst.expressions.CheckOverflowInTableInsert.withNewChildInternal(Cast.scala:2683) at org.apache.spark.sql.catalyst.trees.UnaryLike.$anonfun$mapChildren$5(TreeNode.scala:1315) at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106) at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1314) at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1309) at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:636) at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:570) at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:570)
 

This internal error comes from `CheckOverflowInTableInsert``checkChild`, where we covered only `Cast` expr and `ExpressionProxy` expr, but not the `CaseWhen` expr.",runyao,runyao,Major,Resolved,Fixed,01/Feb/23 23:00,24/Feb/23 18:58
Bug,SPARK-42290,13522751,Spark Driver hangs on OOM during Broadcast when AQE is enabled ,"Repro steps:
{code}
$ spark-shell --conf spark.driver.memory=1g

val df = spark.range(5000000).withColumn(""str"", lit(""abcdabcdabcdabcdabasgasdfsadfasdfasdfasfasfsadfasdfsadfasdf""))
val df2 = spark.range(10).join(broadcast(df), Seq(""id""), ""left_outer"")

df2.collect
{code}

This will cause the driver to hang indefinitely. Heres a thread dump of the {{main}} thread when its stuck
{code}
sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:285)
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2819/629294880.apply(Unknown Source)
org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:809)
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:236) => holding Monitor(java.lang.Object@1932537396})
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:381)
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)
org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4179)
org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3420)
org.apache.spark.sql.Dataset$$Lambda$2390/1803372144.apply(Unknown Source)
org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4169)
org.apache.spark.sql.Dataset$$Lambda$2791/1357377136.apply(Unknown Source)
org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4167)
org.apache.spark.sql.Dataset$$Lambda$2391/1172042998.apply(Unknown Source)
org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
org.apache.spark.sql.execution.SQLExecution$$$Lambda$2402/721269425.apply(Unknown Source)
org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
org.apache.spark.sql.execution.SQLExecution$$$Lambda$2392/11632488.apply(Unknown Source)
org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:809)
org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
org.apache.spark.sql.Dataset.withAction(Dataset.scala:4167)
org.apache.spark.sql.Dataset.collect(Dataset.scala:3420)
{code}


When we disable AQE though we get the following exception instead of driver hang.
{code}
Caused by: org.apache.spark.SparkException: Not enough memory to build and broadcast the table to all worker nodes. As a workaround, you can either disable broadcast by setting spark.sql.autoBroadcastJoinThreshold to -1 or increase the spark driver memory by setting spark.driver.memory to a higher value.
  ... 7 more
Caused by: java.lang.OutOfMemoryError: Java heap space
  at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.grow(HashedRelation.scala:834)
  at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.append(HashedRelation.scala:777)
  at org.apache.spark.sql.execution.joins.LongHashedRelation$.apply(HashedRelation.scala:1086)
  at org.apache.spark.sql.execution.joins.HashedRelation$.apply(HashedRelation.scala:157)
  at org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:1163)
  at org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:1151)
  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:148)
  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$Lambda$2999/145945436.apply(Unknown Source)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:217)
  at org.apache.spark.sql.execution.SQLExecution$$$Lambda$3001/1900142693.call(Unknown Source)
  ... 4 more
{code}
I expect to see the same exception even when AQE is enabled. ",fanjia,shardulm,Critical,Resolved,Fixed,02/Feb/23 09:45,11/Jun/23 11:25
Bug,SPARK-42292,13522776,Spark SQL not use hive partition info,"I use spark3 to count partition num , like : 

table a is external parquet table, it have 3 partition columns (year ,month, day).

query sql : ""select distinct month , day from a where year = '2022' ""

i think spark can find hive metadata and use partition info, but it load all  ""year = '2022'"" partition data.

in spark2.4, it use TableLocalScanExec ,but spark3 use HiveTableRelation and scan hive parquet.
 ",,xuanzhiang,Major,Resolved,Fixed,02/Feb/23 12:08,03/Feb/23 02:11
Bug,SPARK-42331,13522905,Fix metadata col can not been resolved,,ulysses,ulysses,Major,Resolved,Fixed,03/Feb/23 06:19,13/Feb/23 04:44
Bug,SPARK-42344,13523054,The default size of the CONFIG_MAP_MAXSIZE should not be greater than 1048576,"Exception in thread ""main"" io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://172.18.123.24:6443/api/v1/namespaces/default/configmaps. Message: ConfigMap ""spark-exec-ed9f2c861aa40b48-conf-map"" is invalid: []: Too long: must have at most 1048576 bytes. Received status: Status(apiVersion=v1, code=422, details=StatusDetails(causes=[StatusCause(field=[], message=Too long: must have at most 1048576 bytes, reason=FieldValueTooLong, additionalProperties={})], group=null, kind=ConfigMap, name=spark-exec-ed9f2c861aa40b48-conf-map, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=ConfigMap ""spark-exec-ed9f2c861aa40b48-conf-map"" is invalid: []: Too long: must have at most 1048576 bytes, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=Invalid, status=Failure, additionalProperties={}).
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:682)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:661)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:612)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:555)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:518)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:305)
        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:644)
        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:83)
        at io.fabric8.kubernetes.client.dsl.base.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:61)
        at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.setUpExecutorConfigMap(KubernetesClusterSchedulerBackend.scala:88)
        at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.start(KubernetesClusterSchedulerBackend.scala:112)
        at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:222)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:595)
        at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2714)
        at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:953)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
        at org.apache.spark.examples.JavaSparkPi.main(JavaSparkPi.java:37)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",ninebigbig,ninebigbig,Major,Resolved,Fixed,04/Feb/23 05:08,05/Feb/23 11:13
Bug,SPARK-42346,13523064,distinct(count colname) with UNION ALL causes query analyzer bug,"If you combine a UNION ALL with a count(distinct colname) you get a query analyzer bug.

 

This behaviour is introduced in 3.3.0.  The bug was not present in 3.2.1.

 

Here is a reprex in PySpark:

{{df_pd = pd.DataFrame([}}
{{    \{'surname': 'a', 'first_name': 'b'}}}
{{])}}
{{df_spark = spark.createDataFrame(df_pd)}}
{{df_spark.createOrReplaceTempView(""input_table"")}}

{{sql = """"""}}

{{SELECT }}
{{    (SELECT Count(DISTINCT first_name) FROM   input_table) }}
{{        AS distinct_value_count}}
{{FROM   input_table}}
{{UNION ALL}}
{{SELECT }}
{{    (SELECT Count(DISTINCT surname) FROM   input_table) }}
{{        AS distinct_value_count}}
{{FROM   input_table """"""}}

{{spark.sql(sql).toPandas()}}

 ",petertoth,RobinLinacre,Major,Resolved,Fixed,04/Feb/23 08:52,08/Feb/23 19:40
Bug,SPARK-42353,13523168,Cleanup orphan sst and log files in RocksDB checkpoint directory,"When RocksDB version.zip file get overwritten (e.g. concurrent task execution, task/stage/batch reattempts) or the zip file don't get uploaded successfully, the associated sst and log files don't get garbage collected.([https://github.com/databricks/runtime/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala|https://github.com/databricks/runtime/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala#L305-L309]) These files consume storage. We can clean up these SST files during periodic state store maintenance. The major concern is that sst files for ongoing version also appear to be ""orphan"" because they are uploaded before zip file, we have to be careful not to delete them.",Chaoqin,Chaoqin,Major,Resolved,Fixed,06/Feb/23 07:54,09/Feb/23 23:22
Bug,SPARK-42384,13523803,Mask function's generated code does not handle null input,"Example:
{noformat}
create or replace temp view v1 as
select * from values
(null),
('AbCD123-@$#')
as data(col1);

cache table v1;

select mask(col1) from v1;
{noformat}
This query results in a {{NullPointerException}}:
{noformat}
23/02/07 16:36:06 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
{noformat}
The generated code calls {{UnsafeWriter.write(0, value_0)}} regardless of whether {{Mask.transformInput}} returns null or not. The {{UnsafeWriter.write}} method for {{UTF8String}} does not expect a null pointer.
{noformat}
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     UTF8String value_1 = isNull_1 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */
/* 035 */
/* 036 */
/* 037 */
/* 038 */     UTF8String value_0 = null;
/* 039 */     value_0 = org.apache.spark.sql.catalyst.expressions.Mask.transformInput(value_1, ((UTF8String) references[0] /* literal */), ((UTF8String) references[1] /* literal */), ((UTF8String) references[2] /* literal */), ((UTF8String) references[3] /* literal */));;
/* 040 */     if (false) {
/* 041 */       mutableStateArray_0[0].setNullAt(0);
/* 042 */     } else {
/* 043 */       mutableStateArray_0[0].write(0, value_0);
/* 044 */     }
/* 045 */     return (mutableStateArray_0[0].getRow());
/* 046 */   }
{noformat}

The bug is not exercised by a literal null input value, since there appears to be some optimization that simply replaces the entire function call with a null literal:
{noformat}
spark-sql> explain SELECT mask(NULL);
== Physical Plan ==
*(1) Project [null AS mask(NULL, X, x, n, NULL)#47]
+- *(1) Scan OneRowRelation[]

Time taken: 0.026 seconds, Fetched 1 row(s)
spark-sql> SELECT mask(NULL);
NULL
Time taken: 0.042 seconds, Fetched 1 row(s)
spark-sql> 
{noformat}
",bersprockets,bersprockets,Major,Resolved,Fixed,08/Feb/23 16:46,16/Feb/23 00:26
Bug,SPARK-42401,13524302,Incorrect results or NPE when inserting null value into array using array_insert/array_append,"Example:
{noformat}
create or replace temp view v1 as
select * from values
(array(1, 2, 3, 4), 5, 5),
(array(1, 2, 3, 4), 5, null)
as v1(col1,col2,col3);

select array_insert(col1, col2, col3) from v1;
{noformat}
This produces an incorrect result:
{noformat}
[1,2,3,4,5]
[1,2,3,4,0] <== should be [1,2,3,4,null]
{noformat}
A more succint example:
{noformat}
select array_insert(array(1, 2, 3, 4), 5, cast(null as int));
{noformat}
This also produces an incorrect result:
{noformat}
[1,2,3,4,0] <== should be [1,2,3,4,null]
{noformat}
Another example:
{noformat}
create or replace temp view v1 as
select * from values
(array('1', '2', '3', '4'), 5, '5'),
(array('1', '2', '3', '4'), 5, null)
as v1(col1,col2,col3);

select array_insert(col1, col2, col3) from v1;
{noformat}
The above query throws a {{NullPointerException}}:
{noformat}
23/02/10 11:08:05 ERROR SparkSQLDriver: Failed in [select array_insert(col1, col2, col3) from v1]
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.LocalTableScanExec.$anonfun$unsafeRows$1(LocalTableScanExec.scala:44)
{noformat}
{{array_append}} has the same issue:
{noformat}
spark-sql> select array_append(array(1, 2, 3, 4), cast(null as int));
[1,2,3,4,0] <== should be [1,2,3,4,null]
Time taken: 3.679 seconds, Fetched 1 row(s)
spark-sql> select array_append(array('1', '2', '3', '4'), cast(null as string));
23/02/10 11:13:36 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
{noformat}
",bersprockets,bersprockets,Major,Resolved,Fixed,10/Feb/23 23:18,15/Feb/23 02:45
Bug,SPARK-42403,13524305,JsonProtocol should handle null JSON strings,"*Event Log*
{code}
{""Declaring Class"":""org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1"",""Method Name"":""columnartorow_nextBatch_0$"",""File Name"":null,""Line Number"":-1}
{code}

*Apache Spark 3.4*
{code}
3/02/10 16:54:46 ERROR ReplayListenerBus: Exception parsing Spark event log: file:/Users/dongjoon/data/history/eventlog_v2_spark-1676069204164-1qq70hioosynfzib9rmi77wbavnao-driver-job/events_1_spark-1676069204164-1qq70hioosynfzib9rmi77wbavnao-driver-job.zstd
java.lang.IllegalArgumentException: requirement failed: Expected string, got NULL
        at scala.Predef$.require(Predef.scala:281)
        at org.apache.spark.util.JsonProtocol$JsonNodeImplicits.extractString(JsonProtocol.scala:1614)
        at org.apache.spark.util.JsonProtocol$.$anonfun$stackTraceFromJson$1(JsonProtocol.scala:1561)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
        at scala.collection.Iterator.foreach(Iterator.scala:943)
        at scala.collection.Iterator.foreach$(Iterator.scala:943)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
        at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
        at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
        at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
        at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
        at scala.collection.AbstractIterator.to(Iterator.scala:1431)
        at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
        at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)
        at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
        at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1431)
        at org.apache.spark.util.JsonProtocol$.stackTraceFromJson(JsonProtocol.scala:1564)
        at org.apache.spark.util.JsonProtocol$.taskEndReasonFromJson(JsonProtocol.scala:1361)
        at org.apache.spark.util.JsonProtocol$.taskEndFromJson(JsonProtocol.scala:938)
        at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:876)
        at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:865)
        at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:88)
        at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:59)
        at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$3(FsHistoryProvider.scala:1140)
        at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$3$adapted(FsHistoryProvider.scala:1138)
        at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2777)
        at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$1(FsHistoryProvider.scala:1138)
        at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$1$adapted(FsHistoryProvider.scala:1136)
        at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
        at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
        at org.apache.spark.deploy.history.FsHistoryProvider.parseAppEventLogs(FsHistoryProvider.scala:1136)
        at org.apache.spark.deploy.history.FsHistoryProvider.rebuildAppStore(FsHistoryProvider.scala:1117)
{code}",joshrosen,dongjoon,Blocker,Resolved,Fixed,11/Feb/23 01:12,11/Feb/23 05:55
Bug,SPARK-42406,13524334,[PROTOBUF] Recursive field handling is incompatible with delta,"Protobuf deserializer (`from_protobuf()` function()) optionally supports recursive fields by limiting the depth to certain level. See example below. It assigns a 'NullType' for such a field when allowed depth is reached. 

It causes a few issues. E.g. a repeated field as in the following example results in a Array field with 'NullType'. Delta does not support null type in a complex type.

Actually `Array[NullType]` is not really useful anyway.

How about this fix: Drop the recursive field when the limit reached rather than using a NullType. 

The example below makes it clear:

Consider a recursive Protobuf:

 
{code:python}
message TreeNode {
  string value = 1;
  repeated TreeNode children = 2;
}
{code}
Allow depth of 2: 

 
{code:python}
   df.select(
    'proto',
     messageName = 'TreeNode',
     options = { ... ""recursive.fields.max.depth"" : ""2"" }
  ).printSchema()
{code}
Schema looks like this:
{noformat}
root
|– from_protobuf(proto): struct (nullable = true)|
| |– value: string (nullable = true)|
| |– children: array (nullable = false)|
| | |– element: struct (containsNull = false)|
| | | |– value: string (nullable = true)|
| | | |– children: array (nullable = false)|
| | | | |– element: struct (containsNull = false)|
| | | | | |– value: string (nullable = true)|
| | | | | |– children: array (nullable = false). [ === Proposed fix: Drop this field === ]|
| | | | | | |– element: void (containsNull = false) [ === NOTICE 'void' HERE === ] 
{noformat}
When we try to write this to a delta table, we get an error:
{noformat}
AnalysisException: Found nested NullType in column from_protobuf(proto).children which is of ArrayType. Delta doesn't support writing NullType in complex types.
{noformat}
 
We could just drop the field 'element' when recursion depth is reached. It is simpler and does not need to deal with NullType. We are ignoring the value anyway. There is no use in keeping the field.

Another issue is setting for 'recursive.fields.max.depth': It is not enforced correctly. '0' does not make sense. 

 ",rangadi,rangadi,Major,Resolved,Fixed,11/Feb/23 20:53,28/Feb/23 04:45
Bug,SPARK-42410,13524371,Support Scala 2.12/2.13 tests in connect module,"{code}
$ build/sbt -Dscala.version=2.13.8 -Pscala-2.13 -Phadoop-3 assembly/package ""connect/test""
{code}",dongjoon,dongjoon,Major,Resolved,Fixed,12/Feb/23 22:58,01/Mar/23 16:39
Bug,SPARK-42416,13524401,Dateset operations should not resolve the analyzed logical plan again,"For the following query

 
{code:java}
      sql(
        """"""
          |CREATE TABLE app_open (
          |  uid STRING,
          |  st TIMESTAMP,
          |  ds INT
          |) USING parquet PARTITIONED BY (ds);
          |"""""".stripMargin)

      sql(
        """"""
          |create or replace temporary view group_by_error as WITH new_app_open AS (
          |  SELECT
          |    ao.*
          |  FROM
          |    app_open ao
          |)
          |SELECT
          |    uid,
          |    20230208 AS ds
          |  FROM
          |    new_app_open
          |  GROUP BY
          |    1,
          |    2
          |"""""".stripMargin)

      sql(
        """"""
          |select
          |  `uid`
          |from
          |  group_by_error
          |"""""".stripMargin).show(){code}
Spark will throw the following error

 

 
{code:java}
[GROUP_BY_POS_OUT_OF_RANGE] GROUP BY position 20230208 is not in select list (valid range is [1, 2]).; line 9 pos 4 {code}
 

 

This is because the logical plan is not set as analyzed and it is analyzed again. The analyzer rules about aggregation/sort ordinals are not idempotent.",Gengliang.Wang,Gengliang.Wang,Major,Resolved,Fixed,13/Feb/23 05:36,13/Feb/23 18:58
Bug,SPARK-42444,13524752,DataFrame.drop should handle multi columns properly,"{code:java}
from pyspark.sql import Row
df1 = spark.createDataFrame([(14, ""Tom""), (23, ""Alice""), (16, ""Bob"")], [""age"", ""name""])
df2 = spark.createDataFrame([Row(height=80, name=""Tom""), Row(height=85, name=""Bob"")])
df1.join(df2, df1.name == df2.name, 'inner').drop('name', 'age').show()
{code}

This works in 3.3

{code:java}
+------+
|height|
+------+
|    85|
|    80|
+------+
{code}

but fails in 3.4


{code:java}
---------------------------------------------------------------------------
AnalysisException                         Traceback (most recent call last)
Cell In[1], line 4
      2 df1 = spark.createDataFrame([(14, ""Tom""), (23, ""Alice""), (16, ""Bob"")], [""age"", ""name""])
      3 df2 = spark.createDataFrame([Row(height=80, name=""Tom""), Row(height=85, name=""Bob"")])
----> 4 df1.join(df2, df1.name == df2.name, 'inner').drop('name', 'age').show()

File ~/Dev/spark/python/pyspark/sql/dataframe.py:4913, in DataFrame.drop(self, *cols)
   4911     jcols = [_to_java_column(c) for c in cols]
   4912     first_column, *remaining_columns = jcols
-> 4913     jdf = self._jdf.drop(first_column, self._jseq(remaining_columns))
   4915 return DataFrame(jdf, self.sparkSession)

File ~/Dev/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)
   1316 command = proto.CALL_COMMAND_NAME +\
   1317     self.command_header +\
   1318     args_command +\
   1319     proto.END_COMMAND_PART
   1321 answer = self.gateway_client.send_command(command)
-> 1322 return_value = get_return_value(
   1323     answer, self.gateway_client, self.target_id, self.name)
   1325 for temp_arg in temp_args:
   1326     if hasattr(temp_arg, ""_detach""):

File ~/Dev/spark/python/pyspark/errors/exceptions/captured.py:159, in capture_sql_exception.<locals>.deco(*a, **kw)
    155 converted = convert_exception(e.java_exception)
    156 if not isinstance(converted, UnknownException):
    157     # Hide where the exception came from that shows a non-Pythonic
    158     # JVM exception message.
--> 159     raise converted from None
    160 else:
    161     raise

AnalysisException: [AMBIGUOUS_REFERENCE] Reference `name` is ambiguous, could be: [`name`, `name`].

{code}

",podongfeng,podongfeng,Blocker,Resolved,Fixed,15/Feb/23 02:46,24/Feb/23 00:03
Bug,SPARK-42445,13524767,Fix SparkR install.spark function,"{code}
$ R

R version 4.2.1 (2022-06-23) -- ""Funny-Looking Kid""
Copyright (C) 2022 The R Foundation for Statistical Computing
Platform: aarch64-apple-darwin20 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(SparkR)

Attaching package: ‘SparkR’

The following objects are masked from ‘package:stats’:

    cov, filter, lag, na.omit, predict, sd, var, window

The following objects are masked from ‘package:base’:

    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,
    rank, rbind, sample, startsWith, subset, summary, transform, union

> install.spark()
Spark not found in the cache directory. Installation will start.
MirrorUrl not provided.
Looking for preferred site from apache website...
Preferred mirror site found: https://dlcdn.apache.org/spark
Downloading spark-3.3.2 for Hadoop 2.7 from:
- https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop2.7.tgz
trying URL 'https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop2.7.tgz'
simpleWarning in download.file(remotePath, localPath): downloaded length 0 != reported length 196
{code}",dongjoon,dongjoon,Major,Resolved,Fixed,15/Feb/23 05:50,15/Feb/23 17:32
Bug,SPARK-42448,13524813,spark sql shell prompts wrong database info,The current db info is from hive sessionState instead of spark sessionState,yao,yao,Minor,Resolved,Fixed,15/Feb/23 10:23,23/Feb/23 08:47
Bug,SPARK-42462,13524945,Prevent `docker-image-tool.sh` from publishing OCI manifests,https://github.com/docker/buildx/issues/1509,dongjoon,dongjoon,Blocker,Resolved,Fixed,16/Feb/23 04:41,16/Feb/23 05:53
Bug,SPARK-42473,13525148,An explicit cast will be needed when INSERT OVERWRITE SELECT UNION ALL,"*when 'union all' and one select statement use* *Literal as column value , the other* *select statement  has computed expression at the same column , then the whole statement will compile failed. A explicit cast will be needed.*

for example:

{color:#4c9aff}explain{color}

{color:#4c9aff}*INSERT* OVERWRITE *TABLE* test.spark33_decimal_orc{color}

{color:#4c9aff}*select* *null* *as* amt1, {*}cast{*}('256.99' *as* {*}decimal{*}(20,8)) *as* amt2{color}

{color:#4c9aff}*union* *all*{color}

{color:#4c9aff}*select* {*}cast{*}('200.99' *as* {*}decimal{*}(20,8)){*}/{*}100 *as* amt1,{*}cast{*}('256.99' *as* {*}decimal{*}(20,8)) *as* amt2;{color}

*will got error :* 

org.apache.spark.{*}sql{*}.catalyst.expressions.Literal cannot be *cast* *to* org.apache.spark.{*}sql{*}.catalyst.expressions.AnsiCast

The SQL will need to change to : 

{color:#4c9aff}explain{color}

{color:#4c9aff}*INSERT* OVERWRITE *TABLE* test.spark33_decimal_orc{color}

{color:#4c9aff}*select* *null* *as* amt1,{*}cast{*}('256.99' *as* {*}decimal{*}(20,8)) *as* amt2{color}

{color:#4c9aff}*union* *all*{color}

{color:#4c9aff}*select* {color:#de350b}{*}cast{*}({color}{*}cast{*}('200.99' *as* {*}decimal{*}(20,8)){*}/{*}100 *as* {*}decimal{*}(20,8){color:#de350b}){color} *as* amt1,{*}cast{*}('256.99' *as* {*}decimal{*}(20,8)) *as* amt2;{color}

 

*but this is not need in spark3.2.1 , is this a bug for spark3.3.1 ?* ",runyao,kevinshin,Major,Resolved,Fixed,17/Feb/23 06:27,03/Mar/23 06:58
Bug,SPARK-42478,13525197,Make a serializable jobTrackerId instead of a non-serializable JobID in FileWriterFactory,"https://issues.apache.org/jira/browse/SPARK-41448 make consistent MR job IDs in FileBatchWriter and FileFormatWriter, but it breaks a serializable issue, JobId is non-serializable",kaifeiYi,kaifeiYi,Major,Resolved,Fixed,17/Feb/23 11:53,06/Mar/23 22:08
Bug,SPARK-42515,13525616,ClientE2ETestSuite local test failed," 

local run `build/sbt clean ""connect-client-jvm/test""`, `ClientE2ETestSuite#write table` failed, GA not failed.

 
{code:java}
[info] - rite table *** FAILED *** (41 milliseconds)
[info]   io.grpc.StatusRuntimeException: UNKNOWN: org/apache/parquet/hadoop/api/ReadSupport
[info]   at io.grpc.Status.asRuntimeException(Status.java:535)
[info]   at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
[info]   at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)
[info]   at scala.collection.Iterator.foreach(Iterator.scala:943)
[info]   at scala.collection.Iterator.foreach$(Iterator.scala:943)
[info]   at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[info]   at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:169)
[info]   at org.apache.spark.sql.DataFrameWriter.executeWriteOperation(DataFrameWriter.scala:255)
[info]   at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:338)
[info]   at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$12(ClientE2ETestSuite.scala:145)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
[info]   at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
[info]   at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.sql.ClientE2ETestSuite.org$scalatest$BeforeAndAfterAll$$super$run(ClientE2ETestSuite.scala:33)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.sql.ClientE2ETestSuite.run(ClientE2ETestSuite.scala:33)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:750) {code}
 

 

local run ",LuciferYang,LuciferYang,Minor,Resolved,Fixed,21/Feb/23 16:39,28/Feb/23 03:58
Bug,SPARK-42516,13525621,Non-captured session time zone in view creation,"The session time zone config is captured only when it is set explicitly but if it is not the view is instantiated with the current settings. That's might confuse users since query results depends on explicit SQL config settings while creating a view.

The example below portraits the issue:
{code:java}
val viewName = ""v1_capture_test""
withView(viewName) {
  assert(get.sessionLocalTimeZone === ""America/Los_Angeles"")
  createView(viewName,
    """"""select hour(ts) as H from (
      |  select cast('2022-01-01T00:00:00.000 America/Los_Angeles' as timestamp) as ts
      |)"""""".stripMargin, Seq(""H""))
  withDefaultTimeZone(java.time.ZoneId.of(""UTC-09:00"")) {
    withSQLConf(SESSION_LOCAL_TIMEZONE.key -> ""UTC-10:00"") {
      sql(s""select H from $viewName"").show(false)
    }
  }
} {code}
It is expected to output:
{code:java}
+---+
|H  |
+---+
|0  |
+---+ {code}
but actual output is:
{code:java}
+---+
|H  |
+---+
|8  |
+---+ {code}",maxgekk,maxgekk,Major,Resolved,Fixed,21/Feb/23 17:05,22/Feb/23 11:04
Bug,SPARK-42534,13525804,Fix DB2 Limit clause,,ivan.sadikov,ivan.sadikov,Major,Resolved,Fixed,23/Feb/23 01:40,24/Feb/23 12:44
Bug,SPARK-42539,13525927,"User-provided JARs can override Spark's Hive metadata client JARs when using ""builtin""","Recently we observed that on version 3.2.0 and Java 8, it is possible for user-provided Hive JARs to break the ability for Spark, via the Hive metadata client / {{IsolatedClientLoader}}, to communicate with Hive Metastore, when using the default behavior of the ""builtin"" Hive version. After SPARK-35321, when Spark is compiled against Hive >= 2.3.9 and the ""builtin"" Hive client version is used, we will call the method {{Hive.getWithoutRegisterFns()}} (from HIVE-21563) instead of {{Hive.get()}}. If the user has included, for example, {{hive-exec-2.3.8.jar}} on their classpath, the client will break with a {{NoSuchMethodError}}. This particular failure mode was resolved in 3.2.1 by SPARK-37446, but while investigating, we found a general issue that it's possible for user JARs to override Spark's own JARs -- but only inside of the IsolatedClientLoader when using ""builtin"". This happens because even when Spark is configured to use the ""builtin"" Hive classes, it still creates a separate URLClassLoader for the HiveClientImpl used for HMS communication. To get the set of JAR URLs to use for this classloader, Spark [collects all of the JARs used by the user classloader (and its parent, and that classloader's parent, and so on)|https://github.com/apache/spark/blob/87e3d5625e76bb734b8dd753bfb25002822c8585/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala#L412-L438]. Thus the newly created classloader will have all of the same JARs as the user classloader, but the ordering has been reversed! User JARs get prioritized ahead of system JARs, because the classloader hierarchy is traversed from bottom-to-top. For example let's say we have user JARs ""foo.jar"" and ""hive-exec-2.3.8.jar"". The user classloader will look like this:
{code}
MutableURLClassLoader
-- foo.jar
-- hive-exec-2.3.8.jar
-- parent: URLClassLoader
----- spark-core_2.12-3.2.0.jar
----- ...
----- hive-exec-2.3.9.jar
----- ...
{code}

This setup provides the expected behavior within the user classloader; it will first check the parent, so hive-exec-2.3.9.jar takes precedence, and the MutableURLClassLoader is only checked if the class doesn't exist in the parent. But when a JAR list is constructed for the IsolatedClientLoader, it traverses the URLs from MutableURLClassLoader first, then it's parent, so the final list looks like (in order):
{code}
URLClassLoader [IsolatedClientLoader]
-- foo.jar
-- hive-exec-2.3.8.jar
-- spark-core_2.12-3.2.0.jar
-- ...
-- hive-exec-2.3.9.jar
-- ...
-- parent: boot classloader (JVM classes)
{code}
Now when a lookup happens, all of the JARs are within the same URLClassLoader, and the user JARs are in front of the Spark ones, so the user JARs get prioritized. This is the opposite of the expected behavior when using the default user/application classloader in Spark, which has parent-first behavior, prioritizing the Spark/system classes over the user classes. (Note that this behavior is correct when using the {{ChildFirstURLClassLoader}}.)

After SPARK-37446, the NoSuchMethodError is no longer an issue, but this still breaks assumptions about how user JARs should be treated vs. system JARs, and presents the ability for the client to break in other ways. For example in SPARK-37446 it describes a scenario whereby Hive 2.3.8 JARs have been included; the changes in Hive 2.3.9 were needed to improve compatibility with older HMS, so if a user were to accidentally include these older JARs, it could break the ability of Spark to communicate with HMS 1.x

I see two solutions to this:

*(A) Remove the separate classloader entirely when using ""builtin""*
Starting from 3.0.0, due to SPARK-26839, when using Java 9+, we don't even create a new classloader when using ""builtin"". This makes sense, as [called out in this comment|https://github.com/apache/spark/pull/24057#discussion_r265142878], since the point of ""builtin"" is to use the existing JARs on the classpath anyway. This proposes simply extending the changes from SPARK-26839 to all Java versions, instead of restricting to Java 9+ only.

*(B) Reverse the ordering of parent/child JARs when constructing the URL list*
The most targeted fix that can be made is to simply reverse the ordering on [this line in HiveUtils|https://github.com/apache/spark/blob/87e3d5625e76bb734b8dd753bfb25002822c8585/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala#L419], which prioritizes child-classloader JARs over parent-classloader JARs, reversing the expected ordering. There is already special handling for {{ChildFirstURLClassLoader}}, so all that needs to be done is to reverse this order.

I prefer (A) because I think it is a clean solution in that it both simplifies the classloader setup, and reduces divergence / special handling for different Java versions. At the time SPARK-26839 went in (2019), Java 9+ support was newer and less well-tested. Now after a few years we can see that the approach in SPARK-26839 clearly works well, so I see no reason _not_ to extend this approach to other Java versions as well.",xkrogen,xkrogen,Major,Resolved,Fixed,23/Feb/23 17:52,19/Apr/23 00:06
Bug,SPARK-42547,13525982,Make PySpark working with Python 3.7,"{code}
+ ./python/run-tests --python-executables=python3
Running PySpark tests. Output is in /home/ec2-user/spark/python/unit-tests.log
Will test against the following Python executables: ['python3']
Will test the following Python modules: ['pyspark-connect', 'pyspark-core', 'pyspark-errors', 'pyspark-ml', 'pyspark-mllib', 'pyspark-pandas', 'pyspark-pandas-slow', 'pyspark-resource', 'pyspark-sql', 'pyspark-streaming']
python3 python_implementation is CPython
python3 version is: Python 3.7.16
Starting test(python3): pyspark.ml.tests.test_feature (temp output: /home/ec2-user/spark/python/target/8ca9ab1a-05cc-4845-bf89-30d9001510bc/python3__pyspark.ml.tests.test_feature__kg6sseie.log)
Starting test(python3): pyspark.ml.tests.test_base (temp output: /home/ec2-user/spark/python/target/f2264f3b-6b26-4e61-9452-8d6ddd7eb002/python3__pyspark.ml.tests.test_base__0902zf9_.log)
Starting test(python3): pyspark.ml.tests.test_algorithms (temp output: /home/ec2-user/spark/python/target/d1dc4e07-e58c-4c03-abe5-09d8fab22e6a/python3__pyspark.ml.tests.test_algorithms__lh3wb2u8.log)
Starting test(python3): pyspark.ml.tests.test_evaluation (temp output: /home/ec2-user/spark/python/target/3f42dc79-c945-4cf2-a1eb-83e72b40a9ee/python3__pyspark.ml.tests.test_evaluation__89idc7fa.log)
Finished test(python3): pyspark.ml.tests.test_base (16s)
Starting test(python3): pyspark.ml.tests.test_functions (temp output: /home/ec2-user/spark/python/target/5a3b90f0-216b-4edd-9d15-6619d3e03300/python3__pyspark.ml.tests.test_functions__g5u1290s.log)
Traceback (most recent call last):
  File ""/usr/lib64/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib64/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/ec2-user/spark/python/pyspark/ml/tests/test_functions.py"", line 21, in <module>
    from pyspark.ml.functions import predict_batch_udf
  File ""/home/ec2-user/spark/python/pyspark/ml/functions.py"", line 38, in <module>
    from typing import Any, Callable, Iterator, List, Mapping, Protocol, TYPE_CHECKING, Tuple, Union
ImportError: cannot import name 'Protocol' from 'typing' (/usr/lib64/python3.7/typing.py)
Had test failures in pyspark.ml.tests.test_functions with python3; see logs.
{code}",gurwls223,gurwls223,Blocker,Resolved,Fixed,24/Feb/23 03:04,24/Feb/23 05:15
Bug,SPARK-42552,13526023,"Get ParseException when run sql: ""SELECT 1 UNION SELECT 1;""","When I run sql
{code:java}
scala> spark.sql(""SELECT 1 UNION SELECT 1;"") {code}
I get ParseException:
{code:java}
org.apache.spark.sql.catalyst.parser.ParseException:
mismatched input 'SELECT' expecting {<EOF>, ';'}(line 1, pos 15)== SQL ==
SELECT 1 UNION SELECT 1;
---------------^^^  at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:127)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:51)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:77)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:616)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:616)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
  ... 47 elided
 {code}
If I run with parentheses , it works well 
{code:java}
scala> spark.sql(""(SELECT 1) UNION (SELECT 1);"") 
res4: org.apache.spark.sql.DataFrame = [1: int]{code}
This should be a bug

 

 ",chengpan,jiang13021,Major,Resolved,Fixed,24/Feb/23 08:46,19/Apr/23 08:38
Bug,SPARK-42553,13526036,"NonReserved keyword ""interval"" can't be column name","INTERVAL is a Non-Reserved keyword in spark. ""Non-Reserved keywords"" have a special meaning in particular contexts and can be used as identifiers in other contexts. So by design, interval can be used as a column name.
{code:java}
scala> spark.sql(""select interval from mytable"")
org.apache.spark.sql.catalyst.parser.ParseException:
at least one time unit should be given for interval literal(line 1, pos 7)== SQL ==
select interval from mytable
-------^^^  at org.apache.spark.sql.errors.QueryParsingErrors$.invalidIntervalLiteralError(QueryParsingErrors.scala:196)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$parseIntervalLiteral$1(AstBuilder.scala:2481)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.parseIntervalLiteral(AstBuilder.scala:2466)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitInterval$1(AstBuilder.scala:2432)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitInterval(AstBuilder.scala:2431)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitInterval(AstBuilder.scala:57)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$IntervalContext.accept(SqlBaseParser.java:17308)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitIntervalLiteral(SqlBaseBaseVisitor.java:1581)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$IntervalLiteralContext.accept(SqlBaseParser.java:16929)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitConstantDefault(SqlBaseBaseVisitor.java:1511)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ConstantDefaultContext.accept(SqlBaseParser.java:15905)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitValueExpressionDefault(SqlBaseBaseVisitor.java:1392)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ValueExpressionDefaultContext.accept(SqlBaseParser.java:15298)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:61)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1412)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitPredicated$1(AstBuilder.scala:1548)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:1547)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:57)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext.accept(SqlBaseParser.java:14745)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitExpression(SqlBaseBaseVisitor.java:1343)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ExpressionContext.accept(SqlBaseParser.java:14606)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:61)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1412)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitNamedExpression$1(AstBuilder.scala:1434)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedExpression(AstBuilder.scala:1433)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedExpression(AstBuilder.scala:57)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext.accept(SqlBaseParser.java:14124)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:61)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitNamedExpressionSeq$2(AstBuilder.scala:628)
  at scala.collection.immutable.List.map(List.scala:293)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedExpressionSeq(AstBuilder.scala:628)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$withSelectQuerySpecification$1(AstBuilder.scala:734)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.withSelectQuerySpecification(AstBuilder.scala:728)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitRegularQuerySpecification$1(AstBuilder.scala:620)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitRegularQuerySpecification(AstBuilder.scala:608)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitRegularQuerySpecification(AstBuilder.scala:57)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext.accept(SqlBaseParser.java:9679)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitQueryPrimaryDefault(SqlBaseBaseVisitor.java:846)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryPrimaryDefaultContext.accept(SqlBaseParser.java:9184)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitQueryTermDefault(SqlBaseBaseVisitor.java:832)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryTermDefaultContext.accept(SqlBaseParser.java:8953)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:61)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.plan(AstBuilder.scala:112)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitQuery$1(AstBuilder.scala:118)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:117)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:57)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext.accept(SqlBaseParser.java:6398)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitStatementDefault(SqlBaseBaseVisitor.java:69)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$StatementDefaultContext.accept(SqlBaseParser.java:1835)
  at org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:18)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitSingleStatement$1(AstBuilder.scala:78)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleStatement(AstBuilder.scala:78)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.$anonfun$parsePlan$1(ParseDriver.scala:78)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:110)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:51)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:77)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:616)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:616)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
  ... 47 elided
{code}
Since there must be at least one time unit after the interval, why in SqlBaseParser.g4, the definition of interval is
{code:java}
interval
    : INTERVAL (errorCapturingMultiUnitsInterval | errorCapturingUnitToUnitInterval)?
    ;  {code}
instead of
{code:java}
interval
    : INTERVAL (errorCapturingMultiUnitsInterval | errorCapturingUnitToUnitInterval)
    ; {code}
If we remove the ""?"", we ensure that there must be at least one time unit after the interval from the parsing level",jiang13021,jiang13021,Major,Resolved,Fixed,24/Feb/23 09:42,02/Mar/23 15:25
Bug,SPARK-42572,13526138,Logic error for StateStore.validateStateRowFormat,SPARK-42484 Changed the logic of whether to check state store format in StateStore.validateStateRowFormat. Revert it and add unit test to make sure this won't happen again,WweiL,WweiL,Major,Resolved,Fixed,25/Feb/23 00:57,28/Feb/23 03:13
Bug,SPARK-42596,13526279,[YARN] OMP_NUM_THREADS not set to number of executor cores by default,"Run this PySpark script with `spark.executor.cores=1`
{code:python}
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf

spark = SparkSession.builder.getOrCreate()

var_name = 'OMP_NUM_THREADS'

def get_env_var():
  return os.getenv(var_name)

udf_get_env_var = udf(get_env_var)
spark.range(1).toDF(""id"").withColumn(f""env_{var_name}"", udf_get_env_var()).show(truncate=False)
{code}
Output with release `3.3.2`:
{noformat}
+---+-----------------------+
|id |env_OMP_NUM_THREADS    |
+---+-----------------------+
|0  |null                   |
+---+-----------------------+
{noformat}
Output with release `3.3.0`:
{noformat}
+---+-----------------------+
|id |env_OMP_NUM_THREADS    |
+---+-----------------------+
|0  |1                      |
+---+-----------------------+
{noformat}",jzhuge,jzhuge,Major,Resolved,Fixed,27/Feb/23 07:28,28/Feb/23 02:20
Bug,SPARK-42600,13526309,currentDatabase Shall use  NamespaceHelper instead of MultipartIdentifierHelper,,yao,yao,Major,Resolved,Fixed,27/Feb/23 09:49,28/Feb/23 01:59
Bug,SPARK-42608,13526379,Use full column names for inner fields in resolution errors,"If there are multiple inner columns with the same name, resolution errors may be confusing as we only use field names, not full column names.",aokolnychyi,aokolnychyi,Major,Resolved,Fixed,27/Feb/23 19:37,28/Feb/23 09:00
Bug,SPARK-42611,13526417,Insert char/varchar length checks for inner fields during resolution,"In SPARK-36498, we added support for reordering inner fields in structs during resolution. Unfortunately, we don't add any length validation for nested char/varchar columns in that path.",aokolnychyi,aokolnychyi,Major,Resolved,Fixed,28/Feb/23 00:34,01/Mar/23 07:51
Bug,SPARK-42613,13526423,PythonRunner should set OMP_NUM_THREADS to task cpus instead of executor cores by default,"Follow up from [https://github.com/apache/spark/pull/40199#discussion_r1119453996]

If OMP_NUM_THREADS is not set explicitly, we should set it to `spark.task.cpus` instead of `spark.executor.cores` as described in [PR #38699|https://github.com/apache/spark/pull/38699].",jzhuge,jzhuge,Major,Resolved,Fixed,28/Feb/23 02:03,02/Mar/23 00:18
Bug,SPARK-42616,13526451,SparkSQLCLIDriver shall only close started hive sessionState,,yao,yao,Major,Resolved,Fixed,28/Feb/23 05:31,01/Mar/23 01:48
Bug,SPARK-42622,13526546,StackOverflowError reading json that does not conform to schema,"Databricks runtime 12.1 uses a pre-release version of spark 3.4.x we encountered the following problem

 

!https://user-images.githubusercontent.com/133639/221866500-99f187a0-8db3-42a7-85ca-b027fdec160d.png!",jelmer1,jelmer1,Major,Resolved,Fixed,28/Feb/23 15:11,02/Mar/23 14:44
Bug,SPARK-42623,13526563,parameter markers not blocked in DDL,"The parameterized query code does not block DDL statements from referencing parameter markers.
E.g. a 

 
{code:java}
scala> spark.sql(sqlText = ""CREATE VIEW v1 AS SELECT current_timestamp() + :later as stamp, :x * :x AS square"", args = Map(""later"" -> ""INTERVAL'3' HOUR"", ""x"" -> ""15.0"")).show()
++
||
++
++
{code}
It appears we have some protection that fails us when the view is invoked:

 
{code:java}
scala> spark.sql(sqlText = ""SELECT * FROM v1"", args = Map(""later"" -> ""INTERVAL'3' HOUR"", ""x"" -> ""15.0"")).show()
org.apache.spark.sql.AnalysisException: [UNBOUND_SQL_PARAMETER] Found the unbound parameter: `later`. Please, fix `args` and provide a mapping of the parameter to a SQL literal.; line 1 pos 29
{code}

Right now I think affected are:
* DEFAULT definition
* VIEW definition

but any other future standard expression popping up is at risk, such as SQL Functions, or GENERATED COLUMN.

CREATE TABLE AS is debatable, since it it executes the query at definition only.
For simplicity I propose to block the feature from ANY DDL statement (CREATE, ALTER).

 

 ",cloud_fan,srielau,Major,Resolved,Fixed,28/Feb/23 17:34,14/Mar/23 15:42
Bug,SPARK-42625,13526582,Upgrade zstd-jni to 1.5.4-2,,dongjoon,dongjoon,Minor,Resolved,Fixed,28/Feb/23 21:54,01/Mar/23 01:17
Bug,SPARK-42634,13526727,Several counter-intuitive behaviours in the TimestampAdd expression,,,mashplant,Major,Closed,Fixed,01/Mar/23 18:13,01/Mar/23 18:54
Bug,SPARK-42635,13526728,Several counter-intuitive behaviours in the TimestampAdd expression,"# When the time is close to daylight saving time transition, the result may be discontinuous and not monotonic.

We currently have:
{code:scala}
scala> spark.conf.set(""spark.sql.session.timeZone"", ""America/Los_Angeles"")
scala> spark.sql(""select timestampadd(second, 24 * 3600 - 1, timestamp'2011-03-12 03:00:00')"").show
+------------------------------------------------------------------------+
|timestampadd(second, ((24 * 3600) - 1), TIMESTAMP '2011-03-12 03:00:00')|
+------------------------------------------------------------------------+
|                                                     2011-03-13 03:59:59|
+------------------------------------------------------------------------+
scala> spark.sql(""select timestampadd(second, 24 * 3600, timestamp'2011-03-12 03:00:00')"").show
+------------------------------------------------------------------+
|timestampadd(second, (24 * 3600), TIMESTAMP '2011-03-12 03:00:00')|
+------------------------------------------------------------------+
|                                               2011-03-13 03:00:00|
+------------------------------------------------------------------+ {code}
 

In the second query, adding one more second will set the time back one hour instead. Plus, there are only {{23 * 3600}} seconds from {{2011-03-12 03:00:00}} to {{2011-03-13 03:00:00}}, instead of {{24 * 3600}} seconds, due to the daylight saving time transition.

The root cause of the problem is the Spark code at [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L790] wrongly assumes every day has {{MICROS_PER_DAY}} seconds, and does the day and time-in-day split before looking at the timezone.

2. Adding month, quarter, and year silently ignores Int overflow during unit conversion.

The root cause is [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L1246]. {{quantity}} is multiplied by {{3}} or {{MONTHS_PER_YEAR}} without checking overflow. Note that we do have overflow checking in adding the amount to the timestamp, so the behavior is inconsistent.

This can cause counter-intuitive results like this:

{code:scala}
scala> spark.sql(""select timestampadd(quarter, 1431655764, timestamp'1970-01-01')"").show
+------------------------------------------------------------------+
|timestampadd(quarter, 1431655764, TIMESTAMP '1970-01-01 00:00:00')|
+------------------------------------------------------------------+
|                                               1969-09-01 00:00:00|
+------------------------------------------------------------------+{code}

3. Adding sub-month units (week, day, hour, minute, second, millisecond, microsecond)silently ignores Long overflow during unit conversion.

This is similar to the previous problem:

{code:scala}
 scala> spark.sql(""select timestampadd(day, 106751992, timestamp'1970-01-01')"").show(false)
+-------------------------------------------------------------+
|timestampadd(day, 106751992, TIMESTAMP '1970-01-01 00:00:00')|
+-------------------------------------------------------------+
|-290308-12-22 15:58:10.448384                                |
+-------------------------------------------------------------+{code}

 ",mashplant,mashplant,Major,Resolved,Fixed,01/Mar/23 18:16,04/Mar/23 19:16
Bug,SPARK-42644,13526797,Add `hive` dependency to `connect` module,,dongjoon,dongjoon,Major,Resolved,Fixed,02/Mar/23 04:49,02/Mar/23 06:46
Bug,SPARK-42649,13526815,Remove the standard Apache License header from the top of third-party source files,,dongjoon,dongjoon,Major,Resolved,Fixed,02/Mar/23 08:12,02/Mar/23 09:05
Bug,SPARK-42655,13526898,Incorrect ambiguous column reference error,"val df1 = sc.parallelize(List((1,2,3,4,5),(1,2,3,4,5))).toDF(""id"",""col2"",""col3"",""col4"", ""col5"")
val op_cols_same_case = List(""id"",""col2"",""col3"",""col4"", ""col5"", ""id"")
val df2 = df1.select(op_cols_same_case.head, op_cols_same_case.tail: _*)
df2.select(""id"").show()
 
This query runs fine.
 
But when we change the casing of the op_cols to have mix of upper & lower case (""id"" & ""ID"") it throws an ambiguous col ref error:
 
val df1 = sc.parallelize(List((1,2,3,4,5),(1,2,3,4,5))).toDF(""id"",""col2"",""col3"",""col4"", ""col5"")
val op_cols_mixed_case = List(""id"",""col2"",""col3"",""col4"", ""col5"", ""ID"")
val df3 = df1.select(op_cols_mixed_case.head, op_cols_mixed_case.tail: _*)
df3.select(""id"").show()



org.apache.spark.sql.AnalysisException: Reference 'id' is ambiguous, could be: id, id.

  at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:363)

  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:112)

  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$resolveExpressionByPlanChildren$1(Analyzer.scala:1857)

  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$resolveExpression$2(Analyzer.scala:1787)

  at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:60)

  at org.apache.spark.sql.catalyst.analysis.Analyzer.innerResolve$1(Analyzer.scala:1794)

  at org.apache.spark.sql.catalyst.analysis.Analyzer.resolveExpression(Analyzer.scala:1812)

  at org.apache.spark.sql.catalyst.analysis.Analyzer.resolveExpressionByPlanChildren(Analyzer.scala:1863)

  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$17.$anonfun$applyOrElse$94(Analyzer.scala:1577)

  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:193)

  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)

  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:193)

  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:204)

  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:209)

  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)

  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

  at scala.collection.TraversableLike.map(TraversableLike.scala:286)

  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)

  at scala.collection.AbstractTraversable.map(Traversable.scala:108)

  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:209)

 


Since, Spark is case insensitive, it should work for second case also when we have upper and lower case column names in the column list.

It also works fine in Spark 2.3.
 ",unamesk15,unamesk15,Major,Resolved,Fixed,02/Mar/23 18:43,04/Apr/23 13:17
Bug,SPARK-42665,13527027,`simple udf` test failed using Maven ,"{code:java}
simple udf *** FAILED ***
  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.ClientE2ETestSuite
  at io.grpc.Status.asRuntimeException(Status.java:535)
  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
  at org.apache.spark.sql.connect.client.SparkResult.org$apache$spark$sql$connect$client$SparkResult$$processResponses(SparkResult.scala:61)
  at org.apache.spark.sql.connect.client.SparkResult.length(SparkResult.scala:106)
  at org.apache.spark.sql.connect.client.SparkResult.toArray(SparkResult.scala:123)
  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2426)
  at org.apache.spark.sql.Dataset.withResult(Dataset.scala:2747)
  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2425)
  at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$8(ClientE2ETestSuite.scala:85)
  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) {code}",LuciferYang,LuciferYang,Major,Resolved,Fixed,03/Mar/23 17:41,07/Mar/23 07:36
Bug,SPARK-42671,13527102,Fix bug for createDataFrame from complex type schema,,panbingkun,panbingkun,Minor,Resolved,Fixed,05/Mar/23 02:21,15/Mar/23 21:31
Bug,SPARK-42673,13527115,Make build/mvn build Spark only with the verified maven version,GA ,LuciferYang,LuciferYang,Major,Resolved,Fixed,05/Mar/23 08:21,06/Mar/23 19:07
Bug,SPARK-42677,13527168,Fix the invalid tests for broadcast hint,"Currently, there are a lot of test cases for broadcast hint is invalid. Because the data size is smaller than broadcast threshold.",beliefer,beliefer,Major,Resolved,Fixed,06/Mar/23 04:05,06/Mar/23 08:17
Bug,SPARK-42681,13527202,Relax ordering constraint for ALTER TABLE ADD|REPLACE column options,"Currently the grammar for ALTER TABLE ADD|REPLACE column is:

qualifiedColTypeWithPosition
    : name=multipartIdentifier dataType (NOT NULL)? defaultExpression? commentSpec? colPosition?
    ;

This enforces a constraint on the order of: (NOT NULL, DEFAULT value, COMMENT value FIRST|AFTER value). We can update the grammar to allow these options in any order instead, to improve usability.",vli-databricks,vli-databricks,Major,Resolved,Fixed,06/Mar/23 08:36,08/Mar/23 04:04
Bug,SPARK-42697,13527432,/api/v1/applications return 0 for duration,which should be total uptime,yao,yao,Major,Resolved,Fixed,07/Mar/23 10:12,09/Mar/23 05:35
Bug,SPARK-42700,13527451,Add h2 as test dependency of connect-server module,"run 
 # mvn clean install -DskipTests -pl connector/connect/server -am
 # mvn test -pl connector/connect/server

{code:java}
*** RUN ABORTED ***
  java.lang.ClassNotFoundException: org.h2.Driver
  at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)
  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
  at java.base/java.lang.Class.forName0(Native Method)
  at java.base/java.lang.Class.forName(Class.java:398)
  at org.apache.spark.util.Utils$.classForName(Utils.scala:225)
  at org.apache.spark.sql.connect.ProtoToParsedPlanTestSuite.beforeAll(ProtoToParsedPlanTestSuite.scala:68)
  at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)
  at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
  at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
  ...
 {code}",LuciferYang,LuciferYang,Major,Resolved,Fixed,07/Mar/23 12:40,08/Mar/23 04:46
Bug,SPARK-42709,13527576,Do not rely on __file__,We have a lot of places using __file__ which is actually optional. We shouldn't reply on them,gurwls223,gurwls223,Major,Resolved,Fixed,08/Mar/23 05:09,08/Mar/23 18:00
Bug,SPARK-42745,13527953,Improved AliasAwareOutputExpression works with DSv2,"After SPARK-40086 / SPARK-42049 the following, simple subselect expression containing query:
{noformat}
select (select sum(id) from t1)
{noformat}
fails with:

{noformat}
09:48:57.645 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.lang.NullPointerException
	at org.apache.spark.sql.execution.datasources.v2.BatchScanExec.batch$lzycompute(BatchScanExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.BatchScanExec.batch(BatchScanExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.BatchScanExec.hashCode(BatchScanExec.scala:60)
	at scala.runtime.Statics.anyHash(Statics.java:122)
        ...
	at org.apache.spark.sql.catalyst.trees.TreeNode.hashCode(TreeNode.scala:249)
	at scala.runtime.Statics.anyHash(Statics.java:122)
	at scala.collection.mutable.HashTable$HashUtils.elemHashCode(HashTable.scala:416)
	at scala.collection.mutable.HashTable$HashUtils.elemHashCode$(HashTable.scala:416)
	at scala.collection.mutable.HashMap.elemHashCode(HashMap.scala:44)
	at scala.collection.mutable.HashTable.addEntry(HashTable.scala:149)
	at scala.collection.mutable.HashTable.addEntry$(HashTable.scala:148)
	at scala.collection.mutable.HashMap.addEntry(HashMap.scala:44)
	at scala.collection.mutable.HashTable.init(HashTable.scala:110)
	at scala.collection.mutable.HashTable.init$(HashTable.scala:89)
	at scala.collection.mutable.HashMap.init(HashMap.scala:44)
	at scala.collection.mutable.HashMap.readObject(HashMap.scala:195)
        ...
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:85)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1520)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
{noformat}
when DSv2 is enabled.",petertoth,petertoth,Major,Resolved,Fixed,10/Mar/23 09:16,14/Mar/23 15:41
Bug,SPARK-42747,13527973,Fix incorrect internal status of LoR and AFT,"LoR and AFT applied internal status to optimize prediction/transform, but the status is not correctly updated in some case:


{code:java}
from pyspark.sql import Row
from pyspark.ml.classification import *
from pyspark.ml.linalg import Vectors

df = spark.createDataFrame(
    [
        (1.0, 1.0, Vectors.dense(0.0, 5.0)),
        (0.0, 2.0, Vectors.dense(1.0, 2.0)),
        (1.0, 3.0, Vectors.dense(2.0, 1.0)),
        (0.0, 4.0, Vectors.dense(3.0, 3.0)),
    ],
    [""label"", ""weight"", ""features""],
)

lor = LogisticRegression(weightCol=""weight"")
model = lor.fit(df)

# status changes 1
for t in [0.0, 0.1, 0.2, 0.5, 1.0]:
    model.setThreshold(t).transform(df)

# status changes 2
[model.setThreshold(t).predict(Vectors.dense(0.0, 5.0)) for t in [0.0, 0.1, 0.2, 0.5, 1.0]]

for t in [0.0, 0.1, 0.2, 0.5, 1.0]:
    print(t)
    model.setThreshold(t).transform(df).show()                                        #  <- error results
{code}


results:

{code:java}
0.0
+-----+------+---------+--------------------+--------------------+----------+
|label|weight| features|       rawPrediction|         probability|prediction|
+-----+------+---------+--------------------+--------------------+----------+
|  1.0|   1.0|[0.0,5.0]|[0.10932013376341...|[0.52730284774069...|       0.0|
|  0.0|   2.0|[1.0,2.0]|[-0.8619624039359...|[0.29692950635762...|       0.0|
|  1.0|   3.0|[2.0,1.0]|[-0.3634508721860...|[0.41012446452385...|       0.0|
|  0.0|   4.0|[3.0,3.0]|[2.33975176373760...|[0.91211618852612...|       0.0|
+-----+------+---------+--------------------+--------------------+----------+

0.1
+-----+------+---------+--------------------+--------------------+----------+
|label|weight| features|       rawPrediction|         probability|prediction|
+-----+------+---------+--------------------+--------------------+----------+
|  1.0|   1.0|[0.0,5.0]|[0.10932013376341...|[0.52730284774069...|       0.0|
|  0.0|   2.0|[1.0,2.0]|[-0.8619624039359...|[0.29692950635762...|       0.0|
|  1.0|   3.0|[2.0,1.0]|[-0.3634508721860...|[0.41012446452385...|       0.0|
|  0.0|   4.0|[3.0,3.0]|[2.33975176373760...|[0.91211618852612...|       0.0|
+-----+------+---------+--------------------+--------------------+----------+

0.2
+-----+------+---------+--------------------+--------------------+----------+
|label|weight| features|       rawPrediction|         probability|prediction|
+-----+------+---------+--------------------+--------------------+----------+
|  1.0|   1.0|[0.0,5.0]|[0.10932013376341...|[0.52730284774069...|       0.0|
|  0.0|   2.0|[1.0,2.0]|[-0.8619624039359...|[0.29692950635762...|       0.0|
|  1.0|   3.0|[2.0,1.0]|[-0.3634508721860...|[0.41012446452385...|       0.0|
|  0.0|   4.0|[3.0,3.0]|[2.33975176373760...|[0.91211618852612...|       0.0|
+-----+------+---------+--------------------+--------------------+----------+

0.5
+-----+------+---------+--------------------+--------------------+----------+
|label|weight| features|       rawPrediction|         probability|prediction|
+-----+------+---------+--------------------+--------------------+----------+
|  1.0|   1.0|[0.0,5.0]|[0.10932013376341...|[0.52730284774069...|       0.0|
|  0.0|   2.0|[1.0,2.0]|[-0.8619624039359...|[0.29692950635762...|       0.0|
|  1.0|   3.0|[2.0,1.0]|[-0.3634508721860...|[0.41012446452385...|       0.0|
|  0.0|   4.0|[3.0,3.0]|[2.33975176373760...|[0.91211618852612...|       0.0|
+-----+------+---------+--------------------+--------------------+----------+

1.0
+-----+------+---------+--------------------+--------------------+----------+
|label|weight| features|       rawPrediction|         probability|prediction|
+-----+------+---------+--------------------+--------------------+----------+
|  1.0|   1.0|[0.0,5.0]|[0.10932013376341...|[0.52730284774069...|       0.0|
|  0.0|   2.0|[1.0,2.0]|[-0.8619624039359...|[0.29692950635762...|       0.0|
|  1.0|   3.0|[2.0,1.0]|[-0.3634508721860...|[0.41012446452385...|       0.0|
|  0.0|   4.0|[3.0,3.0]|[2.33975176373760...|[0.91211618852612...|       0.0|
+-----+------+---------+--------------------+--------------------+----------+

{code}
",podongfeng,podongfeng,Major,Resolved,Fixed,10/Mar/23 12:58,11/Mar/23 14:47
Bug,SPARK-42749,13528009,CAST(x as int) does not generate error with overflow,"Hi,

When performing the following code:

{{select cast(7.415246799222789E19 as int)}}

according to the documentation, an error is expected as {{7.415246799222789E19 }}is an overflow value for datatype INT.

However, the value 2147483647 is returned. 

The behaviour of the following is correct as it returns NULL:

{{select try_cast(7.415246799222789E19 as int) }}

This results in unexpected behaviour and data corruption.",,tjomme,Major,Resolved,Fixed,10/Mar/23 16:27,13/Mar/23 08:00
Bug,SPARK-42770,13528185,SQLImplicitsTestSuite test failed with Java 17,"[https://github.com/apache/spark/actions/runs/4318647315/jobs/7537203682]
{code:java}
[info] - test implicit encoder resolution *** FAILED *** (1 second, 329 milliseconds)
4429[info]   2023-03-02T23:00:20.404434 did not equal 2023-03-02T23:00:20.404434875 (SQLImplicitsTestSuite.scala:63)
4430[info]   org.scalatest.exceptions.TestFailedException:
4431[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
4432[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
4433[info]   at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)
4434[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)
4435[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.testImplicit$1(SQLImplicitsTestSuite.scala:63)
4436[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.$anonfun$new$2(SQLImplicitsTestSuite.scala:133)
4437[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
4438[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
4439[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
4440[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
4441[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
4442[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
4443[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
4444[info]   at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
4445[info]   at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
4446[info]   at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)
4447[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
4448[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
4449[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
4450[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
4451[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
4452[info]   at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)
4453[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
4454[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
4455[info]   at scala.collection.immutable.List.foreach(List.scala:431)
4456[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
4457[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
4458[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
4459[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
4460[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
4461[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
4462[info]   at org.scalatest.Suite.run(Suite.scala:1114)
4463[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
4464[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
4465[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
4466[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
4467[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
4468[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
4469[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.org$scalatest$BeforeAndAfterAll$$super$run(SQLImplicitsTestSuite.scala:34)
4470[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
4471[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
4472[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
4473[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.run(SQLImplicitsTestSuite.scala:34)
4474[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
4475[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
4476[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
4477[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
4478[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
4479[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
4480[info]   at java.base/java.lang.Thread.run(Thread.java:833) {code}",LuciferYang,LuciferYang,Major,Resolved,Fixed,13/Mar/23 07:18,14/Mar/23 15:53
Bug,SPARK-42784,13528377,Fix the problem of incomplete creation of subdirectories in push merged localDir,"After we massively enabled push-based shuffle in our production environment, we found some warn messages appearing in the server-side log messages.

the warning log like:

ShuffleBlockPusher: Pushing block shufflePush_3_0_5352_935 to BlockManagerId(shuffle-push-merger, zw06-data-hdp-dn08251.mt, 7337, None) failed.
java.lang.RuntimeException: java.lang.RuntimeException: Cannot initialize merged shuffle partition for appId application_1671244879475_44020960 shuffleId 3 shuffleMergeId 0 reduceId 935.

After investigation, we identified the triggering mechanism of the bug。

The driver requested two different containers on the same physical machine. During the creation of the 'push-merged' directory in the first container (container_1), the mergeDir was created first, then the subDir were created based on the value of the ""spark.diskStore.subDirectories"" parameter. However, the resources of container_1 were preempted during the creation of the sub-directories, resulting in subDir not being created (only part of it was created ). As the mergeDir still existed, the second container (container_2) was unable to create further subDir (as it assumed that all directories had already been created).

 ",StoveM,StoveM,Major,Resolved,Fixed,14/Mar/23 08:50,01/Jul/23 03:52
Bug,SPARK-42785,13528391,"[K8S][Core] When spark submit without --deploy-mode, will face NPE in Kubernetes Case","According to this PR [https://github.com/apache/spark/pull/37880#issuecomment-1347777890,] when user spark submit without `--deploy-mode XXX` or `–conf spark.submit.deployMode=XXXX`, may face NPE with this code

 
args.deployMode.equals(""client"")
 
 ",zWangSheng,zWangSheng,Major,Resolved,Fixed,14/Mar/23 09:49,14/Mar/23 15:51
Bug,SPARK-42793,13528513,`connect` module requires `build_profile_flags`,,dongjoon,dongjoon,Major,Resolved,Fixed,14/Mar/23 22:04,15/Mar/23 07:43
Bug,SPARK-42799,13528551,Update SBT build `xercesImpl` version to match with pom.xml,,dongjoon,dongjoon,Minor,Resolved,Fixed,15/Mar/23 03:32,15/Mar/23 07:43
Bug,SPARK-42801,13528561,Fix Flaky ClientE2ETestSuite,,dongjoon,dongjoon,Major,Resolved,Fixed,15/Mar/23 05:50,15/Mar/23 07:43
Bug,SPARK-42812,13528635,client_type is missing from AddArtifactsRequest proto message,The client_type is missing from AddArtifactsRequest proto message,vicennial,vicennial,Major,Resolved,Fixed,15/Mar/23 13:26,20/Mar/23 19:10
Bug,SPARK-42813,13528641,Print application info when waitAppCompletion is false,,chengpan,chengpan,Major,Resolved,Fixed,15/Mar/23 14:02,21/Mar/23 16:08
Bug,SPARK-42817,13528669,Spark driver logs are filled with Initializing service data for shuffle service using name,"With SPARK-34828, we added the ability to make the shuffle service name configurable and we added a log [here|https://github.com/apache/spark/blob/8860f69455e5a722626194c4797b4b42cccd4510/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala#L118] that will log the shuffle service name. However, this log is printed in the driver logs whenever there is new executor launched and pollutes the log. 
{code}
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
{code}
We can just log this once in the driver.",csingh,csingh,Major,Resolved,Fixed,15/Mar/23 16:53,20/Mar/23 19:10
Bug,SPARK-42820,13528715,Update ORC to 1.8.3,,william,william,Major,Resolved,Fixed,16/Mar/23 02:38,16/Mar/23 05:01
Bug,SPARK-42828,13528832,PySpark type hint returns Any for methods on GroupedData,"Since upgrading to PySpark 3.3.x, type hints for
{code:java}
df.groupBy(...).count(){code}
are now returning Any instead of DataFrame, causing type inference issues downstream. This used to be correctly typed prior to 3.3.x.",apachespark,j03wang,Minor,Resolved,Fixed,16/Mar/23 16:44,03/Jul/23 06:38
Bug,SPARK-42851,13529035,EquivalentExpressions methods need to be consistently guarded by supportedExpression,"SPARK-41468 tried to fix a bug but introduced a new regression. Its change to {{EquivalentExpressions}} added a {{supportedExpression()}} guard to the {{addExprTree()}} and {{getExprState()}} methods, but didn't add the same guard to the other ""add"" entry point -- {{addExpr()}}.

As such, uses that add single expressions to CSE via {{addExpr()}} may succeed, but upon retrieval via {{getExprState()}} it'd inconsistently get a {{None}} due to failing the guard.

We need to make sure the ""add"" and ""get"" methods are consistent. It could be done by one of:
1. Adding the same {{supportedExpression()}} guard to {{addExpr()}}, or
2. Removing the guard from {{getExprState()}}, relying solely on the guard on the ""add"" path to make sure only intended state is added.
(or other alternative refactorings to fuse the guard into various methods to make it more efficient)

There are pros and cons to the two directions above, because {{addExpr()}} used to allow (potentially incorrect) more expressions to get CSE'd, making it more restrictive may cause performance regressions (for the cases that happened to work).

Example:
{code:sql}
select max(transform(array(id), x -> x)), max(transform(array(id), x -> x)) from range(2)
{code}

Running this query on Spark 3.2 branch returns the correct value:
{code}
scala> spark.sql(""select max(transform(array(id), x -> x)), max(transform(array(id), x -> x)) from range(2)"").collect
res0: Array[org.apache.spark.sql.Row] = Array([WrappedArray(1),WrappedArray(1)])
{code}
Here, {{transform(array(id), x -> x)}} is an {{AggregateExpression}} that was (potentially unsafely) recognized by {{addExpr()}} as a common subexpression, and {{getExprState()}} doesn't do extra guarding, so during physical planning, in {{PhysicalAggregation}} this expression gets CSE'd in both the aggregation expression list and the result expressions list.
{code}
AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[], functions=[max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false)))])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=11]
      +- SortAggregate(key=[], functions=[partial_max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false)))])
         +- Range (0, 2, step=1, splits=16)
{code}

Running the same query on current master triggers an error when binding the result expression to the aggregate expression in the Aggregate operators (for a WSCG-enabled operator like {{HashAggregateExec}}, the same error would show up during codegen):
{code}
ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 16) (ip-10-110-16-93.us-west-2.compute.internal executor driver): java.lang.IllegalStateException: Couldn't find max(transform(array(id#0L), lambdafunction(lambda x#2L, lambda x#2L, false)))#4 in [max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false)))#3]
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:517)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:532)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:517)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:456)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:94)
	at scala.collection.immutable.List.map(List.scala:297)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:94)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:161)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.generateResultProjection(AggregationIterator.scala:246)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.<init>(AggregationIterator.scala:296)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.<init>(SortBasedAggregationIterator.scala:49)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:79)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)
...
{code}
Note that the aggregate expressions are deduplicated in {{PhysicalAggregation}}, but the result expressions were unable to deduplicate consistently due to the bug mentioned in this ticket.
{code}
AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[], functions=[max(transform(array(id#15L), lambdafunction(lambda x#16L, lambda x#16L, false)))])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=38]
      +- SortAggregate(key=[], functions=[partial_max(transform(array(id#15L), lambdafunction(lambda x#16L, lambda x#16L, false)))])
         +- Range (0, 2, step=1, splits=16)
{code}

Fixing it via method 1 is more correct than method 2 in terms of avoiding incorrect CSE:
{code:diff}
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala
index 330d66a21b..12def60042 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala
@@ -40,7 +40,11 @@ class EquivalentExpressions {
    * Returns true if there was already a matching expression.
    */
   def addExpr(expr: Expression): Boolean = {
-    updateExprInMap(expr, equivalenceMap)
+    if (supportedExpression(expr)) {
+      updateExprInMap(expr, equivalenceMap)
+    } else {
+      false
+    }
   }
 
   /**
{code}
the query runs correctly again, but this time the aggregate expression is NOT CSE'd anymore, done consistently for both aggregate expressions and result expressions:
{code}
AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[], functions=[max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false))), max(transform(array(id#0L), lambdafunction(lambda x#2L, lambda x#2L, false)))])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=11]
      +- SortAggregate(key=[], functions=[partial_max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false))), partial_max(transform(array(id#0L), lambdafunction(lambda x#2L, lambda x#2L, false)))])
         +- Range (0, 2, step=1, splits=16)
{code}
and for this particular case, the CSE that used to take place was actually okay, so losing CSE here means performance regression.",rednaxelafx,rednaxelafx,Major,Resolved,Fixed,18/Mar/23 01:04,21/Mar/23 13:28
Bug,SPARK-42852,13529090,Revert NamedLambdaVariable related changes from EquivalentExpressions,See discussion https://github.com/apache/spark/pull/40473#issuecomment-1474848224,petertoth,petertoth,Major,Resolved,Fixed,18/Mar/23 15:51,20/Mar/23 19:08
Bug,SPARK-42885,13529477,Upgrade `kubernetes-client` to 6.5.1,,dongjoon,dongjoon,Minor,Resolved,Fixed,21/Mar/23 16:14,21/Mar/23 20:53
Bug,SPARK-42899,13529674,DataFrame.to(schema) fails when it contains non-nullable nested field in nullable field,"{{DataFrame.to(schema)}} fails when it contains non-nullable nested field in nullable field:
{code:scala}
scala> val df = spark.sql(""VALUES (1, STRUCT(1 as i)), (NULL, NULL) as t(a, b)"")
df: org.apache.spark.sql.DataFrame = [a: int, b: struct<i: int>]
scala> df.printSchema()
root
 |-- a: integer (nullable = true)
 |-- b: struct (nullable = true)
 |    |-- i: integer (nullable = false)

scala> df.to(df.schema)
org.apache.spark.sql.AnalysisException: [NULLABLE_COLUMN_OR_FIELD] Column or field `b`.`i` is nullable while it's required to be non-nullable.
{code}",ueshin,ueshin,Major,Resolved,Fixed,22/Mar/23 21:53,23/Mar/23 02:16
Bug,SPARK-42906,13529709,Replace a starting digit with `x` in resource name prefix,,chengpan,chengpan,Major,Resolved,Fixed,23/Mar/23 07:16,27/Mar/23 22:35
Bug,SPARK-42921,13530047,SQLQueryTestSuite test failed with `SPARK_ANSI_SQL_MODE=true`,"Run 
{code:java}
SPARK_ANSI_SQL_MODE=true build/sbt ""sql/testOnly org.apache.spark.sql.SQLQueryTestSuite"" {code}
{code:java}
[info] - timestampNTZ/datetime-special.sql_analyzer_test *** FAILED *** (11 milliseconds)
[info]   timestampNTZ/datetime-special.sql_analyzer_test
[info]   Expected ""...date(999999, 3, 18, [false) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, fals]e) AS make_date(-1, ..."", but got ""...date(999999, 3, 18, [true) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, tru]e) AS make_date(-1, ..."" Result did not match for query #1
[info]   select make_date(999999, 3, 18), make_date(-1, 1, 28) (SQLQueryTestSuite.scala:777)
[info]   org.scalatest.exceptions.TestFailedException: {code}",LuciferYang,LuciferYang,Major,Resolved,Fixed,25/Mar/23 02:45,27/Mar/23 02:20
Bug,SPARK-42922,13530074,"Use SecureRandom, instead of Random in security sensitive contexts","Most uses of Random in spark are either in test cases or where we need a pseudo random number which is repeatable.
The following are usages where moving from Random to SecureRandom would be useful

a) HttpAuthUtils.createCookieToken
b) ThriftHttpServlet.RAN",mridulm80,mridulm80,Minor,Resolved,Fixed,25/Mar/23 16:15,28/Mar/23 03:49
Bug,SPARK-42928,13530180,Make resolvePersistentFunction synchronized,Make resolvePersistentFunction synchronized,allisonwang-db,allisonwang-db,Major,Resolved,Fixed,27/Mar/23 05:46,28/Mar/23 08:43
Bug,SPARK-42936,13530280,Unresolved having at the end of analysis when using with LCA with the having clause that can be resolved directly by its child Aggregate,"{code:java}
select sum(value1) as total_1, total_1
from values(1, 'name', 100, 50) AS data(id, name, value1, value2)
having total_1 > 0

SparkException: [INTERNAL_ERROR] Found the unresolved operator: 'UnresolvedHaving (total_1#353L > cast(0 as bigint)) {code}
To trigger the issue, the having condition need to be (can be resolved by) an attribute in the select.
Without the LCA {{{}total_1{}}}, the query works fine.",xyyu,xyyu,Major,Resolved,Fixed,27/Mar/23 17:54,28/Mar/23 08:42
Bug,SPARK-42937,13530284,Join with subquery in condition can fail with wholestage codegen and adaptive execution disabled,"The below left outer join gets an error:
{noformat}
create or replace temp view v1 as
select * from values
(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1),
(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2),
(3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
as v1(key, value1, value2, value3, value4, value5, value6, value7, value8, value9, value10);

create or replace temp view v2 as
select * from values
(1, 2),
(3, 8),
(7, 9)
as v2(a, b);

create or replace temp view v3 as
select * from values
(3),
(8)
as v3(col1);

set spark.sql.codegen.maxFields=10; -- let's make maxFields 10 instead of 100
set spark.sql.adaptive.enabled=false;

select *
from v1
left outer join v2
on key = a
and key in (select col1 from v3);
{noformat}
The join fails during predicate codegen:
{noformat}
23/03/27 12:24:12 WARN Predicate: Expr codegen error and falling back to interpreter mode
java.lang.IllegalArgumentException: requirement failed: input[0, int, false] IN subquery#34 has not finished
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.sql.execution.InSubqueryExec.prepareResult(subquery.scala:144)
	at org.apache.spark.sql.execution.InSubqueryExec.doGenCode(subquery.scala:156)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:201)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:196)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressions$2(CodeGenerator.scala:1278)
	at scala.collection.immutable.List.map(List.scala:293)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressions(CodeGenerator.scala:1278)
	at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:41)
	at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.generate(GeneratePredicate.scala:33)
	at org.apache.spark.sql.catalyst.expressions.Predicate$.createCodeGeneratedObject(predicates.scala:73)
	at org.apache.spark.sql.catalyst.expressions.Predicate$.createCodeGeneratedObject(predicates.scala:70)
	at org.apache.spark.sql.catalyst.expressions.CodeGeneratorWithInterpretedFallback.createObject(CodeGeneratorWithInterpretedFallback.scala:51)
	at org.apache.spark.sql.catalyst.expressions.Predicate$.create(predicates.scala:86)
	at org.apache.spark.sql.execution.joins.HashJoin.boundCondition(HashJoin.scala:146)
	at org.apache.spark.sql.execution.joins.HashJoin.boundCondition$(HashJoin.scala:140)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.boundCondition$lzycompute(BroadcastHashJoinExec.scala:40)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.boundCondition(BroadcastHashJoinExec.scala:40)
{noformat}
It fails again after fallback to interpreter mode:
{noformat}
23/03/27 12:24:12 ERROR Executor: Exception in task 2.0 in stage 2.0 (TID 7)
java.lang.IllegalArgumentException: requirement failed: input[0, int, false] IN subquery#34 has not finished
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.sql.execution.InSubqueryExec.prepareResult(subquery.scala:144)
	at org.apache.spark.sql.execution.InSubqueryExec.eval(subquery.scala:151)
	at org.apache.spark.sql.catalyst.expressions.InterpretedPredicate.eval(predicates.scala:52)
	at org.apache.spark.sql.execution.joins.HashJoin.$anonfun$boundCondition$2(HashJoin.scala:146)
	at org.apache.spark.sql.execution.joins.HashJoin.$anonfun$boundCondition$2$adapted(HashJoin.scala:146)
	at org.apache.spark.sql.execution.joins.HashJoin.$anonfun$outerJoin$1(HashJoin.scala:205)
{noformat}
Both the predicate codegen and the evaluation fail for the same reason: {{PlanSubqueries}} creates {{InSubqueryExec}} with {{shouldBroadcast=false}}. The driver waits for the subquery to finish, but it's the executor that uses the results of the subquery (for predicate codegen or evaluation). Because {{shouldBroadcast}} is set to false, the result is stored in a transient field ({{InSubqueryExec#result}}), so the result of the subquery is not serialized when the {{InSubqueryExec}} instance is sent to the executor.

When wholestage codegen is enabled, the predicate codegen happens on the driver, so the subquery's result is available. When adaptive execution is enabled, {{PlanAdaptiveSubqueries}} always sets {{shouldBroadcast=true}}, so the subquery's result is available on the executor, if needed.
",bersprockets,bersprockets,Major,Resolved,Fixed,27/Mar/23 19:45,28/Mar/23 12:41
Bug,SPARK-42957,13530543,`release-build.sh` should not remove SBOM artifacts,,dongjoon,dongjoon,Blocker,Resolved,Fixed,29/Mar/23 05:30,29/Mar/23 06:30
Bug,SPARK-42967,13530669,Fix SparkListenerTaskStart.stageAttemptId when a task is started after the stage is cancelled,"When a task is started after the stage is cancelled, the stageAttemptId field in  SparkListenerTaskStart event is set to -1. This could lead to unexpected problem for subscribers of SparkListener because -1 is not a legal stageAttemptId.",jiangxb1987,jiangxb1987,Major,Resolved,Fixed,29/Mar/23 18:26,30/Mar/23 22:49
Bug,SPARK-42971,13530706,"When processing the WorkDirCleanup event, if appDirs is empty, should print workdir ","          val appDirs = workDir.listFiles()
          if (appDirs == null) {
            throw new IOException(
              s""ERROR: Failed to list files in ${appDirs.mkString(""dirs("", "", "", "")"")}"")
          }

 

Otherwise, npe will be thrown here when appDirs is null

 ",LuciferYang,LuciferYang,Major,Resolved,Fixed,30/Mar/23 00:48,30/Mar/23 04:53
Bug,SPARK-42974,13530721,Restore `Utils#createTempDir` use  `ShutdownHookManager.registerShutdownDeleteDir` to cleanup tempDir,,LuciferYang,LuciferYang,Minor,Resolved,Fixed,30/Mar/23 04:45,04/Apr/23 07:27
Bug,SPARK-42978,13530739, Derby&PG: RENAME cannot qualify a new-table-Name with a schema-Name.,https://db.apache.org/derby/docs/10.2/ref/rrefnewtablename.html#rrefnewtablename,yao,yao,Major,Resolved,Fixed,30/Mar/23 06:26,31/Mar/23 08:15
Bug,SPARK-42987,13530882,Correct code highlights in SQL protobuf documentation,"Correct code highlights in SQL protobuf documentation.

Some of code highlights was in different format and not in markdown pattern",lucaspompeun,lucaspompeun,Major,Resolved,Fixed,31/Mar/23 01:13,10/Apr/23 03:24
Bug,SPARK-43004,13531144,vendor==vendor typo in ResourceRequest.equals(),"vendor == vendor is always true, this is likely to be a typo.

We should fix `vendor == vendor` with `that.vendor == vendor`, and `discoveryScript == discoveryScript` with `that.discoveryScript == discoveryScript`.",tienhoayu,tienhoayu,Minor,Resolved,Fixed,03/Apr/23 03:03,03/Apr/23 03:38
Bug,SPARK-43005,13531146,`v is v >= 0` typo in pyspark/pandas/config.py,"By comparing compute.isin_limit and plotting.max_rows, {{v is v}} is likely to be a typo.

We should fix {{v is v >= 0}} with {{{}v >= 0{}}}.",tienhoayu,tienhoayu,Minor,Resolved,Fixed,03/Apr/23 03:11,03/Apr/23 13:25
Bug,SPARK-43006,13531147,self.deserialized == self.deserialized typo in StorageLevel __eq__(),"We should fix {{self.deserialized == self.deserialized}} with {{self.deserialized == other.deserialized}}

The original expression is always True, which is likely to be a typo.",tienhoayu,tienhoayu,Minor,Resolved,Fixed,03/Apr/23 03:16,03/Apr/23 13:27
Bug,SPARK-43021,13531368,Shuffle happens when Coalesce Buckets should occur,"h1. What I did

I define the following code:

{{from pyspark.sql import SparkSession}}

{{spark = (}}
{{  SparkSession}}
{{    .builder}}
{{    .appName(""Bucketing"")}}
{{    .master(""local[4]"")}}
{{    .config(""spark.sql.bucketing.coalesceBucketsInJoin.enabled"", True)}}
{{    .config(""spark.sql.autoBroadcastJoinThreshold"", ""-1"")}}
{{    .getOrCreate()}}
{{)}}

{{df1 = spark.range(0, 100)}}
{{df2 = spark.range(0, 100, 2)}}

{{df1.write.bucketBy(4, ""id"").mode(""overwrite"").saveAsTable(""t1"")}}
{{df2.write.bucketBy(2, ""id"").mode(""overwrite"").saveAsTable(""t2"")}}

{{t1 = spark.table(""t1"")}}
{{t2 = spark.table(""t2"")}}

{{t2.join(t1, ""id"").explain()}}

h1. What happened

There is an Exchange node in the join plan

h1. What is expected

The plan should not contain any Exchange/Shuffle nodes, because {{t1}}'s number of buckets is 4 and {{t2}}'s number of buckets is 2, and their ratio is 2 which is less than 4 ({{spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio}}) and [CoalesceBucketsInJoin|https://github.com/apache/spark/blob/c9878a212958bc54be529ef99f5e5d1ddf513ec8/sql/core/src/main/scala/org/apache/spark/sql/execution/bucketing/CoalesceBucketsInJoin.scala] should be applied",Zing,neshkeev,Minor,Resolved,Fixed,04/Apr/23 02:35,14/Apr/23 03:20
Bug,SPARK-43030,13531456,deduplicate relations with metadata columns,,cloud_fan,cloud_fan,Major,Resolved,Fixed,04/Apr/23 14:24,07/Apr/23 03:47
Bug,SPARK-43041,13531648,Restore constructors of exceptions for compatibility in connector API,"Thanks [~aokolnychyi] for raising the issue as shown below:
{quote}
I have a question about changes to exceptions used in the public connector API, such as NoSuchTableException and TableAlreadyExistsException.

I consider those as part of the public Catalog API (TableCatalog uses them in method definitions). However, it looks like PR #37887 has changed them in an incompatible way. Old constructors accepting Identifier objects got removed. The only way to construct such exceptions is either by passing database and table strings or Scala Seq. Shall we add back old constructors to avoid breaking connectors?
{quote}
We should restore constructors of those exceptions to preserve the compatibility in connector API.",aokolnychyi,XinrongM,Blocker,Resolved,Fixed,05/Apr/23 19:03,06/Apr/23 05:26
Bug,SPARK-43049,13531696,Use CLOB instead of VARCHAR(255) for StringType for Oracle jdbc,,yao,yao,Major,Resolved,Fixed,06/Apr/23 05:39,07/Apr/23 03:30
Bug,SPARK-43050,13531751,Fix construct aggregate expressions by replacing grouping functions,"
{code:sql}
CREATE TEMPORARY VIEW grouping AS SELECT * FROM VALUES
  (""1"", ""2"", ""3"", 1),
  (""4"", ""5"", ""6"", 1),
  (""7"", ""8"", ""9"", 1)
  as grouping(a, b, c, d);
{code}

{noformat}
spark-sql (default)> SELECT CASE WHEN a IS NULL THEN count(b) WHEN b IS NULL THEN count(c) END
                   > FROM grouping
                   > GROUP BY GROUPING SETS (a, b, c);
[MISSING_AGGREGATION] The non-aggregating expression ""b"" is based on columns which are not participating in the GROUP BY clause.
{noformat}",yumwang,yumwang,Major,Resolved,Fixed,06/Apr/23 14:20,15/Apr/23 02:22
Bug,SPARK-43052,13531757,Handle stacktrace with null file name in event log,,warrenzhu25,warrenzhu25,Minor,Resolved,Fixed,06/Apr/23 15:15,27/Apr/23 04:33
Bug,SPARK-43067,13531847,Error class resource file in Kafka connector is misplaced,"SPARK-41387 adopted error class framework in Kafka connector. Since it's a connector, the error class file is intentionally put to the Kafka connector module instead of being added to the Spark's central error class file.

Unfortunately I've figured out somehow that the place is wrong. It should be placed to the resources directory in source, not test. The problem cannot be captured in test suite as it's available in test artifact, but I can trigger the problem via adding Kafka connector jar into classpath and initialize KafkaExceptions object.

Hopefully the blast radius of the problem is trivial as Kafka connector uses error class only for assertions which should not be triggered unless some accident happens e.g. topic is deleted and recreated while the streaming query is running with Trigger.Available.",kabhwan,kabhwan,Major,Resolved,Fixed,07/Apr/23 10:17,08/Apr/23 22:19
Bug,SPARK-43069,13531901,Use `sbt-eclipse` instead of `sbteclipse-plugin`,"Since SPARK-34959, Apache Spark 3.2+ uses SBT 1.5.0.

And, SBT 1.4+ can use `set-eclipse` instead of old `sbteclipse-plugin`.
- https://github.com/sbt/sbt-eclipse/releases",dongjoon,dongjoon,Major,Resolved,Fixed,07/Apr/23 18:57,07/Apr/23 19:55
Bug,SPARK-43077,13532012,Improve the error message of UNRECOGNIZED_SQL_TYPE,"UNRECOGNIZED_SQL_TYPE prints the jdbc type id in the error message currently. This is difficult for spark users to understand the meaning of this kind of error, especially when the type id is from a vendor extension.

For example, 
{code:java}
 org.apache.spark.SparkSQLException: Unrecognized SQL type -102{code}",yao,yao,Major,Resolved,Fixed,10/Apr/23 02:49,11/Apr/23 02:47
Bug,SPARK-43093,13532126,"Test case ""Add a directory when spark.sql.legacy.addSingleFileInAddFile set to false"" should use random directories for testing",,LuciferYang,LuciferYang,Minor,Resolved,Fixed,11/Apr/23 05:43,12/Apr/23 02:23
Bug,SPARK-43094,13532168,Array Insert Should Throw On 'Pos' Value 0,"Wanted to start a discussion about the following comment:

[https://github.com/apache/spark/pull/38867/files#r1157449697]

If a 'pos' of 0 is provided, the code here operates as if the user provided a 1. This could be confusing to users (two incides now overlap and lead to the same result, which is logically a little strange), but perhaps more compellingly, the other functions relying on a provided index, such as elementAt [here|https://github.com/Daniel-Davies/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L2344], will raise an exception if provided with an index of 0.",,ddavies1,Minor,Resolved,Fixed,11/Apr/23 11:11,11/Apr/23 11:32
Bug,SPARK-43095,13532191,Avoid Once strategy's idempotence is broken for batch: Infer Filters,,yumwang,yumwang,Major,Resolved,Fixed,11/Apr/23 13:27,15/Apr/23 01:25
Bug,SPARK-43099,13532248,"`Class.getCanonicalName` return null for anonymous class on JDK15+, impacting function registry","On JDK15+, lambda and method references are implemented using hidden classes ([https://openjdk.org/jeps/371)] According to the JEP, 
{quote}{{Class::getCanonicalName}} returns {{{}null{}}}, indicating the hidden class has no canonical name. (Note that the {{Class}} object for an anonymous class in the Java language has the same behavior.)
{quote}

This means [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala#L53] will always be null.

 

This can be fixed by replacing `getCanonicalName` with `getName`

 ",alexjinghn,alexjinghn,Major,Resolved,Fixed,11/Apr/23 22:19,17/Apr/23 03:22
Bug,SPARK-43113,13532412,Codegen error when full outer join's bound condition has multiple references to the same stream-side column,"Example # 1 (sort merge join):
{noformat}
create or replace temp view v1 as
select * from values
(1, 1),
(2, 2),
(3, 1)
as v1(key, value);

create or replace temp view v2 as
select * from values
(1, 22, 22),
(3, -1, -1),
(7, null, null)
as v2(a, b, c);

select *
from v1
full outer join v2
on key = a
and value > b
and value > c;
{noformat}
The join's generated code causes the following compilation error:
{noformat}
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 277, Column 9: Redefinition of local variable ""smj_isNull_7""
{noformat}
Example #2 (shuffle hash join):
{noformat}
select /*+ SHUFFLE_HASH(v2) */ *
from v1
full outer join v2
on key = a
and value > b
and value > c;
{noformat}
The shuffle hash join's generated code causes the following compilation error:
{noformat}
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 174, Column 5: Redefinition of local variable ""shj_value_1"" 
{noformat}
With default configuration, both queries end up succeeding, since Spark falls back to running each query with whole-stage codegen disabled.

The issue happens only when the join's bound condition refers to the same stream-side column more than once.",bersprockets,bersprockets,Major,Resolved,Fixed,12/Apr/23 22:33,24/Apr/23 00:59
Bug,SPARK-43119,13532435,Support Get SQL Keywords Dynamically,Implements the JDBC standard API and an auxiliary function,yao,yao,Major,Resolved,Fixed,13/Apr/23 03:57,21/Apr/23 02:24
Bug,SPARK-43123,13532483,special internal field metadata should not be leaked to catalogs,,cloud_fan,cloud_fan,Major,Resolved,Fixed,13/Apr/23 11:46,14/Apr/23 09:09
Bug,SPARK-43124,13532488,Dataset.show should not trigger job execution on CommandResults,,petertoth,petertoth,Major,Resolved,Fixed,13/Apr/23 12:32,21/Apr/23 00:33
Bug,SPARK-43125,13532493,Connect Server Can't Handle Exception With Null Message Normally,"!image-2023-04-13-21-05-43-617.png|width=626,height=310!!image-2023-04-13-21-05-16-994.png|width=621,height=294!

When Server Throw Exception which without message Like NPE. The setMessage method will report NPE again. But can't throw to client.",fanjia,fanjia,Critical,Resolved,Fixed,13/Apr/23 13:04,14/Apr/23 03:46
Bug,SPARK-43126,13532498,mark two Hive UDF expressions as stateful,,cloud_fan,cloud_fan,Major,Resolved,Fixed,13/Apr/23 13:29,14/Apr/23 00:50
Bug,SPARK-43138,13532606,ClassNotFoundException during RDD block replication/migration,"During RDD block migration during decommissioning we are seeing `ClassNotFoundException` on the receiving Executor. This seems to happen when the blocks contain classes that are from the user jars.
```
2023-04-08 04:15:11,791 ERROR server.TransportRequestHandler: Error while invoking RpcHandler#receive() on RPC id 6425687122551756860
java.lang.ClassNotFoundException: com.class.from.user.jar.ClassName
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
    at java.base/java.lang.Class.forName0(Native Method)
    at java.base/java.lang.Class.forName(Class.java:398)
    at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:71)
    at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2003)
    at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)
    at java.base/java.io.ObjectInputStream.readClass(ObjectInputStream.java:1833)
    at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1658)
    at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)
    at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)
    at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)
    at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)
    at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)
    at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)
    at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)
    at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)
    at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)
    at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)
    at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)
    at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:123)
    at org.apache.spark.network.netty.NettyBlockRpcServer.deserializeMetadata(NettyBlockRpcServer.scala:180)
    at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:119)
    at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:163)
    at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)
    at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
    at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)
    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:829)
```",eejbyfeldt,eejbyfeldt,Major,Resolved,Fixed,14/Apr/23 06:06,11/May/23 13:25
Bug,SPARK-43140,13532638,Override computeStats in DummyLeafNode,,yumwang,yumwang,Major,Resolved,Fixed,14/Apr/23 09:19,17/Apr/23 03:17
Bug,SPARK-43141,13532652,Ignore generated Java files in checkstyle,Files such as {{.../spark/core/target/scala-2.12/src_managed/main/org/apache/spark/status/protobuf/StoreTypes.java}} are checked in checkstyle. We shouldn't check them in the linter.,gurwls223,gurwls223,Major,Resolved,Fixed,14/Apr/23 11:36,17/Apr/23 02:57
Bug,SPARK-43142,13532662,DSL expressions fail on attribute with special characters,"Expressions on implicitly converted attributes fail if the attributes have names containing special characters. They fail even if the attributes are backtick-quoted:
{code:java}
scala> import org.apache.spark.sql.catalyst.dsl.expressions._
import org.apache.spark.sql.catalyst.dsl.expressions._

scala> ""`slashed/col`"".attr
res0: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute = 'slashed/col

scala> ""`slashed/col`"".attr.asc
org.apache.spark.sql.catalyst.parser.ParseException:
mismatched input '/' expecting {<EOF>, '.', '-'}(line 1, pos 7)

== SQL ==
slashed/col
-------^^^
{code}",rshkv,rshkv,Major,Resolved,Fixed,14/Apr/23 13:11,25/Apr/23 10:23
Bug,SPARK-43157,13532807,TreeNode tags can become corrupted and hang driver when the dataset is cached,"If a cached dataset is used by multiple other datasets materialized in separate threads it can corrupt the TreeNode.tags map in any of the cached plan nodes. This will hang the driver forever. This happens because TreeNode.tags is not thread-safe. How this happens:
 # Multiple datasets are materialized at the same time in different threads that reference the same cached dataset
 # AdaptiveSparkPlanExec.onUpdatePlan will call ExplainMode.fromString
 # ExplainUtils uses the TreeNode.tags map to store the operator Id for every node in the plan. This is usually okay because the plan is cloned. When there is an InMemoryScanExec the InMemoryRelation.cachedPlan is not cloned so multiple threads can set the operator Id.

Making the TreeNode.tags field thread-safe does not solve this problem because there is still a correctness issue. The threads may be overwriting each other's operator Ids, which could be different.

Example stack trace of the infinite loop:
{code:scala}
scala.collection.mutable.HashTable.resize(HashTable.scala:265)
scala.collection.mutable.HashTable.addEntry0(HashTable.scala:158)
scala.collection.mutable.HashTable.findOrAddEntry(HashTable.scala:170)
scala.collection.mutable.HashTable.findOrAddEntry$(HashTable.scala:167)
scala.collection.mutable.HashMap.findOrAddEntry(HashMap.scala:44)
scala.collection.mutable.HashMap.put(HashMap.scala:126)
scala.collection.mutable.HashMap.update(HashMap.scala:131)
org.apache.spark.sql.catalyst.trees.TreeNode.setTagValue(TreeNode.scala:108)
org.apache.spark.sql.execution.ExplainUtils$.setOpId$1(ExplainUtils.scala:134)
…
org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:175)
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.onUpdatePlan(AdaptiveSparkPlanExec.scala:662){code}
Example to show the cachedPlan object is not cloned:
{code:java}
import org.apache.spark.sql.execution.SparkPlan
import org.apache.spark.sql.execution.columnar.InMemoryTableScanExec
import spark.implicits._

def findCacheOperator(plan: SparkPlan): Option[InMemoryTableScanExec] = {
  if (plan.isInstanceOf[InMemoryTableScanExec]) {
    Some(plan.asInstanceOf[InMemoryTableScanExec])
  } else if (plan.children.isEmpty && plan.subqueries.isEmpty) {
    None
  } else {
    (plan.subqueries.flatMap(p => findCacheOperator(p)) ++
      plan.children.flatMap(findCacheOperator)).headOption
  }
}

val df = spark.range(10).filter($""id"" < 100).cache()
val df1 = df.limit(1)
val df2 = df.limit(1)

// Get the cache operator (InMemoryTableScanExec) in each plan
val plan1 = findCacheOperator(df1.queryExecution.executedPlan).get
val plan2 = findCacheOperator(df2.queryExecution.executedPlan).get

// Check if InMemoryTableScanExec references point to the same object
println(plan1.eq(plan2))
// returns false// Check if InMemoryRelation references point to the same object

println(plan1.relation.eq(plan2.relation))
// returns false

// Check if the cached SparkPlan references point to the same object
println(plan1.relation.cachedPlan.eq(plan2.relation.cachedPlan))
// returns true
// This shows that the cloned plan2 still has references to the original plan1 {code}",robreeves,robreeves,Major,Resolved,Fixed,16/Apr/23 22:18,18/May/23 06:25
Bug,SPARK-43158,13532809,Set upperbound of pandas version in binder integrations,"{code}
df.toPandas
{code}

fails with

{code}

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[14], line 1
----> 1 df.toPandas()

File /srv/conda/envs/notebook/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:251, in PandasConversionMixin.toPandas(self)
    248 should_check_timedelta = is_timedelta64_dtype(t) and len(pdf) == 0
    250 if (t is not None and not is_timedelta64_dtype(t)) or should_check_timedelta:
--> 251     series = series.astype(t, copy=False)
    253 with catch_warnings():
    254     from pandas.errors import PerformanceWarning

File /srv/conda/envs/notebook/lib/python3.10/site-packages/pandas/core/generic.py:6324, in NDFrame.astype(self, dtype, copy, errors)
   6317     results = [
   6318         self.iloc[:, i].astype(dtype, copy=copy)
   6319         for i in range(len(self.columns))
   6320     ]
   6322 else:
   6323     # else, only a single dtype is given
-> 6324     new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)
   6325     return self._constructor(new_data).__finalize__(self, method=""astype"")
   6327 # GH 33113: handle empty frame or series

File /srv/conda/envs/notebook/lib/python3.10/site-packages/pandas/core/internals/managers.py:451, in BaseBlockManager.astype(self, dtype, copy, errors)
    448 elif using_copy_on_write():
    449     copy = False
--> 451 return self.apply(
    452     ""astype"",
    453     dtype=dtype,
    454     copy=copy,
    455     errors=errors,
    456     using_cow=using_copy_on_write(),
    457 )

File /srv/conda/envs/notebook/lib/python3.10/site-packages/pandas/core/internals/managers.py:352, in BaseBlockManager.apply(self, f, align_keys, **kwargs)
    350         applied = b.apply(f, **kwargs)
    351     else:
--> 352         applied = getattr(b, f)(**kwargs)
    353     result_blocks = extend_blocks(applied, result_blocks)
    355 out = type(self).from_blocks(result_blocks, self.axes)
{code}",gurwls223,gurwls223,Major,Resolved,Fixed,17/Apr/23 00:48,17/Apr/23 04:00
Bug,SPARK-43174,13533018,Fix SparkSQLCLIDriver completer,,yumwang,yumwang,Major,Resolved,Fixed,18/Apr/23 09:52,22/Apr/23 00:16
Bug,SPARK-43190,13533216,ListQuery.childOutput should be consistent with child output,,cloud_fan,cloud_fan,Major,Resolved,Fixed,19/Apr/23 12:42,20/Apr/23 07:14
Bug,SPARK-43192,13533218,Spark connect's user agent validations are too restrictive,"The current restriction on allowed charset and length are too restrictive

 

https://github.com/apache/spark/blob/cac6f58318bb84d532f02d245a50d3c66daa3e4b/python/pyspark/sql/connect/client.py#L274-L275",nija,nija,Major,Resolved,Fixed,19/Apr/23 12:43,21/Apr/23 06:23
Bug,SPARK-43198,13533265,"Fix ""Could not initialise class ammonite..."" error when using filter","When
{code:java}
spark.range(10).filter(n => n % 2 == 0).collectAsList()`{code}
 is run in the ammonite REPL (Spark Connect), the following error is thrown:
{noformat}
io.grpc.StatusRuntimeException: UNKNOWN: ammonite/repl/ReplBridge$
  io.grpc.Status.asRuntimeException(Status.java:535)
  io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
  org.apache.spark.sql.connect.client.SparkResult.org$apache$spark$sql$connect$client$SparkResult$$processResponses(SparkResult.scala:62)
  org.apache.spark.sql.connect.client.SparkResult.length(SparkResult.scala:114)
  org.apache.spark.sql.connect.client.SparkResult.toArray(SparkResult.scala:131)
  org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2687)
  org.apache.spark.sql.Dataset.withResult(Dataset.scala:3088)
  org.apache.spark.sql.Dataset.collect(Dataset.scala:2686)
  org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:2700)
  ammonite.$sess.cmd0$.<init>(cmd0.sc:1)
  ammonite.$sess.cmd0$.<clinit>(cmd0.sc){noformat}",vicennial,vicennial,Major,Resolved,Fixed,19/Apr/23 16:07,26/Apr/23 16:13
Bug,SPARK-43199,13533269,Make InlineCTE idempotent,,petertoth,petertoth,Major,Resolved,Fixed,19/Apr/23 16:34,26/Apr/23 07:33
Bug,SPARK-43203,13533279,Fix DROP table behavior in session catalog,"DROP table behavior is not working correctly in 3.4.0 because we always invoke V1 drop logic if the identifier looks like a V1 identifier. This is a big blocker for external data sources that provide custom session catalogs.

See [here|https://github.com/apache/spark/pull/37879/files#r1170501180] for details.",fanjia,aokolnychyi,Major,Resolved,Fixed,19/Apr/23 19:16,27/Jun/23 22:32
Bug,SPARK-43208,13533312,IsolatedClassLoader should close barrier class InputStream after reading,,chengpan,chengpan,Major,Resolved,Fixed,20/Apr/23 05:30,20/Apr/23 16:53
Bug,SPARK-43237,13533654,Handle null exception message in event log,,warrenzhu25,warrenzhu25,Minor,Resolved,Fixed,22/Apr/23 17:48,29/Apr/23 04:15
Bug,SPARK-43239,13533659,Remove null_counts from info(),"df.info() is broken now. 
It prints 

TypeError                                 Traceback (most recent call last)
Cell In[12], line 1
----> 1 F05.info()

File /opt/spark/python/pyspark/pandas/frame.py:12167, in DataFrame.info(self, verbose, buf, max_cols, null_counts)
  12163     count_func = self.count
  12164     self.count = (  # type: ignore[assignment]
  12165         lambda: count_func()._to_pandas()  # type: ignore[assignment, misc, union-attr]
  12166     )
> 12167     return pd.DataFrame.info(
  12168         self,  # type: ignore[arg-type]
  12169         verbose=verbose,
  12170         buf=buf,
  12171         max_cols=max_cols,
  12172         memory_usage=False,
  12173         null_counts=null_counts,
  12174     )
  12175 finally:
  12176     del self._data

TypeError: DataFrame.info() got an unexpected keyword argument 'null_counts'
",bjornjorgensen,bjornjorgensen,Major,Resolved,Fixed,22/Apr/23 20:13,24/Apr/23 00:14
Bug,SPARK-43240,13533667,df.describe() method may- return wrong result if the last RDD is RDD[UnsafeRow],"When calling the df.describe() method, the result  maybe wrong when the last RDD is RDD[UnsafeRow]. It is because the UnsafeRow will be released after the row is used. ",Jk_Self,Jk_Self,Major,Resolved,Fixed,23/Apr/23 02:50,26/Apr/23 09:25
Bug,SPARK-43249,13533794,df.sql() should send metrics back(),df.sql() does not return the metrics to the client when executed as a command.,grundprinzip-db,grundprinzip-db,Major,Resolved,Fixed,24/Apr/23 08:30,24/Apr/23 08:34
Bug,SPARK-43268,13533876,Use proper error classes when exceptions are constructed with a message,As discussed [here|https://github.com/apache/spark/pull/40679/files#r1159264585].,aokolnychyi,aokolnychyi,Major,Resolved,Fixed,24/Apr/23 16:57,25/Apr/23 00:25
Bug,SPARK-43281,13533987,Fix concurrent writer does not update file metrics,"It uses temp file path to get file status after commit task. However, the temp file has already moved to new path during commit task.",ulysses,ulysses,Major,Resolved,Fixed,25/Apr/23 12:31,16/May/23 01:42
Bug,SPARK-43285,13534031,ReplE2ESuite consistently fails with JDK 17,"[[Comment|https://github.com/apache/spark/pull/40675#discussion_r1174696470] from [~gurwls223]]

This test consistently fails with JDK 17:
{code:java}
[info] ReplE2ESuite:
[info] - Simple query *** FAILED *** (10 seconds, 4 milliseconds)
[info] java.lang.RuntimeException: REPL Timed out while running command: 
[info] spark.sql(""select 1"").collect()
[info] 
[info] Console output: 
[info] Error output: Compiling (synthetic)/ammonite/predef/ArgsPredef.sc
[info] at org.apache.spark.sql.application.ReplE2ESuite.runCommandsInShell(ReplE2ESuite.scala:87)
[info] at org.apache.spark.sql.application.ReplE2ESuite.$anonfun$new$1(ReplE2ESuite.scala:102)
[info] at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info] at org.scalatest.Transformer.apply(Transformer.scala:22)
[info] at org.scalatest.Transformer.apply(Transformer.scala:20)
[info] at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info] at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
[info] at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
[info] at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224){code}

[https://github.com/apache/spark/actions/runs/4780630672/jobs/8498505928#step:9:4647]
[https://github.com/apache/spark/actions/runs/4774942961/jobs/8488946907]
[https://github.com/apache/spark/actions/runs/4769162286/jobs/8479293802]
[https://github.com/apache/spark/actions/runs/4759278349/jobs/8458399201]
[https://github.com/apache/spark/actions/runs/4748319019/jobs/8434392414]",vicennial,vicennial,Major,Resolved,Fixed,25/Apr/23 16:16,25/Apr/23 19:11
Bug,SPARK-43292,13534111,ArtifactManagerSuite can't run using maven,"run
{code:java}
build/mvn  clean install -DskipTests -Phive 
build/mvn test -pl connector/connect/server {code}
ArtifactManagerSuite failed due to 

 
{code:java}
23/04/26 16:00:07.666 ScalaTest-main-running-DiscoverySuite ERROR Executor: Could not find org.apache.spark.repl.ExecutorClassLoader on classpath! {code}",LuciferYang,LuciferYang,Major,Resolved,Fixed,26/Apr/23 08:00,08/May/23 15:10
Bug,SPARK-43293,13534167,__qualified_access_only should be ignored in normal columns,,cloud_fan,cloud_fan,Major,Resolved,Fixed,26/Apr/23 14:25,27/Apr/23 02:55
Bug,SPARK-43298,13534218,predict_batch_udf with scalar input fails when batch size consists of a single value,"This is related to SPARK-42250.  For scalar inputs, the predict_batch_udf will fail if the batch size is 1:
{code:java}
import numpy as np
from pyspark.ml.functions import predict_batch_udf
from pyspark.sql.types import DoubleType

df = spark.createDataFrame([[1.0],[2.0]], schema=[""a""])

def make_predict_fn():
    def predict(inputs):
        return inputs
    return predict

identity = predict_batch_udf(make_predict_fn, return_type=DoubleType(), batch_size=1)
preds = df.withColumn(""preds"", identity(""a"")).collect()
{code}
fails with:
{code:java}
  File ""/.../spark/python/pyspark/worker.py"", line 869, in main
    process()
  File ""/.../spark/python/pyspark/worker.py"", line 861, in process
    serializer.dump_stream(out_iter, outfile)
  File ""/.../spark/python/pyspark/sql/pandas/serializers.py"", line 354, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File ""/.../spark/python/pyspark/sql/pandas/serializers.py"", line 86, in dump_stream
    for batch in iterator:
  File ""/.../spark/python/pyspark/sql/pandas/serializers.py"", line 347, in init_stream_yield_batches
    for series in iterator:
  File ""/.../spark/python/pyspark/worker.py"", line 555, in func
    for result_batch, result_type in result_iter:
  File ""/.../spark/python/pyspark/ml/functions.py"", line 818, in predict
    yield _validate_and_transform_prediction_result(
  File ""/.../spark/python/pyspark/ml/functions.py"", line 339, in _validate_and_transform_prediction_result
    if len(preds_array) != num_input_rows:
TypeError: len() of unsized object
{code}",leewyang,leewyang,Major,Resolved,Fixed,26/Apr/23 20:31,27/Apr/23 18:51
Bug,SPARK-43300,13534241,Cascade failure in Guava cache due to fate-sharing,"Guava cache is widely used in spark, however, it suffers from fate-sharing behavior: If there are multiple requests trying to access the same key in the {{cache}} at the same time when the key is not in the cache, Guava cache will block all requests and create the object only once. If the creation fails, all requests will fail immediately without retry. So we might see task failure due to irrelevant failure in other queries due to fate sharing.

This fate sharing behavior might lead to unexpected results in some situation.

We can wrap around Guava cache with a KeyLock to synchronize all requests with the same key, so they will run individually and fail as if they come one at a time.",liuzq12,liuzq12,Major,Resolved,Fixed,27/Apr/23 01:05,16/May/23 01:49
Bug,SPARK-43313,13534373,Adding missing default values for MERGE INSERT actions,,dtenedor,dtenedor,Major,Resolved,Fixed,28/Apr/23 00:08,18/May/23 18:39
Bug,SPARK-43329,13534580,driver and executors shared same Kubernetes PVC in Spark 3.4+,"I able to shared same PVC for spark 3.3. but on Spark 3.4 onward. i get below error.  I would like all the executors and driver to mount the same PVC. Is this a bug ? I don't want to use SPARK_EXECUTOR_ID or OnDemand because otherwise each of the executors will use an unique and separate PVC. 
 
Error message is ""should contain OnDemand or SPARK_EXECUTOR_ID when requiring multiple executors""
 
below is how I enabled it pvc in spark 3.3 and it works, but does not work in Spark 3.4

{code:sh}
spark.kubernetes.driver.volumes.persistentVolumeClaim.rwxpvc.options.claimName=rwxpvc 
--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.rwxpvc.mount.path=/opt/spark/work-dir 
--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.rwxpvc.options.claimName=rwxpvc 
--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.rwxpvc.mount.path=/opt/spark/work-dir 
 
{code}


 
 
 ",dcoliversun,comet,Major,Resolved,Fixed,01/May/23 03:25,07/May/23 01:48
Bug,SPARK-43330,13534593,Typo in sql-migration-guide.md,"There is a minor typo in [sql-migration-guide.md|#L154]

 
| - In Spark 3.2, `TRANSFORM` operator can support `ArrayType/MapType/StructType` without Hive SerDe, in this mode, we use `StructsToJosn` to convert `ArrayType/MapType/StructType` column to `STRING` and use `JsonToStructs` to parse `STRING` to `ArrayType/MapType/StructType`. In Spark 3.1, Spark just support case `ArrayType/MapType/StructType` column as `STRING` but can't support parse `STRING` to `ArrayType/MapType/StructType` output columns.|

 

StructsToJosn -> StructsToJson",kpark,kpark,Major,Resolved,Fixed,01/May/23 13:04,02/May/23 00:29
Bug,SPARK-43334,13534633,Error while serializing ExecutorPeakMetricsDistributions into API JSON response,"When we try to get the ExecutorPeakMetricsDistributions [through the API|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/status/api/v1/api.scala#L463] (/stages), there is a possibility of encountering an issue while serializing the StagesData into a JSON if the executor metrics are empty. 

 

The following error is thrown : 
{code:java}
Caused by: com.fasterxml.jackson.databind.JsonMappingException: -1 (through reference chain: scala.c
ollection.immutable.$colon$colon[0]->org.apache.spark.status.api.v1.StageData[""executorMetricsDistri
butions""]->org.apache.spark.status.api.v1.ExecutorMetricsDistributions[""peakMemoryMetrics""])
        at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.jav
a:390)
        at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.jav
a:349) {code}
This happens because the indices for the quartiles are populated incorrectly as -1 since the metrics itself are empty and this leads to this exception being thrown.",tejdeepg,tejdeepg,Major,Resolved,Fixed,01/May/23 21:29,24/May/23 23:25
Bug,SPARK-43337,13534646,Asc/desc arrow icons for sorting column does not get displayed in the table column,"The sorting icon is not displayed when the column is clicked to sort by asc/desc.

See attached image: (The index column is sort by asc order by the down arrow is not displayed)

!image (4).png!

This broke due to the upgrade from DataTables from 1.10.20 to 1.10.25. I have confirmed that version 1.13.2 in master branch does not have this problem. 
For reference, in 1.13.2, it looks like the following:
!Screen Shot 2023-05-01 at 10.39.00 PM.png!  ",maytasm,maytasm,Minor,Resolved,Fixed,02/May/23 05:35,05/May/23 19:08
Bug,SPARK-43340,13534698,Handle missing stack-trace field in eventlogs,"Recently I was testing with some 3.0.2 eventlogs.

The SHS-3.4+ does not interpret failed jobs/ failed SQLs correctly.

Instead it will list them as ""Incomplete/Active"" whereas it should be listed as ""Failed"".

The problem is due to missing fields in eventlogs generated by previous versions. In this case the eventlog does not have ""Stack Trace"" field which causes a NPE
{code:java}
{
   ""Event"":""SparkListenerJobEnd"",
   ""Job ID"":31,
   ""Completion Time"":1616171909785,
   ""Job Result"":{
      ""Result"":""JobFailed"",
      ""Exception"":{
         ""Message"":""Job aborted""
      }
   }
} {code}
*The SHS logfile*
{code:java}
23/05/01 21:57:16 INFO FsHistoryProvider: Parsing file:/tmp/nds_q86_fail_test to re-build UI...
23/05/01 21:57:17 ERROR ReplayListenerBus: Exception parsing Spark event log: file:/tmp/nds_q86_fail_test
java.lang.NullPointerException
    at org.apache.spark.util.JsonProtocol$JsonNodeImplicits.extractElements(JsonProtocol.scala:1589)
    at org.apache.spark.util.JsonProtocol$.stackTraceFromJson(JsonProtocol.scala:1558)
    at org.apache.spark.util.JsonProtocol$.exceptionFromJson(JsonProtocol.scala:1569)
    at org.apache.spark.util.JsonProtocol$.jobResultFromJson(JsonProtocol.scala:1423)
    at org.apache.spark.util.JsonProtocol$.jobEndFromJson(JsonProtocol.scala:967)
    at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:878)
    at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:865)
    at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:88)
    at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:59)
    at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$3(FsHistoryProvider.scala:1140)
    at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$3$adapted(FsHistoryProvider.scala:1138)
    at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2786)
    at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$1(FsHistoryProvider.scala:1138)
    at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$1$adapted(FsHistoryProvider.scala:1136)
    at scala.collection.immutable.List.foreach(List.scala:431)
    at org.apache.spark.deploy.history.FsHistoryProvider.parseAppEventLogs(FsHistoryProvider.scala:1136)
    at org.apache.spark.deploy.history.FsHistoryProvider.rebuildAppStore(FsHistoryProvider.scala:1117)
    at org.apache.spark.deploy.history.FsHistoryProvider.createInMemoryStore(FsHistoryProvider.scala:1355)
    at org.apache.spark.deploy.history.FsHistoryProvider.getAppUI(FsHistoryProvider.scala:345)
    at org.apache.spark.deploy.history.HistoryServer.getAppUI(HistoryServer.scala:199)
    at org.apache.spark.deploy.history.ApplicationCache.$anonfun$loadApplicationEntry$2(ApplicationCache.scala:163)
    at org.apache.spark.deploy.history.ApplicationCache.time(ApplicationCache.scala:134)
    at org.apache.spark.deploy.history.ApplicationCache.org$apache$spark$deploy$history$ApplicationCache$$loadApplicationEntry(ApplicationCache.scala:161)
    at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:55)
    at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:51)
    at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
    at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
    at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
    at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
    at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
    at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
    at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
    at org.apache.spark.deploy.history.ApplicationCache.get(ApplicationCache.scala:88)
    at org.apache.spark.deploy.history.ApplicationCache.withSparkUI(ApplicationCache.scala:100)
    at org.apache.spark.deploy.history.HistoryServer.org$apache$spark$deploy$history$HistoryServer$$loadAppUi(HistoryServer.scala:256)
    at org.apache.spark.deploy.history.HistoryServer$$anon$1.doGet(HistoryServer.scala:104)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:503)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:590)
    at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
    at org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
    at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
    at org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
    at org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
    at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
    at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
    at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
    at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
    at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
    at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:772)
    at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
    at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
    at org.sparkproject.jetty.server.Server.handle(Server.java:516)
    at org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
    at org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
    at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:479)
    at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
    at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
    at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)
    at org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
    at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
    at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
    at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
    at java.lang.Thread.run(Thread.java:750)
23/05/01 21:57:17 ERROR ReplayListenerBus: Malformed line #24368: {""Event"":""SparkListenerJobEnd"",""Job ID"":31,""Completion Time"":1616171909785,""Job Result"":{""Result"":""JobFailed"",""Exception"":
{""Message"":""Job aborted""}
}}
23/05/01 21:57:17 INFO FsHistoryProvider: Finished parsing file:/tmp/nds_q86_fail_test
 
{code}
 ",ahussein,ahussein,Major,Resolved,Fixed,02/May/23 14:52,05/May/23 23:04
Bug,SPARK-43342,13534704,Revert SPARK-39006 Show a directional error message for executor PVC dynamic allocation failure,"When using static PVC with Spark 3.4, spark PI example fails with the error below. Previous versions of Spark worked well.
{code:java}
23/04/26 13:22:02 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes for ResourceProfile Id: 0, target: 5, known: 0, sharedSlotFromPendingPods: 2147483647. 23/04/26 13:22:02 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script 23/04/26 13:22:02 ERROR ExecutorPodsSnapshotsStoreImpl: Going to stop due to IllegalArgumentException java.lang.IllegalArgumentException: PVC ClaimName: a1pvc should contain OnDemand or SPARK_EXECUTOR_ID when requiring multiple executors         at org.apache.spark.deploy.k8s.features.MountVolumesFeatureStep.checkPVCClaimName(MountVolumesFeatureStep.scala:135)         at org.apache.spark.deploy.k8s.features.MountVolumesFeatureStep.$anonfun$constructVolumes$4(MountVolumesFeatureStep.scala:75)         at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)         at scala.collection.Iterator.foreach(Iterator.scala:943)         at scala.collection.Iterator.foreach$(Iterator.scala:943)         at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)         at scala.collection.IterableLike.foreach(IterableLike.scala:74)         at scala.collection.IterableLike.foreach$(IterableLike.scala:73)         at scala.collection.AbstractIterable.foreach(Iterable.scala:56)         at scala.collection.TraversableLike.map(TraversableLike.scala:286)         at scala.collection.TraversableLike.map$(TraversableLike.scala:279)         at scala.collection.AbstractTraversable.map(Traversable.scala:108)         at org.apache.spark.deploy.k8s.features.MountVolumesFeatureStep.constructVolumes(MountVolumesFeatureStep.scala:58)         at org.apache.spark.deploy.k8s.features.MountVolumesFeatureStep.configurePod(MountVolumesFeatureStep.scala:35)         at org.apache.spark.scheduler.cluster.k8s.KubernetesExecutorBuilder.$anonfun$buildFromFeatures$5(KubernetesExecutorBuilder.scala:83)         at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)         at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)         at scala.collection.immutable.List.foldLeft(List.scala:91)         at org.apache.spark.scheduler.cluster.k8s.KubernetesExecutorBuilder.buildFromFeatures(KubernetesExecutorBuilder.scala:82)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$1(ExecutorPodsAllocator.scala:430)         at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.requestNewExecutors(ExecutorPodsAllocator.scala:417)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36(ExecutorPodsAllocator.scala:370)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36$adapted(ExecutorPodsAllocator.scala:363)         at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)         at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)         at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.onNewSnapshots(ExecutorPodsAllocator.scala:363)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3(ExecutorPodsAllocator.scala:134)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3$adapted(ExecutorPodsAllocator.scala:134)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.org$apache$spark$scheduler$cluster$k8s$ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber$$processSnapshotsInternal(ExecutorPodsSnapshotsStoreImpl.scala:143)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.processSnapshots(ExecutorPodsSnapshotsStoreImpl.scala:131)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl.$anonfun$addSubscriber$1(ExecutorPodsSnapshotsStoreImpl.scala:85)         at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)         at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)         at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)         at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)         at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)         at java.base/java.lang.Thread.run(Thread.java:833)   {code}
How to reproduce:
 # Create statically provisioned PV, for example nfs PV: [https://kubernetes.io/docs/concepts/storage/volumes/#nfs]
 # Create PVC that binds to PV above.
 # Run Spark PI example: $SPARK_HOME/bin/spark-submit --master k8s://kubernetes.default.svc --properties-file spark.properties $SPARK_HOME/examples/src/main/python/pi.py 10

spark.properties contents:
{code:java}
spark.executor.instances=5
spark.kubernetes.executor.volumes.persistentVolumeClaim.nfs1.mount.path=/isilon/mnts
spark.kubernetes.executor.volumes.persistentVolumeClaim.nfs1.mount.readOnly=false
spark.kubernetes.executor.volumes.persistentVolumeClaim.nfs1.options.claimName=a1pvc
spark.kubernetes.driver.volumes.persistentVolumeClaim.nfs1.options.claimName=a1pvc
spark.kubernetes.driver.volumes.persistentVolumeClaim.nfs1.mount.readOnly=false
spark.kubernetes.driver.volumes.persistentVolumeClaim.nfs1.mount.path=/isilon/mnts {code}",dcoliversun,ofrenkel,Major,Resolved,Fixed,02/May/23 15:24,07/May/23 01:27
Bug,SPARK-43343,13534735,Spark Streaming is not able to read a .txt file whose name has [] special character,"* For example, If a directory contains a following file:
/path/abc[123]
and users would load spark.readStream.format(""text"").load(""/path"") as stream input. It throws an exception, saying no matching path /path/abc[123]. Spark thinks abc[123] is a regex that only matches file named abc1, abc2 and abc3.

 * Upon investigation this is due to how we [getBatch|https://github.com/databricks/runtime/blob/3af402d23620a0952e151d96c3184d2233217c87/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala#L269] in the FileStreamSource. In `FileStreamSource` we already check file pattern matching and find all match file names. However, in DataSource we check for glob characters again and try to expend it [here|https://github.com/databricks/runtime/blob/3af402d23620a0952e151d96c3184d2233217c87/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L274].",siying,siying,Minor,Resolved,Fixed,02/May/23 18:28,09/May/23 03:55
Bug,SPARK-43349,13534765,Fix flaky test for `DataFrame` creation,some test of `test_creation_index` is not working properly in some envs.,itholic,itholic,Major,Resolved,Fixed,03/May/23 00:16,03/May/23 10:41
Bug,SPARK-43357,13534836,Spark AWS Glue date partition push down broken,"When using the following project: [https://github.com/awslabs/aws-glue-data-catalog-client-for-apache-hive-metastore]
To have glue supported as as a hive metastore for spark there is an issue when reading a date-partitioned data set. Writing is fine.
You get the following error: 


{quote}org.apache.hadoop.hive.metastore.api.InvalidObjectException: Unsupported expression '2023 - 05 - 03' (Service: AWSGlue; Status Code: 400; Error Code: InvalidInputException; Request ID: beed68c6-b228-442e-8783-52c25b9d2243; Proxy: null)
{quote}
 

A fix for this is making sure the date passed to glue is quoted",sdehaes,sdehaes,Major,Resolved,Fixed,03/May/23 14:03,10/May/23 07:13
Bug,SPARK-43359,13534872,DELETE from Hive table result in INTERNAL error,"spark-sql (default)> CREATE TABLE T1(c1 INT);
spark-sql (default)> DELETE FROM T1 WHERE c1 = 1;
[INTERNAL_ERROR] Unexpected table relation: HiveTableRelation [`spark_catalog`.`default`.`t1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [c1#3], Partition Cols: []]

org.apache.spark.SparkException: [INTERNAL_ERROR] Unexpected table relation: HiveTableRelation [`spark_catalog`.`default`.`t1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [c1#3], Partition Cols: []]
	at org.apache.spark.SparkException$.internalError(SparkException.scala:77)
	at org.apache.spark.SparkException$.internalError(SparkException.scala:81)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:310)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)
	at org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)",panbingkun,srielau,Minor,Resolved,Fixed,03/May/23 18:43,16/May/23 18:21
Bug,SPARK-43373,13534904,Revert [SPARK-39203][SQL] Rewrite table location to absolute URI based on database URI,,cloud_fan,cloud_fan,Major,Resolved,Fixed,04/May/23 02:56,04/May/23 02:57
Bug,SPARK-43378,13534986,SerializerHelper.deserializeFromChunkedBuffer leaks deserialization streams,The method SerializerHelper.deserializeFromChunkedBuffer leaks serializations stream. This can lead to huge performance regressions when using kryo serializer as the spark application can become bottlenecked on the driver creating expensive kryo objects that are then leaked as part of the deserialization stream,eejbyfeldt,eejbyfeldt,Major,Resolved,Fixed,04/May/23 12:59,06/Jun/23 07:04
Bug,SPARK-43380,13535060,Fix Avro data type conversion issues to avoid producing incorrect results,"We found the following issues with open-source Avro:
 * Interval types can be read as date or timestamp types that would lead to wildly different results
 * Decimal types can be read with lower precision, that leads to data being read as {{null}} instead of suggesting that a wider decimal format should be provided",zeruibao,zeruibao,Major,Resolved,Fixed,04/May/23 23:01,02/Jun/23 22:12
Bug,SPARK-43386,13535197,Improve list of suggested column/attributes in `UNRESOLVED_COLUMN.WITH_SUGGESTION` error class,Match the style of unresolved column/attribute when sorting list of suggested columns. If an unresolved column name is single-part identifier - use same style for suggested columns.,vli-databricks,vli-databricks,Major,Resolved,Fixed,05/May/23 17:13,12/May/23 05:39
Bug,SPARK-43398,13535310,Executor timeout should be max of idleTimeout rddTimeout shuffleTimeout,"When dynamic allocation enabled, Executor timeout should be max of idleTimeout, rddTimeout and shuffleTimeout.",warrenzhu25,warrenzhu25,Major,Resolved,Fixed,07/May/23 18:12,12/Jun/23 07:42
Bug,SPARK-43404,13535324,Filter current version while reusing sst files for RocksDB state store provider while uploading to DFS to prevent id mismatch,Filter current version while reusing sst files for RocksDB state store provider while uploading to DFS to prevent id mismatch,anishshri-db,anishshri-db,Major,Resolved,Fixed,08/May/23 03:59,09/Jun/23 11:50
Bug,SPARK-43413,13535421,IN subquery ListQuery has wrong nullability,"IN subquery expressions are incorrectly marked as non-nullable, even when they are actually nullable. They correctly check the nullability of the left-hand-side, but the right-hand-side of a IN subquery, the ListQuery, is currently defined with nullability = false always. This is incorrect and can lead to incorrect query transformations.

Example: (non_nullable_col IN (select nullable_col)) <=> TRUE . Here the IN expression returns NULL when the nullable_col is null, but our code marks it as non-nullable, and therefore SimplifyBinaryComparison transforms away the <=> TRUE, transforming the expression to non_nullable_col IN (select nullable_col) , which is an incorrect transformation because NULL values of nullable_col now cause the expression to yield NULL instead of FALSE.

This bug can potentially lead to wrong results, but in most cases this doesn't directly cause wrong results end-to-end, because IN subqueries are almost always transformed to semi/anti/existence joins in RewritePredicateSubquery, and this rewrite can also incorrectly discard NULLs, which is another bug. But we can observe it causing wrong behavior in unit tests, and it could easily lead to incorrect query results if there are changes to the surrounding context, so it should be fixed regardless.

This is a long-standing bug that has existed at least since 2016, as long as the ListQuery class has existed.",jchen5,jchen5,Major,Resolved,Fixed,08/May/23 21:12,16/May/23 01:41
Bug,SPARK-43422,13535475,Tags are lost on LogicalRelation when adding _metadata,The  AddMetadataColumns does not copy tags for the LogicalRelation when adding metadata output in addMetadataCol,olaky,olaky,Minor,Resolved,Fixed,09/May/23 06:45,10/May/23 08:17
Bug,SPARK-43425,13535499,Add TimestampNTZType to ColumnarBatchRow,,fokko,fokko,Major,Resolved,Fixed,09/May/23 09:55,11/May/23 00:45
Bug,SPARK-43441,13535700,makeDotNode should not fail when DeterministicLevel is absent,,qitan,qitan,Minor,Resolved,Fixed,10/May/23 21:22,11/May/23 04:20
Bug,SPARK-43471,13535866,Handle missing hadoopProperties and metricsProperties,,qitan,qitan,Minor,Resolved,Fixed,11/May/23 20:06,11/May/23 22:30
Bug,SPARK-43484,13535918,Kafka/Kinesis Assembly should not package hadoop-client-runtime,,chengpan,chengpan,Major,Resolved,Fixed,12/May/23 06:27,12/May/23 16:18
Bug,SPARK-43485,13535919,Confused errors from the DATEADD function,"The code example portraits the issue:

{code:sql}
spark-sql (default)> select dateadd('MONTH', 1, date'2023-05-11');
[WRONG_NUM_ARGS.WITHOUT_SUGGESTION] The `dateadd` requires 2 parameters but the actual number is 3. Please, refer to 'https://spark.apache.org/docs/latest/sql-ref-functions.html' for a fix.; line 1 pos 7
{code}

The error says about number of arguments passed to DATEADD but the issue is about the type of the first argument.
",maxgekk,maxgekk,Major,Resolved,Fixed,12/May/23 06:42,15/May/23 11:34
Bug,SPARK-43502,13536157,DataFrame.drop should support empty column,,podongfeng,podongfeng,Major,Resolved,Fixed,15/May/23 04:12,16/May/23 04:38
Bug,SPARK-43510,13536286,Spark application hangs when YarnAllocator adds running executors after processing completed containers,"I see application hangs when containers are preempted immediately after allocation as follows.
{code:java}
23/05/14 09:11:33 INFO YarnAllocator: Launching container container_e3812_1684033797982_57865_01_000382 on host hdc42-mcc10-01-0910-4207-015-tess0028.stratus.rno.ebay.com for executor with ID 277 for ResourceProfile Id 0 
23/05/14 09:11:33 WARN YarnAllocator: Cannot find executorId for container: container_e3812_1684033797982_57865_01_000382
23/05/14 09:11:33 INFO YarnAllocator: Completed container container_e3812_1684033797982_57865_01_000382 (state: COMPLETE, exit status: -102)
23/05/14 09:11:33 INFO YarnAllocator: Container container_e3812_1684033797982_57865_01_000382 was preempted.{code}
Note the warning log where YarnAllocator cannot find executorId for the container when processing completed containers. The only plausible cause is YarnAllocator added the running executor after processing completed containers. The former happens in a separate thread after executor launch.

YarnAllocator believes there are still running executors, although they are already lost due to preemption. Hence, the application hangs without any running executors.",mauzhang,mauzhang,Major,Resolved,Fixed,15/May/23 15:35,06/Jun/23 13:30
Bug,SPARK-43522,13536374,Creating struct column occurs  error 'org.apache.spark.sql.AnalysisException [DATATYPE_MISMATCH.CREATE_NAMED_STRUCT_WITHOUT_FOLDABLE_STRING]',"When creating a struct column in Dataframe, the code that ran without problems in version 3.3.1 does not work in version 3.4.0.

 

Example
{code:java}
val testDF = Seq(""a=b,c=d,d=f"").toDF.withColumn(""key_value"", split('value, "","")).withColumn(""map_entry"", transform(col(""key_value""), x => struct(split(x, ""="").getItem(0), split(x, ""="").getItem(1) ) )){code}
 

In 3.3.1

 
{code:java}
 
testDF.show()
+-----------+---------------+--------------------+ 
|      value|      key_value|           map_entry| 
+-----------+---------------+--------------------+ 
|a=b,c=d,d=f|[a=b, c=d, d=f]|[{a, b}, {c, d}, ...| 
+-----------+---------------+--------------------+
 
testDF.printSchema()
root
 |-- value: string (nullable = true)
 |-- key_value: array (nullable = true)
 |    |-- element: string (containsNull = false)
 |-- map_entry: array (nullable = true)
 |    |-- element: struct (containsNull = false)
 |    |    |-- col1: string (nullable = true)
 |    |    |-- col2: string (nullable = true)
{code}
 

 

In 3.4.0

 
{code:java}
org.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.CREATE_NAMED_STRUCT_WITHOUT_FOLDABLE_STRING] Cannot resolve ""struct(split(namedlambdavariable(), =, -1)[0], split(namedlambdavariable(), =, -1)[1])"" due to data type mismatch: Only foldable `STRING` expressions are allowed to appear at odd position, but they are [""0"", ""1""].;
'Project [value#41, key_value#45, transform(key_value#45, lambdafunction(struct(0, split(lambda x_3#49, =, -1)[0], 1, split(lambda x_3#49, =, -1)[1]), lambda x_3#49, false)) AS map_entry#48]
+- Project [value#41, split(value#41, ,, -1) AS key_value#45]
   +- LocalRelation [value#41]  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:73)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:269)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
....
 
{code}
 

However, if you do an alias to struct elements, you can get the same result as the previous version.

 
{code:java}
val testDF = Seq(""a=b,c=d,d=f"").toDF.withColumn(""key_value"", split('value, "","")).withColumn(""map_entry"", transform(col(""key_value""), x => struct(split(x, ""="").getItem(0).as(""col1"") , split(x, ""="").getItem(1).as(""col2"") ) )){code}
 

 ",fanjia,Heedo,Minor,Resolved,Fixed,16/May/23 08:00,18/May/23 06:30
Bug,SPARK-43527,13536410,Fix catalog.listCatalogs in PySpark,,podongfeng,podongfeng,Critical,Resolved,Fixed,16/May/23 13:27,16/May/23 23:31
Bug,SPARK-43529,13536464,Support general expressions as OPTIONS values ,,dtenedor,dtenedor,Major,Resolved,Fixed,16/May/23 22:22,12/Jun/23 04:31
Bug,SPARK-43534,13536503,Add log4j-1.2-api and log4j-slf4j2-impl to classpath if active hadoop-provided,"Build Spark:
{code:sh}
./dev/make-distribution.sh --name default --tgz -Phive -Phive-thriftserver -Pyarn -Phadoop-provided
tar -zxf spark-3.5.0-SNAPSHOT-bin-default.tgz {code}
Remove the following jars from spark-3.5.0-SNAPSHOT-bin-default:
{noformat}
jars/log4j-1.2-api-2.20.0.jar
jars/log4j-slf4j2-impl-2.20.0.jar
{noformat}
Add a new log4j2.properties to spark-3.5.0-SNAPSHOT-bin-default/conf:
{code:none}
rootLogger.level = info
rootLogger.appenderRef.file.ref = File
rootLogger.appenderRef.stderr.ref = console

appender.console.type = Console
appender.console.name = console
appender.console.target = SYSTEM_ERR
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss,SSS} %p [%t] %c{2}:%L : %m%n

appender.file.type = RollingFile
appender.file.name = File
appender.file.fileName = /tmp/spark/logs/spark.log
appender.file.filePattern = /tmp/spark/logs/spark.%d{yyyyMMdd-HH}.log
appender.file.append = true
appender.file.layout.type = PatternLayout
appender.file.layout.pattern = %d{yy/MM/dd HH:mm:ss,SSS} %p [%t] %c{2}:%L : %m%n
appender.file.policies.type = Policies
appender.file.policies.time.type = TimeBasedTriggeringPolicy
appender.file.policies.time.interval = 1
appender.file.policies.time.modulate = true
appender.file.policies.size.type = SizeBasedTriggeringPolicy
appender.file.policies.size.size = 256M
appender.file.strategy.type = DefaultRolloverStrategy
appender.file.strategy.max = 100
{code}

Start Spark thriftserver:
{code:java}
sbin/start-thriftserver.sh
{code}

Check the log:
{code:sh}
cat /tmp/spark/logs/spark.log
{code}

",yumwang,yumwang,Minor,Resolved,Fixed,17/May/23 07:08,20/May/23 23:45
Bug,SPARK-43538,13536523,Spark Homebrew Formulae currently depends on non-officially-supported Java 20,"I am not sure if homebrew-related issues can also be reported here? The Homebrew formulae for apache-spark runs on (latest) openjdk 20.

[https://formulae.brew.sh/formula/apache-spark]

However, Apache Spark is documented to work with Java 8/11/17:

[https://spark.apache.org/docs/latest/]

Is this an overlook, or is Java 20 officially supported, too?

Thanks!",yumwang,ghislain.fourny,Minor,Resolved,Fixed,17/May/23 09:42,23/May/23 00:13
Bug,SPARK-43541,13536613,Incorrect column resolution on FULL OUTER JOIN with USING,"This was tested on Spark 3.3.2 and Spark 3.4.0.

{code}
Causes [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `aws_dbr_a`.`key` cannot be resolved. Did you mean one of the following? [`key`].; line 4, pos 7
{code}


FULL OUTER JOIN with USING and/or the WHERE seems relevant since I can get the query to work with any of these modifications. 


{code}
# -- FULL OUTER JOIN
                   WITH
                   aws_dbr_a AS (select key from values ('a') t(key)),
                   gcp_pro_b AS (select key from values ('a') t(key))
                   SELECT aws_dbr_a.key
                   FROM aws_dbr_a
                   FULL OUTER JOIN gcp_pro_b USING (key)
                   WHERE aws_dbr_a.key NOT LIKE 'spark.clusterUsageTags.%';
[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `aws_dbr_a`.`key` cannot be resolved. Did you mean one of the following? [`key`].; line 4 pos 7
# -- INNER JOIN
                   WITH
                   aws_dbr_a AS (select key from values ('a') t(key)),
                   gcp_pro_b AS (select key from values ('a') t(key))
                   SELECT aws_dbr_a.key
                   FROM aws_dbr_a
                   JOIN gcp_pro_b USING (key)
                   WHERE aws_dbr_a.key NOT LIKE 'spark.clusterUsageTags.%';
+-----+
| key |
|-----|
| a   |
+-----+
1 row in set
Time: 0.507s
# -- NO Filter
                   WITH
                   aws_dbr_a AS (select key from values ('a') t(key)),
                   gcp_pro_b AS (select key from values ('a') t(key))
                   SELECT aws_dbr_a.key
                   FROM aws_dbr_a
                   FULL OUTER JOIN gcp_pro_b USING (key);
+-----+
| key |
|-----|
| a   |
+-----+
1 row in set
Time: 1.021s
# -- ON instead of USING
                   WITH
                   aws_dbr_a AS (select key from values ('a') t(key)),
                   gcp_pro_b AS (select key from values ('a') t(key))
                   SELECT aws_dbr_a.key
                   FROM aws_dbr_a
                   FULL OUTER JOIN gcp_pro_b ON aws_dbr_a.key = gcp_pro_b.key
                   WHERE aws_dbr_a.key NOT LIKE 'spark.clusterUsageTags.%';
+-----+
| key |
|-----|
| a   |
+-----+
1 row in set
Time: 0.514s
{code}
",maxgekk,maxgekk,Major,Resolved,Fixed,17/May/23 19:48,19/May/23 00:25
Bug,SPARK-43547,13536638,"Update ""Supported Pandas API"" page to point out the proper pandas docs",[https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/supported_pandas_api.html#supported-pandas-api] not point out the wrong pandas version.,itholic,itholic,Major,Resolved,Fixed,18/May/23 01:36,18/May/23 03:05
Bug,SPARK-43581,13536792,Upgrade `kubernetes-client` to 6.6.2,,dongjoon,dongjoon,Minor,Resolved,Fixed,19/May/23 00:44,19/May/23 02:46
Bug,SPARK-43589,13536824,Fix `cannotBroadcastTableOverMaxTableBytesError` to use `bytesToString`,,dongjoon,dongjoon,Minor,Resolved,Fixed,19/May/23 06:36,19/May/23 15:35
Bug,SPARK-43596,13536922,Subquery decorrelation rewriteDomainJoins failure from ConstantFolding to isnull,"We can get a decorrelation error because of rewrites that run in between DecorrelateInnerQuery and rewriteDomainJoins, that modify the correlation join conditions. In particular, ConstantFolding can transform `innercol <=> null` to `isnull(innercol)` and then rewriteDomainJoins does not recognize this and throws error Unable to rewrite domain join with conditions: ArrayBuffer(isnull(innercol#280)) because the isnull is not an equality, so it isn't usable for rewriting the domain join.

Can fix by recognizing `isnull(innercol)` as `innercol <=> null` in rewriteDomainJoins.

This area is also fragile in general and other rewrites that run between the two steps of decorrelation could potentially break their assumptions, so we may want to investigate longer-term follow ups for that.",,jchen5,Major,Resolved,Fixed,19/May/23 23:49,25/May/23 08:43
Bug,SPARK-43647,13537060,Maven test failed in ClientE2ETestSuite/CatalogSuite/StreamingQuerySuite without -Phive," 
{code:java}
build/mvn clean install -DskipTests
build/mvn test -pl connector/connect/client/jvm{code}
 

13 test failed with similar reasons:

 
{code:java}
- read and write *** FAILED ***
  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated
  at io.grpc.Status.asRuntimeException(Status.java:535)
  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
  at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)
  at scala.collection.Iterator.toStream(Iterator.scala:1417)
  at scala.collection.Iterator.toStream$(Iterator.scala:1416)
  at scala.collection.AbstractIterator.toStream(Iterator.scala:1431)
  at scala.collection.TraversableOnce.toSeq(TraversableOnce.scala:354)
  at scala.collection.TraversableOnce.toSeq$(TraversableOnce.scala:354)
  at scala.collection.AbstractIterator.toSeq(Iterator.scala:1431)
  at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:489)
  ... {code}
 

 ",LuciferYang,LuciferYang,Major,Resolved,Fixed,22/May/23 07:28,26/May/23 01:31
Bug,SPARK-43648,13537062,"Maven test failed: `interrupt all - background queries, foreground interrupt` and `interrupt all - foreground queries, background interrupt`","run 
{code:java}
build/mvn clean install -DskipTests -Phive
build/mvn test -pl connector/connect/client/jvm -Dtest=none -DwildcardSuites=org.apache.spark.sql.ClientE2ETestSuite {code}
`interrupt all - background queries, foreground interrupt` and `interrupt all - foreground queries, background interrupt` failed as follows:
{code:java}
23/05/22 15:44:11 ERROR SparkConnectService: Error during: execute. UserId: . SessionId: 0f4013ca-3af9-443b-a0e5-e339a827e0cf.
java.lang.NoClassDefFoundError: org/apache/spark/sql/connect/client/SparkResult
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
	at java.lang.Class.getDeclaredMethod(Class.java:2128)
	at java.io.ObjectStreamClass.getPrivateMethod(ObjectStreamClass.java:1643)
	at java.io.ObjectStreamClass.access$1700(ObjectStreamClass.java:79)
	at java.io.ObjectStreamClass$3.run(ObjectStreamClass.java:520)
	at java.io.ObjectStreamClass$3.run(ObjectStreamClass.java:494)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:494)
	at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:391)
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:681)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2005)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1852)
	at java.io.ObjectInputStream.readClass(ObjectInputStream.java:1815)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1640)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)
	at org.apache.spark.util.Utils$.deserialize(Utils.scala:148)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.org$apache$spark$sql$connect$planner$SparkConnectPlanner$$unpackUdf(SparkConnectPlanner.scala:1353)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner$TypedScalaUdf$.apply(SparkConnectPlanner.scala:761)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformTypedMapPartitions(SparkConnectPlanner.scala:531)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformMapPartitions(SparkConnectPlanner.scala:495)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:143)
	at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handlePlan(SparkConnectStreamHandler.scala:100)
	at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$2(SparkConnectStreamHandler.scala:87)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:825)
	at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$1(SparkConnectStreamHandler.scala:53)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:209)
	at org.apache.spark.sql.connect.artifact.SparkConnectArtifactManager$.withArtifactClassLoader(SparkConnectArtifactManager.scala:178)
	at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handle(SparkConnectStreamHandler.scala:48)
	at org.apache.spark.sql.connect.service.SparkConnectService.executePlan(SparkConnectService.scala:166)
	at org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:611)
	at org.sparkproject.connect.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
	at org.sparkproject.connect.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:352)
	at org.sparkproject.connect.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:866)
	at org.sparkproject.connect.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.sparkproject.connect.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.connect.client.SparkResult
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	... 56 more {code}",LuciferYang,LuciferYang,Major,Resolved,Fixed,22/May/23 07:47,09/Jun/23 11:09
Bug,SPARK-43717,13537211,Scala Client Dataset#reduce failed to handle null partitions for scala primitive types,"Scala client failed with NPE when running:

assert(spark.range(0, 5, 1, 10).as[Long].reduce(_ + _) == 10)",zhenli,zhenli,Major,Resolved,Fixed,22/May/23 19:32,07/Jun/23 06:31
Bug,SPARK-43718,13537216,References to a specific side's key in a USING join can have wrong nullability,"Assume this data:
{noformat}
create or replace temp view t1 as values (1), (2), (3) as (c1);
create or replace temp view t2 as values (2), (3), (4) as (c1);
{noformat}
The following query produces incorrect results:
{noformat}
spark-sql (default)> select explode(array(t1.c1, t2.c1)) as x1
from t1
full outer join t2
using (c1);
1
-1      <== should be null
2
2
3
3
-1      <== should be null
4
Time taken: 0.663 seconds, Fetched 8 row(s)
spark-sql (default)> 
{noformat}
Similar issues occur with right outer join and left outer join.

{{t1.c1}} and {{t2.c1}} have the wrong nullability at the time the array is resolved, so the array's {{containsNull}} value is incorrect.

Queries that don't use arrays also can get wrong results. Assume this data:
{noformat}
create or replace temp view t1 as values (0), (1), (2) as (c1);
create or replace temp view t2 as values (1), (2), (3) as (c1);
create or replace temp view t3 as values (1, 2), (3, 4), (4, 5) as (a, b);
{noformat}
The following query produces incorrect results:
{noformat}
select t1.c1 as t1_c1, t2.c1 as t2_c1, b
from t1
full outer join t2
using (c1),
lateral (
  select b
  from t3
  where a = coalesce(t2.c1, 1)
) lt3;
1	1	2
NULL	3	4
Time taken: 2.395 seconds, Fetched 2 row(s)
spark-sql (default)> 
{noformat}
The result should be the following:
{noformat}
0	NULL	2
1	1	2
NULL	3	4
{noformat}

",bersprockets,bersprockets,Major,Resolved,Fixed,22/May/23 20:48,23/May/23 04:48
Bug,SPARK-43719,13537224,Handle missing row.excludedInStages field,,dongjoon,dongjoon,Minor,Resolved,Fixed,22/May/23 22:51,23/May/23 05:17
Bug,SPARK-43742,13537284,refactor default column value resolution,,cloud_fan,cloud_fan,Major,Resolved,Fixed,23/May/23 07:50,28/May/23 23:28
Bug,SPARK-43743,13537295,Port HIVE-12188: DoAs does not work properly in non-kerberos secured HS2,Please see: https://issues.apache.org/jira/browse/HIVE-12188,yumwang,yumwang,Major,Resolved,Fixed,23/May/23 08:55,23/May/23 10:35
Bug,SPARK-43757,13537385,Change CheckConnectJvmClientCompatibility to deny list to increase the API check coverage,"The current compatibility check only checks selected classes. So when adding a new class, if a developer forgets to add this class into the checklist, then this API is not covered in compatibility tests. Thus we should change this API check to always include all APIs by default.",zhenli,zhenli,Minor,Resolved,Fixed,23/May/23 21:37,28/Jun/23 14:38
Bug,SPARK-43758,13537392,Upgrade snappy-java to 1.1.10.0,Update {{snappy-java}} to 1.1.10.0,csun,csun,Major,Resolved,Fixed,23/May/23 22:47,24/May/23 02:51
Bug,SPARK-43759,13537396,Expose TimestampNTZType in pyspark.sql.types,{{TimestampNTZType}} is missing in {{__all__}} list in {{pyspark.sql.types}}.,ueshin,ueshin,Major,Resolved,Fixed,23/May/23 23:59,04/Jul/23 02:40
Bug,SPARK-43760,13537399,Incorrect attribute nullability after RewriteCorrelatedScalarSubquery leads to incorrect query results,"The following query:

 
{code:java}
select * from (
 select t1.id c1, (
  select t2.id c from range (1, 2) t2
  where t1.id = t2.id  ) c2
 from range (1, 3) t1 ) t
where t.c2 is not null
-- !query schema
struct<c1:bigint,c2:bigint>
-- !query output
1	1
2	NULL
 {code}
 
should return 1 row, because the second row is supposed to be removed by IsNotNull predicate. However, due to a wrong nullability propagation after subquery decorrelation, the output of the subquery is declared as not-nullable (incorrectly), so the predicate is constant folded into True.",gubichev,gubichev,Major,Resolved,Fixed,24/May/23 00:14,01/Jun/23 07:12
Bug,SPARK-43767,13537414,Fix bug in AvroSuite for 'reading from invalid path throws exception',,panbingkun,panbingkun,Minor,Resolved,Fixed,24/May/23 02:10,24/May/23 04:10
Bug,SPARK-43802,13537722,unbase64 and unhex codegen are invalid with failOnError,"to_binary with hex and base64 generate invalid codegen:

{{spark.range(5).selectExpr('to_binary(base64(cast(id as binary)), ""BASE64"")').show()}}

results in

{{Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 1: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 1: Unknown variable or type ""BASE64""}}

because this is the generated code:



/* 107 */         if (!org.apache.spark.sql.catalyst.expressions.UnBase64.isValidBase64(project_value_1)) {

/* 108 */           throw QueryExecutionErrors.invalidInputInConversionError(

/* 109 */             ((org.apache.spark.sql.types.BinaryType$) references[1] /* to */),

/* 110 */             project_value_1,

/* 111 */             BASE64,

/* 112 */             ""try_to_binary"");

/* 113 */         }",kimahriman,kimahriman,Major,Resolved,Fixed,25/May/23 21:45,27/May/23 02:31
Bug,SPARK-43841,13537934,Non-existent column in projection of full outer join with USING results in StringIndexOutOfBoundsException,"The following query throws a {{StringIndexOutOfBoundsException}}:
{noformat}
with v1 as (
 select * from values (1, 2) as (c1, c2)
),
v2 as (
  select * from values (2, 3) as (c1, c2)
)
select v1.c1, v1.c2, v2.c1, v2.c2, b
from v1
full outer join v2
using (c1);
{noformat}
The query should fail anyway, since {{b}} refers to a non-existent column. But it should fail with a helpful error message, not with a {{StringIndexOutOfBoundsException}}.

The issue seems to be in {{StringUtils#orderSuggestedIdentifiersBySimilarity}}. {{orderSuggestedIdentifiersBySimilarity}} assumes that a list of candidate attributes with a mix of prefixes will never have an attribute name with an empty prefix. But in this case it does ({{c1}} from the {{coalesce}} has no prefix, since it is not associated with any relation or subquery):
{noformat}
+- 'Project [c1#5, c2#6, c1#7, c2#8, 'b]
   +- Project [coalesce(c1#5, c1#7) AS c1#9, c2#6, c2#8] <== c1#9 has no prefix, unlike c2#6 (v1.c2) or c2#8 (v2.c2)
      +- Join FullOuter, (c1#5 = c1#7)
         :- SubqueryAlias v1
         :  +- CTERelationRef 0, true, [c1#5, c2#6]
         +- SubqueryAlias v2
            +- CTERelationRef 1, true, [c1#7, c2#8]
{noformat}
Because of this, {{orderSuggestedIdentifiersBySimilarity}} returns a sorted list of suggestions like this:
{noformat}
ArrayBuffer(.c1, v1.c2, v2.c2)
{noformat}
{{UnresolvedAttribute.parseAttributeName}} chokes on an attribute name that starts with a namespace separator ('.').
",bersprockets,bersprockets,Minor,Resolved,Fixed,28/May/23 18:48,29/May/23 07:05
Bug,SPARK-43842,13537936,Upgrade `gcs-connector` to 2.2.14,,dongjoon,dongjoon,Minor,Resolved,Fixed,28/May/23 21:25,29/May/23 00:19
Bug,SPARK-43889,13538161,Add check to see if column name is legal for __dir__,"In SPARK-43270, we add a support for the autocomplete suggest for {{df.|}} , which also suggest column_name for dataframe. 

However, we found out later that some dataframe can have column which has illegal variable name (e.g: name?1, name 1, 2name etc.) These variable name should be filtered out and this will be consistent with pandas behavior now",jarviscao,jarviscao,Major,Resolved,Fixed,30/May/23 20:55,31/May/23 07:00
Bug,SPARK-43936,13538498,Fix bug for toSQLId,"After SPARK-43910, {{__auto_generated_subquery_name}} from ids in errors should remove, but when the type of {{parts}} is ArrayBuffer, match will fail. causing unexpected behavior.",panbingkun,panbingkun,Minor,Resolved,Fixed,02/Jun/23 02:31,02/Jun/23 05:38
Bug,SPARK-43945,13538527,Fix bug for `SQLQueryTestSuite` when run on local env,,panbingkun,panbingkun,Minor,Resolved,Fixed,02/Jun/23 07:02,02/Jun/23 16:36
Bug,SPARK-43949,13538540,Upgrade Cloudpickle to 2.2.1,Cloudpickle 2.2.1 has a fix for named tuple issue (https://github.com/cloudpipe/cloudpickle/issues/460). PySpark relies on namedtuple heavily especially for RDD. We should upgrade and fix it.,gurwls223,gurwls223,Major,Resolved,Fixed,02/Jun/23 08:16,02/Jun/23 10:40
Bug,SPARK-43951,13538603,RocksDB state store can become corrupt on task retries,"A couple of our streaming jobs have failed since upgrading to Spark 3.4 with an error such as:

org.rocksdb.RocksDBException: Mismatch in unique ID on table file ###. Expected: [###,###} Actual\{###,###} in file ..../MANIFEST-####

This is due to the change from [https://github.com/facebook/rocksdb/commit/6de7081cf37169989e289a4801187097f0c50fae] that enabled unique ID checks by default, and I finally tracked down the exact sequence of steps that leads to this failure in the way RocksDB state store is used.
 # A task fails after uploading the checkpoint to HDFS. Lets say it uploaded 11.zip to version 11 of the table, but the task failed before it could finish after successfully uploading the checkpoint.
 # The same task is retried and goes back to load version 10 of the table as expected.
 # Cleanup/maintenance is called for this partition, which looks in HDFS for persisted versions and sees up through version 11 since that zip file was successfully uploaded on the previous task.
 # As part of resolving what SST files are part of each table version, versionToRocksDBFiles.put(version, newResolvedFiles) is called for version 11 with its SST files that were uploaded in the first failed task.
 # The second attempt at the task commits and goes to sync its checkpoint to HDFS.
 # versionToRocksDBFiles contains the SST files to upload from step 4, and these files are considered ""the same"" as what's in the local working dir because the name and file size match.
 # No SST files are uploaded because they matched above, but in reality the unique ID inside the SST files is different (presumably this is just randomly generated and inserted into each SST file?), it just doesn't affect the size.
 # A new METADATA file is uploaded which has the new unique IDs listed inside.
 # When version 11 of the table is read during the next batch, the unique IDs in the METADATA file don't match the unique IDS in the SST files, which causes the exception.

 

This is basically a ticking time bomb for anyone using RocksDB. Thoughts on possible fixes would be:
 * Disable unique ID verification. I don't currently see a binding for this in the RocksDB java wrapper, so that would probably have to be added first.
 * Disable checking if files are already uploaded with the same size, and just always upload SST files no matter what.
 * Update the ""same file"" check to also be able to do some kind of CRC comparison or something like that.
 * Update the mainteance/cleanup to not update the versionToRocksDBFiles map.",,kimahriman,Major,Resolved,Fixed,02/Jun/23 13:07,02/Jun/23 13:09
Bug,SPARK-43956,13538665,Fix the bug doesn't display column's sql for Percentile[Cont|Disc],"Last year, I committed Percentile[Cont|Disc] functions for Spark SQL.
Recently, I found the sql method of Percentile[Cont|Disc] doesn't display column's sql suitably.",beliefer,beliefer,Major,Resolved,Fixed,03/Jun/23 05:13,04/Jun/23 00:31
Bug,SPARK-43960,13538693,DataFrameConversionTestsMixin is not tested properly.,DataFrameConversionTestsMixin is not tested properly. We should fix it.,itholic,itholic,Major,Resolved,Fixed,04/Jun/23 09:51,04/Jun/23 23:34
Bug,SPARK-43962,13538726,"Improve error messages: CANNOT_DECODE_URL, CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE, CANNOT_PARSE_DECIMAL, CANNOT_READ_FILE_FOOTER, CANNOT_RECOGNIZE_HIVE_TYPE.",Improve error message for usability.,itholic,itholic,Major,Resolved,Fixed,05/Jun/23 04:34,06/Jun/23 07:25
Bug,SPARK-43973,13538832,Structured Streaming UI should display failed queries correctly,"The Structured Streaming UI is designed to be able to show a query's status (active/finished/failed) and if failed, the error message.
Due to a bug in the implementation, the error message in {{QueryTerminatedEvent}} isn't being tracked by the UI data, so in turn the UI always shows failed queries as ""finished"".

Example:
{code:scala}
implicit val ctx = spark.sqlContext
import org.apache.spark.sql.execution.streaming.MemoryStream

spark.conf.set(""spark.sql.ansi.enabled"", ""true"")

val inputData = MemoryStream[(Int, Int)]

val df = inputData.toDF().selectExpr(""_1 / _2 as a"")

inputData.addData((1, 2), (3, 4), (5, 6), (7, 0))
val testQuery = df.writeStream.format(""memory"").queryName(""kristest"").outputMode(""append"").start
testQuery.processAllAvailable()
{code}

Here we intentionally fail a query, but the Spark UI's Structured Streaming tab would show this as ""FINISHED"" without any errors, which is wrong.",rednaxelafx,rednaxelafx,Major,Resolved,Fixed,05/Jun/23 22:39,06/Jun/23 05:08
Bug,SPARK-43976,13538850,Handle the case where modifiedConfigs doesn't exist in event logs,,dongjoon,dongjoon,Major,Resolved,Fixed,06/Jun/23 03:08,06/Jun/23 16:35
Bug,SPARK-43977,13538852,bad case of connect-jvm-client-mima-check,"run 

```

build/sbt ""protobuf/clean""

dev/connect-jvm-client-mima-check

```
{code:java}
Using SPARK_LOCAL_IP=localhost
Using SPARK_LOCAL_IP=localhost
Do connect-client-jvm module mima check ...
Failed to find the jar: spark-protobuf-assembly(.*).jar or spark-protobuf(.*)3.5.0-SNAPSHOT.jar inside folder: /Users/yangjie01/SourceCode/git/spark-mine-sbt/connector/protobuf/target. This file can be generated by similar to the following command: build/sbt package|assembly
finish connect-client-jvm module mima check ...
connect-client-jvm module mima check passed.
 {code}
The check result is wrong,  there are both error messages and checks successful

 ",LuciferYang,LuciferYang,Major,Resolved,Fixed,06/Jun/23 03:46,06/Jun/23 14:46
Bug,SPARK-43985,13538947,Spark protobuf enums.as.ints raises exception on repeated enum types,"For repeated enum types, the `enums.as.ints` being enabled currently raises an exception when trying to deserialize repeated enum fields. We should fix this behavior so that repeated enum fields work correctly.",justaparth,justaparth,Major,Resolved,Fixed,06/Jun/23 15:16,07/Jun/23 03:06
Bug,SPARK-44002,13539145,SparkConnectArtifactStatusesHandler freezes on second request,SparkConnectArtifactStatusesHandler freezes on the second request when a cache exists. Need to free a lock on the block.,maxgekk,maxgekk,Major,Resolved,Fixed,07/Jun/23 20:56,07/Jun/23 23:47
Bug,SPARK-44010,13539351,Python StreamingQueryProgress rowsPerSecond type fix,,WweiL,WweiL,Minor,Resolved,Fixed,09/Jun/23 01:51,09/Jun/23 04:11
Bug,SPARK-44016,13539422,Artifacts with name as an absolute path may overwrite other files ,"In `SparkConnectAddArtifactsHandler`, an artifact being moved to a staging location may overwrite another file when the `name`/`path` of the artifact is an `absolute` path. 

This happens when the [stagedPath|https://github.com/apache/spark/blob/master/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectAddArtifactsHandler.scala#L172] is being computed with the help of the `.resolve(...)` method where the `resolve` method returns the `other` path (in this case, the name of the artifact) if the `other` path is an absolute path.",vicennial,vicennial,Major,Resolved,Fixed,09/Jun/23 12:23,13/Jun/23 17:05
Bug,SPARK-44040,13539864,Incorrect result after count distinct,"When i try to call count after distinct function for Decimal null field, spark return incorrect result starting from spark 3.4.0.
A minimal example to reproduce:

import org.apache.spark.sql.types._
import org.apache.spark.sql.\{Column, DataFrame, Dataset, Row, SparkSession}
import org.apache.spark.sql.types.\{StringType, StructField, StructType}
val schema = StructType( Array(
StructField(""money"", DecimalType(38,6), true),
StructField(""reference_id"", StringType, true)
))

val payDf = spark.createDataFrame(sc.emptyRDD[Row], schema)

val aggDf = payDf.agg(sum(""money"").as(""money"")).withColumn(""name"", lit(""df1""))
val aggDf1 = payDf.agg(sum(""money"").as(""money"")).withColumn(""name"", lit(""df2""))
val unionDF: DataFrame = aggDf.union(aggDf1)
unionDF.select(""money"").distinct.show // return correct result
unionDF.select(""money"").distinct.count // return 2 instead of 1
unionDF.select(""money"").distinct.count == 1 // return false


This block of code returns some assertion error and after that an incorrect count (in spark 3.2.1 everything works fine and i get correct result = 1):

*scala> unionDF.select(""money"").distinct.show // return correct result*
java.lang.AssertionError: assertion failed:
Decimal$DecimalIsFractional
while compiling: <console>
during phase: globalPhase=terminal, enteringPhase=jvm
library version: version 2.12.17
compiler version: version 2.12.17
reconstructed args: -classpath /Users/aleksandrov/.ivy2/jars/org.apache.spark_spark-connect_2.12-3.4.0.jar:/Users/aleksandrov/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar:/Users/aleksandrov/.ivy2/jars/io.delta_delta-storage-2.4.0.jar:/Users/aleksandrov/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar:/Users/aleksandrov/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar -Yrepl-class-based -Yrepl-outdir /private/var/folders/qj/_dn4xbp14jn37qmdk7ylyfwc0000gr/T/spark-f37bb154-75f3-4db7-aea8-3c4363377bd8/repl-350f37a1-1df1-4816-bd62-97929c60a6c1

last tree to typer: TypeTree(class Byte)
tree position: line 6 of <console>
tree tpe: Byte
symbol: (final abstract) class Byte in package scala
symbol definition: final abstract class Byte extends (a ClassSymbol)
symbol package: scala
symbol owners: class Byte
call site: constructor $eval in object $eval in package $line19

== Source file context for tree position ==

3
4object $eval {
5lazyval $result = $line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.res0
6lazyval $print: {_}root{_}.java.lang.String = {
7 $line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw
8
9""""
at scala.reflect.internal.SymbolTable.throwAssertionError(SymbolTable.scala:185)
at scala.reflect.internal.Symbols$Symbol.completeInfo(Symbols.scala:1525)
at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1514)
at scala.reflect.internal.Symbols$Symbol.flatOwnerInfo(Symbols.scala:2353)
at scala.reflect.internal.Symbols$ClassSymbol.companionModule0(Symbols.scala:3346)
at scala.reflect.internal.Symbols$ClassSymbol.companionModule(Symbols.scala:3348)
at scala.reflect.internal.Symbols$ModuleClassSymbol.sourceModule(Symbols.scala:3487)
at scala.reflect.internal.Symbols.$anonfun$forEachRelevantSymbols$1$adapted(Symbols.scala:3802)
at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
at scala.reflect.internal.Symbols.markFlagsCompleted(Symbols.scala:3799)
at scala.reflect.internal.Symbols.markFlagsCompleted$(Symbols.scala:3805)
at scala.reflect.internal.SymbolTable.markFlagsCompleted(SymbolTable.scala:28)
at scala.reflect.internal.pickling.UnPickler$Scan.finishSym$1(UnPickler.scala:324)
at scala.reflect.internal.pickling.UnPickler$Scan.readSymbol(UnPickler.scala:342)
at scala.reflect.internal.pickling.UnPickler$Scan.readSymbolRef(UnPickler.scala:645)
at scala.reflect.internal.pickling.UnPickler$Scan.readType(UnPickler.scala:413)
at scala.reflect.internal.pickling.UnPickler$Scan.$anonfun$readSymbol$10(UnPickler.scala:357)
at scala.reflect.internal.pickling.UnPickler$Scan.at(UnPickler.scala:188)
at scala.reflect.internal.pickling.UnPickler$Scan.readSymbol(UnPickler.scala:357)
at scala.reflect.internal.pickling.UnPickler$Scan.$anonfun$run$1(UnPickler.scala:96)
at scala.reflect.internal.pickling.UnPickler$Scan.run(UnPickler.scala:88)
at scala.reflect.internal.pickling.UnPickler.unpickle(UnPickler.scala:47)
at scala.tools.nsc.symtab.classfile.ClassfileParser.unpickleOrParseInnerClasses(ClassfileParser.scala:1173)
at scala.tools.nsc.symtab.classfile.ClassfileParser.parseClass(ClassfileParser.scala:467)
at scala.tools.nsc.symtab.classfile.ClassfileParser.$anonfun$parse$2(ClassfileParser.scala:160)
at scala.tools.nsc.symtab.classfile.ClassfileParser.$anonfun$parse$1(ClassfileParser.scala:146)
at scala.tools.nsc.symtab.classfile.ClassfileParser.parse(ClassfileParser.scala:129)
at scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader.doComplete(SymbolLoaders.scala:343)
at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:250)
at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.load(SymbolLoaders.scala:269)
at scala.reflect.internal.Symbols$Symbol.exists(Symbols.scala:1104)
at scala.reflect.internal.Symbols$Symbol.toOption(Symbols.scala:2609)
at scala.tools.nsc.interpreter.IMain.translateSimpleResource(IMain.scala:340)
at scala.tools.nsc.interpreter.IMain$TranslatingClassLoader.findAbstractFile(IMain.scala:354)
at scala.reflect.internal.util.AbstractFileClassLoader.findResource(AbstractFileClassLoader.scala:76)
at java.base/java.lang.ClassLoader.getResource(ClassLoader.java:1401)
at java.base/java.lang.ClassLoader.getResourceAsStream(ClassLoader.java:1737)
at scala.reflect.internal.util.RichClassLoader$.classAsStream$extension(ScalaClassLoader.scala:89)
at scala.reflect.internal.util.RichClassLoader$.classBytes$extension(ScalaClassLoader.scala:81)
at scala.reflect.internal.util.ScalaClassLoader.classBytes(ScalaClassLoader.scala:131)
at scala.reflect.internal.util.ScalaClassLoader.classBytes$(ScalaClassLoader.scala:131)
at scala.reflect.internal.util.AbstractFileClassLoader.classBytes(AbstractFileClassLoader.scala:41)
at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:70)
at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)
at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:576)
at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40)
at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:75)
at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:317)
at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8895)
at org.codehaus.janino.UnitCompiler.reclassifyName(UnitCompiler.java:9115)
at org.codehaus.janino.UnitCompiler.reclassifyName(UnitCompiler.java:8806)
at org.codehaus.janino.UnitCompiler.reclassify(UnitCompiler.java:8667)
at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:7194)
at org.codehaus.janino.UnitCompiler.access$18100(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$26.visitAmbiguousName(UnitCompiler.java:6785)
at org.codehaus.janino.UnitCompiler$26.visitAmbiguousName(UnitCompiler.java:6784)
at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4603)
at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6784)
at org.codehaus.janino.UnitCompiler.access$15100(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$25.visitLvalue(UnitCompiler.java:6745)
at org.codehaus.janino.UnitCompiler$25.visitLvalue(UnitCompiler.java:6742)
at org.codehaus.janino.Java$Lvalue.accept(Java.java:4528)
at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6742)
at org.codehaus.janino.UnitCompiler.access$14400(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$23.visitRvalue(UnitCompiler.java:6690)
at org.codehaus.janino.UnitCompiler$23.visitRvalue(UnitCompiler.java:6681)
at org.codehaus.janino.Java$Rvalue.accept(Java.java:4495)
at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6681)
at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9392)
at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:7486)
at org.codehaus.janino.UnitCompiler.access$16100(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$25.visitMethodInvocation(UnitCompiler.java:6756)
at org.codehaus.janino.UnitCompiler$25.visitMethodInvocation(UnitCompiler.java:6742)
at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5470)
at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6742)
at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9590)
at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9475)
at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9391)
at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5232)
at org.codehaus.janino.UnitCompiler.access$9300(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4735)
at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4711)
at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5470)
at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4711)
at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5854)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:4101)
at org.codehaus.janino.UnitCompiler.access$6300(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:4057)
at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:4040)
at org.codehaus.janino.Java$Assignment.accept(Java.java:4864)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:4040)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2523)
at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1580)
at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1575)
at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:3209)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1661)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1646)
at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1579)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1575)
at org.codehaus.janino.Java$Block.accept(Java.java:3115)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2659)
at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1581)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1575)
at org.codehaus.janino.Java$IfStatement.accept(Java.java:3284)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1661)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1646)
at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1579)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1575)
at org.codehaus.janino.Java$Block.accept(Java.java:3115)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2637)
at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1581)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1575)
at org.codehaus.janino.Java$IfStatement.accept(Java.java:3284)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1661)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1646)
at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1579)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1575)
at org.codehaus.janino.Java$Block.accept(Java.java:3115)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2001)
at org.codehaus.janino.UnitCompiler.access$2200(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1584)
at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1575)
at org.codehaus.janino.Java$WhileStatement.accept(Java.java:3389)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1661)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3658)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3329)
at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1447)
at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1420)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:829)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1026)
at org.codehaus.janino.UnitCompiler.access$700(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$3.visitMemberClassDeclaration(UnitCompiler.java:425)
at org.codehaus.janino.UnitCompiler$3.visitMemberClassDeclaration(UnitCompiler.java:418)
at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1533)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:418)
at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:1397)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:864)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:442)
at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$3.visitPackageMemberClassDeclaration(UnitCompiler.java:422)
at org.codehaus.janino.UnitCompiler$3.visitPackageMemberClassDeclaration(UnitCompiler.java:418)
at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1688)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:418)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:392)
at org.codehaus.janino.UnitCompiler.access$000(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$2.visitCompilationUnit(UnitCompiler.java:363)
at org.codehaus.janino.UnitCompiler$2.visitCompilationUnit(UnitCompiler.java:361)
at org.codehaus.janino.Java$CompilationUnit.accept(Java.java:371)
at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:361)
at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:264)
at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:294)
at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:288)
at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:267)
at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:82)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1496)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1586)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1583)
at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1443)
at org.apache.spark.sql.execution.WholeStageCodegenExec.liftedTree1$1(WholeStageCodegenExec.scala:726)
at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:725)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
at org.apache.spark.sql.execution.UnionExec.$anonfun$doExecute$5(basicPhysicalOperators.scala:699)
at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
at scala.collection.TraversableLike.map(TraversableLike.scala:286)
at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
at scala.collection.AbstractTraversable.map(Traversable.scala:108)
at org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:699)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
at org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs(AggregateCodegenSupport.scala:89)
at org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs$(AggregateCodegenSupport.scala:88)
at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:47)
at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:135)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:135)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:140)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:139)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:68)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:68)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:67)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:115)
at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:181)
at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:181)
at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:183)
at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:82)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:266)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:264)
at scala.collection.Iterator.foreach(Iterator.scala:943)
at scala.collection.Iterator.foreach$(Iterator.scala:943)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
at scala.collection.IterableLike.foreach(IterableLike.scala:74)
at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:264)
at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:236)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:381)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)
at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)
at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)
at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)
at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)
at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)
at org.apache.spark.sql.Dataset.head(Dataset.scala:3161)
at org.apache.spark.sql.Dataset.take(Dataset.scala:3382)
at org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)
at org.apache.spark.sql.Dataset.showString(Dataset.scala:323)
at org.apache.spark.sql.Dataset.show(Dataset.scala:809)
at org.apache.spark.sql.Dataset.show(Dataset.scala:768)
at org.apache.spark.sql.Dataset.show(Dataset.scala:777)
at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:33)
at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:35)
at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:37)
at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:39)
at $line19.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:41)
at $line19.$read$$iw$$iw$$iw$$iw.<init>(<console>:43)
at $line19.$read$$iw$$iw$$iw.<init>(<console>:45)
at $line19.$read$$iw$$iw.<init>(<console>:47)
at $line19.$read$$iw.<init>(<console>:49)
at $line19.$read.<init>(<console>:51)
at $line19.$read$.<init>(<console>:55)
at $line19.$read$.<clinit>(<console>)
at $line19.$eval$.$print$lzycompute(<console>:7)
at $line19.$eval$.$print(<console>:6)
at $line19.$eval.$print(<console>)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.base/java.lang.reflect.Method.invoke(Method.java:566)
at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
at org.apache.spark.repl.Main$.doMain(Main.scala:78)
at org.apache.spark.repl.Main$.main(Main.scala:58)
at org.apache.spark.repl.Main.main(Main.scala)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.base/java.lang.reflect.Method.invoke(Method.java:566)
at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1020)
at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)
at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)
at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
error: error while loading Decimal, class file '/Users/aleksandrov/Projects/apache/spark-3.4.0-bin-hadoop3/jars/spark-catalyst_2.12-3.4.0.jar(org/apache/spark/sql/types/Decimal.class)' is broken
(class java.lang.RuntimeException/error reading Scala signature of Decimal.class: assertion failed:
Decimal$DecimalIsFractional
while compiling: <console>
during phase: globalPhase=terminal, enteringPhase=jvm
library version: version 2.12.17
compiler version: version 2.12.17
reconstructed args: -classpath /Users/aleksandrov/.ivy2/jars/org.apache.spark_spark-connect_2.12-3.4.0.jar:/Users/aleksandrov/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar:/Users/aleksandrov/.ivy2/jars/io.delta_delta-storage-2.4.0.jar:/Users/aleksandrov/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar:/Users/aleksandrov/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar -Yrepl-class-based -Yrepl-outdir /private/var/folders/qj/_dn4xbp14jn37qmdk7ylyfwc0000gr/T/spark-f37bb154-75f3-4db7-aea8-3c4363377bd8/repl-350f37a1-1df1-4816-bd62-97929c60a6c1

last tree to typer: TypeTree(class Byte)
tree position: line 6 of <console>
tree tpe: Byte
symbol: (final abstract) class Byte in package scala
symbol definition: final abstract class Byte extends (a ClassSymbol)
symbol package: scala
symbol owners: class Byte
call site: constructor $eval in object $eval in package $line19

== Source file context for tree position ==

3
4object $eval {
5lazyval $result = res0
6lazyval $print: {_}root{_}.java.lang.String = {
7 $iw
8
9"""" )
+-----+
|money|

+-----+
|null|

+-----+

*scala> unionDF.select(""money"").distinct.count // return 2 instead of 1*
res1: Long = 2 

*scala> unionDF.select(""money"").distinct.count == 1 // return False*
res2: Boolean = false",yumwang,boltonidze,Critical,Resolved,Fixed,13/Jun/23 13:42,16/Jun/23 04:07
Bug,SPARK-44053,13540006,Update ORC to 1.8.4,,Guiyankuang,Guiyankuang,Major,Resolved,Fixed,14/Jun/23 08:25,14/Jun/23 17:56
Bug,SPARK-44064,13540138,Maven test `ProductAggSuite` aborted,"run 

 
{code:java}
 ./build/mvn  -DskipTests -Pyarn -Pmesos -Pkubernetes -Pvolcano -Phive -Phive-thriftserver -Phadoop-cloud -Pspark-ganglia-lgpl  clean install
 build/mvn test -pl sql/catalyst     {code}
aborted

 
{code:java}
ProductAggSuite:
*** RUN ABORTED ***
  java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$
  at org.apache.spark.sql.catalyst.expressions.codegen.JavaCode$.variable(javaCode.scala:64)
  at org.apache.spark.sql.catalyst.expressions.codegen.JavaCode$.isNullVariable(javaCode.scala:77)
  at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:200)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:196)
  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.$anonfun$create$1(GenerateSafeProjection.scala:156)
  at scala.collection.immutable.List.map(List.scala:293)
  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:153)
  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:39)
  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1369) {code}
 ",LuciferYang,LuciferYang,Major,Resolved,Fixed,15/Jun/23 05:43,25/Jun/23 23:52
Bug,SPARK-44074,13540299,`Logging plan changes for execution` test failed,"run {{build/sbt clean ""sql/test"" -Dtest.exclude.tags=org.apache.spark.tags.ExtendedSQLTest,org.apache.spark.tags.SlowSQLTest}}

{{}}
{code:java}
2023-06-15T19:58:34.4105460Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32mQueryExecutionSuite:�[0m�[0m
2023-06-15T19:58:34.5395268Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info to a file (77 milliseconds)�[0m�[0m
2023-06-15T19:58:34.5856902Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info to an existing file (49 milliseconds)�[0m�[0m
2023-06-15T19:58:34.6099849Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info to non-existing folder (25 milliseconds)�[0m�[0m
2023-06-15T19:58:34.6136467Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info by invalid path (4 milliseconds)�[0m�[0m
2023-06-15T19:58:34.6425071Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info to a file - explainMode=formatted (28 milliseconds)�[0m�[0m
2023-06-15T19:58:34.7084916Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- limit number of fields by sql config (66 milliseconds)�[0m�[0m
2023-06-15T19:58:34.7432299Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- check maximum fields restriction (34 milliseconds)�[0m�[0m
2023-06-15T19:58:34.7554546Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- toString() exception/error handling (11 milliseconds)�[0m�[0m
2023-06-15T19:58:34.7621424Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- SPARK-28346: clone the query plan between different stages (6 milliseconds)�[0m�[0m
2023-06-15T19:58:34.8001412Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[31m- Logging plan changes for execution *** FAILED *** (12 milliseconds)�[0m�[0m
2023-06-15T19:58:34.8007977Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[31m  testAppender.loggingEvents.exists(((x$10: org.apache.logging.log4j.core.LogEvent) => x$10.getMessage().getFormattedMessage().contains(expectedMsg))) was false (QueryExecutionSuite.scala:232)�[0m�[0m 

{code}
 

but run {{build/sbt ""sql/testOnly *QueryExecutionSuite""}} not this issue, need to investigate. ",LuciferYang,LuciferYang,Major,Resolved,Fixed,16/Jun/23 03:12,20/Jun/23 04:40
Bug,SPARK-44079,13540407,Json reader crashes when a different schema is present,"When using pyspark 3.4, we noticed that when reading a json file with a corrupted record the reader crashes. In pyspark 3.3 this worked fine.

{*}Code{*}:
{code:java}
from pyspark.sql.types import StructType, StructField, IntegerType, StringType
import json


data = """"""[{""a"": ""incorrect"", ""b"": ""correct""}]""""""
schema = StructType([StructField('a', IntegerType(), True), StructField('b', StringType(), True), StructField('_corrupt_record', StringType(), True)])


spark.read.option(""mode"", ""PERMISSIVE"").option(""multiline"",""true"").schema(schema).json(spark.sparkContext.parallelize([data])).show(truncate=False){code}
*Used packages:*
 * Pyspark==3.4.0
 * python==3.10.0
 * delta-spark==2.4.0

 
spark_jars=(
  ""org.apache.spark:spark-avro_2.12:3.4.0""
  "",io.delta:delta-core_2.12:2.4.0""
  "",com.databricks:spark-xml_2.12:0.16.0""
)
 

{*}Expected behaviour{*}:
|a|b|_corrupt_record|
|null|null|[\\{""a"": ""incorrect"", ""b"": ""correct""}]|

 

{*}Actual behaviour{*}:
{code:java}
 
*** py4j.protocol.Py4JJavaError: An error occurred while calling o104.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 2.0 failed 1 times, most recent failure: Lost task 4.0 in stage 2.0 (TID 9) (charlottesmbp2.home executor driver): java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 1
        at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.genericGet(rows.scala:201)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getAs(rows.scala:35)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.get(rows.scala:37)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.get$(rows.scala:37)
        at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.get(rows.scala:195)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.$anonfun$toResultRow$2(FailureSafeParser.scala:47)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.$anonfun$toResultRow$1(FailureSafeParser.scala:47)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:64)
        at org.apache.spark.sql.DataFrameReader.$anonfun$json$10(DataFrameReader.scala:431)
        at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
        at org.apache.spark.scheduler.Task.run(Task.scala:139)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.base/java.lang.Thread.run(Thread.java:1589)
Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
        at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
        at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)
        at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)
        at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)
        at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
        at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)
        at org.apache.spark.sql.Dataset.head(Dataset.scala:3161)
        at org.apache.spark.sql.Dataset.take(Dataset.scala:3382)
        at org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)
        at org.apache.spark.sql.Dataset.showString(Dataset.scala:323)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
        at java.base/java.lang.reflect.Method.invoke(Method.java:578)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.base/java.lang.Thread.run(Thread.java:1589)
Caused by: java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 1
        at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.genericGet(rows.scala:201)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getAs(rows.scala:35)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.get(rows.scala:37)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.get$(rows.scala:37)
        at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.get(rows.scala:195)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.$anonfun$toResultRow$2(FailureSafeParser.scala:47)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.$anonfun$toResultRow$1(FailureSafeParser.scala:47)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:64)
        at org.apache.spark.sql.DataFrameReader.$anonfun$json$10(DataFrameReader.scala:431)
        at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
        at org.apache.spark.scheduler.Task.run(Task.scala:139)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        ... 1 more {code}",fanjia,charlottevdscheun,Major,Resolved,Fixed,16/Jun/23 13:20,29/Jun/23 13:38
Bug,SPARK-44094,13540511,Upgrade Apache Arrow to 12.0.1,,dongjoon,dongjoon,Minor,Resolved,Fixed,18/Jun/23 08:43,18/Jun/23 20:27
Bug,SPARK-44129,13540831,"Use ""3.5.0"" for `master` branch until creating `branch-3.5`",,gurwls223,dongjoon,Minor,Resolved,Fixed,21/Jun/23 01:29,21/Jun/23 02:20
Bug,SPARK-44134,13540908,Can't set resources (GPU/FPGA) to 0 when they are set to positive value in spark-defaults.conf,"With resource aware scheduling, if you specify a default value in the spark-defaults.conf, a user can't override that to set it to 0.

Meaning spark-defaults.conf has something like:
{{spark.executor.resource.\{resourceName}.amount=1}}

{{spark.task.resource.\{resourceName}.amount}} =1

If the user tries to override when submitting an application with {{{}spark.executor.resource.\{resourceName}.amount{}}}=0 and {{spark.task.resource.\{resourceName}.amount}} =0, it gives the user an error:

 
{code:java}
23/06/21 09:12:57 ERROR Main: Failed to initialize Spark session.
org.apache.spark.SparkException: No executor resource configs were not specified for the following task configs: gpu
        at org.apache.spark.resource.ResourceProfile.calculateTasksAndLimitingResource(ResourceProfile.scala:206)
        at org.apache.spark.resource.ResourceProfile.$anonfun$limitingResource$1(ResourceProfile.scala:139)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.resource.ResourceProfile.limitingResource(ResourceProfile.scala:138)
        at org.apache.spark.resource.ResourceProfileManager.addResourceProfile(ResourceProfileManager.scala:95)
        at org.apache.spark.resource.ResourceProfileManager.<init>(ResourceProfileManager.scala:49)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:455)
        at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2704)
        at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:953){code}
This used to work, my guess is this may have gotten broken with the stage level scheduling feature.",tgraves,tgraves,Major,Resolved,Fixed,21/Jun/23 14:14,25/Jun/23 17:43
Bug,SPARK-44136,13540939,StateManager may get materialized in executor instead of driver in FlatMapGroupsWithStateExec,StateManager may get materialized in executor instead of driver in FlatMapGroupsWithStateExec because of a previous change https://issues.apache.org/jira/browse/SPARK-40411,dlgaobo,dlgaobo,Major,Resolved,Fixed,21/Jun/23 21:08,22/Jun/23 00:26
Bug,SPARK-44142,13541026,"Utility to convert python types to spark types compares Python ""type"" object rather than user's ""tpe"" for categorical data types","In the typehints utility that converts python types to spark types, the line:
{code:java}
    # categorical types
    elif isinstance(tpe, CategoricalDtype) or (isinstance(tpe, str) and type == ""category""):
        return types.LongType() {code}
uses Python's 'type' keyword in the comparison. Hence, it will always be false. Here, the user's type is actually stored in the variable 'tpe'.

 

 

See line [here|https://github.com/apache/spark/blob/1b4048bf62dddae7d324c4b12aa409a1bd456dc5/python/pyspark/pandas/typedef/typehints.py#LL217C7-L217C7].",tedjenks,tedjenks,Major,Resolved,Fixed,22/Jun/23 12:05,23/Jun/23 00:11
Bug,SPARK-44158,13541205,Remove unused `spark.kubernetes.executor.lostCheck.maxAttempts`,,dongjoon,dongjoon,Minor,Resolved,Fixed,23/Jun/23 18:08,23/Jun/23 20:49
Bug,SPARK-44161,13541218,Row as UDF inputs causes encoder errors,Ensure row inputs to udfs can be handled correctly.,zhenli,zhenli,Major,Resolved,Fixed,23/Jun/23 21:20,28/Jun/23 01:07
Bug,SPARK-44184,13541334,Remove a wrong doc about ARROW_PRE_0_15_IPC_FORMAT,,dongjoon,dongjoon,Major,Resolved,Fixed,25/Jun/23 22:21,26/Jun/23 01:53
Bug,SPARK-44185,13541343,Inconsistent path qualifying between catalog and data operations,"For example
 * CREATE TABLE statement with relative LOCATION will infer schema from files from the directory relative to the current working directory and store the directory relative to the warehouse path. 
 * CTAS statement with relative LOCATION cannot assert empty root path as it checks the wrong path it will finally use.
 * DataframeWriter does not qualify the path before checking",yao,yao,Major,Resolved,Fixed,26/Jun/23 03:49,03/Jul/23 09:27
Bug,SPARK-44199,13541463,CacheManager refreshes the fileIndex unnecessarily,"The CacheManager on this line [https://github.com/apache/spark/blob/680ca2e56f2c8fc759743ad6755f6e3b1a19c629/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala#L372] uses a prefix based matching to decide which file index needs to be refreshed. However, that can be incorrect if the users have paths which are not subdirectories but share prefixes. For example, in the function below:

 
{code:java}
  private def refreshFileIndexIfNecessary(
      fileIndex: FileIndex,
      fs: FileSystem,
      qualifiedPath: Path): Boolean = {
    val prefixToInvalidate = qualifiedPath.toString
    val needToRefresh = fileIndex.rootPaths
      .map(_.makeQualified(fs.getUri, fs.getWorkingDirectory).toString)
      .exists(_.startsWith(prefixToInvalidate))
    if (needToRefresh) fileIndex.refresh()
    needToRefresh
  } {code}
{{If the prefixToInvalidate is s3://bucket/mypath/table_dir and the file index has one of the root paths as s3://bucket/mypath/table_dir_2/part=1, then the needToRefresh will be true and the file index gets refreshed unnecessarily. This is not just wasted CPU cycles but can cause query failures as well, if there are access restrictions to the path being refreshed.}}",vihangk1,vihangk1,Major,Resolved,Fixed,26/Jun/23 18:30,03/Jul/23 23:08
Bug,SPARK-44204,13541495,Add missing recordHiveCall for getPartitionNames,,chengpan,chengpan,Minor,Resolved,Fixed,27/Jun/23 03:25,27/Jun/23 08:42
Bug,SPARK-44215,13541609,Client receives zero number of chunks in merge meta response which doesn't trigger fallback to unmerged blocks,"We still see instances of the server returning 0 {{numChunks}} in {{mergedMetaResponse}} which causes the executor to fail with {{ArithmeticException}}. 
{code}
java.lang.ArithmeticException: / by zero
	at org.apache.spark.storage.PushBasedFetchHelper.createChunkBlockInfosFromMetaResponse(PushBasedFetchHelper.scala:128)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1047)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:90)
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
{code}
Here the executor doesn't fallback to fetch un-merged blocks and this also doesn't result in a {{FetchFailure}}. So, the application fails.",csingh,csingh,Major,Resolved,Fixed,27/Jun/23 16:55,06/Jul/23 01:52
Bug,SPARK-44241,13541900,Set io.connectionTimeout/connectionCreationTimeout to zero or negative will cause executor incessantes cons/destructions,"{code:java}
2023-06-28 14:57:23 CST Bootstrap WARN - Failed to set channel option 'CONNECT_TIMEOUT_MILLIS' with value '-1000' for channel '[id: 0xf4b54a73]'
java.lang.IllegalArgumentException: connectTimeoutMillis : -1000 (expected: >= 0)
	at io.netty.util.internal.ObjectUtil.checkPositiveOrZero(ObjectUtil.java:144) ~[netty-common-4.1.74.Final.jar:4.1.74.Final] {code}",yao,yao,Major,Resolved,Fixed,29/Jun/23 08:03,30/Jun/23 10:34
Bug,SPARK-44245,13541919,pyspark.sql.dataframe doctests can behave differently,,cdkrot,cdkrot,Minor,Resolved,Fixed,29/Jun/23 09:57,04/Jul/23 23:49
Bug,SPARK-44248,13541987,Kafka Source v2 should return preferred locations,"DSv2 Kafka streaming source seems to miss setting the preferred location, which may destroy the purpose of cache for Kafka consumer (connection) & fetched data.

For DSv1, we have set the preferred location in RDD.

For DSv2, we should provide the info. in input partition, but we don't add the information into KafkaBatchInputPartition.",siying,siying,Major,Resolved,Fixed,29/Jun/23 18:21,29/Jun/23 21:50
Bug,SPARK-44251,13541996,Potential for incorrect results or NPE when full outer USING join has null key value,"The following query produces incorrect results:
{noformat}
create or replace temp view v1 as values (1, 2), (null, 7) as (c1, c2);
create or replace temp view v2 as values (2, 3) as (c1, c2);

select explode(array(c1)) as x
from v1
full outer join v2
using (c1);

-1   <== should be null
1
2
{noformat}
The following query fails with a {{NullPointerException}}:
{noformat}
create or replace temp view v1 as values ('1', 2), (null, 7) as (c1, c2);
create or replace temp view v2 as values ('2', 3) as (c1, c2);

select explode(array(c1)) as x
from v1
full outer join v2
using (c1);

23/06/25 17:06:39 ERROR Executor: Exception in task 0.0 in stage 14.0 (TID 11)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.generate_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.smj_consumeFullOuterJoinRow_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.wholestagecodegen_findNextJoinRows_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
...
{noformat}
",bersprockets,bersprockets,Major,Resolved,Fixed,29/Jun/23 19:53,11/Jul/23 03:22
Bug,MAPREDUCE-7434,13525684,Fix ShuffleHandler tests,"
https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java8-linux-x86_64/1143/testReport/junit/org.apache.hadoop.mapred/TestShuffleHandler/testMapFileAccess/

{code}
Error Message
Server returned HTTP response code: 500 for URL: http://127.0.0.1:13562/mapOutput?job=job_1111111111111_0001&reduce=0&map=attempt_1111111111111_0001_m_000001_0
Stacktrace
java.io.IOException: Server returned HTTP response code: 500 for URL: http://127.0.0.1:13562/mapOutput?job=job_1111111111111_0001&reduce=0&map=attempt_1111111111111_0001_m_000001_0
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1902)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1500)
	at org.apache.hadoop.mapred.TestShuffleHandler.testMapFileAccess(TestShuffleHandler.java:292)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:750)
Standard Output
12:04:17.466 [Time-limited test] DEBUG o.a.h.m.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.mapred.ShuffleHandler$ShuffleMetrics.shuffleConnections with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[# of current shuffle connections])
12:04:17.466 [Time-limited test] DEBUG o.a.h.m.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableCounterLong org.apache.hadoop.mapred.ShuffleHandler$ShuffleMetrics.shuffleOutputBytes with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Shuffle output in bytes])
12:04:17.466 [Time-limited test] DEBUG o.a.h.m.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableCounterInt org.apache.hadoop.mapred.ShuffleHandler$ShuffleMetrics.shuffleOutputsFailed with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[# of failed shuffle outputs])
12:04:17.466 [Time-limited test] DEBUG o.a.h.m.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableCounterInt org.apache.hadoop.mapred.ShuffleHandler$ShuffleMetrics.shuffleOutputsOK with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[# of succeeeded shuffle outputs])
12:04:17.466 [Time-limited test] DEBUG o.a.h.m.impl.MetricsSystemImpl - ShuffleMetrics, Shuffle output metrics
12:04:17.467 [Time-limited test] DEBUG o.a.hadoop.service.AbstractService - Service: mapreduce_shuffle entered state INITED
12:04:17.477 [Time-limited test] DEBUG o.a.hadoop.service.AbstractService - Config has been overridden during init
12:04:17.478 [Time-limited test] INFO  org.apache.hadoop.mapred.IndexCache - IndexCache created with max memory = 10485760
12:04:17.479 [Time-limited test] DEBUG o.a.h.m.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.mapred.ShuffleHandler$ShuffleMetrics.shuffleConnections with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[# of current shuffle connections])
12:04:17.479 [Time-limited test] DEBUG o.a.h.m.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableCounterLong org.apache.hadoop.mapred.ShuffleHandler$ShuffleMetrics.shuffleOutputBytes with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[Shuffle output in bytes])
12:04:17.479 [Time-limited test] DEBUG o.a.h.m.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableCounterInt org.apache.hadoop.mapred.ShuffleHandler$ShuffleMetrics.shuffleOutputsFailed with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[# of failed shuffle outputs])
12:04:17.479 [Time-limited test] DEBUG o.a.h.m.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableCounterInt org.apache.hadoop.mapred.ShuffleHandler$ShuffleMetrics.shuffleOutputsOK with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, valueName=Time, about=, interval=10, type=DEFAULT, value=[# of succeeeded shuffle outputs])
12:04:17.479 [Time-limited test] DEBUG o.a.h.m.impl.MetricsSystemImpl - ShuffleMetrics, Shuffle output metrics
12:04:17.482 [Time-limited test] INFO  o.a.hadoop.mapred.ShuffleHandler - mapreduce_shuffle listening on port 13562
12:04:17.482 [Time-limited test] DEBUG o.a.hadoop.service.AbstractService - Service mapreduce_shuffle is started
12:04:17.483 [Time-limited test] INFO  o.a.hadoop.mapred.ShuffleHandler - Added token for job_1111111111111_0001
12:04:17.486 [ShuffleHandler Netty Worker #0] DEBUG o.a.hadoop.mapred.ShuffleHandler - ShuffleChannelInitializer init; channel='a9df992f'
12:04:17.487 [ShuffleHandler Netty Worker #0] DEBUG o.a.hadoop.mapred.ShuffleHandler - Executing channelActive; channel='a9df992f'
12:04:17.487 [ShuffleHandler Netty Worker #0] DEBUG o.a.hadoop.mapred.ShuffleHandler - Added channel: [id: 0xa9df992f, L:/127.0.0.1:13562 - R:/127.0.0.1:53014], channel id: a9df992f. Accepted number of connections=1
12:04:17.489 [ShuffleHandler Netty Worker #0] DEBUG o.a.hadoop.mapred.ShuffleHandler - Received HTTP request: HttpObjectAggregator$AggregatedFullHttpRequest(decodeResult: success, version: HTTP/1.1, content: CompositeByteBuf(ridx: 0, widx: 0, cap: 0, components=0))
GET /mapOutput?job=job_1111111111111_0001&reduce=0&map=attempt_1111111111111_0001_m_000001_0 HTTP/1.1
name: mapreduce
version: 1.0.0
UrlHash: +OVYyZ3+ni316LsqZvZcFqgV/H8=
User-Agent: Java/1.8.0_352
Host: 127.0.0.1:13562
Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2
Connection: keep-alive
content-length: 0, channel='a9df992f'
12:04:17.489 [ShuffleHandler Netty Worker #0] DEBUG o.a.hadoop.mapred.ShuffleHandler - Received from request header: ShuffleVersion=1.0.0 header name=mapreduce, channel id: a9df992f
12:04:17.489 [ShuffleHandler Netty Worker #0] DEBUG o.a.hadoop.mapred.ShuffleHandler - RECV: /mapOutput?job=job_1111111111111_0001&reduce=0&map=attempt_1111111111111_0001_m_000001_0
  mapId: [attempt_1111111111111_0001_m_000001_0]
  reduceId: [0]
  jobId: [job_1111111111111_0001]
  keepAlive: false
  channel id: a9df992f
12:04:17.489 [ShuffleHandler Netty Worker #0] DEBUG o.a.hadoop.mapred.ShuffleHandler - Verifying request. encryptedURL:13562/mapOutput?job=job_1111111111111_0001&reduce=0&map=attempt_1111111111111_0001_m_000001_0, hash:sqZvZcFqgV/H8, channel id: a9df992f
12:04:17.490 [ShuffleHandler Netty Worker #0] DEBUG o.a.hadoop.mapred.ShuffleHandler - Fetcher request verified. encryptedURL: 13562/mapOutput?job=job_1111111111111_0001&reduce=0&map=attempt_1111111111111_0001_m_000001_0, reply: ii/HLwQUsdwdA, channel id: a9df992f
12:04:17.491 [ShuffleHandler Netty Worker #0] DEBUG o.a.hadoop.mapred.ShuffleHandler - Retrieved pathInfo for AttemptPathIdentifier{attemptId='attempt_1111111111111_0001_m_000001_0', jobId='job_1111111111111_0001'} check for corresponding loaded messages to determine whether it was loaded or cached
12:04:17.491 [ShuffleHandler Netty Worker #0] DEBUG org.apache.hadoop.mapred.IndexCache - IndexCache MISS: MapId attempt_1111111111111_0001_m_000001_0 not found
12:04:17.492 [ShuffleHandler Netty Worker #0] DEBUG o.a.h.f.s.i.IOStatisticsContextIntegration - Created instance IOStatisticsContextImpl{id=5, threadId=136, ioStatistics=counters=();
gauges=();
minimums=();
maximums=();
means=();
}
12:04:17.493 [ShuffleHandler Netty Worker #0] DEBUG o.apache.hadoop.io.nativeio.NativeIO - Got UserName jenkins for ID 910 from the native implementation
12:04:17.494 [ShuffleHandler Netty Worker #0] DEBUG o.apache.hadoop.io.nativeio.NativeIO - Got GroupName jenkins for ID 910 from the native implementation
12:04:17.498 [ShuffleHandler Netty Worker #0] ERROR o.a.hadoop.mapred.ShuffleHandler - Shuffle error while populating headers. Channel id: a9df992f
java.io.IOException: Error Reading IndexFile
	at org.apache.hadoop.mapred.IndexCache.readIndexFileToCache(IndexCache.java:123) ~[hadoop-mapreduce-client-core-3.4.0-SNAPSHOT.jar:na]
	at org.apache.hadoop.mapred.IndexCache.getIndexInformation(IndexCache.java:68) ~[hadoop-mapreduce-client-core-3.4.0-SNAPSHOT.jar:na]
	at org.apache.hadoop.mapred.ShuffleChannelHandler.getMapOutputInfo(ShuffleChannelHandler.java:360) [classes/:na]
	at org.apache.hadoop.mapred.ShuffleChannelHandler.populateHeaders(ShuffleChannelHandler.java:381) [classes/:na]
	at org.apache.hadoop.mapred.ShuffleChannelHandler.channelRead0(ShuffleChannelHandler.java:275) [classes/:na]
	at org.apache.hadoop.mapred.ShuffleChannelHandler.channelRead0(ShuffleChannelHandler.java:130) [classes/:na]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:327) [netty-codec-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:299) [netty-codec-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496) [netty-transport-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995) [netty-common-4.1.77.Final.jar:4.1.77.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.77.Final.jar:4.1.77.Final]
	at java.lang.Thread.run(Thread.java:750) [na:1.8.0_352]
Caused by: java.io.IOException: Owner 'jenkins' for path /tmp/test-shuffle-channel-handler1169959588626711767/job_1111111111111_0001/testUser/attempt_1111111111111_0001_m_000001_0/index did not match expected owner 'testUser'
	at org.apache.hadoop.io.SecureIOUtils.checkStat(SecureIOUtils.java:299) ~[hadoop-common-3.4.0-SNAPSHOT.jar:na]
	at org.apache.hadoop.io.SecureIOUtils.forceSecureOpenFSDataInputStream(SecureIOUtils.java:183) ~[hadoop-common-3.4.0-SNAPSHOT.jar:na]
	at org.apache.hadoop.io.SecureIOUtils.openFSDataInputStream(SecureIOUtils.java:161) ~[hadoop-common-3.4.0-SNAPSHOT.jar:na]
	at org.apache.hadoop.mapred.SpillRecord.<init>(SpillRecord.java:71) ~[hadoop-mapreduce-client-core-3.4.0-SNAPSHOT.jar:na]
	at org.apache.hadoop.mapred.SpillRecord.<init>(SpillRecord.java:62) ~[hadoop-mapreduce-client-core-3.4.0-SNAPSHOT.jar:na]
	at org.apache.hadoop.mapred.IndexCache.readIndexFileToCache(IndexCache.java:119) ~[hadoop-mapreduce-client-core-3.4.0-SNAPSHOT.jar:na]
	... 32 common frames omitted
12:04:17.502 [Time-limited test] DEBUG o.a.hadoop.service.AbstractService - Service: mapreduce_shuffle entered state STOPPED
12:04:17.503 [ShuffleHandler Netty Worker #0] DEBUG o.a.hadoop.mapred.ShuffleHandler - Executing channelInactive; channel='a9df992f'
12:04:17.503 [ShuffleHandler Netty Worker #0] DEBUG o.a.hadoop.mapred.ShuffleHandler - New value of Accepted number of connections=0
{code}",tdomok,tdomok,Major,Resolved,Fixed,22/Feb/23 06:54,01/Mar/23 15:14
Bug,MAPREDUCE-7435,13530277,ManifestCommitter OOM on azure job,"I've got some reports of spark jobs OOM if the manifest committer is used through abfs.

either the manifests are using too much memory, or something is not working with azure stream memory use (or both).

before proposing a solution, first step should be to write a test to load many, many manifests, each with lots of dirs and files to see what breaks.

note: we did have OOM issues with the s3a committer, on teragen but those structures have to include every etag of every block, so the manifest size is O(blocks); the new committer is O(files + dirs).

{code}
java.lang.OutOfMemoryError: Java heap space
at org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:314)
at org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:267)
at java.io.DataInputStream.read(DataInputStream.java:149)
at com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.ensureLoaded(ByteSourceJsonBootstrapper.java:539)
at com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.detectEncoding(ByteSourceJsonBootstrapper.java:133)
at com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.constructParser(ByteSourceJsonBootstrapper.java:256)
at com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1656)
at com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:1085)
at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3585)
at org.apache.hadoop.util.JsonSerialization.fromJsonStream(JsonSerialization.java:164)
at org.apache.hadoop.util.JsonSerialization.load(JsonSerialization.java:279)
at org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.TaskManifest.load(TaskManifest.java:361)
at org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.ManifestStoreOperationsThroughFileSystem.loadTaskManifest(ManifestStoreOperationsThroughFileSystem.java:133)
at org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage.lambda$loadManifest$6(AbstractJobOrTaskStage.java:493)
at org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage$$Lambda$231/1813048085.apply(Unknown Source)
at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:543)
at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:524)
at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding$$Lambda$217/489150849.apply(Unknown Source)
at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:445)
at org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage.loadManifest(AbstractJobOrTaskStage.java:492)
at org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage.fetchTaskManifest(LoadManifestsStage.java:170)
at org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage.processOneManifest(LoadManifestsStage.java:138)
at org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage$$Lambda$229/137752948.run(Unknown Source)
at org.apache.hadoop.util.functional.TaskPool$Builder.lambda$runParallel$0(TaskPool.java:410)
at org.apache.hadoop.util.functional.TaskPool$Builder$$Lambda$230/467893357.run(Unknown Source)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:750)

{code}

",stevel@apache.org,stevel@apache.org,Major,Resolved,Fixed,27/Mar/23 17:34,12/Jun/23 12:44
Bug,MAPREDUCE-7437,13533564,MR Fetcher class to use an AtomicInteger to generate IDs.,"I'm having to do this to get MAPREDUCE-7435 through the build; spotbugs is complaining about the Fetcher constructor incrementing a non-static shared counter. Which is true, just odd it has only just surfaced.

going to fix as a standalone patch but include that in the commit chain of that PR too",stevel@apache.org,stevel@apache.org,Major,Resolved,Fixed,21/Apr/23 14:03,08/Jun/23 20:28
Bug,MAPREDUCE-7441,13541189,Race condition in closing FadvisedFileRegion,"This issue is similar to the one described in MAPREDUCE-7095, just for FadvisedFileRegion.transferSuccessful. There are warning messages when multiple threads are calling the transferSuccessful method:

{code:java}
2023-05-25 08:41:57,288 WARN org.apache.hadoop.mapred.FadvisedFileRegion: Failed to manage OS cache for /hadoop/data04/yarn/nm/usercache/hive/appcache/application_1684916804740_8245/output/attempt_1684916804740_8245_1_00_001154_0_10003/file.out
EBADF: Bad file descriptor
at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)
at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:271)
at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:148)
at org.apache.hadoop.mapred.FadvisedFileRegion.transferSuccessful(FadvisedFileRegion.java:163)
at org.apache.hadoop.mapred.ShuffleChannelHandler.lambda$sendMapOutput$0(ShuffleChannelHandler.java:516)
at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)


{code}
",,bteke,Major,Resolved,Fixed,23/Jun/23 15:09,23/Jun/23 18:42
Bug,KAFKA-14564,13516379,Upgrade Netty to 4.1.86.Final to fix CVEs,"4.1.86 fixes two CVEs:
 * [https://nvd.nist.gov/vuln/detail/CVE-2022-41881]
 * [https://nvd.nist.gov/vuln/detail/CVE-2022-41915]

 ",bribera,bribera,Major,Resolved,Fixed,03/Jan/23 21:31,08/Apr/23 07:58
Bug,KAFKA-14571,13516488,ZkMetadataCache.getClusterMetadata is missing rack information in aliveNodes,"ZkMetadataCache.getClusterMetadata returns a Cluster object where the aliveNodes are missing their rack info.

when ZkMetadataCache updates the metadataSnapshot, includes the rack in `aliveBrokers` but not in `aliveNodes` ",ecomar,ecomar,Minor,Resolved,Fixed,04/Jan/23 14:41,04/Jan/23 23:42
Bug,KAFKA-14609,13517252,Kafka Streams Processor API cannot use state stores,"The recently introduced Kafka Streams Processor API (since 3.3, https://issues.apache.org/jira/browse/KAFKA-13654) likely has a bug with regards to using state stores. The [getStateStore|https://javadoc.io/static/org.apache.kafka/kafka-streams/3.3.1/org/apache/kafka/streams/processor/api/ProcessingContext.html#getStateStore-java.lang.String-] method returns null, even though the store has been registered according to the docs. The old transformer API still works. I created a small project that demonstrates the behavior. It uses both methods to register a store for the transformer, as well as the processor API: https://github.com/bakdata/kafka-streams-state-store-demo/blob/main/src/test/java/com/bakdata/kafka/StreamsStateStoreTest.java",,philipp94831,Major,Resolved,Fixed,09/Jan/23 14:30,09/Jan/23 17:48
Bug,KAFKA-14612,13517497,Topic config records written to log even when topic creation fails,"Config records are added when handling a `CreateTopics` request here: [https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java#L549.] If the subsequent validations fail and the topic is not created, these records will still be written to the log.",andyg2,hachikuji,Major,Resolved,Fixed,10/Jan/23 20:47,13/Jan/23 00:27
Bug,KAFKA-14618,13517929,Off by one error in generated snapshot IDs causes misaligned fetching,"We implemented new snapshot generation logic here: [https://github.com/apache/kafka/pull/12983]. A few days prior to this patch getting merged, we had changed the `RaftClient` API to pass the _exclusive_ offset when generating snapshots instead of the inclusive offset: [https://github.com/apache/kafka/pull/12981]. Unfortunately, the new snapshot generation logic was not updated accordingly. The consequence of this is that the state on replicas can get out of sync. In the best case, the followers fail replication because the offset after loading a snapshot is no longer aligned on a batch boundary.",jsancio,hachikuji,Blocker,Resolved,Fixed,12/Jan/23 22:51,13/Jan/23 23:19
Bug,KAFKA-14623,13518370,OAuth's HttpAccessTokenRetriever potentially leaks secrets in logging  ,"The OAuth code that communicates via HTTP with the IdP (HttpAccessTokenRetriever.java) includes logging that outputs the request and response payloads. Among them are:
 * [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/secured/HttpAccessTokenRetriever.java#L265]
 * [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/secured/HttpAccessTokenRetriever.java#L274]
 * [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/secured/HttpAccessTokenRetriever.java#L320]

It should be determined if there are other places sensitive information might be inadvertently exposed.",kirktrue,kirktrue,Major,Resolved,Fixed,13/Jan/23 21:31,29/Jun/23 21:07
Bug,KAFKA-14637,13520105,Upgrade to 3.4 from old versions (< 0.10) are failing due to incompatible meta.properties check,3.4 has a check in broker startup to ensure cluster.id is provided in `metadata.properties`. This is not always the case if the previous version of Kafka is < 0.10.,akhileshchg,akhileshchg,Blocker,Resolved,Fixed,18/Jan/23 19:40,19/Jan/23 18:17
Bug,KAFKA-14639,13520189,Kafka CooperativeStickyAssignor revokes/assigns partition in one rebalance cycle,"I have an application that runs 6 consumers in parallel. I am getting some unexpected results when I use {{{}CooperativeStickyAssignor{}}}. If I understand the mechanism correctly, if the consumer looses partition in one rebalance cycle, the partition should be assigned in the next rebalance cycle.

This assumption is based on the [RebalanceProtocol|https://kafka.apache.org/31/javadoc/org/apache/kafka/clients/consumer/ConsumerPartitionAssignor.RebalanceProtocol.html] documentation and few blog posts that describe the protocol, like [this one|https://www.confluent.io/blog/cooperative-rebalancing-in-kafka-streams-consumer-ksqldb/] on Confluent blog.
{quote}The assignor should not reassign any owned partitions immediately, but instead may indicate consumers the need for partition revocation so that the revoked partitions can be reassigned to other consumers in the next rebalance event. This is designed for sticky assignment logic which attempts to minimize partition reassignment with cooperative adjustments.
{quote}
{quote}Any member that revoked partitions then rejoins the group, triggering a second rebalance so that its revoked partitions can be assigned. Until then, these partitions are unowned and unassigned.
{quote}
These are the logs from the application that uses {{{}protocol='cooperative-sticky'{}}}. In the same rebalance cycle ({{{}generationId=640{}}}) {{partition 74}} moves from {{consumer-3}} to {{{}consumer-4{}}}. I omitted the lines that are logged by the other 4 consumers.

Mind that the log is in reverse(bottom to top)
{code:java}
2022-12-14 11:18:24 1 — [consumer-3] x.y.z.MyRebalanceHandler1 : New partition assignment: partition-59, seek to min common offset: 85120524
2022-12-14 11:18:24 1 — [consumer-3] x.y.z.MyRebalanceHandler2 : Partitions [partition-59] assigned successfully
2022-12-14 11:18:24 1 — [consumer-3] x.y.z.MyRebalanceHandler1 : Partitions assigned: [partition-59]
2022-12-14 11:18:24 1 — [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Adding newly assigned partitions: partition-59
2022-12-14 11:18:24 1 — [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Notifying assignor about the new Assignment(partitions=[partition-59])
2022-12-14 11:18:24 1 — [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Request joining group due to: need to revoke partitions [partition-26, partition-74] as indicated by the current assignment and re-join
2022-12-14 11:18:24 1 — [consumer-3] x.y.z.MyRebalanceHandler2 : Partitions [partition-26, partition-74] revoked successfully
2022-12-14 11:18:24 1 — [consumer-3] x.y.z.MyRebalanceHandler1 : Finished removing partition data
2022-12-14 11:18:24 1 — [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] (Re-)joining group
2022-12-14 11:18:24 1 — [consumer-4] x.y.z.MyRebalanceHandler1 : New partition assignment: partition-74, seek to min common offset: 107317730
2022-12-14 11:18:24 1 — [consumer-4] x.y.z.MyRebalanceHandler2 : Partitions [partition-74] assigned successfully
2022-12-14 11:18:24 1 — [consumer-4] x.y.z.MyRebalanceHandler1 : Partitions assigned: [partition-74]
2022-12-14 11:18:24 1 — [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Adding newly assigned partitions: partition-74
2022-12-14 11:18:24 1 — [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Notifying assignor about the new Assignment(partitions=[partition-74])
2022-12-14 11:18:24 1 — [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Request joining group due to: need to revoke partitions [partition-57] as indicated by the current assignment and re-join
2022-12-14 11:18:24 1 — [consumer-4] x.y.z.MyRebalanceHandler2 : Partitions [partition-57] revoked successfully
2022-12-14 11:18:24 1 — [consumer-4] x.y.z.MyRebalanceHandler1 : Finished removing partition data
2022-12-14 11:18:22 1 — [consumer-3] x.y.z.MyRebalanceHandler1 : Partitions revoked: [partition-26, partition-74]
2022-12-14 11:18:22 1 — [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Revoke previously assigned partitions partition-26, partition-74
2022-12-14 11:18:22 1 — [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Updating assignment with\n\tAssigned partitions: [partition-59]\n\tCurrent owned partitions: [partition-26, partition-74]\n\tAdded partitions (assigned - owned): [partition-59]\n\tRevoked partitions (owned - assigned): [partition-26, partition-74]
2022-12-14 11:18:22 1 — [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Successfully synced group in generation Generation{generationId=640, memberId='partition-3-my-client-id-my-group-id-c31afd19-3f22-43cb-ad07-9088aa98d3af', protocol='cooperative-sticky'}
2022-12-14 11:18:22 1 — [consumer-3] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-3-my-client-id-my-group-id, groupId=my-group-id] Successfully joined group with generation Generation{generationId=640, memberId='partition-3-my-client-id-my-group-id-c31afd19-3f22-43cb-ad07-9088aa98d3af', protocol='cooperative-sticky'}
2022-12-14 11:18:22 1 — [consumer-4] x.y.z.MyRebalanceHandler1 : Partitions revoked: [partition-57]
2022-12-14 11:18:22 1 — [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Revoke previously assigned partitions partition-57
2022-12-14 11:18:22 1 — [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Updating assignment with\n\tAssigned partitions: [partition-74]\n\tCurrent owned partitions: [partition-57]\n\tAdded partitions (assigned - owned): [partition-74]\n\tRevoked partitions (owned - assigned): [partition-57]
2022-12-14 11:18:21 1 — [id-1] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Successfully synced group in generation Generation{generationId=640, memberId='partition-4-my-client-id-my-group-id-ae2af665-edc9-4a8e-b658-98372d142477', protocol='cooperative-sticky'}
2022-12-14 11:18:21 1 — [consumer-4] o.a.k.c.c.internals.ConsumerCoordinator : [Consumer clientId=partition-4-my-client-id-my-group-id, groupId=my-group-id] Successfully joined group with generation Generation{generationId=640, memberId='partition-4-my-client-id-my-group-id-ae2af665-edc9-4a8e-b658-98372d142477', protocol='cooperative-sticky'} {code}
Is this expected?

Kafka client version is 3.2.1.",pnee,bojanblagojevic,Major,Resolved,Fixed,19/Jan/23 11:51,02/May/23 11:48
Bug,KAFKA-14644,13520862,Process should stop after failure in raft IO thread,"We have seen a few cases where an unexpected error in the Raft IO thread causes the process to enter a zombie state where it is no longer participating in the raft quorum. In this state, a controller can no longer become leader or help in elections, and brokers can no longer update metadata. It may be better to stop the process in this case since there is no way to recover.",hachikuji,hachikuji,Major,Resolved,Fixed,20/Jan/23 22:38,04/May/23 07:23
Bug,KAFKA-14645,13521008,Plugin classloader not used when retrieving connector plugin config defs via REST API,"We don't switch to the plugin classloader when servicing requests to the {{GET /connector-plugins/<type>/config}} endpoint, which can result in classloading failures for, e.g., properties that accept the name of a pluggable class to load.

Reported externally in [https://github.com/kcctl/kcctl/issues/266]",ChrisEgerton,ChrisEgerton,Major,Resolved,Fixed,23/Jan/23 15:58,28/Mar/23 16:43
Bug,KAFKA-14646,13521024,SubscriptionWrapper is of an incompatible version (Kafka Streams 3.2.3 -> 3.3.2),"Hey folks,
 
we've just updated an application from *_Kafka Streams 3.2.3 to 3.3.2_* and started getting the following exceptions:
{code:java}
org.apache.kafka.common.errors.UnsupportedVersionException: SubscriptionWrapper is of an incompatible version. {code}
After swiftly looking through the code, this exception is potentially thrown in two places:
 * [https://github.com/apache/kafka/blob/3.3.2/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionJoinForeignProcessorSupplier.java#L73-L78]
 ** Here the check was changed in Kafka 3.3.x: [https://github.com/apache/kafka/commit/9dd25ecd9ce17e608c6aba98e0422b26ed133c12]

 * [https://github.com/apache/kafka/blob/3.3.2/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionStoreReceiveProcessorSupplier.java#L94-L99]
 ** Here the check wasn't changed.

 
Is it possible that the second check in {{SubscriptionStoreReceiveProcessorSupplier<K, KO>}} was forgotten?
 
Any hints how to resolve this issue without a downgrade?
Since this only affects 2 of 15 topologies in the application, I'm hesitant to just downgrade to Kafka 3.2.3 again since the internal topics might already have been updated to use the ""new"" version of {{{}SubscriptionWrapper{}}}.
 
Related discussion in the Confluent Community Slack: [https://confluentcommunity.slack.com/archives/C48AHTCUQ/p1674497054507119]
h2. Stack trace
{code:java}
org.apache.kafka.streams.errors.StreamsException: Exception caught in process. taskId=1_8, processor=XXX-joined-changed-fk-subscription-registration-source, topic=topic.rev7-XXX-joined-changed-fk-subscription-registration-topic, partition=8, offset=12297976, stacktrace=org.apache.kafka.common.errors.UnsupportedVersionException: SubscriptionWrapper is of an incompatible version.

    at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:750)
    at org.apache.kafka.streams.processor.internals.TaskExecutor.processTask(TaskExecutor.java:100)
    at org.apache.kafka.streams.processor.internals.TaskExecutor.process(TaskExecutor.java:81)
    at org.apache.kafka.streams.processor.internals.TaskManager.process(TaskManager.java:1182)
    at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:770)
    at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:588)
    at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:550) {code}",mjsax,joschi,Major,Resolved,Fixed,23/Jan/23 18:12,14/Feb/23 14:31
Bug,KAFKA-14649,13521182,"Failures instantiating Connect plugins hides other plugins from REST API, or crash worker","Connect plugin path scanning evaluates the version() method of plugins to determine which version of a plugin to load, and what version to advertise as part of the REST API. This process involves reflectively constructing an instance of the class and calling the version method, which can fail in the following scenarios:

1. If a plugin throws an exception from a static initialization block
2. If a plugin does not have a default constructor (such as a non-static inner class)
3. If a plugin has a default constructor is not public
4. If a plugin throws an exception from the default constructor
5. If a plugin's version method throws an exception

If any of the above is true for any single connector or rest extension on the classpath or plugin.path, the plugin path scanning will exit early, and potentially hide other unrelated plugins. This is primarily an issue in development and test environments, because they are easy-to-make code mistakes that would generally not make it to a release. Exceptions from the version method, however, can cause the worker to fail to start up as they are uncaught.

It is desirable for the worker to instead log these exceptions and continue. This will prevent one mis-implemented plugin from affecting other plugins, while still causing integration tests to fail against the plugin itself. We can augment logging to make it clear how to correct these failures, where before it was rather opaque and difficult to debug.",gharris1727,gharris1727,Minor,Resolved,Fixed,24/Jan/23 22:33,03/Mar/23 14:55
Bug,KAFKA-14650,13521189,IQv2 can throw ConcurrentModificationException when accessing Tasks ,"From failure in *[PositionRestartIntegrationTest.verifyStore[cache=false, log=true, supplier=IN_MEMORY_WINDOW, kind=PAPI]|https://ci-builds.apache.org/job/Kafka/job/kafka/job/3.4/63/testReport/junit/org.apache.kafka.streams.integration/PositionRestartIntegrationTest/Build___JDK_11_and_Scala_2_13___verifyStore_cache_false__log_true__supplier_IN_MEMORY_WINDOW__kind_PAPI_/]*
java.util.ConcurrentModificationException
	at java.base/java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1208)
	at java.base/java.util.TreeMap$EntryIterator.next(TreeMap.java:1244)
	at java.base/java.util.TreeMap$EntryIterator.next(TreeMap.java:1239)
	at java.base/java.util.HashMap.putMapEntries(HashMap.java:508)
	at java.base/java.util.HashMap.putAll(HashMap.java:781)
	at org.apache.kafka.streams.processor.internals.Tasks.allTasksPerId(Tasks.java:361)
	at org.apache.kafka.streams.processor.internals.TaskManager.allTasks(TaskManager.java:1537)
	at org.apache.kafka.streams.processor.internals.StreamThread.allTasks(StreamThread.java:1278)
	at org.apache.kafka.streams.KafkaStreams.query(KafkaStreams.java:1921)
	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.iqv2WaitForResult(IntegrationTestUtils.java:168)
	at org.apache.kafka.streams.integration.PositionRestartIntegrationTest.shouldReachExpectedPosition(PositionRestartIntegrationTest.java:438)
	at org.apache.kafka.streams.integration.PositionRestartIntegrationTest.verifyStore(PositionRestartIntegrationTest.java:423)",guozhang,ableegoldman,Major,Resolved,Fixed,24/Jan/23 23:54,09/Feb/23 18:35
Bug,KAFKA-14654,13521543,Connectors have incorrect Thread Context Classloader during initialization,"This is the same underlying issue as https://issues.apache.org/jira/browse/KAFKA-8340 , which fixed the thread context classloader for service loaded plugins, and for plugins which are first loaded _after_ plugin path scanning.
However, that correction PR failed to fix the context classloader for Connector classes, which are first instantiated and initialized during plugin path scanning, via the versionFor method.",gharris1727,gharris1727,Major,Resolved,Fixed,25/Jan/23 17:54,25/May/23 14:23
Bug,KAFKA-14656,13521629,Brokers rejecting LISR during ZK migration,"During the ZK migration, the KRaft controller sends controller RPCs to the ZK brokers (LISR, UMR, etc). Since the migration can begin immediately after a ZK broker starts up with migration enabled, it is possible that this broker is not seen as alive by the rest of the brokers. This is due to the KRaft controller taking over before the ZK controller can send out UMR with the restarted broker.

 

The result is that the parts of the LISR sent by KRaft immediately after the metadata migration is rejected by brokers due the leader being offline. 

 

The fix for this is to send an UMR to all brokers after the migration with the set of alive brokers.",mumrah,mumrah,Blocker,Resolved,Fixed,26/Jan/23 15:43,31/Jan/23 14:50
Bug,KAFKA-14658,13521799,"When listening on fixed ports, defer port opening until we're ready","When we are listening on fixed ports, we should defer opening ports until we're ready to accept traffic. If we open the broker port too early, it can confuse monitoring and deployment systems. This is a particular concern when in KRaft mode, since in that mode, we create the SocketServer object earlier in the startup process than when in ZK mode.

The approach taken in this PR is to defer opening the acceptor port until Acceptor.start is called. Note that when we are listening on a random port, we continue to open the port ""early,"" in the SocketServer constructor. The reason for doing this is that there is no other way to find the random port number the kernel has selected. Since random port assignment is not used in production deployments, this should be reasonable.",cmccabe,cmccabe,Major,Resolved,Fixed,27/Jan/23 18:40,24/May/23 23:01
Bug,KAFKA-14659,13521821,source-record-write-[rate|total] metrics include filtered records,"Source tasks in Kafka connect offer two sets of metrics (documented in [ConnectMetricsRegistry.java|https://github.com/apache/kafka/blob/72cfc994f5675be349d4494ece3528efed290651/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectMetricsRegistry.java#L173-L191]):
||Metric||Description||
|source-record-poll-rate|The average per-second number of records produced/polled (before transformation) by this task belonging to the named source connector in this worker.|
|source-record-write-rate|The average per-second number of records output from the transformations and written to Kafka for this task belonging to the named source connector in this worker. This is after transformations are applied and excludes any records filtered out by the transformations.|

There are also corresponding ""-total"" metrics that capture the total number of records polled and written for the metrics above, respectively.

In short, the ""poll"" metrics capture the number of messages sourced pre-transformation/filtering, and the ""write"" metrics should capture the number of messages ultimately written to Kafka post-transformation/filtering. However, the implementation of the {{source-record-write-*}}  metrics _includes_ records filtered out by transformations (and also records that result in produce failures with the config {{{}errors.tolerance=all{}}}).
h3. Details

In [AbstractWorkerSourceTask.java|https://github.com/apache/kafka/blob/a382acd31d1b53cd8695ff9488977566083540b1/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractWorkerSourceTask.java#L389-L397], each source record is passed through the transformation chain where it is potentially filtered out, checked to see if it was in fact filtered out, and if so it is accounted for in the internal metrics via {{{}counter.skipRecord(){}}}.
{code:java}
for (final SourceRecord preTransformRecord : toSend) {         
    retryWithToleranceOperator.sourceRecord(preTransformRecord);
    final SourceRecord record = transformationChain.apply(preTransformRecord);            
    final ProducerRecord<byte[], byte[]> producerRecord = convertTransformedRecord(record);
    if (producerRecord == null || retryWithToleranceOperator.failed()) {                
        counter.skipRecord();
        recordDropped(preTransformRecord);
        continue;
    }
    ...
{code}
{{SourceRecordWriteCounter.skipRecord()}} is implemented as follows:
{code:java}
    ....
    public SourceRecordWriteCounter(int batchSize, SourceTaskMetricsGroup metricsGroup) {
        assert batchSize > 0;
        assert metricsGroup != null;
        this.batchSize = batchSize;
        counter = batchSize;
        this.metricsGroup = metricsGroup;
    }
    public void skipRecord() {
        if (counter > 0 && --counter == 0) {
            finishedAllWrites();
        }
    }
    ....
    private void finishedAllWrites() {
        if (!completed) {
            metricsGroup.recordWrite(batchSize - counter);
            completed = true;
        }
    }
{code}
For example: If a batch starts with 100 records, {{batchSize}} and {{counter}} will both be initialized to 100. If all 100 records get filtered out, {{counter}} will be decremented 100 times, and {{{}finishedAllWrites(){}}}will record the value 100 to the underlying {{source-record-write-*}}  metrics rather than 0, the correct value according to the documentation for these metrics.
h3. Solutions

Assuming the documentation correctly captures the intent of the {{source-record-write-*}}  metrics, it seems reasonable to fix these metrics such that filtered records do not get counted.

It may also be useful to add additional metrics to capture the rate and total number of records filtered out by transformations, which would require a KIP.

I'm not sure what the best way of accounting for produce failures in the case of {{errors.tolerance=all}} is yet. Maybe these failures deserve their own new metrics?",hgeraldino,cbeard,Minor,Resolved,Fixed,28/Jan/23 01:42,28/Feb/23 14:50
Bug,KAFKA-14660,13522063,Divide by zero security vulnerability (sonatype-2019-0422),"Looks like SonaType has picked up a ""Divide by Zero"" issue reported in a PR and, because the PR was never merged, is now reporting it as a security vulnerability in the latest Kafka Streams library.

 

See:
 * [Vulnerability: sonatype-2019-0422]([https://ossindex.sonatype.org/vulnerability/sonatype-2019-0422?component-type=maven&component-name=org.apache.kafka%2Fkafka-streams&utm_source=ossindex-client&utm_medium=integration&utm_content=1.7.0)]

 * [Original PR]([https://github.com/apache/kafka/pull/7414])

 

While it looks from the comments made by [~mjsax] and [~bbejeck] that the divide-by-zero is not really an issue, the fact that its now being reported as a vulnerability is, especially with regulators.

PITA, but we should consider either getting this vulnerability removed (Google wasn't very helpful in providing info on how to do this), or fixed (Again, not sure how to tag the fix as fixing this issue).  One option may just be to reopen the PR and merge (and then fix forward by switching it to throw an exception).",mjsax,BigAndy,Minor,Resolved,Fixed,30/Jan/23 12:30,08/Feb/23 05:40
Bug,KAFKA-14662,13522101,ACL listings in documentation are out of date,"ACLs listed in https://kafka.apache.org/documentation/#operations_resources_and_protocols are out of date. They only cover API keys up to 47 (OffsetDelete) and don't include DescribeClientQuotas, AlterClientQuotas, DescribeUserScramCredentials, AlterUserScramCredentials, DescribeQuorum, AlterPartition, UpdateFeatures, DescribeCluster, DescribeProducers, UnregisterBroker, DescribeTransactions, ListTransactions, AllocateProducerIds.

This is hard to keep up to date so we should consider whether this could be generated automatically.",tinaselenge,mimaison,Major,Resolved,Fixed,30/Jan/23 17:34,10/May/23 08:23
Bug,KAFKA-14664,13522148,Raft idle ratio is inaccurate,"The `poll-idle-ratio-avg` metric is intended to track how idle the raft IO thread is. When completely idle, it should measure 1. When saturated, it should measure 0. The problem with the current measurements is that they are treated equally with respect to time. For example, say we poll twice with the following durations:

Poll 1: 2s

Poll 2: 0s

Assume that the busy time is negligible, so 2s passes overall.

In the first measurement, 2s is spent waiting, so we compute and record a ratio of 1.0. In the second measurement, no time passes, and we record 0.0. The idle ratio is then computed as the average of these two values (1.0 + 0.0 / 2 = 0.5), which suggests that the process was busy for 1s, which overestimates the true busy time.

Instead, we should sum up the time waiting over the full interval. 2s passes total here and 2s is idle, so we should compute 1.0.",hachikuji,hachikuji,Major,Resolved,Fixed,31/Jan/23 03:09,15/Feb/23 22:40
Bug,KAFKA-14676,13523203,Token endpoint URL used for OIDC cannot be set on the JAAS config,"Kafka allows multiple clients within a JVM to use different SASL configurations by configuring the JAAS configuration in `sasl.jaas.config` instead of the JVM-wide system property. For SASL login, we reuse logins within a JVM by caching logins indexed by their sasl.jaas.config. This relies on login configs being overridable using `sasl.jaas.config`. 

KIP-768 ([https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=186877575)] added support for OIDC for SASL/OAUTHBEARER. The token endpoint used to acquire tokens can currently only be configured using the Kafka config `sasl.oauthbearer.token.endpoint.url`. This prevents different clients within a JVM from using different URLs. We need to either provide a way to override the URL within `sasl.jaas.config` or include more of the client configs in the LoginMetadata used as key for cached logins.",rsivaram,rsivaram,Major,Resolved,Fixed,06/Feb/23 09:57,12/Feb/23 20:04
Bug,KAFKA-14687,13523482,Possible issue on documentation : producerConfig -> retries,"Since Kafka 3, The documentation said [here|https://kafka.apache.org/documentation/#producerconfigs_retries] that
{code:java}
Allowing retries while setting enable.idempotence to false and max.in.flight.requests.per.connection to 1 will potentially change the ordering of records because if two batches are sent to a single partition, and the first fails and is retried but the second succeeds, then the records in the second batch may appear first. {code}
 

 

Or I think that a ""without"" is missing

Allowing retries while setting enable.idempotence to false and max.in.flight.requests.per.connection to 1 will potentially change the ordering of records

Should be

 

Allowing retries while setting enable.idempotence to false and +*without setting*+ max.in.flight.requests.per.connection to 1 will potentially change the ordering of records",,2me,Major,Resolved,Fixed,07/Feb/23 11:36,07/Feb/23 23:33
Bug,KAFKA-14693,13523824,KRaft Controller and ProcessExitingFaultHandler can deadlock shutdown,"h1. Problem

When the kraft controller encounters an error that it cannot handle it calls {{ProcessExitingFaultHandler}} which calls {{Exit.exit}} which calls {{{}Runtime.exit{}}}.

Based on the Runtime.exit documentation:
{quote}All registered [shutdown hooks|https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#addShutdownHook-java.lang.Thread-], if any, are started in some unspecified order and allowed to run concurrently until they finish. Once this is done the virtual machine [halts|https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#halt-int-].
{quote}
One of the shutdown hooks registered by Kafka is {{{}Server.shutdown(){}}}. This shutdown hook eventually calls {{{}KafkaEventQueue.close{}}}. This last close method joins on the controller thread. Unfortunately, the controller thread also joined waiting for the shutdown hook thread to finish.

Here are an sample thread stacks:
{code:java}
   ""QuorumControllerEventHandler"" #45 prio=5 os_prio=0 cpu=429352.87ms elapsed=620807.49s allocated=38544M defined_classes=353 tid=0x00007f5aeb31f800 nid=0x80c in Object.wait()  [0x00007f5a658fb000]
     java.lang.Thread.State: WAITING (on object monitor)                                                                                                                                                                                                            at java.lang.Object.wait(java.base@17.0.5/Native Method)
          - waiting on <no object reference available>
          at java.lang.Thread.join(java.base@17.0.5/Thread.java:1304)
          - locked <0x00000000a29241f8> (a org.apache.kafka.common.utils.KafkaThread)
          at java.lang.Thread.join(java.base@17.0.5/Thread.java:1372)
          at java.lang.ApplicationShutdownHooks.runHooks(java.base@17.0.5/ApplicationShutdownHooks.java:107)
          at java.lang.ApplicationShutdownHooks$1.run(java.base@17.0.5/ApplicationShutdownHooks.java:46)
          at java.lang.Shutdown.runHooks(java.base@17.0.5/Shutdown.java:130)
          at java.lang.Shutdown.exit(java.base@17.0.5/Shutdown.java:173)
          - locked <0x00000000ffe020b8> (a java.lang.Class for java.lang.Shutdown)
          at java.lang.Runtime.exit(java.base@17.0.5/Runtime.java:115)
          at java.lang.System.exit(java.base@17.0.5/System.java:1860)
          at org.apache.kafka.common.utils.Exit$2.execute(Exit.java:43)
          at org.apache.kafka.common.utils.Exit.exit(Exit.java:66)
          at org.apache.kafka.common.utils.Exit.exit(Exit.java:62)
          at org.apache.kafka.server.fault.ProcessExitingFaultHandler.handleFault(ProcessExitingFaultHandler.java:54)
          at org.apache.kafka.controller.QuorumController$ControllerWriteEvent$1.apply(QuorumController.java:891)
          at org.apache.kafka.controller.QuorumController$ControllerWriteEvent$1.apply(QuorumController.java:874)
          at org.apache.kafka.controller.QuorumController.appendRecords(QuorumController.java:969){code}
and
{code:java}
  ""kafka-shutdown-hook"" #35 prio=5 os_prio=0 cpu=43.42ms elapsed=378593.04s allocated=4732K defined_classes=74 tid=0x00007f5a7c09d800 nid=0x4f37 in Object.wait()  [0x00007f5a47afd000]
     java.lang.Thread.State: WAITING (on object monitor)
          at java.lang.Object.wait(java.base@17.0.5/Native Method)
          - waiting on <no object reference available>
          at java.lang.Thread.join(java.base@17.0.5/Thread.java:1304)
          - locked <0x00000000a272bcb0> (a org.apache.kafka.common.utils.KafkaThread)
          at java.lang.Thread.join(java.base@17.0.5/Thread.java:1372)
          at org.apache.kafka.queue.KafkaEventQueue.close(KafkaEventQueue.java:509)
          at org.apache.kafka.controller.QuorumController.close(QuorumController.java:2553)
          at kafka.server.ControllerServer.shutdown(ControllerServer.scala:521)
          at kafka.server.KafkaRaftServer.shutdown(KafkaRaftServer.scala:184)
          at kafka.Kafka$.$anonfun$main$3(Kafka.scala:99)
          at kafka.Kafka$$$Lambda$406/0x0000000800fb9730.apply$mcV$sp(Unknown Source)
          at kafka.utils.Exit$.$anonfun$addShutdownHook$1(Exit.scala:38)
          at kafka.Kafka$$$Lambda$407/0x0000000800fb9a10.run(Unknown Source)
          at java.lang.Thread.run(java.base@17.0.5/Thread.java:833)
          at org.apache.kafka.common.utils.KafkaThread.run(KafkaThread.java:64) {code}
h1. Possible Solution

A possible solution is to have the controller's unhandled fault handler call {{Runtime.halt}} instead of {{{}Runtime.exit{}}}.",jsancio,jsancio,Critical,Resolved,Fixed,08/Feb/23 19:10,14/Feb/23 18:07
Bug,KAFKA-14694,13523844,RPCProducerIdManager should not wait for a new block,"RPCProducerIdManager initiates an async request to the controller to grab a block of producer IDs and then blocks waiting for a response from the controller.

This is done in the request handler threads while holding a global lock. This means that if many producers are requesting producer IDs and the controller is slow to respond, many threads can get stuck waiting for the lock.

This may also be a deadlock concern under the following scenario:

if the controller has 1 request handler thread (1 chosen for simplicity) and receives an InitProducerId request, it may deadlock.
basically any time the controller has N InitProducerId requests where N >= # of request handler threads has the potential to deadlock.

consider this:
1. the request handler thread tries to handle an InitProducerId request to the controller by forwarding an AllocateProducerIds request.
2. the request handler thread then waits on the controller response (timed poll on nextProducerIdBlock)
3. the controller's request handler threads need to pick this request up, and handle it, but the controller's request handler threads are blocked waiting for the forwarded AllocateProducerIds response.

 

We should not block while waiting for a new block and instead return immediately to free the request handler threads.",jeffkbkim,jeffkbkim,Major,Resolved,Fixed,08/Feb/23 22:31,12/Jul/23 03:28
Bug,KAFKA-14696,13523934,CVE-2023-25194: Apache Kafka: Possible RCE/Denial of service attack via SASL JAAS JndiLoginModule configuration using Kafka Connect,"CVE Reference: [https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-34917]

 

Will Kafka 2.8.X provide a patch to fix this vulnerability?

If yes, when will the patch be provided?

 

Thanks",,millie,Major,Resolved,Fixed,09/Feb/23 09:29,14/Feb/23 14:26
Bug,KAFKA-14704,13524170,Follower should truncate before incrementing high watermark,"When a leader becomes a follower, it is likely that it has uncommitted records in its log. When it reaches out to the leader, the leader will detect that they have diverged and it will return the diverging epoch and offset. The follower truncates it log based on this.

There is a small caveat in this process. When the leader return the diverging epoch and offset, it also includes its high watermark, low watermark, start offset and end offset. The current code in the `AbstractFetcherThread` works as follow. First it process the partition data and then it checks whether there is a diverging epoch/offset. The former may accidentally expose uncommitted records as this step updates the local watermark to whatever is received from the leader. As the follower, or the former leader, may have uncommitted records, it will be able to updated the high watermark to a larger offset if the leader has a higher watermark than the current local one. This result in exposing uncommitted records until the log is finally truncated. The time window is short but a fetch requests coming at the right time to the follower could read those records. This is especially true for clients out there which uses recent versions of the fetch request but without implementing KIP-320.

When this happens, the follower logs the following message: `Non-monotonic update of high watermark from (offset=21437 segment=[20998:98390]) to (offset=21434 segment=[20998:97843])`.

This patch proposes to mitigate the issue by starting by checking on whether a diverging epoch/offset is provided by the leader and skip processing the partition data if it is. This basically means that the first fetch request will result in truncating the log and a subsequent fetch request will update the log/high watermarks.",dajac,dajac,Major,Resolved,Fixed,10/Feb/23 07:41,15/Feb/23 08:55
Bug,KAFKA-14711,13524513,kafaka-metadata-quorum.sh does not honor --command-config,https://github.com/apache/kafka/pull/12951 accidentally eliminated support for the `--command-config` option in the `kafka-metadata-quorum.sh` command.  This was an undetected regression in the 3.4.0 release.,rndgstn,rndgstn,Critical,Resolved,Fixed,13/Feb/23 17:32,13/Feb/23 23:43
Bug,KAFKA-14713,13524663,Kafka Streams global table startup takes too long,"*Some context first*

We have a spring based kafka streams application. This application is listening to two topics. Let's call them apartment and visitor. The apartments are stored in a global table, while the visitors are in the stream we are processing, and at one point we are joining the visitor stream together with the apartment table. In our test environment, both topics contain 10 partitions.

*Issue*

At first deployment, everything goes fine, the global table is built and all entries in the stream are processed.

After everything is finished, we shut down the application, restart it and send out a new set of visitors. The application seemingly does not respond.

After some more debugging it turned out that it simply takes 5 minutes to start up, because the global table takes 30 seconds (default value for the global request timeout) to accept that there are no messages in the apartment topics, for each and every partition. If we send out the list of apartments as new messages, the application starts up immediately.

To make matters worse, we have clients with 96 partitions, where the startup time would be 48 minutes. Not having messages in the topics between application shutdown and restart is a valid use case, so this is quite a big problem.

*Possible workarounds*

We could reduce the request timeout, but since this value is not specific for the global table initialization, but a global request timeout for a lot of things, we do not know what else it will affect, so we are not very keen on doing that. Even then, it would mean a 1.5 minute delay for this particular client (more if we will have other use cases in the future where we will need to use more global tables), which is far too much, considering that the application would be able to otherwise start in about 20 seconds.

*Potential solutions we see*
 # Introduce a specific global table initialization timeout in GlobalStateManagerImpl. Then we would be able to safely modify that value without fear of making some other part of kafka unstable.
 # Parallelize the initialization of the global table partitions in GlobalStateManagerImpl: knowing that the delay at startup is constant instead of linear with the number of partitions would be a huge help.
 # As long as we receive a response, accept the empty map in the KafkaConsumer, and continue instead of going into a busy-waiting loop.",,tamasszekeres,Critical,Resolved,Fixed,14/Feb/23 12:29,17/Feb/23 19:56
Bug,KAFKA-14717,13524711,KafkaStreams can' get running if the rebalance happens before StreamThread gets shutdown completely,"I noticed this issue when tracing KAFKA-7109

StreamThread closes the consumer before changing state to DEAD. If the partition rebalance happens quickly, the other StreamThreads can't change KafkaStream state from REBALANCING to RUNNING since there is a PENDING_SHUTDOWN StreamThread",chia7712,chia7712,Major,Resolved,Fixed,14/Feb/23 19:37,24/Feb/23 19:46
Bug,KAFKA-14727,13525066,Connect EOS mode should periodically call task commit,"In non-EOS mode, there is a background thread which periodically commits offsets for a task. If this thread does not have resources to flush on the framework side (records, or offsets) it still calls the task's commit() method to update the internal state of the task.

In EOS mode, there is no background thread, and all offset commits are performed on the main task thread in response to sending records to Kafka. This has the effect of only triggering the task's commit() method when there are records to send to Kafka, which is different than non-EOS mode.

In order to bring the two modes into better alignment, and allow tasks reliant on the non-EOS empty commit() behavior to work in EOS mode out-of-the-box, EOS mode should provide offset commits periodically for tasks which do not produce records.",gharris1727,gharris1727,Major,Resolved,Fixed,16/Feb/23 17:11,17/Feb/23 17:22
Bug,KAFKA-14729,13525134,The kafakConsumer pollForFetches(timer) method takes up a lot of cpu due to the abnormal exit of the heartbeat thread,"h2. case situation:

1. The business program occupies a large amount of memory, causing the `run()` method of HeartbeatThread of kafkaConsumer to exit abnormally.
{code:java}
2023-02-14 06:55:57.771[][ERROR][AbstractCoordinator][kafka-coor][Consumer clientId=consumer-5, groupId=*****_dev_VA] Heartbeat thread failed due to unexpected error java.lang.OutOfMemoryError: Java heap space {code}
2. The finally module of the heartbeat thread ` run()` method only prints the log, but does not update the value of `AbstractCoordinator.state`.
3. For kafkaConsumer with the groupRebalance mechanism enabled, in the `kafkaConsumer#pollForFetches(timer)` method, pollTimeout may eventually take the value `timeToNextHeartbeat(now)`.
4. Since the heartbeat thread has exited, `heartbeatTimer.deadlineMs` will never be updated again.
And the `AbstractCoordinator.state` field value will always be {*}STABLE{*},
So the `timeToNextHeartbeat(long now)` method will return {color:#ff0000}0{color}.
0 will be passed to the underlying `networkClient#poll` method.

 

In the end, the user calls the `poll(duration)` method in an endless loop, and the `kafkaConsumer#pollForFetches(timer)` method will always return very quickly, taking up a lot of cpu.

 
h2. solution:

1. Refer to the note of `MemberState.STABLE` :
{code:java}
the client has joined and is sending heartbeats.{code}
When the heartbeat thread exits, in `finally` module, we should add code:
{code:java}
state = MemberState.UNJOINED;
closed = true;{code}
2. In the `AbstractCoordinator#timeToNextHeartbeat(now)` method, add a new judgment condition: `heartbeatThread.hasFailed()`
{code:java}
if (state.hasNotJoinedGroup() || heartbeatThread.hasFailed())
    return Long.MAX_VALUE;
return heartbeat.timeToNextHeartbeat(now);{code}
 ",RivenSun,RivenSun,Major,Resolved,Fixed,17/Feb/23 05:16,02/Mar/23 03:37
Bug,KAFKA-14743,13525860,MessageConversionsTimeMs for fetch request metric is not updated,"During message conversion, there are 2 metrics we should update as doc written:

!image-2023-02-23-18-09-24-916.png|width=652,height=121!

 

In KAFKA-14295, it is addressing the issue in FetchMessageConversionsPerSec metric. This ticket will address the issue in *kafka.network:type=RequestMetrics,name=MessageConversionsTimeMs,request=fetch.*

 

 

 ",showuon,showuon,Major,Resolved,Fixed,23/Feb/23 10:11,13/Jul/23 06:58
Bug,KAFKA-14744,13525894,NPE while converting OffsetFetch from version < 8 to version >= 8,"While refactoring the OffsetFetch handling in KafkaApis, we introduced a NullPointerException (NPE). The NPE arises when the FetchOffset API is called with a client using a version older than version 8 and using null for the topics to signal that all topic-partition offsets must be returned. This means that this bug mainly impacts admin tools. The consumer does not use null.

This NPE is here: https://github.com/apache/kafka/commit/24a86423e9907b751d98fddc7196332feea2b48d#diff-0f2f19fd03e2fc5aa9618c607b432ea72e5aaa53866f07444269f38cb537f3feR237.

We missed this during the refactor because we had no tests in place to test this mode.",dajac,dajac,Major,Resolved,Fixed,23/Feb/23 12:21,23/Feb/23 17:32
Bug,KAFKA-14774,13526874,the removed listeners should not be reconfigurable,"Users can alter broker configuration to remove specify listeners. However, the removed listeners are NOT removed from `reconfigurables` list. It can result in the idle processors if users increases the network threads subsequently.",chia7712,chia7712,Major,Resolved,Fixed,02/Mar/23 15:09,29/Mar/23 14:08
Bug,KAFKA-14781,13527302,MM2 logs misleading error during topic ACL sync when broker does not have authorizer configured,"When there is no broker-side authorizer configured on a Kafka cluster targeted by MirrorMaker 2, users see error-level log messages like this one:{{{}{}}}
{quote}[2023-03-06 10:53:57,488] ERROR [MirrorSourceConnector|worker] Scheduler for MirrorSourceConnector caught exception in scheduled task: syncing topic ACLs (org.apache.kafka.connect.mirror.Scheduler:102)
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SecurityDisabledException: No Authorizer is configured on the broker
    at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
    at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
    at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
    at org.apache.kafka.connect.mirror.MirrorSourceConnector.listTopicAclBindings(MirrorSourceConnector.java:456)
    at org.apache.kafka.connect.mirror.MirrorSourceConnector.syncTopicAcls(MirrorSourceConnector.java:342)
    at org.apache.kafka.connect.mirror.Scheduler.run(Scheduler.java:93)
    at org.apache.kafka.connect.mirror.Scheduler.executeThread(Scheduler.java:112)
    at org.apache.kafka.connect.mirror.Scheduler.lambda$scheduleRepeating$0(Scheduler.java:50)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.kafka.common.errors.SecurityDisabledException: No Authorizer is configured on the broker
{quote}
This can be misleading as it looks like something is wrong with MM2 or the Kafka cluster. In reality, it's usually fine, since topic ACL syncing is enabled by default and it's reasonable for Kafka clusters (especially in testing/dev environments) to not have authorizers enabled.

We should try to catch this specific case and downgrade the severity of the log message from {{ERROR}} to either {{INFO}} or {{{}DEBUG{}}}. We may also consider suggesting to users that they disable topic ACL syncing if their Kafka cluster doesn't have authorization set up, but this should probably only be emitted once over the lifetime of the connector in order to avoid generating log spam.",ChrisEgerton,ChrisEgerton,Major,Resolved,Fixed,06/Mar/23 16:02,08/Mar/23 15:36
Bug,KAFKA-14792,13527513,Race condition in LazyIndex.get(),"`LazyIndex.get()` has a race condition that can result in a ClassCastException being thrown in some cases.

This was introduced when it was rewritten from Scala to Java.",ijuma,ijuma,Blocker,Resolved,Fixed,07/Mar/23 20:52,07/Mar/23 23:56
Bug,KAFKA-14794,13527672,Unable to deserialize base64 JSON strings ,"h1. Problem

The following test fails:
{code:java}
@Test
public void testBinaryNode() throws IOException {
    byte[] expected = new byte[] {5, 2, 9, 4, 1, 8, 7, 0, 3, 6};
    StringWriter writer = new StringWriter();
    ObjectMapper mapper = new ObjectMapper();

    mapper.writeTree(mapper.createGenerator(writer), new BinaryNode(expected));

    JsonNode binaryNode = mapper.readTree(writer.toString());

    assertTrue(binaryNode.isTextual(), binaryNode.toString());
    byte[] actual = MessageUtil.jsonNodeToBinary(binaryNode, ""Test base64 JSON string"");
    assertEquals(expected, actual);
}
{code}
with the following error:
{code:java}
 Gradle Test Run :clients:test > Gradle Test Executor 20 > MessageUtilTest > testBinaryNode() FAILED
    java.lang.RuntimeException: Test base64 JSON string: expected Base64-encoded binary data.
        at org.apache.kafka.common.protocol.MessageUtil.jsonNodeToBinary(MessageUtil.java:165)
        at org.apache.kafka.common.protocol.MessageUtilTest.testBinaryNode(MessageUtilTest.java:102)
{code}
The reason for the failure is because FasterXML Jackson deserializes base64 JSON strings to a TextNode not to a BinaryNode.
h1. Solution

The method {{MessageUtil::jsonNodeToBinary}} should not assume that the input {{JsonNode}} is always a {{{}BinaryNode{}}}. It should also support decoding {{{}TextNode{}}}.

{{JsonNode::binaryValue}} is supported by both {{BinaryNode}} and {{{}TextNode{}}}.",jsancio,jsancio,Major,Resolved,Fixed,08/Mar/23 17:14,14/Mar/23 19:22
Bug,KAFKA-14797,13527709,MM2 does not emit offset syncs when conservative translation logic exceeds positive max.offset.lag,"This is a regression in MirrorMaker 2 introduced by KAFKA-12468.

Reproduction steps:
1. Set max.offset.lag to a non-zero value.
2. Set up a 1-1 replication flow which does not skip upstream offsets or have a concurrent producer to the target topic.
3. Produce more than max.offset.lag records to the source topic and allow replication to proceed.
4. Examine end offsets, checkpoints and/or target consumer group lag

Expected behavior:
Consumer group lag should be at most max.offset.lag.

Actual behavior:
Consumer group lag is significantly larger than max.offset.lag.",gharris1727,gharris1727,Blocker,Resolved,Fixed,08/Mar/23 23:05,21/Mar/23 13:51
Bug,KAFKA-14799,13527847,Source tasks fail if connector attempts to abort empty transaction,"If a source task invokes [TransactionContext::abortTransaction|https://kafka.apache.org/34/javadoc/org/apache/kafka/connect/source/TransactionContext.html#abortTransaction()] while the current transaction is empty, and then returns an empty batch of records from the next (or current) invocation of {{{}SourceTask::poll{}}}, the task will fail.

This is because the Connect framework will honor the transaction abort request by invoking [KafkaProducer::abortTransaction|https://kafka.apache.org/34/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#abortTransaction()], but without having first invoked [KafkaProducer::beginTransaction|https://kafka.apache.org/34/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#beginTransaction()] (since no records had been received from the task), which leads to an {{{}IllegalStateException{}}}.

An example stack trace for this scenario:
{quote}[2023-03-09 10:41:25,053] ERROR [exactlyOnceQuestionMark|task-0] ExactlyOnceWorkerSourceTask\{id=exactlyOnceQuestionMark-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:210)
java.lang.IllegalStateException: TransactionalId exactly-once-source-integration-test-exactlyOnceQuestionMark-0: Invalid transition attempted from state READY to state ABORTING_TRANSACTION
    at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:974)
    at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:967)
    at org.apache.kafka.clients.producer.internals.TransactionManager.lambda$beginAbort$3(TransactionManager.java:269)
    at org.apache.kafka.clients.producer.internals.TransactionManager.handleCachedTransactionRequestResult(TransactionManager.java:1116)
    at org.apache.kafka.clients.producer.internals.TransactionManager.beginAbort(TransactionManager.java:266)
    at org.apache.kafka.clients.producer.KafkaProducer.abortTransaction(KafkaProducer.java:835)
    at org.apache.kafka.connect.runtime.ExactlyOnceWorkerSourceTask$3.abortTransaction(ExactlyOnceWorkerSourceTask.java:495)
    at org.apache.kafka.connect.runtime.ExactlyOnceWorkerSourceTask$3.shouldCommitTransactionForBatch(ExactlyOnceWorkerSourceTask.java:473)
    at org.apache.kafka.connect.runtime.ExactlyOnceWorkerSourceTask$TransactionBoundaryManager.maybeCommitTransactionForBatch(ExactlyOnceWorkerSourceTask.java:398)
    at org.apache.kafka.connect.runtime.ExactlyOnceWorkerSourceTask.batchDispatched(ExactlyOnceWorkerSourceTask.java:186)
    at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.execute(AbstractWorkerSourceTask.java:362)
    at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:202)
    at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:257)
    at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.run(AbstractWorkerSourceTask.java:75)
    at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:181)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)
{quote}
 

As far as a fix goes, we have a few options:
 # Gracefully handle this case by translating the call to {{TransactionContext::abortTransaction}} into a no-op
 # Throw an exception (probably an {{{}IllegalStateException{}}}) from {{{}TransactionContext::abortTransaction{}}}, which may fail the task, but would give it the option to swallow the exception and continue processing if it would like
 # Forcibly fail the task without giving it the chance to swallow an exception, using a similar strategy to how we fail tasks that request that a transaction be committed and aborted for the same record (see [here|https://github.com/apache/kafka/blob/c5240c0390892fe9ecbe5285185c370e7be8b2aa/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerTransactionContext.java#L78-L86])",ChrisEgerton,ChrisEgerton,Major,Resolved,Fixed,09/Mar/23 15:46,16/Mar/23 12:34
Bug,KAFKA-14800,13528020,Upgrade snappy-java Version to 1.1.9.1,"The current {{kafka-clients}} latest released versions depend on {{snappy-java}} {{{}1.1.8.4{}}}. This particular version is affected by [bug 302|https://github.com/xerial/snappy-java/issues/302], which causes problems when using the OSGI  bundle on ARM based architectures. The bug itself has been fixed in version {{{}1.1.9.0{}}}, could the {{kafka-clients}} bundle be updated to transitively include the fixed version of {{{}snappy-java{}}}?.

—

Example {{build.gradle}} file:
{noformat}
plugins {
    id 'java'
}

group 'org.example'
version '1.0.0SNAPSHOT'

repositories {
    mavenCentral()
}

dependencies {
    implementation 'org.apache.kafka:kafka-clients:3.3.2'
}
{noformat}
 

Dependency Insight:
{noformat}
./gradlew -q dependencyInsight --dependency snappy-java --configuration runtimeClasspath
org.xerial.snappy:snappy-java:1.1.8.4
  Variant runtime:
    | Attribute Name                 | Provided     | Requested    |
    |--------------------------------|--------------|--------------|
    | org.gradle.status              | release      |              |
    | org.gradle.category            | library      | library      |
    | org.gradle.libraryelements     | jar          | jar          |
    | org.gradle.usage               | java-runtime | java-runtime |
    | org.gradle.dependency.bundling |              | external     |
    | org.gradle.jvm.environment     |              | standard-jvm |
    | org.gradle.jvm.version         |              | 11           |

org.xerial.snappy:snappy-java:1.1.8.4
\--- org.apache.kafka:kafka-clients:3.3.2
     \--- runtimeClasspath


{noformat}",jjramos,jjramos,Major,Resolved,Fixed,10/Mar/23 17:50,24/Mar/23 15:28
Bug,KAFKA-14801,13528184,Encoded sensitive configs are not decoded before migration,,akhileshchg,akhileshchg,Blocker,Resolved,Fixed,13/Mar/23 07:10,16/Mar/23 01:22
Bug,KAFKA-14804,13528276,Connect docs fail to build with Gradle Swagger plugin 2.2.8,"There is an incompatibility somewhere between versions 2.2.0 and 2.2.8 that cause the following error when building the connect docs:

{code}
Caused by: org.gradle.api.GradleException: io.swagger.v3.jaxrs2.integration.SwaggerLoader.setOpenAPI31(java.lang.Boolean)
        at io.swagger.v3.plugins.gradle.tasks.ResolveTask.resolve(ResolveTask.java:458)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:125)
        at org.gradle.api.internal.project.taskfactory.StandardTaskAction.doExecute(StandardTaskAction.java:58)
        at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:51)
        at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:29)
        at org.gradle.api.internal.tasks.execution.TaskExecution$3.run(TaskExecution.java:242)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$1.execute(DefaultBuildOperationRunner.java:29)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$1.execute(DefaultBuildOperationRunner.java:26)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:66)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:59)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:157)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:59)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.run(DefaultBuildOperationRunner.java:47)
        at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:68)
        at org.gradle.api.internal.tasks.execution.TaskExecution.executeAction(TaskExecution.java:227)
        at org.gradle.api.internal.tasks.execution.TaskExecution.executeActions(TaskExecution.java:210)
        at org.gradle.api.internal.tasks.execution.TaskExecution.executeWithPreviousOutputFiles(TaskExecution.java:193)
        at org.gradle.api.internal.tasks.execution.TaskExecution.execute(TaskExecution.java:166)
        at org.gradle.internal.execution.steps.ExecuteStep.executeInternal(ExecuteStep.java:93)
        at org.gradle.internal.execution.steps.ExecuteStep.access$000(ExecuteStep.java:44)
        at org.gradle.internal.execution.steps.ExecuteStep$1.call(ExecuteStep.java:57)
        at org.gradle.internal.execution.steps.ExecuteStep$1.call(ExecuteStep.java:54)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$CallableBuildOperationWorker.execute(DefaultBuildOperationRunner.java:204)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$CallableBuildOperationWorker.execute(DefaultBuildOperationRunner.java:199)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:66)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:59)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:157)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:59)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.call(DefaultBuildOperationRunner.java:53)
        at org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:73)
        at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:54)
        at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:44)
        at org.gradle.internal.execution.steps.RemovePreviousOutputsStep.execute(RemovePreviousOutputsStep.java:67)
        at org.gradle.internal.execution.steps.RemovePreviousOutputsStep.execute(RemovePreviousOutputsStep.java:37)
        at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:41)
        at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:74)
        at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:55)
        at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:50)
        at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:28)
        at org.gradle.internal.execution.steps.CaptureStateAfterExecutionStep.executeDelegateBroadcastingChanges(CaptureStateAfterExecutionStep.java:100)
        at org.gradle.internal.execution.steps.CaptureStateAfterExecutionStep.execute(CaptureStateAfterExecutionStep.java:72)
        at org.gradle.internal.execution.steps.CaptureStateAfterExecutionStep.execute(CaptureStateAfterExecutionStep.java:50)
        at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:40)
        at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:29)
        at org.gradle.internal.execution.steps.BuildCacheStep.executeWithoutCache(BuildCacheStep.java:166)
        at org.gradle.internal.execution.steps.BuildCacheStep.lambda$execute$1(BuildCacheStep.java:70)
        at org.gradle.internal.Either$Right.fold(Either.java:175)
        at org.gradle.internal.execution.caching.CachingState.fold(CachingState.java:59)
        at org.gradle.internal.execution.steps.BuildCacheStep.execute(BuildCacheStep.java:68)
        at org.gradle.internal.execution.steps.BuildCacheStep.execute(BuildCacheStep.java:46)
        at org.gradle.internal.execution.steps.StoreExecutionStateStep.execute(StoreExecutionStateStep.java:36)
        at org.gradle.internal.execution.steps.StoreExecutionStateStep.execute(StoreExecutionStateStep.java:25)
        at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:36)
        at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:22)
        at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:91)
        at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$2(SkipUpToDateStep.java:55)
        at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:55)
        at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:37)
        at org.gradle.internal.execution.steps.ResolveChangesStep.execute(ResolveChangesStep.java:65)
        at org.gradle.internal.execution.steps.ResolveChangesStep.execute(ResolveChangesStep.java:36)
        at org.gradle.internal.execution.steps.legacy.MarkSnapshottingInputsFinishedStep.execute(MarkSnapshottingInputsFinishedStep.java:37)
        at org.gradle.internal.execution.steps.legacy.MarkSnapshottingInputsFinishedStep.execute(MarkSnapshottingInputsFinishedStep.java:27)
        at org.gradle.internal.execution.steps.ResolveCachingStateStep.execute(ResolveCachingStateStep.java:76)
        at org.gradle.internal.execution.steps.ResolveCachingStateStep.execute(ResolveCachingStateStep.java:37)
        at org.gradle.internal.execution.steps.ValidateStep.execute(ValidateStep.java:94)
        at org.gradle.internal.execution.steps.ValidateStep.execute(ValidateStep.java:49)
        at org.gradle.internal.execution.steps.CaptureStateBeforeExecutionStep.execute(CaptureStateBeforeExecutionStep.java:71)
        at org.gradle.internal.execution.steps.CaptureStateBeforeExecutionStep.execute(CaptureStateBeforeExecutionStep.java:45)
        at org.gradle.internal.execution.steps.SkipEmptyWorkStep.executeWithNonEmptySources(SkipEmptyWorkStep.java:177)
        at org.gradle.internal.execution.steps.SkipEmptyWorkStep.execute(SkipEmptyWorkStep.java:81)
        at org.gradle.internal.execution.steps.SkipEmptyWorkStep.execute(SkipEmptyWorkStep.java:53)
        at org.gradle.internal.execution.steps.RemoveUntrackedExecutionStateStep.execute(RemoveUntrackedExecutionStateStep.java:32)
        at org.gradle.internal.execution.steps.RemoveUntrackedExecutionStateStep.execute(RemoveUntrackedExecutionStateStep.java:21)
        at org.gradle.internal.execution.steps.legacy.MarkSnapshottingInputsStartedStep.execute(MarkSnapshottingInputsStartedStep.java:38)
        at org.gradle.internal.execution.steps.LoadPreviousExecutionStateStep.execute(LoadPreviousExecutionStateStep.java:36)
        at org.gradle.internal.execution.steps.LoadPreviousExecutionStateStep.execute(LoadPreviousExecutionStateStep.java:23)
        at org.gradle.internal.execution.steps.CleanupStaleOutputsStep.execute(CleanupStaleOutputsStep.java:75)
        at org.gradle.internal.execution.steps.CleanupStaleOutputsStep.execute(CleanupStaleOutputsStep.java:41)
        at org.gradle.internal.execution.steps.AssignWorkspaceStep.lambda$execute$0(AssignWorkspaceStep.java:32)
        at org.gradle.api.internal.tasks.execution.TaskExecution$4.withWorkspace(TaskExecution.java:287)
        at org.gradle.internal.execution.steps.AssignWorkspaceStep.execute(AssignWorkspaceStep.java:30)
        at org.gradle.internal.execution.steps.AssignWorkspaceStep.execute(AssignWorkspaceStep.java:21)
        at org.gradle.internal.execution.steps.IdentityCacheStep.execute(IdentityCacheStep.java:37)
        at org.gradle.internal.execution.steps.IdentityCacheStep.execute(IdentityCacheStep.java:27)
        at org.gradle.internal.execution.steps.IdentifyStep.execute(IdentifyStep.java:42)
        at org.gradle.internal.execution.steps.IdentifyStep.execute(IdentifyStep.java:31)
        at org.gradle.internal.execution.impl.DefaultExecutionEngine$1.execute(DefaultExecutionEngine.java:64)
        at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeIfValid(ExecuteActionsTaskExecuter.java:146)
        at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:135)
        at org.gradle.api.internal.tasks.execution.FinalizePropertiesTaskExecuter.execute(FinalizePropertiesTaskExecuter.java:46)
        at org.gradle.api.internal.tasks.execution.ResolveTaskExecutionModeExecuter.execute(ResolveTaskExecutionModeExecuter.java:51)
        at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:57)
        at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:74)
        at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:36)
        at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.executeTask(EventFiringTaskExecuter.java:77)
        at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.call(EventFiringTaskExecuter.java:55)
        at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.call(EventFiringTaskExecuter.java:52)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$CallableBuildOperationWorker.execute(DefaultBuildOperationRunner.java:204)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$CallableBuildOperationWorker.execute(DefaultBuildOperationRunner.java:199)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:66)
        at org.gradle.internal.operations.DefaultBuildOperationRunner$2.execute(DefaultBuildOperationRunner.java:59)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:157)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.execute(DefaultBuildOperationRunner.java:59)
        at org.gradle.internal.operations.DefaultBuildOperationRunner.call(DefaultBuildOperationRunner.java:53)
        at org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:73)
        at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter.execute(EventFiringTaskExecuter.java:52)
        at org.gradle.execution.plan.LocalTaskNodeExecutor.execute(LocalTaskNodeExecutor.java:42)
        at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:338)
        at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:325)
        at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:318)
        at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:304)
        at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:463)
        at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:380)
        at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
        at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:49)
Caused by: java.lang.NoSuchMethodException: io.swagger.v3.jaxrs2.integration.SwaggerLoader.setOpenAPI31(java.lang.Boolean)
        at io.swagger.v3.plugins.gradle.tasks.ResolveTask.resolve(ResolveTask.java:441)
        ... 117 more
{code}

This can be reproduced by setting the plugin version to 2.2.8 and running the ""releaseTarGz"" Gradle task",mimaison,mumrah,Minor,Resolved,Fixed,13/Mar/23 17:11,14/Mar/23 11:06
Bug,KAFKA-14809,13528498,Connect incorrectly logs that no records were produced by source tasks,"There's an *{{if}}* condition when [committing offsets|https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java#L219] that is referencing the wrong variable, so the statement always evaluates to {*}true{*}.

This causes log statements like the following to be spuriously emitted:
{quote}[2023-03-14 16:18:04,675] DEBUG WorkerSourceTask\{id=job-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:220)
{quote}",hgeraldino,hgeraldino,Minor,Resolved,Fixed,14/Mar/23 20:05,16/Mar/23 12:46
Bug,KAFKA-14812,13528758,ProducerPerformance still counting successful sending in console when sending failed,"When using ProducerPerformance, I found that when the sending fails, it is still counted as successfully sent by stat and the metrics are printed in console. For example, when there is no write permission and cannot be written in, the sending success rate is still magically displayed.",hudeqi,hudeqi,Major,Resolved,Fixed,16/Mar/23 08:52,21/Mar/23 09:01
Bug,KAFKA-14816,13528877,Connect loading SSL configs when contacting non-HTTPS URLs,"Due to changes made here: [https://github.com/apache/kafka/pull/12828]
Connect now unconditionally loads SSL configs from the worker into rest clients it uses for cross-worker communication and uses them even when issuing requests to HTTP (i.e., non-HTTPS) URLs. Previously, it would only attempt to load (and validate) SSL properties when issuing requests to HTTPS URLs. This can cause issues when a Connect cluster has stopped securing its REST API with SSL but its worker configs still contain the old (and now-invalid) SSL properties. When this happens, REST requests that hit a follower worker but need to be forwarded to the leader will fail, and connectors that perform dynamic reconfigurations via [ConnectorContext::requestTaskReconfiguration|https://kafka.apache.org/34/javadoc/org/apache/kafka/connect/connector/ConnectorContext.html#requestTaskReconfiguration()] will fail to trigger that reconfiguration if they are not running on the leader.

In our testing environments - older versions without the linked changes pass with the following configuration, and newer versions with the changes fail:

{{ssl.keystore.location = /mnt/security/test.keystore.jks}}
{{ssl.keystore.password = [hidden]}}
{{ssl.keystore.type = JKS}}
{{ssl.protocol = TLSv1.2}}

It's important to note that the file {{/mnt/security/test.keystore.jks}} isn't generated for our non-SSL tests, however these configs are still included in our worker config file.

This leads to a 500 response when hitting the create connector REST endpoint with the following error:

bq. { ""error_code"":500,   ""message"":""Failed to start RestClient:   /mnt/security/test.keystore.jks is not a valid keystore"" }",ChrisEgerton,imcdo,Blocker,Resolved,Fixed,17/Mar/23 00:15,20/Mar/23 16:40
Bug,KAFKA-14836,13529771,Fix UtilsTest#testToLogDateTimeFormat failure in some cases," 
{code:java}
org.apache.kafka.common.utils.UtilsTest.testToLogDateTimeFormat {code}
test is failing in some cases. It uses hard coded datetime ({*}2020-11-09 12:34:05{*}), while the assertation expected part contains the offset of the current time ({*}Instant.now(){*}). This can lead to problems if there is a summer and winter time for the specific location. We should use the same datetime in the offset query instead of the current time.
{noformat}
org.opentest4j.AssertionFailedError: expected: <2020-11-09 12:34:05,123 -07:00> but was: <2020-11-09 12:34:05,123 -08:00>{noformat}
 ",egyed.t,egyed.t,Major,Resolved,Fixed,23/Mar/23 13:07,24/May/23 01:41
Bug,KAFKA-14839,13529835,Exclude protected variable from JavaDocs,"Cf [https://kafka.apache.org/31/javadoc/org/apache/kafka/streams/kstream/JoinWindows.html#enableSpuriousResultFix]

The variable `enableSpuriousResultFix` is protected, and it's not public API, and thus should not show up in the JavaDocs.",atusharm,mjsax,Major,Resolved,Fixed,23/Mar/23 16:56,13/May/23 01:34
Bug,KAFKA-14843,13529914,Connector plugins config endpoint does not include Common configs,"Connector plugins GET config endpoint introduced in [https://cwiki.apache.org/confluence/display/KAFKA/KIP-769%3A+Connect+APIs+to+list+all+connector+plugins+and+retrieve+their+configuration+definitions]  allows to get plugin configuration from the rest endpoint.

This configuration only includes the plugin configuration, but not the base configuration of the Sink/Source Connector.

For instance, when validating the configuration of a plugin, _all_ configs are returned:

```

curl -s $CONNECT_URL/connector-plugins/io.aiven.kafka.connect.http.HttpSinkConnector/config | jq -r '.[].name' | sort -u | wc -l     
21

curl -s $CONNECT_URL/connector-plugins/io.aiven.kafka.connect.http.HttpSinkConnector/config/validate -XPUT -H 'Content-type: application/json' --data ""\{\""connector.class\"": \""io.aiven.kafka.connect.http.HttpSinkConnector\"", \""topics\"": \""example-topic-name\""}"" | jq -r '.configs[].definition.name' | sort -u | wc -l
39

```

and the missing configs are all from base config:

```

diff validate.txt config.txt                                                                                                    
6,14d5
< config.action.reload
< connector.class
< errors.deadletterqueue.context.headers.enable
< errors.deadletterqueue.topic.name
< errors.deadletterqueue.topic.replication.factor
< errors.log.enable
< errors.log.include.messages
< errors.retry.delay.max.ms
< errors.retry.timeout
16d6
< header.converter
24d13
< key.converter
26d14
< name
33d20
< predicates
35,39d21
< tasks.max
< topics
< topics.regex
< transforms
< value.converter

```

Would be great to get the base configs from the same endpoint as well, so we could rely on it instead of using the validate endpoint to get all configs.",jeqo,jeqo,Major,Resolved,Fixed,24/Mar/23 06:28,28/Mar/23 16:44
Bug,KAFKA-14848,13530088,KafkaConsumer incorrectly passes locally-scoped deserializers to FetchConfig,"[~rayokota] found some {{{}NullPointerException{}}}s that originate because of a recently introduced error in the {{KafkaConsumer}} constructor. The code was changed to pass the deserializer variables into the {{FetchConfig}} constructor. However, this code change incorrectly used the locally-scoped variables, not the instance-scoped variables. Since the locally-scoped variables could be {{{}null{}}}, this results in the {{FetchConfig}} storing {{null}} references, leading to downstream breakage.

Suggested change:
{noformat}
- FetchConfig<K, V> fetchConfig = new FetchConfig<>(config, keyDeserializer, valueDeserializer, isolationLevel);
+ FetchConfig<K, V> fetchConfig = new FetchConfig<>(config, this.keyDeserializer, this.valueDeserializer, isolationLevel);
{noformat}",kirktrue,kirktrue,Major,Resolved,Fixed,25/Mar/23 23:32,29/Jun/23 21:08
Bug,KAFKA-14853,13530269,the serializer/deserialize which extends ClusterResourceListener is not added to Metadata,I noticed this issue when reviewing  KAFKA-14848 ,chia7712,chia7712,Minor,Resolved,Fixed,27/Mar/23 17:07,29/Mar/23 08:02
Bug,KAFKA-14857,13530293,Fix some MetadataLoader bugs,,cmccabe,cmccabe,Major,Resolved,Fixed,27/Mar/23 22:21,24/May/23 23:00
Bug,KAFKA-14862,13530446,Outer stream-stream join does not output all results with multiple input partitions,"If I execute the following Streams app once with two input topics each with 1 partition and then with input topics each with two partitions, I get different results.
  
{code:java}
final KStream<String, String> leftSide = builder.stream(leftSideTopic);
final KStream<String, String> rightSide = builder.stream(rightSideTopic);

final KStream<String, String> leftAndRight = leftSide.outerJoin(
    rightSide,
    (leftValue, rightValue) ->
        (rightValue == null) ? leftValue + ""/NOTPRESENT"": leftValue + ""/"" + rightValue,
    JoinWindows.ofTimeDifferenceAndGrace(
        Duration.ofSeconds(20), 
        Duration.ofSeconds(10)),
    StreamJoined.with(
        Serdes.String(), /* key */
        Serdes.String(), /* left value */
        Serdes.String()  /* right value */
    ));
    leftAndRight.print(Printed.toSysOut());
{code}

To reproduce, produce twice the following batch of records with an interval greater than window + grace period (i.e. > 30 seconds) in between the two batches:
{code}
(0, 0)
(1, 1)
(2, 2)
(3, 3)
(4, 4)
(5, 5)
(6, 6)
(7, 7)
(8, 8)
(9, 9)
{code}

With input topics with 1 partition I get:
{code}
[KSTREAM-PROCESSVALUES-0000000008]: 0, 0/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 1, 1/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 2, 2/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 3, 3/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 4, 4/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 5, 5/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 6, 6/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 7, 7/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 8, 8/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 9, 9/NOTPRESENT
{code}

With input topics with 2 partitions I get:
{code}
[KSTREAM-PROCESSVALUES-0000000008]: 1, 1/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 3, 3/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 4, 4/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 7, 7/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 8, 8/NOTPRESENT
[KSTREAM-PROCESSVALUES-0000000008]: 9, 9/NOTPRESENT
{code}

I would expect to get the same set of records, maybe in a different order due to the partitioning.",mjsax,cadonna,Major,Resolved,Fixed,28/Mar/23 14:16,22/May/23 12:28
Bug,KAFKA-14863,13530487,Plugins which do not have a valid no-args constructor are visible in the REST API,"Currently, the Connect plugin discovery mechanisms only assert that a no-args constructor is present when necessary. In particular, this assertion happens for Connectors when the framework needs to evaluate the connector's version method.
It also happens for ConnectorConfigOverridePolicy, ConnectRestExtension, and ConfigProvider plugins, which are loaded via the ServiceLoader. The ServiceLoader constructs instances of plugins with their no-args constructor during discovery, so these plugins are discovered even if they are not Versioned.

This has the effect that these unusable plugins which are missing a default constructor appear in the REST API, but are not able to be instantiated or used. To make the ServiceLoader and Reflections discovery mechanisms behave more similar, this assertion should be applied to all plugins, and a log message emitted when plugins do not follow the constructor requirements.",gharris1727,gharris1727,Minor,Resolved,Fixed,28/Mar/23 19:11,03/Jun/23 16:55
Bug,KAFKA-14864,13530517,Memory leak in KStreamWindowAggregate with ON_WINDOW_CLOSE emit strategy,"The Streams DSL processor implementation for the ON_WINDOW_CLOSE emit strategy during KStream windowed aggregations opens a key-value iterator but does not call `close()` on it ([link|https://github.com/apache/kafka/blob/5afedd9ac37c4d740f47867cfd31eaed15dc542f/streams/src/main/java/org/apache/kafka/streams/kstream/internals/AbstractKStreamTimeWindowAggregateProcessor.java#L203]), despite the Javadocs for the iterator making clear that users must do so in order to release resources ([link|https://github.com/apache/kafka/blob/5afedd9ac37c4d740f47867cfd31eaed15dc542f/streams/src/main/java/org/apache/kafka/streams/state/KeyValueIterator.java#L27]).  

I discovered this bug while running load testing benchmarks and noticed that some runs were sporadically hitting OOMs, so it is definitely possible to hit this in practice.",vcrfxia,vcrfxia,Major,Resolved,Fixed,28/Mar/23 23:16,04/Apr/23 05:09
Bug,KAFKA-14880,13531546,TransactionMetadata with producer epoch -1 should be expirable ,"We have seen the following error in logs:
{noformat}
""Mar 22, 2019 @ 21:57:56.655"",Error,""kafka-0-0"",""transaction-log-manager-0"",""Uncaught exception in scheduled task 'transactionalId-expiration'"",""java.lang.IllegalArgumentException: Illegal new producer epoch -1
{noformat}

Investigations showed that it is actually possible for a transaction metadata object to still have -1 as producer epoch when it transitions to Dead.

When a transaction metadata is created for the first time (in handleInitProducerId), it has -1 as its producer epoch. Then a producer epoch is attributed and the transaction coordinator tries to persist the change. If the write fail for instance because there is an under min isr, the transaction metadata remains with its epoch as -1 forever or until the init producer id is retried.

This means that it is possible for transaction metadata to remain with -1 as producer epoch until it gets expired. At the moment, this is not allowed because we enforce a producer epoch greater or equals to 0 in prepareTransitionTo.",dajac,dajac,Major,Resolved,Fixed,05/Apr/23 08:11,06/Apr/23 07:18
Bug,KAFKA-14891,13532182,Fix rack-aware range assignor to improve rack-awareness with co-partitioning,We currently check all states for rack-aware assignment with co-partitioning ([https://github.com/apache/kafka/blob/396536bb5aa1ba78c71ea824d736640b615bda8a/clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java#L176).] We should check each group of co-partitioned states separately so that we can use rack-aware assignment with co-partitioning for subsets of topics.,rsivaram,rsivaram,Major,Resolved,Fixed,11/Apr/23 12:42,12/Apr/23 07:36
Bug,KAFKA-14894,13532222,MetadataLoader must call finishSnapshot after loading a snapshot,,cmccabe,cmccabe,Critical,Resolved,Fixed,11/Apr/23 17:24,17/Apr/23 17:50
Bug,KAFKA-14898,13532391,[ MirrorMaker ] sync.topic.configs.enabled not working as expected,"Hello,

In my replication set up , i do not want to sync the topic configs, the use case is to have different retention time for the topic on the target cluster, I am passing the config
{code:java}
 sync.topic.configs.enabled = false{code}
but this is not working as expected the topic retention time is being set to whatever is being set in the source cluster, looking at the mirrormaker logs i can see that MirrorSourceConnector is still setting the above config as true
{code:java}
[2023-04-12 17:04:55,184] INFO [MirrorSourceConnector|task-8] ConsumerConfig values:
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.include.jmx.reporter = true
        auto.offset.reset = earliest
        bootstrap.servers = [sourcecluster.com:9092]
        check.crcs = true
        client.dns.lookup = use_all_dns_ips
        client.id = consumer-null-2
        client.rack =
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = null
        group.instance.id = null
        heartbeat.interval.ms = 3000
        interceptor.classes = []
        internal.leave.group.on.close = true
        internal.throw.on.fetch.stable.offset.unsupported = false
        isolation.level = read_uncommitted
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 300000
        max.poll.records = 500
        metadata.max.age.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.connect.timeout.ms = null
        sasl.login.read.timeout.ms = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.login.retry.backoff.max.ms = 10000
        sasl.login.retry.backoff.ms = 100
        sasl.mechanism = GSSAPI
        sasl.oauthbearer.clock.skew.seconds = 30
        sasl.oauthbearer.expected.audience = null
        sasl.oauthbearer.expected.issuer = null
        sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
        sasl.oauthbearer.jwks.endpoint.url = null
        sasl.oauthbearer.scope.claim.name = scope
        sasl.oauthbearer.sub.claim.name = sub
        sasl.oauthbearer.token.endpoint.url = null
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        session.timeout.ms = 45000
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
        sasl.oauthbearer.jwks.endpoint.url = null
        sasl.oauthbearer.scope.claim.name = scope
        sasl.oauthbearer.sub.claim.name = sub
        sasl.oauthbearer.token.endpoint.url = null
        security.protocol = PLAINTEXT
        source.cluster.alias = prod-240
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.2
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        sync.topic.acls.enabled = true
        sync.topic.acls.interval.seconds = 600
        sync.topic.configs.enabled = true
        sync.topic.configs.interval.seconds = 600
        target.cluster.alias = dev-240
        task.assigned.partitions = [test1-8, test1-18, test1-28, test2-6, test2-16, test2-26]
        tasks.max = 10
        topic.filter.class = class org.apache.kafka.connect.mirror.DefaultTopicFilter
        topics = [test1, test2]
        topics.blacklist = null
        topics.exclude = [.*[\-\.]internal, .*\.replica, __.*]
        transforms = []
        value.converter = null {code}
 

Can you please let me know if i am missing any other config

I am using kafka version - 3.4

Thanks,

-srini",,bseenu,Major,Resolved,Fixed,12/Apr/23 18:05,12/Apr/23 20:41
Bug,KAFKA-14902,13532461,KafkaBasedLog infinite retries can lead to StackOverflowError,"KafkaBasedLog subclasses use an infinite retry on producer sends, using a callback. Sometimes, when specific errors are encountered, the callback is invoked in the send call, on the calling thread. If this happens enough times, a stack overflow happens.

Example stacktrace from 2.5 (but the newest code can also encounter the same):
{code:java}
2023-01-14 12:48:23,487 ERROR org.apache.kafka.connect.runtime.WorkerTask: WorkerSourceTask{id=MirrorSourceConnector-1} Task threw an uncaught and unrecoverable exception java.lang.StackOverflowError: null at org.apache.kafka.common.metrics.stats.SampledStat.record(SampledStat.java:50) at org.apache.kafka.common.metrics.stats.Rate.record(Rate.java:60) at org.apache.kafka.common.metrics.stats.Meter.record(Meter.java:80) at org.apache.kafka.common.metrics.Sensor.record(Sensor.java:188) at org.apache.kafka.common.metrics.Sensor.record(Sensor.java:178) at org.apache.kafka.clients.producer.internals.BufferPool.recordWaitTime(BufferPool.java:202) at org.apache.kafka.clients.producer.internals.BufferPool.allocate(BufferPool.java:147) at org.apache.kafka.clients.producer.internals.RecordAccumulator.append(RecordAccumulator.java:221) at org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:941) at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:862) at org.apache.kafka.connect.util.KafkaBasedLog.send(KafkaBasedLog.java:238) at org.apache.kafka.connect.storage.KafkaStatusBackingStore$4.onCompletion(KafkaStatusBackingStore.java:298) ... at org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:959) at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:862) at org.apache.kafka.connect.util.KafkaBasedLog.send(KafkaBasedLog.java:238) at org.apache.kafka.connect.storage.KafkaStatusBackingStore$4.onCompletion(KafkaStatusBackingStore.java:298){code}
Note the repeated KafkaProducer.send -> KafkaProducer.doSend -> KafkaStatusBackingStore$4.onCompletion calls, causing the issue.",durban,durban,Major,Resolved,Fixed,13/Apr/23 08:20,24/Apr/23 07:50
Bug,KAFKA-14908,13532654,"Sporadic ""Address already in use"" when starting kafka cluster embedded within tests","We have an integration test suite that starts/stops a kafka cluster before/after each test.   Kafka is being started programmatically within the same JVM that is running the tests.

Sometimes we get sporadic failures from with Kafka as it tries to bind the server socket.
{code:java}
org.apache.kafka.common.KafkaException: Socket server failed to bind to 0.0.0.0:9092: Address already in use.
    at kafka.network.Acceptor.openServerSocket(SocketServer.scala:684)
    at kafka.network.Acceptor.<init>(SocketServer.scala:576)
    at kafka.network.DataPlaneAcceptor.<init>(SocketServer.scala:433)
    at kafka.network.SocketServer.createDataPlaneAcceptor(SocketServer.scala:247)
    at kafka.network.SocketServer.createDataPlaneAcceptorAndProcessors(SocketServer.scala:226)
    at kafka.network.SocketServer.$anonfun$new$31(SocketServer.scala:173)
    at kafka.network.SocketServer.$anonfun$new$31$adapted(SocketServer.scala:173)
    at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:575)
    at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:573)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:933)
    at kafka.network.SocketServer.<init>(SocketServer.scala:173)
    at kafka.server.KafkaServer.startup(KafkaServer.scala:331) {code}
Investigation has shown that the socket is in the timed_wait state from a previous test.

I know Kafka supports ephemeral ports, but this isn't convenient to our use-case.  

I'd like to suggest that Kafka is changed to set the SO_REUSEADDR on the server socket.  I believe this is standard practice for server applications that run on well known ports .

I don't believe this change would introduce a backward compatibility concerns. 

 

I will open a PR so that can be considered. Thank you.",,kwall,Minor,Resolved,Fixed,14/Apr/23 11:53,27/Apr/23 09:38
Bug,KAFKA-14938,13534138,Flaky test org.apache.kafka.connect.integration.ExactlyOnceSourceIntegrationTest#testConnectorBoundary,"Test seems to be failing with 

```
ava.lang.AssertionError: Not enough records produced by source connector. Expected at least: 100 + but got 72
h4. Stacktrace
java.lang.AssertionError: Not enough records produced by source connector. Expected at least: 100 + but got 72
 at org.junit.Assert.fail(Assert.java:89)
 at org.junit.Assert.assertTrue(Assert.java:42)
 at org.apache.kafka.connect.integration.ExactlyOnceSourceIntegrationTest.testConnectorBoundary(ExactlyOnceSourceIntegrationTest.java:421)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
 at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
 at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
 at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
 at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
 at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:108)
 at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
 at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:40)
 at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:60)
 at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:52)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
 at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
 at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
 at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
 at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker$2.run(TestWorker.java:176)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)
 at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
 at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:113)
 at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:65)
 at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)
```",sajain16,sagarrao,Major,Resolved,Fixed,26/Apr/23 12:02,12/Jul/23 15:59
Bug,KAFKA-14943,13534235,Fix ClientQuotaControlManager validation,,cmccabe,cmccabe,Major,Resolved,Fixed,26/Apr/23 23:10,24/May/23 23:00
Bug,KAFKA-14946,13534272,KRaft controller node shutting down while renouncing leadership,"* We are using the zookeeper less Kafka (kafka Kraft).
 * The cluster is having 3 nodes.
 * One of the nodes gets automatically shut down randomly.
 * Checked the logs but didn't get the exact reason.
 * Kafka version - 3.3.1
 * Attaching the log files. 
 * Time - 2023-04-21 16:28:23

*state-change.log -*
[https://drive.google.com/file/d/1eS-ShKlhGPsIJoybHndlhahJnucU8RWA/view?usp=share_link]
 
*server.log -*
[https://drive.google.com/file/d/1Ov5wrQIqx2AS4J7ppFeHJaDySsfsK588/view?usp=share_link]",showuon,akshaykumar001,Critical,Resolved,Fixed,27/Apr/23 08:09,04/May/23 01:45
Bug,KAFKA-14956,13534707,Flaky test org.apache.kafka.connect.integration.OffsetsApiIntegrationTest#testGetSinkConnectorOffsetsDifferentKafkaClusterTargeted,"```
h4. Error
org.opentest4j.AssertionFailedError: Condition not met within timeout 15000. Sink connector consumer group offsets should catch up to the topic end offsets ==> expected: <true> but was: <false>
h4. Stacktrace
org.opentest4j.AssertionFailedError: Condition not met within timeout 15000. Sink connector consumer group offsets should catch up to the topic end offsets ==> expected: <true> but was: <false>
 at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)
 at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)
 at app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)
 at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)
 at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:211)
 at app//org.apache.kafka.test.TestUtils.lambda$waitForCondition$4(TestUtils.java:337)
 at app//org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:385)
 at app//org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:334)
 at app//org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:318)
 at app//org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:291)
 at app//org.apache.kafka.connect.integration.OffsetsApiIntegrationTest.getAndVerifySinkConnectorOffsets(OffsetsApiIntegrationTest.java:150)
 at app//org.apache.kafka.connect.integration.OffsetsApiIntegrationTest.testGetSinkConnectorOffsetsDifferentKafkaClusterTargeted(OffsetsApiIntegrationTest.java:131)
 at java.base@17.0.7/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at java.base@17.0.7/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
 at java.base@17.0.7/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.base@17.0.7/java.lang.reflect.Method.invoke(Method.java:568)
 at app//org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
 at app//org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at app//org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
 at app//org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at app//org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at app//org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at app//org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
 at app//org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
 at app//org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
 at app//org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
 at app//org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
 at app//org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
 at app//org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
 at app//org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
 at app//org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
 at app//org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
 at app//org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
 at app//org.junit.runners.ParentRunner.run(ParentRunner.java:413)
 at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:108)
 at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
 at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:40)
 at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:60)
 at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:52)
 at java.base@17.0.7/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at java.base@17.0.7/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
 at java.base@17.0.7/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.base@17.0.7/java.lang.reflect.Method.invoke(Method.java:568)
 at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
 at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
 at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
 at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
 at jdk.proxy1/jdk.proxy1.$Proxy2.processTestClass(Unknown Source)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker$2.run(TestWorker.java:176)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)
 at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
 at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:113)
 at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:65)
 at app//worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)
 at app//worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)
```
[https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-13594/5/tests/]
 
The test failure seems unrelated to the changes in the PR. [~yash.mayya] , can you plz take a look?",yash.mayya,sagarrao,Major,Resolved,Fixed,02/May/23 15:38,29/May/23 07:01
Bug,KAFKA-14962,13534858,Whitespace in ACL configuration causes Kafka startup to fail,"Kafka's startup can fail if there is a trailing or leading whitespace for a configuration value. This fix makes it more tolerant towards cases where a user might accidentally add a trailing or leading whitespace in ACL configuration.
{code:java}
ERROR [KafkaServer id=3] Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)

java.lang.IllegalArgumentException: For input string: ""true ""

    at scala.collection.StringOps$.toBooleanImpl$extension(StringOps.scala:943)

    at kafka.security.authorizer.AclAuthorizer.$anonfun$configure$4(AclAuthorizer.scala:153) {code}",divijvaidya,divijvaidya,Minor,Resolved,Fixed,03/May/23 16:49,15/May/23 12:32
Bug,KAFKA-14963,13534868,Incorrect partition count metrics for kraft controllers,"It is possible for the KRaft controller to report more partitions than are available in the cluster. This is because the following test fail against 3.4.0:
{code:java}
       @Test
      public void testPartitionCountDecreased() {
          ControllerMetrics metrics = new MockControllerMetrics();
          ControllerMetricsManager manager = new ControllerMetricsManager(metrics);          Uuid createTopicId = Uuid.randomUuid();
          Uuid createPartitionTopicId = new Uuid(
              createTopicId.getMostSignificantBits(),
              createTopicId.getLeastSignificantBits()
          );
          Uuid removeTopicId = new Uuid(createTopicId.getMostSignificantBits(), createTopicId.getLeastSignificantBits());
          manager.replay(topicRecord(""test"", createTopicId));
          manager.replay(partitionRecord(createPartitionTopicId, 0, 0, Arrays.asList(0, 1, 2)));
          manager.replay(partitionRecord(createPartitionTopicId, 1, 0, Arrays.asList(0, 1, 2)));
          manager.replay(removeTopicRecord(removeTopicId));
          assertEquals(0, metrics.globalPartitionCount());
      }
{code}",jsancio,jsancio,Major,Resolved,Fixed,03/May/23 17:41,05/May/23 07:55
Bug,KAFKA-14978,13535466,ExactlyOnceWorkerSourceTask does not remove parent metrics,"ExactlyOnceWorkerSourceTask removeMetrics does not invoke super.removeMetrics, meaning that only the transactional metrics are removed, and common source task metrics are not.",durban,durban,Major,Resolved,Fixed,09/May/23 06:03,19/May/23 12:50
Bug,KAFKA-14980,13535540,MirrorMaker consumers don't get configs prefixed with source.cluster,"As part of KAFKA-14021, we made a change to MirrorConnectorConfig.sourceConsumerConfig() to grab all configs that start with ""source."". Previously it was grabbing configs prefixed with ""source.cluster."". 

This means existing connector configuration stop working, as configurations such as bootstrap.servers are not passed to source consumers.

For example, the following connector configuration was valid in 3.4 and now makes the connector tasks fail:

{code:json}
{
    ""connector.class"": ""org.apache.kafka.connect.mirror.MirrorSourceConnector"",
    ""name"": ""source"",
    ""topics"": ""test"",
    ""tasks.max"": ""30"",
    ""source.cluster.alias"": ""one"",
    ""target.cluster.alias"": ""two"",
    ""source.cluster.bootstrap.servers"": ""localhost:9092"",
   ""target.cluster.bootstrap.servers"": ""localhost:29092""
}
{code}


The connector attempts to start source consumers with bootstrap.servers = [] and the task crash with 


{noformat}
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:837)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:671)
	at org.apache.kafka.connect.mirror.MirrorUtils.newConsumer(MirrorUtils.java:59)
	at org.apache.kafka.connect.mirror.MirrorSourceTask.start(MirrorSourceTask.java:103)
	at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.initializeAndStart(AbstractWorkerSourceTask.java:274)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:202)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:259)
	at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.run(AbstractWorkerSourceTask.java:75)
	at 
org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:181)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
{noformat}


",ChrisEgerton,mimaison,Blocker,Resolved,Fixed,09/May/23 16:34,22/May/23 14:04
Bug,KAFKA-14994,13535990, jose4j is vulnerable to CVE- Improper Cryptographic Algorithm,"Jose4j has the following vulnerability with high score of 7.1. 
jose4j is vulnerable to Improper Cryptographic Algorithm. The vulnerability exists due to the way `RSA1_5` and `RSA_OAEP` is implemented, allowing an attacker to decrypt `RSA1_5` or `RSA_OAEP` encrypted ciphertexts, and in addition, it may be feasible to sign with affected keys.

Please help upgrade the library to latest version
Current version in use: 0.7.9
Latest version with the fix: 0.9.3
CVE-
- Improper Cryptographic Algorithm
- Severity: HIGH
- CVSS: 7.1
- Disclosure Date: 07 Feb 2023 19:00PM EST
- Vulnerability Info: https://sca.analysiscenter.veracode.com/vulnerability-database/vulnerabilities/40398",atusharm,jetlyg,Major,Resolved,Fixed,12/May/23 13:34,13/May/23 11:40
Bug,KAFKA-14996,13536016,The KRaft controller should properly handle overly large user operations,"If an attempt is made to create a topic with

num partitions >= QuorumController.MAX_RECORDS_PER_BATCH  (10000)

the client receives an UnknownServerException - it could rather receive a better error.

The controller logs

{{2023-05-12 19:25:10,018] WARN [QuorumController id=1] createTopics: failed with unknown server exception IllegalStateException at epoch 2 in 21956 us.  Renouncing leadership and reverting to the last committed offset 174. (org.apache.kafka.controller.QuorumController)}}
{{java.lang.IllegalStateException: Attempted to atomically commit 10001 records, but maxRecordsPerBatch is 10000}}
{{    at org.apache.kafka.controller.QuorumController.appendRecords(QuorumController.java:812)}}
{{    at org.apache.kafka.controller.QuorumController$ControllerWriteEvent.run(QuorumController.java:719)}}
{{    at org.apache.kafka.queue.KafkaEventQueue$EventContext.run(KafkaEventQueue.java:127)}}
{{    at org.apache.kafka.queue.KafkaEventQueue$EventHandler.handleEvents(KafkaEventQueue.java:210)}}
{{    at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:181)}}
{{    at java.base/java.lang.Thread.run(Thread.java:829)}}
{{[}}",cmccabe,ecomar,Blocker,Resolved,Fixed,12/May/23 18:30,02/Jun/23 14:57
Bug,KAFKA-14997,13536074,JmxToolTest failing with initializationError,"Noticed that JmxToolTest fails with 

```
h4. Error
java.io.IOException: Cannot bind to URL [rmi://:44743/jmxrmi]: javax.naming.ServiceUnavailableException [Root exception is java.rmi.ConnectException: Connection refused to host: 40.117.157.99; nested exception is: 
 java.net.ConnectException: Connection timed out (Connection timed out)]
h4. Stacktrace
java.io.IOException: Cannot bind to URL [rmi://:44743/jmxrmi]: javax.naming.ServiceUnavailableException [Root exception is java.rmi.ConnectException: Connection refused to host: 40.117.157.99; nested exception is: 
 java.net.ConnectException: Connection timed out (Connection timed out)]
 at javax.management.remote.rmi.RMIConnectorServer.newIOException(RMIConnectorServer.java:827)
 at javax.management.remote.rmi.RMIConnectorServer.start(RMIConnectorServer.java:432)
 at org.apache.kafka.tools.JmxToolTest.startJmxAgent(JmxToolTest.java:337)
 at org.apache.kafka.tools.JmxToolTest.beforeAll(JmxToolTest.java:55)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
 at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
 at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptLifecycleMethod(TimeoutExtension.java:128)
 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptBeforeAllMethod(TimeoutExtension.java:70)
 at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
 at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
 at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
 at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeBeforeAllMethods$13(ClassBasedTestDescriptor.java:411)
 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeBeforeAllMethods(ClassBasedTestDescriptor.java:409)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:215)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:84)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)
 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
 at java.util.ArrayList.forEach(ArrayList.java:1259)
 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
 at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
 at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
 at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
 at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
 at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
 at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
 at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
 at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:110)
 at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:90)
 at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:85)
 at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:62)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
 at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
 at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
 at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
 at com.sun.proxy.$Proxy2.stop(Unknown Source)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:193)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)
 at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)
 at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
 at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:113)
 at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:65)
 at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)
 at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)
 Suppressed: java.lang.NullPointerException
 at org.apache.kafka.tools.JmxToolTest.afterAll(JmxToolTest.java:61)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
 at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
 at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptLifecycleMethod(TimeoutExtension.java:128)
 at org.junit.jupiter.engine.extension.TimeoutExtension.interceptAfterAllMethod(TimeoutExtension.java:118)
 at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
 at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
 at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
 at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
 at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeAfterAllMethods$15(ClassBasedTestDescriptor.java:439)
 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeAfterAllMethods$16(ClassBasedTestDescriptor.java:437)
 at java.util.ArrayList.forEach(ArrayList.java:1259)
 at java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1082)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeAfterAllMethods(ClassBasedTestDescriptor.java:437)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.after(ClassBasedTestDescriptor.java:231)
 at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.after(ClassBasedTestDescriptor.java:84)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:161)
 at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
 at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:161)
 ... 48 more
Caused by: javax.naming.ServiceUnavailableException [Root exception is java.rmi.ConnectException: Connection refused to host: 40.117.157.99; nested exception is: 
 java.net.ConnectException: Connection timed out (Connection timed out)]
 at com.sun.jndi.rmi.registry.RegistryContext.bind(RegistryContext.java:161)
 at com.sun.jndi.toolkit.url.GenericURLContext.bind(GenericURLContext.java:241)
 at javax.naming.InitialContext.bind(InitialContext.java:425)
 at javax.management.remote.rmi.RMIConnectorServer.bind(RMIConnectorServer.java:644)
 at javax.management.remote.rmi.RMIConnectorServer.start(RMIConnectorServer.java:427)
 ... 76 more
Caused by: java.rmi.ConnectException: Connection refused to host: 40.117.157.99; nested exception is: 
 java.net.ConnectException: Connection timed out (Connection timed out)
 at sun.rmi.transport.tcp.TCPEndpoint.newSocket(TCPEndpoint.java:623)
 at sun.rmi.transport.tcp.TCPChannel.createConnection(TCPChannel.java:216)
 at sun.rmi.transport.tcp.TCPChannel.newConnection(TCPChannel.java:202)
 at sun.rmi.server.UnicastRef.newCall(UnicastRef.java:343)
 at sun.rmi.registry.RegistryImpl_Stub.bind(RegistryImpl_Stub.java:65)
 at com.sun.jndi.rmi.registry.RegistryContext.bind(RegistryContext.java:155)
 ... 80 more
Caused by: java.net.ConnectException: Connection timed out (Connection timed out)
 at java.net.PlainSocketImpl.socketConnect(Native Method)
 at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
 at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
 at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
 at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
 at java.net.Socket.connect(Socket.java:607)
 at java.net.Socket.connect(Socket.java:556)
 at java.net.Socket.<init>(Socket.java:452)
 at java.net.Socket.<init>(Socket.java:229)
 at sun.rmi.transport.proxy.RMIDirectSocketFactory.createSocket(RMIDirectSocketFactory.java:40)
 at sun.rmi.transport.proxy.RMIMasterSocketFactory.createSocket(RMIMasterSocketFactory.java:148)
 at sun.rmi.transport.tcp.TCPEndpoint.newSocket(TCPEndpoint.java:617)
 ... 85 more
```
One such build [https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-13453/6/tests]
 
[~fvaleri] i am assigning it to you. ",fvaleri,sagarrao,Major,Resolved,Fixed,13/May/23 12:59,16/May/23 02:31
Bug,KAFKA-15003,13536594,TopicIdReplicaAssignment is not updated in migration (dual-write) when partitions are changed for topic,,akhileshchg,akhileshchg,Blocker,Resolved,Fixed,17/May/23 16:31,02/Jun/23 14:58
Bug,KAFKA-15004,13536598,Topic config changes are not synced during zk to kraft migration (dual-write),,akhileshchg,akhileshchg,Blocker,Resolved,Fixed,17/May/23 16:32,02/Jun/23 14:59
Bug,KAFKA-15007,13536769,MV is not set correctly in the MetadataPropagator in migration.,"MV changes are not set in propagator unless we're in DUAL_WRITE mode. But we do this, we'll skip any known MV changes. The propagator should always know the correct MV so that it sends correct UMR and LISR during DUAL_WRITE.",akhileshchg,akhileshchg,Blocker,Resolved,Fixed,18/May/23 17:20,24/May/23 22:07
Bug,KAFKA-15009,13536918,New ACLs are not written to ZK during migration,"While handling snapshots in dual-write mode, we are missing the logic to detect new ACLs created in KRaft. This means we will not write these new ACLs back to ZK and they would be missing if a user rolled back their cluster to ZK mode. ",akhileshchg,akhileshchg,Blocker,Resolved,Fixed,19/May/23 22:52,24/May/23 22:06
Bug,KAFKA-15010,13536920,KRaft Controller doesn't reconcile with Zookeeper metadata upon becoming new controller while in dual write mode.,"When a KRaft controller fails over, the existing migration driver (in dual write mode) can fail in between Zookeeper writes and may leave Zookeeper with incomplete and inconsistent data. So when a new controller becomes active (and by extension new migration driver becomes active), this first thing we should do is load the in-memory snapshot and use it to write metadata to Zookeeper to have a steady state. We currently do not do this and it may leave Zookeeper in inconsistent state.",davidarthur,akhileshchg,Blocker,Resolved,Fixed,19/May/23 23:19,02/Jun/23 14:47
Bug,KAFKA-15012,13537176,JsonConverter fails when there are leading Zeros in a field,"When there are leading zeros in a field in the Kakfa Record, a sink connector using JsonConverter fails with the below exception

 
{code:java}
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:206)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:132)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:494)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:474)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.kafka.connect.errors.DataException: Converting byte[] to Kafka Connect data failed due to serialization error: 
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:324)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertKey(WorkerSinkTask.java:531)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$1(WorkerSinkTask.java:494)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:156)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:190)
	... 13 more
Caused by: org.apache.kafka.common.errors.SerializationException: com.fasterxml.jackson.core.JsonParseException: Invalid numeric value: Leading zeroes not allowed
 at [Source: (byte[])""00080153032837""; line: 1, column: 2]
Caused by: com.fasterxml.jackson.core.JsonParseException: Invalid numeric value: Leading zeroes not allowed
 at [Source: (byte[])""00080153032837""; line: 1, column: 2]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1840)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:712)
	at com.fasterxml.jackson.core.base.ParserMinimalBase.reportInvalidNumber(ParserMinimalBase.java:551)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._verifyNoLeadingZeroes(UTF8StreamJsonParser.java:1520)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parsePosNumber(UTF8StreamJsonParser.java:1372)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._nextTokenNotInObject(UTF8StreamJsonParser.java:855)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:754)
	at com.fasterxml.jackson.databind.ObjectMapper._readTreeAndClose(ObjectMapper.java:4247)
	at com.fasterxml.jackson.databind.ObjectMapper.readTree(ObjectMapper.java:2734)
	at org.apache.kafka.connect.json.JsonDeserializer.deserialize(JsonDeserializer.java:64)
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:322)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertKey(WorkerSinkTask.java:531)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$1(WorkerSinkTask.java:494)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:156)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:190)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:132)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:494)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:474)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:329)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:232)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:201)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:188)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:237)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829) {code}
 

 

To resolve the issue, we need to add the below line when in the JsonSerializer.java and JsonDeserializer.java classes.
{code:java}
objectMapper.enable(JsonReadFeature.ALLOWLEADINGZEROSFORNUMBERS.mappedFeature()); {code}
Attaching a patch file showing the changes here. 

 ",yash.mayya,ranjanrao,Major,Resolved,Fixed,22/May/23 14:23,02/Jun/23 15:20
Bug,KAFKA-15015,13537196,Binaries contain 2 versions of reload4j,"These releases ship 2 versions of reload4j:
- reload4j-1.2.19.jar
- reload4j-1.2.25.jar",atusharm,mimaison,Major,Resolved,Fixed,22/May/23 15:56,24/May/23 01:34
Bug,KAFKA-15016,13537198,LICENSE-binary file contains dependencies not included anymore,While adjusting LICENSE-binary for 3.5.0 I noticed a few entries are not dependencies anymore. We should resync the file properly,mimaison,mimaison,Major,Resolved,Fixed,22/May/23 16:00,24/May/23 15:34
Bug,KAFKA-15017,13537354,New ClientQuotas are not written to ZK from snapshot ,Similar issue to KAFKA-15009,pprovenzano,mumrah,Critical,Resolved,Fixed,23/May/23 14:39,01/Jun/23 14:52
Bug,KAFKA-15019,13537560,Improve handling of broker heartbeat timeouts,Improve handling of overload situations in the KRaft controller,cmccabe,cmccabe,Major,Resolved,Fixed,24/May/23 23:03,05/Jun/23 21:08
Bug,KAFKA-15021,13537681,KRaft controller increases leader epoch when shrinking ISR,"When the KRaft controller shrinks the ISR it also forces the leader epoch to increase. This is unnecessary and cases all of the follower replica fetches to get invalidated.

Here is an example trace of this behavior after replica 8 was shutdown:
{code:java}
kafka-dump-log --cluster-metadata-decoder --files __cluster_metadata-0/00000000000038589501.log | grep Pd7wMb4lSkKI00--SrWNXw
...
| offset: 38655592 CreateTime: 1683849857362 keySize: -1 valueSize: 41 sequence: -1 headerKeys: [] payload: {""type"":""PARTITION_CHANGE_RECORD"",""version"":0,""data"":{""partitionId"":7,""topicId"":""Pd7wMb4lSkKI00--SrWNXw"",""isr"":[3,1],""leader"":1}}
| offset: 38655593 CreateTime: 1683849857362 keySize: -1 valueSize: 41 sequence: -1 headerKeys: [] payload: {""type"":""PARTITION_CHANGE_RECORD"",""version"":0,""data"":{""partitionId"":5,""topicId"":""Pd7wMb4lSkKI00--SrWNXw"",""isr"":[0,4],""leader"":4}}
| offset: 38655594 CreateTime: 1683849857362 keySize: -1 valueSize: 41 sequence: -1 headerKeys: [] payload: {""type"":""PARTITION_CHANGE_RECORD"",""version"":0,""data"":{""partitionId"":6,""topicId"":""Pd7wMb4lSkKI00--SrWNXw"",""isr"":[0,1],""leader"":0}}
| offset: 38656159 CreateTime: 1683849974945 keySize: -1 valueSize: 39 sequence: -1 headerKeys: [] payload: {""type"":""PARTITION_CHANGE_RECORD"",""version"":0,""data"":{""partitionId"":7,""topicId"":""Pd7wMb4lSkKI00--SrWNXw"",""isr"":[3,1,8]}}
| offset: 38656256 CreateTime: 1683849994297 keySize: -1 valueSize: 39 sequence: -1 headerKeys: [] payload: {""type"":""PARTITION_CHANGE_RECORD"",""version"":0,""data"":{""partitionId"":5,""topicId"":""Pd7wMb4lSkKI00--SrWNXw"",""isr"":[0,4,8]}}
| offset: 38656299 CreateTime: 1683849997139 keySize: -1 valueSize: 39 sequence: -1 headerKeys: [] payload: {""type"":""PARTITION_CHANGE_RECORD"",""version"":0,""data"":{""partitionId"":6,""topicId"":""Pd7wMb4lSkKI00--SrWNXw"",""isr"":[0,1,8]}}
| offset: 38657003 CreateTime: 1683850157379 keySize: -1 valueSize: 30 sequence: -1 headerKeys: [] payload: {""type"":""PARTITION_CHANGE_RECORD"",""version"":0,""data"":{""partitionId"":7,""topicId"":""Pd7wMb4lSkKI00--SrWNXw"",""leader"":8}} {code}
Also, notice how the leader epoch was not increased when the ISR was expanded.",jsancio,jsancio,Major,Resolved,Fixed,25/May/23 14:55,09/Jun/23 11:35
Bug,KAFKA-15044,13538303,Snappy v.1.1.9.1 NoClassDefFound on ARM machines,We upgraded our snappy dependency but v1.1.9.1 has compatibility issues with arm. We should upgrade to v1.1.10.0 which resolves this issue.,david.mao,david.mao,Major,Resolved,Fixed,31/May/23 17:01,01/Jun/23 09:47
Bug,KAFKA-15053,13538657,Regression for security.protocol validation starting from 3.3.0,"[This|https://issues.apache.org/jira/browse/KAFKA-13793] Jira issue introduced validations on multiple configs. As a consequence, config {{security.protocol}} now only allows upper case values such as PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL. Before this change, lower case values like sasl_ssl, ssl are also supported, there's even a case insensitive logic inside [SecurityProtocol|https://github.com/apache/kafka/blob/146a6976aed0d9f90c70b6f21dca8b887cc34e71/clients/src/main/java/org/apache/kafka/common/security/auth/SecurityProtocol.java#L70-L73] to handle the lower case values.

I think we should treat this as a regression bug since we don't support lower case values anymore since 3.3.0. For versions later than 3.3.0, we are getting error like this when using lower case value sasl_ssl

{{Invalid value sasl_ssl for configuration security.protocol: String must be one of: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL}}",dlgaobo,dlgaobo,Major,Resolved,Fixed,02/Jun/23 23:31,29/Jun/23 17:24
Bug,KAFKA-15059,13538830,Exactly-once source tasks fail to start during pending rebalances,"When asked to perform a round of zombie fencing, the distributed herder will [reject the request|https://github.com/apache/kafka/blob/17fd30e6b457f097f6a524b516eca1a6a74a9144/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L1249-L1250] if a rebalance is pending, which can happen if (among other things) a config for a new connector or a new set of task configs has been recently read from the config topic.

Normally this can be alleviated with a simple task restart, which isn't great but isn't terrible.

However, when running MirrorMaker 2 in dedicated mode, there is no API to restart failed tasks, and it can be more common to see this kind of failure on a fresh cluster because three connector configurations are written in rapid succession to the config topic.

 

In order to provide a better experience for users of both vanilla Kafka Connect and dedicated MirrorMaker 2 clusters, we can retry (likely with the same exponential backoff introduced with KAFKA-14732) zombie fencing attempts that fail due to a pending rebalance.",ChrisEgerton,ChrisEgerton,Blocker,Resolved,Fixed,05/Jun/23 21:38,11/Jul/23 14:23
Bug,KAFKA-15077,13539449,FileTokenRetriever doesn't trim the token before returning it.,"The {{FileTokenRetriever}} class is used to read the access_token from a file on the clients system and then the info is passed along with jaas config to the {{{}OAuthBearerSaslServer{}}}.

The server uses the class {{OAuthBearerClientInitialResponse}} to validate the token format.

In case the token was sent using {{FileTokenRetriever}} on the client side, some EOL character is getting appended to the token, causing authentication to fail with the message (in case to topic create):
 {{ERROR org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed during authentication due to invalid credentials with SASL mechanism OAUTHBEARER}}
 
On the server side the following line [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponse.java#L68] with throw an exception failing the request.",smjn,sushmahajn,Minor,Resolved,Fixed,09/Jun/23 14:18,11/Jun/23 06:23
Bug,KAFKA-15080,13539612,Fetcher's lag never set when partition is idle,"The PartitionFetchState's lag field is set to None when the state is created and it is updated when bytes are received for a partition. For idle partitions (newly created or not), the lag is never updated because `validBytes > 0` is never true. As a side effect, the partition is considered out-of-sync and could be incorrectly throttled.",dajac,dajac,Major,Resolved,Fixed,12/Jun/23 07:55,13/Jun/23 13:26
Bug,KAFKA-15096,13540430,CVE 2023-34455 - Vulnerability identified with Apache kafka,"A new vulnerability CVE-2023-34455 is identified with apache kafka dependency. The vulnerability is coming from snappy-java:1.1.8.4

Version 1.1.10.1 contains a patch for this issue. Please upgrade the snappy-java version to fix this issue

 
snappy-java is a fast compressor/decompressor for Java. Due to use of an unchecked chunk length, an unrecoverable fatal error can occur in versions prior to 1.1.10.1.
The code in the function hasNextChunk in the fileSnappyInputStream.java checks if a given stream has more chunks to read. It does that by attempting to read 4 bytes. If it wasn’t possible to read the 4 bytes, the function returns false. Otherwise, if 4 bytes were available, the code treats them as the length of the next chunk.
In the case that the `compressed` variable is null, a byte array is allocated with the size given by the input data. Since the code doesn’t test the legality of the `chunkSize` variable, it is possible to pass a negative number (such as 0xFFFFFFFF which is -1), which will cause the code to raise a `java.lang.NegativeArraySizeException` exception. A worse case would happen when passing a huge positive value (such as 0x7FFFFFFF), which would raise the fatal `java.lang.OutOfMemoryError` error.",manyanda,Sasikumarms,Major,Resolved,Fixed,16/Jun/23 15:39,07/Jul/23 13:26
Bug,KAFKA-15098,13540455,KRaft migration does not proceed and broker dies if authorizer.class.name is set,"[ERROR] 2023-06-16 20:14:14,298 [main] kafka.Kafka$ - Exiting Kafka due to fatal exception
java.lang.IllegalArgumentException: requirement failed: ZooKeeper migration does not yet support authorizers. Remove authorizer.class.name before performing a migration.
",mumrah,rndgstn,Blocker,Resolved,Fixed,16/Jun/23 21:18,22/Jun/23 21:00
Bug,KAFKA-15109,13540787,ISR shrink/expand issues on ZK brokers during migration,"KAFKA-15021 introduced a new controller behavior that avoids increasing the leader epoch during the controlled shutdown scenario. This prevents some unnecessary thrashing of metadata and threads on the brokers and clients. 

While a cluster is in a KIP-866 migration and has a KRaft controller with ZK brokers, we cannot employ this leader epoch bump avoidance. The ZK brokers must have the leader epoch bump in order for ReplicaManager to react to the LeaderAndIsrRequest.",mumrah,mumrah,Critical,Resolved,Fixed,20/Jun/23 16:00,27/Jun/23 15:49
Bug,KAFKA-15114,13541046,StorageTool help specifies user as parameter not name,"StorageTool help message current specifies setting a {{user}} parameter when creating a SCRAM record for bootstrap.

The StorageTool parses and only accepts the parameter as {{name}} and so the help message is wrong.

The choice of using {{name}} vs. {{user}} as a parameter is because internally the record uses name, all tests using the StorageTool use name as a parameter, KafkaPrincipals are created with {{name}} and because creating SCRAM credentials is done with {{--entity-name}}

I will change the help to specify {{name}} instead of {{user}}.


 ",pprovenzano,pprovenzano,Minor,Resolved,Fixed,22/Jun/23 14:41,05/Jul/23 16:45
Bug,IGNITE-18495,13516334,Fix RAFT snapshot installation hang due to response swap on retry,"The scenario follows:
 # InstallSnapshot request is sent, its processing starts hanging forever (it will be cancelled on step 3)
 # After a timeout, second InstallSnapshot request is sent with same index+term as the first had; in JRaft, it causes a special handling (previous request processing is NOT cancelled)
 # After a timeout, third InstallSnapshot request is sent with DIFFERENT index, so it cancels the first snapshot processing effectively unblocking the first thread

In the original JRaft implementation, after being unblocked, the first thread fails to clean up, so subsequent retries will always see a phantom of an unfinished snapshot, so the snapshotting process will be jammed. Also, node stop might stuck because one 'download' task will remain unfinished forever.",rpuch,rpuch,Major,Resolved,Fixed,03/Jan/23 14:37,04/Jan/23 07:52
Bug,IGNITE-18497,13516355,Read only get returns a first one value getting from primary index,"*Motivation*

Indexes store all value associated with different versions of one entry. By the reason, for getting value by primary index, we scan the index with the specific key. If we insert, delete and again insert an entry with the same indexed fields, the entry can resolve in versioned storage for different row ids. But only one resolution should return not empty value, because only one entry can exist by the unique index.

*Implementation notes*

The resolution happens here:
{code:java}
PartitionReplicaListener#resolveRowByPk(BinaryRow, HybridTimestamp){code}
But in case when a read result is resolved to null, need to continue the loop, because the actual value associated with the key may be removed (this is the null value, but it is not actual) and inserted again.
 
h3. upd 1:
 # Seems that it's rather simple in case of primary index (and given ticket is about primary index) because keys are unmodifiable, so it's only possible to have multiple entries for particular search key only in case of creation-> removal -> creation. That means that we can have one and only one non empty readResult for any read timestamp for the given key. Which in turn means that if we found non empty readResult during PK index iteration we can proceed with readResult resolution and stop the iteration, oterwise, in case of empty read result we should continue the iteration as Vlad told. So basically we should add 
{code:java}
if (readResult.isEmpty()) {continue;}{code}
after 
{code:java}
ReadResult readResult = mvDataStorage.read(rowId, ts);{code}

 # We should probably fix PartitionReplicaListener#resolveReadResult instead of returning null in case of null readResult we should complete future with null.

{code:java}
    private CompletableFuture<BinaryRow> resolveReadResult(
            ReadResult readResult,
            @Nullable UUID txId,
            @Nullable HybridTimestamp timestamp,
            @Nullable Supplier<BinaryRow> lastCommitted
    ) {
        if (readResult == null) {
            return null;
             {code}",Denis Chudov,v.pyatkov,Major,Resolved,Fixed,03/Jan/23 17:54,09/Feb/23 17:48
Bug,IGNITE-18503,13516635,Need to return the ability of WatchListener to listen to one specific key,"h3. Motivation
The MetaStorageManager#registerWatch method was removed in  IGNITE-18413. Usage of 'registerWatch' was changed on 'registerWatchByPrefix' in DistributionZoneManager. So now WatchListener in DistributionZoneManager is triggered not only by key 'distributionZones.logicalTopology' but also 'distributionZones.logicalTopologyVersion'.

h3. Definition of Done
Need to check where 'registerWatch' was used and return the ability to listen to one specific key.",apolovtcev,Sergey Uttsel,Major,Resolved,Fixed,05/Jan/23 12:09,09/Jan/23 08:21
Bug,IGNITE-18506,13516647,ItDataSchemaSyncTest.checkSchemasCorrectlyRestore() is flaky,"[https://ci.ignite.apache.org/test/5151808410902558750?currentProjectId=ApacheIgnite3xGradle_Test_IntegrationTests&expandTestHistoryChartSection=true&branch=]

In failing runs, nodes leave the cluster and return to it during the test. This seems to make session hang (until timeout of 5 minutes triggers).",zstan,rpuch,Major,Resolved,Fixed,05/Jan/23 13:37,10/Mar/23 12:25
Bug,IGNITE-18508,13516655,Metastorage-api does not generate jar,"Need to add
{noformat}
apply from: ""$rootDir/buildscripts/publishing.gradle""{noformat}
into the module's build.gradle.

The same for metastorage module.",Berkov,Berkov,Major,Resolved,Fixed,05/Jan/23 14:58,12/Jan/23 11:02
Bug,IGNITE-18510,13517207,Sql. CompositePublisher is not thread safe,"Currently our CompositePublisher is not threadsafe, but we have to deal with 3 types of potentially concurrent signals when arbitrating {*}Flow.Subscription{*}s:
 # A *request(long)* call from downstream that has to be routed to the current Flow.Subscription
 # A *cancel()* call from downstream that has to be routed to the current *Flow.Subscription* and cancel any future {*}Flow.Subscription{*}.
 # A *setSubscription(Flow.Subscription)* that is called by the current *Flow.Subscriber* after subscribing to any *Flow.Publisher* which is not guaranteed to happen on the same thread *subscribe()* is called

Let's reimplement our CompositePublisher",jooger,jooger,Major,Resolved,Fixed,09/Jan/23 09:16,17/Jan/23 12:19
Bug,IGNITE-18518,13517450,Sorted index scan may return obsolete row versions,"According to the MV store design, indexes must be filtered before returning data to user. For example, imagine that a row is deleted. This means that there's a tombstone.

But, there's still an entry in indexes, for column values that the row used to have. These should not be visible to end user.",sdanilov,ibessonov,Major,Resolved,Fixed,10/Jan/23 12:44,22/May/23 08:25
Bug,IGNITE-18521,13517466,Sql. Query hangs if exception is thrown in mapping phase,"Look at the closure passed to {{taskExecutor}} at the beginning of {{org.apache.ignite.internal.sql.engine.exec.ExecutionServiceImpl.DistributedQueryManager#execute}}. Any exception thrown before the try-catch block will cause the root fragment not to be initialized and, as a consequence, the query will hang forever.

Let's add the proper exception handling to this place.",xtern,korlov,Major,Resolved,Fixed,10/Jan/23 15:46,31/Jan/23 17:43
Bug,IGNITE-18530,13517584,Integration test failures on Windows,"Some of the runner integration tests fail on Windows due to the hardcoded ""\n"" conversion when parsing the SQL query results. The tests are ItProjectScanMergeRuleTest, ItJoinTest and ItAggregatesTest.",vpakhnushev,vpakhnushev,Major,Resolved,Fixed,11/Jan/23 10:02,13/Feb/23 14:35
Bug,IGNITE-18532,13517617,IndexOutOfBoundsException in KafkaToIgniteCdcStreamer,"If the applier threads for some reason, then whiling joining the runner threads, join is waiting  for extra thread which actually does not exist.
{code:java}
class org.apache.ignite.IgniteException: Index 8 out of bounds for length 8
2	at org.apache.ignite.cdc.kafka.AbstractKafkaToIgniteCdcStreamer.run(AbstractKafkaToIgniteCdcStreamer.java:128)
3	at org.apache.ignite.cdc.kafka.KafkaToIgniteCdcStreamer.run(KafkaToIgniteCdcStreamer.java:71)
4	at org.apache.ignite.cdc.kafka.KafkaToIgniteCommandLineStartup.main(KafkaToIgniteCommandLineStartup.java:68)
5Caused by: java.lang.IndexOutOfBoundsException: Index 8 out of bounds for length 8
6	at java.base/jdk.internal.util.Preconditions.outOfBounds(Preconditions.java:64)
7	at java.base/jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Preconditions.java:70)
8	at java.base/jdk.internal.util.Preconditions.checkIndex(Preconditions.java:248)
9	at java.base/java.util.Objects.checkIndex(Objects.java:372)
10	at java.base/java.util.ArrayList.get(ArrayList.java:459)
11	at org.apache.ignite.cdc.kafka.AbstractKafkaToIgniteCdcStreamer.runAppliers(AbstractKafkaToIgniteCdcStreamer.java:184)
12	at org.apache.ignite.cdc.kafka.KafkaToIgniteCdcStreamer.runx(KafkaToIgniteCdcStreamer.java:110)
13	at org.apache.ignite.cdc.kafka.AbstractKafkaToIgniteCdcStreamer.run(AbstractKafkaToIgniteCdcStreamer.java:125)
14	... 2 more {code}
Problematic code is
{code:java}
try { 
      for (int i = 0; i < threadCnt + 1; i++) 
          runners.get(i).join(); 
}
{code}
runners is as array list of threads whose size is configured to specified number of threadCount.

{{runners = new ArrayList<>(streamerCfg.getThreadCount());}}

In our yaml {{threadCount: 8}}

If this is the case, then in above code, while joining threads why are we iterating for 9 threads ({{{}threadCnt + 1{}}} = 8 +1)

As a solution we need to iterate over runners with condition {{i < threadCnt}} and not {{i < threadCnt + 1}}",timonin.maksim,KB_Open,Major,Resolved,Fixed,11/Jan/23 13:12,12/Apr/23 16:18
Bug,IGNITE-18534,13517766,The WalWritingRate metric is not calculated when walMode is LOG_ONLY or BACKGROUND,"The WalWritingRate metric is not calculated when walMode is LOG_ONLY or BACKGROUND.
Reproducer  [^IoDatastorageMetricsTest.java].
Quick fix patch  [^patch.patch].
",AldoRaine,AldoRaine,Major,Resolved,Fixed,12/Jan/23 08:11,21/Apr/23 12:20
Bug,IGNITE-18540,13517847,It is necessary to process the default distribution zone like all other zones,"h3. Motivation

The default distribution zone is separated from other zones. So need to check all places where is processed zonesConfiguration.distributionZones() and add processing of zonesConfiguration.defaultDistributionZone().
h3. Definition of Done

The default distribution zone is processed like all other zones.",Sergey Uttsel,Sergey Uttsel,Major,Resolved,Fixed,12/Jan/23 13:08,18/Jan/23 13:58
Bug,IGNITE-18545,13517921,"Compute ""withXXX"" modifiers in some cases are not applied to the user compute method call.","When the ""withXXX"" IgniteCompute methods are called, we set the corresponding thread-local parameters that are applied to the first task executed (see IgniteComputeImpl#withName).
When we run the task, we clear all the thread-local settings that were set earlier.

Consider the IgniteCompute#affinityCall method.
During its execution, in some cases we run a system task to get the affinity distribution from other nodes before user task execution (see GridAffinityProcessor#affinityKey and GridAffinityProcessor#affinityInfoFromNode). As a result, the mentioned above modifiers are applied to the system task that requests the affinity distribution but not the user task.

It seems that the current approach to setting task parameters via thread-local variables is  error-prone and should be refactored.

",PetrovMikhail,PetrovMikhail,Minor,Resolved,Fixed,12/Jan/23 20:07,11/Feb/23 09:18
Bug,IGNITE-18553,13519712,"When tests are run using IDEA runner, wrong log format is used","
An example of such test is {{{}ItComputeTest{}}}. When run with Gradle Runner, I get the following in the log:

2023-01-16 11:41:06:381 +0400 [INFO][Test worker][ConnectionManager] Server started [address=/0:0:0:0:0:0:0:0:3344]

But when I run same test with IDEA runner, I get this:

янв. 16, 2023 12:01:03 PM org.apache.ignite.internal.logger.IgniteLogger logInternal
INFO: Server started [address=/0:0:0:0:0:0:0:0:3344]

The latter is wrong, so it looks like under IDEA runner our logging properties are not picked up.",Mikhail Pochatkin,rpuch,Major,Resolved,Fixed,16/Jan/23 08:03,16/Mar/23 13:45
Bug,IGNITE-18558,13519735,Fix Meta Storage prefix reads,"Meta Storage Manager prefix method implementations use {{KeyCriterion#nextKey}} method to create an upper bound of a prefix. This method is, unfortunately, incorrect, because it adds a 0 to the array, if the last byte overflows. It also uses signed arithmetic, while internal RocksDB implementation uses unsigned.",apolovtcev,apolovtcev,Major,Resolved,Fixed,16/Jan/23 09:35,16/Jan/23 14:17
Bug,IGNITE-18569,13520013,Sql. Some queries exceptionally executed if some rules are disabled.,"Found that if only one aggregate rule *HashAggregateConverterRule.COLOCATED* become available (in practice it can win by summary weights), we obtain exception, like :

concrete this test can be found in */sqlite/aggregates/agg1.test*.
There are also numerous faled tests if only one aggregate rule become enabled.

{noformat}
Error at: (agg1.test_slow:113). sql: SELECT DISTINCT + 30 AS col0 FROM tab0, tab1 AS cor0

Caused by: java.lang.IllegalStateException
	at org.apache.ignite.internal.sql.engine.trait.DistributionFunction$SingletonDistribution.destination(DistributionFunction.java:205)
	at org.apache.ignite.internal.sql.engine.trait.DistributionTrait.destination(DistributionTrait.java:109)
	at org.apache.ignite.internal.sql.engine.exec.LogicalRelImplementor.visit(LogicalRelImplementor.java:161)
{noformat}


{noformat}
            //HashAggregateConverterRule.COLOCATED,
            HashAggregateConverterRule.MAP_REDUCE,
            //SortAggregateConverterRule.COLOCATED,
            //SortAggregateConverterRule.MAP_REDUCE,
{noformat}
",zstan,zstan,Major,Resolved,Fixed,18/Jan/23 09:44,03/Feb/23 09:52
Bug,IGNITE-18584,13520171,Possible memory errors and corruptions because of insufficient size of compression buffer.,"{{org.apache.ignite.internal.processors.compress.CompressionProcessorImpl#compressBuf}} is initialized with 1024 extra space, that may be not enough for compressing.
I.e. snappy requires 2762 extra space for 16K pages.

The overhead must be calculated during the initialization with help of specialized methods, such as 

{{Snappy#maxCompressedLength}}",ivandasch,ivandasch,Major,Resolved,Fixed,19/Jan/23 10:16,20/Apr/23 13:53
Bug,IGNITE-18588,13520208,.NET: Thin 3.0: BinaryTupleReader incorrect behavior on type mismatch,"Add the following test to *BinaryTupleTests*:

{code:c#}
[Test]
public void TestShortAsByte()
{
    var bytes = Build((ref BinaryTupleBuilder b) => b.AppendShort(257));
    var reader = new BinaryTupleReader(bytes, 1);

    Assert.AreEqual(257, reader.GetByte(0));
}
{code}

The result is assertion failure ""Expected: 257  But was:  1"" - we get incorrect value, but there should be an exception when the data does not fit into the requested type.",ptupitsyn,ptupitsyn,Major,Resolved,Fixed,19/Jan/23 14:35,20/Jan/23 10:21
Bug,IGNITE-18590,13520226,Transaction recovery tries rollback a tx concurrently with committing it,"This is a bug introduced in the patch [https://github.com/apache/ignite/pull/8822.]

When a transaction commits on primary node during TxRecovery procedure, it also sends rollback request (GridDhtTxFinishRequest#commit = false) on backup nodes.

Then it's possible that such a request (commit = false) reaches a backup node earlier than GridCacheTxRecoveryResponse(commit = true).  Then primary node will commit the transaction, and backup node rollbacks it. Then cluster will be in inconsistent state.",timonin.maksim,timonin.maksim,Major,Resolved,Fixed,19/Jan/23 17:16,13/Feb/23 12:14
Bug,IGNITE-18597,13520572,RocksUtils#incrementArray does not strip tail zeroes,"Consider the following data: {{[1 FF, 2, 2 0]}}. When searching for a {{1 FF}} prefix, current implementation of {{incrementArray}} will return a {{2 0}} upper bound, which will include the {{2}} value. ",apolovtcev,apolovtcev,Minor,Resolved,Fixed,20/Jan/23 12:09,20/Jan/23 14:19
Bug,IGNITE-18598,13520828,Fix compilation in the main branch,,apolovtcev,apolovtcev,Blocker,Resolved,Fixed,20/Jan/23 15:11,20/Jan/23 15:19
Bug,IGNITE-18600,13520837,Meta Storage Watch events must be atomically persisted in the Vault,"Before IGNITE-18397 all data, that has been processed by Meta Storage Watches, was automatically persisted in the Vault. In IGNITE-18397 this feature was removed, which breaks the expected design. This feature should be re-implemented with the following guarantees:

# Only keys that match any of the registered watches should be saved to the Vault;
# Data should be saved to the Vault atomically in a single batch along with the applied revision;
# If any of the Watch Listeners throws an exception, then no data should be saved.",apolovtcev,apolovtcev,Major,Resolved,Fixed,20/Jan/23 16:37,23/Jan/23 15:11
Bug,IGNITE-18601,13520936,ItJdbcJoinsSelfTest#testJoin is flaky,"ItJdbcJoinsSelfTest#testJoin sometimes failes with the following exception:

{noformat}
java.sql.SQLException: Exception while executing query [query=CREATE TABLE PUBLIC.BLOOD_GROUP_INFO_PJ (ID INT, BLOOD_GROUP VARCHAR(64), UNIVERSAL_DONOR VARCHAR(64), PRIMARY KEY (ID));]. Error message:IGN-CMN-65535 TraceId:59081ebb-c327-440b-9ddb-90cb61568f35 IGN-CMN-65535 TraceId:59081ebb-c327-440b-9ddb-90cb61568f35 Should not be called
{noformat}

For some reason, PK configuration gets updated twice, even though the data inside the configuration updates is the same.",apolovtcev,apolovtcev,Critical,Resolved,Fixed,22/Jan/23 16:52,25/Jan/23 07:31
Bug,IGNITE-18602,13520971,ColumnMetadata.scale does not conform to Javadoc,"Javadoc says ""Returns SQL column scale or -1 if scale is not applicable for this type"" (https://github.com/apache/ignite-3/blob/316545c3a15135a6441bbcfa6725f960152408a5/modules/api/src/main/java/org/apache/ignite/sql/ColumnMetadata.java#L66), 

but implementation returns ""UNDEFINED_SCALE = Integer.MIN_VALUE"" (https://github.com/apache/ignite-3/blob/316545c3a15135a6441bbcfa6725f960152408a5/modules/api/src/main/java/org/apache/ignite/sql/ColumnMetadata.java#L30)",mzhuravkov,ptupitsyn,Major,Resolved,Fixed,23/Jan/23 10:31,03/Feb/23 11:39
Bug,IGNITE-18614,13521097,.NET: Thin 3.0: Tests fail due to incorrect data cleanup when run with same node twice,"1. Start an external node as described in DEVNOTES
2. Run .NET tests, 1st time it works
3. Rune .NET tests again, some of them fail

Some data is not cleaned up properly.",ptupitsyn,ptupitsyn,Minor,Resolved,Fixed,24/Jan/23 09:55,25/Jan/23 10:27
Bug,IGNITE-18618,13521110,.NET: Thin 3.0: Test timeout is too short,"In a log from [https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_RunNetTests/7024431?hideProblemsFromDependencies=false&hideTestsFromDependencies=false&pluginCoverage=true&expandBuildTestsSection=true] (attached to this ticket) the following can be seen:

 

[12:06:40] :  [dotnet test]   Passed TestReplaceExact [65 ms]
[12:06:42] :  [dotnet test] Jan 24, 2023 11:06:42 AM io.scalecube.cluster.membership.MembershipProtocolImpl lambda$onDeadMemberDetected$42
[12:06:42] :  [dotnet test] INFO: [default:org.apache.ignite.internal.runner.app.PlatformTestNodeRunner_2:2f32f38d-529d-4d6f-92c7-a0b05d92475e@172.120.0.8:3345] Member leaved without notification: default:org.apache.ignite.internal.runner.app.PlatformTestNodeRunner:db118c3f-14cc-4b63-a0b8-e0a2cdf3cd2e@172.120.0.8:3344... skipped ...[12:06:43] :  [dotnet test] INFO: Node added to the logical topology [node=org.apache.ignite.internal.runner.app.PlatformTestNodeRunner]
[12:06:46] :  [dotnet test] [11:06:46] [Info] [ClientFailoverSocket] Primary socket connection lost, reconnecting.... skipped ...[12:06:46] :  [dotnet test]   Failed TestContains [6 s]
[12:06:46] :  [dotnet test]   Error Message:
[12:06:46] :  [dotnet test]    System.TimeoutException : The operation has timed out.

 

Looks like a node left and returned back (a probable reason for leaving is a GC pause), but it took a few seconds because it's guided by timeouts. So it seems that the timeout of 5 seconds is too low in Dotnet tests, it should be probably set to a higher value.",ptupitsyn,rpuch,Major,Resolved,Fixed,24/Jan/23 11:18,27/Jan/23 07:52
Bug,IGNITE-18625,13521135,Sql. Code cleanup,"We have a bunch of muted tests that could be unmuted partially or fully.
Let's do it for the following mentions:
IGNITE-16952

IGNITE-16905

IGNITE-16529
IGNITE-17304
In addition, let's amend using join() method on futures in SQL test to timeboxed org.apache.ignite.internal.testframework.IgniteTestUtils#await()",amashenkov,jooger,Major,Resolved,Fixed,24/Jan/23 15:30,01/Feb/23 13:31
Bug,IGNITE-18626,13521137,.NET: Thin 3.0: LINQ tests crash in Release mode,"*dotnet test --filter Linq -c Release*


{code}
The active test run was aborted. Reason: Test host process crashed : Fatal error. System.AccessViolationException: Attempted to read or write protected memory. This is often an indication that other memory is corrupt.
   at Apache.Ignite.Internal.Sql.ResultSet`1[[System.Nullable`1[[System.Int32, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]], System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].ReadRow(System.Collections.Generic.IReadOnlyList`1<Apache.Ignite.Sql.IColumnMetadata>, Apache.Ignite.Internal.Proto.MsgPack.MsgPackReader ByRef)
   at Apache.Ignite.Internal.Sql.ResultSet`1+<>c__DisplayClass33_0+<<EnumerateRows>g__EnumeratePage|0>d[[System.Nullable`1[[System.Int32, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]], System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].MoveNext()
   at Apache.Ignite.Internal.Sql.ResultSet`1+<EnumerateRows>d__33[[System.Nullable`1[[System.Int32, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]], System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].MoveNext()
   at System.Runtime.CompilerServices.AsyncMethodBuilderCore.Start[[System.__Canon, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]](System.__Canon ByRef)
   at Apache.Ignite.Internal.Sql.ResultSet`1+<EnumerateRows>d__33[[System.Nullable`1[[System.Int32, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]], System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].System.Collections.Generic.IAsyncEnumerator<T>.MoveNextAsync()
   at Apache.Ignite.Internal.Linq.IgniteQueryExecutor+<ExecuteSingleInternalAsync>d__9`1[[System.Nullable`1[[System.Int32, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]], System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].MoveNext()
   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Nullable`1[[System.Int32, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]], System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e],[System.__Canon, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].ExecutionContextCallback(System.Object)
   at System.Threading.ExecutionContext.RunInternal(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object)
   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Nullable`1[[System.Int32, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]], System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e],[System.__Canon, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].MoveNext(System.Threading.Thread)
   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Nullable`1[[System.Int32, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]], System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e],[System.__Canon, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].MoveNext()
   at System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(System.Runtime.CompilerServices.IAsyncStateMachineBox, Boolean)
   at System.Threading.Tasks.Task.RunContinuations(System.Object)
   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1[[System.__Canon, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].SetExistingTaskResult(System.Threading.Tasks.Task`1<System.__Canon>, System.__Canon)
   at Apache.Ignite.Internal.Linq.IgniteQueryExecutor+<ExecuteResultSetInternalAsync>d__8`1[[System.Nullable`1[[System.Int32, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]], System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].MoveNext()
   at System.Threading.ExecutionContext.RunInternal(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object)
   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.__Canon, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e],[System.__Canon, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].MoveNext(System.Threading.Thread)
   at System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(System.Runtime.CompilerServices.IAsyncStateMachineBox, Boolean)
   at System.Threading.Tasks.Task.RunContinuations(System.Object)
   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1[[System.__Canon, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].SetExistingTaskResult(System.Threading.Tasks.Task`1<System.__Canon>, System.__Canon)
   at Apache.Ignite.Internal.Sql.Sql+<ExecuteAsyncInternal>d__8`1[[System.Nullable`1[[System.Int32, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]], System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].MoveNext()
   at System.Threading.ExecutionContext.RunInternal(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object)
   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.__Canon, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e],[System.__Canon, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].MoveNext(System.Threading.Thread)
   at System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(System.Runtime.CompilerServices.IAsyncStateMachineBox, Boolean)
   at System.Threading.Tasks.Task.RunContinuations(System.Object)
   at System.Threading.Tasks.Task`1[[System.ValueTuple`2[[Apache.Ignite.Internal.Buffers.PooledBuffer, Apache.Ignite, Version=3.0.0.0, Culture=neutral, PublicKeyToken=2b5ffea60c6b4a0c],[System.__Canon, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]], System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].TrySetResult(System.ValueTuple`2<Apache.Ignite.Internal.Buffers.PooledBuffer,System.__Canon>)
   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1[[System.ValueTuple`2[[Apache.Ignite.Internal.Buffers.PooledBuffer, Apache.Ignite, Version=3.0.0.0, Culture=neutral, PublicKeyToken=2b5ffea60c6b4a0c],[System.__Canon, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]], System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].SetResult(System.ValueTuple`2<Apache.Ignite.Internal.Buffers.PooledBuffer,System.__Canon>)
   at Apache.Ignite.Internal.ClientFailoverSocket+<DoOutInOpAndGetSocketAsync>d__19.MoveNext()
{code}

Seems to crash whenever *Nullable<T>* is used as result type: TestStringFunctions, TestAggregateNullable*, etc.",ptupitsyn,ptupitsyn,Critical,Resolved,Fixed,24/Jan/23 15:51,25/Jan/23 07:41
Bug,IGNITE-18627,13521138,.NET: Thin 3.0: IndexOutOfRangeException when reading empty result set with IgniteDbDataReader,"
{code}
System.IndexOutOfRangeException
  HResult=0x80131508
  Message=Index was outside the bounds of the array.
  Source=Apache.Ignite
  StackTrace:
   at Apache.Ignite.Internal.Proto.MsgPack.MsgPackReader.ReadBinaryHeader() in Apache.Ignite.Internal.Proto.MsgPack\MsgPackReader.cs:line 226
   at Apache.Ignite.Sql.IgniteDbDataReader.<FetchNextPage>g__ReadFirstRowInCurrentPage|62_0() in Apache.Ignite.Sql\IgniteDbDataReader.cs:line 536
   at Apache.Ignite.Sql.IgniteDbDataReader.<FetchNextPage>d__62.MoveNext() in Apache.Ignite.Sql\IgniteDbDataReader.cs:line 530
   at Apache.Ignite.Sql.IgniteDbDataReader.Read() in Apache.Ignite.Sql\IgniteDbDataReader.cs:line 310
   at System.Data.Common.DataAdapter.FillLoadDataRow(SchemaMapping mapping)
   at System.Data.Common.DataAdapter.FillFromReader(DataSet dataset, DataTable datatable, String srcTable, DataReaderContainer dataReader, Int32 startRecord, Int32 maxRecords, DataColumn parentChapterColumn, Object parentChapterValue)
   at System.Data.Common.DataAdapter.Fill(DataTable[] dataTables, IDataReader dataReader, Int32 startRecord, Int32 maxRecords)
   at System.Data.DataTable.Load(IDataReader reader, LoadOption loadOption, FillErrorEventHandler errorHandler)
   at System.Data.DataTable.Load(IDataReader reader)
{code}
",ptupitsyn,ptupitsyn,Critical,Resolved,Fixed,24/Jan/23 15:52,25/Jan/23 08:21
Bug,IGNITE-18631,13521257,ItSqlAsynchronousApiTest#checkMixedTransactionsForIndex is flaky,"Looks like some data updates miss the index, probable because of IGNITE-18203",apolovtcev,apolovtcev,Critical,Resolved,Fixed,25/Jan/23 10:06,25/Jan/23 13:48
Bug,IGNITE-18632,13521264,Barrier for locks after cleanup started,"*Motivation*
Transaction locks are acquired on primary replicas and must be released on tx cleanup (on processing of cleanup request to primary replica). This means, no lock for certain transaction can be acquired after the processing of cleanup request has begun. However, messaging protocol doesn't provide any linearization guarantees, so we should have this synchronization implemented in replica listener. For current implementation the following is possible:
- put request and cleanup request for the same primary replica and for the same transaction are reordered by messaging;
- cleanup starts and releases locks for corresponding tx;
- processing of put request starts and acquires locks that will never be released.

*Definition of done:*
After cleanup for certain transaction on certain primary replilca has started, it's impossible to acquire lock for this transaction on this primary replica.

*Implementation notes:*
There is synchronization between adding write intents to storage on primary replica and cleanup start (see IGNITE-18527 ). The implementation should take into account that cleanup can't start while write intentis being written into a storage, and cleanup command should not be fired before update command. However, this ticket is not only about update requests/commands, as read requests also acquire locks on primary replica, and they also need to be synchronized with cleanup.
",v.pyatkov,Denis Chudov,Major,Resolved,Fixed,25/Jan/23 10:46,16/Feb/23 09:33
Bug,IGNITE-18650,13521733,Occasional NullPointerException in TableManager.assignments,"*TestExecuteColocatedUpdatesTableCacheOnTableDrop*:

{code:java}
Apache.Ignite.IgniteException : Exception of type 'Apache.Ignite.IgniteException' was thrown.
  ----> Apache.Ignite.IgniteException : java.lang.NullPointerException
  at org.apache.ignite.internal.table.distributed.TableManager.assignments(TableManager.java:1051)
  at org.apache.ignite.client.handler.requests.table.ClientTablePartitionAssignmentGetRequest.process(ClientTablePartitionAssignmentGetRequest.java:47)
  at org.apache.ignite.client.handler.ClientInboundMessageHandler.processOperation(ClientInboundMessageHandler.java:493)
  at org.apache.ignite.client.handler.ClientInboundMessageHandler.processOperation(ClientInboundMessageHandler.java:345)
  at org.apache.ignite.client.handler.ClientInboundMessageHandler.channelRead(ClientInboundMessageHandler.java:195)
  at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
  at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
  at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
  at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:336)
  at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:308)
  at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
  at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
  at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
  at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
  at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
  at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
  at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
  at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
  at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
  at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
  at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
  at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
  at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
  at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
  at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
  at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
  at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
  at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
  at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
  at java.base/java.lang.Thread.run(Thread.java:834)
   at Apache.Ignite.Internal.ClientFailoverSocket.DoOutInOpAndGetSocketAsync(ClientOp clientOp, Transaction tx, PooledArrayBuffer request, PreferredNode preferredNode) in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite/Internal/ClientFailoverSocket.cs:line 203
   at Apache.Ignite.Internal.ClientFailoverSocket.DoOutInOpAsync(ClientOp clientOp, PooledArrayBuffer request, PreferredNode preferredNode) in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite/Internal/ClientFailoverSocket.cs:line 163
   at Apache.Ignite.Internal.Table.Table.LoadPartitionAssignmentAsync() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite/Internal/Table/Table.cs:line 353
   at Apache.Ignite.Internal.Table.Table.GetPartitionAssignmentAsync() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite/Internal/Table/Table.cs:line 226
   at Apache.Ignite.Internal.Table.Table.GetPreferredNode(Int32 colocationHash, ITransaction transaction) in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite/Internal/Table/Table.cs:line 196
   at Apache.Ignite.Internal.Compute.Compute.ExecuteColocatedAsync[T,TKey](String tableName, TKey key, Func`2 serializerHandlerFunc, String jobClassName, Object[] args)
   at Apache.Ignite.Internal.Compute.Compute.ExecuteColocatedAsync[T](String tableName, IIgniteTuple key, String jobClassName, Object[] args)
   at Apache.Ignite.Tests.Compute.ComputeTests.TestExecuteColocatedUpdatesTableCacheOnTableDrop() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/Compute/ComputeTests.cs:line 281
   at Apache.Ignite.Tests.Compute.ComputeTests.TestExecuteColocatedUpdatesTableCacheOnTableDrop() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/Compute/ComputeTests.cs:line 288
{code}

Looks like we don't handle dropped table correctly.

https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_RunNetTests/7035872?hideProblemsFromDependencies=false&hideTestsFromDependencies=false&expandBuildTestsSection=true",ptupitsyn,ptupitsyn,Major,Resolved,Fixed,27/Jan/23 13:32,02/Feb/23 15:44
Bug,IGNITE-18652,13521934,CdcMain is not committing state during index rebuilding,"For persistent cluster CdcMain is not committing state during index rebuilding if there are no data updates.
It can cause to sharp increase of cdc WAL-archive size in case of big indexes.

Here is a reproducer patch:  [^CdcRebuildTest.patch] ",nizhikov,shishkovilja,Major,Resolved,Fixed,29/Jan/23 16:15,24/Apr/23 14:58
Bug,IGNITE-18679,13522220,Fix org.apache.ignite.internal.pagememory.persistence.checkpoint.CheckpointTimeoutLockTest#testCheckpointReadLock,It was found that the wrong intervals were chosen in *org.apache.ignite.internal.pagememory.persistence.checkpoint.CheckpointTimeoutLockTest#testCheckpointReadLock* which could lead to flaky tests.,ktkalenko@gridgain.com,ktkalenko@gridgain.com,Major,Resolved,Fixed,31/Jan/23 12:24,31/Jan/23 13:15
Bug,IGNITE-18680,13522225,Fix ClientArchTest source provider,"org.apache.ignite.client.ClientArchTest take sources for analize from ignite-client.jar file from build/lib directory. This jar contains class files only from client module. However, in a normal build pipeline, the tests are run before the jar files are built and Gradle test task doesn't depend on jar task. As result, we have false failed test. 
So, us solution we can change location provider org.apache.ignite.client.ClientArchTest.ClassesWithLibsLocationProvider and took *.java files for analizing insted of class files.",Mikhail Pochatkin,Mikhail Pochatkin,Major,Resolved,Fixed,31/Jan/23 13:07,31/Jan/23 13:50
Bug,IGNITE-18684,13522411,Fix busyLock.leaveBusy() in IndexManager#onIndexDrop,"An incorrect use of *busyLock.leaveBusy()* in *IndexManager#onIndexDrop* was detected, we must call method *busyLock.leaveBusy()* on the same thread.",ktkalenko@gridgain.com,ktkalenko@gridgain.com,Major,Resolved,Fixed,01/Feb/23 07:10,01/Feb/23 07:53
Bug,IGNITE-18686,13522430,GradualTaskExecutorTest.nonFinishedTasksAreCancelledWhenExecutorIsClosed is flaky,https://ggtc.gridgain.com/buildConfiguration/GridGain9Gradle_Test_UnitTests_PageMemory/8624795?showRootCauses=false&expandBuildChangesSection=true&expandBuildProblemsSection=true&expandBuildTestsSection=true,rpuch,rpuch,Major,Resolved,Fixed,01/Feb/23 08:22,01/Feb/23 09:25
Bug,IGNITE-18690,13522528,Assertion fires on snapshot partition sending with paired connections.,"An assertion in 
{code:java}
ClientConnectionPool.reserveClient(ClusterNode node, int connIdx)
{code}
can raise on snapshot restoring when set 
{code:java}
TcpCommunicationSpi.setUsePairedConnections(true)
{code}
 and snapshot manager sends partition files:
{code:java}
java.lang.AssertionError: 1026
	at org.apache.ignite.spi.communication.tcp.internal.ConnectionClientPool.reserveClient(ConnectionClientPool.java:190)
	at org.apache.ignite.spi.communication.tcp.internal.CommunicationWorker.processDisconnect(CommunicationWorker.java:376)
	at org.apache.ignite.spi.communication.tcp.internal.CommunicationWorker.body(CommunicationWorker.java:174)
	at org.apache.ignite.internal.util.worker.GridWorker.run(GridWorker.java:125)
	at org.apache.ignite.spi.communication.tcp.TcpCommunicationSpi$3.body(TcpCommunicationSpi.java:848)
	at org.apache.ignite.spi.IgniteSpiThread.run(IgniteSpiThread.java:58)
{code}

Test attached.

If a node failure handler is set and '-ea' JVM option is used, nodes can fail on such snapshot restoration.




",vladsz83,vladsz83,Major,Resolved,Fixed,01/Feb/23 14:31,11/Feb/23 08:12
Bug,IGNITE-18692,13522535,Rebalance test is failed,"org.apache.ignite.internal.rebalance.ItRebalanceTest#assignmentsChangingOnNodeLeaveNodeJoin failed.

The failure is caused by commits:

db8f1e38 ""IGNITE-18397 Rework Watches based on Raft Learners (#1490)""

ff27d76d ""IGNITE-18598 Fix compilation after merge (#1560)""

I created separated branch with this test: [https://github.com/gridgain/apache-ignite-3/tree/ignite-18088_test] which based on ff27d76d ""IGNITE-18598 Fix compilation after merge (#1560)""

 
{code:java}
org.opentest4j.AssertionFailedError: expected: <true> but was: <false>
    at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55)
    at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:40)
    at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:35)
    at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:179)
    at app//org.apache.ignite.internal.rebalance.ItRebalanceTest.assignmentsChangingOnNodeLeaveNodeJoin(ItRebalanceTest.java:132) {code}",kgusakov,Sergey Uttsel,Major,Resolved,Fixed,01/Feb/23 15:45,13/Jun/23 12:21
Bug,IGNITE-18701,13522755,Incorrect binary tuple column mapping,org.apache.ignite.internal.schema.BinaryTupleSchema.DenseRowSchema#columnIndex incorrectly subtracts the base index from the element index while it should add them.,vpakhnushev,vpakhnushev,Major,Resolved,Fixed,02/Feb/23 09:59,08/Feb/23 10:41
Bug,IGNITE-18713,13523187,"Compute ""withXXX"" modifiers are not applied if they came before withExecutor modifier.","After IGNITE-18545 the following code 

{code:java}
IgniteCompute compute = grid(0).compute().withTimeout(1000).withExecutor(""executor"").execute(<task>);
{code}

will result in task execution without timeout modifier applied.
",PetrovMikhail,PetrovMikhail,Major,Resolved,Fixed,06/Feb/23 08:21,11/Feb/23 09:18
Bug,IGNITE-18714,13523188,Not thread safe usage of InternalTableImpl#partitionMap,"h3. Motivation

The mutex is used to update the partitionMap, but it is not used when reading from the partitionMap
h3. Definition of Done

Thread-safe usage of InternalTableImpl#partitionMap",Sergey Uttsel,Sergey Uttsel,Major,Resolved,Fixed,06/Feb/23 08:34,09/Feb/23 16:51
Bug,IGNITE-18717,13523204,Fix parallel read and delete RowVersion for PageMemoryMvPartitionStorage,"It was found that we do not have protection against parallel deletion and read RowVersion for PageMemoryMvPartitionStorage. 

See the epic for a little more details.

For a start, you can see the problems if you enable tests:
* *org.apache.ignite.internal.storage.pagememory.mv.PersistentPageMemoryMvPartitionStorageConcurrencyTest*
* *org.apache.ignite.internal.storage.pagememory.mv.VolatilePageMemoryMvPartitionStorageConcurrencyTest*",ktkalenko@gridgain.com,ktkalenko@gridgain.com,Major,Resolved,Fixed,06/Feb/23 09:59,14/Feb/23 14:53
Bug,IGNITE-18723,13523249,ClientArchTest should check client dependencies,ClientArchTest should check not only client source code but also all depended Ignite modules.,aleksandr.pakhomov,Mikhail Pochatkin,Major,Resolved,Fixed,06/Feb/23 15:06,03/Mar/23 07:14
Bug,IGNITE-18724,13523256,Fix broken JavaDoc because of incorrect CFG_VIEW link,https://ci.ignite.apache.org/viewLog.html?buildId=7054874&tab=buildResultsDiv&buildTypeId=IgniteTests24Java8_Javadoc,PetrovMikhail,PetrovMikhail,Minor,Resolved,Fixed,06/Feb/23 15:17,06/Feb/23 15:48
Bug,IGNITE-18725,13523259,IndexQuery skips critertia after checking first IN clause,"[https://github.com/apache/ignite/issues/10515]

If a IN criterion for field F is specified in IndexQuery, it checks all criteria until F and skips all following criteria.

Note, that this bug affects queries where a IN criterion applies to not first index field:

1. ""eq"" + ""in"" - works
2. ""in"" + ""eq""  - works
3. ""eq"" + ""in"" + ""eq"" - doesn't work.

 ",timonin.maksim,timonin.maksim,Major,Resolved,Fixed,06/Feb/23 15:34,21/Apr/23 12:28
Bug,IGNITE-18737,13523479,Tests from ItIgniteNodeRestartTest  became flaky  ,"h3. *Motivation*

After https://issues.apache.org/jira/browse/IGNITE-18088 test from ItIgniteNodeRestartTest started to fail. Need to investigate the reason of fails
{noformat}
Caused by: java.util.concurrent.CancellationException
	at java.base/java.util.concurrent.CompletableFuture.cancel(CompletableFuture.java:2396)
	at org.apache.ignite.internal.raft.RaftGroupServiceImpl.sendWithRetry(RaftGroupServiceImpl.java:491)
	at org.apache.ignite.internal.raft.RaftGroupServiceImpl.sendWithRetry(RaftGroupServiceImpl.java:473)
	at org.apache.ignite.internal.raft.RaftGroupServiceImpl.refreshAndGetLeaderWithTerm(RaftGroupServiceImpl.java:232)
	at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.ensureReplicaIsPrimary(PartitionReplicaListener.java:1880)
	at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.invoke(PartitionReplicaListener.java:275)
	at org.apache.ignite.internal.replicator.Replica.processRequest(Replica.java:61)
	at org.apache.ignite.internal.replicator.ReplicaManager.lambda$new$5(ReplicaManager.java:162)
	at org.apache.ignite.network.DefaultMessagingService.sendToSelf(DefaultMessagingService.java:279)
	at org.apache.ignite.network.DefaultMessagingService.invoke0(DefaultMessagingService.java:226)
	at org.apache.ignite.network.DefaultMessagingService.invoke(DefaultMessagingService.java:154)
	at org.apache.ignite.internal.replicator.ReplicaService.sendToReplica(ReplicaService.java:91)
	at org.apache.ignite.internal.replicator.ReplicaService.invoke(ReplicaService.java:178)
	at org.apache.ignite.internal.tx.impl.TxManagerImpl.cleanup(TxManagerImpl.java:150)
	at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.lambda$processTxFinishAction$42(PartitionReplicaListener.java:984)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1072)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
	at org.apache.ignite.internal.raft.RaftGroupServiceImpl.lambda$sendWithRetry$38(RaftGroupServiceImpl.java:526)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:479)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183){noformat}
The root cause is that distribution zones data nodes watch listener in TableManager invokes updatePendingAssignmentsKeys with the same data nodes as in table configuration assignments so RaftGroupServiceImpl stops and new the same RaftGroupServiceImpl starts. It cancels invokes to old RaftGroupServiceImpl so we have CancellationException at sendWithRetry.
h3. *Implementation Notes*

It is not need to change the assignments if the calculated assignments equals to the assignments in the table configuration.",Sergey Uttsel,maliev,Major,Resolved,Fixed,07/Feb/23 11:10,23/Feb/23 09:40
Bug,IGNITE-18740,13523498,ItLearnersTest Raft retry timeout is too small,ItLearnersTest#testChangePeersToAddLearnerToSameNodeAsPeer has become flaky (it fails with a TimeoutException). This is mostly likely caused by a custom Raft retry timeout value in the configuration (which was changed to speed up some tests). This timeout should be changed back to the default value.,apolovtcev,apolovtcev,Blocker,Resolved,Fixed,07/Feb/23 12:22,07/Feb/23 13:26
Bug,IGNITE-18746,13523511,Sql. Some muted slow tests failed after IGNITE-13022,"orderby1_10_7.test_slow failed after IGNITE-13022
it can be fixed by porting: https://issues.apache.org/jira/browse/IGNITE-17889
Probably in scope of this issue it would be helpful to fix code duplication :
RexUtils#buildHashSearchBounds(org.apache.calcite.plan.RelOptCluster, org.apache.calcite.rex.RexNode, org.apache.calcite.rel.type.RelDataType, org.apache.calcite.util.ImmutableBitSet)
RexUtils#buildHashSearchBounds(org.apache.calcite.plan.RelOptCluster, org.apache.calcite.rel.RelCollation, org.apache.calcite.rex.RexNode, org.apache.calcite.rel.type.RelDataType, org.apache.calcite.util.ImmutableBitSet)",zstan,zstan,Major,Resolved,Fixed,07/Feb/23 13:42,07/Mar/23 07:04
Bug,IGNITE-18750,13523536,.NET: Compute modificators are not reset properly after task execution.,"Consider the followig exampte (It can be placed in ComputeApiTest to run it)
{code:java}
[Test]
        public void TestTaskOptionsPropagation()
        {
            ICompute compute = _grid1.GetCompute();
            
            compute.WithTimeout(500).Call(new ComputeFunc());

            compute.ExecuteJavaTask<object>(ComputeClientTests.TestTask, (long)1000);
        }
{code}
Currently compute.ExecuteJavaTask call will fail with timeout exception even no ""withTimeout"" modifier was applied.

It happens because during PlatformCompute#processInLongOutLong execution we apply task modifiers to both compute instances. As a result after
`compute.WithTimeout(500).Call(new ComputeFunc());` execution
`computeForPlatform` compute instance will reset its task modifiers and `compute`
comput instance will not.

So the next task execution after `compute.WithTimeout(500).Call(new ComputeFunc());` that uses `compute`instance will be executed with timout modifier applied even it was not applied by the user.",PetrovMikhail,PetrovMikhail,Major,Resolved,Fixed,07/Feb/23 16:56,13/Feb/23 12:36
Bug,IGNITE-18753,13523712,Sql. Conversion to relational algebra failed to preserve datatypes.,"MRE:

{code:java}
@Test
public void testThis() {
    assertQuery(""SELECT ?::INTEGER = '8'"")
            .withParams(8)
            .returns(true)
            .check();
}
{code}

Output:
{code:java}
Conversion to relational algebra failed to preserve datatypes:
validated type:
RecordType(BOOLEAN NOT NULL EXPR$0) NOT NULL
converted type:
RecordType(BOOLEAN EXPR$0) NOT NULL
rel:
LogicalProject(EXPR$0=[=(?0, 8)])
  LogicalValues(tuples=[[{ 0 }]])

	at org.apache.calcite.sql2rel.SqlToRelConverter.checkConvertedType(SqlToRelConverter.java:492)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:607)
	at org.apache.ignite.internal.sql.engine.prepare.IgnitePlanner.rel(IgnitePlanner.java:222)
	at org.apache.ignite.internal.sql.engine.prepare.PlannerHelper.optimize(PlannerHelper.java:63)
	at org.apache.ignite.internal.sql.engine.prepare.PrepareServiceImpl.lambda$prepareExplain$0(PrepareServiceImpl.java:208)
{code}
It seems that nullability attribute is being lost somewhere at the validation stage.

The error does not depend on a type of a dynamic parameter because the query below also trigger the same assertion:

{code:java}
@Test
public void oops() {
    assertQuery(""SELECT ?::VARCHAR = '8'"")
            .withParams(""8"")
            .returns(true)
            .check();
}
{code}

P.S.
From the first glance it may look like it is related to dynamic parameters. This error is present in main branch.
Then I switched to commit that introduces IGNITE-18282: Illegal use of dynamic parameter exception in SQL functions (#1418) (November)  and commented out IgniteSqlValidator::inferUnknownTypes. And I still got this error.
",zstan,mzhuravkov,Major,Resolved,Fixed,08/Feb/23 06:26,10/Feb/23 10:23
Bug,IGNITE-18754,13523713,Sql. Jdbc. ItJdbcResultSetSelfTest#testTimestamp fails locally but works on CI,"{code:java}
2023-02-02 20:06:47:409 +0400 [INFO][Test worker][ItJdbcResultSetSelfTest] >>> Stopping test: ItJdbcResultSetSelfTest#testTimestamp, displayName: testTimestamp(), cost: 58ms.

expected: <-10800000> but was: <-14400000>
Expected :-10800000
Actual   :-14400000

at app//org.apache.ignite.jdbc.ItJdbcResultSetSelfTest.testTimestamp(ItJdbcResultSetSelfTest.java:551)
{code}
",apolovtcev,mzhuravkov,Minor,Resolved,Fixed,08/Feb/23 06:30,03/Mar/23 08:58
Bug,IGNITE-18761,13523757,Sql. TypeOf can short-circuit only when its argument is a constant expression.,"The current implement of typeof simply checks the type its operand but this is not correct because the operand maybe an invalid expression or a function that produces a side effect.
MRE:

{code:java}
assertThrows(RuntimeException.class, () -> assertQuery(""SELECT TYPEOF(CAST('NOT_A_NUMBER' AS INTEGER))"").check());
{code}
This query should return an error but it returns a string INTEGER.

",mzhuravkov,mzhuravkov,Major,Resolved,Fixed,08/Feb/23 11:34,03/May/23 10:49
Bug,IGNITE-18764,13523781,Cluster snapshot doesn't restore marshaller data,"Case:
 # There is a cache - IgniteCache<Integer, Account>
 # cache.put(0, new BetterAccount()), where BetterAccount extends Account
 # createSnapshot()
 # restoreSnapshot on clean environment
 # cache.get(0) throws java.lang.ClassNotFoundException: Failed to resolve class name [platformId=0, platform=Java, typeId=-68243360]

Reason  that Ignite restores binary meta but misses marshaller data.",timonin.maksim,timonin.maksim,Major,Resolved,Fixed,08/Feb/23 13:58,13/Feb/23 10:57
Bug,IGNITE-18766,13523880,Incorrect id check in ClusterGroupAdapter.forNodeId,"ClusterGroup.forNodeId checks *id* in a loop instead of *id0*:

{code:java}
                for (UUID id0 : ids) {
                    if (contains(id))
                        nodeIds.add(id0);
                }
{code}

https://github.com/apache/ignite/blob/3de1dc44f53ea510328badf6cb6b423fe6975bd8/modules/core/src/main/java/org/apache/ignite/internal/cluster/ClusterGroupAdapter.java#L461

The following unit test demonstrates the problem:


{code:java}
    @Test
    public void testProjectionWithBadId() {
        ClusterNode locNode = ignite.cluster().localNode();

        ClusterGroup prj = ignite.cluster().forNodeId(UUID.randomUUID(), locNode.id());
        Collection<ClusterNode> nodes = prj.nodes();

        assertEquals(1, nodes.size());
    }
{code}

(add to GridProjectionForCachesSelfTest)",brat_kuzma,ptupitsyn,Minor,Resolved,Fixed,09/Feb/23 06:16,14/Apr/23 07:54
Bug,IGNITE-18768,13523975,Incorrect safe time initialization,"There is following code in TableManager, which initializes safe time tracker:
{code:java}
PendingComparableValuesTracker<HybridTimestamp> safeTime = new PendingComparableValuesTracker<>(clock.now());
{code}
This means means that safe time is at least greater than now, whic is not correct. It should be initialized with 0.",maliev,Denis Chudov,Blocker,Resolved,Fixed,09/Feb/23 13:15,21/Feb/23 21:03
Bug,IGNITE-18769,13523994,Test CacheMetricsForClusterGroupSelfTest.testMetricsStatisticsEnabled fails,"Test {{CacheMetricsForClusterGroupSelfTest.testMetricsStatisticsEnabled}} fails due to two reasons:
 # Last started node assumed to be non affinity node (it was daemon node, but become regular server node after IGNITE-18304)
 # Sometimes intermediate metrics state is cought by metrics snapshot ",alex_pl,alex_pl,Major,Resolved,Fixed,09/Feb/23 15:17,10/Feb/23 08:04
Bug,IGNITE-18771,13524010,Fix compilation in main,,ibessonov,ibessonov,Major,Resolved,Fixed,09/Feb/23 16:10,09/Feb/23 16:17
Bug,IGNITE-18774,13524214,PartitionReplicaListenerTest#testReadOnlyGetAllAfterRowRewrite hangs forever,https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_RunUnitTests/7069824?hideProblemsFromDependencies=false&hideTestsFromDependencies=false&expandBuildProblemsSection=true&expandBuildChangesSection=true,Denis Chudov,Denis Chudov,Blocker,Resolved,Fixed,10/Feb/23 11:44,14/Feb/23 10:38
Bug,IGNITE-18783,13524498,DistributedProcess hangs if a single future completes with AssertionError,"DistributedProcess#sendSingleMessage tries to cast local process future error to Exception, while it might completed with AssertionError.

In a such case SingleMessage isn't sent, and originated node hangs while waiting response from the node.",timonin.maksim,timonin.maksim,Major,Resolved,Fixed,13/Feb/23 16:05,16/Feb/23 08:44
Bug,IGNITE-18786,13524601,Sql. TPC-H query#6: Failed to parse query: BETWEEN operator has no terminating AND,"Sql parser is unable to parse TPC-H query#6.
Error:
{code:java}
org.apache.ignite.sql.SqlException: IGN-SQL-3 TraceId:f9be2841-ea95-4995-bde3-27742e8642dd Failed to parse query: BETWEEN operator has no terminating AND
	at org.apache.ignite.internal.util.ExceptionUtils.lambda$withCauseAndCode$3(ExceptionUtils.java:408)
	at org.apache.ignite.internal.util.ExceptionUtils.withCauseInternal(ExceptionUtils.java:435)
	at org.apache.ignite.internal.util.ExceptionUtils.withCauseAndCode(ExceptionUtils.java:408)
	at org.apache.ignite.internal.sql.engine.util.Commons.parse(Commons.java:751)
	at org.apache.ignite.internal.sql.engine.framework.TestNode.prepare(TestNode.java:190)
	at org.apache.ignite.internal.sql.engine.benchmarks.TpcTest$RunQuery.executeQuery(TpcTest.java:154)
	at org.apache.ignite.internal.sql.engine.benchmarks.TpcTest$RunQuery.execute(TpcTest.java:141)
	at org.junit.jupiter.engine.descriptor.DynamicTestTestDescriptor.lambda$execute$0(DynamicTestTestDescriptor.java:53)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.api.extension.InvocationInterceptor.interceptDynamicTest(InvocationInterceptor.java:167)
	at org.junit.jupiter.api.extension.InvocationInterceptor.interceptDynamicTest(InvocationInterceptor.java:184)
	at org.junit.jupiter.engine.descriptor.DynamicTestTestDescriptor.lambda$execute$1(DynamicTestTestDescriptor.java:61)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptorCall.lambda$ofVoid$0(InvocationInterceptorChain.java:78)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.descriptor.DynamicTestTestDescriptor.execute(DynamicTestTestDescriptor.java:60)
	at org.junit.jupiter.engine.descriptor.DynamicTestTestDescriptor.execute(DynamicTestTestDescriptor.java:32)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
	at java.base/java.util.Optional.ifPresent(Optional.java:183)
	at org.junit.jupiter.engine.descriptor.TestFactoryTestDescriptor.lambda$invokeTestMethod$1(TestFactoryTestDescriptor.java:108)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestFactoryTestDescriptor.invokeTestMethod(TestFactoryTestDescriptor.java:95)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:99)
	at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:79)
	at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:75)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy5.stop(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:193)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)
	at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:133)
	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:71)
	at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)
	at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)
Caused by: org.apache.calcite.sql.parser.SqlParseException: BETWEEN operator has no terminating AND
	at org.apache.ignite.internal.generated.query.calcite.sql.IgniteSqlParserImpl.convertException(IgniteSqlParserImpl.java:422)
	at org.apache.ignite.internal.generated.query.calcite.sql.IgniteSqlParserImpl.normalizeException(IgniteSqlParserImpl.java:175)
	at org.apache.calcite.sql.parser.SqlParser.handleException(SqlParser.java:156)
	at org.apache.calcite.sql.parser.SqlParser.parseStmtList(SqlParser.java:211)
	at org.apache.ignite.internal.sql.engine.util.Commons.parse(Commons.java:770)
	at org.apache.ignite.internal.sql.engine.util.Commons.parse(Commons.java:749)
	... 94 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: BETWEEN operator has no terminating AND
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:505)
	at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:599)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:932)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:917)
	at org.apache.calcite.sql.fun.SqlBetweenOperator.reduceExpr(SqlBetweenOperator.java:219)
	at org.apache.calcite.sql.parser.SqlParserUtil$OldTokenSequenceImpl.lambda$parser$0(SqlParserUtil.java:906)
	at org.apache.calcite.util.PrecedenceClimbingParser.partialParse(PrecedenceClimbingParser.java:121)
	at org.apache.calcite.sql.parser.SqlParserUtil.toTreeEx(SqlParserUtil.java:668)
	at org.apache.calcite.sql.parser.SqlParserUtil.toTree(SqlParserUtil.java:631)
	at org.apache.ignite.internal.generated.query.calcite.sql.IgniteSqlParserImpl.Expression(IgniteSqlParserImpl.java:4452)
	at org.apache.ignite.internal.generated.query.calcite.sql.IgniteSqlParserImpl.Where(IgniteSqlParserImpl.java:3319)
	at org.apache.ignite.internal.generated.query.calcite.sql.IgniteSqlParserImpl.SqlSelect(IgniteSqlParserImpl.java:1974)
	at org.apache.ignite.internal.generated.query.calcite.sql.IgniteSqlParserImpl.LeafQuery(IgniteSqlParserImpl.java:719)
	at org.apache.ignite.internal.generated.query.calcite.sql.IgniteSqlParserImpl.LeafQueryOrExpr(IgniteSqlParserImpl.java:4430)
	at org.apache.ignite.internal.generated.query.calcite.sql.IgniteSqlParserImpl.QueryOrExpr(IgniteSqlParserImpl.java:4296)
	at org.apache.ignite.internal.generated.query.calcite.sql.IgniteSqlParserImpl.OrderedQueryOrExpr(IgniteSqlParserImpl.java:576)
	at org.apache.ignite.internal.generated.query.calcite.sql.IgniteSqlParserImpl.SqlStmt(IgniteSqlParserImpl.java:1066)
	at org.apache.ignite.internal.generated.query.calcite.sql.IgniteSqlParserImpl.SqlStmtList(IgniteSqlParserImpl.java:1026)
	at org.apache.ignite.internal.generated.query.calcite.sql.IgniteSqlParserImpl.parseSqlStmtList(IgniteSqlParserImpl.java:231)
{code}
",mzhuravkov,mzhuravkov,Minor,Resolved,Fixed,14/Feb/23 07:00,27/Feb/23 10:34
Bug,IGNITE-18787,13524603,Sql. TPC-H query#22:  Add SUBSTR alias for SUBSTRING function ,"SqlValidator rejects TPC-H query#22 with an error that a function does not exist.

{code:java}
Failed to prepare a query
org.apache.calcite.runtime.CalciteContextException: From line 15, column 18 to line 15, column 38: No match found for function signature SUBSTR(<CHARACTER>, <NUMERIC>, <NUMERIC>)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:505)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:932)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:917)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5362)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1955)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:326)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:231)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:6373)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:6360)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:161)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1869)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1854)
	at org.apache.calcite.sql.type.SqlTypeUtil.deriveType(SqlTypeUtil.java:200)
	at org.apache.calcite.sql.type.InferTypes.lambda$static$0(InferTypes.java:47)
	at org.apache.ignite.internal.sql.engine.prepare.IgniteSqlValidator.inferUnknownTypes(IgniteSqlValidator.java:541)
	at org.apache.ignite.internal.sql.engine.prepare.IgniteSqlValidator.inferUnknownTypes(IgniteSqlValidator.java:564)
	at org.apache.ignite.internal.sql.engine.prepare.IgniteSqlValidator.inferUnknownTypes(IgniteSqlValidator.java:564)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateWhereOrOn(SqlValidatorImpl.java:4422)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateWhereClause(SqlValidatorImpl.java:4414)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3700)
	at org.apache.ignite.internal.sql.engine.prepare.IgniteSqlValidator.validateSelect(IgniteSqlValidator.java:251)
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:64)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:89)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1107)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1078)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3381)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3360)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3697)
	at org.apache.ignite.internal.sql.engine.prepare.IgniteSqlValidator.validateSelect(IgniteSqlValidator.java:251)
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:64)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:89)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1107)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1078)
	at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:248)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1053)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:759)
	at org.apache.ignite.internal.sql.engine.prepare.IgniteSqlValidator.validate(IgniteSqlValidator.java:136)
	at org.apache.ignite.internal.sql.engine.prepare.IgnitePlanner.validateAndGetTypeMetadata(IgnitePlanner.java:210)
	at org.apache.ignite.internal.sql.engine.prepare.PrepareServiceImpl.lambda$prepareQuery$3(PrepareServiceImpl.java:234)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
	Suppressed: java.lang.RuntimeException: This is a trimmed root
		at org.apache.ignite.internal.testframework.IgniteTestUtils.await(IgniteTestUtils.java:752)
		at org.apache.ignite.internal.sql.engine.framework.TestNode.prepare(TestNode.java:194)
		at org.apache.ignite.internal.sql.engine.benchmarks.TpcTest$RunQuery.executeQuery(TpcTest.java:154)
		at org.apache.ignite.internal.sql.engine.benchmarks.TpcTest$RunQuery.execute(TpcTest.java:141)
		at org.junit.jupiter.engine.descriptor.DynamicTestTestDescriptor.lambda$execute$0(DynamicTestTestDescriptor.java:53)
		at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
		at org.junit.jupiter.api.extension.InvocationInterceptor.interceptDynamicTest(InvocationInterceptor.java:167)
		at org.junit.jupiter.api.extension.InvocationInterceptor.interceptDynamicTest(InvocationInterceptor.java:184)
		at org.junit.jupiter.engine.descriptor.DynamicTestTestDescriptor.lambda$execute$1(DynamicTestTestDescriptor.java:61)
		at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptorCall.lambda$ofVoid$0(InvocationInterceptorChain.java:78)
		at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
		at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
		at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
		at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
		at org.junit.jupiter.engine.descriptor.DynamicTestTestDescriptor.execute(DynamicTestTestDescriptor.java:60)
		at org.junit.jupiter.engine.descriptor.DynamicTestTestDescriptor.execute(DynamicTestTestDescriptor.java:32)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
		at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
		at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
		at java.base/java.util.Optional.ifPresent(Optional.java:183)
		at org.junit.jupiter.engine.descriptor.TestFactoryTestDescriptor.lambda$invokeTestMethod$1(TestFactoryTestDescriptor.java:108)
		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
		at org.junit.jupiter.engine.descriptor.TestFactoryTestDescriptor.invokeTestMethod(TestFactoryTestDescriptor.java:95)
		at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
		at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
		at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
		at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
		at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
		at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
		at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
		at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
		at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
		at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
		at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
		at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
		at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
		at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
		at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
		at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
		at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
		at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
		at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
		at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
		at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
		at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
		at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:99)
		at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:79)
		at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:75)
		at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:566)
		at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
		at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
		at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
		at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
		at com.sun.proxy.$Proxy5.stop(Unknown Source)
		at org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:193)
		at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)
		at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)
		at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)
		at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
		at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:133)
		at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:71)
{code}
",mzhuravkov,mzhuravkov,Major,Resolved,Fixed,14/Feb/23 07:04,01/Mar/23 13:05
Bug,IGNITE-18788,13524608,Query listener in python thin client fire events only if a debug logging level enabled,,ivandasch,ivandasch,Minor,Resolved,Fixed,14/Feb/23 07:42,16/Feb/23 11:36
Bug,IGNITE-18795,13524647,ItClusterManagerTest#testNodeRestart is flaky,"This test fails sometimes with the following error:

{noformat}
java.lang.AssertionError: 
Expected: is iterable with items [<ClusterNode [id=65b15c06-2a55-4718-ac16-033a4e12775c, name=icmt_tnr_10000, address=192.168.10.11:10000, nodeMetadata=null]>, <ClusterNode [id=a2386ee4-1190-4160-bda2-004d469c282b, name=icmt_tnr_10001, address=192.168.10.11:10001, nodeMetadata=null]>] in any order
     but: was <[ClusterNode [id=65b15c06-2a55-4718-ac16-033a4e12775c, name=icmt_tnr_10000, address=192.168.10.11:10000, nodeMetadata=null]]>
{noformat}
",apolovtcev,apolovtcev,Blocker,Resolved,Fixed,14/Feb/23 10:25,15/Feb/23 15:26
Bug,IGNITE-18803,13524686,Fix compilation error of ignite-hibernate-ext,"Build fails because {{CacheMetricsMXBean}} was deleted from Apache Ignite:
{noformat}
/opt/buildagent/work/9319dd66c384518/modules/hibernate-ext/hibernate/src/main/java/org/apache/ignite/cache/hibernate/HibernateCacheProxy.java:[44,32] cannot find symbol
  symbol:   class CacheMetricsMXBean
  location: package org.apache.ignite.mxbean
/opt/buildagent/work/9319dd66c384518/modules/hibernate-ext/hibernate/src/main/java/org/apache/ignite/cache/hibernate/HibernateCacheProxy.java:[549,22] cannot find symbol
  symbol:   class CacheMetricsMXBean
  location: class org.apache.ignite.cache.hibernate.HibernateCacheProxy
/opt/buildagent/work/9319dd66c384518/modules/hibernate-ext/hibernate/src/main/java/org/apache/ignite/cache/hibernate/HibernateCacheProxy.java:[554,22] cannot find symbol
  symbol:   class CacheMetricsMXBean
  location: class org.apache.ignite.cache.hibernate.HibernateCacheProxy
/opt/buildagent/work/9319dd66c384518/modules/hibernate-ext/hibernate/src/main/java/org/apache/ignite/cache/hibernate/HibernateCacheProxy.java:[549,5] method does not override or implement a method from a supertype
/opt/buildagent/work/9319dd66c384518/modules/hibernate-ext/hibernate/src/main/java/org/apache/ignite/cache/hibernate/HibernateCacheProxy.java:[550,30] cannot find symbol
  symbol:   method clusterMxBean()
  location: interface org.apache.ignite.internal.processors.cache.IgniteInternalCache<java.lang.Object,java.lang.Object>
/opt/buildagent/work/9319dd66c384518/modules/hibernate-ext/hibernate/src/main/java/org/apache/ignite/cache/hibernate/HibernateCacheProxy.java:[554,5] method does not override or implement a method from a supertype
/opt/buildagent/work/9319dd66c384518/modules/hibernate-ext/hibernate/src/main/java/org/apache/ignite/cache/hibernate/HibernateCacheProxy.java:[555,30] cannot find symbol
  symbol:   method localMxBean()
  location: interface org.apache.ignite.internal.processors.cache.IgniteInternalCache<java.lang.Object,java.lang.Object>
{noformat}

Build history on TC1 & TC2:
# https://ci.ignite.apache.org/buildConfiguration/IgniteExtensions_Tests_Hibernate?branch=%3Cdefault%3E&mode=builds
# https://ci2.ignite.apache.org/buildConfiguration/IgniteExtensions_Tests_Hibernate?branch=%3Cdefault%3E&mode=builds",aonikolaev,shishkovilja,Minor,Resolved,Fixed,14/Feb/23 15:16,15/Feb/23 09:15
Bug,IGNITE-18814,13524855,Fix ItTxDistributedTestSingleNode#assertPartitionsSame,"*Motivation*
{code:java}
MvPartitionStorage storage = listener.getMvStorage();

if (hash == 0) {
    hash = storage.hashCode();
} else if (hash != storage.hashCode()) {
    return false;
}{code}
This code is wrong, hash code is not defined for storage.
*Implementation notes*
To summarize all the mentioned before, the main issue is finding a way to compare data stored in different replicas.
The obvious approach to calculate hash all object that stored in each replica storage. But the implementation can be difficult and time expensive in the test.
The easy way to compare stored indexies ( _MvPartitionStorage#lastAppliedIndex_ , _MvPartitionStorage#persistedIndex_). The both are applicable for comparison, but _persistedIndex_ upgrades latter.
{code}
MvPartitionStorage storage = listener.getMvStorage();

if (idx == 0) {
    idx = storage.persistedIndex();
} else if (idx != storage.persistedIndex()) {
    return false;
}
{code}",v.pyatkov,ibessonov,Critical,Resolved,Fixed,15/Feb/23 13:45,13/Jun/23 11:36
Bug,IGNITE-18815,13524922,C++ 3.0: Transaction should be rolled back synchronously on destruction,"Currently, {{transaction_impl}} is rolled back asynchronously on destruction. This can result in locking conflicts looking like this:
{noformat}
Failed to acquire a lock due to a conflict [txId=000f7d63-d563-0000-0242-ac78000a0000, conflictingWaiter=WaiterImpl [txId=000f7d63-d55b-0000-0242-ac78000a0000, intendedLockMode=null, lockMode=X, ex=null, isDone=true]
{noformat}
Transaction should be rolled back synchronously on destruction.

Test is to be un-muted when the issue is fixed: https://ci.ignite.apache.org/test/-3102268643379533778?currentProjectId=ApacheIgnite3xGradle_Test",isapego,isapego,Major,Resolved,Fixed,15/Feb/23 23:02,16/Feb/23 06:59
Bug,IGNITE-18826,13525032,Java thin client hangs on close,"There are several places where thin client threads can hang:

GridNioClientConnectionMultiplexer: SSL enabled + PA enabled
{noformat}
// Can return finished future with error -> sslHandshakeFut will not complete.
GridNioFuture<GridNioSession> sesFut = srv.createSession(ch, meta, false, null);

 if (sslHandshakeFut != null)
      sslHandshakeFut.get();
{noformat}

TcpClientChannel: PA enabled
{noformat}
ClientRequestFuture fut = new ClientRequestFuture(requestId, ClientOperation.HANDSHAKE);

 // TcpClientChannel may be closed and fut will not complete.
assert !closed.get();

pendingReqs.put(requestId, fut);
...
ByteBuffer buf = timeout > 0 ? fut.get(timeout) : fut.get();
{noformat}
",NSAmelchev,NSAmelchev,Major,Resolved,Fixed,16/Feb/23 14:26,21/Feb/23 12:25
Bug,IGNITE-18835,13525212,Get rid of skipping safe time waiting on a primary node,"Currently, we have a logic of skipping safe time waiting on a primary node in {{PartitionReplicaListener}} for read-only transaction, for example in {{PartitionReplicaListener#processReadOnlySingleEntryAction}}


{code:java}
        CompletableFuture<Void> safeReadFuture = isPrimary ? completedFuture(null) : safeTime.waitFor(request.readTimestamp());

        return safeReadFuture.thenCompose(unused -> resolveRowByPkForReadOnly(searchRow, readTimestamp));
{code}

This is a wrong behaviour, we expect that this awaiting must be done on a primary node as well",v.pyatkov,maliev,Major,Resolved,Fixed,17/Feb/23 13:30,11/May/23 15:22
Bug,IGNITE-18843,13525373,Fix the behavior of MvPartitionStorage#pollForVacuum when trying to delete the same row in parallel,"It is necessary to slightly correct the behavior of the *org.apache.ignite.internal.storage.MvPartitionStorage#pollForVacuum* when we are trying to delete the same *RowVersion* in parallel.

At the moment, when trying to delete 2 *RowVersion* in parallel, we will actually delete the first *RowVersion* for one thread, and the second thread will notice this and simply return *null*, and as a result, we will have one *RowVersion* that needs to be deleted.

We need to try to get *RowVersion*s until the queue is empty.

It needs to be fixed for all implementations.",ktkalenko@gridgain.com,ktkalenko@gridgain.com,Major,Resolved,Fixed,20/Feb/23 06:24,23/Feb/23 07:41
Bug,IGNITE-18851,13525445,.NET: Thin 3.0: TestReconnectAfterFullClusterRestart is flaky,"{code}
Condition not reached after 00:00:03.0086244
   at Apache.Ignite.Tests.TestUtils.WaitForCondition(Func`1 condition, Int32 timeoutMs, Func`1 messageFactory) in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/TestUtils.cs:line 62
   at Apache.Ignite.Tests.ReconnectTests.TestReconnectAfterFullClusterRestart() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/ReconnectTests.cs:line 156
   at NUnit.Framework.Internal.TaskAwaitAdapter.GenericAdapter`1.BlockUntilCompleted()
   at NUnit.Framework.Internal.MessagePumpStrategy.NoMessagePumpStrategy.WaitForCompletion(AwaitAdapter awaiter)
   at NUnit.Framework.Internal.AsyncToSyncAdapter.Await(Func`1 invoke)
   at NUnit.Framework.Internal.Commands.TestMethodCommand.RunTestMethod(TestExecutionContext context)
   at NUnit.Framework.Internal.Commands.TestMethodCommand.Execute(TestExecutionContext context)
   at NUnit.Framework.Internal.Execution.SimpleWorkItem.<>c__DisplayClass4_0.<PerformWork>b__0()
   at NUnit.Framework.Internal.ContextUtils.<>c__DisplayClass1_0`1.<DoIsolated>b__0(Object _)
1)    at Apache.Ignite.Tests.TestUtils.WaitForCondition(Func`1 condition, Int32 timeoutMs, Func`1 messageFactory) in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/TestUtils.cs:line 62
   at Apache.Ignite.Tests.ReconnectTests.TestReconnectAfterFullClusterRestart() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/ReconnectTests.cs:line 156
{code}

https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_RunNetTests/7088557?hideProblemsFromDependencies=false&hideTestsFromDependencies=false&pluginCoverage=true&expandBuildTestsSection=true",ptupitsyn,ptupitsyn,Major,Resolved,Fixed,20/Feb/23 15:26,20/Mar/23 09:07
Bug,IGNITE-18861,13525583,NPE in DumpThreadsOnTimeout breaking resource cleanup,"In test runs from CI founded follow stacktrace:
{code:java}
java.lang.NullPointerException
	at org.apache.ignite.internal.testframework.junit.DumpThreadsOnTimeout.isJunitMethodTimeout(DumpThreadsOnTimeout.java:68)
	at org.apache.ignite.internal.testframework.junit.DumpThreadsOnTimeout.handleThrowable(DumpThreadsOnTimeout.java:53)
	at org.apache.ignite.internal.testframework.junit.DumpThreadsOnTimeout.handleTestExecutionException(DumpThreadsOnTimeout.java:49)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestExecutionExceptionHandlers$8(TestMethodTestDescriptor.java:228)
	at org.junit.jupiter.engine.descriptor.JupiterTestDescriptor.invokeExecutionExceptionHandlers(JupiterTestDescriptor.java:123)
	at org.junit.jupiter.engine.descriptor.JupiterTestDescriptor.invokeExecutionExceptionHandlers(JupiterTestDescriptor.java:110)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestExecutionExceptionHandlers(TestMethodTestDescriptor.java:227)
	...
	at org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:193)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)
	at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:133)
	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:71)
	at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)
	at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)
{code}

In case any tests failed with TimeoutException *org.apache.ignite.internal.testframework.junit.DumpThreadsOnTimeout*  tried to produce readable message but fail with *NPE * in case when Exception message is null. After this NPE JUnit thread die and *BeforeAll\BeforeEach* methods are not called. As result, we have unkilled AI3 live nodes which holded ports and next tests on the CI Agent where it failed cannot run correctly with reason *Caused by: java.lang.IllegalStateException: No available port in range [3344-3344]*",rpuch,Mikhail Pochatkin,Major,Resolved,Fixed,21/Feb/23 14:04,23/Feb/23 04:23
Bug,IGNITE-18866,13525593,JdbcUrlRegistryImpl should stop fetching URLs after disconnect,"{{org.apache.ignite.internal.cli.core.repl.registry.impl.JdbcUrlRegistryImpl}} starts a thread executor on connect which starts polling the JDBC URLs from the node and doesn't stop doing this leading to the logs pollution especially in integration tests where a test connects to the node and even if it disonnects, the it continue to fetch URLs.",vpakhnushev,vpakhnushev,Major,Resolved,Fixed,21/Feb/23 14:28,22/Feb/23 15:02
Bug,IGNITE-18868,13525689,Recursive update of pendingInvokes in ReplicaService,"h3. Motivation

Take a look at the code below:
{code:java}
// ReplicaService#sendToReplica
           if (errResp.throwable() instanceof ReplicaUnavailableException) {
                        pendingInvokes.compute(targetNodeConsistentId, (clusterNode, fut) -> {
                            if (fut == null) {
                                AwaitReplicaRequest awaitReplicaReq = ...

                                fut = messagingService.invoke(...)
                                        .whenComplete((response0, throwable0) -> {
                                            pendingInvokes.remove(targetNodeConsistentId);
                                        });
                            }
{code}
In case {{messagingService}} respond immediately, {{whenComplete}} callback will be executed in the same thread causing recursive update ({{{}remove{}}} inside of {{{}compute{}}}) which is not allowed.
{code:java}
Caused by: java.lang.IllegalStateException: Recursive update
	at java.base/java.util.concurrent.ConcurrentHashMap.replaceNode(ConcurrentHashMap.java:1167)
	at java.base/java.util.concurrent.ConcurrentHashMap.remove(ConcurrentHashMap.java:1102)
	at org.apache.ignite.internal.replicator.ReplicaService.lambda$sendToReplica$0(ReplicaService.java:125)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:883)
	at java.base/java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2251)
	at org.apache.ignite.internal.replicator.ReplicaService.lambda$sendToReplica$3(ReplicaService.java:124)

{code}
a
h3. Definition of Done

Need to move 
{code:java}
.whenComplete(
    (response0, throwable0) -> { pendingInvokes.remove(targetNodeConsistentId); }
);
{code}
from the _compute_ block and add it to the future which is returned from the {_}compute{_}.",v.pyatkov,korlov,Major,Resolved,Fixed,22/Feb/23 07:21,28/Mar/23 10:35
Bug,IGNITE-18882,13525840,Fix tombstone is stored if it is the first entry of version chain,"This test in AbstractMvPartitionStorageGcTest should pass   


{code:java}
void testTombstoneFirst() {
        addAndCommit(null);

        addAndCommit(TABLE_ROW);

        addAndCommit(TABLE_ROW2);

        BinaryRowAndRowId row = pollForVacuum(HybridTimestamp.MAX_VALUE);

        assertRowMatches(row.binaryRow(), TABLE_ROW);
    }
{code}


At this moment, storages will store the tombstone if it is the first committed value which disrupts the GC flow.

addWrite with null argument as a first version of row is valid, for example: 


{code:sql}
CREATE TABLE test (
  id INT
);
BEGIN;
INSERT INTO test VALUES(1);
DELETE from test where id = 1;
COMMIT;
{code}


is ok, but tombstone should not be stored (so the operation should be no-op)",sdanilov,sdanilov,Major,Resolved,Fixed,23/Feb/23 08:00,02/Mar/23 16:05
Bug,IGNITE-18908,13526325,Distributed configuration doesn't generate events when the configuration property is updated with the same value,"Steps to reproduce:

Add the following code to org.apache.ignite.internal.runner.app.ItIgniteNodeRestartTest#testCfgGapWithoutData

 
{code:java}
IgniteImpl ignite = startNode(0);
Integer value = ignite.clusterConfiguration()
        .getConfiguration(RocksDbStorageEngineConfiguration.KEY)
        .flushDelayMillis().value();

ignite.clusterConfiguration()
        .getConfiguration(RocksDbStorageEngineConfiguration.KEY)
        .flushDelayMillis().update(value).join();

stopNode(0); {code}
Revision of the cluster configuration will be changed, but events will not be generated. So nodes joining the cluster will be unable to update their version of the configuration and the recovery will not be completed. 
{code:java}
// Recovery future must be created before configuration listeners are triggered.
CompletableFuture<?> recoveryFuture = RecoveryCompletionFutureFactory.create(
        clusterCfgMgr,
        fut -> new ConfigurationCatchUpListener(cfgStorage, fut, LOG)
)  {code}
 ",ivan.gagarkin,ivan.gagarkin,Critical,Resolved,Fixed,27/Feb/23 11:46,12/Apr/23 12:20
Bug,IGNITE-18918,13526509,Disable ItIgniteNodeRestartTest#testMetastorageStop,"The test depends on the count of the config changes. 

The condition is never met and the test always passes.  
{code:java}
if (rev == cfgGap / 2) {
    log.info(""Stopping METASTORAGE"");

    stopNode(0);

    log.info(""Starting METASTORAGE"");

    startNode(0);

    log.info(""Restarted METASTORAGE"");
} {code}
But if change the condition to be true (e.g., rev == 5), the test fails.  

The test should be disabled. ",ivan.gagarkin,ivan.gagarkin,Critical,Resolved,Fixed,28/Feb/23 11:30,28/Feb/23 15:07
Bug,IGNITE-18926,13526571,Open API spec falidation fails,"IGNITE-18881 introduced a regression in terms of openapi spec validation. It has duplicates in some parts. To reproduce the issue just pull the latest main branch and run ./gradlew check -x test -x integrationTest.

The reason for this is javadoc which adds parameter descriptions. From them the duplicated info is generated.",vpakhnushev,aleksandr.pakhomov,Major,Resolved,Fixed,28/Feb/23 19:12,06/Mar/23 11:22
Bug,IGNITE-18935,13526714,Late stopping of TTL workers during deactivation leads to corrupted PDS,"Step to reproduce
1. Reduce wal history size and wal segment size to 16MB and 8MB respectively, set checkpoint frequency to 10000
2. Perform heavy load with a lot of entries with TTL 5000 and with eager ttl enabled
3. Perform deactivation of cluster, stop grid and restart, provided that an expiration process is active during the process of restart.

{code}
[15:11:58,022][SEVERE][ttl-cleanup-worker-#52%None%][] Critical system error detected. Will be handled accordingly to configured handler [hnd=StopNodeOrHaltFailureHandler [tryStop=false, timeout=0, super=AbstractFailureHandler [ignoredFailureTypes=UnmodifiableSet [SYSTEM_WORKER_BLOCKED, SYSTEM_CRITICAL_OPERATION_TIMEOUT]]], failureCtx=FailureContext [type=CRITICAL_ERROR, err=class o.a.i.i.processors.cache.persistence.tree.CorruptedTreeException: B+Tree is corrupted [groupId=-459905951, pageIds=[], msg=Runtime failure on bounds: [lower=null, upper=PendingRow []]]]]
class org.apache.ignite.internal.processors.cache.persistence.tree.CorruptedTreeException: B+Tree is corrupted [groupId=-459905951, pageIds=[], msg=Runtime failure on bounds: [lower=null, upper=PendingRow []]]
	at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree.corruptedTreeException(BPlusTree.java:6434)
	at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree.find(BPlusTree.java:1294)
	at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree.find(BPlusTree.java:1249)
	at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree.find(BPlusTree.java:1237)
	at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree.find(BPlusTree.java:1232)
	at org.apache.ignite.internal.processors.cache.persistence.GridCacheOffheapManager$GridCacheDataStore.purgeExpiredInternal(GridCacheOffheapManager.java:3061)
	at org.apache.ignite.internal.processors.cache.persistence.GridCacheOffheapManager$GridCacheDataStore.purgeExpired(GridCacheOffheapManager.java:3010)
	at org.apache.ignite.internal.processors.cache.persistence.GridCacheOffheapManager.expire(GridCacheOffheapManager.java:1213)
	at org.apache.ignite.internal.processors.cache.GridCacheTtlManager.expire(GridCacheTtlManager.java:246)
	at org.apache.ignite.internal.processors.cache.GridCacheSharedTtlCleanupManager$CleanupWorker.lambda$body$0(GridCacheSharedTtlCleanupManager.java:199)
	at java.util.concurrent.ConcurrentHashMap.computeIfPresent(ConcurrentHashMap.java:1769)
	at org.apache.ignite.internal.processors.cache.GridCacheSharedTtlCleanupManager$CleanupWorker.body(GridCacheSharedTtlCleanupManager.java:198)
	at org.apache.ignite.internal.util.worker.GridWorker.run(GridWorker.java:125)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTreeRuntimeException: org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTreeRuntimeException: java.lang.IllegalStateException: Item not found: 24
	at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree.findLowerUnbounded(BPlusTree.java:1216)
	at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree.find(BPlusTree.java:1276)
	... 12 more
Caused by: org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTreeRuntimeException: java.lang.IllegalStateException: Item not found: 24
	at org.apache.ignite.internal.processors.cache.persistence.CacheDataRowAdapter.doInitFromLink(CacheDataRowAdapter.java:345)
	at org.apache.ignite.internal.processors.cache.persistence.CacheDataRowAdapter.initFromLink(CacheDataRowAdapter.java:165)
	at org.apache.ignite.internal.processors.cache.persistence.CacheDataRowAdapter.initFromLink(CacheDataRowAdapter.java:136)
	at org.apache.ignite.internal.processors.cache.persistence.CacheDataRowAdapter.initFromLink(CacheDataRowAdapter.java:123)
	at org.apache.ignite.internal.processors.cache.tree.PendingRow.initKey(PendingRow.java:73)
	at org.apache.ignite.internal.processors.cache.tree.PendingEntriesTree.getRow(PendingEntriesTree.java:128)
	at org.apache.ignite.internal.processors.cache.tree.PendingEntriesTree.getRow(PendingEntriesTree.java:32)
	at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree$ForwardCursor.fillFromBuffer0(BPlusTree.java:6115)
	at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree$AbstractForwardCursor.fillFromBuffer(BPlusTree.java:5864)
	at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree$AbstractForwardCursor.init(BPlusTree.java:5790)
	at org.apache.ignite.internal.processors.cache.persistence.tree.BPlusTree.findLowerUnbounded(BPlusTree.java:1205)
	... 13 more
Caused by: java.lang.IllegalStateException: Item not found: 24
	at org.apache.ignite.internal.processors.cache.persistence.tree.io.AbstractDataPageIO.findIndirectItemIndex(AbstractDataPageIO.java:488)
	at org.apache.ignite.internal.processors.cache.persistence.tree.io.AbstractDataPageIO.getDataOffset(AbstractDataPageIO.java:596)
	at org.apache.ignite.internal.processors.cache.persistence.tree.io.AbstractDataPageIO.readPayload(AbstractDataPageIO.java:638)
	at org.apache.ignite.internal.processors.cache.persistence.CacheDataRowAdapter.readIncomplete(CacheDataRowAdapter.java:380)
	at org.apache.ignite.internal.processors.cache.persistence.CacheDataRowAdapter.doInitFromLink(CacheDataRowAdapter.java:316)
	... 23 more
{code}",ivandasch,ivandasch,Major,Resolved,Fixed,01/Mar/23 16:13,24/Apr/23 14:53
Bug,IGNITE-18938,13526822,SSL integration tests fail on Windows,"Integration tests using {{org.apache.ignite.internal.rest.ssl.RestNode#bootstrapCfg}} fail on Windows.

Micronaut file path properties like {{micronaut.server.ssl.key-store.path}} expect the ""file:"" prefix, but they don't use proper handling and simply cut the prefix, so on Windows the property should look like {{file:C:\tmp}} instead of {{file:/C:/tmp}}.
Also the config file parser expect escaped strings, so the backslashes should be escaped and the whole string should be quoted.",vpakhnushev,vpakhnushev,Major,Resolved,Fixed,02/Mar/23 09:25,06/Mar/23 11:22
Bug,IGNITE-18946,13526940,Fix 'REFRESH' command about statistics in the ddl doc,"Fix doc about the 'REFRESH' command about the STATISTICS, 

see [https://ignite.apache.org/docs/2.14.0/sql-reference/ddl#refresh]

the command should be 'REFRESH STATISTICS'",chenjian2664,chenjian2664,Trivial,Resolved,Fixed,03/Mar/23 04:50,03/Mar/23 06:09
Bug,IGNITE-18966,13527208,Sql. Custom data types. Fix least restrictive type and nullability.,"1. Calcite uses ANY type for the DEFAULT operator and introduction of custom data types caused a regression that broke that rule.
  
2. Nullable attribute is not correctly set for custom data types - it creates a custom data type with nullability = true when it should be false.

3. Update commonTypeForBinaryComparison to convert to/from custom data type in binary comparison operators.
",mzhuravkov,mzhuravkov,Minor,Resolved,Fixed,06/Mar/23 08:57,16/Mar/23 07:35
Bug,IGNITE-18968,13527217,Possible race between updating a low watermark and processing the last batch for storage in a background GC,"After some analysis of the background garbage collector code, a possible race was found between updating the low watermark and processing the last batch of storage.
Needs to be fixed.",ktkalenko@gridgain.com,ktkalenko@gridgain.com,Major,Resolved,Fixed,06/Mar/23 09:21,07/Mar/23 08:40
Bug,IGNITE-18972,13527292,SSL configuration validation doesn't work,IGNITE-18850 doesn't properly define configuration validation. Proper annotation types should be used and tests should be added to test that validation actually works.,vpakhnushev,vpakhnushev,Major,Resolved,Fixed,06/Mar/23 14:38,07/Mar/23 09:37
Bug,IGNITE-18976,13527395,Affinity broken on thick client after reconnection,"1 Using AffinyKey +BynaryTypeconfiguration 
2 Client is reconnected
3 Affinity is Broken and Binary marshalling is broken:

on Affinity.partition wrong value is returning:
{noformat}
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:631)
	at org.apache.ignite.testframework.junits.JUnitAssertAware.assertEquals(JUnitAssertAware.java:95)
	at org.apache.ignite.internal.IgniteClientReconnectAffinityTest.doReconnectClientAffinityKeyPartition(IgniteClientReconnectAffinityTest.java:213)
	at org.apache.ignite.internal.IgniteClientReconnectAffinityTest.testReconnectClientAnnotatedAffinityKeyWithBinaryConfigPartition(IgniteClientReconnectAffinityTest.java:123)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.ignite.testframework.junits.GridAbstractTest$6.run(GridAbstractTest.java:2504)
	at java.lang.Thread.run(Thread.java:748)
{noformat} [^IgniteClientReconnectAffinityTest.java] 

Exception on cache.get :
{noformat}
class org.apache.ignite.binary.BinaryObjectException: Failed to serialize object [typeName=org.apache.ignite.internal.IgniteClientReconnectAffinityTest$TestAnnotatedKey]

	at org.apache.ignite.internal.binary.BinaryClassDescriptor.write(BinaryClassDescriptor.java:916)
	at org.apache.ignite.internal.binary.BinaryWriterExImpl.marshal0(BinaryWriterExImpl.java:232)
	at org.apache.ignite.internal.binary.BinaryWriterExImpl.marshal(BinaryWriterExImpl.java:165)
	at org.apache.ignite.internal.binary.BinaryWriterExImpl.marshal(BinaryWriterExImpl.java:152)
	at org.apache.ignite.internal.binary.GridBinaryMarshaller.marshal(GridBinaryMarshaller.java:251)
	at org.apache.ignite.internal.processors.cache.binary.CacheObjectBinaryProcessorImpl.marshalToBinary(CacheObjectBinaryProcessorImpl.java:583)
	at org.apache.ignite.internal.processors.cache.binary.CacheObjectBinaryProcessorImpl.toBinary(CacheObjectBinaryProcessorImpl.java:1492)
	at org.apache.ignite.internal.processors.cache.binary.CacheObjectBinaryProcessorImpl.toCacheKeyObject(CacheObjectBinaryProcessorImpl.java:1287)
	at org.apache.ignite.internal.processors.cache.GridCacheContext.toCacheKeyObject(GridCacheContext.java:1818)
	at org.apache.ignite.internal.processors.cache.distributed.dht.colocated.GridDhtColocatedCache.getAsync(GridDhtColocatedCache.java:279)
	at org.apache.ignite.internal.processors.cache.GridCacheAdapter.get(GridCacheAdapter.java:4759)
	at org.apache.ignite.internal.processors.cache.GridCacheAdapter.repairableGet(GridCacheAdapter.java:4725)
	at org.apache.ignite.internal.processors.cache.GridCacheAdapter.get(GridCacheAdapter.java:1373)
	at org.apache.ignite.internal.processors.cache.IgniteCacheProxyImpl.get(IgniteCacheProxyImpl.java:1108)
	at org.apache.ignite.internal.processors.cache.GatewayProtectedCacheProxy.get(GatewayProtectedCacheProxy.java:686)
	at org.apache.ignite.internal.IgniteClientReconnectAffinityTest.doReconnectClientAffinityKeyGet(IgniteClientReconnectAffinityTest.java:180)
	at org.apache.ignite.internal.IgniteClientReconnectAffinityTest.testReconnectClientAnnotatedAffinityKeyWithBinaryConfigGet(IgniteClientReconnectAffinityTest.java:118)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.ignite.testframework.junits.GridAbstractTest$6.run(GridAbstractTest.java:2504)
	at java.lang.Thread.run(Thread.java:748)
Caused by: class org.apache.ignite.binary.BinaryObjectException: Binary type has different affinity key fields [typeName=org.apache.ignite.internal.IgniteClientReconnectAffinityTest$TestAnnotatedKey, affKeyFieldName1=null, affKeyFieldName2=annotatedAffinityKey]
	at org.apache.ignite.internal.binary.BinaryUtils.mergeMetadata(BinaryUtils.java:1026)
	at org.apache.ignite.internal.processors.cache.binary.BinaryMetadataTransport.requestMetadataUpdate(BinaryMetadataTransport.java:210)
	at org.apache.ignite.internal.processors.cache.binary.CacheObjectBinaryProcessorImpl.addMeta(CacheObjectBinaryProcessorImpl.java:638)
	at org.apache.ignite.internal.processors.cache.binary.CacheObjectBinaryProcessorImpl$1.addMeta(CacheObjectBinaryProcessorImpl.java:292)
	at org.apache.ignite.internal.binary.BinaryContext.updateMetadata(BinaryContext.java:1342)
	at org.apache.ignite.internal.binary.BinaryClassDescriptor.write(BinaryClassDescriptor.java:875)
	... 26 more

{noformat}",ivandasch,macrergate,Major,Resolved,Fixed,07/Mar/23 08:11,24/Apr/23 14:50
Bug,IGNITE-18981,13527635,Fix build issues after merge of the ticket,"After https://issues.apache.org/jira/browse/IGNITE-18835 was merged without updating with main, several build issues have been occurred. 

# {{AbstractBasicIntegrationTest}} was renamed to {{ClusterPerClassIntegrationTest}}, so appropriate class should be used in {{ItReadOnlyTransactionTest}} as a base class
# {{IgnitionManager#init}} method parameters were changed, so {{InitParameters}} should be used in {{ItRoReadsTest}}",maliev,maliev,Major,Resolved,Fixed,08/Mar/23 12:15,08/Mar/23 12:46
Bug,IGNITE-18987,13527723,Problem with SslContextProviderTest on Windows,"The following unit tests fail for me on Windows:

SslContextProviderTest > throwsIgniteExceptionWhenWrongTruststorePathConfigured() FAILED
java.lang.AssertionError at SslContextProviderTest.java:162

SslContextProviderTest > throwsIgniteExceptionWhenWrongKeystorePathConfigured() FAILED
java.lang.AssertionError at SslContextProviderTest.java:145

More specifically such assert fails:

assertThat(thrown.getMessage(){color:#cc7832}, {color}containsString({color:#6a8759}""File /no/such/file.pfx not found""{color}));

The real message contains the path with backslashes: \no\such\file.pfx.",ademakov,ademakov,Major,Resolved,Fixed,09/Mar/23 00:48,09/Mar/23 07:41
Bug,IGNITE-18993,13527874,ODBC: Regression. Missed handling of single quotes,"When {{SQLTables}} is called with param 'table type' {{'TABLES'}} (with single comma), it returns an empty table list.
It is a quite common way of calling this procedure (i.e. by INFORMATICA)",ivandasch,ivandasch,Major,Resolved,Fixed,09/Mar/23 20:35,24/Apr/23 14:52
Bug,IGNITE-19004,13527984,ignite config show is broken on connected state,"Steps to reproduce:

* connect to an initialized cluster
* {{node config show}} (ok)
* {{node config show \-\-node-name}}     <---- without node name
* {{node config show}} (broken)
",vpakhnushev,aleksandr.pakhomov,Major,Resolved,Fixed,10/Mar/23 14:23,20/Mar/23 09:13
Bug,IGNITE-19005,13527986,Fix current metric names from prefix.prefix.metric to prefix.prefix.Metric,"JvmMetricSource has the metric names, which violates metric naming conventions: 
memory.heap.init must be replaced by memory.heap.Init and etc.",slava.koptilin,kgusakov,Major,Resolved,Fixed,10/Mar/23 14:24,08/Jun/23 07:43
Bug,IGNITE-19014,13528222,Topology aware Raft client creates its own message handler which is not aware of Raft group,"Topology aware client creates its own message handler for leader change notifications. These notifications does not contain group ids, which is a problem for adding such clients for partition replication groups. Moreover, having same message handlers for one type of messages in number that is equal to the count of topology aware clients seems not to be a good decision and leads to weird problems after adding topology aware clients for partition Raft groups, which appear as random integration tests fails. 

We should get rid of this handler in topology aware client and create dedicated message processor in RPC.",Denis Chudov,Denis Chudov,Major,Resolved,Fixed,13/Mar/23 10:58,16/Mar/23 16:57
Bug,IGNITE-19018,13528345,Sql. Investigate AssertionError in ScanNode. ItCorrelatesTest fails when distribution of VALUES is changed from Broadcast to Single.,"The query in ItCorrelatesTest triggers AssertionError in ScanNode. This behaviour is observed in after modification operations were changed in https://issues.apache.org/jira/browse/IGNITE-18225 is implemented. 

{code:java}
 assertQuery(""SELECT * FROM test1 WHERE ""
                + ""EXISTS(SELECT * FROM test2 WHERE (SELECT test1.a)=test2.a AND (SELECT test1.b)<>test2.c) ""
                + ""AND NOT EXISTS(SELECT * FROM test2 WHERE (SELECT test1.a)=test2.a AND (SELECT test1.b)<test2.c)"")
                .returns(12, 2)
                .check();
{code}

AssertionError:

{code:java}
Caused by: java.lang.AssertionError: rowsCnt=512, requested=512
	at org.apache.ignite.internal.sql.engine.exec.rel.ScanNode.request(ScanNode.java:53)
	at org.apache.ignite.internal.sql.engine.exec.rel.ProjectNode.request(ProjectNode.java:59)
	at org.apache.ignite.internal.sql.engine.exec.rel.HashAggregateNode.request(HashAggregateNode.java:110)
	at org.apache.ignite.internal.sql.engine.exec.rel.Outbox.flush(Outbox.java:267)
	at org.apache.ignite.internal.sql.engine.exec.rel.Outbox.onRequest(Outbox.java:108)
	at org.apache.ignite.internal.sql.engine.exec.ExchangeServiceImpl.lambda$onMessage$3(ExchangeServiceImpl.java:178)
	at org.apache.ignite.internal.sql.engine.exec.ExchangeServiceImpl.onMessage(ExchangeServiceImpl.java:187)
	at org.apache.ignite.internal.sql.engine.exec.ExchangeServiceImpl.lambda$start$1(ExchangeServiceImpl.java:73)
	at org.apache.ignite.internal.sql.engine.message.MessageServiceImpl.onMessageInternal(MessageServiceImpl.java:164)
	at org.
{code}

",amashenkov,mzhuravkov,Critical,Resolved,Fixed,14/Mar/23 06:30,03/May/23 08:33
Bug,IGNITE-19020,13528388,Specify expire time during conflict resolution,"In case usage of putAllConflict, expireTime behaviour differs from standart cache operations.
We must allow to the plugin providers to specify expiryTime when resolving entries conflicts.",nizhikov,nizhikov,Major,Resolved,Fixed,14/Mar/23 09:35,24/Apr/23 14:47
Bug,IGNITE-19026,13528454,Ignite extensions fail to compile after TcpClientCache#putAllConflict method signature has changed.,"https://issues.apache.org/jira/browse/IGNITE-17449 caused the signature of the TcpClientCache#putAllConflict method to change, which in turn caused the compilation of the CDC extension to fail.

TC Build example:
https://ci.ignite.apache.org/viewLog.html?buildId=7129807&tab=buildResultsDiv&buildTypeId=IgniteExtensions_Tests_Cdc",,PetrovMikhail,Blocker,Resolved,Fixed,14/Mar/23 14:44,20/Apr/23 10:46
Bug,IGNITE-19038,13528601,sql command does not enter repl,"{code:bash}
[defaultNode]> sql ""create table test(i int primary key)""
SQL query execution error
Exception while executing query [query=""create table test(i int primary key)""]. Error message:IGN-SQL-3 TraceId:9f1d43d1-5f6d-4319-94a1-2225e931a80d IGN-SQL-3 TraceId:9f1d43d1-5f6d-4319-94a1-2225e931a80d Failed to parse query: Non-query expression encountered in illegal context <--- Why I get this error?
[defaultNode]> SQL    <---- here type exactly 3 letters and hit enter
SQL query execution error
Exception while executing query [query=""create table test(i int primary key)""]. Error message:IGN-SQL-3 TraceId:801a965d-3adf-443c-8028-d080fa894fb8 IGN-SQL-3 TraceId:801a965d-3adf-443c-8028-d080fa894fb8 Failed to parse query: Non-query expression encountered in illegal context
{code}

I would expect to enter the repl, but I get the same error message that I've got before. 

Also, the first error I've got seems strange, exactly the same query works in the repl.

Note: if I type ""sql "" with the space after -- it enters the repl. ",vpakhnushev,aleksandr.pakhomov,Major,Resolved,Fixed,15/Mar/23 09:55,17/Apr/23 17:16
Bug,IGNITE-19039,13528611,Fix PlatformTestNodeRunner SSL config on Windows,There are problems on Windows with resolving path of SSL key path.,ademakov,ademakov,Major,Resolved,Fixed,15/Mar/23 11:11,15/Mar/23 14:56
Bug,IGNITE-19042,13528659,NPE on DistributionZoneManager#saveDataNodesToMetaStorageOnScaleUp,"h3. Motivation

NullPointerException in log on start the first node in the cluster if DistributionZoneConfigurationSchema#dataNodesAutoAdjustScaleUp and DistributionZoneConfigurationSchema#dataNodesAutoAdjustScaleDown are 0.

How to reproduce:
 # Set DistributionZoneConfigurationSchema#dataNodesAutoAdjustScaleUp = 0 and DistributionZoneConfigurationSchema#dataNodesAutoAdjustScaleDown = 0.
 # Start the test which starts the node. For example ItTablesApiTest#testAddIndex.
 # See NullPointerException in the log.

{code:java}
Caused by: java.lang.NullPointerException	at java.base/java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:108)	at org.apache.ignite.internal.util.ByteUtils.fromBytes(ByteUtils.java:152)	at org.apache.ignite.internal.distributionzones.DistributionZoneManager.lambda$saveDataNodesToMetaStorageOnScaleUp$32(DistributionZoneManager.java:1096)	at org.apache.ignite.internal.util.IgniteUtils.inBusyLock(IgniteUtils.java:870)	at org.apache.ignite.internal.distributionzones.DistributionZoneManager.lambda$saveDataNodesToMetaStorageOnScaleUp$33(DistributionZoneManager.java:1090)	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1072)	... 13 more {code}
The root cause is zoneDataNodesKey, zoneScaleUpChangeTriggerKey, zoneScaleDownChangeTriggerKey of zones are not initialized on node start if vaultMgr.get(zonesLogicalTopologyKey()) is null. It's possible for default distribution zone because custom zone the keys initialized in DistributionZoneManager.ZonesConfigurationListener#onCreate.

*Definition of Done*

Properly handle non-initialized values of zoneDataNodesKey, zoneScaleUpChangeTriggerKey, zoneScaleDownChangeTriggerKey in DistributionZoneManager#saveDataNodesToMetaStorageOnScaleUp and DistributionZoneManager#
saveDataNodesToMetaStorageOnScaleDown",Sergey Uttsel,Sergey Uttsel,Major,Resolved,Fixed,15/Mar/23 15:39,20/Mar/23 13:58
Bug,IGNITE-19043,13528661,ItRaftCommandLeftInLogUntilRestartTest: PageMemoryHashIndexStorage lacks data after cluster restart,"After enabling ItRaftCommandLeftInLogUntilRestartTest failed with
{code:java}
org.opentest4j.AssertionFailedError: expected: not <null> {code}
while trying to retrieve previously added data after cluster restart. Seems that it's because there's no corresponding data in PK index.

It is worth to mention that originally given test is about about raft log re-application on node restart. So, I've commented all  partitionUpdateInhibitor in order to check whether it's related to re-application or indexes themselves, problem is reproducible without re-application logic.

It might be related to rocks to page memory defaults migration. Further investigation required.
h3. Implementation notes

After the investigation it's occurred that the reason of the failure is that raft log re-appliance is skipped within PartitionListener#handleUpdateCommand and PartitionListener#handleUpdateAllCommand because of following logic
{code:java}
        TxMeta txMeta = txStateStorage.get(cmd.txId());
        if (txMeta != null && (txMeta.txState() == COMMITED || txMeta.txState() == ABORTED)) {
            storage.runConsistently(() -> {
                storage.lastApplied(commandIndex, commandTerm);
                return null;
            });
        } 
 
{code}
Full scenario is following:

1. tx1.put populates raft log and mvPartitionStorage with corresponding log record and data.

2. tx1.commit also populates raft log with raft record and finished the transaction within txnStateStorage along wiht cleanup in mvPartitionStorage.

3. RocksDB based txnStateStorage flushes its state to a disk and page memory based doesn't.

4. After node restart raft replays the log, both put and commit commands, however on commit partition we skip put re-application  because of aforementioned
{code:java}
if (txMeta != null && (txMeta.txState() == COMMITED || txMeta.txState() == ABORTED)){code}
Just in case, transaction is considered to be committed because txnStateStorage flushes its state before stop.

 

So, in order to fix given issue it's enough to just remove the skip logic.",alapin,alapin,Major,Resolved,Fixed,15/Mar/23 15:57,31/Mar/23 13:56
Bug,IGNITE-19044,13528667,ItIgniteInMemoryNodeRestartTest#inMemoryNodeRestartNotLeader fails,"{code:java}
org.opentest4j.AssertionFailedError: expected: <true> but was: <false>  at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)  at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)  at app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)  at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)  at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:31)  at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:180)  at app//org.apache.ignite.internal.runner.app.ItIgniteInMemoryNodeRestartTest.inMemoryNodeRestartNotLeader(ItIgniteInMemoryNodeRestartTest.java:224) {code}
h3. Implementation Notes
 * ItIgniteInMemoryNodeRestartTest#inMemoryNodeRestartNotLeader was fixed within https://issues.apache.org/jira/browse/IGNITE-19043

 * And I've also enabled inMemoryNodeFullPartitionRestart(that was disabled with https://issues.apache.org/jira/browse/IGNITE-18822)  because the problem was probably fixed within https://issues.apache.org/jira/browse/IGNITE-19059 and https://issues.apache.org/jira/browse/IGNITE-19022

 ",alapin,alapin,Major,Resolved,Fixed,15/Mar/23 16:39,31/Mar/23 15:21
Bug,IGNITE-19058,13528927,ClientLoggingTest.testBasicLogging is flaky,"1. The test is flaky
2. The assertion is not helpful: ""expected: <true> but was: <false>"" - provide more details
3. First run takes a lot of time?

 !screenshot-1.png! 

https://ci.ignite.apache.org/test/8764679646897676088?currentProjectId=ApacheIgnite3xGradle_Test&expandTestHistoryChartSection=true&branch=


{code}
org.opentest4j.AssertionFailedError: expected: <true> but was: <false>
  at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)
  at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)
  at app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)
  at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)
  at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:31)
  at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:180)
  at app//org.apache.ignite.client.ClientLoggingTest.testBasicLogging(ClientLoggingTest.java:97)
{code}
",ptupitsyn,ptupitsyn,Major,Resolved,Fixed,17/Mar/23 09:14,17/Mar/23 14:49
Bug,IGNITE-19060,13528946,Snapshot operation NPE on a client node,"NPE if a client node does not have data storage configuration:
{noformat}
java.lang.NullPointerException: null
   at org.apache.ignite.internal.util.future.GridFinishedFuture.chain(GridFinishedFuture.java:145) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
   at org.apache.ignite.internal.processors.cache.persistence.snapshot.IgniteSnapshotManager.initLocalSnapshotEndStage(IgniteSnapshotManager.java:1350) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
   at org.apache.ignite.internal.util.distributed.DistributedProcess.lambda$new$2(DistributedProcess.java:151) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
   at org.apache.ignite.internal.managers.discovery.GridDiscoveryManager$3.onDiscovery0(GridDiscoveryManager.java:760) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
   at org.apache.ignite.internal.managers.discovery.GridDiscoveryManager$3.access$7300(GridDiscoveryManager.java:547) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
   at org.apache.ignite.internal.managers.discovery.GridDiscoveryManager$3$NotificationTask.run(GridDiscoveryManager.java:980) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
   at org.apache.ignite.internal.managers.discovery.GridDiscoveryManager$3$SecurityAwareNotificationTask.run(GridDiscoveryManager.java:950) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
   at org.apache.ignite.internal.managers.discovery.GridDiscoveryManager$DiscoveryMessageNotifierWorker.body0(GridDiscoveryManager.java:2822) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
   at org.apache.ignite.internal.managers.discovery.GridDiscoveryManager$DiscoveryMessageNotifierWorker.body(GridDiscoveryManager.java:2860) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
   at org.apache.ignite.internal.util.worker.GridWorker.run(GridWorker.java:125) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
   at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_201]
{noformat}
",NSAmelchev,NSAmelchev,Major,Resolved,Fixed,17/Mar/23 10:54,19/Mar/23 08:44
Bug,IGNITE-19068,13529205,ClassCastException in SQL on certain column combination,"Place attached *Reproducer.java* to *modules/runner/src/integrationTest/java/org/apache/ignite/internal/runner/app/* and run.

It fails with an exception:
{code}
IGN-CMN-65535 TraceId:0d8e3c54-2e5e-4b46-9c2c-306c11cf4d91 Remote query execution
	at org.apache.ignite.lang.IgniteException.wrap(IgniteException.java:289)
	at org.apache.ignite.internal.sql.engine.AsyncSqlCursorImpl.lambda$requestNextAsync$0(AsyncSqlCursorImpl.java:77)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.ignite.internal.sql.engine.exec.rel.AsyncRootNode.lambda$closeAsync$0(AsyncRootNode.java:157)
	at java.base/java.util.concurrent.ConcurrentLinkedQueue.forEachFrom(ConcurrentLinkedQueue.java:1037)
	at java.base/java.util.concurrent.ConcurrentLinkedQueue.forEach(ConcurrentLinkedQueue.java:1054)
	at org.apache.ignite.internal.sql.engine.exec.rel.AsyncRootNode.closeAsync(AsyncRootNode.java:157)
	at org.apache.ignite.internal.sql.engine.exec.rel.AsyncRootNode.onError(AsyncRootNode.java:112)
	at org.apache.ignite.internal.sql.engine.exec.ExecutionServiceImpl$DistributedQueryManager.lambda$onError$2(ExecutionServiceImpl.java:509)
	at java.base/java.util.concurrent.CompletableFuture.uniAcceptNow(CompletableFuture.java:753)
	at java.base/java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:731)
	at java.base/java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:2108)
	at org.apache.ignite.internal.sql.engine.exec.ExecutionServiceImpl$DistributedQueryManager.onError(ExecutionServiceImpl.java:508)
	at org.apache.ignite.internal.sql.engine.exec.ExecutionServiceImpl.onMessage(ExecutionServiceImpl.java:357)
	at org.apache.ignite.internal.sql.engine.exec.ExecutionServiceImpl.lambda$start$4(ExecutionServiceImpl.java:216)
	at org.apache.ignite.internal.sql.engine.message.MessageServiceImpl.onMessageInternal(MessageServiceImpl.java:164)
	at org.apache.ignite.internal.sql.engine.message.MessageServiceImpl.lambda$onMessage$1(MessageServiceImpl.java:135)
	at org.apache.ignite.internal.sql.engine.exec.QueryTaskExecutorImpl.lambda$execute$0(QueryTaskExecutorImpl.java:80)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.ignite.internal.sql.engine.metadata.RemoteException: Remote query execution
	at org.apache.ignite.internal.sql.engine.exec.ExecutionServiceImpl.onMessage(ExecutionServiceImpl.java:355)
	... 7 more
Caused by: java.lang.ClassCastException: class java.time.Instant cannot be cast to class java.lang.Long (java.time.Instant and java.lang.Long are in module java.base of loader 'bootstrap')
	at SC.execute(Unknown Source)
	at org.apache.ignite.internal.sql.engine.exec.exp.ExpressionFactoryImpl$ProjectImpl.apply(ExpressionFactoryImpl.java:654)
	at org.apache.ignite.internal.sql.engine.exec.rel.StorageScanNode.push(StorageScanNode.java:197)
	at org.apache.ignite.internal.sql.engine.exec.rel.StorageScanNode$SubscriberImpl.lambda$onComplete$2(StorageScanNode.java:303)
	at org.apache.ignite.internal.sql.engine.exec.ExecutionContext.lambda$execute$0(ExecutionContext.java:315)
	... 4 more
{code}

Note that is does NOT fail if we exclude either UUID or TIMESTAMP column from the SELECT query.",zstan,ptupitsyn,Major,Resolved,Fixed,20/Mar/23 08:11,10/Apr/23 06:38
Bug,IGNITE-19069,13529217,.NET: Thin 3.0: TestReconnectAfterFullClusterRestart is still flaky,"https://ci.ignite.apache.org/test/5088070784193128850?currentProjectId=ApacheIgnite3xGradle_Test&expandTestHistoryChartSection=true

{code}
Expected: No Exception to be thrown
  But was:  <Apache.Ignite.IgniteClientConnectionException: Failed to connect to endpoint: 127.0.0.1:42477
 ---> System.TimeoutException: The operation has timed out.
   at Apache.Ignite.Internal.ClientSocket.ConnectAsync(SocketEndpoint endPoint, IgniteClientConfiguration configuration, Action`1 assignmentChangeCallback) in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite/Internal/ClientSocket.cs:line 195
   --- End of inner exception stack trace ---
   at Apache.Ignite.Internal.ClientSocket.ConnectAsync(SocketEndpoint endPoint, IgniteClientConfiguration configuration, Action`1 assignmentChangeCallback) in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite/Internal/ClientSocket.cs:line 213
   at Apache.Ignite.Internal.ClientFailoverSocket.ConnectAsync(SocketEndpoint endpoint) in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite/Internal/ClientFailoverSocket.cs:line 414
   at Apache.Ignite.Internal.ClientFailoverSocket.GetNextSocketAsync() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite/Internal/ClientFailoverSocket.cs:line 365
   at Apache.Ignite.Internal.ClientFailoverSocket.GetSocketAsync(PreferredNode preferredNode) in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite/Internal/ClientFailoverSocket.cs:line 285
   at Apache.Ignite.Internal.ClientFailoverSocket.DoOutInOpAndGetSocketAsync(ClientOp clientOp, Transaction tx, PooledArrayBuffer request, PreferredNode preferredNode) in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite/Internal/ClientFailoverSocket.cs:line 183
   at Apache.Ignite.Internal.ClientFailoverSocket.DoOutInOpAsync(ClientOp clientOp, PooledArrayBuffer request, PreferredNode preferredNode) in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite/Internal/ClientFailoverSocket.cs:line 145
   at Apache.Ignite.Internal.Table.Tables.GetTablesAsync() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite/Internal/Table/Tables.cs:line 64
   at Apache.Ignite.Tests.ReconnectTests.<>c__DisplayClass6_0.<<TestReconnectAfterFullClusterRestart>b__2>d.MoveNext() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/ReconnectTests.cs:line 162
--- End of stack trace from previous location ---
   at NUnit.Framework.Internal.TaskAwaitAdapter.GenericAdapter`1.BlockUntilCompleted()
   at NUnit.Framework.Internal.MessagePumpStrategy.NoMessagePumpStrategy.WaitForCompletion(AwaitAdapter awaiter)
   at NUnit.Framework.Internal.AsyncToSyncAdapter.Await(Func`1 invoke)
   at NUnit.Framework.Internal.ExceptionHelper.RecordException(Delegate parameterlessDelegate, String parameterName)>
   at Apache.Ignite.Tests.ReconnectTests.TestReconnectAfterFullClusterRestart() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/ReconnectTests.cs:line 162
   at NUnit.Framework.Internal.TaskAwaitAdapter.GenericAdapter`1.BlockUntilCompleted()
   at NUnit.Framework.Internal.MessagePumpStrategy.NoMessagePumpStrategy.WaitForCompletion(AwaitAdapter awaiter)
   at NUnit.Framework.Internal.AsyncToSyncAdapter.Await(Func`1 invoke)
   at NUnit.Framework.Internal.Commands.TestMethodCommand.RunTestMethod(TestExecutionContext context)
   at NUnit.Framework.Internal.Commands.TestMethodCommand.Execute(TestExecutionContext context)
   at NUnit.Framework.Internal.Execution.SimpleWorkItem.<>c__DisplayClass4_0.<PerformWork>b__0()
   at NUnit.Framework.Internal.ContextUtils.<>c__DisplayClass1_0`1.<DoIsolated>b__0(Object _)
1)    at Apache.Ignite.Tests.ReconnectTests.TestReconnectAfterFullClusterRestart() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/ReconnectTests.cs:line 162
{code}",ptupitsyn,ptupitsyn,Major,Resolved,Fixed,20/Mar/23 09:07,23/Mar/23 05:34
Bug,IGNITE-19079,13529372,ExecutionTimeout in ItIgniteNodeRestartTest,"ItIgniteNodeRestartTest#testTwoNodesRestartDirect fails with ExecutionTimeout
{code:java}
023-03-20 03:52:36:208 +0300 [INFO][%iinrt_ttnrd_0%JRaft-ElectionTimer-17][NodeImpl] Unsuccessful election round number 662
    2023-03-20 03:52:36:209 +0300 [INFO][%iinrt_ttnrd_0%JRaft-ElectionTimer-17][NodeImpl] Node <d73a4612-9277-4061-9331-b3b78b0ee85d_part_0/iinrt_ttnrd_0> term 1 start preVote.
    2023-03-20 03:52:36:601 +0300 [INFO][%iinrt_ttnrd_0%JRaft-ElectionTimer-18][NodeImpl] Unsuccessful election round number 659
    2023-03-20 03:52:36:601 +0300 [INFO][%iinrt_ttnrd_0%JRaft-ElectionTimer-18][NodeImpl] Node <4d8ec640-9e96-4939-86e8-acb0c9460da8_part_1/iinrt_ttnrd_0> term 1 start preVote.
    2023-03-20 03:52:37:992 +0300 [INFO][%iinrt_ttnrd_0%JRaft-ElectionTimer-6][NodeImpl] Unsuccessful election round number 663
    2023-03-20 03:52:37:992 +0300 [INFO][%iinrt_ttnrd_0%JRaft-ElectionTimer-6][NodeImpl] Node <d73a4612-9277-4061-9331-b3b78b0ee85d_part_0/iinrt_ttnrd_0> term 1 start preVote.
    2023-03-20 03:52:38:049 +0300 [INFO][%iinrt_ttnrd_0%JRaft-ElectionTimer-19][NodeImpl] Unsuccessful election round number 660
    2023-03-20 03:52:38:049 +0300 [INFO][%iinrt_ttnrd_0%JRaft-ElectionTimer-19][NodeImpl] Node <4d8ec640-9e96-4939-86e8-acb0c9460da8_part_1/iinrt_ttnrd_0> term 1 start preVote.
    2023-03-20 03:52:38:299 +0300 [INFO][%iinrt_ttnrd_0%JRaft-ElectionTimer-2][NodeImpl] Unsuccessful election round number 659
    2023-03-20 03:52:38:300 +0300 [INFO][%iinrt_ttnrd_0%JRaft-ElectionTimer-2][NodeImpl] Node <4d8ec640-9e96-4939-86e8-acb0c9460da8_part_0/iinrt_ttnrd_0> term 1 start preVote.
    2023-03-20 03:52:42:870 +0300 [INFO][%iinrt_ttnrd_0%JRaft-ElectionTimer-1][NodeImpl] Unsuccessful election round number 662 {code}
https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_RunAllTests/7138347?expandCode+Inspection=true&expandBuildProblemsSection=true&hideProblemsFromDependencies=false&hideTestsFromDependencies=false",Denis Chudov,alapin,Major,Resolved,Fixed,21/Mar/23 07:32,14/Jun/23 07:08
Bug,IGNITE-19086,13529500,Fix NullPointerException on building indexes if the number of replicas is more than one,"Fix NullPointerException on building indexes if the number of replicas is more than one. Fall is not stable.
{noformat}
Caused by: java.lang.NullPointerException
	at org.apache.ignite.internal.sql.engine.schema.IgniteTableImpl.lambda$insertAll$2(IgniteTableImpl.java:417)
	at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
	at org.apache.ignite.internal.replicator.ReplicaService.lambda$sendToReplica$2(ReplicaService.java:147)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
	at org.apache.ignite.network.DefaultMessagingService.onInvokeResponse(DefaultMessagingService.java:363)
	at org.apache.ignite.network.DefaultMessagingService.onMessage(DefaultMessagingService.java:328)
	at org.apache.ignite.network.DefaultMessagingService.lambda$onMessage$3(DefaultMessagingService.java:306)
	... 3 more
{noformat}
",ktkalenko@gridgain.com,ktkalenko@gridgain.com,Major,Resolved,Fixed,21/Mar/23 19:07,20/Apr/23 12:02
Bug,IGNITE-19088,13529554,Sql. Investigate why severals tests ItTableRaftSnapshotsTest hang/fail.,"After enabling distrubuted execution of DML queries in https://issues.apache.org/jira/browse/IGNITE-18225, performing DML in several test cases in ItTableRaftSnapshotsTest fail to terminate.
Internally the code hangs indefinitely at 

some_node-sql-execution-pool-0
{code:java}
 java.lang.Thread.State: WAITING (parking)
	at jdk.internal.misc.Unsafe.park(java.base@11.0.14.1/Native Method)
	- parking to wait for  <0x00000005daab1fe8> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.14.1/LockSupport.java:194)
	at java.util.concurrent.CompletableFuture$Signaller.block(java.base@11.0.14.1/CompletableFuture.java:1796)
	at java.util.concurrent.ForkJoinPool.managedBlock(java.base@11.0.14.1/ForkJoinPool.java:3128)
	at java.util.concurrent.CompletableFuture.waitingGet(java.base@11.0.14.1/CompletableFuture.java:1823)
	at java.util.concurrent.CompletableFuture.join(java.base@11.0.14.1/CompletableFuture.java:2043)
	at org.apache.ignite.internal.sql.engine.message.MessageServiceImpl.send(MessageServiceImpl.java:102)
	at org.apache.ignite.internal.sql.engine.exec.ExchangeServiceImpl.request(ExchangeServiceImpl.java:106)
	at org.apache.ignite.internal.sql.engine.exec.rel.Inbox.requestBatches(Inbox.java:341)
	at org.apache.ignite.internal.sql.engine.exec.rel.Inbox.lambda$new$0(Inbox.java:89)
	at org.apache.ignite.internal.sql.engine.exec.rel.Inbox$$Lambda$2832/0x0000000800e16040.request(Unknown Source)
	at org.apache.ignite.internal.sql.engine.exec.rel.Inbox$RemoteSource.requestNextBatchIfNeeded(Inbox.java:555)
	at org.apache.ignite.internal.sql.engine.exec.rel.Inbox.pushUnordered(Inbox.java:331)
	at org.apache.ignite.internal.sql.engine.exec.rel.Inbox.push(Inbox.java:194)
	at org.apache.ignite.internal.sql.engine.exec.rel.Inbox.doPush(Inbox.java:175)
	at org.apache.ignite.internal.sql.engine.exec.rel.Inbox$$Lambda$2835/0x0000000800e34c40.run(Unknown Source)
	at
{code}
some_node-sql-execution-pool-1:
 
{code:java}
java.lang.Thread.State: WAITING (parking)
	at jdk.internal.misc.Unsafe.park(java.base@11.0.14.1/Native Method)
	- parking to wait for  <0x00000005daabbeb0> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.14.1/LockSupport.java:194)
	at java.util.concurrent.CompletableFuture$Signaller.block(java.base@11.0.14.1/CompletableFuture.java:1796)
	at java.util.concurrent.ForkJoinPool.managedBlock(java.base@11.0.14.1/ForkJoinPool.java:3128)
	at java.util.concurrent.CompletableFuture.waitingGet(java.base@11.0.14.1/CompletableFuture.java:1823)
	at java.util.concurrent.CompletableFuture.join(java.base@11.0.14.1/CompletableFuture.java:2043)
	at org.apache.ignite.internal.sql.engine.message.MessageServiceImpl.send(MessageServiceImpl.java:102)
	at org.apache.ignite.internal.sql.engine.exec.ExecutionServiceImpl$DistributedQueryManager.sendFragment(ExecutionServiceImpl.java:495)
	at org.apache.ignite.internal.sql.engine.exec.ExecutionServiceImpl$DistributedQueryManager.lambda$execute$7(ExecutionServiceImpl.java:708)
	at org.apache.ignite.internal.sql.engine.exec.ExecutionServiceImpl$DistributedQueryManager$$Lambda$2790/0x000000080096a040.run(Unknown Source)
	at org.apache.ignite.internal.sql.engine.exec.QueryTaskExecutorImpl.lambda$execute$0(QueryTaskExecutorImpl.java:80)
	at org.a
{code}


",mzhuravkov,mzhuravkov,Major,Resolved,Fixed,22/Mar/23 07:30,03/May/23 08:32
Bug,IGNITE-19092,13529570,ItIgniteNodeRestartTest::startPartialNode does not provide configuration of sql-engine component.,"After https://issues.apache.org/jira/browse/IGNITE-18225 several tests in ItIgniteNodeRestartTest started to fail, because the sql-engine component is not configured for startPartialNode. 
Update startPartialNode to include sql-engine component. If it is possible, include all available components, so adding news components to this method in future won't be necessary.
h3. Implementation Notes

As was suggested I've added sql-engine(qryEngine) along with corresponding dependencies such as catalogManager and distributionZoneManager and that helped.

> If it is possible, include all available components, so adding news components to this method in future won't be necessary.

Well it's by design partial node, so components should be added by demand. We may elaborate proposed solution in a separate ticket, however I'd favor to unmute given tests ASAP so simplest solution of just adding sql-engine is preferable.  ",alapin,mzhuravkov,Minor,Resolved,Fixed,22/Mar/23 08:29,29/Mar/23 15:36
Bug,IGNITE-19094,13529599,Fix flaky IgniteWorkerTest#testUpdateHeartbeat,"Fix flaky *org.apache.ignite.internal.util.worker.IgniteWorkerTest#testUpdateHeartbeat*, [Tc link|https://ci.ignite.apache.org/viewLog.html?buildId=7142884&buildTypeId=ApacheIgnite3xGradle_Test_RunUnitTests&fromSakuraUI=true].",ktkalenko@gridgain.com,ktkalenko@gridgain.com,Major,Resolved,Fixed,22/Mar/23 11:02,23/Mar/23 05:06
Bug,IGNITE-19111,13529782,Storage corruption if pages changed after last checkpoint during deactivation,"During cluster deactivation we force checkpoint (with ""caches stop"" reason) and remove checkpoint listeners before actual caches stop. But if there are some activity with data pages on the node after that checkpoint, but before caches stops and next checkpoint is started, the storage can be corrupted.

Reproducer:
{code:java}
    /** {@inheritDoc} */
    @Override protected IgniteConfiguration getConfiguration(String igniteInstanceName) throws Exception {
        return super.getConfiguration(igniteInstanceName)
            .setDataStorageConfiguration(new DataStorageConfiguration()
                .setDefaultDataRegionConfiguration(new DataRegionConfiguration().setPersistenceEnabled(true))
                .setCheckpointFrequency(1_000L))
            .setFailureHandler(new StopNodeFailureHandler());
    }

    /** */
    @Test
    public void testCpAfterClusterDeactivate() throws Exception {
        IgniteEx ignite0 = startGrid(0);
        IgniteEx ignite1 = startGrid(1);

        ignite0.cluster().state(ClusterState.ACTIVE);

        ignite0.getOrCreateCache(new CacheConfiguration<>(DEFAULT_CACHE_NAME).setBackups(1)
            .setAffinity(new RendezvousAffinityFunction(false, 10)));

        try (IgniteDataStreamer<Integer, Integer> streamer = ignite0.dataStreamer(DEFAULT_CACHE_NAME)) {
            for (int i = 0; i < 100_000; i++)
                streamer.addData(i, i);
        }

        stopGrid(0);

        try (IgniteDataStreamer<Integer, Integer> streamer = ignite1.dataStreamer(DEFAULT_CACHE_NAME)) {
            streamer.allowOverwrite(true);
            for (int i = 0; i < 100_000; i++)
                streamer.addData(i, i + 1);
        }

        ignite0 = startGrid(0);
        ((GridCacheDatabaseSharedManager)ignite0.context().cache().context().database()).addCheckpointListener(new CheckpointListener() {
            @Override public void onMarkCheckpointBegin(Context ctx) {
                // No-op.
            }

            @Override public void onCheckpointBegin(Context ctx) {
                if (""caches stop"".equals(ctx.progress().reason()))
                    doSleep(1_000L);
            }

            @Override public void beforeCheckpointBegin(Context ctx) {
                // No-op.
            }
        });

        ignite0.cluster().state(ClusterState.INACTIVE);

        doSleep(2_000L);

        ignite0.cluster().state(ClusterState.ACTIVE);

        IgniteCache<Integer, Integer> cache = ignite0.cache(DEFAULT_CACHE_NAME);

        for (int i = 0; i < 100_000; i++)
            assertEquals((Integer)(i + 1), cache.get(i));
    } {code}
This reproducer shuts down the node with some probability (about 1/5 on my laptop) on activation or on last check with {{{}CorruptedTreeException{}}}.",alex_pl,alex_pl,Major,Resolved,Fixed,23/Mar/23 13:52,31/Mar/23 07:40
Bug,IGNITE-19115,13529814,Possible deadlock in handling pending cache messages when the cache is recreated,"Let's consider the following scenario:
  Precondition:
    there is a cluster of two server nodes (node A - coordinator, and node B) and an atomic cache that resides on that nodes.
    current topology version is (x, y)

Node B initiates putting a new key-value pair into the atomic cache. Let's assume the primary partition, which belongs to the key, resides on node A.

The previous step requires acquiring a gateway lock for the corresponding cache (GridCacheGateway read lock) and registering GridNearAtomicSingleUpdateFuture into the MVCC manager. It is important to note, that cache future does not acquire topology lock and so should not block PME

Concurrently, node A initiates destroying the cache. Corresponding PME will be successfully completed on the coordinator node and blocked on node B just because the gateway is already acquired

{noformat}
Thread [name=""sys-#105%dht.IgniteCacheRecreateTest1%"", id=123, state=TIMED_WAITING, blockCnt=0, waitCnt=350]
        at java.lang.Thread.sleep(Native Method)
        at o.a.i.i.util.IgniteUtils.sleep(IgniteUtils.java:8316)
        at o.a.i.i.processors.cache.GridCacheGateway.onStopped(GridCacheGateway.java:324)
        at o.a.i.i.processors.cache.GridCacheProcessor.stopGateway(GridCacheProcessor.java:2582)
        at o.a.i.i.processors.cache.GridCacheProcessor.lambda$processCacheStopRequestOnExchangeDone$1c59e5cf$1(GridCacheProcessor.java:2776)
        at o.a.i.i.processors.cache.GridCacheProcessor$$Lambda$714/770930142.apply(Unknown Source)
        at o.a.i.i.util.IgniteUtils.doInParallel(IgniteUtils.java:11628)
        at o.a.i.i.util.IgniteUtils.doInParallel(IgniteUtils.java:11530)
        at o.a.i.i.processors.cache.GridCacheProcessor.processCacheStopRequestOnExchangeDone(GridCacheProcessor.java:2755)
        at o.a.i.i.processors.cache.GridCacheProcessor.onExchangeDone(GridCacheProcessor.java:2945)
        at o.a.i.i.processors.cache.distributed.dht.preloader.GridDhtPartitionsExchangeFuture.onDone(GridDhtPartitionsExchangeFuture.java:2528)
        at o.a.i.i.processors.cache.distributed.dht.preloader.GridDhtPartitionsExchangeFuture.processFullMessage(GridDhtPartitionsExchangeFuture.java:4785)
        at o.a.i.i.processors.cache.distributed.dht.preloader.GridDhtPartitionsExchangeFuture.access$1500(GridDhtPartitionsExchangeFuture.java:161)
        at o.a.i.i.processors.cache.distributed.dht.preloader.GridDhtPartitionsExchangeFuture$4.apply(GridDhtPartitionsExchangeFuture.java:4453)
        at o.a.i.i.processors.cache.distributed.dht.preloader.GridDhtPartitionsExchangeFuture$4.apply(GridDhtPartitionsExchangeFuture.java:4441)
        at o.a.i.i.util.future.GridFutureAdapter.notifyListener(GridFutureAdapter.java:464)
        at o.a.i.i.util.future.GridFutureAdapter.listen(GridFutureAdapter.java:355)
        at o.a.i.i.processors.cache.distributed.dht.preloader.GridDhtPartitionsExchangeFuture.onReceiveFullMessage(GridDhtPartitionsExchangeFuture.java:4441)
        at o.a.i.i.processors.cache.GridCachePartitionExchangeManager.processFullPartitionUpdate(GridCachePartitionExchangeManager.java:1991)
        at o.a.i.i.processors.cache.GridCachePartitionExchangeManager$3.onMessage(GridCachePartitionExchangeManager.java:469)
        at o.a.i.i.processors.cache.GridCachePartitionExchangeManager$3.onMessage(GridCachePartitionExchangeManager.java:454)
        at o.a.i.i.processors.cache.GridCachePartitionExchangeManager$MessageHandler.apply(GridCachePartitionExchangeManager.java:3765)
        at o.a.i.i.processors.cache.GridCachePartitionExchangeManager$MessageHandler.apply(GridCachePartitionExchangeManager.java:3744)
        at o.a.i.i.processors.cache.GridCacheIoManager.processMessage(GridCacheIoManager.java:1151)
        at o.a.i.i.processors.cache.GridCacheIoManager.onMessage0(GridCacheIoManager.java:592)
        at o.a.i.i.processors.cache.GridCacheIoManager.handleMessage(GridCacheIoManager.java:393)
        at o.a.i.i.processors.cache.GridCacheIoManager.handleMessage(GridCacheIoManager.java:319)
        at o.a.i.i.processors.cache.GridCacheIoManager.access$100(GridCacheIoManager.java:110)
        at o.a.i.i.processors.cache.GridCacheIoManager$1.onMessage(GridCacheIoManager.java:309)
        at o.a.i.i.managers.communication.GridIoManager.invokeListener(GridIoManager.java:1907)
        at o.a.i.i.managers.communication.GridIoManager.processRegularMessage0(GridIoManager.java:1528)
        at o.a.i.i.managers.communication.GridIoManager.access$5300(GridIoManager.java:243)
        at o.a.i.i.managers.communication.GridIoManager$9.execute(GridIoManager.java:1421)
        at o.a.i.i.managers.communication.TraceRunnable.run(TraceRunnable.java:55)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
{noformat}

Node A initiates creating a new cache with the same name as previously destroyed.

Node A received a cache update message but it cannot be processed, because a new cache (cache with the same cacheId) is starting, so, the processing of this message should be postponed until PME is completed (In this case the GridDhtForceKeysFuture is created, and the message will not be processed until PME is completed. So, the near node will not receive a response and it will not be able to complete the previous exchange future. see IGNITE-10251).

new PME on node B cannot proceed further just because of 3.",slava.koptilin,slava.koptilin,Major,Resolved,Fixed,23/Mar/23 15:55,14/Apr/23 20:18
Bug,IGNITE-19116,13529847,Sql. UPDATE statement fails with NPE when table does not exist,"UPDATE statement fails with NPE when table does not exist.
{code:java}
@Test
public void test() {
   sql(""UPDATE unknown SET j = j + 1"");
}
{code}

Error:
{code:java}
java.lang.NullPointerException
	at org.apache.ignite.internal.sql.engine.prepare.IgniteSqlValidator.createSourceSelectForUpdate(IgniteSqlValidator.java:175)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1476)
	at org.apache.ignite.internal.sql.engine.prepare.IgniteSqlValidator.performUnconditionalRewrites(IgniteSqlValidator.java:383)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1046)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:759)
	at org.apache.ignite.internal.sql.engine.prepare.IgniteSqlValidator.validate(IgniteSqlValidator.java:135)
	at org.apache.ignite.internal.sql.engine.prepare.IgnitePlanner.validate(IgnitePlanner.java:189)
{code}

*Expected behavoir*

It should return throw objectNotFound error:

{code:java}
Object 'UNKNOWN' not found
{code}

",xtern,mzhuravkov,Minor,Resolved,Fixed,23/Mar/23 18:05,12/Apr/23 19:55
Bug,IGNITE-19121,13530196,Future returned by a dropped message send is never completed,This causes problems like IGNITE-19088. We should return a completed future without sending the message.,rpuch,rpuch,Major,Resolved,Fixed,27/Mar/23 07:59,27/Mar/23 13:34
Bug,IGNITE-19126,13530235,Make ItTableRaftSnapshotsTest less invasive,"The current approach to make sure a follower does not get any logs entries from the leader is to drop messages. This approach is too invasive (as some other code might rely on those messages); also, it will stop working after IGNITE-18712.

We should either drop only the vare minimum of the messages, or switch to stopping a node to make sure it does not get log entries.",rpuch,rpuch,Major,Resolved,Fixed,27/Mar/23 11:26,10/Apr/23 13:06
Bug,IGNITE-19127,13530251,Sql. Custom data types. Fix type inference in the presence of nullable types.,"Nullable types are least restrictive than non-nullable types. This is not true for custom data types and should be fixed.
",mzhuravkov,mzhuravkov,Minor,Resolved,Fixed,27/Mar/23 13:35,03/May/23 08:32
Bug,IGNITE-19128,13530254,Sql. Custom data types. IgniteIndexScan has incorrect types in searchBounds. ,"IgniteIndexScanNode has incorrect types in its search bounds although the filter has correct types in it condition.
Example:

{code:java}
 @Test
  public void test() {
        sql(""CREATE TABLE tx (id INTEGER PRIMARY KEY, test_key UUID)"");
        sql(""INSERT INTO tx VALUES(1, ?)"", new UUID(2, 0));
        sql(""CREATE INDEX tx_test_key_idx ON tx (test_key)"");
        String query = format(
                ""SELECT * FROM tx WHERE test_key > '{}'::UUID AND test_key < '{}'::UUID ORDER BY id"",
                new UUID(1, 0), new UUID(3, 0)
        );
        sql(query);
    }
{code}

Error:

{code:java}
Caused by: java.lang.AssertionError: storageType is class java.util.UUID value must also be class java.util.UUID but it was: 00000000-0000-0001-0000-000000000001
	at org.apache.ignite.internal.sql.engine.util.SafeCustomTypeInternalConversion.tryConvertFromInternal(SafeCustomTypeInternalConversion.java:72)
	at org.apache.ignite.internal.sql.engine.util.TypeUtils.fromInternal(TypeUtils.java:330)
	at org.apache.ignite.internal.sql.engine.exec.RowConverter.toByteBuffer(RowConverter.java:141)
	at org.apache.ignite.internal.sql.engine.exec.RowConverter.toBinaryTuplePrefix(RowConverter.java:85)
	at org.apache.ignite.internal.sql.engine.exec.rel.IndexScanNode.toBinaryTuplePrefix(IndexScanNode.java:208)
	at org.apache.ignite.internal.sql.engine.exec.rel.IndexScanNode.partitionPublisher(IndexScanNode.java:146)
{code}

Plan:

{code:java}
IgniteExchange(distribution=[single]), id = 344
  IgniteSort(sort0=[$0], dir0=[ASC]), id = 343
    IgniteIndexScan(table=[[PUBLIC, TX]], index=[TX_TEST_KEY_IDX], type=[SORTED], searchBounds=[[RangeBounds [lowerBound=_UTF-8'00000000-0000-0001-0000-000000000000', upperBound=_UTF-8'00000000-0000-0003-0000-000000000000', lowerInclude=false, upperInclude=false]]], filters=[AND(>($t1, CAST(_UTF-8'00000000-0000-0001-0000-000000000000'):UUID NOT NULL), <($t1, CAST(_UTF-8'00000000-0000-0003-0000-000000000000'):UUID NOT NULL))], requiredColumns=[{0, 1}], collation=[[1]]), id = 326
{code}

But the following works:

{code:java}
 @Test
 public void test2() {
     sql(""CREATE TABLE tx (id INTEGER PRIMARY KEY, test_key UUID)"");
     sql(""INSERT INTO tx VALUES(1, ?)"", new UUID(2, 0));
     sql(""CREATE INDEX tx_test_key_idx ON tx (test_key)"");
     sql(""SELECT * FROM tx WHERE test_key > ? AND test_key < ? ORDER BY id"", new UUID(1, 0), new UUID(3, 0));
 }
{code}

Working Plan:


{code:java}
IgniteExchange(distribution=[single]), id = 291
  IgniteSort(sort0=[$0], dir0=[ASC]), id = 290
    IgniteIndexScan(table=[[PUBLIC, TX]], index=[TX_TEST_KEY_IDX], type=[SORTED], searchBounds=[[RangeBounds [lowerBound=?0, upperBound=?1, lowerInclude=false, upperInclude=false]]], filters=[AND(>($t1, ?0), <($t1, ?1))], requiredColumns=[{0, 1}], collation=[[1]]), id = 273
{code}



",mzhuravkov,mzhuravkov,Major,Resolved,Fixed,27/Mar/23 14:03,01/Jun/23 11:11
Bug,IGNITE-19136,13530392,Handling timeout on waiting for replica readiness,"*Motivation*
There are several reasons by the replica can respond _ReplicaNotReadyException_ (storage recovery has not completed yet, indexes have not created). In this case, required sending AwaitReplicaRequest and don't try requesting any more until AwaitReplicaResponse doesn't be received.

But the reason is not obvious when we receive a timeout on waiting for the replica readiness. The result is an exception, which is easy to confuse with that we don't try handling _ReplicaNotReadyException_:
{noformat}
Replica is not ready [replicationGroupId=474283c9-a39e-431a-895f-751003052d7a_part_10, nodeName=irott_n_1]
  at app//org.apache.ignite.internal.replicator.ReplicaManager.sendReplicaUnavailableErrorResponse(ReplicaManager.java:385)
  at app//org.apache.ignite.internal.replicator.ReplicaManager.onReplicaMessageReceived(ReplicaManager.java:167)
  at app//org.apache.ignite.network.DefaultMessagingService.onMessage(DefaultMessagingService.java:358)
  at app//org.apache.ignite.network.DefaultMessagingService.lambda$onMessage$3(DefaultMessagingService.java:314)
  at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
  at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
  at java.base@11.0.17/java.lang.Thread.run(Thread.java:834)
{noformat}

*Definition of Done*
A message that describes the situation where we cannot wait for replica for timeout.
{noformat}
Could not wait for the replica become ready for the timeout [replicationGroupId=474283c9-a39e-431a-895f-751003052d7a_part_10, nodeName=irott_n_1, timeout=3000]
{noformat}
",v.pyatkov,v.pyatkov,Major,Resolved,Fixed,28/Mar/23 09:55,09/Jun/23 20:46
Bug,IGNITE-19141,13530411,Node failes on rebalance during deactivation,"Failure handler triggered if caches is stopped (for example, due to deactivation) and node is processing partitions supply message. Reproducer:
{code:java}
@Override protected FailureHandler getFailureHandler(String igniteInstanceName) {
    return new StopNodeFailureHandler();
}

@Test
public void testRebalanceOnDeactivate() throws Exception {
    IgniteEx ignite0 = startGrid(0);
    IgniteEx ignite1 = startGrid(1);
    ignite0.cluster().state(ClusterState.ACTIVE);
    ignite0.cluster().baselineAutoAdjustEnabled(false);

    for (int i = 0; i < 10; i++) {
        IgniteCache<Integer, Integer> cache = ignite0.getOrCreateCache(
            new CacheConfiguration<Integer, Integer>(DEFAULT_CACHE_NAME).setBackups(1)
                .setAffinity(new RendezvousAffinityFunction(false, 2)));

        cache.clear();

        stopGrid(0);

        try (IgniteDataStreamer<Integer, Integer> streamer = ignite1.dataStreamer(DEFAULT_CACHE_NAME)) {
            for (int j = 0; j < 100_000; j++)
                streamer.addData(j, j);
        }

        ignite0 = startGrid(0);

        ignite0.cluster().state(ClusterState.INACTIVE);

        ignite0.cluster().state(ClusterState.ACTIVE);
    }
}{code}
 Fails with:
{noformat}
 java.lang.AssertionError: stopping=false, groupName=null, caches=[]
    at org.apache.ignite.internal.processors.cache.CacheGroupContext.singleCacheContext(CacheGroupContext.java:447) ~[classes/:?]
    at org.apache.ignite.internal.processors.cache.distributed.dht.preloader.GridDhtPartitionDemander.handleSupplyMessage(GridDhtPartitionDemander.java:584) ~[classes/:?]
    at org.apache.ignite.internal.processors.cache.distributed.dht.preloader.GridDhtPreloader.lambda$handleSupplyMessage$0(GridDhtPreloader.java:346) ~[classes/:?]{noformat}",alex_pl,alex_pl,Major,Resolved,Fixed,28/Mar/23 11:22,31/Mar/23 07:30
Bug,IGNITE-19142,13530416,IncomingSnapshotCopier.cancel() blocks forever if called from multiple threads,"One of the test runs hang forever. Thread dump and process state analysis shown the following:
 # Some thread A invoked IncomingSnapshotCopier.cancel() (twice) on the same copier, which made its busyLock block any operations
 # Another thread B, trying to process an InstallSnapshotRequest, found that the leader has changed (probably, due to the cluster being shut down) and triggered 'interrupt download snapshots'
 # As a result, this thread B called IncomingSnapshotCopier.cancel() on the same copier, but its busyLock.block() (which internally just takes a write lock) blocks this thread forever (as the lock is taken by thread A on step 1)
 # Before trying to cancel the snapshot downloading, thread B took Node.writeLock. As the thread is now blocked forever, it cannot release it, so every operation on the JRaft Node is blocked, including shutdown
 # So the cluster hangs forever on stop, making the tests hang forever as well

We should make IncomingSnapshotCopier.cancel() idempotent even when called from different threads.",rpuch,rpuch,Major,Resolved,Fixed,28/Mar/23 11:46,28/Mar/23 18:40
Bug,IGNITE-19143,13530419,ClientInboundMessageHandler memory leak,"*AssignmentsChangeListener* is leaked by *ClientInboundMessageHandler*. Each of the following two lines allocate new object implementing Consumer interface, so unsubscribe does not work as expected:

{code:java}
igniteTables.addAssignmentsChangeListener(this::onPartitionAssignmentChanged);
igniteTables.removeAssignmentsChangeListener(this::onPartitionAssignmentChanged);
{code}",ptupitsyn,ptupitsyn,Major,Resolved,Fixed,28/Mar/23 12:14,29/Mar/23 07:50
Bug,IGNITE-19152,13530567,Named list support in local file configuration is broken. ,"After IGNITE-18581 we have started to store local configuration in local config file, instead of vault.

The current flow with the saving configuration to a file has a bug. In the method {{LocalFileConfigurationStorage#write}} we call {{LocalFileConfigurationStorage#saveValues}} to save configuration fields to a file, where we call {{{}LocalFileConfigurationStorage#renderHoconString{}}}. Named list value has internal id which is {{{}UUID{}}}, but {{com.typesafe}} do not support {{{}UUID{}}}, so the whole process of saving configuration to a file fails with
{noformat}
Caused by: com.typesafe.config.ConfigException$BugOrBroken: bug in method caller: not valid to create ConfigValue from: 489e16e8-3123-44a3-b27d-6e410863eb24
	at app//com.typesafe.config.impl.ConfigImpl.fromAnyRef(ConfigImpl.java:282)
	at app//com.typesafe.config.impl.PropertiesParser.fromPathMap(PropertiesParser.java:165)
	at app//com.typesafe.config.impl.PropertiesParser.fromPathMap(PropertiesParser.java:95)
	at app//com.typesafe.config.impl.ConfigImpl.fromAnyRef(ConfigImpl.java:265)
	at app//com.typesafe.config.impl.ConfigImpl.fromPathMap(ConfigImpl.java:201)
	at app//com.typesafe.config.ConfigFactory.parseMap(ConfigFactory.java:1225)
	at app//com.typesafe.config.ConfigFactory.parseMap(ConfigFactory.java:1236)
	at app//org.apache.ignite.internal.configuration.storage.LocalFileConfigurationStorage.renderHoconString(LocalFileConfigurationStorage.java:208)
	at app//org.apache.ignite.internal.configuration.storage.LocalFileConfigurationStorage.saveValues(LocalFileConfigurationStorage.java:185)
	at app//org.apache.ignite.internal.configuration.storage.LocalFileConfigurationStorage.write(LocalFileConfigurationStorage.java:138)
	at app//org.apache.ignite.internal.configuration.ConfigurationChanger.changeInternally0(ConfigurationChanger.java:606)
	at app//org.apache.ignite.internal.configuration.ConfigurationChanger.lambda$changeInternally$1(ConfigurationChanger.java:541)
{noformat}
h3. More details

The problem is trickier than it may seem.

Configuration storages receive data in ""flat"" data format, meaning that the entire tree is converted into a list of pairs:
{code:java}
[{ ""dot-separated key string"", ""serializable value"" }]{code}
LocalFileConfigurationStorage interprets keys as literal paths in HOCON representation, which is simply not correct. These keys and values also have meta-information, associated with them, such as:
 * order of elements in named list configuration
 * internal ids for named list elements

To see, what's exactly in there, you may refer to the {{{}org.apache.ignite.internal.configuration.tree.NamedListNodeTest{}}}. It has everything laid out explicitly.
h3. Proposed fix

Well, the ideal approach would be rendering the configuration more or less the same way, as we do it for REST.

It means calling {{ConfigurationUtil#fillFromPrefixMap}} for every local root.

Local roots can be retrieved using {{{}ConfigurationModule{}}}, by reading them all from the class path.

Resulting nodes are converted to maps using {{{}ConverterToMapVisitor{}}}. Then maps are converted to HOCON using its own API.

There are several hidden problems here.
 * {-}we must check, that HOCON preserves order of keys{-}, and that we use linked hash maps in {{fillFromPrefixMap}}
EDIT: HOCON sorts keys alphabetically. Ok
 * {{ConverterToMapVisitor}} does not expect null nodes, because it always works with ""full"" trees. Fixing it would require some fine-tuning, otherwise one may end up with a bunch of empty nodes in the config file, which is bad
 * {{ConverterToMapVisitor}} uses array syntax for named lists. You can see it in action in {{{}HoconConverterTest{}}}.
Yes, there are two ways of representing named lists in the system. We should make rendering mode configurable, because local configuration, at the moment, only needs basic tree representation (for node attributes)

We should also add tests for most of these improvements. First of all, to {{{}HoconConverterTest{}}}.
h3. Misc

Another extremely uncertain thing is the way we handle default values. This may be a topic for another issue, or maybe not.

For example, if user explicitly configure network port to 47500, we will fail to save it into the file, because it matches default and we ignore everything that has default value. We *need* tests for this.",aleksandr.pakhomov,maliev,Major,Resolved,Fixed,29/Mar/23 08:01,26/Apr/23 16:05
Bug,IGNITE-19169,13530777,Deadlock detected while calling MvPartitionStorage#pollForVacuum,"h2. *Update:*

For now, a possible deadlock fo *MvPartitionStorage#pollForVacuum* inside *MvPartitionStorage#runConsistently* is fine, because we need to maintain consistency when working with indexes.

It is enough for us to fix *StorageUpdateHandler#executeBatchGc* so that it runs outside the general *MvPartitionStorage#runConsistently* and each *StorageUpdateHandler#internalVacuum* is in its own *MvPartitionStorage#runConsistently*.

h2. *Problem:*

Deadlock detected while calling *org.apache.ignite.internal.storage.MvPartitionStorage#pollForVacuum*, reproducer:
{code:java}
@Test
public void testDeadLock() {
    RowId rowId2 = new RowId(PARTITION_ID);
    for (int i = 0; i < REPEATS; i++) {
        addWriteCommitted(ROW_ID, TABLE_ROW, clock.now());
        addWriteCommitted(rowId2, TABLE_ROW2, clock.now());
        addWriteCommitted(ROW_ID, TABLE_ROW, clock.now());
        addWriteCommitted(rowId2, TABLE_ROW2, clock.now());
        addWriteCommitted(ROW_ID, null, clock.now());
        addWriteCommitted(rowId2, null, clock.now());

        RunnableX remove2rowsAction = () -> {
            storage.runConsistently(() -> {
                storage.pollForVacuum(HybridTimestamp.MAX_VALUE);
                storage.pollForVacuum(HybridTimestamp.MAX_VALUE);
                return null;
            });
        };

        runRace(remove2rowsAction, remove2rowsAction);
        assertNull(storage.closestRowId(RowId.lowestRowId(PARTITION_ID)));
    }
}
{code}
",ktkalenko@gridgain.com,ktkalenko@gridgain.com,Major,Resolved,Fixed,30/Mar/23 10:10,03/Apr/23 08:00
Bug,IGNITE-19171,13530786,ItClusterInitTest.testDoubleInit is flaky,"Example failure: https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_IntegrationTests_ModuleRunner/7122199

The problem is that {{IgnitionManager.init}} sends the Init message to all nodes that were passed to it. The first two calls to {{IgnitionManager.init}} in the test only send this message to one node, while the third call sends it to both nodes. So, there's a race between sending {{ClusterState}} message from the CMG leader to the second node (after the CMG group has been created as a result of the first init call) and the third explicit {{init}} call, which may result in a CMG Raft node being started with a different configuration.",apolovtcev,apolovtcev,Major,Resolved,Fixed,30/Mar/23 11:23,31/Mar/23 08:32
Bug,IGNITE-19176,13530915,Sql. RW transaction skips rows without value.,"When we try to insert values into a single column  table using RW transaction, those values are not visible in the same transaction.

Example
{code:java}
create table test(id int primary key) // single column table
{code}
{code:java}
// case 1
start RW tx 
  insert into test values (0), (1)
  select count(*) from test // returns 0 instead of 2
finish tx{code}
{code:java}
// case 2
start RW tx
   insert into test values (0), (1)
commit tx

start RW tx
   select count(*) from test // returns 0 instead of 2
finish tx{code}

If we do this using RO transaction - all works fine.
If we add a column to the table, everything will work fine.

Reproducer
{code:java}
@Test
public void test() {
    sql(""CREATE TABLE myTbl (id INT PRIMARY KEY) WITH REPLICAS=2, PARTITIONS=10"");
    Ignite ignite = CLUSTER_NODES.get(0);
    ignite.transactions().runInTransaction(tx -> {
        sql(tx, ""INSERT INTO myTbl VALUES (0), (1)"");
        List<List<Object>> rows = sql(tx, ""SELECT count(*) from myTbl"");
        assertEquals(2L, rows.get(0).get(0));
    });
} {code}
 ",xtern,xtern,Major,Resolved,Fixed,31/Mar/23 07:56,09/May/23 04:23
Bug,IGNITE-19179,13530978,ItClusterManagerTest#testNodeRestart start to fail after LogicalNode introducing,"While https://issues.apache.org/jira/browse/IGNITE-18953 was implementing, {{ItClusterManagerTest#testNodeRestart}} started to fail periodically.

 
{noformat}
java.lang.AssertionError: 
Expected: is iterable with items [<ClusterNode [id=ac994fa6-d2d3-42e4-a247-fc437792b617, name=icmt_tnr_10000, address=127.0.0.1:10000, nodeMetadata=null]>, <ClusterNode [id=622f46cb-3f34-4aa7-8332-ce0aa90980e8, name=icmt_tnr_10001, address=127.0.0.1:10001, nodeMetadata=null]>] in any order
     but: was <[ClusterNode [id=ac994fa6-d2d3-42e4-a247-fc437792b617, name=icmt_tnr_10000, address=127.0.0.1:10000, nodeMetadata=null]]>
Expected :is iterable with items [<ClusterNode [id=ac994fa6-d2d3-42e4-a247-fc437792b617, name=icmt_tnr_10000, address=127.0.0.1:10000, nodeMetadata=null]>, <ClusterNode [id=622f46cb-3f34-4aa7-8332-ce0aa90980e8, name=icmt_tnr_10001, address=127.0.0.1:10001, nod ...Actual   :<[ClusterNode [id=ac994fa6-d2d3-42e4-a247-fc437792b617, name=icmt_tnr_10000, address=127.0.0.1:10000, nodeMetadata=null]]>
<Click to see difference>
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)
    at org.apache.ignite.internal.cluster.management.ItClusterManagerTest.testNodeRestart(ItClusterManagerTest.java:174)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
    at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
    at 
{noformat}
 

From the log with additional debug logging we can see, that after we call {{waitForLogicalTopology()}} we still write some new topology to {{{}LogicalTopologyImpl#storage{}}}, moreover we write topology that contains only learner node, or even empty topology, which is definitely wrong. Seems that there is some race and topology in {{waitForLogicalTopology()}} is not a final topology after restart, so after that assertion fails, because topology snapshot changes.",apolovtcev,maliev,Major,Resolved,Fixed,31/Mar/23 13:32,06/Apr/23 07:49
Bug,IGNITE-19182,13531159,Fix NPE on building indexes,"After the implementation of index building, fluky tests appeared:
* *org.apache.ignite.internal.runner.app.ItDataSchemaSyncTest#checkSchemasCorrectUpdate*

Stack traces of errors:
{noformat}
java.util.concurrent.CompletionException: java.lang.NullPointerException: Cannot invoke ""java.util.List.isEmpty()"" because ""batch"" is null
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:315)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:320)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1159)
	at java.base/java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:482)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.lang.NullPointerException: Cannot invoke ""java.util.List.isEmpty()"" because ""batch"" is null
	at org.apache.ignite.internal.index.IndexBuilder$BuildIndexTask.getNextRowIdForNextBatch(IndexBuilder.java:250)
	at org.apache.ignite.internal.index.IndexBuilder$BuildIndexTask.lambda$run$1(IndexBuilder.java:165)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1150)
	... 4 more
{noformat}",ktkalenko@gridgain.com,ktkalenko@gridgain.com,Major,Resolved,Fixed,03/Apr/23 04:44,11/Apr/23 12:13
Bug,IGNITE-19183,13531173,"IgniteToStringBuilder#toString(java.lang.Class<T>, T) do not properly print inherited classes","{*}Expected behaviour{*}:
{code:java}
class A {
   int f1;
}

class B extend A {
  int f2;
}
{code}
{{S.toString(B.class, this);}} will return ""B [f1 = x, f2 = y]""

{*}Actual behaviour{*}:
{{S.toString(B.class, this);}} returns ""B []""",slava.koptilin,maliev,Major,Resolved,Fixed,03/Apr/23 06:53,18/May/23 14:07
Bug,IGNITE-19185,13531202,Fix index destruction after index creation started,"It was found that if the destruction of the index occurs immediately after the start of its creation, then errors occur during its building: *org.apache.ignite.internal.sql.api.ItSqlSynchronousApiTest#ddl*
{code:java}
2023-04-02 13:53:16:882 +0300 [INFO][%issat_n_0%build-index-0][IndexBuilder] Start building the index: [table=TEST, tableId=8370aa11-b3ea-4e00-a546-0776ff12fffa, partitionId=14, index=TEST_IDX, indexId=55e899e3-5404-4409-8570-aad4459c2908]
2023-04-02 13:53:16:883 +0300 [INFO][%issat_n_0%build-index-6][IndexBuilder] Start building the index: [table=TEST, tableId=8370aa11-b3ea-4e00-a546-0776ff12fffa, partitionId=8, index=TEST_IDX, indexId=55e899e3-5404-4409-8570-aad4459c2908]
2023-04-02 13:53:16:884 +0300 [INFO][vault0][IndexManager] Index dropped [schema=PUBLIC, index=TEST_IDX]
2023-04-02 13:53:16:885 +0300 [ERROR][%issat_n_0%build-index-6][IndexBuilder] Index build error: [table=TEST, tableId=8370aa11-b3ea-4e00-a546-0776ff12fffa, partitionId=15, index=TEST_IDX, indexId=55e899e3-5404-4409-8570-aad4459c2908]
java.util.concurrent.CompletionException: org.apache.ignite.internal.storage.StorageException: IGN-STORAGE-1 TraceId:f9741480-3eaf-4c32-8220-66e9a9118d48 Index configuration for ""55e899e3-5404-4409-8570-aad4459c2908"" could not be found
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:315)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:320)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1159)
	at java.base/java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:482)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.ignite.internal.storage.StorageException: IGN-STORAGE-1 TraceId:f9741480-3eaf-4c32-8220-66e9a9118d48 Index configuration for ""55e899e3-5404-4409-8570-aad4459c2908"" could not be found
	at org.apache.ignite.internal.storage.engine.MvTableStorage.getOrCreateIndex(MvTableStorage.java:94)
	at org.apache.ignite.internal.index.IndexBuilder$BuildIndexTask.collectRowIdBatch(IndexBuilder.java:258)
	at org.apache.ignite.internal.index.IndexBuilder$BuildIndexTask.lambda$run$1(IndexBuilder.java:164)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1150)
	... 4 more
2023-04-02 13:53:16:883 +0300 [INFO][%issat_n_0%build-index-2][IndexBuilder] Start building the index: [table=TEST, tableId=8370aa11-b3ea-4e00-a546-0776ff12fffa, partitionId=1, index=TEST_IDX, indexId=55e899e3-5404-4409-8570-aad4459c2908]
2023-04-02 13:53:16:887 +0300 [ERROR][%issat_n_0%build-index-6][IndexBuilder] Index build error: [table=TEST, tableId=8370aa11-b3ea-4e00-a546-0776ff12fffa, partitionId=18, index=TEST_IDX, indexId=55e899e3-5404-4409-8570-aad4459c2908]

{code}
",ktkalenko@gridgain.com,ktkalenko@gridgain.com,Major,Resolved,Fixed,03/Apr/23 08:34,11/Apr/23 10:02
Bug,IGNITE-19187,13531227,Sql. Handle StorageRebalanceException during rowsCount estimation,"We need to handle StorageRebalanceException which may be thrown from {{org.apache.ignite.internal.storage.MvPartitionStorage#rowsCount}} during row count estimation ({{org.apache.ignite.internal.sql.engine.schema.IgniteTableImpl.StatisticsImpl#getRowCount}}).


{code:java}
Caused by: org.apache.ignite.internal.storage.StorageRebalanceException: IGN-STORAGE-4 TraceId:a943b5f5-8018-4c4b-9e66-cc5060796848 Storage in the process of rebalancing: [table=TEST, partitionId=0]
  at app//org.apache.ignite.internal.storage.util.StorageUtils.throwExceptionDependingOnStorageState(StorageUtils.java:129)
  at app//org.apache.ignite.internal.storage.util.StorageUtils.throwExceptionIfStorageNotInRunnableState(StorageUtils.java:51)
  at app//org.apache.ignite.internal.storage.pagememory.mv.AbstractPageMemoryMvPartitionStorage.throwExceptionIfStorageNotInRunnableState(AbstractPageMemoryMvPartitionStorage.java:894)
  at app//org.apache.ignite.internal.storage.pagememory.mv.AbstractPageMemoryMvPartitionStorage.lambda$rowsCount$24(AbstractPageMemoryMvPartitionStorage.java:707)
  at app//org.apache.ignite.internal.storage.pagememory.mv.AbstractPageMemoryMvPartitionStorage.busy(AbstractPageMemoryMvPartitionStorage.java:785)
  at app//org.apache.ignite.internal.storage.pagememory.mv.AbstractPageMemoryMvPartitionStorage.rowsCount(AbstractPageMemoryMvPartitionStorage.java:706)
  at app//org.apache.ignite.internal.sql.engine.schema.IgniteTableImpl$StatisticsImpl.getRowCount(IgniteTableImpl.java:551)
  at app//org.apache.calcite.prepare.RelOptTableImpl.getRowCount(RelOptTableImpl.java:238)
  at app//org.apache.ignite.internal.sql.engine.rel.ProjectableFilterableTableScan.computeSelfCost(ProjectableFilterableTableScan.java:156)
{code}
",xtern,korlov,Major,Resolved,Fixed,03/Apr/23 09:52,10/Apr/23 14:35
Bug,IGNITE-19188,13531229,Fix broken compilation in AbstractMvStorageUpdateHandlerTest,I need to fix my broken compilation in *org.apache.ignite.internal.table.distributed.AbstractMvStorageUpdateHandlerTest*.,ktkalenko@gridgain.com,ktkalenko@gridgain.com,Major,Resolved,Fixed,03/Apr/23 09:58,03/Apr/23 10:27
Bug,IGNITE-19200,13531421,DistributedQueryManager#close() fails to complete cancellation future,"var finalStepFut = compoundCancelFut.thenRun(() -> {
    queryManagerMap.remove(ctx.queryId());

    try {
        ctx.cancel().cancel();
    } catch (Exception ex) {
        // NO-OP
    }

    cancelFut.complete(null);
});

The step above must be executed after compoundCancelFut is completed regardless of whether the completion is normal or exceptional; instead, the current code only executes this code on normal completion.",rpuch,rpuch,Major,Resolved,Fixed,04/Apr/23 09:59,04/Apr/23 13:55
Bug,IGNITE-19231,13531553,Change thread pool for metastore raft group,"It was discovered that the common thread pool is used for raft group the metastorage and partitions, which can lead to deadlocks. The metastorage needs its own thread pool.",ktkalenko@gridgain.com,ktkalenko@gridgain.com,Major,Resolved,Fixed,05/Apr/23 09:02,10/Apr/23 08:18
Bug,IGNITE-19235,13531630,Enable ItGeneratedRestClientTest.initCluster(),"After optimizing the ItGeneratedRestClientTest by using the BeforeAll and AfterAll methods to start and stop the cluster once for all the tests, it was observed that the ""initCluster"" test breaks the ""logicalTopology"" tests. Specifically, the logical topology changes after the second ""initCluster"" call, which should have no effect on the cluster state.

It seems that this no longer the case and we should enable the test.",vpakhnushev,aleksandr.pakhomov,Major,Resolved,Fixed,05/Apr/23 15:47,19/Apr/23 16:16
Bug,IGNITE-19238,13531654,ItDataTypesTest and ItCreateTableDdlTest are flaky,"h3. Description & Root cause

1. ItDataTypesTest is flaky because previous ItCreateTableDdlTest tests failed to stop replicas on node stop:

!Снимок экрана от 2023-04-06 10-39-32.png!
{code:java}
java.lang.AssertionError: There are replicas alive [replicas=[b86c60a8-4ea3-4592-abef-6438cfc4cdb2_part_21, b86c60a8-4ea3-4592-abef-6438cfc4cdb2_part_6, b86c60a8-4ea3-4592-abef-6438cfc4cdb2_part_13, b86c60a8-4ea3-4592-abef-6438cfc4cdb2_part_8, b86c60a8-4ea3-4592-abef-6438cfc4cdb2_part_9, b86c60a8-4ea3-4592-abef-6438cfc4cdb2_part_11]]
    at org.apache.ignite.internal.replicator.ReplicaManager.stop(ReplicaManager.java:341)
    at org.apache.ignite.internal.app.LifecycleManager.lambda$stopAllComponents$1(LifecycleManager.java:133)
    at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)
    at org.apache.ignite.internal.app.LifecycleManager.stopAllComponents(LifecycleManager.java:131)
    at org.apache.ignite.internal.app.LifecycleManager.stopNode(LifecycleManager.java:115){code}
2. The reason why we failed to stop replicas is the race between tablesToStopInCaseOfError cleanup and adding tables to tablesByIdVv.

On TableManager stop, we stop and cleanup all table resources like replicas and raft nodes
{code:java}
public void stop() {
  ...
  Map<UUID, TableImpl> tables = tablesByIdVv.latest();  // 1*
  cleanUpTablesResources(tables); 
  cleanUpTablesResources(tablesToStopInCaseOfError);
  ...
}{code}
where tablesToStopInCaseOfError is a sort of pending tables list which one is cleared on cfg storage revision update.

tablesByIdVv *listens same storage revision update event* in order to publish tables related to the given revision or in other words make such tables accessible from tablesByIdVv.latest(); that one that is used in order to retrieve tables for cleanup on components stop (see // 1* above)
{code:java}
public TableManager(
  ... 
  tablesByIdVv = new IncrementalVersionedValue<>(registry, HashMap::new);

  registry.accept(token -> {
    tablesToStopInCaseOfError.clear();
    
    return completedFuture(null);
  });
  {code}
However inside IncrementalVersionedValue we have async storageRevision update processing
{code:java}
updaterFuture = updaterFuture.whenComplete((v, t) -> versionedValue.complete(causalityToken, localUpdaterFuture)); {code}
As a result it's possible that we will clear tablesToStopInCaseOfError before publishing same revision tables to tablesByIdVv, so that we will miss that cleared tables in tablesByIdVv.latest() which is used in TableManager#stop.
h3. Implementation Notes

1. First of all I've renamed tablesToStopInCaseOfError to pending tables, because they aren't only ...InCaseOfError.

2. I've also reworked tablesToStopInCaseOfError cleanup by substituting tablesToStopInCaseOfError.clear on revision change with
{code:java}
tablesByIdVv.get(causalityToken).thenAccept(ignored -> inBusyLock(busyLock,  ()-> {      
  pendingTables.remove(tblId);
})); {code}
meaning that we

2.1. remove specific table by id instead of whole map clear.

2.2. do that removal on corresponding table publishing wihtin tablesByIdVv.

3. That means that at some point right after the publishing but before removal it's possible to have same table both within tablesByIdVv and pendingTables thus in order not to stop same table twice (which is safe by the way because of idempotentce) I've substituted
{code:java}
cleanUpTablesResources(tables);
cleanUpTablesResources(tablesToStopInCaseOfError); {code}
with
{code:java}
Map<UUID, TableImpl> tablesToStop = Stream.concat(tablesByIdVv.latest().entrySet().stream(), pendingTables.entrySet().stream()).
        collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (v1, v2) -> v1));

cleanUpTablesResources(tablesToStop); {code}",alapin,alapin,Major,Resolved,Fixed,05/Apr/23 20:18,07/Apr/23 10:25
Bug,IGNITE-19248,13531748,Fix snapshot restore hanging if the prepare stage fails.,Snapshot restore hangs if the prepare stage fails.,NSAmelchev,NSAmelchev,Major,Resolved,Fixed,06/Apr/23 13:46,07/Apr/23 15:38
Bug,IGNITE-19252,13531777,The incremental snapshot restore operation fails if there is a node not from the baseline.,"The incremental snapshot restore operation fails if there is a node not from the baseline:


{noformat}
21:20:40.324 [disco-notifier-worker-#147%server-1%] ERROR org.apache.ignite.internal.processors.cache.persistence.snapshot.SnapshotRestoreProcess - Failed to restore snapshot cache groups [reqId=55eead09-4da7-4232-8e98-976dba117d91].
org.apache.ignite.IgniteCheckedException: Snapshot metafile cannot be read due to it doesn't exist: /work/snapshots/snp1/increments/0000000000000001/server_3.smf
	at org.apache.ignite.internal.processors.cache.persistence.snapshot.IgniteSnapshotManager.readFromFile(IgniteSnapshotManager.java:2001) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
	at org.apache.ignite.internal.processors.cache.persistence.snapshot.IgniteSnapshotManager.readIncrementalSnapshotMetadata(IgniteSnapshotManager.java:1098) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
	at org.apache.ignite.internal.processors.cache.persistence.snapshot.IncrementalSnapshotProcessor.process(IncrementalSnapshotProcessor.java:94) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
	at org.apache.ignite.internal.processors.cache.persistence.snapshot.SnapshotRestoreProcess.restoreIncrementalSnapshot(SnapshotRestoreProcess.java:1466) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
	at org.apache.ignite.internal.processors.cache.persistence.snapshot.SnapshotRestoreProcess.lambda$incrementalSnapshotRestore$35(SnapshotRestoreProcess.java:1417) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
	at org.apache.ignite.internal.processors.security.thread.SecurityAwareRunnable.run(SecurityAwareRunnable.java:51) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_201]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_201]
	at org.apache.ignite.internal.processors.security.thread.SecurityAwareRunnable.run(SecurityAwareRunnable.java:51) ~[ignite-core-15.0.0-SNAPSHOT.jar:15.0.0-SNAPSHOT]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_201]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_201]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_201]
{noformat}
",NSAmelchev,NSAmelchev,Major,Resolved,Fixed,06/Apr/23 18:50,07/Apr/23 14:04
Bug,IGNITE-19253,13531855,[Missing Tests] check fails on windows agents,"{noformat}
org.apache.ignite.tools.surefire.testsuites.CheckAllTestsInSuites
12:53:08     check
12:53:08       java.nio.file.InvalidPathException: Illegal char <:> at index 2: /C:/BuildAgent/work/6429fa5c3148cb5c/modules/tools/target/classes/
12:53:08       java.nio.file.InvalidPathException: Illegal char <:> at index 2: /C:/BuildAgent/work/6429fa5c3148cb5c/modules/tools/target/classes/
        at org.apache.ignite.tools.surefire.testsuites.CheckAllTestsInSuites.check(CheckAllTestsInSuites.java:79)
{noformat}",av,av,Major,Resolved,Fixed,07/Apr/23 11:22,20/Apr/23 10:36
Bug,IGNITE-19255,13531872,Fix broken unit tests in distribution-zones module,"In IGNITE-19105 I've changed some internal shenanigans of the MetaStorageManager (without affecting its API in any way). After that, nearly all unit tests in the {{distribution-zones}} module started to fail. Turns out it happened because of extensive mock usages that emulate behavior of the Meta Storage. So I decided to replace it with the {{StandaloneMetaStorageManager}} implementation and all hell broke loose: many tests emulate Meta Storage incorrectly, a lot of races appeared, because many methods became truly asynchronous.

This situation is very frustrating: a different component internals were changed with no API changes and a completely unrelated module is not longer able to pass its tests. Though I fixed most of the failures, some tests are still failing and I'm going to try to describe, what's wrong with them:

*{{DistributionZoneManagerScaleUpTest#testDataNodesPropagationAfterScaleUpTriggeredOnNewCluster}}* - this test tests a scenario when we start a node after logical topology was updated. I don't know how realistic is this scenario, but the problem is that ""data nodes"" don't get populated with the logical topology nodes on {{distributionZoneManager}} start, because {{scheduleTimers}} method, that get's invoked from the Meta Storage Watch, doesn't go inside the {{if (!addedNodes.isEmpty() && autoAdjustScaleUp != INFINITE_TIMER_VALUE)}} branch.

*{{DistributionZoneManagerScaleUpTest#testDataNodesPropagationForDefaultZoneAfterScaleUpTriggered}}* - same issue as above.

*{{DistributionZoneManagerScaleUpTest#testDataNodesPropagationForDefaultZoneAfterScaleDownTriggered}}* - same issue as above.

*{{DistributionZoneManagerScaleUpTest#testUpdateZoneScaleUpTriggersDataNodePropagation}}* - this test fails with the following assertion error: {_}Expected revision that is greater or equal to already seen meta storage events.{_}. This is because TestConfigurationStorage does not use the same revision as the Meta Storage, therefore their revisions can't be compared directly. This should either be converted to an integration test or it should use `DistributedConfigurationStrorage` instead.
(ticket is created https://issues.apache.org/jira/browse/IGNITE-19342)

*{{DistributionZoneManagerScaleUpTest#testUpdateZoneScaleDownTriggersDataNodePropagation}}* - same issue as above. (ticket is created https://issues.apache.org/jira/browse/IGNITE-19342)

*{{DistributionZoneManagerScaleUpTest#testDropZoneDoNotPropagateDataNodesAfterScaleUp}}* - this test is flaky, because notifications from test configuration storage and from Meta Storage Watches are not related to each other (unlike real-life Distributed Configuration Storage which is built on top of Watches), so notifications from the configuration storage and Meta Storage can arrive in a undetermined order. (ticket is created https://issues.apache.org/jira/browse/IGNITE-19342)

*{{DistributionZoneManagerScaleUpTest#testDropZoneDoNotPropagateDataNodesAfterScaleDown}}* - same issue as above.
(ticket is created https://issues.apache.org/jira/browse/IGNITE-19342)

*{{DistributionZoneManagerWatchListenerTest#testDataNodesOfDefaultZoneUpdatedOnWatchListenerEvent}}* - this test is flaky, probably due to some races between Watch and Configuration Listener execution (sometimes a retry on {{invoke}} happens and {{Mockito#verify}} fails). (ticket is created https://issues.apache.org/jira/browse/IGNITE-19342)

 

*New tests* from [https://github.com/gridgain/apache-ignite-3/tree/ignite-18756]

*DistributionZoneAwaitDataNodesTest#testRemoveZoneWhileAwaitingDataNodes* - this test must remove the zone after MetastorageTopologyListener updates the topVerTracker and before 
MetastorageDataNodesListener updates scaleUpRevisionTracker/scaleDownRevisionTracker. Now it's impossible to do it with StandaloneMetaStorageManager. (https://issues.apache.org/jira/browse/IGNITE-19343)
*DistributionZoneAwaitDataNodesTest#testScaleUpScaleDownAreChangedWhileAwaitingDataNodes* - same issue as above but here we need to update scaleUp and scaleDown instead of removing the zone. (https://issues.apache.org/jira/browse/IGNITE-19343)",Sergey Uttsel,apolovtcev,Blocker,Resolved,Fixed,07/Apr/23 13:30,17/May/23 12:00
Bug,IGNITE-19257,13531893,The cache is not destroyed if snapshot restore start cache stage failed.,Add the {{RESTORE_CACHE_GROUP_SNAPSHOT_START}} stage to the {{IncrementalSnapshotTest#testStagesFail}} test to reproduce the fail.,NSAmelchev,NSAmelchev,Major,Resolved,Fixed,07/Apr/23 15:41,08/May/23 13:28
Bug,IGNITE-19262,13532029,Sql. DDL silently ignores transaction,"In *ItSqlSynchronousApiTest*, update *checkDdl*:

{code:java}
    private static void checkDdl(boolean expectedApplied, Session ses, String sql) {
        Transaction tx = CLUSTER_NODES.get(0).transactions().begin();
        ResultSet res = ses.execute(
                tx,
                sql
        );

        assertEquals(expectedApplied, res.wasApplied());
        assertFalse(res.hasRowSet());
        assertEquals(-1, res.affectedRows());

        res.close();
        tx.rollback();
    }
{code}

All tests pass, even though we call rollback. 

DDL does not support transactions. We should throw an exception when *tx* is not null with a DDL statement to make this clear to the users.",amashenkov,ptupitsyn,Major,Resolved,Fixed,10/Apr/23 08:07,19/Apr/23 15:28
Bug,IGNITE-19263,13532030,Complete future at Checkpointer#syncUpdatedPageStores when Checkpointer shuts down,"It was found that the future did not complete on the *Checkpointer* shuts down, which can lead to hangs.
*org.apache.ignite.internal.pagememory.persistence.checkpoint.Checkpointer#syncUpdatedPageStores*
{code:java}
for (int i = 0; i < checkpointThreads; i++) {
                int threadIdx = i;

                pageWritePool.execute(() -> {
                    Map.Entry<GroupPartitionId, LongAdder> entry = queue.poll();

                    try {
                        while (entry != null) {
                            if (shutdownNow) {
                                return;
                            }

                            fsyncDeltaFile(currentCheckpointProgress, entry.getKey(), entry.getValue());

                            entry = queue.poll();
                        }

                        futures[threadIdx].complete(null);
                    } catch (Throwable t) {
                        futures[threadIdx].completeExceptionally(t);
                    }
                });
            }

            blockingSectionBegin();

            try {
                // Hangs there.
                CompletableFuture.allOf(futures).join();
            } finally {
                blockingSectionEnd();
            }
{code}
",ktkalenko@gridgain.com,ktkalenko@gridgain.com,Major,Resolved,Fixed,10/Apr/23 08:13,10/Apr/23 09:04
Bug,IGNITE-19272,13532190,Uninformative jdbc exception. The cause is not logged on the server side.,"Steps to reproduce.
{code:java}
try (Statement stmt = conn.createStatement()) {
stmt.executeUpdate(""CREATE TABLE timetest(id INT PRIMARY KEY,""
                    + ""ts TIMESTAMP, ts_tz TIMESTAMP WITH LOCAL TIME ZONE)"");

            stmt.executeUpdate(""INSERT INTO timetest VALUES ""
                    + ""(3, '2011-01-01 01:01:01', TIMESTAMP WITH LOCAL TIME ZONE '2011-01-01 01:01:01')""); // <- ok
            stmt.executeUpdate(""INSERT INTO timetest VALUES ""
                    + ""(1, '2011-01-01 01:01:01', '2011-01-01 01:01:01')""); // <- failed

}
{code}

failed with no reason why:

{noformat}
java.sql.SQLException: Exception while executing query [query=INSERT INTO timetest VALUES (3, '2011-01-01 01:01:01', '2011-01-01 01:01:01')]. Error message:IGN-CMN-65535 TraceId:ba72bbc8-a0e3-414b-a520-6eed739b3f45 Remote query execution

	at org.apache.ignite.internal.jdbc.proto.IgniteQueryErrorCode.createJdbcSqlException(IgniteQueryErrorCode.java:57)
	at org.apache.ignite.internal.jdbc.JdbcStatement.execute0(JdbcStatement.java:148)
	at org.apache.ignite.internal.jdbc.JdbcStatement.executeUpdate(JdbcStatement.java:177)
{noformat}
Also, there is no stacktrace on the server side.
",amashenkov,zstan,Major,Resolved,Fixed,11/Apr/23 13:27,08/May/23 12:59
Bug,IGNITE-19277,13532246,Add authorization of Ignite Cluster Node stop/restart operations.,"Before https://issues.apache.org/jira/browse/IGNITE-15322 IgniteCluster#restartNodes()/stopNodes() operations were authorized by the name of the internal tasks that is used during their execution - org.apache.ignite.internal.cluster.IgniteKillTask.

After https://issues.apache.org/jira/browse/IGNITE-15322 has been resolved, authorization of internal tasks is no longer performed. So IgniteCluster#restartNodes()/stopNodes()  operations are not authorized at all.

We must to fix it.",PetrovMikhail,PetrovMikhail,Critical,Resolved,Fixed,11/Apr/23 21:32,24/Apr/23 15:46
Bug,IGNITE-19278,13532285,Table hang on InternalTableImpl$PartitionScanPublisher$PartitionScanSubscription.lambda$scanBatch,"1) Create table

 
{noformat}
CREATE TABLE order_line (
    ol_w_id        bigint        NOT NULL,
    ol_d_id        bigint        NOT NULL,
    ol_o_id        bigint        NOT NULL,
    ol_number      int           NOT NULL,
    ol_i_id        bigint        NOT NULL,
    ol_delivery_d  bigint,
    ol_amount      double        NOT NULL,
    ol_supply_w_id bigint        NOT NULL,
    ol_quantity    double        NOT NULL,
    ol_dist_info   char(24)      NOT NULL,
    PRIMARY KEY (ol_w_id, ol_d_id, ol_o_id, ol_number)
);{noformat}
 

2) Populate 30k random rows into it

3) Execute query

SELECT OL_I_ID, OL_SUPPLY_W_ID, OL_QUANTITY, OL_AMOUNT, OL_DELIVERY_D   FROM order_line WHERE OL_O_ID = ?   AND OL_D_ID = ?   AND OL_W_ID = ?

Get error:

 
{noformat}
 [Code: 0, SQL State: 50000]  Failed to fetch query results [curId=3]. Error message:java.util.concurrent.CompletionException: org.apache.ignite.lang.IgniteException: IGN-CMN-65535 TraceId:276bc015-a683-4e9d-83c3-c03243d1e0a5 Remote query execution
    at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314)
    at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:319)
... 
Caused by: java.lang.IllegalStateException: Queue full
    at java.base/java.util.AbstractQueue.add(AbstractQueue.java:98)
    at org.apache.ignite.internal.sql.engine.exec.rel.StorageScanNode$SubscriberImpl.onNext(StorageScanNode.java:278)
    at org.apache.ignite.internal.util.subscription.ConcatenatedPublisher$ConcatenatedSubscriber.onNext(ConcatenatedPublisher.java:96)
    at org.apache.ignite.internal.sql.engine.exec.rel.StorageScanNode$1.onNext(StorageScanNode.java:158)
    at org.apache.ignite.internal.sql.engine.exec.rel.StorageScanNode$1.onNext(StorageScanNode.java:150)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
    at org.apache.ignite.internal.table.distributed.storage.InternalTableImpl$PartitionScanPublisher$PartitionScanSubscription.lambda$scanBatch$2(InternalTableImpl.java:1398)
    at java.base/java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:714){noformat}
Full error stack trace in attachment

Similar error on any query with this table, queries with the others works well.",xtern,Berkov,Major,Resolved,Fixed,12/Apr/23 07:13,19/May/23 06:49
Bug,IGNITE-19279,13532301,AssertionError on client request to inactive cluster,"AssertionError is raised for thin client cache operations on inactive cluster. For a such operation clients try to update affinity info. In inactive cluster the topology is not initialized (topVer=-1), and it fails while preparing cache partitions assignment:

 

 
{code:java}
 java.lang.AssertionError: AffinityTopologyVersion [topVer=-1, minorTopVer=0]
    at org.apache.ignite.internal.processors.affinity.GridAffinityAssignmentCache.cachedAffinity(GridAffinityAssignmentCache.java:809) ~[classes/:?]
    at org.apache.ignite.internal.processors.cache.GridCacheAffinityManager.assignment(GridCacheAffinityManager.java:233) ~[classes/:?]
    at org.apache.ignite.internal.processors.cache.GridCacheAffinityManager.assignment(GridCacheAffinityManager.java:218) ~[classes/:?]
    at org.apache.ignite.internal.processors.platform.client.cache.ClientCachePartitionsRequest.getCacheAssignment(ClientCachePartitionsRequest.java:170) ~[classes/:?]
    at org.apache.ignite.internal.processors.platform.client.cache.ClientCachePartitionsRequest.processCache(ClientCachePartitionsRequest.java:144) ~[classes/:?]
    at org.apache.ignite.internal.processors.platform.client.cache.ClientCachePartitionsRequest.process(ClientCachePartitionsRequest.java:103) ~[classes/:?]
    at org.apache.ignite.internal.processors.platform.client.ClientRequestHandler.handle(ClientRequestHandler.java:101) ~[classes/:?]
    at org.apache.ignite.internal.processors.odbc.ClientListenerNioListener.onMessage(ClientListenerNioListener.java:204) ~[classes/:?]
    at org.apache.ignite.internal.processors.odbc.ClientListenerNioListener.onMessage(ClientListenerNioListener.java:55) ~[classes/:?]
    at org.apache.ignite.internal.util.nio.GridNioFilterChain$TailFilter.onMessageReceived(GridNioFilterChain.java:279) ~[classes/:?]
    at org.apache.ignite.internal.util.nio.GridNioFilterAdapter.proceedMessageReceived(GridNioFilterAdapter.java:109) ~[classes/:?]
    at org.apache.ignite.internal.util.nio.GridNioAsyncNotifyFilter$3.body(GridNioAsyncNotifyFilter.java:97) ~[classes/:?]
    at org.apache.ignite.internal.util.worker.GridWorker.run(GridWorker.java:125) ~[classes/:?]
    at org.apache.ignite.internal.util.worker.GridWorkerPool$1.run(GridWorkerPool.java:70) ~[classes/:?]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_322]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_322]
    at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_322]
 
{code}
 

Reproducer:

 

 

 
{code:java}
package org.apache.ignite;
import org.apache.ignite.client.IgniteClient;
import org.apache.ignite.cluster.ClusterState;
import org.apache.ignite.configuration.CacheConfiguration;
import org.apache.ignite.configuration.ClientConfiguration;
import org.apache.ignite.configuration.DataRegionConfiguration;
import org.apache.ignite.configuration.DataStorageConfiguration;
import org.apache.ignite.configuration.IgniteConfiguration;
import org.apache.ignite.internal.IgniteEx;
import org.apache.ignite.internal.client.thin.AbstractThinClientTest;
import org.apache.ignite.internal.util.typedef.G;
import org.junit.Test;
/** */
public class Reproducer extends AbstractThinClientTest {
@Override protected IgniteConfiguration getConfiguration(String igniteInstanceName) throws Exception
{ IgniteConfiguration cfg = super.getConfiguration(igniteInstanceName); cfg.setDataStorageConfiguration(new DataStorageConfiguration() .setDefaultDataRegionConfiguration(new DataRegionConfiguration().setPersistenceEnabled(true))); cfg.setCacheConfiguration(new CacheConfiguration(""default"")); return cfg; }
/** */
@Test
public void test() throws Exception {
IgniteEx ign = startGrids(2);
ign.cluster().state(ClusterState.ACTIVE);
ClientConfiguration ccfg = getClientConfiguration(G.allGrids().toArray(new Ignite[]{}));
stopAllGrids();
startGrid(0);
IgniteClient cln = Ignition.startClient(ccfg);
cln.cache(""default"").get(0);
}
}
 
 
{code}
 ",timonin.maksim,timoninmaxim,Major,Resolved,Fixed,12/Apr/23 08:29,14/Apr/23 09:33
Bug,IGNITE-19283,13532458,JDBC URL is not taken from the session,"After connecting to the node with {{connect}} in REPL mode and running {{sql}} command without arguments, JDBC URL is constructed from the default value in config rather than from the connected node.",vpakhnushev,vpakhnushev,Major,Resolved,Fixed,13/Apr/23 07:50,17/Apr/23 17:27
Bug,IGNITE-19286,13532556,NPE in case of simultaneous cache destroy and transaction rollback,"Reproducer attached. NPE in case of simultaneous cache destroy and transaction rollback:

{noformat}
java.lang.NullPointerException: null
	at org.apache.ignite.internal.processors.cache.transactions.IgniteTxManager.notifyEvictions(IgniteTxManager.java:1967) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.transactions.IgniteTxManager.rollbackTx(IgniteTxManager.java:1723) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.transactions.IgniteTxLocalAdapter.userRollback(IgniteTxLocalAdapter.java:1103) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.near.GridNearTxLocal.localFinish(GridNearTxLocal.java:3736) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.near.GridNearTxFinishFuture.doFinish(GridNearTxFinishFuture.java:468) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.near.GridNearTxFinishFuture.finish(GridNearTxFinishFuture.java:417) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.near.GridNearTxLocal$24.apply(GridNearTxLocal.java:4032) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.near.GridNearTxLocal$24.apply(GridNearTxLocal.java:4005) ~[classes/:?]
	at org.apache.ignite.internal.util.future.GridFutureAdapter.notifyListener(GridFutureAdapter.java:464) ~[classes/:?]
	at org.apache.ignite.internal.util.future.GridFutureAdapter.unblock(GridFutureAdapter.java:348) ~[classes/:?]
	at org.apache.ignite.internal.util.future.GridFutureAdapter.unblockAll(GridFutureAdapter.java:336) ~[classes/:?]
	at org.apache.ignite.internal.util.future.GridFutureAdapter.onDone(GridFutureAdapter.java:576) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.GridCacheCompoundFuture.onDone(GridCacheCompoundFuture.java:56) ~[classes/:?]
	at org.apache.ignite.internal.util.future.GridFutureAdapter.onDone(GridFutureAdapter.java:555) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.near.GridNearOptimisticTxPrepareFuture.onComplete(GridNearOptimisticTxPrepareFuture.java:298) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.near.GridNearOptimisticTxPrepareFuture.onDone(GridNearOptimisticTxPrepareFuture.java:274) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.near.GridNearOptimisticTxPrepareFuture.onDone(GridNearOptimisticTxPrepareFuture.java:79) ~[classes/:?]
	at org.apache.ignite.internal.util.future.GridFutureAdapter.onDone(GridFutureAdapter.java:543) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.near.GridNearOptimisticTxPrepareFutureAdapter.prepareOnTopology(GridNearOptimisticTxPrepareFutureAdapter.java:201) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.near.GridNearOptimisticTxPrepareFutureAdapter.lambda$prepareOnTopology$27f50bf2$1(GridNearOptimisticTxPrepareFutureAdapter.java:234) ~[classes/:?]
	at org.apache.ignite.internal.processors.timeout.GridTimeoutProcessor$2.apply(GridTimeoutProcessor.java:181) ~[classes/:?]
	at org.apache.ignite.internal.processors.timeout.GridTimeoutProcessor$2.apply(GridTimeoutProcessor.java:173) ~[classes/:?]
	at org.apache.ignite.internal.util.future.GridFutureAdapter.notifyListener(GridFutureAdapter.java:464) ~[classes/:?]
	at org.apache.ignite.internal.util.future.GridFutureAdapter.unblock(GridFutureAdapter.java:348) ~[classes/:?]
	at org.apache.ignite.internal.util.future.GridFutureAdapter.unblockAll(GridFutureAdapter.java:336) ~[classes/:?]
	at org.apache.ignite.internal.util.future.GridFutureAdapter.onDone(GridFutureAdapter.java:576) ~[classes/:?]
	at org.apache.ignite.internal.util.future.GridFutureAdapter.onDone(GridFutureAdapter.java:555) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.dht.preloader.GridDhtPartitionsExchangeFuture.onDone(GridDhtPartitionsExchangeFuture.java:2586) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.dht.preloader.GridDhtPartitionsExchangeFuture.finishExchangeOnCoordinator(GridDhtPartitionsExchangeFuture.java:4044) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.dht.preloader.GridDhtPartitionsExchangeFuture.onAllReceived(GridDhtPartitionsExchangeFuture.java:3813) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.dht.preloader.GridDhtPartitionsExchangeFuture.processSingleMessage(GridDhtPartitionsExchangeFuture.java:3320) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.dht.preloader.GridDhtPartitionsExchangeFuture.access$100(GridDhtPartitionsExchangeFuture.java:161) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.dht.preloader.GridDhtPartitionsExchangeFuture$2.apply(GridDhtPartitionsExchangeFuture.java:3107) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.dht.preloader.GridDhtPartitionsExchangeFuture$2.apply(GridDhtPartitionsExchangeFuture.java:3095) ~[classes/:?]
	at org.apache.ignite.internal.util.future.GridFutureAdapter.notifyListener(GridFutureAdapter.java:464) ~[classes/:?]
	at org.apache.ignite.internal.util.future.GridFutureAdapter.listen(GridFutureAdapter.java:355) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.distributed.dht.preloader.GridDhtPartitionsExchangeFuture.onReceiveSingleMessage(GridDhtPartitionsExchangeFuture.java:3095) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.GridCachePartitionExchangeManager.processSinglePartitionUpdate(GridCachePartitionExchangeManager.java:2065) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.GridCachePartitionExchangeManager.access$1100(GridCachePartitionExchangeManager.java:197) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.GridCachePartitionExchangeManager$2.onMessage(GridCachePartitionExchangeManager.java:449) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.GridCachePartitionExchangeManager$2.onMessage(GridCachePartitionExchangeManager.java:417) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.GridCachePartitionExchangeManager$MessageHandler.apply(GridCachePartitionExchangeManager.java:3765) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.GridCachePartitionExchangeManager$MessageHandler.apply(GridCachePartitionExchangeManager.java:3744) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.GridCacheIoManager.processMessage(GridCacheIoManager.java:1151) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.GridCacheIoManager.onMessage0(GridCacheIoManager.java:592) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.GridCacheIoManager.handleMessage(GridCacheIoManager.java:393) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.GridCacheIoManager.handleMessage(GridCacheIoManager.java:319) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.GridCacheIoManager.access$100(GridCacheIoManager.java:110) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.GridCacheIoManager$1.onMessage(GridCacheIoManager.java:309) ~[classes/:?]
	at org.apache.ignite.internal.managers.communication.GridIoManager.invokeListener(GridIoManager.java:1907) ~[classes/:?]
	at org.apache.ignite.internal.managers.communication.GridIoManager.processRegularMessage0(GridIoManager.java:1528) ~[classes/:?]
	at org.apache.ignite.internal.managers.communication.GridIoManager.access$5300(GridIoManager.java:243) ~[classes/:?]
	at org.apache.ignite.internal.managers.communication.GridIoManager$9.execute(GridIoManager.java:1421) ~[classes/:?]
	at org.apache.ignite.internal.managers.communication.TraceRunnable.run(TraceRunnable.java:55) ~[classes/:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_352]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_352]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_352]
{noformat}
",NSAmelchev,NSAmelchev,Major,Resolved,Fixed,13/Apr/23 18:31,17/Apr/23 16:43
Bug,IGNITE-19298,13533028,.NET: Thin 3.0: MetricsTests.TestRequestsActive is flaky,"{code}
Expected: 0
  But was:  -1
   at Apache.Ignite.Tests.MetricsTests.TestRequestsActive() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/MetricsTests.cs:line 164
   at NUnit.Framework.Internal.TaskAwaitAdapter.GenericAdapter`1.BlockUntilCompleted()
   at NUnit.Framework.Internal.MessagePumpStrategy.NoMessagePumpStrategy.WaitForCompletion(AwaitAdapter awaiter)
   at NUnit.Framework.Internal.AsyncToSyncAdapter.Await(Func`1 invoke)
   at NUnit.Framework.Internal.Commands.TestMethodCommand.RunTestMethod(TestExecutionContext context)
   at NUnit.Framework.Internal.Commands.TestMethodCommand.Execute(TestExecutionContext context)
   at NUnit.Framework.Internal.Commands.BeforeAndAfterTestCommand.<>c__DisplayClass1_0.<Execute>b__0()
   at NUnit.Framework.Internal.Commands.DelegatingTestCommand.RunTestMethodInThreadAbortSafeZone(TestExecutionContext context, Action action)
1)    at Apache.Ignite.Tests.MetricsTests.TestRequestsActive() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/MetricsTests.cs:line 164
   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1.AsyncStateMachineBox`1.ExecutionContextCallback(Object s)
{code}

https://ci.ignite.apache.org/test/4249445999771063462?currentProjectId=ApacheIgnite3xGradle_Test",ptupitsyn,ptupitsyn,Major,Resolved,Fixed,18/Apr/23 10:59,02/May/23 09:15
Bug,IGNITE-19302,13533041,Phantom reads protection is broken,"{{SortedIndexLocker#acquireLockNextKey}} is poorly implemented and not tested properly. It ignores the documented contract of {{hasNext}} and {{peek}} from {{{}PeekCursor{}}}.

""Has next"" checks must essentially be replaced with {{{}peek() != null{}}}.
Unit tests, that would simulate concurrent operations, or maybe even do concurrent operations, must be implemented.",ibessonov,ibessonov,Major,Resolved,Fixed,18/Apr/23 12:30,04/May/23 17:51
Bug,IGNITE-19304,13533055,Create index with the same column twice,"Unable to create index with same column twice:
{noformat}
create table tindex(
id int primary key,
v1 int, 
v2 int);
create index titest on tindex(v1,v1){noformat}
error:
{noformat}
[Code: 0, SQL State: 50000]  Exception while executing query [query=create index titest on tindex(v1,v1)]. Error message:IGN-CMN-65535 TraceId:203a2c0c-fbf1-4143-aa47-50c97223ad84 Named List element with key ""V1"" already exists{noformat}
But documentation [https://ignite.apache.org/docs/3.0.0-beta/sql-reference/ddl#create-index] do not block it. Postgres allow to execute such DDL too.",amashenkov,Berkov,Minor,Resolved,Fixed,18/Apr/23 14:32,12/Jun/23 14:28
Bug,IGNITE-19305,13533056,Index alive after drop indexed column,"In [documentation|https://ignite.apache.org/docs/3.0.0-beta/sql-reference/ddl#alter-table-if-exists-table-drop-column-if-exists-column1-column2-int] I see: ""If the column was indexed, the index has to be dropped manually in advance by using the 'DROP INDEX' command."" But it seems like an alpha version limitation.

Unable to create index with the same name after dropping indexed column:
{noformat}
create table tindex(
id int primary key,
v1 int, 
v2 int);
create index titest on tindex(v1, v2)
alter table tindex drop column v1;
create index titest on tindex(v2);{noformat}
error:
{noformat}
[Code: 0, SQL State: 50000]  Exception while executing query [query=create index titest on tindex(v2)]. Error message:IGN-SQL-17 TraceId:5f61df67-cc61-4eb1-98be-3bbaa25a7c44 IGN-SQL-17 TraceId:5f61df67-cc61-4eb1-98be-3bbaa25a7c44 Index already exists [name=""PUBLIC"".""TITEST""]{noformat}
Expected behaviour:

1) index titest drop while dropping v1 columnd

2) index titest successfully created on the last DDL operation.",xtern,Berkov,Major,Resolved,Fixed,18/Apr/23 14:39,15/May/23 09:14
Bug,IGNITE-19306,13533061,Drop multiple column if exists without brace,"Wrong parameter name {{IF NOT EXISTS}}

[https://ignite.apache.org/docs/3.0.0-beta/sql-reference/ddl#alter-table-if-exists-table-drop-column-if-exists-column1-column2-int]

Correct one: IF EXISTS",igusev,Berkov,Trivial,Resolved,Fixed,18/Apr/23 15:15,04/May/23 12:06
Bug,IGNITE-19315,13533158,Validate node configuration on node start,"After the changes made in IGNITE-19152, the configuration validation is no longer performed on node start (see ItSslConfigurationValidationTest).

It's not clear where the validation should be applied, as it could be either in ConfigurationChanger or LocalFileConfigurationStorage. 

Additionally, we need to write more tests to ensure that the validation works as expected.",ivan.gagarkin,aleksandr.pakhomov,Critical,Resolved,Fixed,19/Apr/23 08:04,16/May/23 12:29
Bug,IGNITE-19316,13533177,ClusterConfigRegistryImpl should update its internal state,"ItIgnitePicocliCommandsTest.clusterConfigShowSuggested is flaky because ClusterConfigRegistryImpl returns an empty config. Probably, we have to log exceptions if the onConnect method. At least, we will know why it is happening.

Also, I would like to add some robust code that will try to update the internal config instance if it is empty.

 
{code:java}
org.awaitility.core.ConditionTimeoutException: Condition with alias 'For given parsed words: [cluster, config, show,  --node-name, nodeName, ]' didn't complete within 10 seconds because lambda expression in org.apache.ignite.internal.cli.core.repl.executor.ItIgnitePicocliCommandsTest: expected iterable with items [""aimem"", ""aipersist"", ""metrics"", ""rocksDb"", ""table"", ""zone"", ""security"", ""schemaSync""] in any order but no item matches: ""aimem"", ""aipersist"", ""metrics"", ""rocksDb"", ""table"", ""zone"", ""security"", ""schemaSync"" in [].
          at app//org.awaitility.core.ConditionAwaiter.await(ConditionAwaiter.java:167)
{code}
",aleksandr.pakhomov,aleksandr.pakhomov,Major,Resolved,Fixed,19/Apr/23 09:14,03/May/23 12:02
Bug,IGNITE-19318,13533205,Unable to build stand alone/fat jar with JDBC driver,"After running

 
{noformat}
gradlew ignite-jdbc:shadowJar{noformat}
in modules/jdbc/build/libs/ignite-jdbc-3.0.0-SNAPSHOT-all.jar file size is only 806 bytes.

 

{color:#d1d2d3} {color}",vpakhnushev,Berkov,Major,Resolved,Fixed,19/Apr/23 12:05,15/May/23 13:31
Bug,IGNITE-19320,13533223,NPE on connection close (if there was 0 operation),"Exception
{noformat}
WARNING: Exception in client connector pipeline [remoteAddress=/127.0.0.1:51586]: IGN-CMN-65535 TraceId:fb08f569-9e4a-4d15-80a6-dafdcbff7acd
org.apache.ignite.lang.IgniteInternalException: IGN-CMN-65535 TraceId:fb08f569-9e4a-4d15-80a6-dafdcbff7acd
    at org.apache.ignite.client.handler.ClientResourceRegistry.close(ClientResourceRegistry.java:130)
    at org.apache.ignite.client.handler.ClientInboundMessageHandler.channelInactive(ClientInboundMessageHandler.java:225)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
    at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:411)
    at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:376)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
    at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
    at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:813)
    at io.netty.util.concurrent.AbstractEventExecutor.runTask$$$capture(AbstractEventExecutor.java:174)
    at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java)
    at io.netty.util.concurrent.AbstractEventExecutor.safeExecute$$$capture(AbstractEventExecutor.java:167)
    at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java)
    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.NullPointerException{noformat}
on close connection without a single query:
{code:java}
public static void main(String[] args) throws InterruptedException {
    try (Connection conn = DriverManager.getConnection(""jdbc:ignite:thin://127.0.0.1:10800"")) {
        // NoOp.
    } catch (SQLException e) {
        throw new RuntimeException(e);
    }
} {code}
Because in 

{color:#000000}JdbcQueryEventHandlerImpl.{color}{color:#000000}JdbcConnectionContext{color}.close()

there is no check if connectionId is Null and SessionManager.session(SessionId) tries to get null from its 

{color:#0033b3}private final {color}{color:#000000}Map{color}<{color:#000000}SessionId{color}, {color:#000000}Session{color}> {color:#871094}activeSessions {color}= {color:#0033b3}new {color}ConcurrentHashMap<>();",xtern,Berkov,Major,Resolved,Fixed,19/Apr/23 13:09,02/May/23 09:12
Bug,IGNITE-19327,13533348,Fix a bug on scheduling a new low watermark update,"I found an error in the logs, I need to fix it.

{noformat}
2023-04-20 11:19:35:279 +0300 [ERROR][%sqllogic1%low-watermark-updater-0][LowWatermark] Failed to update low watermark, will schedule again: HybridTimestamp [physical=1681976065287, logical=0]
java.util.concurrent.CompletionException: java.lang.AssertionError: previous scheduled task has not finished
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314)
	at java.base/java.util.concurrent.CompletableFuture.uniRunNow(CompletableFuture.java:819)
	at java.base/java.util.concurrent.CompletableFuture.uniRunStage(CompletableFuture.java:799)
	at java.base/java.util.concurrent.CompletableFuture.thenRun(CompletableFuture.java:2121)
	at org.apache.ignite.internal.table.distributed.LowWatermark.lambda$updateLowWatermark$10(LowWatermark.java:186)
	at org.apache.ignite.internal.util.IgniteUtils.inBusyLock(IgniteUtils.java:889)
	at org.apache.ignite.internal.table.distributed.LowWatermark.updateLowWatermark(LowWatermark.java:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:264)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.AssertionError: previous scheduled task has not finished
	at org.apache.ignite.internal.table.distributed.LowWatermark.scheduleUpdateLowWatermarkBusy(LowWatermark.java:214)
	at org.apache.ignite.internal.table.distributed.LowWatermark.runGcAndScheduleUpdateLowWatermarkBusy(LowWatermark.java:208)
	at org.apache.ignite.internal.table.distributed.LowWatermark.lambda$updateLowWatermark$7(LowWatermark.java:189)
	at org.apache.ignite.internal.util.IgniteUtils.inBusyLock(IgniteUtils.java:889)
	at org.apache.ignite.internal.table.distributed.LowWatermark.lambda$updateLowWatermark$8(LowWatermark.java:186)
	at java.base/java.util.concurrent.CompletableFuture.uniRunNow(CompletableFuture.java:815)
	... 12 more
{noformat}",ktkalenko@gridgain.com,ktkalenko@gridgain.com,Major,Resolved,Fixed,20/Apr/23 08:23,20/Apr/23 10:49
Bug,IGNITE-19329,13533377,Tx. POSITIVE_INF inside SortedIndexLocker is not unique across partitions,"POSITIVE_INF is a static constant using to take lock on positive infinite if no more key available in the cursor. The problem is that this value is not unique across different partitions, resulting in cross partition interference. Assume the following case:


{code:java}
        sql(""CREATE TABLE test (id INT, aff_key INT, val INT, PRIMARY KEY (id, aff_key)) COLOCATE BY (aff_key) "");
        sql(""CREATE INDEX test_val_asc_idx ON test (val ASC)"");
        sql(""INSERT INTO test VALUES (1, 1, 1), (2, 1, 2), (3, 1, 3)"");

        Transaction tx = CLUSTER_NODES.get(0).transactions().begin();

        sql(tx, ""SELECT * FROM test WHERE val <= 1 ORDER BY val"");

        sql(""INSERT INTO test VALUES (4, 1, 4)""); // <-- this INSERT uses implicit transaction
{code}

Expected behaviour is that second insert completes without delays or errors, but {{LockException: IGN-TX-4 TraceId:cd2e7f6f-ca7b-47aa-9a42-9398517660f3 Failed to acquire a lock due to a conflict}} is thrown (according to current mode of deadlock prevention).
",v.pyatkov,korlov,Major,Resolved,Fixed,20/Apr/23 10:44,17/May/23 08:10
Bug,IGNITE-19335,13533419,CommandHandler SSL migration (from GridSslBasicContextFactory to SslContextFactory),"Almost whole Ignite use {{Factory<SSLContext>}} at SLL configuration, while CommandHandler still uses {{GridSslContextFactory}}",av,av,Major,Resolved,Fixed,20/Apr/23 15:21,28/Apr/23 11:46
Bug,IGNITE-19341,13533555,SQL: SUBSTRING function does not support NULL values (try 2),"ANSI99 SQL specification (""6.18 <string value function>"") says the following:
{code:java}
3) If <character substring function> is specified, then:
  a) Let C be the value of the <character value expression>, ..., and let S be the value of the <start position>.
  b) If <string length> is specified, then let L be the value of <string length> ...
  c) If either C, S, or L is the null value, then the result of the <character substring function> is the null value.
{code}
So, we should expect the following behavior:
{code:sql}
SUBSTRING('text' FROM 1 FOR NULL) -> NULL
SUBSTRING('text' FROM NULL FOR 2) -> NULL
SUBSTRING(NULL FROM 1 FOR 2) -> NULL
{code}

Instead, we got errors for these queries:
{code:sql}
sql-cli> SELECT SUBSTRING('text' FROM 1 FOR NULL);
SQL query execution error
Exception while executing query [query=SELECT SUBSTRING('text' FROM 1 FOR NULL);]. Error message:From line 1, column 8
to line 1, column 40: Cannot apply 'SUBSTRING' to arguments of type 'SUBSTRING(<CHAR(4)> FROM <INTEGER> FOR <NULL>)'. Supported form(s): 'SUBSTRING(<CHAR> FROM <INTEGER>)'
'SUBSTRING(<CHAR> FROM <INTEGER> FOR <INTEGER>)'
'SUBSTRING(<VARCHAR> FROM <INTEGER>)'
'SUBSTRING(<VARCHAR> FROM <INTEGER> FOR <INTEGER>)'
'SUBSTRING(<BINARY> FROM <INTEGER>)'
'SUBSTRING(<BINARY> FROM <INTEGER> FOR <INTEGER>)'
'SUBSTRING(<VARBINARY> FROM <INTEGER>)'
'SUBSTRING(<VARBINARY> FROM <INTEGER> FOR <INTEGER>)'

sql-cli> SELECT SUBSTRING('text' FROM NULL FOR 2);
SQL query execution error
Exception while executing query [query=SELECT SUBSTRING('text' FROM NULL FOR 2);]. Error message:From line 1, column 8
to line 1, column 40: Cannot apply 'SUBSTRING' to arguments of type 'SUBSTRING(<CHAR(4)> FROM <NULL> FOR <INTEGER>)'. Supported form(s): 'SUBSTRING(<CHAR> FROM <INTEGER>)'
'SUBSTRING(<CHAR> FROM <INTEGER> FOR <INTEGER>)'
'SUBSTRING(<VARCHAR> FROM <INTEGER>)'
'SUBSTRING(<VARCHAR> FROM <INTEGER> FOR <INTEGER>)'
'SUBSTRING(<BINARY> FROM <INTEGER>)'
'SUBSTRING(<BINARY> FROM <INTEGER> FOR <INTEGER>)'
'SUBSTRING(<VARBINARY> FROM <INTEGER>)'
'SUBSTRING(<VARBINARY> FROM <INTEGER> FOR <INTEGER>)'
{code}

Only such request works fine:
{code:sql}
sql-cli> SELECT SUBSTRING(NULL FROM 1 FOR 2);
╔═════════╗
║ EXPR$0  ║
╠═════════╣
║ null    ║
╚═════════╝
{code}",zstan,akhitrin,Major,Resolved,Fixed,21/Apr/23 12:42,09/Jun/23 08:31
Bug,IGNITE-19342,13533563,Use DistributedConfigurationStrorage in the several test classes of the DistributionZone module.,"{{DistributionZoneManagerScaleUpTest#testUpdateZoneScaleUpTriggersDataNodePropagation}}, {{DistributionZoneManagerScaleUpTest#testUpdateZoneScaleDownTriggersDataNodePropagation}} - this tests fail with the following assertion error: Expected revision that is greater or equal to already seen meta storage events. This is because {{TestConfigurationStorage}} does not use the same revision as the Meta Storage, therefore their revisions can't be compared directly. We should use {{DistributedConfigurationStrorage}} instead, so configuration will use the same revisions, as the metastorage.

Also {{DistributionZoneManagerScaleUpTest#testDropZoneDoNotPropagateDataNodesAfterScaleUp}}, {{DistributionZoneManagerScaleUpTest#testDropZoneDoNotPropagateDataNodesAfterScaleDown}} failures can be fixed with the introduction of {{DistributedConfigurationStrorage}} in the DistributionZoneManagerScaleUpTest - this test is flaky, because notifications from test configuration storage and from Meta Storage Watches are not related to each other (unlike real-life Distributed Configuration Storage which is built on top of Watches), so notifications from the configuration storage and Meta Storage can arrive in a undetermined order.

Also {{DistributionZoneManagerWatchListenerTest#testDataNodesOfDefaultZoneUpdatedOnWatchListenerEvent}} could be fixed after introduction of {{DistributedConfigurationStrorage}}  in DistributionZoneManagerWatchListenerTest - this test is flaky, probably due to some races between Watch and Configuration Listener execution (sometimes a retry on invoke happens and Mockito#verify fails).

Test {{DistributionZoneManagerScaleUpTest#testDataNodesPropagationAfterScaleUpTriggered}} could fail with 
{noformat}
java.lang.AssertionError: 
Expected: is <1L>
     but: was <3L>
java.lang.AssertionError:
Expected: is <1L>
     but: was <3L>
  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)
  at org.apache.ignite.internal.distributionzones.util.DistributionZonesTestUtil.assertValueInStorage(DistributionZonesTestUtil.java:211)
  at org.apache.ignite.internal.distributionzones.util.DistributionZonesTestUtil.assertZoneScaleUpChangeTriggerKey(DistributionZonesTestUtil.java:80)
  at org.apache.ignite.internal.distributionzones.DistributionZoneManagerScaleUpTest.testDataNodesPropagationAfterScaleUpTriggered(DistributionZoneManagerScaleUpTest.java:107)
{noformat}

This is highly likely connected to the same fact, that metastorage and configuration do not share the same storage
",Sergey Uttsel,maliev,Blocker,Resolved,Fixed,21/Apr/23 13:58,16/May/23 10:56
Bug,IGNITE-19343,13533568,Enable some muted test from DistributionZoneAwaitDataNodesTest,"h3. *Motivation*

When IGNITE-18756 was developed, some of the test were disabled, because we thought that after introduction of {{StandaloneMetaStorageManager}} it will be impossible to test some concurrent scenarios with MetaStorage. But seems that is it possible, see {{DistributionZoneManagerScaleUpTest#testScaleUpDidNotChangeDataNodesWhenTriggerKeyWasConcurrentlyChanged}}

Disabled tests: 
{{{}DistributionZoneAwaitDataNodesTest#testRemoveZoneWhileAwaitingDataNodes{}}}, 
{{DistributionZoneAwaitDataNodesTest#testScaleUpScaleDownAreChangedWhileAwaitingDataNodes}}
h3. *Definition of Done*

testRemoveZoneWhileAwaitingDataNodes and testScaleUpScaleDownAreChangedWhileAwaitingDataNodes are enabled.
h3. *Implementation Notes*

DistributedConfigurationStorage is used in BaseDistributionZoneManagerTest to fix an issue when metaStorageManager and zonesConfiguration use different storage and have different storage revisions. So https://issues.apache.org/jira/browse/IGNITE-19342 is partly implemented.",Sergey Uttsel,maliev,Blocker,Resolved,Fixed,21/Apr/23 14:17,15/May/23 12:19
Bug,IGNITE-19345,13533572,SQL: incorrect NULLIF behavior in some cases,"During the validation of IGNITE-18167, the following error was found:
{code:sql}
sql-cli> SELECT NULLIF(1, 1) IS NULL;
SQL query execution error
Exception while executing query [query=SELECT NULLIF(1, 1) IS NULL;]. Error message:From line 1, column 8 to line 1, column 19: Illegal mixing of types in CASE or COALESCE statement
{code}

For chars, it works correctly:
{code:sql}
sql-cli> SELECT NULLIF('1', '1') IS NULL;
╔═════════╗
║ EXPR$0  ║
╠═════════╣
║ true    ║
╚═════════╝
{code}

For slightly more complex numeric query, it also works correctly:
{code:sql}
sql-cli> SELECT x IS NULL FROM (SELECT NULLIF(1, 1) AS x);
╔═════════╗
║ EXPR$0  ║
╠═════════╣
║ true    ║
╚═════════╝
{code}",xtern,akhitrin,Major,Resolved,Fixed,21/Apr/23 14:44,16/May/23 14:26
Bug,IGNITE-19346,13533578,Allow using default trust store in client connections,"Java client incorrectly sets up SSL context so that if trust manager path is null, it doesn't fall back to the default Java trust store.
CLI also should support using default trust store for JDBC connections by introducing separate ssl-enabled property.
CLI REST client and Java client implementations are slightly different in that the CLI uses automatic key store type deduction while Java client requires it to be set explicitly.",vpakhnushev,vpakhnushev,Major,Resolved,Fixed,21/Apr/23 15:17,27/Apr/23 12:15
Bug,IGNITE-19348,13533635,Fix test build,Some methods were renamed in the IGNITE-19194 which broke test compilation after IGNITE-19036 was merged.,vpakhnushev,vpakhnushev,Blocker,Resolved,Fixed,22/Apr/23 12:19,24/Apr/23 10:02
Bug,IGNITE-19350,13533720,Fix assertion error in case of empty SQL query,"Empty SQL query throws AssertionError or IndexOutOfBoundsException. SqlParseException is more suitable for this case.

{noformat}
[20:43:59,672][SEVERE][client-connector-#84%48a3ed6f-4945-42c9-a5f6-09c1f1d26e35%][ClientListenerNioListener] Failed to process client request [req=o.a.i.i.processors.platform.client.cache.ClientCacheSqlFieldsQueryRequest@5fa679b6, msg=null]
java.lang.AssertionError
	at org.apache.ignite.internal.processors.platform.client.cache.ClientCacheSqlFieldsQueryRequest.process(ClientCacheSqlFieldsQueryRequest.java:152)
	at org.apache.ignite.internal.processors.platform.client.ClientRequestHandler.handle(ClientRequestHandler.java:101)
	at org.apache.ignite.internal.processors.odbc.ClientListenerNioListener.onMessage(ClientListenerNioListener.java:209)
	at org.apache.ignite.internal.processors.odbc.ClientListenerNioListener.onMessage(ClientListenerNioListener.java:55)
	at org.apache.ignite.internal.util.nio.GridNioFilterChain$TailFilter.onMessageReceived(GridNioFilterChain.java:279)
	at org.apache.ignite.internal.util.nio.GridNioFilterAdapter.proceedMessageReceived(GridNioFilterAdapter.java:109)
	at org.apache.ignite.internal.util.nio.GridNioAsyncNotifyFilter$3.body(GridNioAsyncNotifyFilter.java:97)
	at org.apache.ignite.internal.util.worker.GridWorker.run(GridWorker.java:125)
	at org.apache.ignite.internal.util.worker.GridWorkerPool$1.run(GridWorkerPool.java:70)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[20:43:59] (err) Unexpected exception message.org.apache.ignite.client.ClientException: Ignite failed to process request [3]: 50000: null (server status code [1])
{noformat}

{noformat}
[20:42:55] (err) Unexpected exception.java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.rangeCheck(ArrayList.java:659)
	at java.util.ArrayList.get(ArrayList.java:435)
	at org.apache.ignite.internal.processors.query.GridQueryProcessor.querySqlFields(GridQueryProcessor.java:2985)
{noformat}
",NSAmelchev,NSAmelchev,Major,Resolved,Fixed,23/Apr/23 17:48,24/Apr/23 14:41
Bug,IGNITE-19353,13533841,Sql. Incorrect type conversion for dynamic parameters - CAST operation ignores type precision.,"Current implementation of expression execution runtime incorrectly translates types of dynamic parameters because (a) it losses type informations of dynamic parameters (see https://issues.apache.org/jira/browse/IGNITE-18831), (b) it goes against the rules of calcite's enumerables/link4j (on which the code is based), which expect dynamic parameters to be converted into their java values according to their inferred types.

The following code illustrates the problem with character types:
{code:java}
 @Test
    public void test() {
        assertQuery(""SELECT CAST(? AS VARCHAR(2))"").withParams(""abcd"").returns(""ab"").check();
    }
{code}

The code returns `abcd` when `ab` is expected.

Problem with numeric types:

{code:java}
 @Test
    public void test() {
        assertQuery(""SELECT CAST(? AS DECIMAL(2))"").withParams(new BigDecimal(""123"")).check();
    }
{code}

The code returns original value where Postgres/Oracle/SQL Server return conversion error. 

*Solution*: convert values of dynamic parameters to java values according to type information inferred at the validation stage and pass converted values to expression execution runtime.


",mzhuravkov,mzhuravkov,Major,Resolved,Fixed,24/Apr/23 12:14,28/Jun/23 14:45
Bug,IGNITE-19354,13533843,Java thin 3.0: Same schema version is retrieved multiple times in concurrent scenarios,"When *ClientTable.getSchema* is called, we send a request for the given schema version every time, even if another request for that version is active (e.g. in case of multiple concurrent TUPLE_GET requests).

Instead of caching *ClientSchema*, we should cache *CompletableFuture<ClientSchema>*, and use *computeIfAbsent* to guarantee only one request for the given version. Make sure to handle failures - if a cached future is failed, send a new request.",ptupitsyn,ptupitsyn,Major,Resolved,Fixed,24/Apr/23 12:17,17/May/23 07:05
Bug,IGNITE-19355,13533846,.NET: Thin 3.0: Same schema version is retrieved multiple times in concurrent scenarios,"When *Table.ReadSchemaAsync* is called, we send a request for the given schema version every time, even if another request for that version is active (e.g. in case of multiple concurrent TUPLE_GET requests).

Instead of caching *ClientSchema*, we should cache *Task<ClientSchema>*, and use *GetOrAdd* to guarantee only one request for the given version. Make sure to handle failures - if a cached Task is failed, send a new request.",ptupitsyn,ptupitsyn,Major,Resolved,Fixed,24/Apr/23 12:23,15/May/23 10:59
Bug,IGNITE-19359,13533977,.NET: Thin client: 'Affinity keys are not supported exception' on put,"Reproducer from [StackOverflow|https://stackoverflow.com/questions/76099508/why-cant-i-add-data-with-an-affinitykey-into-an-apache-ignite-cache]

{code:C#}
using Apache.Ignite.Core;
using Apache.Ignite.Core.Cache.Affinity;
using Apache.Ignite.Core.Cache.Configuration;
using Apache.Ignite.Core.Cache.Query;
using Apache.Ignite.Core.Client;
using Apache.Ignite.Core.Client.Cache;

public record PERSONKEY(
    [property:AffinityKeyMapped] [property: QuerySqlField(NotNull = true)] long COMPANYID, 
    [property: QuerySqlField(NotNull = true)] long PERSONID);

public record PERSONVALUE(
    [property: QuerySqlField(NotNull = true)] string FIRSTNAME, 
    [property: QuerySqlField(NotNull = true)] string LASTNAME);

internal class Program
{
    public static void Main(string[] args)
    {
        var cfg = new IgniteClientConfiguration
        {
            Endpoints = new[] {""10.7.116.49:10800""},
        };
        using var client = Ignition.StartClient(cfg);

        var schemaBuilder = client.GetOrCreateCache<int, int>(new CacheClientConfiguration
        {
            Name = ""RR"", SqlSchema = ""PUBLIC""
        });

        schemaBuilder.Query(new SqlFieldsQuery(
            $@""CREATE TABLE IF NOT EXISTS PERSON (
                COMPANYID BIGINT NOT NULL,
                PERSONID BIGINT NOT NULL,
                FIRSTNAME VARCHAR NOT NULL,
                LASTNAME VARCHAR NOT NULL,
                PRIMARY KEY(COMPANYID, PERSONID)
            ) WITH """"TEMPLATE=PARTITIONED,BACKUPS=1,AFFINITY_KEY=COMPANYID,CACHE_NAME=PERSON,
                     KEY_TYPE={typeof(PERSONKEY).FullName},VALUE_TYPE={typeof(PERSONVALUE).FullName}""""""
        ) { Schema = ""PUBLIC"" }).GetAll();
        
        var cache = client.GetCache<PERSONKEY, PERSONVALUE>(""PERSON"");
        
        var key = new PERSONKEY(1, 2);
        var value = new PERSONVALUE(""JOHN"", ""SMITH"");
        
        // Throws an exception
        // Apache.Ignite.Core.Common.IgniteException: Affinity keys are not supported.
        // Object 'PERSONKEY { COMPANYID = 1, PERSONID = 2 }' has an affinity key.
        cache.Put(key, value);
        
        // Does not throw an exception
        cache.PutAll(new[]{KeyValuePair.Create(key, value)});
        
        var people = cache.Query(new SqlFieldsQuery(""SELECT * FROM PERSON WHERE COMPANYID = ?"", key.COMPANYID)).GetAll();
        // Correctly prints ""1 2 JOHN SMITH""
        Console.WriteLine(string.Join(""\n"", people.Select(p => $""{p[0]} {p[1]} {p[2]} {p[3]}"")));
    }
}
{code}

*Exception:*

{code}
Apache.Ignite.Core.Common.IgniteException: Affinity keys are not supported. Object 'PERSONKEY { COMPANYID = 1, PERSONID = 2 }' has an affinity key.
{code}

We should not throw an exception when partition can't be calculated. Instead, log a warning and bypass partition awareness.",ptupitsyn,ptupitsyn,Major,Resolved,Fixed,25/Apr/23 10:02,18/May/23 09:47
Bug,IGNITE-19364,13534013,"Release build failure on ""Assemble RPM/DEB packages"" step","Release build failure on ""Assemble RPM/DEB packages"" step due with error:  ""'.../ignitevisorcmd.sh': No such file or directory"". 
Visor command line tool was removed, apache-ignite.spec should reflect these changes.",alex_pl,alex_pl,Critical,Resolved,Fixed,25/Apr/23 14:47,25/Apr/23 15:23
Bug,IGNITE-19369,13534130,Metadata topic offset must be stored only after commit,"Improvement of {{KafkaToIgniteMetadataUpdater}} lead to error when data not returned to the consumer from the topic.
Read offsets must be stored only after consumer#commit successfully.",nizhikov,nizhikov,Blocker,Resolved,Fixed,26/Apr/23 10:47,19/May/23 09:09
Bug,IGNITE-19380,13534415,Fix NPE on snapshot create operation for in-memory cluster,"See {{testClusterSnapshotInMemoryFail}} log for details.

{noformat}
[2023-04-28T10:43:10,983][ERROR][disco-notifier-worker-#49%snapshot.PlainSnapshotTest0%][DistributedProcess] Failed to handle InitMessage [id=7777a671-1ec6-46d2-b4c4-4ac565d8a3be]
 java.lang.NullPointerException: null
	at org.apache.ignite.internal.util.future.GridFinishedFuture.chain(GridFinishedFuture.java:145) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.persistence.snapshot.IgniteSnapshotManager.initLocalSnapshotEndStage(IgniteSnapshotManager.java:1381) ~[classes/:?]
	at org.apache.ignite.internal.util.distributed.DistributedProcess.lambda$new$2(DistributedProcess.java:151) ~[classes/:?]
	at org.apache.ignite.internal.managers.discovery.GridDiscoveryManager$3.onDiscovery0(GridDiscoveryManager.java:760) [classes/:?]
	at org.apache.ignite.internal.managers.discovery.GridDiscoveryManager$3.access$7300(GridDiscoveryManager.java:547) [classes/:?]
	at org.apache.ignite.internal.managers.discovery.GridDiscoveryManager$3$NotificationTask.run(GridDiscoveryManager.java:980) [classes/:?]
	at org.apache.ignite.internal.managers.discovery.GridDiscoveryManager$DiscoveryMessageNotifierWorker.body0(GridDiscoveryManager.java:2822) [classes/:?]
	at org.apache.ignite.internal.managers.discovery.GridDiscoveryManager$DiscoveryMessageNotifierWorker.body(GridDiscoveryManager.java:2860) [classes/:?]
	at org.apache.ignite.internal.util.worker.GridWorker.run(GridWorker.java:125) [classes/:?]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_352]
{noformat}
",NSAmelchev,NSAmelchev,Major,Resolved,Fixed,28/Apr/23 07:56,28/Apr/23 20:18
Bug,IGNITE-19381,13534434,TimestampAware messages sometimes lacks timestamps,"h3. Motivation

While HybridTimestamp refactoring [~ibessonov] noticed that RW related messages might have null as a timestamp that seems to be incorrect. Probably we've missed 
{code:java}
.timestampLong(clock.nowLong());{code}
within some RW  messages, e.g. while building ReadWriteScanRetrieveBatchReplicaRequest.
h3. Definition of done

Thus it's required to:
 * add timestampLong() wherever needed.
 * add assertion that will check that TimestampAware messages have @NotNull timestamp field.
 * replace ""nullableHybridTimestamp(...)"" call with ""hybridTimestamp(...), etc. 

 ",v.pyatkov,alapin,Blocker,Resolved,Fixed,28/Apr/23 09:38,19/May/23 20:55
Bug,IGNITE-19387,13534559,Fix deployment tests,After enabling tests in IGNITE-19139 they started failing due to changes in IGNITE-19107,vpakhnushev,vpakhnushev,Major,Resolved,Fixed,30/Apr/23 12:42,02/May/23 07:12
Bug,IGNITE-19393,13534656,Java thin 3.0: testAccessLockedKeyTimesOut is flaky,"{code:java}
java.lang.AssertionError: 
Expected: a string containing ""Replication is timed out""
     but: was ""IGN-TX-4 TraceId:d54813d7-793a-4d81-9456-b7140cee2209 IGN-TX-4 TraceId:d54813d7-793a-4d81-9456-b7140cee2209 IGN-TX-4 TraceId:d54813d7-793a-4d81-9456-b7140cee2209 Failed to acquire a lock due to a conflict [txId=00000187-db61-2a99-0000-0000bb66f319, conflictingWaiter=WaiterImpl [txId=00000187-db61-2a99-0000-0000bb66f318, intendedLockMode=null, lockMode=X, ex=null, isDone=true]]""
java.lang.AssertionError:
Expected: a string containing ""Replication is timed out""
     but: was ""IGN-TX-4 TraceId:d54813d7-793a-4d81-9456-b7140cee2209 IGN-TX-4 TraceId:d54813d7-793a-4d81-9456-b7140cee2209 IGN-TX-4 TraceId:d54813d7-793a-4d81-9456-b7140cee2209 Failed to acquire a lock due to a conflict [txId=00000187-db61-2a99-0000-0000bb66f319, conflictingWaiter=WaiterImpl [txId=00000187-db61-2a99-0000-0000bb66f318, intendedLockMode=null, lockMode=X, ex=null, isDone=true]]""
  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)
  at org.apache.ignite.internal.runner.app.client.ItThinClientTransactionsTest.testAccessLockedKeyTimesOut(ItThinClientTransactionsTest.java:200)
  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.base/java.lang.reflect.Method.invoke(Method.java:566)
  at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
  at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
  at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
  at org.junit.jupiter.engine.extension.SameThreadTimeoutInvocation.proceed(SameThreadTimeoutInvocation.java:45)
  at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
  at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
  at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
  at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
  at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
  at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
  at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
  at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
  at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
  at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
  at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
  at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
  at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
  at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
  at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
  at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
  at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
  at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
  at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
  at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
  at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
  at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
  at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
  at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
  at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
  at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
  at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
  at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
  at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
  at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
  at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
  at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
  at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
  at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
  at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:99)
  at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:79)
  at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:75)
  at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61)
  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.base/java.lang.reflect.Method.invoke(Method.java:566)
  at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
  at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
  at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
  at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
  at com.sun.proxy.$Proxy5.stop(Unknown Source)
  at org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:193)
  at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)
  at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)
  at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)
  at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
  at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:133)
  at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:71)
  at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)
  at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)
------- Stderr: -------
2023-05-02 10:33:10:928 +0300 [INFO][Test worker][ItThinClientTransactionsTest] >>> Starting test: ItThinClientTransactionsTest#testAccessLockedKeyTimesOut, displayName: testAccessLockedKeyTimesOut(), workDir: /opt/buildagent/work/b8d4df1365f1f1e5/modules/runner/build/work/ItThinClientTransactionsTest/testAccessLockedKeyTimesOut_7239297144788614
2023-05-02 10:33:10:952 +0300 [WARNING][%itctt_n_3344%JRaft-Request-Processor-41][ReplicaManager] Failed to process replica request [request=ReadWriteSingleRowReplicaRequestImpl [binaryRow=org.apache.ignite.internal.schema.row.Row@78687b9a, commitPartitionId=null, groupId=efde61e4-b791-46d5-8441-b9a47bc17736_part_7, requestType=RW_GET, term=1, timestamp=HybridTimestamp [physical=1683012790948, logical=0], transactionId=00000187-db61-2a99-0000-0000bb66f319]]
java.util.concurrent.CompletionException: org.apache.ignite.internal.tx.LockException: IGN-TX-4 TraceId:d54813d7-793a-4d81-9456-b7140cee2209 Failed to acquire a lock due to a conflict [txId=00000187-db61-2a99-0000-0000bb66f319, conflictingWaiter=WaiterImpl [txId=00000187-db61-2a99-0000-0000bb66f318, intendedLockMode=null, lockMode=X, ex=null, isDone=true]]
  at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
  at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
  at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
  at java.base/java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:2094)
  at org.apache.ignite.internal.tx.impl.HeapLockManager.acquire(HeapLockManager.java:103)
  at org.apache.ignite.internal.table.distributed.HashIndexLocker.locksForLookup(HashIndexLocker.java:68)
  at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.resolveRowByPk(PartitionReplicaListener.java:1154)
  at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.processSingleEntryAction(PartitionReplicaListener.java:1624)
  at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.lambda$processRequest$1(PartitionReplicaListener.java:282)
  at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.appendTxCommand(PartitionReplicaListener.java:1200)
  at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.processRequest(PartitionReplicaListener.java:282)
  at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.lambda$invoke$0(PartitionReplicaListener.java:275)
  at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1072)
  at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
  at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
  at org.apache.ignite.internal.raft.RaftGroupServiceImpl.lambda$sendWithRetry$39(RaftGroupServiceImpl.java:538)
  at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
  at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
  at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
  at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
  at org.apache.ignite.network.DefaultMessagingService.onInvokeResponse(DefaultMessagingService.java:371)
  at org.apache.ignite.network.DefaultMessagingService.send0(DefaultMessagingService.java:194)
  at org.apache.ignite.network.DefaultMessagingService.respond(DefaultMessagingService.java:137)
  at org.apache.ignite.network.MessagingService.respond(MessagingService.java:89)
  at org.apache.ignite.raft.jraft.rpc.impl.IgniteRpcServer$NetworkRpcContext.sendResponse(IgniteRpcServer.java:233)
  at org.apache.ignite.raft.jraft.rpc.RpcRequestProcessor.handleRequest(RpcRequestProcessor.java:52)
  at org.apache.ignite.raft.jraft.rpc.RpcRequestProcessor.handleRequest(RpcRequestProcessor.java:29)
  at org.apache.ignite.raft.jraft.rpc.impl.IgniteRpcServer$RpcMessageHandler.lambda$onReceived$0(IgniteRpcServer.java:192)
  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
  at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.ignite.internal.tx.LockException: IGN-TX-4 TraceId:d54813d7-793a-4d81-9456-b7140cee2209 Failed to acquire a lock due to a conflict [txId=00000187-db61-2a99-0000-0000bb66f319, conflictingWaiter=WaiterImpl [txId=00000187-db61-2a99-0000-0000bb66f318, intendedLockMode=null, lockMode=X, ex=null, isDone=true]]
  at org.apache.ignite.internal.tx.impl.HeapLockManager$LockState.lockException(HeapLockManager.java:303)
  at org.apache.ignite.internal.tx.impl.HeapLockManager$LockState.isWaiterReadyToNotify(HeapLockManager.java:281)
  at org.apache.ignite.internal.tx.impl.HeapLockManager$LockState.tryAcquire(HeapLockManager.java:230)
  at org.apache.ignite.internal.tx.impl.HeapLockManager.acquire(HeapLockManager.java:95)
  ... 26 more
2023-05-02 10:33:10:961 +0300 [WARNING][ForkJoinPool.commonPool-worker-3][ClientInboundMessageHandler] Error processing client request [id=5, op=12, remoteAddress=/127.0.0.1:60078]:org.apache.ignite.tx.TransactionException: IGN-TX-4 TraceId:d54813d7-793a-4d81-9456-b7140cee2209 Failed to acquire a lock due to a conflict [txId=00000187-db61-2a99-0000-0000bb66f319, conflictingWaiter=WaiterImpl [txId=00000187-db61-2a99-0000-0000bb66f318, intendedLockMode=null, lockMode=X, ex=null, isDone=true]]
java.util.concurrent.CompletionException: org.apache.ignite.tx.TransactionException: IGN-TX-4 TraceId:d54813d7-793a-4d81-9456-b7140cee2209 Failed to acquire a lock due to a conflict [txId=00000187-db61-2a99-0000-0000bb66f319, conflictingWaiter=WaiterImpl [txId=00000187-db61-2a99-0000-0000bb66f318, intendedLockMode=null, lockMode=X, ex=null, isDone=true]]
  at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314)
  at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:319)
  at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:932)
  at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
  at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
  at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
  at org.apache.ignite.internal.replicator.ReplicaService.lambda$sendToReplica$3(ReplicaService.java:159)
  at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
  at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
  at java.base/java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:479)
  at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
  at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
  at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
  at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
  at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
Caused by: org.apache.ignite.tx.TransactionException: IGN-TX-4 TraceId:d54813d7-793a-4d81-9456-b7140cee2209 Failed to acquire a lock due to a conflict [txId=00000187-db61-2a99-0000-0000bb66f319, conflictingWaiter=WaiterImpl [txId=00000187-db61-2a99-0000-0000bb66f318, intendedLockMode=null, lockMode=X, ex=null, isDone=true]]
  at org.apache.ignite.internal.util.ExceptionUtils.lambda$withCause$0(ExceptionUtils.java:346)
  at org.apache.ignite.internal.util.ExceptionUtils.withCauseInternal(ExceptionUtils.java:432)
  at org.apache.ignite.internal.util.ExceptionUtils.withCause(ExceptionUtils.java:346)
  at org.apache.ignite.internal.table.distributed.storage.InternalTableImpl.wrapReplicationException(InternalTableImpl.java:1472)
  at org.apache.ignite.internal.table.distributed.storage.InternalTableImpl.lambda$postEnlist$9(InternalTableImpl.java:480)
  at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
  at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
  at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
  at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
  at org.apache.ignite.internal.table.distributed.storage.InternalTableImpl.lambda$enlistWithRetry$5(InternalTableImpl.java:459)
  at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
  at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
  at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
  at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
  at org.apache.ignite.internal.replicator.ReplicaService.lambda$sendToReplica$3(ReplicaService.java:156)
  ... 8 more
Caused by: org.apache.ignite.internal.tx.LockException: IGN-TX-4 TraceId:d54813d7-793a-4d81-9456-b7140cee2209 Failed to acquire a lock due to a conflict [txId=00000187-db61-2a99-0000-0000bb66f319, conflictingWaiter=WaiterImpl [txId=00000187-db61-2a99-0000-0000bb66f318, intendedLockMode=null, lockMode=X, ex=null, isDone=true]]
  at org.apache.ignite.internal.tx.impl.HeapLockManager$LockState.lockException(HeapLockManager.java:303)
  at org.apache.ignite.internal.tx.impl.HeapLockManager$LockState.isWaiterReadyToNotify(HeapLockManager.java:281)
  at org.apache.ignite.internal.tx.impl.HeapLockManager$LockState.tryAcquire(HeapLockManager.java:230)
  at org.apache.ignite.internal.tx.impl.HeapLockManager.acquire(HeapLockManager.java:95)
  at org.apache.ignite.internal.table.distributed.HashIndexLocker.locksForLookup(HashIndexLocker.java:68)
  at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.resolveRowByPk(PartitionReplicaListener.java:1154)
  at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.processSingleEntryAction(PartitionReplicaListener.java:1624)
  at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.lambda$processRequest$1(PartitionReplicaListener.java:282)
  at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.appendTxCommand(PartitionReplicaListener.java:1200)
  at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.processRequest(PartitionReplicaListener.java:282)
  at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.lambda$invoke$0(PartitionReplicaListener.java:275)
  at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1072)
  at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
  at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
  at org.apache.ignite.internal.raft.RaftGroupServiceImpl.lambda$sendWithRetry$39(RaftGroupServiceImpl.java:538)
  at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
  at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
  at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
  at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
  at org.apache.ignite.network.DefaultMessagingService.onInvokeResponse(DefaultMessagingService.java:371)
  at org.apache.ignite.network.DefaultMessagingService.send0(DefaultMessagingService.java:194)
  at org.apache.ignite.network.DefaultMessagingService.respond(DefaultMessagingService.java:137)
  at org.apache.ignite.network.MessagingService.respond(MessagingService.java:89)
  at org.apache.ignite.raft.jraft.rpc.impl.IgniteRpcServer$NetworkRpcContext.sendResponse(IgniteRpcServer.java:233)
  at org.apache.ignite.raft.jraft.rpc.RpcRequestProcessor.handleRequest(RpcRequestProcessor.java:52)
  at org.apache.ignite.raft.jraft.rpc.RpcRequestProcessor.handleRequest(RpcRequestProcessor.java:29)
  at org.apache.ignite.raft.jraft.rpc.impl.IgniteRpcServer$RpcMessageHandler.lambda$onReceived$0(IgniteRpcServer.java:192)
  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
  at java.base/java.lang.Thread.run(Thread.java:834)
2023-05-02 10:33:10:966 +0300 [INFO][Test worker][ItThinClientTransactionsTest] >>> Stopping test: ItThinClientTransactionsTest#testAccessLockedKeyTimesOut, displayName: testAccessLockedKeyTimesOut(), cost: 39ms.
{code}
[https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_IntegrationTests_ModuleRunner/7212281?hideProblemsFromDependencies=false&hideTestsFromDependencies=false&expandBuildProblemsSection=true&expandBuildTestsSection=true&expandBuildChangesSection=true]
h3. Upd 1

As [~ptupitsyn] mentioned
{code:java}
        Transaction tx1 = client().transactions().begin();
        Transaction tx2 = client().transactions().begin();

        kvView.put(tx2, -100, ""1"");

        var ex = assertThrows(IgniteException.class, () -> kvView.get(tx1, -100));
        assertThat(ex.getMessage(), containsString(""Replication is timed out"")); {code}
thows both {{TransactionException}}{color:#1d1c1d}  with  {color}{{IGN-REP-3 Rreplication is timed out}}{color:#1d1c1d}  message, and sometimes also {color}{{{}IGN-TX-4 Failed to acquire a lock due to a conflict{}}}{color:#1d1c1d}.{color}",v.pyatkov,ptupitsyn,Blocker,Resolved,Fixed,02/May/23 07:43,03/Jul/23 12:42
Bug,IGNITE-19394,13534658,RW index scan may skip entries because of GC,"The problem is in a peek cursor contract.

Right now, SortedIndexLocker#acquireLockNextKey assumes that if ""peek"" returned a value with successfully acquired lock, ""next"" will iterate over the same value.

This is not true, if the result of ""peek"" call was removed by GC. In such case, ""next"" will actually skip the real next value, that should have been returned to the user.

We must introduce a new invariant - ""hasNext"" and ""next"" always take into account latest result of ""peek"", it it's been called *after* previous ""hasNext""/""next"". This will fix the bug.

Tricky concurrent test required.
h4. Tangent issues

Two consecutive calls of ""peek"", before and after the lock, make us scan index twice. Not sure if there's an easy way to reduce the amount of reads though.

Also, RW and RO cursors should probably be implemented differently, forcing all these features into a single implementation is nonsense. RO cursor should never peek anything, it may even throw an exception. This will allow it to have internal cache, for example, making it much more efficient.",ibessonov,ibessonov,Major,Resolved,Fixed,02/May/23 07:50,05/May/23 11:59
Bug,IGNITE-19410,13534828,Node failure in case multiple nodes  join and leave a cluster simultaneously with security is enabled.,"The case when nodes with security enabled join and leave the cluster simultaneously can cause the joining nodes to fail with the following exception:


{code:java}
[2023-05-03T14:54:31,208][ERROR][disco-notifier-worker-#332%ignite.NodeSecurityContextTest2%][IgniteTestResources] Critical system error detected. Will be handled accordingly to configured handler [hnd=StopNodeOrHaltFailureHandler [tryStop=false, timeout=0, super=AbstractFailureHandler [ignoredFailureTypes=UnmodifiableSet [SYSTEM_WORKER_BLOCKED, SYSTEM_CRITICAL_OPERATION_TIMEOUT]]], failureCtx=FailureContext [type=SYSTEM_WORKER_TERMINATION, err=java.lang.IllegalStateException: Failed to find security context for subject with given ID : 4725544a-f144-4486-a705-46b2ac200011]]
 java.lang.IllegalStateException: Failed to find security context for subject with given ID : 4725544a-f144-4486-a705-46b2ac200011
    at org.apache.ignite.internal.processors.security.IgniteSecurityProcessor.withContext(IgniteSecurityProcessor.java:164) ~[classes/:?]
    at org.apache.ignite.internal.managers.discovery.GridDiscoveryManager$3$SecurityAwareNotificationTask.run(GridDiscoveryManager.java:949) ~[classes/:?]
    at org.apache.ignite.internal.managers.discovery.GridDiscoveryManager$DiscoveryMessageNotifierWorker.body0(GridDiscoveryManager.java:2822) ~[classes/:?]
    at org.apache.ignite.internal.managers.discovery.GridDiscoveryManager$DiscoveryMessageNotifierWorker.body(GridDiscoveryManager.java:2860) [classes/:?]
    at org.apache.ignite.internal.util.worker.GridWorker.run(GridWorker.java:125) [classes/:?]
    at java.lang.Thread.run(Thread.java:750) [?:1.8.0_351] {code}
Reproducer is attached.

Simplified steps that leads to the failure:

1. The client node sends an arbitrary discovery message which produces an acknowledgement message when it processed by the all cluster nodes .
2. The client node gracefully leaves the cluster.
3. The new node joins the cluster and receives a topology snapshot that does not include the left client node.
4. The new node receives an acknowledgment for the message from the step 1 and fails during its processing because message originator node is not listed in the current discovery cache or discovery cache history (see IgniteSecurityProcessor#withContext(java.util.UUID)) . This is because currently the GridDiscoveryManager#historicalNode method only aware of the topology history that occurs after a node has joined the cluster. The complete cluster topology history that exists at the time a new node joined the cluster is stored in GridDiscoveryManager#topHist and is not taken into account by the GridDiscoveryManager#historicalNode method.

 ",PetrovMikhail,PetrovMikhail,Major,Resolved,Fixed,03/May/23 12:34,06/Jun/23 15:40
Bug,IGNITE-19412,13534831,Potential memory leak in MailboxRegistryImpl under intensive SQL load,"I tried to run a SQL benchmark (slightly modified [Benchbase TPC-C scenario|https://github.com/cmu-db/benchbase/wiki/TPC-C]) against Ignite3 (commit hash b48ddcba7cd2bd3b9a053ae131c25b44a0400e27).

Ignite3 was running with -Xmx6G -Xms6G memory limit. After some time it has reached memory limits and failed due to OutOfMemoryError. A heap dump was generated on failure. I can't attach this dump due to its size, but I've opened it in MAT and created a report (see zip file attached).

The report says that most of the retained heap (almost 5G) is occupied by a single instance of o.a.i.internal.sql.engine.exec.MailboxRegistryImpl class (see image attached).

{*}Hypothesis{*}: on an intense load, MailboxRegistryImpl creates a lot of lambda expressions. This happens faster than they're being utilized. With limited memory size, it could lead to OOM at some moment.

Benchmark is quite trivial by mechanics:

1. Create a table structure (see DDL file attached)
2. Fill tables with randomized data (several tens of thousands rows is enough)
3. Run select queries in a loop (actually, it wouldn't happen; OOM was raised on data insertion).",korlov,akhitrin,Major,Resolved,Fixed,03/May/23 12:53,16/May/23 09:39
Bug,IGNITE-19416,13534869,Test JavaThinCompatibilityTest.testCurrentClientToOldServer(Version 2.14.0) fails,Test JavaThinCompatibilityTest.testCurrentClientToOldServer(Version 2.14.0) always fails after IGNITE-17449,alex_pl,alex_pl,Major,Resolved,Fixed,03/May/23 17:48,04/May/23 09:14
Bug,IGNITE-19418,13534961,Fix gradle build with JDK 17,"Running {{./gradlew check}} with JAVA_HOME set to JDK 17, build fails with the following message:
{noformat}
Could not determine the dependencies of task ':ignite-arch-test:checkstyleTest'.
> Could not resolve all task dependencies for configuration ':ignite-arch-test:testCompileClasspath'.
   > Could not resolve project :packaging.
     Required by:
         project :ignite-arch-test
      > No matching variant of project :packaging was found. The consumer was configured to find an API of a library compatible with Java 11, preferably in the form of class files, preferably optimized for standard JVMs, and its dependencies declared externally but:
          - Variant 'apiElements' capability org.apache.ignite:packaging:3.0.0-SNAPSHOT declares an API of a library, packaged as a jar, and its dependencies declared externally:
              - Incompatible because this component declares a component compatible with Java 17 and the consumer needed a component compatible with Java 11
{noformat}
",vpakhnushev,vpakhnushev,Major,Resolved,Fixed,04/May/23 10:54,08/May/23 09:59
Bug,IGNITE-19422,13535009,"Return latest data in ""get"" index cursor","During the implementation https://issues.apache.org/jira/browse/IGNITE-19394 it was noted, that ""get"" method doesn't work properly in concurrent environment. We should fix it, so that it works more like ""scan"" in sorted indexes.",ibessonov,ibessonov,Major,Resolved,Fixed,04/May/23 14:53,08/Jun/23 09:54
Bug,IGNITE-19426,13535128,Control utility is not working out of the box for slim and lgpl assemblies,During {{slim}} and {{lgpl}} assemblies creation module {{control-utility}} is copied to {{libs/optional}} instead of {{libs}}.,alex_pl,alex_pl,Major,Resolved,Fixed,05/May/23 09:08,05/May/23 12:26
Bug,IGNITE-19427,13535149,Change the default value of dataNodesAutoAdjustScaleUp for custom distribution zones to IMMEDIATE_TIMER_VALUE,"h3. *Motivation*

At https://issues.apache.org/jira/browse/IGNITE-18624 the value of DistributionZoneConfigurationSchema#dataNodesAutoAdjustScaleUp was changed to IMMEDIATE_TIMER_VALUE. But this only affected the default zone. Custom zones are still created with INFINITE_TIMER_VALUE by default. We need to change default value of dataNodesAutoAdjustScaleUp for custom zones to IMMEDIATE_TIMER_VALUE.

Also this change will fix some flaky tests, for example ItPageMemoryStorageExampleTest.testPersistentExample():
{code:java}
Caused by: java.lang.AssertionError
  at org.apache.ignite.internal.raft.RaftGroupServiceImpl.randomNode(RaftGroupServiceImpl.java:680)
  at org.apache.ignite.internal.raft.RaftGroupServiceImpl.randomNode(RaftGroupServiceImpl.java:667)
  at org.apache.ignite.internal.raft.RaftGroupServiceImpl.refreshLeader(RaftGroupServiceImpl.java:222)
  at org.apache.ignite.internal.raft.RaftGroupServiceImpl.start(RaftGroupServiceImpl.java:178) {code}
h3. *Definition of Done*

dataNodesAutoAdjustScaleUp is IMMEDIATE_TIMER_VALUE for new custom zones.",Sergey Uttsel,Sergey Uttsel,Blocker,Resolved,Fixed,05/May/23 10:48,11/May/23 15:26
Bug,IGNITE-19429,13535172,Forgot to wait for the RaftServer#raftNodeReadyFuture when start the raft node metastorage,"Forgot to wait for the *org.apache.ignite.internal.raft.server.RaftServer#raftNodeReadyFuture* when starting the raft node metastorage (*org.apache.ignite.internal.raft.Loza#startRaftGroupNode(org.apache.ignite.internal.raft.RaftNodeId, org.apache.ignite.internal.raft.PeersAndLearners, org.apache.ignite.internal.raft.service.RaftGroupListener, org.apache.ignite.internal.raft.RaftGroupEventsListener)*), which can lead to hangs on restarting the node.",ktkalenko@gridgain.com,ktkalenko@gridgain.com,Critical,Resolved,Fixed,05/May/23 13:37,05/May/23 15:24
Bug,IGNITE-19432,13535185,Tests in PartitionReplicaListenerTest are not independent,"If test methods execution order is changed, the tests start falling (for instance, this happens if one changes 'Retrive' to 'Retrieve' in method names). This is because test fixtures are static and mocks are not reset between test invocations. This can be fixed by making fixtures non-static (as nothing mandates them to be static) and use Mockito standard features to make the mocks reset properly.

While doing this, we can also clean the code of the class a bit as in IDEA too many warnings are seen.",rpuch,rpuch,Major,Resolved,Fixed,05/May/23 14:23,09/May/23 12:24
Bug,IGNITE-19439,13535468,AssertionError in RemoveWriteOnGcInvokeClosure during insertion,"I run a test that in an endless loop inserts 10k rows , clears the table, and starts over, but it failed on 4th iteration with AssertionError.

The test is as follow:
{code:java}
// org.apache.ignite.internal.sql.engine.ItDmlTest

private static final String STRING_PAYLOAD = IgniteTestUtils.randomString(new Random(), 32);

@Test
public void test() throws SQLException {
    int TABLE_SIZE = 10_000;
    sql(""CREATE TABLE t (id INT PRIMARY KEY, name VARCHAR, surname VARCHAR, age TINYINT)"");

    int iteration = 1;
    do {
        System.out.println(""Iteration "" + (iteration++) + "" started"");
        long before = System.currentTimeMillis();
        for (int i = 1; i <= TABLE_SIZE; i++) {
            sql(""INSERT INTO t VALUES (?, ?, ?, ?)"", i, STRING_PAYLOAD, STRING_PAYLOAD, (byte) i);

            if (i % 10 == 0) {
                long after = System.currentTimeMillis();
                System.out.println(i + "" rows inserted. dT="" + Duration.ofMillis(after - before));

                before = after;
            }
        }

        sql(""DELETE FROM t"");
    } while (true);
} {code}
The assertion is:
{code:java}
023-05-08 20:17:51:016 +0300 [WARNING][%idt_n_0%JRaft-Request-Processor-0][ReplicaManager] Failed to process replica request [request=ReadWriteMultiRowReplicaRequestImpl [binaryRows=ArrayList [org.apache.ignite.internal.schema.row.Row@2b6ae605], commitPartitionId=f6b9512f-b735-446e-85a3-d6b8636c847c_part_18, groupId=f6b9512f-b735-446e-85a3-d6b8636c847c_part_1, requestType=RW_INSERT_ALL, term=1, timestampLong=110334199137239040, transactionId=0187fc5e-9a26-0000-0000-000063a26a19]]
java.util.concurrent.CompletionException: org.apache.ignite.internal.storage.StorageException: IGN-STORAGE-1 TraceId:c48307b5-59a0-4895-a79d-63153a9d47c6 Error removing row version from version chain on garbage collection: [rowId=RowId [partitionId=1, uuid=ca451c56-ea47-422e-b228-67c093ae1a10], rowTimestamp=HybridTimestamp [time=110334012513058818], table=T, partitionId=1]
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314)
	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1113)
	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
	at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.lambda$processMultiEntryAction$75(PartitionReplicaListener.java:1487)
	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1106)
	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
	at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.processMultiEntryAction(PartitionReplicaListener.java:1445)
	at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.lambda$processRequest$2(PartitionReplicaListener.java:285)
	at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.appendTxCommand(PartitionReplicaListener.java:1197)
	at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.processRequest(PartitionReplicaListener.java:285)
	at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.lambda$invoke$0(PartitionReplicaListener.java:274)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1072)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
	at org.apache.ignite.internal.raft.RaftGroupServiceImpl.lambda$sendWithRetry$39(RaftGroupServiceImpl.java:538)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
	at org.apache.ignite.network.DefaultMessagingService.onInvokeResponse(DefaultMessagingService.java:371)
	at org.apache.ignite.network.DefaultMessagingService.send0(DefaultMessagingService.java:194)
	at org.apache.ignite.network.DefaultMessagingService.respond(DefaultMessagingService.java:137)
	at org.apache.ignite.network.MessagingService.respond(MessagingService.java:89)
	at org.apache.ignite.raft.jraft.rpc.impl.IgniteRpcServer$NetworkRpcContext.sendResponse(IgniteRpcServer.java:233)
	at org.apache.ignite.raft.jraft.rpc.RpcRequestProcessor.handleRequest(RpcRequestProcessor.java:52)
	at org.apache.ignite.raft.jraft.rpc.RpcRequestProcessor.handleRequest(RpcRequestProcessor.java:29)
	at org.apache.ignite.raft.jraft.rpc.impl.IgniteRpcServer$RpcMessageHandler.lambda$onReceived$0(IgniteRpcServer.java:192)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.ignite.internal.storage.StorageException: IGN-STORAGE-1 TraceId:c48307b5-59a0-4895-a79d-63153a9d47c6 Error removing row version from version chain on garbage collection: [rowId=RowId [partitionId=1, uuid=ca451c56-ea47-422e-b228-67c093ae1a10], rowTimestamp=HybridTimestamp [time=110334012513058818], table=T, partitionId=1]
	at org.apache.ignite.internal.storage.pagememory.mv.AbstractPageMemoryMvPartitionStorage.removeWriteOnGc(AbstractPageMemoryMvPartitionStorage.java:968)
	at org.apache.ignite.internal.storage.pagememory.mv.AbstractPageMemoryMvPartitionStorage.vacuum(AbstractPageMemoryMvPartitionStorage.java:952)
	at org.apache.ignite.internal.storage.MvPartitionStorage.lambda$pollForVacuum$0(MvPartitionStorage.java:292)
	at org.apache.ignite.internal.storage.pagememory.mv.PersistentPageMemoryMvPartitionStorage.runConsistently(PersistentPageMemoryMvPartitionStorage.java:143)
	at org.apache.ignite.internal.storage.MvPartitionStorage.pollForVacuum(MvPartitionStorage.java:282)
	at org.apache.ignite.internal.table.distributed.raft.snapshot.outgoing.SnapshotAwarePartitionDataStorage.pollForVacuum(SnapshotAwarePartitionDataStorage.java:141)
	at org.apache.ignite.internal.table.distributed.StorageUpdateHandler.internalVacuum(StorageUpdateHandler.java:345)
	at org.apache.ignite.internal.table.distributed.StorageUpdateHandler.lambda$vacuumBatch$4(StorageUpdateHandler.java:332)
	at org.apache.ignite.internal.storage.pagememory.mv.PersistentPageMemoryMvPartitionStorage.lambda$runConsistently$0(PersistentPageMemoryMvPartitionStorage.java:155)
	at org.apache.ignite.internal.storage.pagememory.mv.AbstractPageMemoryMvPartitionStorage.busy(AbstractPageMemoryMvPartitionStorage.java:774)
	at org.apache.ignite.internal.storage.pagememory.mv.PersistentPageMemoryMvPartitionStorage.runConsistently(PersistentPageMemoryMvPartitionStorage.java:145)
	at org.apache.ignite.internal.table.distributed.raft.snapshot.outgoing.SnapshotAwarePartitionDataStorage.runConsistently(SnapshotAwarePartitionDataStorage.java:66)
	at org.apache.ignite.internal.table.distributed.StorageUpdateHandler.vacuumBatch(StorageUpdateHandler.java:332)
	at org.apache.ignite.internal.table.distributed.StorageUpdateHandler.executeBatchGc(StorageUpdateHandler.java:202)
	at org.apache.ignite.internal.table.distributed.StorageUpdateHandler.handleUpdateAll(StorageUpdateHandler.java:192)
	at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.applyUpdateAllCommand(PartitionReplicaListener.java:1600)
	at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.lambda$processMultiEntryAction$72(PartitionReplicaListener.java:1487)
	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1106)
	... 28 more
Caused by: org.apache.ignite.internal.pagememory.tree.CorruptedTreeException: IGN-CMN-65535 TraceId:0f815e82-c0df-4f6e-8784-be38e9ed94cc B+Tree is corrupted [groupId=1, pageIds=[562954248388665], groupName=T, msg=Runtime failure on search row: org.apache.ignite.internal.storage.pagememory.mv.VersionChainKey@13e0d6a6]
	at org.apache.ignite.internal.pagememory.tree.BplusTree.corruptedTreeException(BplusTree.java:6724)
	at org.apache.ignite.internal.pagememory.tree.BplusTree.invoke(BplusTree.java:2135)
	at org.apache.ignite.internal.storage.pagememory.mv.AbstractPageMemoryMvPartitionStorage.removeWriteOnGc(AbstractPageMemoryMvPartitionStorage.java:961)
	... 45 more
Caused by: java.lang.AssertionError: rowId=RowId [partitionId=1, uuid=ca451c56-ea47-422e-b228-67c093ae1a10], storage=table=T, partitionId=1
	at org.apache.ignite.internal.storage.pagememory.mv.RemoveWriteOnGcInvokeClosure.call(RemoveWriteOnGcInvokeClosure.java:70)
	at org.apache.ignite.internal.storage.pagememory.mv.RemoveWriteOnGcInvokeClosure.call(RemoveWriteOnGcInvokeClosure.java:42)
	at org.apache.ignite.internal.pagememory.tree.BplusTree$Invoke.invokeClosure(BplusTree.java:4298)
	at org.apache.ignite.internal.pagememory.tree.BplusTree.invokeDown(BplusTree.java:2209)
	at org.apache.ignite.internal.pagememory.tree.BplusTree.invokeDown(BplusTree.java:2191)
	at org.apache.ignite.internal.pagememory.tree.BplusTree.invoke(BplusTree.java:2109)
	... 46 more
 {code}
Full log is attached below. [^logs.txt]",ibessonov,korlov,Major,Resolved,Fixed,09/May/23 06:32,10/May/23 15:13
Bug,IGNITE-19443,13535521,Empty data nodes shouldn't be propagated to pending assignments  after filtering ,"*Motivation:*
After https://issues.apache.org/jira/browse/IGNITE-18955 is implemented, it is possible, that data nodes will be empty according to provided filter. Such data nodes should not be propagated to pending assignments (meaning that new rebalance should not be triggered)

*Definition of done:*
* We have two places, where such propagation can be done, one is a reaction on replica factor hanging, another one is when data nodes was changed. Both places must skip pending key propagation if data nodes become empty after filtering.
* Corresponding tests for both scenarios must be provided",maliev,maliev,Major,Resolved,Fixed,09/May/23 12:40,18/May/23 20:13
Bug,IGNITE-19444,13535600,Excess size of serialized lease ,"*Motivation*

The size of serialized lease along with it's key is close to 1 kb, in spite of that the lease consists of cluster node, two timestamps and couple of booleans. It is important because there are plenty of leases that are saved to meta storage every few seconds. ClusterNode should be replaced with node name, and algorithm of serialization should be changed in order to optimize size, in general, the size of serialized lease can be a several dozens of bytes.

Currently this leads to degradation during the execution of integration tests, the time of raft calls for meta storage group raises and starts to time out after some time, so multiple tests are failing after enabling PlacementDriverManager in IgniteImpl:
{code:java}
2023-05-11 17:20:18:946 +0300 [WARNING][CompletableFutureDelayScheduler][RaftGroupServiceImpl] Recoverable error during the request type=ActionRequestImpl occurred (will be retried on the randomly selected node): 
java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException
    at java.base/java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:367)
    at java.base/java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:376)
    at java.base/java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:1093)
    at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
    at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2159)
    at java.base/java.util.concurrent.CompletableFuture$Timeout.run(CompletableFuture.java:2871)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:830)
Caused by: java.util.concurrent.TimeoutException
    ... 7 more {code}
It should be fixed for integrating the placement driver into Ignite.

*Definition of done*

The size of serialized lease is significantly reduced.",Denis Chudov,Denis Chudov,Major,Resolved,Fixed,10/May/23 07:43,18/May/23 08:45
Bug,IGNITE-19445,13535627,NPE during transaction recovery,"The transaction can be enlisted during recovery(two different threads). The {{GridIntList}} used in the {{IgniteTxStateImpl}} is not thread safe - it produces NPE:

{noformat}
[2023-05-10T12:23:32,438][ERROR][sys-#129%dht.TxRecoveryStoreEnabledTest1%][IgniteTestResources] Critical system error detected. Will be handled accordingly to configured handler [hnd=StopNodeFailureHandler [super=AbstractFailureHandler [ignoredFailureTypes=UnmodifiableSet [SYSTEM_WORKER_BLOCKED, SYSTEM_CRITICAL_OPERATION_TIMEOUT]]], failureCtx=FailureContext [type=CRITICAL_ERROR, err=class o.a.i.IgniteCheckedException: null]]
 org.apache.ignite.IgniteCheckedException: null
	at org.apache.ignite.internal.util.IgniteUtils.cast(IgniteUtils.java:7929) ~[classes/:?]
	at org.apache.ignite.internal.processors.closure.GridClosureProcessor$1.body(GridClosureProcessor.java:659) [classes/:?]
	at org.apache.ignite.internal.util.worker.GridWorker.run(GridWorker.java:125) [classes/:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_352]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_352]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_352]
Caused by: java.lang.NullPointerException
	at org.apache.ignite.internal.processors.cache.transactions.IgniteTxStateImpl.storeWriteThrough(IgniteTxStateImpl.java:342) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.transactions.IgniteTxAdapter.storeWriteThrough(IgniteTxAdapter.java:517) ~[classes/:?]
	at org.apache.ignite.internal.processors.cache.transactions.IgniteTxManager$TxRecoveryInitRunnable.run(IgniteTxManager.java:3341) ~[classes/:?]
	at org.apache.ignite.internal.util.IgniteUtils.wrapThreadLoader(IgniteUtils.java:7487) ~[classes/:?]
	at org.apache.ignite.internal.processors.closure.GridClosureProcessor$1.body(GridClosureProcessor.java:649) ~[classes/:?]
	... 4 more
{noformat}
",av,NSAmelchev,Major,Resolved,Fixed,10/May/23 11:35,22/May/23 10:56
Bug,IGNITE-19446,13535648,Non-REPL connect command should only allow URLs,"Running {{ignite3 connect}} prints 
{code:bash}
No TypeConverter registered for org.apache.ignite.internal.cli.commands.node.NodeNameOrUrl of field org.apache.ignite.internal.cli.commands.node.NodeNameOrUrl org.apache.ignite.internal.cli.commands.connect.ConnectCommand.nodeNameOrUrl
{code}
Node name registry is not available in non-REPL mode so the connect command should only allow URL parameter.",vpakhnushev,vpakhnushev,Major,Resolved,Fixed,10/May/23 14:05,10/May/23 15:44
Bug,IGNITE-19473,13536012,Incorrect schema name in sys view documentation.,"Schema [1] need to be changed:

_.setSchema(""IGNITE"")_ need to be changed into: _.setSchema(""SYS"")_

 

[1] [https://ignite.apache.org/docs/latest/monitoring-metrics/system-views]

 ",igusev,zstan,Major,Resolved,Fixed,12/May/23 17:41,25/May/23 13:48
Bug,IGNITE-19488,13536443,RemoteFragmentExecutionException when inserting more than 30 000 rows into one table,"h1. Steps to reproduce

Ignite 3 main branch commit 45380a6c802203dab0d72bd1eb9fb202b2a345b0
 # Create table with 5 columns
 # Insert into table rows by batches 1000 rows each batch.
 # Repeat previous step untill exception is thrown.

h1. Expected behaviour

Created more than 30 000 rows.
h1. Actual behaviour

An exception after 29 000 rows are inserted:
{code:java}
Exception while executing query [query=SELECT COUNT(*) FROM rows_capacity_table]. Error message:IGN-CMN-1 TraceId:24c93463-f078-410a-8831-36b5c549a907 IGN-CMN-1 TraceId:24c93463-f078-410a-8831-36b5c549a907 Query remote fragment execution failed: nodeName=TablesAmountCapacityTest_cluster_0, queryId=ecd14026-5366-4ee2-b73a-f38757d3ba4f, fragmentId=1561, originalMessage=IGN-CMN-1 TraceId:24c93463-f078-410a-8831-36b5c549a907
java.sql.SQLException: Exception while executing query [query=SELECT COUNT(*) FROM rows_capacity_table]. Error message:IGN-CMN-1 TraceId:24c93463-f078-410a-8831-36b5c549a907 IGN-CMN-1 TraceId:24c93463-f078-410a-8831-36b5c549a907 Query remote fragment execution failed: nodeName=TablesAmountCapacityTest_cluster_0, queryId=ecd14026-5366-4ee2-b73a-f38757d3ba4f, fragmentId=1561, originalMessage=IGN-CMN-1 TraceId:24c93463-f078-410a-8831-36b5c549a907
	at org.apache.ignite.internal.jdbc.proto.IgniteQueryErrorCode.createJdbcSqlException(IgniteQueryErrorCode.java:57)
	at org.apache.ignite.internal.jdbc.JdbcStatement.execute0(JdbcStatement.java:149)
	at org.apache.ignite.internal.jdbc.JdbcStatement.executeQuery(JdbcStatement.java:108) {code}
Logs are in the attachment.

[^logs.zip]",zstan,lunigorn,Critical,Resolved,Fixed,16/May/23 17:25,30/May/23 12:16
Bug,IGNITE-19494,13536526,Correctly stop a replica,"If we look at the implementation of stopping the replica in (*org.apache.ignite.internal.replicator.ReplicaManager#stopReplicaInternal*), then we simply delete the future.

I think that this is not right, and we should wait for it or something and only after that stop it.
Maybe I don't understand something and then we'll just close the ticket.


",v.pyatkov,ktkalenko@gridgain.com,Major,Resolved,Fixed,17/May/23 09:44,08/Jun/23 14:08
Bug,IGNITE-19495,13536528,"SQL onheap cache stores row on per-cache basis, but access these rows on per-index basis","If there are two indexes on the same cache and sqlOnHeapCache property is enabled there can be node failures such as:
{noformat}
org.apache.ignite.internal.processors.cache.persistence.tree.CorruptedTreeException: B+Tree is corrupted [groupId=-917681634, pageIds=[844420635164678], cacheId=-917681634, cacheName=..., indexName=_key_PK, msg=Runtime failure on row: Row@71aeb1b9[ ... ][  ]]
...
Caused by: org.apache.ignite.IgniteException: Failed to store new index row.
...
Caused by: java.lang.UnsupportedOperationException: 13 cannot be used for inline type 19
{noformat}
Root cause: {{IndexRowCache}} is created per cache group (see {{IndexRowCacheRegistry#onCacheRegistered}}), but stores {{IndexRowImpl}} binded to particular index (see {{InlineIndexTree#createIndexRow}}). In case, when another index requires the same data row, it gets row from the onheap cache with the wrong index row handler.",alex_pl,alex_pl,Major,Resolved,Fixed,17/May/23 09:49,18/May/23 08:34
Bug,IGNITE-19523,13536721,Unmute TxAbstractTest.testRollbackUpgradedLock,"The ticket IGNITE-15939 was resolved, so need to check and unmute the following test: testRollbackUpgradedLock
",slava.koptilin,slava.koptilin,Major,Resolved,Fixed,18/May/23 09:21,23/May/23 07:17
Bug,IGNITE-19533,13537171,Rename UNKNOWN_ERR error code to INTERNAL_ERR,"The UNKNOWN_ERR error code should be renamed to INTERNAL_ERR. The @Deprecated should be removed as well.

The INTRNAL_ERR error code should be considered as the product's internal error caused by faulty logic or coding in the product. In general, this error code represents a non-recoverable error that should be provided to code maintainers.

It seems to me the UNEXPECTED_ERR and Sql.INTERNAL_ERR  error codes just duplicate the `Common.INTERNAL_ERR` one, so, both of them should be removed.

[1] https://docs.oracle.com/en/database/oracle/oracle-database/19/errmg/using-messages.html#GUID-3D523C69-502E-4E8B-8E56-BEA97EBE50ED
{noformat}
ORA-00600: internal error code, arguments: [string], [string], [string], [string], [string], [string], [string], [string], [string], [string], [string], [string]
Cause: This is the generic internal error number for Oracle program exceptions. It indicates that a process has encountered a low-level, unexpected condition. The first argument is the internal message number. This argument and the database version number are critical in identifying the root cause and the potential impact to your system.

Action: Visit My Oracle Support to access the ORA-00600 Lookup tool (reference Note 600.1) for more information regarding the specific ORA-00600 error encountered. An Incident has been created for this error in the Automatic Diagnostic Repository (ADR). When logging a service request, use the Incident Packaging Service (IPS) from the Support Workbench or the ADR Command Interpreter (ADRCI) to automatically package the relevant trace information (reference My Oracle Support Note 411.1). The following information should also be gathered to help determine the root cause: - changes leading up to the error - events or unusual circumstances leading up to the error - operations attempted prior to the error - conditions of the operating system and databases at the time of the error Note: The cause of this message may manifest itself as different errors at different times. Be aware of the history of errors that occurred before this internal error.
{noformat}

[2] https://www.postgresql.org/docs/current/errcodes-appendix.html
{noformat}
Class XX — Internal Error
XX000	internal_error
XX001	data_corrupted
XX002	index_corrupted
{noformat}
",slava.koptilin,slava.koptilin,Major,Resolved,Fixed,22/May/23 14:00,24/May/23 15:35
Bug,IGNITE-19534,13537173,Duplicating error code in the message.,"Error code can be duplicated in the message as follows:
{code:java}
org.apache.ignite.sql.SqlException: IGN-SQL-20 TraceId:8c53228d-6463-4dec-8ef6-7f7a19baab49 IGN-SQL-20 TraceId:8c53228d-6463-4dec-8ef6-7f7a19baab49 IGN-SQL-20 TraceId:8c53228d-6463-4dec-8ef6-7f7a19baab49
  at java.base@11.0.17/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
  at java.base@11.0.17/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
  at java.base@11.0.17/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
  at java.base@11.0.17/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
  at app//org.apache.ignite.lang.IgniteException.wrap(IgniteException.java:276)
  at app//org.apache.ignite.sql.Session.execute(Session.java:60)
  at app//org.apache.ignite.internal.sqllogic.ScriptContext.executeQuery(ScriptContext.java:89)
  at app//org.apache.ignite.internal.sqllogic.Statement.execute(Statement.java:108)
  ... 7 more
Caused by: org.apache.ignite.sql.SqlException: IGN-SQL-20 TraceId:8c53228d-6463-4dec-8ef6-7f7a19baab49 IGN-SQL-20 TraceId:8c53228d-6463-4dec-8ef6-7f7a19baab49
  ...
  ... 3 more
Caused by: org.apache.ignite.sql.SqlException: IGN-SQL-20 TraceId:8c53228d-6463-4dec-8ef6-7f7a19baab49
  at app//org.apache.ignite.internal.sql.engine.exec.ExecutionServiceImpl.convertDdlException(ExecutionServiceImpl.java:311)
  at app//org.apache.ignite.internal.sql.engine.exec.ExecutionServiceImpl.lambda$executeDdl$8(ExecutionServiceImpl.java:289)
  ... 12 more
Caused by: java.util.concurrent.TimeoutException
  ... 8 more
{code}
-It seems to me, this behavior is caused by `IgniteException.wrap`.-
-By the way, this method is a good candidate to be moved to `ExceptionUtils` along with `IgniteException.getIgniteErrorCode()`.-

It seems to me, that the real cause of the issue is that `IgniteException` extends the user-provided message with an error code and trace id, but this behavior contradicts the implementation of Throwable:
 - the user-defined message should not be changed in any way, and `getMessage()` should return exactly the same message.
 - any additional information is included by `toString()` implementation (such as adding FQCN, error code, and traceId)",slava.koptilin,slava.koptilin,Major,Resolved,Fixed,22/May/23 14:10,01/Jun/23 09:33
Bug,IGNITE-19539,13537181,Introduce exception handler to prevent leaking internal exceptions to the end user,"It seems reasonable to introduce some kind of mapper/handler of internal exceptions in order to prevent leaking internal exceptions to the end user.
All public methods/interfaces should only throw IgniteException/IgniteCheckedException instances and their subclasses.
All internal exceptions should be mapped to IgniteException with the INTERNAL_ERR error code and be considered as a bug.",slava.koptilin,slava.koptilin,Major,Resolved,Fixed,22/May/23 14:48,28/Jun/23 13:36
Bug,IGNITE-19552,13537337,ArrayStoreException on connect to remote server node,"The problem consistently reproduces with the remote server node, sometimes - with the local one on AI3 after {color:#d1d2d3}a4912c63 IGNITE-19318 Unable to build stand alone/fat jar with JDBC driver (#2048){color}.

1) Start single server node

2) Active cluster

3) Connect to the cluster via JDBC driver.

Expected result: connection established

Actual result:
{code:java}
[16:08:46][INFO ][Thread-2] Create table request: CREATE TABLE IF NOT EXISTS usertable (yscb_key VARCHAR PRIMARY KEY, field0 VARCHAR, field1 VARCHAR, field2 VARCHAR, field3 VARCHAR, field4 VARCHAR, field5 VARCHAR, field6 VARCHAR, field7 VARCHAR, field8 VARCHAR, field9 VARCHAR);May 23, 2023 4:08:46 PM io.netty.channel.ChannelInitializer exceptionCaughtWARNING: Failed to initialize a channel. Closing: [id: 0x0392f8b4, L:/127.0.0.1:10800 - R:/127.0.0.1:53528]java.lang.ArrayStoreException: org.apache.ignite.internal.client.proto.ClientMessageDecoder    at org.apache.ignite.client.handler.ClientHandlerModule$1.initChannel(ClientHandlerModule.java:250)    at io.netty.channel.ChannelInitializer.initChannel(ChannelInitializer.java:129)    at io.netty.channel.ChannelInitializer.handlerAdded(ChannelInitializer.java:112)    at io.netty.channel.AbstractChannelHandlerContext.callHandlerAdded(AbstractChannelHandlerContext.java:1114)    at io.netty.channel.DefaultChannelPipeline.callHandlerAdded0(DefaultChannelPipeline.java:609)    at io.netty.channel.DefaultChannelPipeline.access$100(DefaultChannelPipeline.java:46)    at io.netty.channel.DefaultChannelPipeline$PendingHandlerAddedTask.execute(DefaultChannelPipeline.java:1463)    at io.netty.channel.DefaultChannelPipeline.callHandlerAddedForAllHandlers(DefaultChannelPipeline.java:1115)    at io.netty.channel.DefaultChannelPipeline.invokeHandlerAddedIfNeeded(DefaultChannelPipeline.java:650)    at io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:514)    at io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:429)    at io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:486)    at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)    at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)    at java.base/java.lang.Thread.run(Thread.java:829) {code}
 ",Mikhail Pochatkin,Berkov,Major,Resolved,Fixed,23/May/23 13:19,13/Jun/23 10:10
Bug,IGNITE-19559,13537464,NPE in deploy/undeploy calls in non-REPL mode,"When running ItDeployUndeployCallsTest the following exception is printed:
{code:java}
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1690)
	at org.apache.ignite.internal.cli.logger.CliLoggers.addApiClient(CliLoggers.java:65)
	at org.apache.ignite.internal.cli.core.rest.ApiClientFactory.getClient(ApiClientFactory.java:83)
	at org.apache.ignite.internal.cli.call.unit.ListUnitCall.execute(ListUnitCall.java:46)
	at org.apache.ignite.internal.cli.core.repl.registry.impl.UnitsRegistryImpl.lambda$updateState$1(UnitsRegistryImpl.java:57)
{code}
This happens due to the {{UnitsRegistryImpl}} being called from the {{DeployUnitCall}} even if we are not in the REPL mode.",vpakhnushev,vpakhnushev,Major,Resolved,Fixed,24/May/23 10:22,05/Jun/23 14:44
Bug,IGNITE-19560,13537470,Thin 3.0: Netty buffer leak in ConfigurationTest,"{code}
ClientTupleTest > testTypedGetters() PASSED
  org.apache.ignite.client.ClientTupleTest.testTypedGettersWithIncorrectType()
  ClientTupleTest > testTypedGettersWithIncorrectType() PASSED
org.apache.ignite.client.ConfigurationTest
  ConfigurationTest STANDARD_ERROR
      2023-05-24 13:53:59:238 +0300 [INFO][Test worker][ClientHandlerModule] Thin client protocol started successfully [port=10800]
      2023-05-24 13:53:59:249 +0300 [ERROR][nioEventLoopGroup-168-1][ResourceLeakDetector] LEAK: ByteBuf.release() was not called before it's garbage-collected. See https://netty.io/wiki/reference-counted-objects.html for more information.
      Recent access records:
      #1:
        io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:300)
        io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
        io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
        io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
        io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
        io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
        io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
{code}

https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_RunUnitTests/7247037?hideProblemsFromDependencies=false&hideTestsFromDependencies=false



",ptupitsyn,ptupitsyn,Critical,Resolved,Fixed,24/May/23 11:31,21/Jun/23 14:18
Bug,IGNITE-19563,13537643,Deadlock on indexes creation with multiple tables creation ,"* Disruptor stripe took CP read lock and waits for index future (PK) to be completed. This future should be completed in configuration registry thread (see IndexManager#createIndexLocally);
 * Checkpoint thread is trying to take CP write lock;
 * Configuration registry thread is stuck on waiting for CP read lock as well, so it never will complete index future.

  
{code:java}
""%sqllogic0%JRaft-FSMCaller-Disruptor-_stripe_6-0"" #34 daemon prio=5 os_prio=0 cpu=46.88ms elapsed=26.72s tid=0x000002befd805000 nid=0x42e8 waiting on condition  [0x000000c8b92fe000]    java.lang.Thread.State: WAITING (parking) at jdk.internal.misc.Unsafe.park(java.base@13.0.1/Native Method) - parking to wait for  <0x000000060f6d08a0> (a java.util.concurrent.CompletableFuture$Signaller) at java.util.concurrent.locks.LockSupport.park(java.base@13.0.1/LockSupport.java:194) at java.util.concurrent.CompletableFuture$Signaller.block(java.base@13.0.1/CompletableFuture.java:1867) at java.util.concurrent.ForkJoinPool.managedBlock(java.base@13.0.1/ForkJoinPool.java:3137) at java.util.concurrent.CompletableFuture.waitingGet(java.base@13.0.1/CompletableFuture.java:1894) at java.util.concurrent.CompletableFuture.join(java.base@13.0.1/CompletableFuture.java:2114) at org.apache.ignite.internal.table.TableImpl.awaitIndexes(TableImpl.java:374) at org.apache.ignite.internal.table.TableImpl$1.get(TableImpl.java:232) at org.apache.ignite.internal.table.distributed.index.IndexUpdateHandler.buildIndex(IndexUpdateHandler.java:136) at org.apache.ignite.internal.table.distributed.raft.PartitionListener.lambda$handleBuildIndexCommand$14(PartitionListener.java:481) at org.apache.ignite.internal.table.distributed.raft.PartitionListener$$Lambda$2041/0x000000080151ac40.execute(Unknown Source) at org.apache.ignite.internal.storage.pagememory.mv.PersistentPageMemoryMvPartitionStorage.lambda$runConsistently$0(PersistentPageMemoryMvPartitionStorage.java:157) at org.apache.ignite.internal.storage.pagememory.mv.PersistentPageMemoryMvPartitionStorage$$Lambda$1875/0x00000008014cf840.get(Unknown Source) at org.apache.ignite.internal.storage.pagememory.mv.AbstractPageMemoryMvPartitionStorage.busy(AbstractPageMemoryMvPartitionStorage.java:784) at org.apache.ignite.internal.storage.pagememory.mv.PersistentPageMemoryMvPartitionStorage.runConsistently(PersistentPageMemoryMvPartitionStorage.java:147) at org.apache.ignite.internal.table.distributed.raft.snapshot.outgoing.SnapshotAwarePartitionDataStorage.runConsistently(SnapshotAwarePartitionDataStorage.java:66) at org.apache.ignite.internal.table.distributed.raft.PartitionListener.handleBuildIndexCommand(PartitionListener.java:471) at org.apache.ignite.internal.table.distributed.raft.PartitionListener.lambda$onWrite$2(PartitionListener.java:187) at org.apache.ignite.internal.table.distributed.raft.PartitionListener$$Lambda$2037/0x0000000801518440.accept(Unknown Source) at java.util.Iterator.forEachRemaining(java.base@13.0.1/Iterator.java:133) at org.apache.ignite.internal.table.distributed.raft.PartitionListener.onWrite(PartitionListener.java:149) at org.apache.ignite.internal.raft.server.impl.JraftServerImpl$DelegatingStateMachine.onApply(JraftServerImpl.java:592) at org.apache.ignite.raft.jraft.core.FSMCallerImpl.doApplyTasks(FSMCallerImpl.java:561) at org.apache.ignite.raft.jraft.core.FSMCallerImpl.doCommitted(FSMCallerImpl.java:529) at org.apache.ignite.raft.jraft.core.FSMCallerImpl.runApplyTask(FSMCallerImpl.java:448) at org.apache.ignite.raft.jraft.core.FSMCallerImpl$ApplyTaskHandler.onEvent(FSMCallerImpl.java:136) at org.apache.ignite.raft.jraft.core.FSMCallerImpl$ApplyTaskHandler.onEvent(FSMCallerImpl.java:130) at org.apache.ignite.raft.jraft.disruptor.StripedDisruptor$StripeEntryHandler.onEvent(StripedDisruptor.java:217) at org.apache.ignite.raft.jraft.disruptor.StripedDisruptor$StripeEntryHandler.onEvent(StripedDisruptor.java:181) at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:137) at java.lang.Thread.run(java.base@13.0.1/Thread.java:830)   

""%sqllogic0%vault-3"" #107 prio=5 os_prio=0 cpu=171.88ms elapsed=26.34s tid=0x000002befec1f000 nid=0x6f74 waiting on condition  [0x000000c8be8fd000]    java.lang.Thread.State: WAITING (parking) at jdk.internal.misc.Unsafe.park(java.base@13.0.1/Native Method) - parking to wait for  <0x000000060f4689b0> (a java.util.concurrent.CompletableFuture$Signaller) at java.util.concurrent.locks.LockSupport.park(java.base@13.0.1/LockSupport.java:194) at java.util.concurrent.CompletableFuture$Signaller.block(java.base@13.0.1/CompletableFuture.java:1867) at java.util.concurrent.ForkJoinPool.managedBlock(java.base@13.0.1/ForkJoinPool.java:3137) at java.util.concurrent.CompletableFuture.waitingGet(java.base@13.0.1/CompletableFuture.java:1894) at java.util.concurrent.CompletableFuture.get(java.base@13.0.1/CompletableFuture.java:2069) at org.apache.ignite.internal.util.IgniteUtils.getUninterruptibly(IgniteUtils.java:824) at org.apache.ignite.internal.pagememory.persistence.checkpoint.CheckpointTimeoutLock.checkpointReadLock(CheckpointTimeoutLock.java:163) at org.apache.ignite.internal.storage.pagememory.mv.PersistentPageMemoryMvPartitionStorage.lambda$runConsistently$0(PersistentPageMemoryMvPartitionStorage.java:152) at org.apache.ignite.internal.storage.pagememory.mv.PersistentPageMemoryMvPartitionStorage$$Lambda$1875/0x00000008014cf840.get(Unknown Source) at org.apache.ignite.internal.storage.pagememory.mv.AbstractPageMemoryMvPartitionStorage.busy(AbstractPageMemoryMvPartitionStorage.java:784) at org.apache.ignite.internal.storage.pagememory.mv.PersistentPageMemoryMvPartitionStorage.runConsistently(PersistentPageMemoryMvPartitionStorage.java:147) at org.apache.ignite.internal.storage.pagememory.mv.PersistentPageMemoryMvPartitionStorage.getOrCreateHashIndex(PersistentPageMemoryMvPartitionStorage.java:303) at org.apache.ignite.internal.storage.pagememory.AbstractPageMemoryTableStorage.lambda$getOrCreateHashIndex$9(AbstractPageMemoryTableStorage.java:214) at org.apache.ignite.internal.storage.pagememory.AbstractPageMemoryTableStorage$$Lambda$2007/0x0000000801508840.get(Unknown Source) at org.apache.ignite.internal.util.IgniteUtils.inBusyLock(IgniteUtils.java:872) at org.apache.ignite.internal.storage.pagememory.AbstractPageMemoryTableStorage.busy(AbstractPageMemoryTableStorage.java:229) at org.apache.ignite.internal.storage.pagememory.AbstractPageMemoryTableStorage.getOrCreateHashIndex(AbstractPageMemoryTableStorage.java:207) at org.apache.ignite.internal.storage.engine.MvTableStorage.getOrCreateIndex(MvTableStorage.java:91) at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.startBuildIndex(PartitionReplicaListener.java:2476) at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener$1.lambda$onCreate$0(PartitionReplicaListener.java:2436) at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener$1$$Lambda$2006/0x0000000801508440.run(Unknown Source) at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener.inBusyLock(PartitionReplicaListener.java:2506) at org.apache.ignite.internal.table.distributed.replicator.PartitionReplicaListener$1.onCreate(PartitionReplicaListener.java:2432) at org.apache.ignite.internal.configuration.notifications.ConfigurationNotifier$1$$Lambda$1755/0x000000080147bc40.apply(Unknown Source) at org.apache.ignite.internal.configuration.notifications.ConfigurationNotifier.notifyPublicListeners(ConfigurationNotifier.java:488) at org.apache.ignite.internal.configuration.notifications.ConfigurationNotifier$1.visitNamedListNode(ConfigurationNotifier.java:206) at org.apache.ignite.internal.configuration.notifications.ConfigurationNotifier$1.visitNamedListNode(ConfigurationNotifier.java:129) at org.apache.ignite.internal.schema.configuration.TablesNode.traverseChildren(Unknown Source) at org.apache.ignite.internal.configuration.notifications.ConfigurationNotifier.notifyListeners(ConfigurationNotifier.java:129) at org.apache.ignite.internal.configuration.notifications.ConfigurationNotifier.notifyListeners(ConfigurationNotifier.java:91) at org.apache.ignite.internal.configuration.ConfigurationRegistry$3$1.visitInnerNode(ConfigurationRegistry.java:271) at org.apache.ignite.internal.configuration.ConfigurationRegistry$3$1.visitInnerNode(ConfigurationRegistry.java:254) at org.apache.ignite.internal.configuration.SuperRoot.traverseChildren(SuperRoot.java:105) at org.apache.ignite.internal.configuration.ConfigurationRegistry$3.onConfigurationUpdated(ConfigurationRegistry.java:254) at org.apache.ignite.internal.configuration.ConfigurationChanger$3.lambda$onEntriesChanged$1(ConfigurationChanger.java:658) at org.apache.ignite.internal.configuration.ConfigurationChanger$3$$Lambda$1747/0x0000000801479040.apply(Unknown Source) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(java.base@13.0.1/CompletableFuture.java:1146) at java.util.concurrent.CompletableFuture.postComplete(java.base@13.0.1/CompletableFuture.java:506) at java.util.concurrent.CompletableFuture$AsyncRun.run(java.base@13.0.1/CompletableFuture.java:1813) at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@13.0.1/ThreadPoolExecutor.java:1128) at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@13.0.1/ThreadPoolExecutor.java:628) at java.lang.Thread.run(java.base@13.0.1/Thread.java:830)   

""%sqllogic0%checkpoint-thread-1"" #133 prio=5 os_prio=0 cpu=31.25ms elapsed=26.26s tid=0x000002befec3a800 nid=0x214c waiting on condition  [0x000000c8c03fe000]    java.lang.Thread.State: WAITING (parking) at jdk.internal.misc.Unsafe.park(java.base@13.0.1/Native Method) - parking to wait for  <0x00000006050084b0> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync) at java.util.concurrent.locks.LockSupport.park(java.base@13.0.1/LockSupport.java:194) at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(java.base@13.0.1/AbstractQueuedSynchronizer.java:885) at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(java.base@13.0.1/AbstractQueuedSynchronizer.java:917) at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(java.base@13.0.1/AbstractQueuedSynchronizer.java:1240) at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(java.base@13.0.1/ReentrantReadWriteLock.java:959) at org.apache.ignite.internal.pagememory.persistence.checkpoint.CheckpointReadWriteLock.writeLock(CheckpointReadWriteLock.java:129) at org.apache.ignite.internal.pagememory.persistence.checkpoint.CheckpointWorkflow.markCheckpointBegin(CheckpointWorkflow.java:209) at org.apache.ignite.internal.pagememory.persistence.checkpoint.Checkpointer.doCheckpoint(Checkpointer.java:280) at org.apache.ignite.internal.pagememory.persistence.checkpoint.Checkpointer.body(Checkpointer.java:205) at org.apache.ignite.internal.util.worker.IgniteWorker.run(IgniteWorker.java:108) at java.lang.Thread.run(java.base@13.0.1/Thread.java:830){code}
 

Could be reproduced very fast on creation of multiple tables like it is described in IGNITE-19275 and with raft fsync turned off.",ibessonov,Denis Chudov,Major,Resolved,Fixed,25/May/23 11:15,30/May/23 09:33
Bug,IGNITE-19564,13537649,Snapshot restoration metric is not always assigned when an error occurs.,"The metric of last snapshot process might not be assigned in case of failed snapshot restoration. Happens when an error occurs on the process preparation, before launching. The cause is that we assign the holder of last process context after some checks which can raise an exception.",vladsz83,vladsz83,Minor,Resolved,Fixed,25/May/23 11:41,31/May/23 10:56
Bug,IGNITE-19571,13537772,Thin 3.0: Can't connect to cluster with default configuration,"We have a server running on local machine on the default port (10800). We try to connect the client like this:
{code:java}
var client = IgniteClient.builder().addresses(""localhost"").build();
{code}

It fails with an exception:
{code}
[WARNING][nioEventLoopGroup-12-1][ReliableChannel] Failed to establish connection to localhost:10896: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:1ebe1f74-892c-471a-a520-b8bccecc246c Client failed to connect: Connection refused: localhost/127.0.0.1:10896
java.util.concurrent.CompletionException: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:1ebe1f74-892c-471a-a520-b8bccecc246c Client failed to connect: Connection refused: localhost/127.0.0.1:10896
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1063)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.ignite.internal.client.io.netty.NettyClientConnectionMultiplexer.lambda$openAsync$1(NettyClientConnectionMultiplexer.java:197)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:1ebe1f74-892c-471a-a520-b8bccecc246c Client failed to connect: Connection refused: localhost/127.0.0.1:10896
	at org.apache.ignite.internal.client.io.netty.NettyClientConnectionMultiplexer.lambda$openAsync$1(NettyClientConnectionMultiplexer.java:194)
	... 17 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:10896
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
2023-05-26 11:42:11:269 +0300 [WARNING][nioEventLoopGroup-12-2][ReliableChannel] Failed to establish connection to localhost:10895: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:13d626f5-4a92-48d2-bbc7-cdf98b63104f Client failed to connect: Connection refused: localhost/127.0.0.1:10895
java.util.concurrent.CompletionException: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:13d626f5-4a92-48d2-bbc7-cdf98b63104f Client failed to connect: Connection refused: localhost/127.0.0.1:10895
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1063)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.ignite.internal.client.io.netty.NettyClientConnectionMultiplexer.lambda$openAsync$1(NettyClientConnectionMultiplexer.java:197)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:13d626f5-4a92-48d2-bbc7-cdf98b63104f Client failed to connect: Connection refused: localhost/127.0.0.1:10895
	at org.apache.ignite.internal.client.io.netty.NettyClientConnectionMultiplexer.lambda$openAsync$1(NettyClientConnectionMultiplexer.java:194)
	... 17 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:10895
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
2023-05-26 11:42:11:273 +0300 [WARNING][nioEventLoopGroup-12-3][ReliableChannel] Failed to establish connection to localhost:10894: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:679f5a1d-6f0f-4acc-ad97-cdaf425951e2 Client failed to connect: Connection refused: localhost/127.0.0.1:10894
java.util.concurrent.CompletionException: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:679f5a1d-6f0f-4acc-ad97-cdaf425951e2 Client failed to connect: Connection refused: localhost/127.0.0.1:10894
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1063)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.ignite.internal.client.io.netty.NettyClientConnectionMultiplexer.lambda$openAsync$1(NettyClientConnectionMultiplexer.java:197)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:679f5a1d-6f0f-4acc-ad97-cdaf425951e2 Client failed to connect: Connection refused: localhost/127.0.0.1:10894
	at org.apache.ignite.internal.client.io.netty.NettyClientConnectionMultiplexer.lambda$openAsync$1(NettyClientConnectionMultiplexer.java:194)
	... 17 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:10894
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
2023-05-26 11:42:11:280 +0300 [WARNING][nioEventLoopGroup-12-4][ReliableChannel] Failed to establish connection to localhost:10893: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:b5cd2028-f3bc-4c7d-9ade-5b209b539086 Client failed to connect: Connection refused: localhost/127.0.0.1:10893
java.util.concurrent.CompletionException: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:b5cd2028-f3bc-4c7d-9ade-5b209b539086 Client failed to connect: Connection refused: localhost/127.0.0.1:10893
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1063)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.ignite.internal.client.io.netty.NettyClientConnectionMultiplexer.lambda$openAsync$1(NettyClientConnectionMultiplexer.java:197)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:b5cd2028-f3bc-4c7d-9ade-5b209b539086 Client failed to connect: Connection refused: localhost/127.0.0.1:10893
	at org.apache.ignite.internal.client.io.netty.NettyClientConnectionMultiplexer.lambda$openAsync$1(NettyClientConnectionMultiplexer.java:194)
	... 17 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:10893
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
2023-05-26 11:42:11:284 +0300 [WARNING][nioEventLoopGroup-12-5][ReliableChannel] Failed to establish connection to localhost:10900: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:ac355982-ce70-43f4-bdb1-c4168f024d3e Client failed to connect: Connection refused: localhost/127.0.0.1:10900
java.util.concurrent.CompletionException: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:ac355982-ce70-43f4-bdb1-c4168f024d3e Client failed to connect: Connection refused: localhost/127.0.0.1:10900
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1063)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.ignite.internal.client.io.netty.NettyClientConnectionMultiplexer.lambda$openAsync$1(NettyClientConnectionMultiplexer.java:197)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:ac355982-ce70-43f4-bdb1-c4168f024d3e Client failed to connect: Connection refused: localhost/127.0.0.1:10900
	at org.apache.ignite.internal.client.io.netty.NettyClientConnectionMultiplexer.lambda$openAsync$1(NettyClientConnectionMultiplexer.java:194)
	... 17 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:10900
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
2023-05-26 11:42:11:287 +0300 [WARNING][nioEventLoopGroup-12-6][ReliableChannel] Failed to establish connection to localhost:10899: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:ca5aaea9-e03d-4aec-8de3-554685d29894 Client failed to connect: Connection refused: localhost/127.0.0.1:10899
java.util.concurrent.CompletionException: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:ca5aaea9-e03d-4aec-8de3-554685d29894 Client failed to connect: Connection refused: localhost/127.0.0.1:10899
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1063)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.ignite.internal.client.io.netty.NettyClientConnectionMultiplexer.lambda$openAsync$1(NettyClientConnectionMultiplexer.java:197)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583)
	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.ignite.client.IgniteClientConnectionException: IGN-CLIENT-1 TraceId:ca5aaea9-e03d-4aec-8de3-554685d29894 Client failed to connect: Connection refused: localhost/127.0.0.1:10899
	at org.apache.ignite.internal.client.io.netty.NettyClientConnectionMultiplexer.lambda$openAsync$1(NettyClientConnectionMultiplexer.java:194)
	... 17 more
{code}


*Explanation*
* {{IgniteClientConfiguration.DFLT_PORT_RANGE}} is 100.
* We try all endpoints in random order (because they go into a Map - see initChannelHolders).
* Default RetryPolicy has retryLimit = 16

We fail after 16 tries, never checking port 10800.

*Potential fixes*
* Reduce default port range (both on client and server)
* Remove port range from the client completely (controversial feature - other db drivers do not have that)
* Fix connection logic to try ports in order
* Increase default retry limit",ptupitsyn,ptupitsyn,Major,Resolved,Fixed,26/May/23 08:33,15/Jun/23 14:05
Bug,IGNITE-19572,13537773,Non-REPL node commands show node name option,"{{node config show --help}} shows that it's possible to use {{-n, --node-name=<nodeName>}} option but it can only be used in the REPL mode while connected.",vpakhnushev,vpakhnushev,Major,Resolved,Fixed,26/May/23 08:35,26/May/23 14:53
Bug,IGNITE-19574,13537781,Implement restoring of global states of DistributionZoneManager after restart,"In https://issues.apache.org/jira/browse/IGNITE-19061 we have provided design for a correct restart of a node and recovering all Distributed Zones states. 

In this ticket we want to restore {{DistributionZoneManager#nodesAttributes}}, check that {{DistributionZoneManager#logicalTopology}} is restored correctly. To achieve restoring {{DistributionZoneManager#nodesAttributes}}, we need to save it synchronously to Vault when we handle {{DistributionZoneManager#createMetastorageTopologyListener}}, and read this value on a restart in {{DistributionZoneManager#start}}
Details could be found in the design document.

As a result, this global states must be restored to the state that they had before restart. Test for restart scenarios must be provided, so we want to test, that any restart of a node leads to correct global states restoring, meaning that their state before restart are restored.",maliev,maliev,Major,Resolved,Fixed,26/May/23 08:58,20/Jun/23 11:18
Bug,IGNITE-19576,13537801,org.apache.ignite.internal.cluster.management.ItClusterManagerTest#testClusterConfigurationIsRemovedFromClusterStateAfterUpdating is flaky,"The problem is the test tries to find the cluster leader twice.
{code:java}
// Wait for a new leader to be elected.
Awaitility.await().until(() -> findLeaderNode(cluster).isPresent());
// Find the new CMG leader.
MockNode newLeaderNode = findLeaderNode(cluster).orElseThrow(); {code}
 # Awaits for appearing of the new leader
 # Gets the leader

From time to time the test can't find the leader the second time. 

We should investigate it and fix it. ",ivan.gagarkin,ivan.gagarkin,Critical,Resolved,Fixed,26/May/23 13:06,29/May/23 14:45
Bug,IGNITE-19580,13537806,Implement restoring of zones states of DistributionZoneManager after restart,"In https://issues.apache.org/jira/browse/IGNITE-19061 we have provided design for a correct restart of a node and recovering all Distributed Zones states.

In this ticket we want to restore {{DistributionZoneManager.ZoneState}} per every zone. Note that in this ticket we don't want to take into account immediate scale up when filter was changed, this will be done in a separate ticket (https://issues.apache.org/jira/browse/IGNITE-19581). 


h4. The overall flow is the following: 
We need to save topologyAugmentationMap to Vault synchronously on metastorage watch listener for zonesLogicalTopologyPrefix. This is the only place where {{topologyAugmentationMap}} are changed. So, when restart, for every zone from configuration, when {{onCreate}} for every zone is called, restore topologyAugmentationMap from Vault. After that restoring the values for these local fields will be as they were before restart. After that schedule scale Up timer with {{maxScUpFromMap}} and schedule a scale down timer with {{maxScDownFromMap}}. 

Details could be found in the design document.

As a result, zones' states must be restored to the state that they had before restart. It includes data nodes field, topologyAugmentationMap, and all scheduled before restart timers. Tests scenarios could be found in the document as well",maliev,maliev,Major,Resolved,Fixed,26/May/23 13:54,28/Jun/23 12:54
Bug,IGNITE-19581,13537808,Implement restoring of immediate scale up intent after restart when a filter was changed,"In https://issues.apache.org/jira/browse/IGNITE-19061 we have provided design for a correct restart of a node and recovering all Distributed Zones states.

In this ticket we want to take into account immediate scale up when filter was changed, so this intent must be restored after restart. In the ticket https://issues.apache.org/jira/browse/IGNITE-19580 we restored all zones' states, including all scheduled before restart timers, but node could be restarted when immediate scale up was running, so data nodes weren't updated before restart. This immediate propagation must be done while node is restarting.

The overall idea is to save synchronously revision of the event of filter change to Vault (let’s name it filterChangeRevision). On a restart, for every zone, when we will recover scheduled timers using topologyAugmentationMap, we will check value of zoneScaleUpChangeTriggerKey from Vault and compare it with the maxScUpFromMap and filterChangeRevision. 

Details could be found in the design document",maliev,maliev,Major,Resolved,Fixed,26/May/23 13:59,06/Jul/23 07:33
Bug,IGNITE-19591,13538006,"Possible bottleneck with RocksDB on multiple table creation, with RocksDB as table storage","Scenario:
 * 1 node cluster
 * rocksdb table storage ( created via SQL query {{create zone test_zone engine rocksdb with partitions=25}} )
 * sequential tables creation using this zone (queries like {{CREATE TABLE table_0(id INT PRIMARY KEY, column_1 VARCHAR, column_2 VARCHAR, column_3 VARCHAR, column_4 VARCHAR) with primary_zone='TEST_ZONE' }} ). Number of talbes: 1000, pause between query executions: 30 ms.

After creating a couple of hundreds of tables, queries start failing with timeout.

Threads are busy with RocksDB write operations:

  
{code:java}
""%sqllogic0%JRaft-FSMCaller-Disruptor-_stripe_0-0"" #28 daemon prio=5 os_prio=0 cpu=203.13ms elapsed=58.63s tid=0x000001e4966a0800 nid=0x37e0 runnable  [0x0000002e032ff000]
        java.lang.Thread.State: RUNNABLE
        at org.rocksdb.RocksDB.write1(Native Method)
        at org.rocksdb.RocksDB.write(RocksDB.java:1722)
        at org.apache.ignite.internal.storage.rocksdb.RocksDbMvPartitionStorage.lambda$runConsistently$1(RocksDbMvPartitionStorage.java:236)
        at org.apache.ignite.internal.storage.rocksdb.RocksDbMvPartitionStorage$$Lambda$2003/0x00000008014ecc40.get(Unknown Source)
        at org.apache.ignite.internal.storage.rocksdb.RocksDbMvPartitionStorage.busy(RocksDbMvPartitionStorage.java:1415)
        at org.apache.ignite.internal.storage.rocksdb.RocksDbMvPartitionStorage.runConsistently(RocksDbMvPartitionStorage.java:222)
        at org.apache.ignite.internal.table.distributed.raft.snapshot.outgoing.SnapshotAwarePartitionDataStorage.runConsistently(SnapshotAwarePartitionDataStorage.java:66)
        at org.apache.ignite.internal.table.distributed.raft.PartitionListener.handleBuildIndexCommand(PartitionListener.java:471)
        at org.apache.ignite.internal.table.distributed.raft.PartitionListener.lambda$onWrite$2(PartitionListener.java:187)
        at org.apache.ignite.internal.table.distributed.raft.PartitionListener$$Lambda$2187/0x0000000801536040.accept(Unknown Source)
        at java.util.Iterator.forEachRemaining(java.base@13.0.1/Iterator.java:133)
        at org.apache.ignite.internal.table.distributed.raft.PartitionListener.onWrite(PartitionListener.java:149)
        at org.apache.ignite.internal.raft.server.impl.JraftServerImpl$DelegatingStateMachine.onApply(JraftServerImpl.java:592)
        at org.apache.ignite.raft.jraft.core.FSMCallerImpl.doApplyTasks(FSMCallerImpl.java:561)
        at org.apache.ignite.raft.jraft.core.FSMCallerImpl.doCommitted(FSMCallerImpl.java:529)
        at org.apache.ignite.raft.jraft.core.FSMCallerImpl.runApplyTask(FSMCallerImpl.java:448)
        at org.apache.ignite.raft.jraft.core.FSMCallerImpl$ApplyTaskHandler.onEvent(FSMCallerImpl.java:136)
        at org.apache.ignite.raft.jraft.core.FSMCallerImpl$ApplyTaskHandler.onEvent(FSMCallerImpl.java:130)
        at org.apache.ignite.raft.jraft.disruptor.StripedDisruptor$StripeEntryHandler.onEvent(StripedDisruptor.java:217)
        at org.apache.ignite.raft.jraft.disruptor.StripedDisruptor$StripeEntryHandler.onEvent(StripedDisruptor.java:181)
        at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:137)
        at java.lang.Thread.run(java.base@13.0.1/Thread.java:830)

""%sqllogic0%vault-1"" #92 prio=5 os_prio=0 cpu=2687.50ms elapsed=58.52s tid=0x000001e49bdfc000 nid=0x6f24 runnable  [0x0000002e079fd000]
        java.lang.Thread.State: RUNNABLE
        at org.rocksdb.RocksDB.open(Native Method)
        at org.rocksdb.RocksDB.open(RocksDB.java:307)
        at org.apache.ignite.internal.storage.rocksdb.RocksDbTableStorage.lambda$start$1(RocksDbTableStorage.java:251)
        at org.apache.ignite.internal.storage.rocksdb.RocksDbTableStorage$$Lambda$1786/0x000000080148a040.run(Unknown Source)
        at org.apache.ignite.internal.util.IgniteUtils.inBusyLock(IgniteUtils.java:889)
        at org.apache.ignite.internal.storage.rocksdb.RocksDbTableStorage.start(RocksDbTableStorage.java:223)
        at org.apache.ignite.internal.table.distributed.TableManager.createTableStorage(TableManager.java:1159)
        at org.apache.ignite.internal.table.distributed.TableManager.createTableLocally(TableManager.java:1103)
        at org.apache.ignite.internal.table.distributed.TableManager.onTableCreate(TableManager.java:547)
        at org.apache.ignite.internal.table.distributed.TableManager$1.onCreate(TableManager.java:460)
        at org.apache.ignite.internal.configuration.notifications.ConfigurationNotifier$1$$Lambda$1711/0x0000000801460440.apply(Unknown Source)
        at org.apache.ignite.internal.configuration.notifications.ConfigurationNotifier.notifyPublicListeners(ConfigurationNotifier.java:488)
        at org.apache.ignite.internal.configuration.notifications.ConfigurationNotifier$1.visitNamedListNode(ConfigurationNotifier.java:206)
        at org.apache.ignite.internal.configuration.notifications.ConfigurationNotifier$1.visitNamedListNode(ConfigurationNotifier.java:129)
        at org.apache.ignite.internal.schema.configuration.TablesNode.traverseChildren(Unknown Source)
        at org.apache.ignite.internal.configuration.notifications.ConfigurationNotifier.notifyListeners(ConfigurationNotifier.java:129)
        at org.apache.ignite.internal.configuration.notifications.ConfigurationNotifier.notifyListeners(ConfigurationNotifier.java:91)
        at org.apache.ignite.internal.configuration.ConfigurationRegistry$3$1.visitInnerNode(ConfigurationRegistry.java:271)
        at org.apache.ignite.internal.configuration.ConfigurationRegistry$3$1.visitInnerNode(ConfigurationRegistry.java:254)
        at org.apache.ignite.internal.configuration.SuperRoot.traverseChildren(SuperRoot.java:105)
        at org.apache.ignite.internal.configuration.ConfigurationRegistry$3.onConfigurationUpdated(ConfigurationRegistry.java:254)
        at org.apache.ignite.internal.configuration.ConfigurationChanger$3.lambda$onEntriesChanged$1(ConfigurationChanger.java:658)
        at org.apache.ignite.internal.configuration.ConfigurationChanger$3$$Lambda$1702/0x000000080145d440.apply(Unknown Source)
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(java.base@13.0.1/CompletableFuture.java:1146)
        at java.util.concurrent.CompletableFuture.postComplete(java.base@13.0.1/CompletableFuture.java:506)
        at java.util.concurrent.CompletableFuture$AsyncRun.run(java.base@13.0.1/CompletableFuture.java:1813)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@13.0.1/ThreadPoolExecutor.java:1128)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@13.0.1/ThreadPoolExecutor.java:628)
        at java.lang.Thread.run(java.base@13.0.1/Thread.java:830)

""%sqllogic0%JRaft-FSMCaller-Disruptor-_stripe_1-0"" #29 daemon prio=5 os_prio=0 cpu=140.63ms elapsed=58.63s tid=0x000001e49669c800 nid=0x2214 runnable  [0x0000002e033fe000]
        java.lang.Thread.State: RUNNABLE
        at org.rocksdb.RocksDB.write1(Native Method)
        at org.rocksdb.RocksDB.write(RocksDB.java:1722)
        at org.apache.ignite.internal.storage.rocksdb.RocksDbMvPartitionStorage.lambda$runConsistently$1(RocksDbMvPartitionStorage.java:236)
        at org.apache.ignite.internal.storage.rocksdb.RocksDbMvPartitionStorage$$Lambda$2003/0x00000008014ecc40.get(Unknown Source)
        at org.apache.ignite.internal.storage.rocksdb.RocksDbMvPartitionStorage.busy(RocksDbMvPartitionStorage.java:1415)
        at org.apache.ignite.internal.storage.rocksdb.RocksDbMvPartitionStorage.runConsistently(RocksDbMvPartitionStorage.java:222)
        at org.apache.ignite.internal.table.distributed.raft.snapshot.outgoing.SnapshotAwarePartitionDataStorage.runConsistently(SnapshotAwarePartitionDataStorage.java:66)
        at org.apache.ignite.internal.table.distributed.raft.PartitionListener.handleSafeTimeSyncCommand(PartitionListener.java:378)
        at org.apache.ignite.internal.table.distributed.raft.PartitionListener.lambda$onWrite$2(PartitionListener.java:185)
        at org.apache.ignite.internal.table.distributed.raft.PartitionListener$$Lambda$2187/0x0000000801536040.accept(Unknown Source)
        at java.util.Iterator.forEachRemaining(java.base@13.0.1/Iterator.java:133)
        at org.apache.ignite.internal.table.distributed.raft.PartitionListener.onWrite(PartitionListener.java:149)
        at org.apache.ignite.internal.raft.server.impl.JraftServerImpl$DelegatingStateMachine.onApply(JraftServerImpl.java:592)
        at org.apache.ignite.raft.jraft.core.FSMCallerImpl.doApplyTasks(FSMCallerImpl.java:561)
        at org.apache.ignite.raft.jraft.core.FSMCallerImpl.doCommitted(FSMCallerImpl.java:529)
        at org.apache.ignite.raft.jraft.core.FSMCallerImpl.runApplyTask(FSMCallerImpl.java:448)
        at org.apache.ignite.raft.jraft.core.FSMCallerImpl$ApplyTaskHandler.onEvent(FSMCallerImpl.java:136)
        at org.apache.ignite.raft.jraft.core.FSMCallerImpl$ApplyTaskHandler.onEvent(FSMCallerImpl.java:130)
        at org.apache.ignite.raft.jraft.disruptor.StripedDisruptor$StripeEntryHandler.onEvent(StripedDisruptor.java:217)
        at org.apache.ignite.raft.jraft.disruptor.StripedDisruptor$StripeEntryHandler.onEvent(StripedDisruptor.java:181)
        at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:137)
        at java.lang.Thread.run(java.base@13.0.1/Thread.java:830)


""%sqllogic0%JRaft-FSMCaller-Disruptor-_stripe_2-0"" #30 daemon prio=5 os_prio=0 cpu=140.63ms elapsed=58.63s tid=0x000001e49669d000 nid=0x7770 runnable  [0x0000002e034fe000]
        java.lang.Thread.State: RUNNABLE
        at org.rocksdb.RocksDB.write1(Native Method)
        at org.rocksdb.RocksDB.write(RocksDB.java:1722)
        at org.apache.ignite.internal.storage.rocksdb.RocksDbMvPartitionStorage.lambda$runConsistently$1(RocksDbMvPartitionStorage.java:236)
        at org.apache.ignite.internal.storage.rocksdb.RocksDbMvPartitionStorage$$Lambda$2003/0x00000008014ecc40.get(Unknown Source)
        at org.apache.ignite.internal.storage.rocksdb.RocksDbMvPartitionStorage.busy(RocksDbMvPartitionStorage.java:1415)
        at org.apache.ignite.internal.storage.rocksdb.RocksDbMvPartitionStorage.runConsistently(RocksDbMvPartitionStorage.java:222)
        at org.apache.ignite.internal.table.distributed.raft.snapshot.outgoing.SnapshotAwarePartitionDataStorage.runConsistently(SnapshotAwarePartitionDataStorage.java:66)
        at org.apache.ignite.internal.table.distributed.raft.PartitionListener.onConfigurationCommitted(PartitionListener.java:398)
        at org.apache.ignite.internal.raft.server.impl.JraftServerImpl$DelegatingStateMachine.onRawConfigurationCommitted(JraftServerImpl.java:669)
        at org.apache.ignite.raft.jraft.core.FSMCallerImpl.doCommitted(FSMCallerImpl.java:511)
        at org.apache.ignite.raft.jraft.core.FSMCallerImpl.runApplyTask(FSMCallerImpl.java:448)
        at org.apache.ignite.raft.jraft.core.FSMCallerImpl$ApplyTaskHandler.onEvent(FSMCallerImpl.java:136)
        at org.apache.ignite.raft.jraft.core.FSMCallerImpl$ApplyTaskHandler.onEvent(FSMCallerImpl.java:130)
        at org.apache.ignite.raft.jraft.disruptor.StripedDisruptor$StripeEntryHandler.onEvent(StripedDisruptor.java:217)
        at org.apache.ignite.raft.jraft.disruptor.StripedDisruptor$StripeEntryHandler.onEvent(StripedDisruptor.java:181)
        at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:137)
        at java.lang.Thread.run(java.base@13.0.1/Thread.java:830){code}",ibessonov,Denis Chudov,Major,Resolved,Fixed,29/May/23 15:18,19/Jun/23 12:17
Bug,IGNITE-19595,13538086,ItClusterManagerTest#testClusterConfigurationIsRemovedFromClusterStateAfterUpdating increase timeout,"{code:java}
assertThat(newLeaderNode.clusterManager().clusterConfigurationToUpdate(), willThrow(CancellationException.class)); {code}
1 second is not enough for the node to cancel the action. ",ivan.gagarkin,ivan.gagarkin,Major,Resolved,Fixed,30/May/23 08:51,30/May/23 12:43
Bug,IGNITE-19596,13538089,Partition creation fails with NSFE,"You can reproduce it by creating tables in a loop. I did that with fsync disabled.
{noformat}
java.util.concurrent.CompletionException: java.io.UncheckedIOException: java.nio.file.NoSuchFileException: build/work/ItSqlLogicTest/static_2773294090214/sqllogic0/db/db/table-89/part-6-delta-0.bin.tmp
    at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314)
    at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:683)
    at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
    at java.base/java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:2094)
    at org.apache.ignite.internal.storage.util.MvPartitionStorages.create(MvPartitionStorages.java:122)
    at org.apache.ignite.internal.storage.pagememory.AbstractPageMemoryTableStorage.lambda$createMvPartition$5(AbstractPageMemoryTableStorage.java:173)
    at org.apache.ignite.internal.util.IgniteUtils.inBusyLock(IgniteUtils.java:877)
    at org.apache.ignite.internal.storage.pagememory.AbstractPageMemoryTableStorage.busy(AbstractPageMemoryTableStorage.java:229)
    at org.apache.ignite.internal.storage.pagememory.AbstractPageMemoryTableStorage.createMvPartition(AbstractPageMemoryTableStorage.java:173)
    at org.apache.ignite.internal.table.distributed.TableManager.getOrCreatePartitionStorages(TableManager.java:2288)
    at org.apache.ignite.internal.table.distributed.TableManager.lambda$updateAssignmentInternal$21(TableManager.java:682)
    at org.apache.ignite.internal.util.IgniteUtils.inBusyLock(IgniteUtils.java:877)
    at org.apache.ignite.internal.table.distributed.TableManager.lambda$updateAssignmentInternal$22(TableManager.java:648)
    at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
    at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
    at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
    at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
    at org.apache.ignite.internal.causality.BaseVersionedValue.lambda$copyState$6(BaseVersionedValue.java:307)
    at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
    at java.base/java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:883)
    at java.base/java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2251)
    at org.apache.ignite.internal.causality.BaseVersionedValue.copyState(BaseVersionedValue.java:303)
    at org.apache.ignite.internal.causality.BaseVersionedValue.complete(BaseVersionedValue.java:189)
    at org.apache.ignite.internal.causality.IncrementalVersionedValue.lambda$completeInternal$1(IncrementalVersionedValue.java:236)
    at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
    at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
    at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
    at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
    at org.apache.ignite.internal.raft.RaftGroupServiceImpl.lambda$sendWithRetry$39(RaftGroupServiceImpl.java:540)
    at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
    at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
    at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
    at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
    at org.apache.ignite.network.DefaultMessagingService.onInvokeResponse(DefaultMessagingService.java:371)
    at org.apache.ignite.network.DefaultMessagingService.send0(DefaultMessagingService.java:194)
    at org.apache.ignite.network.DefaultMessagingService.respond(DefaultMessagingService.java:137)
    at org.apache.ignite.network.MessagingService.respond(MessagingService.java:89)
    at org.apache.ignite.raft.jraft.rpc.impl.IgniteRpcServer$NetworkRpcContext.sendResponse(IgniteRpcServer.java:233)
    at org.apache.ignite.raft.jraft.rpc.impl.ActionRequestProcessor$1.result(ActionRequestProcessor.java:104)
    at org.apache.ignite.internal.raft.server.impl.JraftServerImpl$DelegatingStateMachine$1$1.result(JraftServerImpl.java:631)
    at org.apache.ignite.internal.metastorage.server.raft.MetaStorageWriteHandler.handleWriteWithTime(MetaStorageWriteHandler.java:171)
    at org.apache.ignite.internal.metastorage.server.raft.MetaStorageWriteHandler.handleWriteCommand(MetaStorageWriteHandler.java:90)
    at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)
    at org.apache.ignite.internal.metastorage.server.raft.MetaStorageListener.onWrite(MetaStorageListener.java:147)
    at org.apache.ignite.internal.raft.server.impl.JraftServerImpl$DelegatingStateMachine.onApply(JraftServerImpl.java:592)
    at org.apache.ignite.raft.jraft.core.FSMCallerImpl.doApplyTasks(FSMCallerImpl.java:561)
    at org.apache.ignite.raft.jraft.core.FSMCallerImpl.doCommitted(FSMCallerImpl.java:529)
    at org.apache.ignite.raft.jraft.core.FSMCallerImpl.runApplyTask(FSMCallerImpl.java:448)
    at org.apache.ignite.raft.jraft.core.FSMCallerImpl$ApplyTaskHandler.onEvent(FSMCallerImpl.java:136)
    at org.apache.ignite.raft.jraft.core.FSMCallerImpl$ApplyTaskHandler.onEvent(FSMCallerImpl.java:130)
    at org.apache.ignite.raft.jraft.disruptor.StripedDisruptor$StripeEntryHandler.onEvent(StripedDisruptor.java:217)
    at org.apache.ignite.raft.jraft.disruptor.StripedDisruptor$StripeEntryHandler.onEvent(StripedDisruptor.java:181)
    at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:137)
    at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.UncheckedIOException: java.nio.file.NoSuchFileException: build/work/ItSqlLogicTest/static_2773294090214/sqllogic0/db/db/table-89/part-6-delta-0.bin.tmp
    at java.base/java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:87)
    at java.base/java.nio.file.FileTreeIterator.hasNext(FileTreeIterator.java:103)
    at java.base/java.util.Iterator.forEachRemaining(Iterator.java:132)
    at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
    at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
    at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
    at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:550)
    at java.base/java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)
    at java.base/java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:517)
    at org.apache.ignite.internal.pagememory.persistence.store.FilePageStoreManager.findPartitionDeltaFiles(FilePageStoreManager.java:429)
    at org.apache.ignite.internal.pagememory.persistence.store.FilePageStoreManager.initialize(FilePageStoreManager.java:293)
    at org.apache.ignite.internal.storage.pagememory.PersistentPageMemoryTableStorage.ensurePartitionFilePageStoreExists(PersistentPageMemoryTableStorage.java:158)
    at org.apache.ignite.internal.storage.pagememory.PersistentPageMemoryTableStorage.getOrCreatePartitionMetaOnCreatePartition(PersistentPageMemoryTableStorage.java:550)
    at org.apache.ignite.internal.storage.pagememory.PersistentPageMemoryTableStorage.createMvPartitionStorage(PersistentPageMemoryTableStorage.java:116)
    at org.apache.ignite.internal.storage.pagememory.PersistentPageMemoryTableStorage.createMvPartitionStorage(PersistentPageMemoryTableStorage.java:56)
    at org.apache.ignite.internal.storage.pagememory.AbstractPageMemoryTableStorage.lambda$createMvPartition$4(AbstractPageMemoryTableStorage.java:174)
    at org.apache.ignite.internal.storage.util.MvPartitionStorages.lambda$create$1(MvPartitionStorages.java:123)
    at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:680)
    ... 52 more
Caused by: java.nio.file.NoSuchFileException: build/work/ItSqlLogicTest/static_2773294090214/sqllogic0/db/db/table-89/part-6-delta-0.bin.tmp
    at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
    at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
    at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
    at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
    at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
    at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
    at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
    at java.base/java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219)
    at java.base/java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276)
    at java.base/java.nio.file.FileTreeWalker.next(FileTreeWalker.java:373)
    at java.base/java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:83)
    ... 69 more
{noformat}",ktkalenko@gridgain.com,ibessonov,Major,Resolved,Fixed,30/May/23 09:10,02/Jun/23 11:22
Bug,IGNITE-19602,13538210,API and tests for causality data nodes in DistributionZoneManager,"h3. *Motivation*

In https://issues.apache.org/jira/browse/IGNITE-19506 we will create implementation for getting data nodes from zone manager with causality token. In this ticket we need to create API, test plan and implement tests with test infrastructure.
h3. *Definition of Done*

API and tests for causality data nodes in DistributionZoneManager are implemented.

 

*Test plan*

*Test1* topologyLeapUpdate
Prerequisite:
1. zonesByRevision contains [revision=5, zoneId==1 -> [nodeA, nodeB]]
2. zonesByRevision contains [revision=5, zoneId==2 -> [nodeA, nodeB]]
3. A zone1 with immediate scaleUp and immediate scaleDown.
4. A zone2 with scaleUp=1 and scaleDown=1 timers.

Test steps:
1. Invoke `fut1=versionedDataNodes(zoneId=1, revision=6)`
2. Invoke `fut2=versionedDataNodes(zoneId=2, revision=6)`
3. Check that `!fut1.isDone()`.
4. Check that `!fut2.isDone()`.
5. Change topology to [nodeA, nodeC]. Topology revision is 6.
6. assertThat(fut1, willSucceedFast())
7. assertThat(fut2, willSucceedFast())
8. Check that fut1 is completed with [nodeA, nodeC]
9. Check that fut2 is completed with [nodeA, nodeB].
10. Wait when data nodes for zone2 is updated in the metastorage (with revision x).
11. Invoke `fut3=versionedDataNodes(zoneId=2, revision=x)`
12. Check that fut3 is completed with [nodeA, nodeC].

Check this test case for a zone with default and custom zones.

*Test2* topologyLeapUpdateScaleUpNotImmediateAndScaleDownImmediate
Prerequisite:
1. zonesByRevision contains [revision=5, zoneId==1 -> [nodeA, nodeB]]
4. A zone1 with scaleUp=1 and scaleDown=immediate timers.

Test steps:
1. Invoke `fut1=versionedDataNodes(zoneId=1, revision=6)`
2. Check that `!fut1.isDone()`.
3. Change topology to [nodeA, nodeC]. Topology revision is 6.
4. assertThat(fut1, willSucceedFast())
5. Check that fut1 is completed with [nodeA, nodeB, nodeC].
6. Wait when data nodes for zone2 is updated in the metastorage (with revision x).
7. Invoke `fut2=versionedDataNodes(zoneId=1, revision=x)`
8. Check that fut2 is completed with [nodeA, nodeC].

Check this test case for a zone with default and custom zone.

*Test3* topologyLeapUpdateScaleUpImmediateAndScaleDownNotImmediate
Prerequisite:
1. zonesByRevision contains [revision=5, zoneId==1 -> [nodeA, nodeB]]
2. A zone1 with scaleUp=immediate and scaleDown=1 timers.

Test steps:
1. Invoke `fut1=versionedDataNodes(zoneId=1, revision=6)`
2. Check that `!fut1.isDone()`.
3. Change topology to [nodeA, nodeC]. Topology revision is 6.
4. assertThat(fut1, willSucceedFast())
5. Check that fut1 is completed with [nodeA].
6. Wait when data nodes for zone1 is updated in the metastorage (with revision x).
7. Invoke `fut2=versionedDataNodes(zoneId=1, revision=x)`
8. Check that fut2 is completed with [nodeA, nodeC].

Check this test case for a zone with default and custom zone.

*Test4* (Change scaleUp to immediate) dataNodesUpdatedAfterScaleUpChanged
Prerequisite:
1. zonesByRevision contains [revision=5, zoneId==1 -> [nodeA]]
2. A zone1 with scaleUp=10000 timer and immediate scaleDown timer.

Test steps:
1. Invoke `fut1=versionedDataNodes(zoneId=1, revision=6)`
2. Check that `!fut1.isDone()`.
3. Change topology to [nodeA, nodeB]. Topology revision is 6.
4. assertThat(fut1, willSucceedFast()) with [nodeA].
5. Change scaleUp to immediate. The configuration change revision is 7.
6. Invoke `fut2=versionedDataNodes(zoneId=1, revision=6)`. Check that fut2 is completed with [nodeA].
7. Invoke `fut3=versionedDataNodes(zoneId=1, revision=7)`. Check that fut3 is completed with [nodeA, nodeB].

Check this test case for a zone with default and custom zone.

*Test5* (Change scaleDown to immediate) dataNodesUpdatedAfterScaleDownChanged
Prerequisite:
1. zonesByRevision contains [revision=5, zoneId==1 -> [nodeA, nodeB]]
2. A zone1 with immediate scaleUp timer and scaleDown=10000.

Test steps:
1. Invoke `fut1=versionedDataNodes(zoneId=1, revision=6)`
2. Check that `!fut1.isDone()`.
3. Change topology to [nodeA]. Topology revision is 6.
4. assertThat(fut1, willSucceedFast()) with [nodeA, nodeB].
5. Change scaleDown to immediate. The configuration change revision is 7.
6. Invoke `fut2=versionedDataNodes(zoneId=1, revision=6)`. Check that fut2 is completed with [nodeA, nodeB].
7. Invoke `fut3=versionedDataNodes(zoneId=1, revision=7)`. Check that fut3 is completed with [nodeA].

Check this test case for a zone with default and custom zone.

*Test6* (drop zone) scheduleScaleUpTaskThenDropZone
Prerequisite:
1. zonesByRevision contains [revision=5, zoneId==1 -> [nodeA]]
2. A zone1 with scaleUp=10000 timer and immediate scaleDown timer.

Test steps:
1. Invoke `fut1=versionedDataNodes(zoneId=1, revision=6)`
2. Check that `!fut1.isDone()`.
3. Change topology to [nodeA, nodeB]. Topology revision is 6.
4. assertThat(fut1, willSucceedFast()) with [nodeA].
5. Drop the zone. The drop event revision is 7.
6. Invoke `fut2=versionedDataNodes(zoneId=1, revision=6)`. Check that fut2 is completed with [nodeA].
7. Invoke `fut3=versionedDataNodes(zoneId=1, revision=7)`. Check that fut3 is completed with exception DistributionZoneNotFoundException.

Check this test case for a zone with default and custom zone.

*Test7* (drop zone) scheduleScaleDownTaskThenDropZone
Prerequisite:
1. zonesByRevision contains [revision=5, zoneId==1 -> [nodeA, nodeB]]
2. A zone1 with immediate scaleUp timer and scaleDown=10000 timer.

Test steps:
1. Invoke `fut1=versionedDataNodes(zoneId=1, revision=6)`
2. Check that `!fut1.isDone()`.
3. Change topology to [nodeA]. Topology revision is 6.
4. assertThat(fut1, willSucceedFast()) with [nodeA, nodeB].
5. Drop the zone. The drop event revision is 7.
6. Invoke `fut2=versionedDataNodes(zoneId=1, revision=6)`. Check that fut2 is completed with [nodeA, nodeB].
7. Invoke `fut3=versionedDataNodes(zoneId=1, revision=7)`. Check that fut3 is completed with exception DistributionZoneNotFoundException.

Check this test case for a zone with default and custom zone.

*Test8* createThenDropZone
Prerequisite:
1. The last applied revision is 6.
2. The zone with zoneId=1 is not created.
3. Current topology: [nodeA, nodeB].

Test steps:
1. Invoke `fut1=versionedDataNodes(zoneId=1, revision=6)`
2. Check that fut1 is completed exceptionally with DistributionZoneNotFoundException.
3. Create zone with zoneId=1. A create zone revision is 7.
4. Invoke `fut2=versionedDataNodes(zoneId=1, revision=7)`
5. Check that fut2 is completed with current topology data nodes.
6. Drop zone with zoneId=1. A drop zone revision is 8.
7. Invoke `fut3=versionedDataNodes(zoneId=1, revision=8)`
8. Check that fut3 is completed exceptionally with DistributionZoneNotFoundException.

Check this test case for a zone with immediate and not immediate timers, default and custom zones.

*Test9* validationTest
Test steps:
1. Invoke `fut1=versionedDataNodes(zoneId=0, revision=0)`
2. Check that fut1 is completed exceptionally with IllegalArgumentException (or AssertionError).
3. Invoke `fut1=versionedDataNodes(zoneId=0, revision=-1)`
4. Check that fut1 is completed exceptionally with Exception (or AssertionError).

*Test10* simpleTopologyChanges
Test steps:
1. Do topology changes:
revision 1 - [nodeA],
revision 5 - [nodeA, nodeB]
2. Create zone1 with zoneId=1 and immediate timers. Create event revision is 7.
4. Get futures:
Invoke `versionedDataNodes(zoneId=1, revision=7)`,
Invoke `versionedDataNodes(zoneId=1, revision=10)`,
Invoke `versionedDataNodes(zoneId=1, revision=15)`.
5. Check that futures are not completed.
3. Do topology changes:
revision 10 - [nodeA, nodeB, nodeC],
revision 15 - [nodeA, nodeC].
5. Check that futures are completed with corresponds values:
revision 7 - [nodeA, nodeB],
revision 10 - [nodeA, nodeB, nodeC],
revision 15 - [nodeA, nodeC].",Sergey Uttsel,Sergey Uttsel,Major,Resolved,Fixed,31/May/23 07:38,19/Jun/23 07:42
Bug,IGNITE-19605,13538226,ItDeploymentUnitTest is flaky,"Even after the IGNITE-19139 was fixed, the {{ItDeploymentUnitTest}} is still flaky, the root cause is still looks like the Micronaut's shared converters registry. We should look for a better workaround.",vpakhnushev,vpakhnushev,Major,Resolved,Fixed,31/May/23 08:55,02/Jun/23 09:53
Bug,IGNITE-19606,13538235,Linearize metaStorageManager.deployWatches and metaStorageManager.start(),"h3. Motivation

Occurred that in some tests deployWatches was called on not started metaStorageManager which leads to failures and unpredictable artifacts. In order to prevent such possibility it's required to linearize it though metaStorageSvcFut.
h3. Definition of Done

Calling deployWatches will chain with metaStorageManager.start().
h3. Implementation Notes
 * I don't think that it's worth to prepare some general solution for all IgniteComponents that will prevent calling their methods before start, for now, it's only about MetaStorage and deployWatches.
 * MetaStorageManager has two implementations MetaStorageManagerImpl and StandaloneMetaStorageManager, so we should have two tests for a given logic.
 * Linearization itself might be implemented though `metaStorageSvcFut.thenRun(() -> \{storage.startWatches(this::onRevisionApplied);});`

 ",v.pyatkov,alapin,Major,Resolved,Fixed,31/May/23 09:19,15/Jun/23 14:01
Bug,IGNITE-19607,13538257,Flaky test fix - testStreamingIntoInMemoryDoesntAffectSnapshot(),"+
{code:java}
awaitPartitionMapExchange();
{code}
after
{code:java}
destroyCache()
{code}
to avoid
{code:java}
IgniteCheckedException: Unable to restore cache group - directory is not empty.
{code}
",vladsz83,vladsz83,Minor,Resolved,Fixed,31/May/23 11:55,01/Jun/23 11:02
Bug,IGNITE-19615,13538374,Index is not used while performing SELECT over an indexed column,"Apache Ignite 3, rev. faac3854f687daab2de4580fd9666fb227bf4c3a
 
Performing SELECT on an indexed column does not utilize the index. This is shown on EXPLAIN query.
 
{noformat}
11:35:27.760 [Test worker] INFO org.gridgain.ai3tests.tests.IndexTests – Update: create table index_test_table_1(id INTEGER not null, field_1 TINYINT, field_2 SMALLINT, field_3 INTEGER, field_4 FLOAT, field_5 VARCHAR(50), primary key (id))
11:35:28.924 [Test worker] INFO org.gridgain.ai3tests.tests.IndexTests – Update: CREATE INDEX index_test_index_1_1 ON index_test_table_1 (field_2)
Jun 01, 2023 11:35:28 AM org.apache.ignite.internal.logger.IgniteLogger logInternal
INFO: Partition assignment change notification received [remoteAddress=localhost:10801]
11:35:29.003 [Test worker] INFO org.gridgain.ai3tests.tests.IndexTests – Update: INSERT INTO index_test_table_1 (id, field_1, field_2, field_3, field_4, field_5) VALUES (0, 0, 0, 0, 0.0, 'string_value_0')
11:35:30.529 [Test worker] INFO org.gridgain.ai3tests.tests.IndexTests – Update: INSERT INTO index_test_table_1 (id, field_1, field_2, field_3, field_4, field_5) VALUES (1, 1, 1, 1, 1.1, 'string_value_1')
11:35:30.571 [Test worker] INFO org.gridgain.ai3tests.tests.IndexTests – Update: INSERT INTO index_test_table_1 (id, field_1, field_2, field_3, field_4, field_5) VALUES (2, 2, 2, 2, 2.2, 'string_value_2')
...
11:35:33.189 [Test worker] INFO org.gridgain.ai3tests.tests.IndexTests – Update: INSERT INTO index_test_table_1 (id, field_1, field_2, field_3, field_4, field_5) VALUES (97, 97, 97, 97, 106.7, 'string_value_97')
11:35:33.212 [Test worker] INFO org.gridgain.ai3tests.tests.IndexTests – Update: INSERT INTO index_test_table_1 (id, field_1, field_2, field_3, field_4, field_5) VALUES (98, 98, 98, 98, 107.80000000000001, 'string_value_98')
11:35:33.236 [Test worker] INFO org.gridgain.ai3tests.tests.IndexTests – Update: INSERT INTO index_test_table_1 (id, field_1, field_2, field_3, field_4, field_5) VALUES (99, 99, 99, 99, 108.9, 'string_value_99')
11:35:33.258 [Test worker] INFO org.gridgain.ai3tests.tests.IndexTests – Query: SELECT * FROM index_test_table_1 WHERE field_2 = 50
11:35:33.513 [Test worker] INFO org.gridgain.ai3tests.tests.IndexTests – Query: explain plan for SELECT * FROM index_test_table_1 WHERE field_2 = 50
11:35:33.565 [Test worker] INFO org.gridgain.ai3tests.tests.IndexTests – Result: IgniteExchange(distribution=[single]): rowcount = 1500.0, cumulative cost = IgniteCost [rowCount=11500.0, cpu=41500.0, memory=0.0, io=240000.0, network=36000.0], id = 10103
IgniteTableScan(table=[[PUBLIC, INDEX_TEST_TABLE_1]], tableId=[8239005d-346b-40d9-afe7-c15923ef3020], filters=[=(CAST($t2):INTEGER, 50)], requiredColumns=[

{0, 1, 2, 3, 4, 5}

]): rowcount = 1500.0, cumulative cost = IgniteCost [rowCount=10000.0, cpu=40000.0, memory=0.0, io=240000.0, network=0.0], id = 10102
{noformat}


It is expected that {{explain plan for SELECT * FROM index_test_table_1 WHERE field_2 = 50}} query will contain {{IgniteIndexScan` with `index_test_index_1_1}}.",mzhuravkov,Artukhov,Major,Resolved,Fixed,01/Jun/23 08:44,04/Jul/23 15:19
Bug,IGNITE-19621,13538413,Slow query planning,"Query from the TPC-H benchmark took 13 to 18 seconds to plan (try to execute on an empty TPCH tables)

Problem query is:
{code:java}
select   sum(case when nation = 'aaaa' then volume else 0 end) / sum(volume) as mkt_share , o_year
from ( 
   select floor(UNIX_MILLIS(o_orderdate) / (cast (365 as bigint) * 86400000))  as o_year, 
   l_extendedprice * (1 - l_discount) as volume, 
   n2.n_name as nation 
   from part, supplier, lineitem, orders, customer, nation n1, nation n2, region 
   where p_partkey = l_partkey and s_suppkey = l_suppkey and l_orderkey = o_orderkey and o_custkey = c_custkey and c_nationkey = n1.n_nationkey 
       and n1.n_regionkey = r_regionkey and r_name = 'rrr2' and s_nationkey = n2.n_nationkey 
       and o_orderdate between '2020-01-01' and '2020-03-01'
       and p_type = 1111
) as all_nations 
group by o_year 
order by o_year     
{code}
Second run took about 50ms (query cache works fine).

See ddl in attachment.",zstan,Berkov,Major,Resolved,Fixed,01/Jun/23 12:45,23/Jun/23 14:23
Bug,IGNITE-19627,13538437,Fix ItReadOnlyTransactionTest failed tests,"Tests from {{ItReadOnlyTransactionTest}} started to fail

https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_IntegrationTests_ModuleRunner/7264289?hideProblemsFromDependencies=false&hideTestsFromDependencies=false&expandBuildProblemsSection=true&expandBuildChangesSection=true&expandBuildTestsSection=true


{code:java}
org.opentest4j.AssertionFailedError: Too long to execute [delay=306] ==> expected: <true> but was: <false>
{code}



{code:java}
org.opentest4j.AssertionFailedError: expected: <str 100> but was: <new str 100>
{code}


The root cause is that the tests were sharing the same table, and when one test failed ({{testFutureRead}}), the second one ({{testPastRead}}) started to fail because table state was wrong. Previously the state of the table was cleaned at the tests itself, not in the {{@ AfterEach}} block.

The {{testFutureRead()}} test failure was connected with the fact that test run was too slow and invariant for read from future was violated. I propose to increase a bit {{FUTURE_GAP}} -- the gap for read in future for RO tx. ",maliev,maliev,Major,Resolved,Fixed,01/Jun/23 15:20,01/Jun/23 16:08
Bug,IGNITE-19634,13538562,Fix PMD error when building the SQL module,"When building the project, I get the following error when executing the {{:ignite-sql-engine:pmdMain}} task:

{noformat}
PMDException: Error while parsing modules/sql-engine/src/main/java/org/apache/ignite/internal/sql/engine/exec/exp/RexToLixTranslator.java
{noformat}

Looks like updating the PMD plugin solves this issue.",apolovtcev,apolovtcev,Trivial,Resolved,Fixed,02/Jun/23 10:09,02/Jun/23 12:34
Bug,IGNITE-19641,13538578,Catalog events are triggered too early.,"As of now, Catalog events are triggered before a new version of Catalog is registered.
Catalog events doesn’t provide a new version of Catalog itself, but only a version number.
Thus, events doesn’t contain enough information in a context, which some components may need, and Catalog doesn’t contains a new version.",ktkalenko@gridgain.com,amashenkov,Major,Resolved,Fixed,02/Jun/23 11:21,23/Jun/23 07:22
Bug,IGNITE-19642,13538579,CatalogService should use HybridClock to register new Catalog version.,"As of now we use System.currentTimestamp to assign timestamp for a new Catalog version.
Let's use HybridClock instead.

Also, there are bunch of Thread.sleep(5) in CatalogServiceSelftTest tests as a workaround for this issue. Let's also remove any waiting in tests.",amashenkov,amashenkov,Major,Resolved,Fixed,02/Jun/23 11:24,09/Jun/23 14:28
Bug,IGNITE-19648,13538615,Failed to cancel rebalance,"{noformat}
Caused by: org.apache.ignite.internal.storage.StorageRebalanceException: IGN-STORAGE-4 TraceId:8e22c456-103e-4445-9b35-e021e22cd615 Storage in the process of starting a rebalance: [table=TEST, partitionId=0]
    at org.apache.ignite.internal.storage.util.MvPartitionStorages.throwExceptionDependingOnOperationForRebalance(MvPartitionStorages.java:400)
    at org.apache.ignite.internal.storage.util.MvPartitionStorages.lambda$abortRebalance$16(MvPartitionStorages.java:265)
    at java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1932)
    at org.apache.ignite.internal.storage.util.MvPartitionStorages.abortRebalance(MvPartitionStorages.java:261)
    at org.apache.ignite.internal.storage.pagememory.AbstractPageMemoryTableStorage.lambda$abortRebalancePartition$17(AbstractPageMemoryTableStorage.java:250)
    at org.apache.ignite.internal.util.IgniteUtils.inBusyLock(IgniteUtils.java:897)
    at org.apache.ignite.internal.storage.pagememory.AbstractPageMemoryTableStorage.abortRebalancePartition(AbstractPageMemoryTableStorage.java:250)    at org.apache.ignite.internal.table.distributed.raft.snapshot.PartitionAccessImpl.abortRebalance(PartitionAccessImpl.java:221){noformat}
Unfortunately, the entire log is lost. Issue occurred in one of org.apache.ignite.internal.raftsnapshot.ItTableRaftSnapshotsTest tests.",ktkalenko@gridgain.com,ibessonov,Major,Resolved,Fixed,02/Jun/23 14:16,07/Jun/23 08:50
Bug,IGNITE-19650,13538712,Broken serialization of communication messages due to incorrect GridCacheQueryRequest class marshalling.,"GridCacheQueryRequest class writeTo/readForm methods does not increment message writer/reader state sequentially. Currently idxQryDescBytes field corresponds message reader/writer state with number 27. But state number 26 is skipped. This can lead to the following situation:

We write on sender node all fields preceding idxQryDescBytes to byte buffer. But no  space remains for idxQryDescBytes. So we save the state with number 26 and returns.
When enough free space become available - we do not find any field that corresponds 26 state and complete serialization without idxQryDescBytes field being recorded.

Receiver node tries to read all fields including  idxQryDescBytes which was not recorded in the byte buffer by the sender node. As a result receiver node can read a part of the next message stored in the byte buffer while trying to read skipped idxQryDescBytes field. This results in corrupted serialization of communication messages.

User can face the following exceptions in this case. The concrete exception depends on the message that come after GridCacheQueryRequest.
{code:java}
2023-05-31 14:49:02.249 [ERROR][grid-nio-worker-tcp-comm-15-#134%TcpCommunicationSpi%][org.apache.ignite.internal.util.nio.GridDirectParser] Failed to read message [msg=GridIoMessage [plc=0, topic=null, topicOrd=-1, ordered=false, timeout=0, skipOnTimeout=false, msg=null], buf=java.nio.DirectByteBuffer[pos=2 lim=16384 cap=132678], reader=DirectMessageReader [state=DirectMessageState [pos=0, stack=[StateItem [stream=DirectByteBufferStreamImplV2 [baseOff=139878897874032, arrOff=-1, tmpArrOff=0, valReadBytes=0, tmpArrBytes=0, msgTypeDone=true, msg=GridDhtAtomicNearResponse [partId=0, futId=0, primaryId=null, errs=null, flags=], mapIt=null, it=null, arrPos=-1, keyDone=false, readSize=-1, readItems=0, prim=0, primShift=0, uuidState=0, uuidMost=0, uuidLeast=0, uuidLocId=0], state=0], StateItem [stream=DirectByteBufferStreamImplV2 [baseOff=139878897874032, arrOff=-1, tmpArrOff=0, valReadBytes=0, tmpArrBytes=0, msgTypeDone=false, msg=null, mapIt=null, it=null, arrPos=-1, keyDone=false, readSize=-1, readItems=0, prim=0, primShift=0, uuidState=0, uuidMost=0, uuidLeast=0, uuidLocId=0], state=0], StateItem [stream=DirectByteBufferStreamImplV2 [baseOff=16, arrOff=-1, tmpArrOff=0, valReadBytes=0, tmpArrBytes=0, msgTypeDone=false, msg=null, mapIt=null, it=null, arrPos=-1, keyDone=false, readSize=-1, readItems=0, prim=0, primShift=0, uuidState=0, uuidMost=0, uuidLeast=0, uuidLocId=0], state=0], StateItem [stream=DirectByteBufferStreamImplV2 [baseOff=16, arrOff=-1, tmpArrOff=0, valReadBytes=0, tmpArrBytes=0, msgTypeDone=false, msg=null, mapIt=null, it=null, arrPos=-1, keyDone=false, readSize=-1, readItems=0, prim=0, primShift=0, uuidState=0, uuidMost=0, uuidLeast=0, uuidLocId=0], state=0], null, null, null, null, null, null]], protoVer=3, lastRead=false], ses=GridSelectorNioSessionImpl [worker=DirectNioClientWorker [super=AbstractNioClientWorker [idx=15, bytesRcvd=183689349864, bytesSent=0, bytesRcvd0=24790, bytesSent0=0, select=true, super=GridWorker [name=grid-nio-worker-tcp-comm-15, igniteInstanceName=TcpCommunicationSpi, finished=false, heartbeatTs=1685533742239, hashCode=864882861, interrupted=false, runner=grid-nio-worker-tcp-comm-15-#134%TcpCommunicationSpi%]]], writeBuf=java.nio.DirectByteBuffer[pos=0 lim=32768 cap=32768], readBuf=java.nio.DirectByteBuffer[pos=24790 lim=24790 cap=32768], inRecovery=GridNioRecoveryDescriptor [acked=713638068, resendCnt=532, rcvCnt=738238432, sentCnt=713638600, reserved=true, lastAck=738238432, nodeLeft=false, node=TcpDiscoveryNode [id=0881a6ed-5a62-4564-99ad-7cf1bc8e0288, consistentId=tps_ise_pplad-pprbtspm0053.ca.sbrf.ru, addrs=ArrayList [10.127.119.241, 127.0.0.1, 192.168.122.1], sockAddrs=HashSet [/192.168.122.1:47500, /10.127.119.241:47500, /127.0.0.1:47500], discPort=47500, order=6, intOrder=6, lastExchangeTime=1685394761595, loc=false, ver=14.1.0#20221221-sha1:24506aa7, isClient=false], connected=false, connectCnt=21, queueLimit=4096, reserveCnt=22, pairedConnections=false], outRecovery=GridNioRecoveryDescriptor [acked=713638068, resendCnt=532, rcvCnt=738238432, sentCnt=713638600, reserved=true, lastAck=738238432, nodeLeft=false, node=TcpDiscoveryNode [id=0881a6ed-5a62-4564-99ad-7cf1bc8e0288, consistentId=tps_ise_pplad-pprbtspm0053.ca.sbrf.ru, addrs=ArrayList [10.127.119.241, 127.0.0.1, 192.168.122.1], sockAddrs=HashSet [/192.168.122.1:47500, /10.127.119.241:47500, /127.0.0.1:47500], discPort=47500, order=6, intOrder=6, lastExchangeTime=1685394761595, loc=false, ver=14.1.0#20221221-sha1:24506aa7, isClient=false], connected=false, connectCnt=21, queueLimit=4096, reserveCnt=22, pairedConnections=false], closeSocket=true, outboundMessagesQueueSizeMetric=o.a.i.i.processors.metric.impl.LongAdderMetric@69a257d1, super=GridNioSessionImpl [locAddr=/10.97.19.164:31489, rmtAddr=/10.127.119.241:47100, createTime=1685533742239, closeTime=0, bytesSent=0, bytesRcvd=24790, bytesSent0=0, bytesRcvd0=24790, sndSchedTime=1685533742239, lastSndTime=1685533742239, lastRcvTime=1685533742239, readsPaused=false, filterChain=FilterChain[filters=[GridNioCodecFilter [parser=o.a.i.i.util.nio.GridDirectParser@7529b084, directMode=true], GridConnectionBytesVerifyFilter, SSL filter], accepted=false, markedForClose=false]]]
org.apache.ignite.IgniteException: Invalid message type: 384
    at org.apache.ignite.internal.managers.communication.IgniteMessageFactoryImpl.create(IgniteMessageFactoryImpl.java:133) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.spi.communication.tcp.internal.GridNioServerWrapper$2.create(GridNioServerWrapper.java:813) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.direct.stream.v2.DirectByteBufferStreamImplV2.readMessage(DirectByteBufferStreamImplV2.java:1189) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.direct.DirectMessageReader.readMessage(DirectMessageReader.java:336) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.processors.cache.GridCacheMessage.readFrom(GridCacheMessage.java:710) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.processors.cache.GridCacheIdMessage.readFrom(GridCacheIdMessage.java:91) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.processors.cache.distributed.dht.atomic.GridDhtAtomicNearResponse.readFrom(GridDhtAtomicNearResponse.java:255) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.direct.stream.v2.DirectByteBufferStreamImplV2.readMessage(DirectByteBufferStreamImplV2.java:1200) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.direct.DirectMessageReader.readMessage(DirectMessageReader.java:336) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.managers.communication.GridIoMessage.readFrom(GridIoMessage.java:271) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.managers.communication.GridIoSecurityAwareMessage.readFrom(GridIoSecurityAwareMessage.java:123) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridDirectParser.decode(GridDirectParser.java:90) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioCodecFilter.onMessageReceived(GridNioCodecFilter.java:113) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioFilterAdapter.proceedMessageReceived(GridNioFilterAdapter.java:109) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridConnectionBytesVerifyFilter.onMessageReceived(GridConnectionBytesVerifyFilter.java:88) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioFilterAdapter.proceedMessageReceived(GridNioFilterAdapter.java:109) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.ssl.GridNioSslFilter.onMessageReceived(GridNioSslFilter.java:408) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioFilterAdapter.proceedMessageReceived(GridNioFilterAdapter.java:109) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioServer$HeadFilter.onMessageReceived(GridNioServer.java:3752) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioFilterChain.onMessageReceived(GridNioFilterChain.java:175) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioServer$DirectNioClientWorker.processRead(GridNioServer.java:1379) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioServer$AbstractNioClientWorker.processSelectedKeysOptimized(GridNioServer.java:2526) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioServer$AbstractNioClientWorker.bodyInternal(GridNioServer.java:2281) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioServer$AbstractNioClientWorker.body(GridNioServer.java:1910) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.worker.GridWorker.run(GridWorker.java:125) ~[ignite-core-14.1.0.jar:14.1.0]
    at java.lang.Thread.run(Thread.java:829) ~[?:?] {code}
 
{code:java}
2023-05-31 00:26:50.270 [ERROR][grid-nio-worker-tcp-comm-7-#126%TcpCommunicationSpi%][org.apache.ignite.spi.communication.tcp.TcpCommunicationSpi] Failed to process selector key [ses=GridSelectorNioSessionImpl [worker=DirectNioClientWorker [super=AbstractNioClientWorker [idx=7, bytesRcvd=307961844272, bytesSent=0, bytesRcvd0=788711, bytesSent0=0, select=true, super=GridWorker [name=grid-nio-worker-tcp-comm-7, igniteInstanceName=TcpCommunicationSpi, finished=false, heartbeatTs=1685482009267, hashCode=1355815730, interrupted=false, runner=grid-nio-worker-tcp-comm-7-#126%TcpCommunicationSpi%]]], writeBuf=java.nio.DirectByteBuffer[pos=0 lim=32594 cap=32768], readBuf=java.nio.DirectByteBuffer[pos=11514 lim=11514 cap=32768], inRecovery=GridNioRecoveryDescriptor [acked=435904000, resendCnt=0, rcvCnt=478893674, sentCnt=435904020, reserved=true, lastAck=478893664, nodeLeft=false, node=TcpDiscoveryNode [id=6dbee697-1202-4d6c-b077-f0b00ec69adf, consistentId=tps_ise_pplad-pprbtspm0049.ca.sbrf.ru, addrs=ArrayList [10.97.19.163, 127.0.0.1, 192.168.122.1], sockAddrs=null, discPort=47500, order=3, intOrder=3, lastExchangeTime=1685394761693, loc=false, ver=14.1.0#20221221-sha1:24506aa7, isClient=false], connected=true, connectCnt=0, queueLimit=4096, reserveCnt=2, pairedConnections=false], outRecovery=GridNioRecoveryDescriptor [acked=435904000, resendCnt=0, rcvCnt=478893674, sentCnt=435904020, reserved=true, lastAck=478893664, nodeLeft=false, node=TcpDiscoveryNode [id=6dbee697-1202-4d6c-b077-f0b00ec69adf, consistentId=tps_ise_pplad-pprbtspm0049.ca.sbrf.ru, addrs=ArrayList [10.97.19.163, 127.0.0.1, 192.168.122.1], sockAddrs=null, discPort=47500, order=3, intOrder=3, lastExchangeTime=1685394761693, loc=false, ver=14.1.0#20221221-sha1:24506aa7, isClient=false], connected=true, connectCnt=0, queueLimit=4096, reserveCnt=2, pairedConnections=false], closeSocket=true, outboundMessagesQueueSizeMetric=o.a.i.i.processors.metric.impl.LongAdderMetric@69a257d1, super=GridNioSessionImpl [locAddr=/10.127.119.240:47100, rmtAddr=/10.97.19.163:44182, createTime=1685398068848, closeTime=0, bytesSent=270613275246, bytesRcvd=307961844272, bytesSent0=870481, bytesRcvd0=788711, sndSchedTime=1685398068848, lastSndTime=1685482009267, lastRcvTime=1685482009267, readsPaused=false, filterChain=FilterChain[filters=[GridNioCodecFilter [parser=o.a.i.i.util.nio.GridDirectParser@8f4fb5a, directMode=true], GridConnectionBytesVerifyFilter, SSL filter], accepted=true, markedForClose=false]]]
java.lang.AssertionError: null
    at org.apache.ignite.internal.binary.GridBinaryMarshaller.deserialize(GridBinaryMarshaller.java:299) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.binary.BinaryMarshaller.unmarshal0(BinaryMarshaller.java:101) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.marshaller.AbstractNodeNameAwareMarshaller.unmarshal(AbstractNodeNameAwareMarshaller.java:80) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.IgniteUtils.unmarshal(IgniteUtils.java:10742) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.managers.communication.GridIoManager.onMessage0(GridIoManager.java:1273) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.managers.communication.GridIoManager.access$300(GridIoManager.java:243) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.managers.communication.GridIoManager$2.onMessage(GridIoManager.java:509) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.spi.communication.tcp.TcpCommunicationSpi.notifyListener(TcpCommunicationSpi.java:1220) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.spi.communication.tcp.TcpCommunicationSpi$1.onMessage(TcpCommunicationSpi.java:689) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.spi.communication.tcp.TcpCommunicationSpi$1.onMessage(TcpCommunicationSpi.java:687) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.spi.communication.tcp.internal.InboundConnectionHandler.onMessage(InboundConnectionHandler.java:393) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.spi.communication.tcp.internal.InboundConnectionHandler.onMessage(InboundConnectionHandler.java:79) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioFilterChain$TailFilter.onMessageReceived(GridNioFilterChain.java:279) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioFilterAdapter.proceedMessageReceived(GridNioFilterAdapter.java:109) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioCodecFilter.onMessageReceived(GridNioCodecFilter.java:116) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioFilterAdapter.proceedMessageReceived(GridNioFilterAdapter.java:109) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridConnectionBytesVerifyFilter.onMessageReceived(GridConnectionBytesVerifyFilter.java:133) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioFilterAdapter.proceedMessageReceived(GridNioFilterAdapter.java:109) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.ssl.GridNioSslFilter.onMessageReceived(GridNioSslFilter.java:408) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioFilterAdapter.proceedMessageReceived(GridNioFilterAdapter.java:109) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioServer$HeadFilter.onMessageReceived(GridNioServer.java:3752) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioFilterChain.onMessageReceived(GridNioFilterChain.java:175) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioServer$DirectNioClientWorker.processRead(GridNioServer.java:1379) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioServer$AbstractNioClientWorker.processSelectedKeysOptimized(GridNioServer.java:2526) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioServer$AbstractNioClientWorker.bodyInternal(GridNioServer.java:2281) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioServer$AbstractNioClientWorker.body(GridNioServer.java:1910) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.worker.GridWorker.run(GridWorker.java:125) ~[ignite-core-14.1.0.jar:14.1.0]
    at java.lang.Thread.run(Thread.java:829) ~[?:?]
2023-05-31 00:26:50.271 [ERROR][grid-nio-worker-tcp-comm-7-#126%TcpCommunicationSpi%][org.apache.ignite.spi.communication.tcp.TcpCommunicationSpi] Closing NIO session because of unhandled exception.
org.apache.ignite.internal.util.nio.GridNioException: null
    at org.apache.ignite.internal.util.nio.GridNioServer$AbstractNioClientWorker.processSelectedKeysOptimized(GridNioServer.java:2552) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioServer$AbstractNioClientWorker.bodyInternal(GridNioServer.java:2281) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioServer$AbstractNioClientWorker.body(GridNioServer.java:1910) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.worker.GridWorker.run(GridWorker.java:125) ~[ignite-core-14.1.0.jar:14.1.0]
    at java.lang.Thread.run(Thread.java:829) ~[?:?]
Caused by: java.lang.AssertionError
    at org.apache.ignite.internal.binary.GridBinaryMarshaller.deserialize(GridBinaryMarshaller.java:299) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.binary.BinaryMarshaller.unmarshal0(BinaryMarshaller.java:101) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.marshaller.AbstractNodeNameAwareMarshaller.unmarshal(AbstractNodeNameAwareMarshaller.java:80) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.IgniteUtils.unmarshal(IgniteUtils.java:10742) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.managers.communication.GridIoManager.onMessage0(GridIoManager.java:1273) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.managers.communication.GridIoManager.access$300(GridIoManager.java:243) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.managers.communication.GridIoManager$2.onMessage(GridIoManager.java:509) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.spi.communication.tcp.TcpCommunicationSpi.notifyListener(TcpCommunicationSpi.java:1220) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.spi.communication.tcp.TcpCommunicationSpi$1.onMessage(TcpCommunicationSpi.java:689) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.spi.communication.tcp.TcpCommunicationSpi$1.onMessage(TcpCommunicationSpi.java:687) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.spi.communication.tcp.internal.InboundConnectionHandler.onMessage(InboundConnectionHandler.java:393) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.spi.communication.tcp.internal.InboundConnectionHandler.onMessage(InboundConnectionHandler.java:79) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioFilterChain$TailFilter.onMessageReceived(GridNioFilterChain.java:279) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioFilterAdapter.proceedMessageReceived(GridNioFilterAdapter.java:109) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioCodecFilter.onMessageReceived(GridNioCodecFilter.java:116) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioFilterAdapter.proceedMessageReceived(GridNioFilterAdapter.java:109) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridConnectionBytesVerifyFilter.onMessageReceived(GridConnectionBytesVerifyFilter.java:133) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioFilterAdapter.proceedMessageReceived(GridNioFilterAdapter.java:109) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.ssl.GridNioSslFilter.onMessageReceived(GridNioSslFilter.java:408) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioFilterAdapter.proceedMessageReceived(GridNioFilterAdapter.java:109) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioServer$HeadFilter.onMessageReceived(GridNioServer.java:3752) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioFilterChain.onMessageReceived(GridNioFilterChain.java:175) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioServer$DirectNioClientWorker.processRead(GridNioServer.java:1379) ~[ignite-core-14.1.0.jar:14.1.0]
    at org.apache.ignite.internal.util.nio.GridNioServer$AbstractNioClientWorker.processSelectedKeysOptimized(GridNioServer.java:2526) ~[ignite-core-14.1.0.jar:14.1.0]
    ... 4 more {code}
 ",PetrovMikhail,PetrovMikhail,Blocker,Resolved,Fixed,04/Jun/23 20:22,07/Jun/23 17:45
Bug,IGNITE-19651,13538782,ignite-code-deployment module shouldn't depends on ignite-rest-api,"Currently, ignite-code-deployment module depends on ignite-rest-api module due to needing for org.apache.ignite.internal.rest.api.deployment.DeploymentStatus. 

We need to split the internal and external presentation of deployment status. ",ivan.gagarkin,ivan.gagarkin,Major,Resolved,Fixed,05/Jun/23 13:53,20/Jun/23 14:19
Bug,IGNITE-19652,13538786,Ignite is not working on JDK 21,ByteBuffer constructor in JDK 21 has different arguments: https://github.com/openjdk/jdk/commit/a56598f5a534cc9223367e7faa8433ea38661db9,,wendigo,Major,Resolved,Fixed,05/Jun/23 14:13,16/Jun/23 10:39
Bug,IGNITE-19653,13538853,Java thin: NullPointerException logged from testRequestsMetrics,"TeamCity now triggers a failure condition on *NullReferenceException* in the log - it usually indicates a bug.
*ClientMetricsTest.testRequestsMetrics* logs NPE:
{code}
org.apache.ignite.client.ClientMetricsTest
03:43:58     ClientMetricsTest > testRequestsMetrics() STANDARD_ERROR
03:43:58         2023-06-06 06:43:58:466 +0300 [WARNING][TestServer--srv-worker-1][ClientInboundMessageHandler] Error processing client request [id=2, op=50, remoteAddress=/127.0.0.1:50230]:null
03:43:58         java.lang.NullPointerException
03:43:58           at org.apache.ignite.client.handler.requests.sql.ClientSqlExecuteRequest.readSession(ClientSqlExecuteRequest.java:168)
03:43:58           at org.apache.ignite.client.handler.requests.sql.ClientSqlExecuteRequest.process(ClientSqlExecuteRequest.java:72)
03:43:58           at org.apache.ignite.client.handler.ClientInboundMessageHandler.processOperation(ClientInboundMessageHandler.java:613)
03:43:58           at org.apache.ignite.client.handler.ClientInboundMessageHandler.processOperation(ClientInboundMessageHandler.java:451)
03:43:58           at org.apache.ignite.client.handler.ClientInboundMessageHandler.channelRead(ClientInboundMessageHandler.java:250)
03:43:58           at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
03:43:58           at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
03:43:58           at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
03:43:58           at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:93)
{code}

It is caused by mocked *IgniteSql* at *TestClientHandlerModule:197*.",ptupitsyn,ptupitsyn,Major,Resolved,Fixed,06/Jun/23 03:53,06/Jun/23 09:34
Bug,IGNITE-19655,13538871,Distributed Sql keeps mapping query fragments to a node that has already left,"There are two test failures: [https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_RunAllTests/7271211?expandCode+Inspection=true&expandBuildProblemsSection=true&hideProblemsFromDependencies=false&expandBuildTestsSection=true&hideTestsFromDependencies=false] and [https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_RunAllTests/7272905?hideProblemsFromDependencies=false&hideTestsFromDependencies=false&expandCode+Inspection=true&expandBuildProblemsSection=true&expandBuildChangesSection=true&expandBuildTestsSection=true] (org.apache.ignite.internal.raftsnapshot.ItTableRaftSnapshotsTest.entriesKeepAppendedAfterSnapshotInstallation and org.apache.ignite.internal.raftsnapshot.ItTableRaftSnapshotsTest.snapshotInstallTimeoutDoesNotBreakSubsequentInstallsWhenSecondAttemptIsIdenticalToFirst, correspondingly).

In both cases, the test code creates a table with 3 replicas on a cluster of 3 nodes, then it stops the last node and tries to make an insert using one of the 2 remaining nodes. The RAFT majority (2 of 3) is still preserved, so the insert should succeed. It's understood that the insert might be issued before the remaining nodes understand that the third node has left, so we have a retry mechanism in place, it makes up to 5 attempts for almost 8 seconds (in total).

But in both the failed runs, each of 5 attempts failed because a fragment of the INSERT query was mapped to the missing node. This seems to be a bad luck (as the tests pass most of the time, fail rate is about 2.5%), but anyway: the SQL engine does not seem to care about the fact that the node has already left.

Probably, the SQL engine should track the Logical Topology events and avoid mapping query fragments to the missing nodes.",mzhuravkov,rpuch,Major,Resolved,Fixed,06/Jun/23 07:36,08/Jun/23 14:48
Bug,IGNITE-19658,13538886,NullPointerException in MetaStorageListener.handlePaginationCommand,"*ItMetaStorageServiceTest.testRangeWithNullAsKeyTo*

{code}
2023-06-06 07:40:23:613 +0300 [INFO][%imsst_trwnakt_20000%JRaft-ReadOnlyService-Disruptor-_stripe_1-0][ActionRequestProcessor] Error occurred on a user's state machine
    java.lang.NullPointerException
      at org.apache.ignite.internal.metastorage.server.raft.MetaStorageListener.handlePaginationCommand(MetaStorageListener.java:131)
      at org.apache.ignite.internal.metastorage.server.raft.MetaStorageListener.onRead(MetaStorageListener.java:99)
      at org.apache.ignite.raft.jraft.rpc.impl.ActionRequestProcessor$2.run(ActionRequestProcessor.java:130)
      at org.apache.ignite.raft.jraft.closure.ReadIndexClosure.run(ReadIndexClosure.java:95)
      at org.apache.ignite.raft.jraft.core.ReadOnlyServiceImpl.notifySuccess(ReadOnlyServiceImpl.java:446)
      at org.apache.ignite.raft.jraft.core.ReadOnlyServiceImpl$ReadIndexResponseClosure.run(ReadOnlyServiceImpl.java:194)
      at org.apache.ignite.raft.jraft.core.NodeImpl.readLeader(NodeImpl.java:1777)
      at org.apache.ignite.raft.jraft.core.NodeImpl.handleReadIndexRequest(NodeImpl.java:1722)
      at org.apache.ignite.raft.jraft.core.ReadOnlyServiceImpl.executeReadIndexEvents(ReadOnlyServiceImpl.java:245)
      at org.apache.ignite.raft.jraft.core.ReadOnlyServiceImpl$ReadIndexEventHandler.onEvent(ReadOnlyServiceImpl.java:138)
      at org.apache.ignite.raft.jraft.core.ReadOnlyServiceImpl$ReadIndexEventHandler.onEvent(ReadOnlyServiceImpl.java:121)
      at org.apache.ignite.raft.jraft.disruptor.StripedDisruptor$StripeEntryHandler.onEvent(StripedDisruptor.java:217)
      at org.apache.ignite.raft.jraft.disruptor.StripedDisruptor$StripeEntryHandler.onEvent(StripedDisruptor.java:181)
      at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:137)
      at java.base/java.lang.Thread.run(Thread.java:834)
{code}

https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_IntegrationTests_ModuleMetastorageClient/7273154?hideProblemsFromDependencies=false&hideTestsFromDependencies=false&expandBuildProblemsSection=true&expandBuildChangesSection=true",apolovtcev,ptupitsyn,Blocker,Resolved,Fixed,06/Jun/23 08:51,08/Jun/23 05:54
Bug,IGNITE-19659,13538888,NullPointerException in ClusterConfigRegistryImpl.fetchConfig,"*ItSslTest.connectToSecuredNode*
{code:java}
2023-06-06 07:43:15:455 +0300 [WARNING][ForkJoinPool.commonPool-worker-31][LazyObjectRef] Got exception when fetch from source
    java.util.concurrent.CompletionException: java.lang.NullPointerException
      at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314)
      at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:319)
      at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1702)
      at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1692)
      at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
      at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
      at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
      at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
      at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
    Caused by: java.lang.NullPointerException
      at org.apache.ignite.internal.cli.core.repl.registry.impl.ClusterConfigRegistryImpl.fetchConfig(ClusterConfigRegistryImpl.java:50)
      at org.apache.ignite.internal.cli.core.repl.registry.impl.ClusterConfigRegistryImpl.lambda$onConnect$0(ClusterConfigRegistryImpl.java:43)
      at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
      ... 6 more
    2023-06-06 07:43:15:461 +0300 [ERROR][Test worker][ExceptionHandler] Unhandled exception
    java.lang.NullPointerException: factory
      at picocli.CommandLine$Assert.notNull(CommandLine.java:18266)
      at picocli.CommandLine.<init>(CommandLine.java:228)
      at picocli.CommandLine.<init>(CommandLine.java:224)
      at org.apache.ignite.internal.cli.core.repl.executor.ReplExecutor.createPicocliCommands(ReplExecutor.java:177)
      at org.apache.ignite.internal.cli.core.repl.executor.ReplExecutor.execute(ReplExecutor.java:114)
      at org.apache.ignite.internal.cli.ReplManager.startReplMode(ReplManager.java:53)
      at org.apache.ignite.internal.cli.commands.connect.ConnectCommand.call(ConnectCommand.java:61)
      at org.apache.ignite.internal.cli.commands.connect.ConnectCommand.call(ConnectCommand.java:37)
      at picocli.CommandLine.executeUserObject(CommandLine.java:2041)
      at picocli.CommandLine.access$1500(CommandLine.java:148)
      at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2461)
      at picocli.CommandLine$RunLast.handle(CommandLine.java:2453)
      at picocli.CommandLine$RunLast.handle(CommandLine.java:2415)
      at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2273)
      at picocli.CommandLine$RunLast.execute(CommandLine.java:2417)
      at picocli.CommandLine.execute(CommandLine.java:2170)
      at org.apache.ignite.internal.cli.commands.CliCommandTestNotInitializedIntegrationBase.execute(CliCommandTestNotInitializedIntegrationBase.java:123)
      at org.apache.ignite.internal.cli.ssl.ItSslTest.connectToSecuredNode(ItSslTest.java:53){code}
[https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_IntegrationTests_ModuleCli/7273149?hideProblemsFromDependencies=false&hideTestsFromDependencies=false&expandBuildProblemsSection=true&expandBuildChangesSection=true]",vpakhnushev,ptupitsyn,Blocker,Resolved,Fixed,06/Jun/23 08:53,07/Jun/23 15:54
Bug,IGNITE-19665,13538935,High performance drop in key-value put() operations introduced between May 23 and June 5,"This ticket is a product of subsequent work on https://issues.apache.org/jira/browse/IGNITE-19664.

There are high (more than 4x on my local machine) performance drop in {{KeyValueView#put}} operations introduced somewhere between the following commit:

{noformat}
commit 0c68cbe3f016e508bd9d53ce5320c88acba1acff (HEAD)
Author: Slava Koptilin <slava.koptilin@gmail.com>
Date:   Tue May 23 10:17:53 2023 +0300

    IGNITE-17883 Removed not implemented 'invoke' functionality (#2090)
{noformat}

and the following one:

{code:java}
commit a2254434c403bc54685f05e0d6f51bef56abea2a (HEAD -> main, origin/main, origin/HEAD)
Author: Vadim Pakhnushev <8614891+valepakh@users.noreply.github.com>
Date:   Mon Jun 5 17:43:07 2023 +0300

    IGNITE-19559 NPE in deploy/undeploy calls in non-REPL mode (#2131)

{code}

The test is the ""Test 1"" from https://issues.apache.org/jira/browse/IGNITE-19664, i.e.: 
1. Start an Ignite 3 server node with attached {{ignite-config.conf}}. {{raft.fsync=false}} is set in the config.
2. Start YCSB client which makes {{KeyValueView#put}} operations within a ""100% insert"" profile.

Results for {{0c68cbe3f016e508bd9d53ce5320c88acba1acff}} were as follows:

{noformat}
[OVERALL], RunTime(ms), 282482
[OVERALL], Throughput(ops/sec), 3540.048569466373
[INSERT], Operations, 1000000
[INSERT], AverageLatency(us), 1067.488346
[INSERT], MinLatency(us), 492
[INSERT], MaxLatency(us), 421375
[INSERT], 95thPercentileLatency(us), 2059
[INSERT], 99thPercentileLatency(us), 5151
[INSERT], Return=OK, 1000000
{noformat}

Results for {{a2254434c403bc54685f05e0d6f51bef56abea2a}} are more than 4x worse in terms of throughput:

{code:java}
[OVERALL], RunTime(ms), 1325870
[OVERALL], Throughput(ops/sec), 754.2217562807816
[INSERT], Operations, 1000000
[INSERT], AverageLatency(us), 5229.54584
[INSERT], MinLatency(us), 1297
[INSERT], MaxLatency(us), 164223
[INSERT], 95thPercentileLatency(us), 9871
[INSERT], 99thPercentileLatency(us), 14271
[INSERT], Return=OK, 1000000
{code}

Logs for {{0c68cbe3f016e508bd9d53ce5320c88acba1acff}}: see https://issues.apache.org/jira/browse/IGNITE-19664

Logs for {{a2254434c403bc54685f05e0d6f51bef56abea2a}}:
- node's config:  [^ignite-config.conf] 
- node's log:  [^ignite3db-0.log] 
- node's GC log:  [^gc.log.20230606_075104] 
- YCSB client log:  [^ycsb-run10.log] ",rpuch,Artukhov,Major,Closed,Fixed,06/Jun/23 14:28,23/Jun/23 08:51
Bug,IGNITE-19668,13539016,Sql. testLengthExpressionWithDynamicParameter failed after check fix,"ItVarBinaryExpressionTest#testLengthExpressionWithDynamicParameter no *.check()* is called, after fix it become broken.

ItVarBinaryExpressionTest#testLengthExpression need to be fixed too.",zstan,zstan,Major,Resolved,Fixed,07/Jun/23 07:35,28/Jun/23 13:10
Bug,IGNITE-19669,13539027,Support boxed long in direct marshaller,,,sdanilov,Major,Resolved,Fixed,07/Jun/23 08:50,07/Jun/23 08:55
Bug,IGNITE-19674,13539095,Provide an ability to restart StandaloneMetaStorageManager with the recovered data,"Currently if we use {{StandaloneMetaStorageManager#create()}} with {{RocksDbKeyValueStorage}} in a scenarios when a node is restarted, data in a metastorage is not recovering, because the logic in {{RocksDbKeyValueStorage#start}} is the following: when we restart {{RocksDbKeyValueStorage}}, we call {{recreateDb}} which deletes existing data, recovery is relying on the raft's snapshot and log playback.

When we use {{StandaloneMetaStorageManager}} we mock raft layer, meaning that we do not restore metastorage using raft's snapshot.

I propose to create a test version of {{RocksDbKeyValueStorage}} so it won't delete data when it is used in tests scenarios.",maliev,maliev,Blocker,Resolved,Fixed,07/Jun/23 13:40,08/Jun/23 09:37
Bug,IGNITE-19688,13539281,NPE in ItNodeTest#testChangePeersStepsDownInJointConsensus,"org.apache.ignite.raft.jraft.core.ItNodeTest#testChangePeersStepsDownInJointConsensus
{noformat}
java.lang.NullPointerException  
 at org.apache.ignite.raft.jraft.core.ItNodeTest.testChangePeersStepsDownInJointConsensus(ItNodeTest.java:3385)  
 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  
 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  
 at java.base/java.lang.reflect.Method.invoke(Method.java:566) 
{noformat}

*Motivation*
The root cause of the issue is the method of getting leader (_TestCluster#getLeader_) might return _null_ even if the test waits a leader before (_TestCluster#waitLeader_).

*Implementation Notes*
Create a new method that returns a leader just after it has waited it (_TestCluster#waitAndGetLeader_).

*Definition of Done*
Verify all places where method of waiting leader is used and replace it to new one in that case where getting leader is not expected to receive _null_.

",maliev,jooger,Blocker,Resolved,Fixed,08/Jun/23 13:29,10/Jul/23 13:42
Bug,IGNITE-19693,13539400,getAll does not preserve order and does not return nulls for missing keys,"* getAll does not preserve element order.
* getAll does not return nulls for non-existent records.

This is specified in Javadoc  in [RecordView|https://github.com/apache/ignite-3/blob/0c68cbe3f016e508bd9d53ce5320c88acba1acff/modules/api/src/main/java/org/apache/ignite/table/RecordView.java#L56-L60]:
{code}
Records with all columns filled from the table. The order of collection elements is guaranteed to be the same as the order of {@code keyRecs}. If a record does not exist, the element at the corresponding index of the resulting collection is null.
{code}

Without those properties, *getAll* is not very useful: it is hard to correspond the results with the original collection.

h2. Reproducer

Add the following code to *ItTableApiContractTest*:
{code:java}
    @Test
    public void testGetAll() {
        RecordView<Tuple> tbl = ignite.tables().table(TABLE_NAME).recordView();
        // recordView.insert(tx, Tuple.create().set(""name"", ""k1"").set(""balance"", 1));

        var recs = IntStream.range(1, 50)
                .mapToObj(i -> Tuple.create().set(""name"", ""id_"" + i * 2).set(""balance"", i * 2))
                .collect(Collectors.toList());

        tbl.upsertAll(null, recs);

        var keys = IntStream.range(1, 100)
                .mapToObj(i -> Tuple.create().set(""name"", ""id_"" + i))
                .collect(Collectors.toList());

        List<Tuple> res = (List<Tuple>) tbl.getAll(null, keys);

        assertNull(res.get(0));
        assertEquals(2L, res.get(1).longValue(0));
        assertEquals(100, res.size());
    }
{code}

Tuples are returned in random order, and only 50 of them, not 100",ktkalenko@gridgain.com,ptupitsyn,Blocker,Resolved,Fixed,09/Jun/23 09:30,26/Jun/23 07:32
Bug,IGNITE-19700,13539458,RocksDB scan time spikes up at some moments,"Motivation:

We have ItSqlLogicTest which performs user load with creation and desctruction of the tables, and need to have background load on meta storage to be able to refresh leases, but seems that this additional load causes weird behavior of meta storage's underlying rocksdb storage, which leads to timeouts on DDL operations. Actually, there are too long range scans in rocksdb storage, which don't correlate with number of entries (including tombstones) and amount of lease data written in background.

It can be reproduced on RocksDBKeyValueStorage.

Prerequisites:

RocksDBKeyValueStorage filled with 1000 random keys and values, 5000 random tombstones, 100 values with prefix ""tables"", 5000 tombstones with prefix ""tables"".

Load profile:
 * Thread1: performs storage.invoke with the same key and value of size 500k bytes, once per 100 ms.
 * Thread2: performs storage.range for prefix ""tables"" once per 200 ms, collects all entries from cursor.
 * Thread3: performs storage.get with random key once per 3 ms.

Each operation performed by Thread2 mostly takes 20-50 ms, but sometimes ({*}after 5+ minutes of test running{*}) this time spikes up to hundreds of milliseconds (or even seconds): and this lasts for some time (I observed up to half of a minute), after that the time returns to normal values:
{code:java}
2023-06-09 17:09:05:971 +0300 [INFO][Thread-5][RocksDBLoadTest] time 31, size 100
2023-06-09 17:09:06:223 +0300 [INFO][Thread-5][RocksDBLoadTest] time 50, size 100
2023-06-09 17:09:06:471 +0300 [INFO][Thread-5][RocksDBLoadTest] time 47, size 100
2023-06-09 17:09:06:715 +0300 [INFO][Thread-5][RocksDBLoadTest] time 44, size 100
2023-06-09 17:09:07:483 +0300 [INFO][Thread-5][RocksDBLoadTest] time 566, size 100
2023-06-09 17:09:08:228 +0300 [INFO][Thread-5][RocksDBLoadTest] time 543, size 100
2023-06-09 17:09:09:000 +0300 [INFO][Thread-5][RocksDBLoadTest] time 571, size 100
2023-06-09 17:09:09:774 +0300 [INFO][Thread-5][RocksDBLoadTest] time 572, size 100
2023-06-09 17:09:10:570 +0300 [INFO][Thread-5][RocksDBLoadTest] time 596, size 100
2023-06-09 17:09:11:323 +0300 [INFO][Thread-5][RocksDBLoadTest] time 552, size 100
2023-06-09 17:09:12:103 +0300 [INFO][Thread-5][RocksDBLoadTest] time 579, size 100
2023-06-09 17:09:12:861 +0300 [INFO][Thread-5][RocksDBLoadTest] time 556, size 100{code}
On teamcity it was even over 6 seconds:

[https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_RunUnitTests/7283421?buildTab=log&focusLine=46540&expandAll=true&logFilter=debug&logView=flowAware]

 

Reproducer:
{code:java}
package org.apache.ignite.internal.metastorage;

import static org.apache.ignite.internal.metastorage.dsl.Conditions.notExists;
import static org.apache.ignite.internal.metastorage.dsl.Conditions.or;
import static org.apache.ignite.internal.metastorage.dsl.Conditions.value;
import static org.apache.ignite.internal.metastorage.dsl.Operations.noop;
import static org.apache.ignite.internal.metastorage.dsl.Operations.put;
import static org.apache.ignite.internal.metastorage.server.Value.TOMBSTONE;

import java.io.ObjectStreamException;
import java.nio.charset.StandardCharsets;
import java.nio.file.Path;
import java.util.ArrayList;
import java.util.List;
import java.util.UUID;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.logging.Logger;
import org.apache.ignite.internal.hlc.HybridClock;
import org.apache.ignite.internal.hlc.HybridClockImpl;
import org.apache.ignite.internal.logger.IgniteLogger;
import org.apache.ignite.internal.logger.Loggers;
import org.apache.ignite.internal.metastorage.dsl.CompoundCondition;
import org.apache.ignite.internal.metastorage.dsl.ConditionType;
import org.apache.ignite.internal.metastorage.dsl.SimpleCondition;
import org.apache.ignite.internal.metastorage.server.AndCondition;
import org.apache.ignite.internal.metastorage.server.Condition;
import org.apache.ignite.internal.metastorage.server.ExistenceCondition;
import org.apache.ignite.internal.metastorage.server.OrCondition;
import org.apache.ignite.internal.metastorage.server.RevisionCondition;
import org.apache.ignite.internal.metastorage.server.TombstoneCondition;
import org.apache.ignite.internal.metastorage.server.ValueCondition;
import org.apache.ignite.internal.metastorage.server.persistence.RocksDbKeyValueStorage;
import org.apache.ignite.internal.testframework.WorkDirectory;
import org.apache.ignite.internal.testframework.WorkDirectoryExtension;
import org.apache.ignite.internal.util.Cursor;
import org.apache.ignite.lang.ByteArray;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;

@ExtendWith(WorkDirectoryExtension.class)
public class RocksDBLoadTest {
    private static final IgniteLogger LOG = Loggers.forClass(RocksDBLoadTest.class);

    private byte[] randomBytes() {
        return UUID.randomUUID().toString().getBytes(StandardCharsets.UTF_8);
    }

    private byte[] randomBytes(String prefix) {
        return (prefix + UUID.randomUUID()).getBytes(StandardCharsets.UTF_8);
    }

    /**
     * Increments the last character of the given string.
     */
    private static String incrementLastChar(String str) {
        char lastChar = str.charAt(str.length() - 1);

        return str.substring(0, str.length() - 1) + (char) (lastChar + 1);
    }


    @Test
    public void test(@WorkDirectory Path path) throws InterruptedException {
        System.out.println(""start"");
        HybridClock clock = new HybridClockImpl();
        RocksDbKeyValueStorage storage = new RocksDbKeyValueStorage(""asd"", path.resolve(""rocksdbtest""));
        storage.start();
        for (int i = 0; i < 1000; i++) {
            storage.put(randomBytes(), randomBytes(), clock.now());
        }
        for (int i = 0; i < 5000; i++) {
            storage.put(randomBytes(), TOMBSTONE, clock.now());
        }
        for (int i = 0; i < 100; i++) {
            storage.put(randomBytes(""tables""), randomBytes(), clock.now());
        }
        for (int i = 0; i < 5000; i++) {
            storage.put(randomBytes(""tables""), TOMBSTONE, clock.now());
        }

        ByteArray leaseKey = ByteArray.fromString(""placementdriver.leases"");
        AtomicBoolean leasesStopped = new AtomicBoolean();
        AtomicBoolean rangeStopped = new AtomicBoolean();
        Thread leases = new Thread(() -> {
            byte[] leaseRaw = new byte[500_000];
            byte a = 0;
            while (!leasesStopped.get()) {
                byte[] renewedLease = new byte[500_000];
                renewedLease[0] = ++a;
                storage.invoke(
                        toCondition(or(notExists(leaseKey), value(leaseKey).eq(leaseRaw))),
                        List.of(put(leaseKey, renewedLease)),
                        List.of(noop()),
                        clock.now()
                );
                leaseRaw = renewedLease;

                try {
                    Thread.sleep(100);
                } catch (InterruptedException e) {
                    throw new RuntimeException(e);
                }
            }
        });
        leases.start();

        Thread range = new Thread(() -> {
            while (!rangeStopped.get()) {
                long start = System.currentTimeMillis();
                Cursor<Entry> cursor =
                        storage.range(""tables"".getBytes(StandardCharsets.UTF_8), incrementLastChar(""tables"").getBytes(StandardCharsets.UTF_8));
                List<Object> list = new ArrayList<>();
                for(Entry e : cursor) {
                    if (!e.tombstone()) {
                        list.add(e.value());
                    }
                }
                LOG.info(""time "" + (System.currentTimeMillis() - start) + "", size "" + list.size());
                cursor.close();

                try {
                    Thread.sleep(200);
                } catch (InterruptedException e) {
                    throw new RuntimeException(e);
                }
            }
        });
        range.start();

        for (int i = 0; i < 180_000; i++) {
            storage.get(randomBytes());
            Thread.sleep(3);
        }

        leasesStopped.set(true);
        rangeStopped.set(true);
        leases.join();
        range.join();
    }



    private static Condition toCondition(org.apache.ignite.internal.metastorage.dsl.Condition condition) {
        if (condition instanceof SimpleCondition.ValueCondition) {
            var valueCondition = (SimpleCondition.ValueCondition) condition;

            return new ValueCondition(
                    toValueConditionType(valueCondition.type()),
                    valueCondition.key(),
                    valueCondition.value()
            );
        } else if (condition instanceof SimpleCondition.RevisionCondition) {
            var revisionCondition = (SimpleCondition.RevisionCondition) condition;

            return new RevisionCondition(
                    toRevisionConditionType(revisionCondition.type()),
                    revisionCondition.key(),
                    revisionCondition.revision()
            );
        } else if (condition instanceof SimpleCondition) {
            var simpleCondition = (SimpleCondition) condition;

            switch (simpleCondition.type()) {
                case KEY_EXISTS:
                    return new ExistenceCondition(ExistenceCondition.Type.EXISTS, simpleCondition.key());

                case KEY_NOT_EXISTS:
                    return new ExistenceCondition(ExistenceCondition.Type.NOT_EXISTS, simpleCondition.key());

                case TOMBSTONE:
                    return new TombstoneCondition(simpleCondition.key());

                default:
                    throw new IllegalArgumentException(""Unexpected simple condition type "" + simpleCondition.type());
            }
        } else if (condition instanceof CompoundCondition) {
            CompoundCondition compoundCondition = (CompoundCondition) condition;

            Condition leftCondition = toCondition(compoundCondition.leftCondition());
            Condition rightCondition = toCondition(compoundCondition.rightCondition());

            switch (compoundCondition.type()) {
                case AND:
                    return new AndCondition(leftCondition, rightCondition);

                case OR:
                    return new OrCondition(leftCondition, rightCondition);

                default:
                    throw new IllegalArgumentException(""Unexpected compound condition type "" + compoundCondition.type());
            }
        } else {
            throw new IllegalArgumentException(""Unknown condition "" + condition);
        }
    }

    private static ValueCondition.Type toValueConditionType(ConditionType type) {
        switch (type) {
            case VAL_EQUAL:
                return ValueCondition.Type.EQUAL;
            case VAL_NOT_EQUAL:
                return ValueCondition.Type.NOT_EQUAL;
            case VAL_GREATER:
                return ValueCondition.Type.GREATER;
            case VAL_GREATER_OR_EQUAL:
                return ValueCondition.Type.GREATER_OR_EQUAL;
            case VAL_LESS:
                return ValueCondition.Type.LESS;
            case VAL_LESS_OR_EQUAL:
                return ValueCondition.Type.LESS_OR_EQUAL;
            default:
                throw new IllegalArgumentException(""Unexpected value condition type "" + type);
        }
    }

    private static RevisionCondition.Type toRevisionConditionType(ConditionType type) {
        switch (type) {
            case REV_EQUAL:
                return RevisionCondition.Type.EQUAL;
            case REV_NOT_EQUAL:
                return RevisionCondition.Type.NOT_EQUAL;
            case REV_GREATER:
                return RevisionCondition.Type.GREATER;
            case REV_GREATER_OR_EQUAL:
                return RevisionCondition.Type.GREATER_OR_EQUAL;
            case REV_LESS:
                return RevisionCondition.Type.LESS;
            case REV_LESS_OR_EQUAL:
                return RevisionCondition.Type.LESS_OR_EQUAL;
            default:
                throw new IllegalArgumentException(""Unexpected revision condition type "" + type);
        }
    }
} {code}",ibessonov,Denis Chudov,Blocker,Resolved,Fixed,09/Jun/23 16:27,20/Jun/23 10:51
Bug,IGNITE-19707,13539640,AssertionError: Data nodes was not initialized in testScaleDownDidNotChangeDataNodesWhenTriggerKeyWasConcurrentlyChanged,"Assertion error in *DistributionZoneManagerScaleUpTest#testScaleDownDidNotChangeDataNodesWhenTriggerKeyWasConcurrentlyChanged*

{code}
DistributionZoneManagerScaleUpTest > testScaleDownDidNotChangeDataNodesWhenTriggerKeyWasConcurrentlyChanged() STANDARD_ERROR
09:33:57         2023-06-12 12:33:57:775 +0300 [INFO][Test worker][LogicalTopologyImpl] Node added to logical topology [node=LogicalNode [nodeAttributes=EmptyMap {}, super=ClusterNode [id=1, name=node1, address=localhost:123, nodeMetadata=null]], topology=LogicalTopologySnapshot [version=1, nodes=Set12 [LogicalNode [nodeAttributes=EmptyMap {}, super=ClusterNode [id=1, name=node1, address=localhost:123, nodeMetadata=null]]]]]
09:33:57         2023-06-12 12:33:57:886 +0300 [INFO][Test worker][LogicalTopologyImpl] Node removed from logical topology [node=LogicalNode [nodeAttributes=EmptyMap {}, super=ClusterNode [id=1, name=node1, address=localhost:123, nodeMetadata=null]], topology=LogicalTopologySnapshot [version=2, nodes=SetN []]]
09:33:57         2023-06-12 12:33:57:890 +0300 [ERROR][%test%metastorage-watch-executor-3][WatchProcessor] Error occurred when processing a watch event
09:33:57         java.lang.AssertionError: Data nodes was not initialized.
09:33:57           at org.apache.ignite.internal.distributionzones.DistributionZoneManager$3.onUpdate(DistributionZoneManager.java:1396)
09:33:57           at org.apache.ignite.internal.metastorage.server.Watch.onUpdate(Watch.java:67)
09:33:57           at org.apache.ignite.internal.metastorage.server.WatchProcessor.lambda$notifyWatch$7(WatchProcessor.java:168)
09:33:57           at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1072)
09:33:57           at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
09:33:57           at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1705)
09:33:57           at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
09:33:57           at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
09:33:57           at java.base/java.lang.Thread.run(Thread.java:834)
09:33:57         2023-06-12 12:33:57:891 +0300 [WARNING][%test%metastorage-watch-executor-3][DistributionZoneManager] Unable to process data nodes event
09:33:57         java.lang.AssertionError: Data nodes was not initialized.
09:33:57           at org.apache.ignite.internal.distributionzones.DistributionZoneManager$3.onUpdate(DistributionZoneManager.java:1396)
09:33:57           at org.apache.ignite.internal.metastorage.server.Watch.onUpdate(Watch.java:67)
09:33:57           at org.apache.ignite.internal.metastorage.server.WatchProcessor.lambda$notifyWatch$7(WatchProcessor.java:168)
09:33:57           at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1072)
09:33:57           at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
09:33:57           at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1705)
09:33:57           at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
09:33:57           at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
09:33:57           at java.base/java.lang.Thread.run(Thread.java:834)
{code}

https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_RunUnitTests/7288160?hideProblemsFromDependencies=false&hideTestsFromDependencies=false&expandBuildProblemsSection=true&expandBuildChangesSection=true&showLog=7288160_51209_86.50902&logFilter=debug&logView=flowAware",maliev,ptupitsyn,Critical,Resolved,Fixed,12/Jun/23 10:00,13/Jun/23 06:47
Bug,IGNITE-19714,13539787,AssertionError: Data nodes was not initialized in testStaleWatchEvent,"Assertion error in *DistributionZoneManagerWatchListenerTest#testStaleWatchEvent*

{code}
DistributionZoneManagerWatchListenerTest > testStaleWatchEvent() STANDARD_ERROR
06:29:54         2023-06-13 09:29:54:748 +0300 [ERROR][%test%metastorage-watch-executor-3][WatchProcessor] Error occurred when processing a watch event
06:29:54         java.lang.AssertionError: Data nodes was not initialized.
06:29:54           at org.apache.ignite.internal.distributionzones.DistributionZoneManager$3.onUpdate(DistributionZoneManager.java:1396)
06:29:54           at org.apache.ignite.internal.metastorage.server.Watch.onUpdate(Watch.java:67)
06:29:54           at org.apache.ignite.internal.metastorage.server.WatchProcessor.lambda$notifyWatch$7(WatchProcessor.java:168)
06:29:54           at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1072)
06:29:54           at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
06:29:54           at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1705)
06:29:54           at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
06:29:54           at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
06:29:54           at java.base/java.lang.Thread.run(Thread.java:834)
06:29:54         2023-06-13 09:29:54:749 +0300 [WARNING][%test%metastorage-watch-executor-3][DistributionZoneManager] Unable to process data nodes event
06:29:54         java.lang.AssertionError: Data nodes was not initialized.
06:29:54           at org.apache.ignite.internal.distributionzones.DistributionZoneManager$3.onUpdate(DistributionZoneManager.java:1396)
06:29:54           at org.apache.ignite.internal.metastorage.server.Watch.onUpdate(Watch.java:67)
06:29:54           at org.apache.ignite.internal.metastorage.server.WatchProcessor.lambda$notifyWatch$7(WatchProcessor.java:168)
06:29:54           at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1072)
06:29:54           at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
06:29:54           at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1705)
06:29:54           at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
06:29:54           at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
06:29:54           at java.base/java.lang.Thread.run(Thread.java:834)
{code}

https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_RunAllTests/7290145?hideProblemsFromDependencies=false&hideTestsFromDependencies=false&expandBuildChangesSection=true&expandCode+Inspection=true&expandBuildProblemsSection=true&showLog=7290143_52135_98.50904&logFilter=debug&logView=flowAware",maliev,ptupitsyn,Blocker,Resolved,Fixed,13/Jun/23 06:46,14/Jun/23 10:56
Bug,IGNITE-19715,13539812,Thin client operations can take a long time if PA is enabled and some cluster nodes are not network reachable.,"Thin client operations can take a long time if PA is enabled and some cluster nodes are not reachable over network.

Consider the following scenario:

1. The thin client have already sucessfully established connection to all configured node addresses.
2. A particular cluster node becomes unreachable over network. It can be reproduced with iptables -A INPUT -p tcp --dport for Linux.
3. The thin client periodically sends put request which is mapped by PA to the unreachable node.
4. Firstly  all attempts to perform put will lead to `ClientException: Timeout was reached before computation completed.` exception. But eventually the connection to the unreachable node will be closed by OS (see tcp_keepalive_time for Linux).

This will lead to reestablishing connection to the unreachable node during handling of the next put (see ReliableChannel.java:1012)

We currently do not set a timeout for the open connection operation (see GridNioClientConnectionMultiplexer#open, here we use Integer.MAX_VALUE for Socket#connect(java.net.SocketAddress, int))

As a result socket#connect operation (and hence put operation) hangs for a significant amount of time (it depends on OS parameters, usually it is couple of minutes). This is confusing for users because a single put may take much longer than the configured ClientConfiguration#setTimeout property.",PetrovMikhail,PetrovMikhail,Major,Resolved,Fixed,13/Jun/23 09:06,14/Jun/23 13:19
Bug,IGNITE-19716,13539825,Some tests failed because of AssertionError,"Some tests of Ignite3 fail with the following AssertionError:
{noformat}
    at org.apache.ignite.internal.raft.RaftGroupServiceImpl.randomNode(RaftGroupServiceImpl.java:681)
    at org.apache.ignite.internal.raft.RaftGroupServiceImpl.randomNode(RaftGroupServiceImpl.java:668)
    at org.apache.ignite.internal.raft.RaftGroupServiceImpl.refreshLeader(RaftGroupServiceImpl.java:222)
    at org.apache.ignite.internal.raft.RaftGroupServiceImpl.start(RaftGroupServiceImpl.java:178)
    at org.apache.ignite.internal.raft.client.TopologyAwareRaftGroupService.start(TopologyAwareRaftGroupService.java:180)
    at org.apache.ignite.internal.raft.client.TopologyAwareRaftGroupServiceFactory.startRaftGroupService(TopologyAwareRaftGroupServiceFactory.java:71)
    at org.apache.ignite.internal.raft.Loza.startRaftGroupService(Loza.java:310)
    at org.apache.ignite.internal.table.distributed.TableManager.lambda$createTablePartitionsLocally$13(TableManager.java:834)
    at org.apache.ignite.internal.util.IgniteUtils.inBusyLock(IgniteUtils.java:892)
    at org.apache.ignite.internal.table.distributed.TableManager.lambda$createTablePartitionsLocally$14(TableManager.java:831)
    at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1072){noformat}
The proper fix will be available under IGNITE-19466. The main goal of this ticket is to try to suppress the AssertionError until IGNITE-19466 is resolved.

 ",slava.koptilin,slava.koptilin,Major,Resolved,Fixed,13/Jun/23 10:16,20/Jun/23 10:23
Bug,IGNITE-19728,13539957,.NET: DataStreamerTests.TestAutoFlushFrequency is flaky,"*DataStreamerTests.TestAutoFlushFrequency(True)*:

* History: https://ci.ignite.apache.org/test/4035794459336688174?currentProjectId=ApacheIgnite3xGradle_Test&expandTestHistoryChartSection=true
* Failure: https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_RunNetTests/7291573?hideProblemsFromDependencies=false&hideTestsFromDependencies=false&expandBuildChangesSection=true&expandBuildTestsSection=true&buildTab=overview&showLog=7291573_31617_1111.1117&logFilter=debug&logView=flowAware

{code}
 Failed TestAutoFlushFrequency(True) [256 ms]
14:54:54       Error Message:
14:54:54          Expected: True
14:54:54       But was:  False
14:54:54     
14:54:54       Stack Trace:
14:54:54          at Apache.Ignite.Tests.Table.DataStreamerTests.TestAutoFlushFrequency(Boolean enabled) in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/Table/DataStreamerTests.cs:line 103
14:54:54        at NUnit.Framework.Internal.TaskAwaitAdapter.GenericAdapter`1.BlockUntilCompleted()
14:54:54        at NUnit.Framework.Internal.MessagePumpStrategy.NoMessagePumpStrategy.WaitForCompletion(AwaitAdapter awaiter)
14:54:54        at NUnit.Framework.Internal.AsyncToSyncAdapter.Await(Func`1 invoke)
14:54:54        at NUnit.Framework.Internal.Commands.TestMethodCommand.RunTestMethod(TestExecutionContext context)
14:54:54        at NUnit.Framework.Internal.Commands.TestMethodCommand.Execute(TestExecutionContext context)
14:54:54        at NUnit.Framework.Internal.Commands.BeforeAndAfterTestCommand.<>c__DisplayClass1_0.<Execute>b__0()
14:54:54        at NUnit.Framework.Internal.Commands.DelegatingTestCommand.RunTestMethodInThreadAbortSafeZone(TestExecutionContext context, Action action)
14:54:54     
14:54:54     1)    at Apache.Ignite.Tests.Table.DataStreamerTests.TestAutoFlushFrequency(Boolean enabled) in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/Table/DataStreamerTests.cs:line 103
14:54:54     
14:54:54     
14:54:54       Standard Output Messages:
14:54:54      [17:54:53] [Trace] [ClientSocket-3984] Sending request [op=SchemasGet, remoteAddress=127.0.0.1:10942, requestId=3]
14:54:54      [17:54:53] [Debug] [Table] Schema loaded [tableId=1, schemaVersion=1]
14:54:54      [17:54:53] [Trace] [ClientSocket-3985] Sending request [op=PartitionAssignmentGet, remoteAddress=127.0.0.1:10943, requestId=2]
14:54:54      [17:54:53] [Trace] [ClientSocket-3984] Sending request [op=TupleDeleteAll, remoteAddress=127.0.0.1:10942, requestId=4]
14:54:54      [17:54:53] [Trace] [ClientSocket-3985] Sending request [op=TupleUpsertAll, remoteAddress=127.0.0.1:10943, requestId=3]
14:54:54      [17:54:53] [Trace] [ClientSocket-3984] Sending request [op=TupleContainsKey, remoteAddress=127.0.0.1:10942, requestId=5]
{code}",ptupitsyn,ptupitsyn,Major,Resolved,Fixed,14/Jun/23 03:45,14/Jun/23 07:42
Bug,IGNITE-19730,13539974,Sql. Different tests failed in scope of ExecutionServiceImplTest,"different tests failed under ExecutionServiceImplTest, with :


{code:java}
Expected :true
Actual   :false
	at org.apache.ignite.internal.sql.engine.exec.ExecutionServiceImplTest.tearDown(ExecutionServiceImplTest.java:159)
{code}
reproduced in main (79a841d95247c338b80ab91a9)
Step to reproduce : run ExecutionServiceImplTest *until failure* and after 5+ run unstable tests become visible.",zstan,zstan,Major,Resolved,Fixed,14/Jun/23 06:13,06/Jul/23 07:19
Bug,IGNITE-19731,13539991,ItTableScanTest different test failed.,"ItTableScanTest different tests are failed, reproduced locally (just call run until failure and on 5+ iteration it will fail) main (79a841d95247c338b80ab91a93ec18ee7) TC also decide them flaky
 !image-2023-06-14-09-56-02-062.png! ",v.pyatkov,zstan,Major,Resolved,Fixed,14/Jun/23 06:56,21/Jun/23 15:14
Bug,IGNITE-19736,13540052,Do not cancel tasks in DistributionZoneManager#executor if they were created by immediate scaleUp/scaleDown events.,"h3. *Motivation*

When topology is changed it triggers scheduling a task in the executor to update data nodes. If there is another not started task in the queue then an old task will be canceled. So even if scaleUp/scaleDown timers are immediate it is possible that some topology change will be skipped because the task will be canceled. 

We need do not cancel tasks in an executor if the task was triggered by immediate data nodes calculation.
h3. *Definition of Done*
 # Do not cancel tasks in an executor if the task was triggered by immediate data nodes calculation.
 # All data nodes calculation tasks must be executed in order of creation.",Sergey Uttsel,Sergey Uttsel,Major,Resolved,Fixed,14/Jun/23 13:40,27/Jun/23 08:30
Bug,IGNITE-19737,13540082,TestMvStorageUpdateHandlerTest.testConcurrentExecuteBatchGc is flacky,"Look at the history over all branches:

[https://ci.ignite.apache.org/test/-4740616007659837895?currentProjectId=ApacheIgnite3xGradle_Test&expandTestHistoryChartSection=true&branch=]

{noformat}
org.opentest4j.AssertionFailedError: expected: <null> but was: <RowId [partitionId=0, uuid=3eb0684a-6d15-4c0b-bc32-1f242852d60c]>
  at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)
  at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)
  at app//org.junit.jupiter.api.AssertNull.failNotNull(AssertNull.java:50)
  at app//org.junit.jupiter.api.AssertNull.assertNull(AssertNull.java:35)
  at app//org.junit.jupiter.api.AssertNull.assertNull(AssertNull.java:30)
  at app//org.junit.jupiter.api.Assertions.assertNull(Assertions.java:276)
  at app//org.apache.ignite.internal.table.distributed.AbstractMvStorageUpdateHandlerTest.testConcurrentExecuteBatchGc(AbstractMvStorageUpdateHandlerTest.java:159)
  at java.base@11.0.17/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at java.base@11.0.17/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at java.base@11.0.17/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.base@11.0.17/java.lang.reflect.Method.invoke(Method.java:566)
{noformat}",ktkalenko@gridgain.com,v.pyatkov,Major,Resolved,Fixed,14/Jun/23 17:07,16/Jun/23 11:50
Bug,IGNITE-19745,13540238,Add ability to retrieve/scan over meta storage revisions for a specific key,"h3. *Motivation*

Occurred that it's not possible to subscribe to old meta storage revision. Alternative solution will be a combination of subscription for new ones along with scanning/retrieving old revisions in a following way:
 # Register a watch listener that will receive new entries.
 # Read an applied revision.
 # Retrieve existing entries up to the applied revision.
 # If an existing entries list contains the expected entry then use it. Else wait when the watch listener receive the expected entry.

Currently there's no method in meta storage API that will provide an ability to scan over revisions {*}locally{*}, so that we may add such mehod, e.g.
{code:java}
private List<Entry> doGet(byte[] key, long revLowerBound, long revUpperBound) {code}
Aforementioned method is a method in RocksDbKeyValueStorage that should be both properly exposed to ms API and also implemented in KeyValueStorage.
h3. Definition of Done.

It's possible to retrieve/scan over meta storage revisions for a specific key.

 ",Sergey Uttsel,Sergey Uttsel,Major,Resolved,Fixed,15/Jun/23 13:43,22/Jun/23 18:56
Bug,IGNITE-19746,13540240,control.sh --performance-statistics status doesn't not print actual status,"The status sub-command of the control.sh --performance-statistics doesn't not print the actual status to console.

Previously it was like (note the *Disabled.* word):
{noformat}
Control utility [ver. 15.0.0-SNAPSHOT#20230422-sha1:7f80003d]
2023 Copyright(C) Apache Software Foundation
User: ducker
Time: 2023-04-23T22:17:12.489
Command [PERFORMANCE-STATISTICS] started
Arguments: --host x.x.x.x --performance-statistics status --user admin --password *****
--------------------------------------------------------------------------------
Disabled.
Command [PERFORMANCE-STATISTICS] finished with code: 0
Control utility has completed execution at: 2023-04-23T22:17:13.271
Execution time: 782 ms
{noformat}
 

Now it's like (note the absence of the *Disabled.* word):
{noformat}
Control utility [ver. 15.0.0-SNAPSHOT#20230613-sha1:cacee58d]
2023 Copyright(C) Apache Software Foundation
User: ducker
Time: 2023-06-15T15:46:41.586
Command [PERFORMANCE-STATISTICS] started
Arguments: --host x.x.x.x --performance-statistics status  --user admin --password *****
--------------------------------------------------------------------------------
Command [PERFORMANCE-STATISTICS] finished with code: 0
Control utility has completed execution at: 2023-06-15T15:46:42.523
Execution time: 937 ms
{noformat}
 

Outputs of other sub-commands are also need to be checked.",nizhikov,serge.korotkov,Major,Resolved,Fixed,15/Jun/23 13:48,27/Jun/23 11:18
Bug,IGNITE-19747,13540241,ItRebalanceDistributedTest.testOnLeaderElectedRebalanceRestart is flacky,"The test have suspicious history on TC:
[https://ci.ignite.apache.org/test/3244623774880880898?currentProjectId=ApacheIgnite3xGradle_Test_IntegrationTests]
In all cases, it is an exception like this:
{noformat}
org.apache.ignite.lang.IgniteInternalException: IGN-CMN-65535 TraceId:fcdca6b2-e939-4e79-b03f-79ac4dca5d16 No such partition 0 in table TBL1
  at app//org.apache.ignite.internal.table.distributed.storage.InternalTableImpl.partitionRaftGroupService(InternalTableImpl.java:1141)
  at app//org.apache.ignite.internal.configuration.storage.ItRebalanceDistributedTest.testOnLeaderElectedRebalanceRestart(ItRebalanceDistributedTest.java:386)
  at java.base@11.0.17/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at java.base@11.0.17/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
{noformat}
The root cause is we have no guaranty that a table available on each node if we got it only at one.
{code:title=Test code fragment}
TableImpl table = (TableImpl) await(nodes.get(1).tableManager.createTableAsync(
                ""TBL1"",
                zoneName,
                tblChanger -> SchemaConfigurationConverter.convert(schTbl1, tblChanger)));
...
TableImpl nonLeaderTable = (TableImpl) findNodeByConsistentId(nonLeaderNodeConsistentId).tableManager.table(""TBL1"");
...
nonLeaderTable.internalTable().partitionRaftGroupService(0)
{code}
We wait the table on node with index 1, but expected that is completed on another node (nonLeaderNodeConsistentId).",Denis Chudov,v.pyatkov,Major,Resolved,Fixed,15/Jun/23 13:49,20/Jun/23 10:55
Bug,IGNITE-19749,13540248,AccessDeniedException on Windows,"{noformat}
org.apache.ignite.lang.IgniteInternalException: IGN-CMN-65535 TraceId:ac3ebe36-c426-4388-ba3f-02294db2cc19 org.apache.ignite.lang.IgniteInternalCheckedException: IGN-CMN-65535 TraceId:b9bc45c5-0f89-42cd-b530-b7f356ca268f Error when renaming delta file: D:\work\ignite-3\modules\ignite3-ycsb-work\1686841339026\db\db\table-1\part-0-delta-0.bin.tmp
	at org.apache.ignite.internal.pagememory.persistence.checkpoint.Checkpointer.body(Checkpointer.java:224)
	at org.apache.ignite.internal.util.worker.IgniteWorker.run(IgniteWorker.java:108)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.util.concurrent.CompletionException: org.apache.ignite.lang.IgniteInternalCheckedException: IGN-CMN-65535 TraceId:b9bc45c5-0f89-42cd-b530-b7f356ca268f Error when renaming delta file: D:\work\ignite-3\modules\ignite3-ycsb-work\1686841339026\db\db\table-1\part-0-delta-0.bin.tmp
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$BiRelay.tryFire(CompletableFuture.java:1423)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.ignite.internal.pagememory.persistence.checkpoint.Checkpointer.lambda$syncUpdatedPageStores$0(Checkpointer.java:490)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.ignite.lang.IgniteInternalCheckedException: IGN-CMN-65535 TraceId:b9bc45c5-0f89-42cd-b530-b7f356ca268f Error when renaming delta file: D:\work\ignite-3\modules\ignite3-ycsb-work\1686841339026\db\db\table-1\part-0-delta-0.bin.tmp
	at org.apache.ignite.internal.pagememory.persistence.checkpoint.Checkpointer.renameDeltaFileOnCheckpointThread(Checkpointer.java:782)
	at org.apache.ignite.internal.pagememory.persistence.checkpoint.Checkpointer.fsyncDeltaFile(Checkpointer.java:521)
	at org.apache.ignite.internal.pagememory.persistence.checkpoint.Checkpointer.lambda$syncUpdatedPageStores$0(Checkpointer.java:483)
	... 3 more
Caused by: java.nio.file.FileAlreadyExistsException: D:\work\ignite-3\modules\ignite3-ycsb-work\1686841339026\db\db\table-1\part-0-delta-0.bin.tmp -> D:\work\ignite-3\modules\ignite3-ycsb-work\1686841339026\db\db\table-1\part-0-delta-0.bin
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:87)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1421)
	at org.apache.ignite.internal.util.IgniteUtils.atomicMoveFile(IgniteUtils.java:771)
	at org.apache.ignite.internal.pagememory.persistence.store.AbstractFilePageStoreIo.renameFilePath(AbstractFilePageStoreIo.java:598)
	at org.apache.ignite.internal.pagememory.persistence.checkpoint.Checkpointer.renameDeltaFileOnCheckpointThread(Checkpointer.java:780)
	... 5 more
	Suppressed: java.nio.file.AccessDeniedException: D:\work\ignite-3\modules\ignite3-ycsb-work\1686841339026\db\db\table-1\part-0-delta-0.bin.tmp -> D:\work\ignite-3\modules\ignite3-ycsb-work\1686841339026\db\db\table-1\part-0-delta-0.bin
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1421)
		at org.apache.ignite.internal.util.IgniteUtils.atomicMoveFile(IgniteUtils.java:754)
		... 7 more
{noformat}

Reproducable by running ycsb benchmark:
https://github.com/gridgain/apache-ignite-3/tree/ignite3-ycsb
.\modules\ycsb\README.md
",ktkalenko@gridgain.com,ascherbakov,Major,Resolved,Fixed,15/Jun/23 15:08,19/Jun/23 11:00
Bug,IGNITE-19756,13540370,Thin 3.0: Colocation column order is not preserved,"[ClientTableCommon#writeSchema|https://github.com/apache/ignite-3/blob/317aca963ed9c87e14dd6eebd3284408fb550c8c/modules/client-handler/src/main/java/org/apache/ignite/client/handler/requests/table/ClientTableCommon.java#L75] does not preserve the order of colocation columns. It assumes that key column order is the same as colocation column order, which is not always true.

* Fix *writeSchema* somehow to specify the order of colocation columns (maybe send their indices as a separate array)
* Fix Java client accordingly",ptupitsyn,ptupitsyn,Critical,Resolved,Fixed,16/Jun/23 10:29,06/Jul/23 04:13
Bug,IGNITE-19764,13540410,FileDeployerService does not stop underlying thread pool,"The result is that a run of tests in the runner module might have tens (or maybe even more) of ""deployment0"" threads.

Probably the class needs a method for stopping/closing it; this method would stop the thread pool.",Mikhail Pochatkin,rpuch,Major,Resolved,Fixed,16/Jun/23 13:40,22/Jun/23 16:35
Bug,IGNITE-19772,13540583,Compilation error in main branch,"/opt/buildagent/work/b8d4df1365f1f1e5/modules/distribution-zones/src/test/java/org/apache/ignite/internal/distributionzones/DistributionZoneCausalityDataNodesTest.java:51: error: cannot find symbol
import org.apache.ignite.internal.distributionzones.exception.DistributionZoneNotFoundException;",Sergey Uttsel,Sergey Uttsel,Blocker,Resolved,Fixed,19/Jun/23 08:46,19/Jun/23 09:23
Bug,IGNITE-19774,13540684,.NET: IgniteClientTests.TestToString is flaky,"{code}
 Expected: String containing ""Name = org.apache.ignite.internal.runner.app.PlatformTestNodeRunner, Address = 127.0.0.1:109""
  But was:  ""IgniteClientInternal { Connections = [ ClusterNode { Id = 18f19c31-ccc3-4f4f-8096-f263f2e93a59, Name = org.apache.ignite.internal.runner.app.PlatformTestNodeRunner_2, Address = 127.0.0.1:10943 } ] }""
   at Apache.Ignite.Tests.IgniteClientTests.TestToString() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/IgniteClientTests.cs:line 59
   at NUnit.Framework.Internal.TaskAwaitAdapter.GenericAdapter`1.BlockUntilCompleted()
   at NUnit.Framework.Internal.MessagePumpStrategy.NoMessagePumpStrategy.WaitForCompletion(AwaitAdapter awaiter)
   at NUnit.Framework.Internal.AsyncToSyncAdapter.Await(Func`1 invoke)
   at NUnit.Framework.Internal.Commands.TestMethodCommand.RunTestMethod(TestExecutionContext context)
   at NUnit.Framework.Internal.Commands.TestMethodCommand.Execute(TestExecutionContext context)
   at NUnit.Framework.Internal.Commands.BeforeAndAfterTestCommand.<>c__DisplayClass1_0.<Execute>b__0()
   at NUnit.Framework.Internal.Commands.DelegatingTestCommand.RunTestMethodInThreadAbortSafeZone(TestExecutionContext context, Action action)
1)    at Apache.Ignite.Tests.IgniteClientTests.TestToString() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/IgniteClientTests.cs:line 59
{code}",ptupitsyn,ptupitsyn,Minor,Resolved,Fixed,20/Jun/23 04:17,20/Jun/23 06:31
Bug,IGNITE-19777,13540729,Perform local metastorage recovery ,"Upon recovery, metastorage should request current cluster-wide metastorage revision and wait until this revision is achieved.
After that it is safe to continue starting other components depending on the metastorage and perform their own recovery based on the metastorage data at said revision. After recovery is performed, watches must be installed with the revision + 1.",sdanilov,sdanilov,Major,Resolved,Fixed,20/Jun/23 09:54,27/Jun/23 10:10
Bug,IGNITE-19778,13540730,Restore components state on metastorage recovery,"Upon metastorage signalling (https://issues.apache.org/jira/browse/IGNITE-19777) that certain revision that was available when recovery started is achieved by the local metastorage storage, components should restore their state using a ""snapshot"" of the data with that specific revision. For example, TableManager might want to start tables that it sees in that snapshot. Note that tables that are missing from the data snapshot, must be destroyed as they are now obsolete.
So state must be restored and outdated remains of the previous state must be cleared out.",sdanilov,sdanilov,Major,Resolved,Fixed,20/Jun/23 09:54,12/Jul/23 13:59
Bug,IGNITE-19794,13540895,Thin 3.0: ClientMessagePacker packs binary incorrect tuples,"Take a look at the method:

{code:java}
    // org.apache.ignite.internal.client.proto.ClientMessagePacker
    public void packBinaryTuple(BinaryTupleReader binaryTupleReader, int elementCount) {
        ByteBuffer buf = binaryTupleReader.byteBuffer();
        int len = buf.limit() - buf.position();

        if (elementCount > -1) {
            binaryTupleReader.seek(elementCount - 1);
            len = binaryTupleReader.end();
            buf.limit(len + buf.position());
        }

        packBinaryHeader(len);
        writePayload(buf);
    }
{code}

If {{elementCount}} is provided (> -1), tuple will be truncated up to the element with index {{elementCount - 1}}. A truncated tuple, although retaining logical equality, is not binary equal to the same tuple, but correctly assembled, since neither offset table nor null map are adjusted.

Because {{ClientMessagePacker}} is a general purpose library to implement protocol between client and server, such behaviour is rather dangerous and should be fixed.

In the attachment there is a test that highlights the problem.",ptupitsyn,korlov,Minor,Resolved,Fixed,21/Jun/23 13:09,06/Jul/23 05:12
Bug,IGNITE-19801,13540980,Configuration prematurely executes metastore revision update listener,"Now there is a problem due to the fact that the configuration completes the *VersionedValue* on updating the revision of the metastore, even if no changes were received for the configuration.

Especially now it manifests itself when, for example, we add a table to the catalog and while listening to this event we try to update any *VersionedValue*, we get an error because the version has already been compiled.",ibessonov,ktkalenko@gridgain.com,Major,Resolved,Fixed,22/Jun/23 05:22,12/Jul/23 11:10
Bug,IGNITE-19803,13540990,Sql. Change erroneously annotated sql logic tests under ignite-15623 ,"Some tests are erroneously annotated under already completed issue [1], need to be changed for [2].

[1] https://issues.apache.org/jira/browse/IGNITE-15623
[2] https://issues.apache.org/jira/browse/IGNITE-19804",zstan,zstan,Major,Resolved,Fixed,22/Jun/23 07:59,26/Jun/23 07:08
Bug,IGNITE-19805,13540992,AssertionError in TableManager.latestTablesById,"The following assertion fails intermittently:
{code:java}
            // ""tablesByIdVv"" is always completed strictly before the ""assignmentsUpdatedVv"".
            assert tablesByIdFuture.isDone();
{code}

{code}
at org.apache.ignite.internal.table.distributed.TableManager.latestTablesById(TableManager.java:1834)
    at org.apache.ignite.internal.table.distributed.TableManager.lambda$tableAsyncInternal$85(TableManager.java:1975)
    at org.apache.ignite.internal.util.IgniteUtils.inBusyLock(IgniteUtils.java:892)
    at org.apache.ignite.internal.table.distributed.TableManager.lambda$tableAsyncInternal$86(TableManager.java:1970)
    at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1106)
    at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
    at org.apache.ignite.internal.table.distributed.TableManager.tableAsyncInternal(TableManager.java:1970)
    at org.apache.ignite.internal.table.distributed.TableManager.lambda$tableAsyncInternal$78(TableManager.java:1949)
    at org.apache.ignite.internal.util.IgniteUtils.inBusyLock(IgniteUtils.java:892)
    at org.apache.ignite.internal.table.distributed.TableManager.lambda$tableAsyncInternal$79(TableManager.java:1944)
    at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1072)
    at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
    at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1705)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:834)
{code}

https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_RunAllTests/7311388?hideProblemsFromDependencies=false&hideTestsFromDependencies=false&expandCode+Inspection=true&expandBuildProblemsSection=true&expandBuildTestsSection=true",ibessonov,ptupitsyn,Blocker,Resolved,Fixed,22/Jun/23 08:13,22/Jun/23 10:34
Bug,IGNITE-19808,13541012,.NET: Thin 3.0: MetricsTests.TestHandshakesFailedTimeout is flaky,"*MetricsTests.TestHandshakesFailedTimeout* is flaky:

{code}
Expected: 0
  But was:  2
   at Apache.Ignite.Tests.MetricsTests.TestHandshakesFailedTimeout() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/MetricsTests.cs:line 146
1)    at Apache.Ignite.Tests.MetricsTests.TestHandshakesFailedTimeout() in /opt/buildagent/work/b8d4df1365f1f1e5/modules/platforms/dotnet/Apache.Ignite.Tests/MetricsTests.cs:line 146
{code}

https://ci.ignite.apache.org/buildConfiguration/ApacheIgnite3xGradle_Test_RunNetTests/7312733?hideProblemsFromDependencies=false&hideTestsFromDependencies=false&expandBuildChangesSection=true&expandBuildTestsSection=true",ptupitsyn,ptupitsyn,Major,Resolved,Fixed,22/Jun/23 10:29,03/Jul/23 16:03
Bug,IGNITE-19811,13541045,Continuous qieries backup acknowledge message sending fails for expired entries,"Expire entry event has {{null}} in topology version field (see {{CacheContinuousQueryEntry}} constructor in the {{CacheContinuousQueryManager#onEntryExpired}} method). When Backup acknowledge is sending for such a message it silently (without warnings to log) fails with NPE on {{GridDiscoveryManager#cacheGroupAffinityNodes}} -> {{GridDiscoveryManager#resolveDiscoCache}} for {{null}} topology version (see {{CacheContinuousQueryHandler#sendBackupAcknowledge}}).
This can lead to leaks in query entries buffer ({{CacheContinuousQueryEventBuffer#backupQ}}).",alex_pl,alex_pl,Major,Resolved,Fixed,22/Jun/23 14:37,06/Jul/23 16:51
Bug,IGNITE-19812,13541064,Do not swallow exceptions during start of a partition,"Currently, such exceptions are just logged. They should be propagated.",rpuch,rpuch,Major,Resolved,Fixed,22/Jun/23 16:25,12/Jul/23 16:23
Bug,IGNITE-19815,13541130,Make ItTableScanTest resilient ,"After IGNITE-17951 a few tests (namely ItTableScanTest and ItInternalTableReadWriteScanTest) were switched to a new scan method accepting tx id only. The difference between old method and new method is that latter lack the tx finalisation step in case of any error. This causes cascade failures of tests within the same class, since no one rollbacks the tx manually.

To make tests more resilient to cascade failures, let's restore the logic for tx finalisation in case of errors during scan.",korlov,korlov,Major,Resolved,Fixed,23/Jun/23 07:35,23/Jun/23 13:07
Bug,IGNITE-19818,13541136,Calcite engine. Query planning failed when cache size is too big,"We use cache size as estimation for row count while planning, but use {{cache.size()}} method, that returns int value. But if cache size is more than {{Integer.MAX_VALUE}} we get an wrong size or even negative sometimes, which cause assertion errors during planning.
We should fix it to {{cache.sizeLong()}}.",alex_pl,alex_pl,Major,Resolved,Fixed,23/Jun/23 07:59,06/Jul/23 16:54
Bug,IGNITE-19826,13541170,.NET: Thin 3.0: Partition awareness uses wrong node id,"There is a confusion between *Local Node ID* (which we get from ConnectionContext.ClusterNode.Id) and *Consistent Node ID* (which is the same as Node Name, comes from ConnectionContext.ClusterNode.Name). Partition assignment (returned by ClientTablePartitionAssignmentGetRequest on the server) is based on *Consistent Node ID*.

All partition awareness tests use FakeServer, so we never tested that it actually works.",ptupitsyn,ptupitsyn,Blocker,Resolved,Fixed,23/Jun/23 12:31,29/Jun/23 14:37
Bug,IGNITE-19827,13541171,Java thin 3.0: Partition awareness uses wrong node id,"There is a confusion between *Local Node ID* (which we get from protocolContext().clusterNode().id()) and *Consistent Node ID* (which is the same as Node Name, comes from protocolContext().clusterNode().name()). Partition assignment (returned by ClientTablePartitionAssignmentGetRequest on the server) is based on *Consistent Node ID*.

All partition awareness tests use FakeServer, so we never tested that it actually works.",ptupitsyn,ptupitsyn,Blocker,Resolved,Fixed,23/Jun/23 12:34,03/Jul/23 07:47
Bug,IGNITE-19851,13541518,Add MetaStorage update update listeners and use instead of configuration revision update listeners,"The main problem is described in IGNITE-19801, but due to the fact that we now have code closely related to updating the configuration revision, it is difficult to do everything at once in IGNITE-19801, so it will be divided into parts.

In this task, I will add listeners for updating the revision of the *MetaStorage* and replace it with its use in the production code.",ibessonov,ktkalenko@gridgain.com,Major,Resolved,Fixed,27/Jun/23 07:58,28/Jun/23 11:27
Bug,IGNITE-19857,13541585,Default zone replica count is not listened by rebalance trigger,"As a result, when 'replicas' is changed on a default zone, rebalance is not triggered",rpuch,rpuch,Major,Resolved,Fixed,27/Jun/23 14:38,30/Jun/23 06:51
Bug,IGNITE-19859,13541588,Java thin 3.0: testAutoFlushByTimer is flaky,"https://ci.ignite.apache.org/test/-2896687331391880214?currentProjectId=ApacheIgnite3xGradle_Test_IntegrationTests&expandTestHistoryChartSection=true&branch=

{code}
org.opentest4j.AssertionFailedError: expected: <true> but was: <false>
  at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)
  at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)
  at app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)
  at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)
  at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:31)
  at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:180)
  at app//org.apache.ignite.internal.streamer.ItAbstractDataStreamerTest.testAutoFlushByTimer(ItAbstractDataStreamerTest.java:144)
{code}",ptupitsyn,ptupitsyn,Major,Resolved,Fixed,27/Jun/23 14:45,13/Jul/23 08:08
Bug,IGNITE-19878,13541896,Restrict possibility to create two caches with the same schema and index name,"It’s possible to create two different caches with the same schema and index name inside it from a client and after that get a failing of the server node during start-up with the error ""Duplicate index name... "" during start caches.

scenario:

start at least two server nodes

start client node which creates two different caches during start-up (via setting cache configs to config of client node) with the same schema name and index name, it leads to crushing the cluster

then try to start the cluster and get the error ""Duplicate index name...""",zstan,zstan,Major,Resolved,Fixed,29/Jun/23 07:01,05/Jul/23 10:23
Bug,HIVE-26904,13516461,QueryCompactor failed in commitCompaction if the tmp table dir is already removed ,"commitCompaction() of query-based compactions just remove the dirs of tmp tables. It should not fail the compaction if the dirs are already removed.

We've seen such a failure in Impala's test (IMPALA-11756):
{noformat}
2023-01-02T02:09:26,306  INFO [HiveServer2-Background-Pool: Thread-695] ql.Driver: Executing command(queryId=jenkins_20230102020926_69112755-b783-4214-89e5-1c7111dfe15f): alter table partial_catalog_info_test.insert_only_partitioned partition (part=1) compact 'minor' and wait
2023-01-02T02:09:26,306  INFO [HiveServer2-Background-Pool: Thread-695] ql.Driver: Starting task [Stage-0:DDL] in serial mode
2023-01-02T02:09:26,317  INFO [HiveServer2-Background-Pool: Thread-695] exec.Task: Compaction enqueued with id 15
...
2023-01-02T02:12:55,849 ERROR [impala-ec2-centos79-m6i-4xlarge-ondemand-1428.vpc.cloudera.com-48_executor] compactor.Worker: Caught exception while trying to compact id:15,dbname:partial_catalog_info_test,tableName:insert_only_partitioned,partName:part=1,state:^@,type:MINOR,enqueueTime:0,start:0,properties:null,runAs:jenkins,tooManyAborts:false,hasOldAbort:false,highestWriteId:3,errorMessage:null,workerId: null,initiatorId: null,retryRetention0. Marking failed to avoid repeated failures
java.io.FileNotFoundException: File hdfs://localhost:20500/tmp/hive/jenkins/092b533a-81c8-4b95-88e4-9472cf6f365d/_tmp_space.db/62ec04fb-e2d2-4a99-a454-ae709a3cccfe does not exist.
        at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:1275) ~[hadoop-hdfs-client-3.1.1.7.2.15.4-6.jar:?]
        at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:1249) ~[hadoop-hdfs-client-3.1.1.7.2.15.4-6.jar:?]
        at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1194) ~[hadoop-hdfs-client-3.1.1.7.2.15.4-6.jar:?]
        at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1190) ~[hadoop-hdfs-client-3.1.1.7.2.15.4-6.jar:?]
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-3.1.1.7.2.15.4-6.jar:?]
        at org.apache.hadoop.hdfs.DistributedFileSystem.listLocatedStatus(DistributedFileSystem.java:1208) ~[hadoop-hdfs-client-3.1.1.7.2.15.4-6.jar:?]
        at org.apache.hadoop.fs.FileSystem.listLocatedStatus(FileSystem.java:2144) ~[hadoop-common-3.1.1.7.2.15.4-6.jar:?]
        at org.apache.hadoop.fs.FileSystem$5.<init>(FileSystem.java:2302) ~[hadoop-common-3.1.1.7.2.15.4-6.jar:?]
        at org.apache.hadoop.fs.FileSystem.listFiles(FileSystem.java:2299) ~[hadoop-common-3.1.1.7.2.15.4-6.jar:?]
        at org.apache.hadoop.hive.ql.txn.compactor.QueryCompactor$Util.cleanupEmptyDir(QueryCompactor.java:261) ~[hive-exec-3.1.3000.2022.0.13.0-60.jar:3.1.3000.2022.0.13.0-60]
        at org.apache.hadoop.hive.ql.txn.compactor.MmMinorQueryCompactor.commitCompaction(MmMinorQueryCompactor.java:72) ~[hive-exec-3.1.3000.2022.0.13.0-60.jar:3.1.3000.2022.0.13.0-60]
        at org.apache.hadoop.hive.ql.txn.compactor.QueryCompactor.runCompactionQueries(QueryCompactor.java:146) ~[hive-exec-3.1.3000.2022.0.13.0-60.jar:3.1.3000.2022.0.13.0-60]
        at org.apache.hadoop.hive.ql.txn.compactor.MmMinorQueryCompactor.runCompaction(MmMinorQueryCompactor.java:63) ~[hive-exec-3.1.3000.2022.0.13.0-60.jar:3.1.3000.2022.0.13.0-60]
        at org.apache.hadoop.hive.ql.txn.compactor.Worker.findNextCompactionAndExecute(Worker.java:435) ~[hive-exec-3.1.3000.2022.0.13.0-60.jar:3.1.3000.2022.0.13.0-60]
        at org.apache.hadoop.hive.ql.txn.compactor.Worker.lambda$run$0(Worker.java:115) ~[hive-exec-3.1.3000.2022.0.13.0-60.jar:3.1.3000.2022.0.13.0-60]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_261]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_261]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_261]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_261]
2023-01-02T02:12:55,858  INFO [impala-ec2-centos79-m6i-4xlarge-ondemand-1428.vpc.cloudera.com-48_executor] compactor.Worker: Deleting result directories created by the compactor:2023-01-02T02:12:55,858  INFO [impala-ec2-centos79-m6i-4xlarge-ondemand-1428.vpc.cloudera.com-48_executor] compactor.Worker: hdfs://localhost:20500/test-warehouse/managed/partial_catalog_info_test.db/insert_only_partitioned/part=1/delta_0000001_0000003_v0001827
2023-01-02T02:12:55,859  INFO [impala-ec2-centos79-m6i-4xlarge-ondemand-1428.vpc.cloudera.com-48_executor] compactor.Worker: hdfs://localhost:20500/test-warehouse/managed/partial_catalog_info_test.db/insert_only_partitioned/part=1/delete_delta_0000001_0000003_v0001827
2023-01-02T02:12:55,859  INFO [impala-ec2-centos79-m6i-4xlarge-ondemand-1428.vpc.cloudera.com-48_executor] compactor.CompactionHeartbeatService: Stopping heartbeat task for TXN 1827
2023-01-02T02:12:55,859  INFO [impala-ec2-centos79-m6i-4xlarge-ondemand-1428.vpc.cloudera.com-48_executor] compactor.CompactionHeartbeatService$CompactionHeartbeater: Shutting down compaction txn heartbeater instance.
2023-01-02T02:12:55,859  INFO [impala-ec2-centos79-m6i-4xlarge-ondemand-1428.vpc.cloudera.com-48_executor] compactor.CompactionHeartbeatService$CompactionHeartbeater: Compaction txn heartbeater instance is successfully stopped.
2023-01-02T02:12:55,872  INFO [impala-ec2-centos79-m6i-4xlarge-ondemand-1428.vpc.cloudera.com-48] compactor.Worker: Worker thread finished one loop.
2023-01-02T02:12:55,872  INFO [impala-ec2-centos79-m6i-4xlarge-ondemand-1428.vpc.cloudera.com-48_executor] compactor.Worker: Processing compaction request null
2023-01-02T02:12:56,400  INFO [HiveServer2-Background-Pool: Thread-695] exec.Task: Compaction with id 15 finished with status: failed  {noformat}",stigahuang,stigahuang,Major,Resolved,Fixed,04/Jan/23 11:35,16/Jun/23 10:04
Bug,HIVE-26905,13516579,Backport HIVE-25173 to 3.2.0: Exclude pentaho-aggdesigner-algorithm from upgrade-acid build.,"In the current branch-3, upgrade-acid has a dependency on an old hive-exec version that has a transitive dependency to org.pentaho:pentaho-aggdesigner-algorithm. This artifact is no longer available in commonly supported Maven repositories, which causes a build failure. We can safely exclude the dependency, as was originally done in HIVE-25173.",cnauroth,cnauroth,Major,Resolved,Fixed,05/Jan/23 05:19,28/Mar/23 16:04
Bug,HIVE-26911,13516965,Renaming a translated external table with a specified location fails with 'location already exists' exception,"Renaming a translated external table with a specified location fails with 'location already exists' exception.
Below are steps for repro
{code:java}
create database tmp;
use tmp;
create table b(s string) stored as parquet location 'hdfs://localhost:20500/test-warehouse/tmp.db/some_location';
alter table b rename to bb;
ERROR: InvalidOperationException: New location for this table hive.tmp.bb already exists : hdfs://localhost:20500/test-warehouse/tmp.db/some_location {code}",hemanth619,hemanth619,Major,Resolved,Fixed,06/Jan/23 03:52,16/Jun/23 10:04
Bug,HIVE-26919,13517395,Fix-test-case-TestReplicationOptimisedBootstrap.testReverseFailoverBeforeOptimizedBootstrap,This Jira is related to test case failure corresponding to Jira CDPD-48053,atsaonerk,atsaonerk,Minor,Resolved,Fixed,10/Jan/23 06:26,09/Mar/23 05:16
Bug,HIVE-26922,13517431,Deadlock when rebuilding Materialized view stored by Iceberg,"{code}
create table tbl_ice(a int, b string, c int) stored by iceberg stored as orc tblproperties ('format-version'='1');
insert into tbl_ice values (1, 'one', 50), (2, 'two', 51), (3, 'three', 52), (4, 'four', 53), (5, 'five', 54);

create materialized view mat1 stored by iceberg stored as orc tblproperties ('format-version'='1') as
select tbl_ice.b, tbl_ice.c from tbl_ice where tbl_ice.c > 52;

insert into tbl_ice values (10, 'ten', 60);

alter materialized view mat1 rebuild;
{code}",kkasa,kkasa,Major,Resolved,Fixed,10/Jan/23 10:31,19/Jan/23 05:10
Bug,HIVE-26923,13517485,Create Table As fails for iceberg when the columns are not specified.,"Create Table As fails for iceberg when the columns are not defined.

Create table statement
{code:java}
create table xyz stored by iceberg stored by iceberg stored as orc TBLPROPERTIES ('format-version':'2') AS select name,age from studenttab10k group by name,age {code}
Error logs
{code:java}
2023-01-10T12:26:36,003 INFO  [pool-3-thread-1] jdbc.TestDriver: Query: create table xyz stored by iceberg stored by iceberg stored as orc TBLPROPERTIES ('format-version':'2') AS select name,age from studenttab10k group by name,age
2023-01-10T12:26:36,094 INFO  [Thread-5] jdbc.TestDriver: INFO  : Compiling command(queryId=hive_20230110122636_8b4d7e37-c7b7-4554-a75a-51dfe37a4716): create table xyz stored by iceberg stored by iceberg stored as orc TBLPROPERTIES ('format-version':'2') AS select name,age from studenttab10k group by name,age
2023-01-10T12:26:36,095 INFO  [Thread-5] jdbc.TestDriver: ERROR : FAILED: ParseException line 1:42 mismatched input 'by' expecting AS near 'stored' in table file format specification
2023-01-10T12:26:36,095 INFO  [Thread-5] jdbc.TestDriver: org.apache.hadoop.hive.ql.parse.ParseException: line 1:42 mismatched input 'by' expecting AS near 'stored' in table file format specification
2023-01-10T12:26:36,095 INFO  [Thread-5] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:127)
2023-01-10T12:26:36,095 INFO  [Thread-5] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:93)
2023-01-10T12:26:36,095 INFO  [Thread-5] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:85)
2023-01-10T12:26:36,096 INFO  [Thread-5] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.Compiler.parse(Compiler.java:174)
2023-01-10T12:26:36,096 INFO  [Thread-5] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:103)
2023-01-10T12:26:36,096 INFO  [Thread-5] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:201)
2023-01-10T12:26:36,096 INFO  [Thread-5] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:657)
2023-01-10T12:26:36,096 INFO  [Thread-5] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:603)
2023-01-10T12:26:36,097 INFO  [Thread-5] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:597)
2023-01-10T12:26:36,097 INFO  [Thread-5] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:127)
2023-01-10T12:26:36,097 INFO  [Thread-5] jdbc.TestDriver: 	at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:206)
2023-01-10T12:26:36,097 INFO  [Thread-5] jdbc.TestDriver: 	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336)
2023-01-10T12:26:36,097 INFO  [Thread-5] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-10T12:26:36,097 INFO  [Thread-5] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-10T12:26:36,098 INFO  [Thread-5] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-10T12:26:36,098 INFO  [Thread-5] jdbc.TestDriver: 	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:358)
2023-01-10T12:26:36,098 INFO  [Thread-5] jdbc.TestDriver: 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
2023-01-10T12:26:36,098 INFO  [Thread-5] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-10T12:26:36,099 INFO  [Thread-5] jdbc.TestDriver: 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
2023-01-10T12:26:36,099 INFO  [Thread-5] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-10T12:26:36,099 INFO  [Thread-5] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-10T12:26:36,099 INFO  [Thread-5] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-10T12:26:36,099 INFO  [Thread-5] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-10T12:26:36,099 INFO  [Thread-5] jdbc.TestDriver: 
 {code}
However for Hive the below query works
{code:java}
create table xyz stored as orc TBLPROPERTIES ('transactional' = 'true') AS select name,age from studenttab10k group by name,age;{code}",,dharmikt,Critical,Resolved,Fixed,10/Jan/23 18:51,10/Jan/23 19:01
Bug,HIVE-26924,13517490,Alter materialized view enable rewrite throws SemanticException for source iceberg table,"alter materialized view enable rewrite throws SemanticException for source iceberg table

SQL test
{code:java}
>>> create materialized view mv_rewrite as select t, si from all100k where t>115;

>>> analyze table mv_rewrite compute statistics for columns;

>>> set hive.explain.user=false;

>>> explain select si,t from all100k where t>116 and t<120;
!!! match row_contains
          alias: iceberg_test_db_hive.mv_rewrite

>>> alter materialized view mv_rewrite disable rewrite;

>>> explain select si,t from all100k where t>116 and t<120;
!!! match row_contains
          alias: all100k

>>> alter materialized view mv_rewrite enable rewrite;

>>> explain select si,t from all100k where t>116 and t<120;
!!! match row_contains
          alias: iceberg_test_db_hive.mv_rewrite

>>> drop materialized view mv_rewrite; {code}
 

Error
{code:java}
2023-01-10T18:40:34,303 INFO  [pool-3-thread-1] jdbc.TestDriver: Query: alter materialized view mv_rewrite enable rewrite
2023-01-10T18:40:34,365 INFO  [Thread-10] jdbc.TestDriver: INFO  : Compiling command(queryId=hive_20230110184034_f557b4a6-40a0-42ba-8e67-2f273f50af36): alter materialized view mv_rewrite enable rewrite
2023-01-10T18:40:34,426 INFO  [Thread-10] jdbc.TestDriver: ERROR : FAILED: SemanticException Automatic rewriting for materialized view cannot be enabled if the materialized view uses non-transactional tables
2023-01-10T18:40:34,426 INFO  [Thread-10] jdbc.TestDriver: org.apache.hadoop.hive.ql.parse.SemanticException: Automatic rewriting for materialized view cannot be enabled if the materialized view uses non-transactional tables
2023-01-10T18:40:34,426 INFO  [Thread-10] jdbc.TestDriver:      at org.apache.hadoop.hive.ql.ddl.view.materialized.alter.rewrite.AlterMaterializedViewRewriteAnalyzer.analyzeInternal(AlterMaterializedViewRewriteAnalyzer.java:75)
2023-01-10T18:40:34,426 INFO  [Thread-10] jdbc.TestDriver:      at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:313)
2023-01-10T18:40:34,427 INFO  [Thread-10] jdbc.TestDriver:      at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:222)
2023-01-10T18:40:34,427 INFO  [Thread-10] jdbc.TestDriver:      at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:105)
2023-01-10T18:40:34,427 INFO  [Thread-10] jdbc.TestDriver:      at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:201)
2023-01-10T18:40:34,427 INFO  [Thread-10] jdbc.TestDriver:      at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:657)
2023-01-10T18:40:34,427 INFO  [Thread-10] jdbc.TestDriver:      at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:603)
2023-01-10T18:40:34,427 INFO  [Thread-10] jdbc.TestDriver:      at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:597)
2023-01-10T18:40:34,427 INFO  [Thread-10] jdbc.TestDriver:      at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:127)
2023-01-10T18:40:34,427 INFO  [Thread-10] jdbc.TestDriver:      at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:206)
2023-01-10T18:40:34,428 INFO  [Thread-10] jdbc.TestDriver:      at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336)
2023-01-10T18:40:34,428 INFO  [Thread-10] jdbc.TestDriver:      at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-10T18:40:34,428 INFO  [Thread-10] jdbc.TestDriver:      at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-10T18:40:34,428 INFO  [Thread-10] jdbc.TestDriver:      at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-10T18:40:34,428 INFO  [Thread-10] jdbc.TestDriver:      at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:358)
2023-01-10T18:40:34,428 INFO  [Thread-10] jdbc.TestDriver:      at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
2023-01-10T18:40:34,428 INFO  [Thread-10] jdbc.TestDriver:      at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-10T18:40:34,428 INFO  [Thread-10] jdbc.TestDriver:      at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
2023-01-10T18:40:34,428 INFO  [Thread-10] jdbc.TestDriver:      at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-10T18:40:34,428 INFO  [Thread-10] jdbc.TestDriver:      at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-10T18:40:34,429 INFO  [Thread-10] jdbc.TestDriver:      at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-10T18:40:34,429 INFO  [Thread-10] jdbc.TestDriver:      at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-10T18:40:34,429 INFO  [Thread-10] jdbc.TestDriver:
2023-01-10T18:40:34,429 INFO  [Thread-10] jdbc.TestDriver: INFO  : Completed compiling command(queryId=hive_20230110184034_f557b4a6-40a0-42ba-8e67-2f273f50af36); Time taken: 0.03 seconds
2023-01-10T18:40:34,520 ERROR [pool-3-thread-1] jdbc.TestDriver: Error while compiling statement: FAILED: SemanticException Automatic rewriting for materialized view cannot be enabled if the materialized view uses non-transactional tables {code}",kkasa,dharmikt,Critical,Resolved,Fixed,10/Jan/23 19:46,19/Jan/23 11:13
Bug,HIVE-26925,13517496,MV with iceberg storage format fails when contains 'PARTITIONED ON' clause due to column number/types difference.,"MV with iceberg storage format fails when contains 'PARTITIONED ON' clause due to column number/types difference.
{code:java}
!!! annotations iceberg
>>> use iceberg_test_db_hive;
No rows affected
>>> set hive.exec.max.dynamic.partitions=2000;
>>> set hive.exec.max.dynamic.partitions.pernode=2000;
>>> drop materialized view if exists mv_agg_gby_col_partitioned;
>>> create materialized view mv_agg_gby_col_partitioned PARTITIONED ON (t) stored by iceberg stored as orc tblproperties ('format-version'='1') as select b,f,sum(b), sum(f),t from all100k group by b,f,v,c,t;
>>> analyze table mv_agg_gby_col_partitioned compute statistics for columns;
>>> set hive.explain.user=false;

>>> explain select b,f,sum(b) from all100k where t=93 group by c,v,f,b;
!!! match row_contains
          alias: iceberg_test_db_hive.mv_agg_gby_col_partitioned

>>> drop materialized view mv_agg_gby_col_partitioned;
 {code}
Error
{code:java}
2023-01-10T20:31:17,514 INFO  [pool-5-thread-1] jdbc.TestDriver: Query: create materialized view mv_agg_gby_col_partitioned PARTITIONED ON (t) stored by iceberg stored as orc tblproperties ('format-version'='1') as select b,f,sum(b), sum(f),t from all100k group by b,f,v,c,t
2023-01-10T20:31:18,099 INFO  [Thread-21] jdbc.TestDriver: INFO  : Compiling command(queryId=hive_20230110203117_6c333b6a-1642-40e7-80bc-e78dede47980): create materialized view mv_agg_gby_col_partitioned PARTITIONED ON (t) stored by iceberg stored as orc tblproperties ('format-version'='1') as select b,f,sum(b), sum(f),t from all100k group by b,f,v,c,t
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver: INFO  : No Stats for iceberg_test_db_hive@all100k, Columns: b, c, t, f, v
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver: ERROR : FAILED: SemanticException Line 0:-1 Cannot insert into target table because column number/types are different 'TOK_TMP_FILE': Table insclause-0 has 6 columns, but query has 5 columns.
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver: org.apache.hadoop.hive.ql.parse.SemanticException: Line 0:-1 Cannot insert into target table because column number/types are different 'TOK_TMP_FILE': Table insclause-0 has 6 columns, but query has 5 columns.
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genConversionSelectOperator(SemanticAnalyzer.java:8905)
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:8114)
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:11583)
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:11455)
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:12424)
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:12290)
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:13038)
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:756)
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:13154)
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:472)
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:313)
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:222)
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:105)
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:201)
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:657)
2023-01-10T20:31:18,100 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:603)
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:597)
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:127)
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:206)
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336)
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver:     at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver:     at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver:     at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:358)
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver:     at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver:     at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver:     at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver:     at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver:     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver:     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver:     at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver:
2023-01-10T20:31:18,101 INFO  [Thread-21] jdbc.TestDriver: INFO  : Completed compiling command(queryId=hive_20230110203117_6c333b6a-1642-40e7-80bc-e78dede47980); Time taken: 0.526 seconds
2023-01-10T20:31:18,306 ERROR [pool-5-thread-1] jdbc.TestDriver: Error while compiling statement: FAILED: SemanticException Line 0:-1 Cannot insert into target table because column number/types are different 'TOK_TMP_FILE': Table insclause-0 has 6 columns, but query has 5 columns. {code}
Similar query works for Hive native materialized views.",kkasa,dharmikt,Critical,Resolved,Fixed,10/Jan/23 20:44,10/Feb/23 02:45
Bug,HIVE-26931,13517611,REPL LOAD command does not throw any error for incorrect syntax,"In some cases, users are using the REPL LOAD command incorrectly. It does not really throw any meaningful error/warning message, and as expected it does not replicate the database as well.

For example,
{code:java}
repl load target_db with ('hive.repl.rootdir'='hdfs://c3649-node2.coelab.cloudera.com:8020/user/hive/repl', 'hive.repl.include.external.tables'= 'true', 'hive.repl.replica.external.table.base.dir'='hdfs://c3649node2.coelab.cloudera.com:8020/warehouse/tablespace/external/hive/target_db.db'){code}
The above command does not follow the REPL LOAD syntax. This does not produce any error message, nor it replicates the database. So, it causes confusion.
{code:java}
0: jdbc:hive2://nightly7x-us-bj-3.nightly7x-u> repl load test_1_replica with ('hive.repl.rootdir'='hdfs://c3649-node2.coelab.cloudera.com:8020/user/repl', 'hive.repl.include.external.tables'= 'true', 'hive.repl.replica.external.table.base.dir'='hdfs://c3649node2.coelab.cloudera.com:8020/warehouse/tablespace/external/hive/test_1_replica.db');
INFO  : Compiling command(queryId=hive_20221201113704_08ee46a6-ede9-4c92-9502-82b9fbc416bd): repl load test_1_replica with ('hive.repl.rootdir'='hdfs://c3649-node2.coelab.cloudera.com:8020/user/repl', 'hive.repl.include.external.tables'= 'true', 'hive.repl.replica.external.table.base.dir'='hdfs://c3649node2.coelab.cloudera.com:8020/warehouse/tablespace/external/hive/test_1_replica.db')
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Created Hive schema: Schema(fieldSchemas:null, properties:null)
INFO  : Completed compiling command(queryId=hive_20221201113704_08ee46a6-ede9-4c92-9502-82b9fbc416bd); Time taken: 0.051 seconds
INFO  : Executing command(queryId=hive_20221201113704_08ee46a6-ede9-4c92-9502-82b9fbc416bd): repl load test_1_replica with ('hive.repl.rootdir'='hdfs://c3649-node2.coelab.cloudera.com:8020/user/repl', 'hive.repl.include.external.tables'= 'true', 'hive.repl.replica.external.table.base.dir'='hdfs://c3649node2.coelab.cloudera.com:8020/warehouse/tablespace/external/hive/test_1_replica.db')
INFO  : Completed executing command(queryId=hive_20221201113704_08ee46a6-ede9-4c92-9502-82b9fbc416bd); Time taken: 0.001 seconds
INFO  : OK
No rows affected (0.065 seconds)
0: jdbc:hive2://nightly7x-us-bj-3.nightly7x-u>{code}
Ideally, since this is a wrong command, it should throw an error.",subhasis.gorai,subhasis.gorai,Major,Resolved,Fixed,11/Jan/23 12:33,13/Apr/23 12:12
Bug,HIVE-26936,13517756,Floating point zero and signed zero comparison as a part of join predicate is considered as not equal.,"Steps to reproduce:
{quote}CREATE TABLE t0(c0 INT);
CREATE TABLE t1(c0 DOUBLE);
INSERT INTO t0 VALUES(0);
INSERT INTO t1 VALUES('-0');

SELECT * FROM t0, t1 WHERE t0.c0 = t1.c0;
{quote}
*Expected Result:* 0.0 -0.0
*Actual:* No Rows selected

That the predicate should evaluate to TRUE can be verified with the following statement:
{quote}SELECT t0.c0 = t1.c0 FROM t0, t1; 
-------
_c0
-------
true
-------
{quote}
Similar issue fixed earlier as a part of HIVE-11174  for where clause condition, now join condition is having issue.

 ",Dayakar,Dayakar,Major,Resolved,Fixed,12/Jan/23 06:49,16/Jan/23 07:45
Bug,HIVE-26939,13518074,Hive LLAP Application Master fails to come up with Hadoop 3.3.4,"When current oss master hive tries to bring up the LLAP Application Master, it fails with this issue :
{code:java}
Executing the launch command\nINFO client.ServiceClient: Loading service definition from local FS: /var/lib/ambari-agent/tmp/llap-yarn-service_2023-01-10_07-56-46/Yarnfile\nERROR utils.JsonSerDeser: Exception while parsing json input stream\ncom.fasterxml.jackson.databind.exc.InvalidFormatException: Cannot deserialize value of type `org.apache.hadoop.yarn.service.api.records.PlacementScope` from String \""NODE\"": not one of the values accepted for Enum class: [node, rack]\n at [Source: (org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream); line: 31, column: 22] (through reference chain: org.apache.hadoop.yarn.service.api.records.Service[\""components\""]->java.util.ArrayList[0]->org.apache.hadoop.yarn.service.api.records.Component[\""placement_policy\""]->org.apache.hadoop.yarn.service.api.records.PlacementPolicy[\""constraints\""]->java.util.ArrayList[0]->org.apache.hadoop.yarn.service.api.records.PlacementConstraint[\""scope\""])\n\tat com.fasterxml.jackson.databind.exc.InvalidFormatException.from(InvalidFormatException.java:67) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.DeserializationContext.weirdStringException(DeserializationContext.java:1851) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.DeserializationContext.handleWeirdStringValue(DeserializationContext.java:1079) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.std.EnumDeserializer._deserializeAltString(EnumDeserializer.java:339) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.std.EnumDeserializer._fromString(EnumDeserializer.java:214) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.std.EnumDeserializer.deserialize(EnumDeserializer.java:188) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:129) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:324) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:187) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.std.CollectionDeserializer._deserializeFromArray(CollectionDeserializer.java:355) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:244) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:28) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:129) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:324) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:187) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:129) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:324) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:187) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.std.CollectionDeserializer._deserializeFromArray(CollectionDeserializer.java:355) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:244) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:28) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:129) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:324) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:187) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:322) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4593) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3585) ~[jackson-databind-2.12.7.jar:2.12.7]\n\tat org.apache.hadoop.yarn.service.utils.JsonSerDeser.fromStream(JsonSerDeser.java:137) [hadoop-yarn-services-core-3.3.4.5.1-SNAPSHOT.jar:?]\n\tat org.apache.hadoop.yarn.service.utils.JsonSerDeser.load(JsonSerDeser.java:179) [hadoop-yarn-services-core-3.3.4.5.1-SNAPSHOT.jar:?]\n\tat org.apache.hadoop.yarn.service.client.ServiceClient.loadAppJsonFromLocalFS(ServiceClient.java:208) [hadoop-yarn-services-core-3.3.4.5.1-SNAPSHOT.jar:?]\n\tat org.apache.hadoop.yarn.service.client.ServiceClient.actionLaunch(ServiceClient.java:546) [hadoop-yarn-services-core-3.3.4.5.1-SNAPSHOT.jar:?]\n\tat org.apache.hadoop.hive.llap.cli.LlapSliderUtils.startCluster(LlapSliderUtils.java:102) [hive-llap-server-3.1.2.5.1-SNAPSHOT.jar:3.1.2.5.1-SNAPSHOT]\n\tat org.apache.hadoop.hive.llap.cli.LlapServiceDriver.run(LlapServiceDriver.java:601) [hive-llap-server-3.1.2.5.1-SNAPSHOT.jar:3.1.2.5.1-SNAPSHOT]\n\tat org.apache.hadoop.hive.llap.cli.LlapServiceDriver.main(LlapServiceDriver.java:119) [hive-llap-server-3.1.2.5.1-SNAPSHOT.jar:3.1.2.5.1-SNAPSHOT]\n {code}
This is because llap_server uses templates.py yarnfile to bringup the LLAP Application Master which has the placement_policy as ""NODE"". Hadoop has started using 2.12.7 jackson version which has issues in parsing NODE to either [node, rack]. Therefore it is not able to cast the placement policy correctly.

 

Resolution :

We will have to send placement_policy as ""node"" from the templates.py of Hive Llap Server module. Will raise a PR for this issue :",amanraj2520,amanraj2520,Major,Resolved,Fixed,13/Jan/23 06:39,29/Jan/23 15:32
Bug,HIVE-26947,13519761,Hive compactor.Worker can respawn connections to HMS at extremely high frequency,"After catching the exception generated by the findNextCompactionAndExecute() task, HS2 appears to immediately rerun the task with no delay or backoff.  As a result there are ~3500 connection attempts from HS2 to HMS over just a 5 second period in the HS2 log

The compactor.Worker should wait between failed attempts and maybe do an exponential backoff.",akshatm,akshatm,Major,Resolved,Fixed,16/Jan/23 11:02,16/Jun/23 10:04
Bug,HIVE-26955,13519898,Select query fails when decimal column data type is changed to string/char/varchar in Parquet,"Steps to reproduce
{noformat}
create table test_parquet (id decimal) stored as parquet;
insert into test_parquet values(238);
alter table test_parquet change id id string;
select * from test_parquet;

Error: java.io.IOException: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs:/namenode:8020/warehouse/tablespace/managed/hive/test_parquet/delta_0000001_0000001_0000/000000_0 (state=,code=0)
    at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:624)
    at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:531)
    at org.apache.hadoop.hive.ql.exec.FetchTask.executeInner(FetchTask.java:194)
    ... 55 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file file:/home/centos/Apache-Hive-Tarak/itests/qtest/target/localfs/warehouse/test_parquet/000000_0
    at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:255)
    at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)
    at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:87)
    at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:89)
    at org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit.getRecordReader(FetchOperator.java:771)
    at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:335)
    at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:562)
    ... 57 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo cannot be cast to org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo
    at org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter$8$5.convert(ETypeConverter.java:669)
    at org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter$8$5.convert(ETypeConverter.java:664)
    at org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter$BinaryConverter.addBinary(ETypeConverter.java:977)
    at org.apache.parquet.column.impl.ColumnReaderBase$2$6.writeValue(ColumnReaderBase.java:360)
    at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:410)
    at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
    at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
    at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:230)
    ... 63 more{noformat}
However the same is working as expected in ORC table
{noformat}
create table test_orc (id decimal) stored as orc;
insert into test_orc values(238);
alter table test_orc change id id string;
select * from test_orc;
+--------------+
| test_orc.id  |
+--------------+
| 238          |
+--------------+{noformat}
As well as text table
{noformat}
create table test_text (id decimal) stored as textfile;
insert into test_text values(238);
alter table test_text change id id string;
select * from test_text;
+---------------+
| test_text.id  |
+---------------+
| 238           |
+---------------+{noformat}
Similar exception is thrown when the altered datatype is varchar and char datatype.",sbadhya,tarak271,Major,Resolved,Fixed,17/Jan/23 12:36,16/Jun/23 10:03
Bug,HIVE-26960,13519990,Optimized bootstrap does not drop newly added tables at source.,"Scenario:

Replication is setup from DR to PROD after failover from PROD to DR and no existing tables are modified at PROD but a new table is added at PROD.

Observations:
 * _bootstrap directory won't be created during second cycle of optimized bootstrap because existing tables were not modified.
 * Based on this, it will not initialize list of tables to drop at PROD.
 * This leads to the new table created at PROD not being dropped.",rakshithc,rakshithc,Major,Resolved,Fixed,18/Jan/23 06:59,01/Feb/23 01:23
Bug,HIVE-26963,13520002,Unset repl.faliover.endpoint during second cycle of optimized bootstrap.,"Scenario:

When second cycle of optimized bootstrap is initiated from DR to PROD. repl.failover.endpoint is not unset on DR.

Due to this background threads like statsUpdater, partitionManagement etc. ignore the replicated db on DR. ",rakshithc,rakshithc,Major,Resolved,Fixed,18/Jan/23 08:23,03/Feb/23 06:37
Bug,HIVE-26967,13520225,Deadlock when enabling/disabling Materialized view stored by Iceberg,"{code}
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

create table all100k(
        t int,
        si int,
        i int,
        b bigint,
        f float,
        d double,
        s string,
        dc decimal(38,18),
        bo boolean,
        v string,
        c string,
        ts timestamp,
        dt date)
    partitioned by spec (BUCKET(16, t))
    stored by iceberg
    stored as parquet;

create materialized view mv_rewrite stored by iceberg as select t, si from all100k where t>115;

explain select si,t from all100k where t>116 and t<120;

alter materialized view mv_rewrite disable rewrite;
{code}",kkasa,kkasa,Major,Resolved,Fixed,19/Jan/23 17:13,20/Jan/23 08:08
Bug,HIVE-26976,13520967,Failure in direct SQL processing do not undo the changes made to database before fallback to JDO resulting in stale entries in database forever,"*[Description]* 

Failure in direct SQL processing do not undo the changes made to database before falling back to JDO based processing. Thus results in stale/dangling entries in database forever.

For instance, during dropPartitions() direct SQL processing, after dropping rows from few tables(like PARTITIONS, PARTITION_PARAMS, PARTITION_KEY_VALS etc), it causes typecast exception in dropStorageDescriptors() and fallback to JDO processing. But datanucleus JDO processing cannot delete rows from remaining tables(i.e., from sds, serdes, sds_params, serde_params, sort_cols, bucketing_cols, skewed cols/values/location if any for the partitions) since the partition is already deleted in the same transaction during direct SQL processing.

!image-2023-01-23-15-23-00-975.png!

 

 

*[Steps to reproduce]* 

Reproduction steps are described in issue - https://issues.apache.org/jira/browse/HIVE-26860",VenuReddy,VenuReddy,Major,Resolved,Fixed,23/Jan/23 09:56,16/Jun/23 10:04
Bug,HIVE-26980,13521215,CTAS and CMV fails if target table is Iceberg and source table has unsupported column type,"MV creation using iceberg storage format for Hive table containing tinyint column fails with error ""java.lang.ClassCastException: class java.lang.Byte cannot be cast to class java.lang.Integer""

Logs
{code:java}
 2023-01-24T21:36:22,153 INFO  [pool-2-thread-1] jdbc.TestDriver: Beginning Test at 2023-01-24 21:36:22,153
2023-01-24T21:36:22,154 INFO  [pool-2-thread-1] jdbc.TestDriver: BEGIN MAIN
2023-01-24T21:36:22,155 INFO  [pool-38-thread-1] jdbc.TestDriver: Running SessionGroup{name='SG_TNMECIDQ31', initialDelay=0, repeats=1, repeatDelay=0}
2023-01-24T21:36:22,155 INFO  [pool-38-thread-1] jdbc.TestDriver: Connecting as user 'hrt_qa'
2023-01-24T21:36:22,409 INFO  [pool-38-thread-1] jdbc.TestDriver: Query: set hive.cbo.enable=true
2023-01-24T21:36:22,619 INFO  [pool-38-thread-1] jdbc.TestDriver: No output to verify
2023-01-24T21:36:22,619 INFO  [pool-38-thread-1] jdbc.TestDriver: Query: set hive.materializedview.rewriting
2023-01-24T21:36:22,946 INFO  [pool-38-thread-1] jdbc.TestDriver: Query: set iceberg.mr.schema.auto.conversion=true
2023-01-24T21:36:23,183 INFO  [pool-38-thread-1] jdbc.TestDriver: No output to verify
2023-01-24T21:36:23,183 INFO  [pool-38-thread-1] jdbc.TestDriver: Query: drop materialized view if exists mv_project
2023-01-24T21:36:23,243 INFO  [Thread-401] jdbc.TestDriver: INFO  : Compiling command(queryId=hive_20230124213623_e80df624-1ca6-4487-9f4b-616f756f91b3): drop materialized view if exists mv_project
2023-01-24T21:36:23,302 INFO  [Thread-401] jdbc.TestDriver: INFO  : Semantic Analysis Completed (retrial = false)
2023-01-24T21:36:23,302 INFO  [Thread-401] jdbc.TestDriver: INFO  : Created Hive schema: Schema(fieldSchemas:null, properties:null)
2023-01-24T21:36:23,302 INFO  [Thread-401] jdbc.TestDriver: INFO  : Completed compiling command(queryId=hive_20230124213623_e80df624-1ca6-4487-9f4b-616f756f91b3); Time taken: 0.03 seconds
2023-01-24T21:36:23,302 INFO  [Thread-401] jdbc.TestDriver: INFO  : Executing command(queryId=hive_20230124213623_e80df624-1ca6-4487-9f4b-616f756f91b3): drop materialized view if exists mv_project
2023-01-24T21:36:23,302 INFO  [Thread-401] jdbc.TestDriver: INFO  : Starting task [Stage-0:DDL] in serial mode
2023-01-24T21:36:23,302 INFO  [Thread-401] jdbc.TestDriver: INFO  : Completed executing command(queryId=hive_20230124213623_e80df624-1ca6-4487-9f4b-616f756f91b3); Time taken: 0.008 seconds
2023-01-24T21:36:23,302 INFO  [Thread-401] jdbc.TestDriver: INFO  : OK
2023-01-24T21:36:23,423 INFO  [pool-38-thread-1] jdbc.TestDriver: No output to verify
2023-01-24T21:36:23,423 INFO  [pool-38-thread-1] jdbc.TestDriver: Query: create materialized view mv_project stored by iceberg as select t, si, avg(t) from all100k where t>115 group by t, si
2023-01-24T21:36:24,015 INFO  [Thread-402] jdbc.TestDriver: INFO  : Compiling command(queryId=hive_20230124213623_e6e322e5-fa9b-4fb2-a3cb-ae244699ee5d): create materialized view mv_project stored by iceberg as select t, si, avg(t) from all100k where t>115 group by t, si
2023-01-24T21:36:24,015 INFO  [Thread-402] jdbc.TestDriver: INFO  : Semantic Analysis Completed (retrial = false)
2023-01-24T21:36:24,015 INFO  [Thread-402] jdbc.TestDriver: INFO  : Created Hive schema: Schema(fieldSchemas:[FieldSchema(name:t, type:tinyint, comment:null), FieldSchema(name:si, type:smallint, comment:null), FieldSchema(name:_c2, type:double, comment:null)], properties:null)
2023-01-24T21:36:24,015 INFO  [Thread-402] jdbc.TestDriver: INFO  : Completed compiling command(queryId=hive_20230124213623_e6e322e5-fa9b-4fb2-a3cb-ae244699ee5d); Time taken: 0.535 seconds
2023-01-24T21:36:24,015 INFO  [Thread-402] jdbc.TestDriver: INFO  : Executing command(queryId=hive_20230124213623_e6e322e5-fa9b-4fb2-a3cb-ae244699ee5d): create materialized view mv_project stored by iceberg as select t, si, avg(t) from all100k where t>115 group by t, si
2023-01-24T21:36:24,015 INFO  [Thread-402] jdbc.TestDriver: INFO  : Compute 'qe-vw-dwx-hive-snwh' is active.
2023-01-24T21:36:24,015 INFO  [Thread-402] jdbc.TestDriver: INFO  : Query ID = hive_20230124213623_e6e322e5-fa9b-4fb2-a3cb-ae244699ee5d
2023-01-24T21:36:24,015 INFO  [Thread-402] jdbc.TestDriver: INFO  : Total jobs = 1
2023-01-24T21:36:24,015 INFO  [Thread-402] jdbc.TestDriver: INFO  : Starting task [Stage-4:DDL] in serial mode
2023-01-24T21:36:25,697 INFO  [Thread-402] jdbc.TestDriver: INFO  : Starting task [Stage-5:DDL] in serial mode
2023-01-24T21:36:25,697 INFO  [Thread-402] jdbc.TestDriver: INFO  : Launching Job 1 out of 1
2023-01-24T21:36:25,697 INFO  [Thread-402] jdbc.TestDriver: INFO  : Starting task [Stage-1:MAPRED] in serial mode
2023-01-24T21:36:25,697 INFO  [Thread-402] jdbc.TestDriver: INFO  : Subscribed to counters: [] for queryId: hive_20230124213623_e6e322e5-fa9b-4fb2-a3cb-ae244699ee5d
2023-01-24T21:36:25,697 INFO  [Thread-402] jdbc.TestDriver: INFO  : Tez session hasn't been created yet. Opening session
2023-01-24T21:36:25,697 INFO  [Thread-402] jdbc.TestDriver: INFO  : Dag name: create materialized view mv_project sto...si (Stage-1)
2023-01-24T21:36:26,257 INFO  [Thread-402] jdbc.TestDriver: INFO  : HS2 Host: [hiveserver2-0], Query ID: [hive_20230124213623_e6e322e5-fa9b-4fb2-a3cb-ae244699ee5d], Dag ID: [dag_1674594626929_0001_124], DAG Session ID: [application_1674594626929_0001]
2023-01-24T21:36:26,818 INFO  [Thread-402] jdbc.TestDriver: INFO  : Status: Running (Executing on YARN cluster with App id application_1674594626929_0001)
2023-01-24T21:36:26,818 INFO  [Thread-402] jdbc.TestDriver: 
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: ERROR : Status: Failed
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: ERROR : Vertex failed, vertexName=Reducer 2, vertexId=vertex_1674594626929_0001_124_01, diagnostics=[Task failed, taskId=task_1674594626929_0001_124_01_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1674594626929_0001_124_01_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:409)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:257)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:318)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	... 15 more
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:511)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:400)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	... 18 more
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: Caused by: java.lang.ClassCastException
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: ], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1674594626929_0001_124_01_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:409)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:257)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:318)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	... 15 more
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:511)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:400)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	... 18 more
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: Caused by: java.lang.ClassCastException: class java.lang.Byte cannot be cast to class java.lang.Integer (java.lang.Byte and java.lang.Integer are in module java.base of loader 'bootstrap')
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ColumnWriter$2.write(ColumnWriter.java:38)
2023-01-24T21:36:59,705 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ParquetValueWriters$PrimitiveWriter.write(ParquetValueWriters.java:134)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ParquetValueWriters$OptionWriter.write(ParquetValueWriters.java:342)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ParquetValueWriters$StructWriter.write(ParquetValueWriters.java:562)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ParquetWriter.add(ParquetWriter.java:131)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.DataWriter.write(DataWriter.java:61)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.RollingFileWriter.write(RollingFileWriter.java:86)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.RollingDataWriter.write(RollingDataWriter.java:33)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.ClusteredWriter.write(ClusteredWriter.java:103)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.ClusteredDataWriter.write(ClusteredDataWriter.java:32)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.mr.hive.writer.HiveIcebergRecordWriter.write(HiveIcebergRecordWriter.java:53)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:1168)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.process(VectorFileSinkOperator.java:111)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:980)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:158)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:968)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.flushOutput(VectorGroupByOperator.java:1305)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.writeGroupRow(VectorGroupByOperator.java:1299)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.access$2300(VectorGroupByOperator.java:81)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeReduceMergePartial.doProcessBatch(VectorGroupByOperator.java:964)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeBase.processBatch(VectorGroupByOperator.java:198)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:1250)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:498)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	... 19 more
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: ], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1674594626929_0001_124_01_000000_2:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:409)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:257)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:318)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	... 15 more
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:511)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:400)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	... 18 more
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: Caused by: java.lang.ClassCastException
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: ], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1674594626929_0001_124_01_000000_3:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:409)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:257)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:318)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	... 15 more
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:511)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:400)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	... 18 more
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: Caused by: java.lang.ClassCastException: class java.lang.Byte cannot be cast to class java.lang.Integer (java.lang.Byte and java.lang.Integer are in module java.base of loader 'bootstrap')
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ColumnWriter$2.write(ColumnWriter.java:38)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ParquetValueWriters$PrimitiveWriter.write(ParquetValueWriters.java:134)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ParquetValueWriters$OptionWriter.write(ParquetValueWriters.java:342)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ParquetValueWriters$StructWriter.write(ParquetValueWriters.java:562)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ParquetWriter.add(ParquetWriter.java:131)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.DataWriter.write(DataWriter.java:61)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.RollingFileWriter.write(RollingFileWriter.java:86)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.RollingDataWriter.write(RollingDataWriter.java:33)
2023-01-24T21:36:59,706 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.ClusteredWriter.write(ClusteredWriter.java:103)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.ClusteredDataWriter.write(ClusteredDataWriter.java:32)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.mr.hive.writer.HiveIcebergRecordWriter.write(HiveIcebergRecordWriter.java:53)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:1168)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.process(VectorFileSinkOperator.java:111)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:980)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:158)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:968)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.flushOutput(VectorGroupByOperator.java:1305)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.writeGroupRow(VectorGroupByOperator.java:1299)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.access$2300(VectorGroupByOperator.java:81)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeReduceMergePartial.doProcessBatch(VectorGroupByOperator.java:964)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeBase.processBatch(VectorGroupByOperator.java:198)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:1250)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:498)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	... 19 more
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: ]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:1, Vertex vertex_1674594626929_0001_124_01 [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: ERROR : Vertex killed, vertexName=Reducer 3, vertexId=vertex_1674594626929_0001_124_02, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to OTHER_VERTEX_FAILURE, failedTasks:0 killedTasks:1, Vertex vertex_1674594626929_0001_124_02 [Reducer 3] killed/failed due to:OTHER_VERTEX_FAILURE]
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: ERROR : DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  : org.apache.tez.common.counters.DAGCounter:
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    NUM_FAILED_TASKS: 7
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    NUM_KILLED_TASKS: 1
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    NUM_SUCCEEDED_TASKS: 1
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    TOTAL_LAUNCHED_TASKS: 9
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    DATA_LOCAL_TASKS: 1
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    AM_CPU_MILLISECONDS: 980
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    AM_GC_TIME_MILLIS: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  : File System Counters:
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    FILE_BYTES_READ: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    FILE_BYTES_WRITTEN: 21689
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    FILE_READ_OPS: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    FILE_LARGE_READ_OPS: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    FILE_WRITE_OPS: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    LLAPCACHE_BYTES_READ: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    LLAPCACHE_BYTES_WRITTEN: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    LLAPCACHE_READ_OPS: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    LLAPCACHE_LARGE_READ_OPS: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    LLAPCACHE_WRITE_OPS: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    S3A_BYTES_READ: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    S3A_BYTES_WRITTEN: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    S3A_READ_OPS: 2
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    S3A_LARGE_READ_OPS: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    S3A_WRITE_OPS: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  : org.apache.tez.common.counters.TaskCounter:
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    SPILLED_RECORDS: 4699
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    TASK_DURATION_MILLIS: 115
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    INPUT_RECORDS_PROCESSED: 98
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    INPUT_SPLIT_LENGTH_BYTES: 6193255
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    OUTPUT_RECORDS: 4699
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    OUTPUT_LARGE_RECORDS: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    OUTPUT_BYTES: 42310
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    OUTPUT_BYTES_WITH_OVERHEAD: 51720
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    OUTPUT_BYTES_PHYSICAL: 21633
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    ADDITIONAL_SPILLS_BYTES_WRITTEN: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    ADDITIONAL_SPILLS_BYTES_READ: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    ADDITIONAL_SPILL_COUNT: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    SHUFFLE_CHUNK_COUNT: 1
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  : HIVE:
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    DESERIALIZE_ERRORS: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    RECORDS_IN_Map_1: 100000
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    RECORDS_OUT_INTERMEDIATE_Map_1: 4699
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_FIL_19: 4718
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_GBY_20: 4699
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_MAP_0: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_RS_21: 4699
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_TS_0: 100000
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  : org.apache.hadoop.hive.llap.counters.LlapWmCounters:
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    GUARANTEED_QUEUED_NS: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    GUARANTEED_RUNNING_NS: 0
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    SPECULATIVE_QUEUED_NS: 76277
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    SPECULATIVE_RUNNING_NS: 113146858
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  : org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable$LlapExecutorCounters:
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    EXECUTOR_CPU_NS: 49230478
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    EXECUTOR_USER_NS: 40000000
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  : org.apache.hadoop.hive.ql.exec.tez.HiveInputCounters:
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    GROUPED_INPUT_SPLITS_Map_1: 1
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    INPUT_DIRECTORIES_Map_1: 1
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    INPUT_FILES_Map_1: 1
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: INFO  :    RAW_INPUT_SPLITS_Map_1: 1
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: ERROR : FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 2, vertexId=vertex_1674594626929_0001_124_01, diagnostics=[Task failed, taskId=task_1674594626929_0001_124_01_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1674594626929_0001_124_01_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-24T21:36:59,707 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:409)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:257)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:318)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	... 15 more
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:511)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:400)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	... 18 more
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: Caused by: java.lang.ClassCastException
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: ], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1674594626929_0001_124_01_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:409)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:257)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:318)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	... 15 more
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:511)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:400)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	... 18 more
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: Caused by: java.lang.ClassCastException: class java.lang.Byte cannot be cast to class java.lang.Integer (java.lang.Byte and java.lang.Integer are in module java.base of loader 'bootstrap')
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ColumnWriter$2.write(ColumnWriter.java:38)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ParquetValueWriters$PrimitiveWriter.write(ParquetValueWriters.java:134)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ParquetValueWriters$OptionWriter.write(ParquetValueWriters.java:342)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ParquetValueWriters$StructWriter.write(ParquetValueWriters.java:562)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ParquetWriter.add(ParquetWriter.java:131)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.DataWriter.write(DataWriter.java:61)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.RollingFileWriter.write(RollingFileWriter.java:86)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.RollingDataWriter.write(RollingDataWriter.java:33)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.ClusteredWriter.write(ClusteredWriter.java:103)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.ClusteredDataWriter.write(ClusteredDataWriter.java:32)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.mr.hive.writer.HiveIcebergRecordWriter.write(HiveIcebergRecordWriter.java:53)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:1168)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.process(VectorFileSinkOperator.java:111)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:980)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:158)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:968)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.flushOutput(VectorGroupByOperator.java:1305)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.writeGroupRow(VectorGroupByOperator.java:1299)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.access$2300(VectorGroupByOperator.java:81)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeReduceMergePartial.doProcessBatch(VectorGroupByOperator.java:964)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeBase.processBatch(VectorGroupByOperator.java:198)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:1250)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:498)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	... 19 more
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: ], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1674594626929_0001_124_01_000000_2:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:409)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:257)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:318)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	... 15 more
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:511)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:400)
2023-01-24T21:36:59,708 INFO  [Thread-402] jdbc.TestDriver: 	... 18 more
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: Caused by: java.lang.ClassCastException
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: ], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1674594626929_0001_124_01_000000_3:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:409)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:257)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:318)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	... 15 more
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:511)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:400)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	... 18 more
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: Caused by: java.lang.ClassCastException: class java.lang.Byte cannot be cast to class java.lang.Integer (java.lang.Byte and java.lang.Integer are in module java.base of loader 'bootstrap')
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ColumnWriter$2.write(ColumnWriter.java:38)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ParquetValueWriters$PrimitiveWriter.write(ParquetValueWriters.java:134)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ParquetValueWriters$OptionWriter.write(ParquetValueWriters.java:342)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ParquetValueWriters$StructWriter.write(ParquetValueWriters.java:562)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.parquet.ParquetWriter.add(ParquetWriter.java:131)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.DataWriter.write(DataWriter.java:61)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.RollingFileWriter.write(RollingFileWriter.java:86)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.RollingDataWriter.write(RollingDataWriter.java:33)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.ClusteredWriter.write(ClusteredWriter.java:103)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.io.ClusteredDataWriter.write(ClusteredDataWriter.java:32)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.iceberg.mr.hive.writer.HiveIcebergRecordWriter.write(HiveIcebergRecordWriter.java:53)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:1168)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.process(VectorFileSinkOperator.java:111)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:980)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:158)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:968)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.flushOutput(VectorGroupByOperator.java:1305)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.writeGroupRow(VectorGroupByOperator.java:1299)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.access$2300(VectorGroupByOperator.java:81)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeReduceMergePartial.doProcessBatch(VectorGroupByOperator.java:964)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeBase.processBatch(VectorGroupByOperator.java:198)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:1250)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:498)
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: 	... 19 more
2023-01-24T21:36:59,709 INFO  [Thread-402] jdbc.TestDriver: ]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:1, Vertex vertex_1674594626929_0001_124_01 [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]Vertex killed, vertexName=Reducer 3, vertexId=vertex_1674594626929_0001_124_02, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to OTHER_VERTEX_FAILURE, failedTasks:0 killedTasks:1, Vertex vertex_1674594626929_0001_124_02 [Reducer 3] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
2023-01-24T21:37:00,129 INFO  [Thread-402] jdbc.TestDriver: INFO  : Completed executing command(queryId=hive_20230124213623_e6e322e5-fa9b-4fb2-a3cb-ae244699ee5d); Time taken: 36.038 seconds
2023-01-24T21:37:00,129 INFO  [Thread-402] jdbc.TestDriver: INFO  : OK
2023-01-24T21:37:00,489 ERROR [pool-38-thread-1] jdbc.TestDriver: Error while compiling statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 2, vertexId=vertex_1674594626929_0001_124_01, diagnostics=[Task failed, taskId=task_1674594626929_0001_124_01_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1674594626929_0001_124_01_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:409)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:257)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:318)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
	... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:511)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:400)
	... 18 more
Caused by: java.lang.ClassCastException
], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1674594626929_0001_124_01_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:409)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:257)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:318)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
	... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:511)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:400)
	... 18 more
Caused by: java.lang.ClassCastException: class java.lang.Byte cannot be cast to class java.lang.Integer (java.lang.Byte and java.lang.Integer are in module java.base of loader 'bootstrap')
	at org.apache.iceberg.parquet.ColumnWriter$2.write(ColumnWriter.java:38)
	at org.apache.iceberg.parquet.ParquetValueWriters$PrimitiveWriter.write(ParquetValueWriters.java:134)
	at org.apache.iceberg.parquet.ParquetValueWriters$OptionWriter.write(ParquetValueWriters.java:342)
	at org.apache.iceberg.parquet.ParquetValueWriters$StructWriter.write(ParquetValueWriters.java:562)
	at org.apache.iceberg.parquet.ParquetWriter.add(ParquetWriter.java:131)
	at org.apache.iceberg.io.DataWriter.write(DataWriter.java:61)
	at org.apache.iceberg.io.RollingFileWriter.write(RollingFileWriter.java:86)
	at org.apache.iceberg.io.RollingDataWriter.write(RollingDataWriter.java:33)
	at org.apache.iceberg.io.ClusteredWriter.write(ClusteredWriter.java:103)
	at org.apache.iceberg.io.ClusteredDataWriter.write(ClusteredDataWriter.java:32)
	at org.apache.iceberg.mr.hive.writer.HiveIcebergRecordWriter.write(HiveIcebergRecordWriter.java:53)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:1168)
	at org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.process(VectorFileSinkOperator.java:111)
	at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:980)
	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:158)
	at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:968)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.flushOutput(VectorGroupByOperator.java:1305)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.writeGroupRow(VectorGroupByOperator.java:1299)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.access$2300(VectorGroupByOperator.java:81)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeReduceMergePartial.doProcessBatch(VectorGroupByOperator.java:964)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeBase.processBatch(VectorGroupByOperator.java:198)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:1250)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:498)
	... 19 more
], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1674594626929_0001_124_01_000000_2:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:409)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:257)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:318)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
	... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:511)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:400)
	... 18 more
Caused by: java.lang.ClassCastException
], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1674594626929_0001_124_01_000000_3:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:409)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:257)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:318)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
	... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) (vectorizedVertexNum 1)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:511)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:400)
	... 18 more
Caused by: java.lang.ClassCastException: class java.lang.Byte cannot be cast to class java.lang.Integer (java.lang.Byte and java.lang.Integer are in module java.base of loader 'bootstrap')
	at org.apache.iceberg.parquet.ColumnWriter$2.write(ColumnWriter.java:38)
	at org.apache.iceberg.parquet.ParquetValueWriters$PrimitiveWriter.write(ParquetValueWriters.java:134)
	at org.apache.iceberg.parquet.ParquetValueWriters$OptionWriter.write(ParquetValueWriters.java:342)
	at org.apache.iceberg.parquet.ParquetValueWriters$StructWriter.write(ParquetValueWriters.java:562)
	at org.apache.iceberg.parquet.ParquetWriter.add(ParquetWriter.java:131)
	at org.apache.iceberg.io.DataWriter.write(DataWriter.java:61)
	at org.apache.iceberg.io.RollingFileWriter.write(RollingFileWriter.java:86)
	at org.apache.iceberg.io.RollingDataWriter.write(RollingDataWriter.java:33)
	at org.apache.iceberg.io.ClusteredWriter.write(ClusteredWriter.java:103)
	at org.apache.iceberg.io.ClusteredDataWriter.write(ClusteredDataWriter.java:32)
	at org.apache.iceberg.mr.hive.writer.HiveIcebergRecordWriter.write(HiveIcebergRecordWriter.java:53)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:1168)
	at org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.process(VectorFileSinkOperator.java:111)
	at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:980)
	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:158)
	at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:968)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.flushOutput(VectorGroupByOperator.java:1305)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.writeGroupRow(VectorGroupByOperator.java:1299)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.access$2300(VectorGroupByOperator.java:81)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeReduceMergePartial.doProcessBatch(VectorGroupByOperator.java:964)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeBase.processBatch(VectorGroupByOperator.java:198)
	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:1250)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:498)
	... 19 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:1, Vertex vertex_1674594626929_0001_124_01 [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]Vertex killed, vertexName=Reducer 3, vertexId=vertex_1674594626929_0001_124_02, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to OTHER_VERTEX_FAILURE, failedTasks:0 killedTasks:1, Vertex vertex_1674594626929_0001_124_02 [Reducer 3] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
2023-01-24T21:37:00,520 INFO  [pool-2-thread-1] jdbc.TestDriver: Ending Test at 2023-01-24 21:37:00,520
2023-01-24T21:37:00,520 INFO  [pool-2-thread-1] jdbc.TestDriver: TEST FAILED in 38 seconds.
2023-01-24T21:37:00,520 INFO  [pool-2-thread-1] jdbc.TestDriver: Annotations: [canary_sqlcoverage, canary_complexquery, pvc_ozone]{code}
 

SQL Test
{code:java}
!!! annotations canary_sqlcoverage canary_complexquery pvc_ozone
>>> set hive.cbo.enable=true;
>>> set hive.materializedview.rewriting;
hive.materializedview.rewriting=true
>>> set iceberg.mr.schema.auto.conversion=true;

>>> drop materialized view if exists mv_project;
>>> create materialized view mv_project stored by iceberg as select t, si, avg(t) from all100k where t>115 group by t, si;
No rows affected
>>> analyze table mv_project compute statistics for columns;
No rows affected
>>> set hive.explain.user=false;

>>> explain select t, avg(t) from all100k where t>115 group by t, si;
!!! match row_contains
          alias: default.mv_project

>>> drop materialized view mv_project;
 {code}
 

Source table
{code:java}
create external table all100k_txt(
t tinyint,
si smallint,
i int,
b bigint,
f float,
d double,
s string,
dc decimal(38,18),
bo boolean,
v varchar(25),
c char(25),
ts timestamp,
dt date)
row format delimited
fields terminated by '|'
stored as textfile
location 's3a://dwx-testdata/user/hrt_qa/tgtnv/core/tests/data/all100k';

create table all100k stored as parquet as select * from all100k_txt;{code}",kkasa,dharmikt,Major,Resolved,Fixed,25/Jan/23 06:53,31/Jan/23 09:43
Bug,HIVE-26982,13521226,"Select * from a table containing timestamp column with default defined using TIMESTAMPLOCALTZ fails with error "" ORC doesn't handle primitive category TIMESTAMPLOCALTZ""","Select * from a table containing timestamp column with default defined using TIMESTAMPLOCALTZ fails with error "" ORC doesn't handle primitive category TIMESTAMPLOCALTZ""

Logs
{code:java}
2023-01-24T20:37:48,831 INFO  [pool-2-thread-1] jdbc.TestDriver: Beginning Test at 2023-01-24 20:37:48,831
2023-01-24T20:37:48,833 INFO  [pool-2-thread-1] jdbc.TestDriver: BEGIN MAIN
2023-01-24T20:37:48,834 INFO  [pool-9-thread-1] jdbc.TestDriver: Running SessionGroup{name='SG_JZSL3SA0OG', initialDelay=0, repeats=1, repeatDelay=0}
2023-01-24T20:37:48,834 INFO  [pool-9-thread-1] jdbc.TestDriver: Connecting as user 'hrt_qa'
2023-01-24T20:37:49,173 INFO  [pool-9-thread-1] jdbc.TestDriver: Query: drop table if exists t1_default
2023-01-24T20:37:49,237 INFO  [Thread-64] jdbc.TestDriver: INFO  : Compiling command(queryId=hive_20230124203749_09b0f95f-4cf1-4c2c-9f08-1b91fdb4a6ca): drop table if exists t1_default
2023-01-24T20:37:49,299 INFO  [Thread-64] jdbc.TestDriver: INFO  : Semantic Analysis Completed (retrial = false)
2023-01-24T20:37:49,299 INFO  [Thread-64] jdbc.TestDriver: INFO  : Created Hive schema: Schema(fieldSchemas:null, properties:null)
2023-01-24T20:37:49,299 INFO  [Thread-64] jdbc.TestDriver: INFO  : Completed compiling command(queryId=hive_20230124203749_09b0f95f-4cf1-4c2c-9f08-1b91fdb4a6ca); Time taken: 0.031 seconds
2023-01-24T20:37:49,299 INFO  [Thread-64] jdbc.TestDriver: INFO  : Executing command(queryId=hive_20230124203749_09b0f95f-4cf1-4c2c-9f08-1b91fdb4a6ca): drop table if exists t1_default
2023-01-24T20:37:49,299 INFO  [Thread-64] jdbc.TestDriver: INFO  : Starting task [Stage-0:DDL] in serial mode
2023-01-24T20:37:49,299 INFO  [Thread-64] jdbc.TestDriver: INFO  : Completed executing command(queryId=hive_20230124203749_09b0f95f-4cf1-4c2c-9f08-1b91fdb4a6ca); Time taken: 0.012 seconds
2023-01-24T20:37:49,299 INFO  [Thread-64] jdbc.TestDriver: INFO  : OK
2023-01-24T20:37:49,416 INFO  [pool-9-thread-1] jdbc.TestDriver: No output to verify
2023-01-24T20:37:49,416 INFO  [pool-9-thread-1] jdbc.TestDriver: Query: create table t1_default ( 	t tinyint default 1Y, 	si smallint default 1S, 	i int default 1, 	b bigint default 1L,     f double default double(5.7),     d double,     s varchar(25) default cast('col1' as varchar(25)),     dc decimal(38,18),     bo varchar(5),     v varchar(25),     c char(25) default cast('var1' as char(25)),     ts timestamp DEFAULT TIMESTAMP'2016-02-22 12:45:07.000000000',     dt date default cast('2015-03-12' as DATE),     tz timestamp with local time zone DEFAULT TIMESTAMPLOCALTZ'2016-01-03 12:26:34 America/Los_Angeles')     STORED AS TEXTFILE
2023-01-24T20:37:49,476 INFO  [Thread-65] jdbc.TestDriver: INFO  : Compiling command(queryId=hive_20230124203749_75ffcf31-6bd6-46d7-ba02-f39efb2c4279): create table t1_default ( 	t tinyint default 1Y, 	si smallint default 1S, 	i int default 1, 	b bigint default 1L,     f double default double(5.7),     d double,     s varchar(25) default cast('col1' as varchar(25)),     dc decimal(38,18),     bo varchar(5),     v varchar(25),     c char(25) default cast('var1' as char(25)),     ts timestamp DEFAULT TIMESTAMP'2016-02-22 12:45:07.000000000',     dt date default cast('2015-03-12' as DATE),     tz timestamp with local time zone DEFAULT TIMESTAMPLOCALTZ'2016-01-03 12:26:34 America/Los_Angeles')     STORED AS TEXTFILE
2023-01-24T20:37:50,036 INFO  [Thread-65] jdbc.TestDriver: INFO  : Semantic Analysis Completed (retrial = false)
2023-01-24T20:37:50,036 INFO  [Thread-65] jdbc.TestDriver: INFO  : Created Hive schema: Schema(fieldSchemas:null, properties:null)
2023-01-24T20:37:50,036 INFO  [Thread-65] jdbc.TestDriver: INFO  : Completed compiling command(queryId=hive_20230124203749_75ffcf31-6bd6-46d7-ba02-f39efb2c4279); Time taken: 0.038 seconds
2023-01-24T20:37:50,036 INFO  [Thread-65] jdbc.TestDriver: INFO  : Executing command(queryId=hive_20230124203749_75ffcf31-6bd6-46d7-ba02-f39efb2c4279): create table t1_default ( 	t tinyint default 1Y, 	si smallint default 1S, 	i int default 1, 	b bigint default 1L,     f double default double(5.7),     d double,     s varchar(25) default cast('col1' as varchar(25)),     dc decimal(38,18),     bo varchar(5),     v varchar(25),     c char(25) default cast('var1' as char(25)),     ts timestamp DEFAULT TIMESTAMP'2016-02-22 12:45:07.000000000',     dt date default cast('2015-03-12' as DATE),     tz timestamp with local time zone DEFAULT TIMESTAMPLOCALTZ'2016-01-03 12:26:34 America/Los_Angeles')     STORED AS TEXTFILE
2023-01-24T20:37:50,036 INFO  [Thread-65] jdbc.TestDriver: INFO  : Starting task [Stage-0:DDL] in serial mode
2023-01-24T20:37:50,105 INFO  [Thread-65] jdbc.TestDriver: INFO  : Completed executing command(queryId=hive_20230124203749_75ffcf31-6bd6-46d7-ba02-f39efb2c4279); Time taken: 0.55 seconds
2023-01-24T20:37:50,105 INFO  [Thread-65] jdbc.TestDriver: INFO  : OK
2023-01-24T20:37:50,222 INFO  [pool-9-thread-1] jdbc.TestDriver: No output to verify
2023-01-24T20:37:50,222 INFO  [pool-9-thread-1] jdbc.TestDriver: Query: insert into t1_default(t,si) values (2,5)
2023-01-24T20:37:50,282 INFO  [Thread-66] jdbc.TestDriver: INFO  : Compiling command(queryId=hive_20230124203750_1747a338-b515-440a-916b-7da63fd77e1d): insert into t1_default(t,si) values (2,5)
2023-01-24T20:37:50,841 INFO  [Thread-66] jdbc.TestDriver: INFO  : Semantic Analysis Completed (retrial = false)
2023-01-24T20:37:50,841 INFO  [Thread-66] jdbc.TestDriver: INFO  : Created Hive schema: Schema(fieldSchemas:[FieldSchema(name:_col0, type:tinyint, comment:null), FieldSchema(name:_col1, type:smallint, comment:null), FieldSchema(name:_col2, type:int, comment:null), FieldSchema(name:_col3, type:bigint, comment:null), FieldSchema(name:_col4, type:double, comment:null), FieldSchema(name:_col5, type:double, comment:null), FieldSchema(name:_col6, type:varchar(25), comment:null), FieldSchema(name:_col7, type:decimal(38,18), comment:null), FieldSchema(name:_col8, type:varchar(5), comment:null), FieldSchema(name:_col9, type:varchar(25), comment:null), FieldSchema(name:_col10, type:char(25), comment:null), FieldSchema(name:_col11, type:timestamp, comment:null), FieldSchema(name:_col12, type:date, comment:null), FieldSchema(name:_col13, type:timestamp with local time zone, comment:null)], properties:null)
2023-01-24T20:37:50,841 INFO  [Thread-66] jdbc.TestDriver: INFO  : Completed compiling command(queryId=hive_20230124203750_1747a338-b515-440a-916b-7da63fd77e1d); Time taken: 0.207 seconds
2023-01-24T20:37:50,841 INFO  [Thread-66] jdbc.TestDriver: INFO  : Executing command(queryId=hive_20230124203750_1747a338-b515-440a-916b-7da63fd77e1d): insert into t1_default(t,si) values (2,5)
2023-01-24T20:37:50,841 INFO  [Thread-66] jdbc.TestDriver: INFO  : Compute 'qe-vw-dwx-hive-snwh' is active.
2023-01-24T20:37:50,841 INFO  [Thread-66] jdbc.TestDriver: INFO  : Query ID = hive_20230124203750_1747a338-b515-440a-916b-7da63fd77e1d
2023-01-24T20:37:50,841 INFO  [Thread-66] jdbc.TestDriver: INFO  : Total jobs = 1
2023-01-24T20:37:50,841 INFO  [Thread-66] jdbc.TestDriver: INFO  : Launching Job 1 out of 1
2023-01-24T20:37:50,841 INFO  [Thread-66] jdbc.TestDriver: INFO  : Starting task [Stage-1:MAPRED] in serial mode
2023-01-24T20:37:50,841 INFO  [Thread-66] jdbc.TestDriver: INFO  : Subscribed to counters: [] for queryId: hive_20230124203750_1747a338-b515-440a-916b-7da63fd77e1d
2023-01-24T20:37:50,841 INFO  [Thread-66] jdbc.TestDriver: INFO  : Tez session hasn't been created yet. Opening session
2023-01-24T20:37:50,841 INFO  [Thread-66] jdbc.TestDriver: INFO  : Dag name: insert into t1_default(t,si) values (2,5) (Stage-1)
2023-01-24T20:37:50,841 INFO  [Thread-66] jdbc.TestDriver: INFO  : HS2 Host: [hiveserver2-0], Query ID: [hive_20230124203750_1747a338-b515-440a-916b-7da63fd77e1d], Dag ID: [dag_1674591732462_0001_63], DAG Session ID: [application_1674591732462_0001]
2023-01-24T20:37:50,841 INFO  [Thread-66] jdbc.TestDriver: INFO  : Status: Running (Executing on YARN cluster with App id application_1674591732462_0001)
2023-01-24T20:37:50,841 INFO  [Thread-66] jdbc.TestDriver: 
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : Status: DAG finished successfully in 1.07 seconds
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : DAG ID: dag_1674591732462_0001_63
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : 
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : Query Execution Summary
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : OPERATION                            DURATION
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : Compile Query                           0.21s
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : Prepare Plan                            0.04s
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : Get Query Coordinator (AM)              0.06s
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : Submit Plan                             0.01s
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : Start DAG                               0.00s
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : Run DAG                                 1.07s
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : 
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : Task Execution Summary
2023-01-24T20:37:51,960 INFO  [Thread-66] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :   VERTICES      DURATION(ms)   CPU_TIME(ms)    GC_TIME(ms)   INPUT_RECORDS   OUTPUT_RECORDS
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :      Map 1            570.00              0              0               3                0
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : 
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : LLAP IO Summary
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :   VERTICES ROWGROUPS  META_HIT  META_MISS  DATA_HIT  DATA_MISS  TOTAL_IO
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :      Map 1         0         0          0        0B         0B     0.00s
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : 
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : FileSystem Counters Summary
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : 
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : Scheme: FILE
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :   VERTICES      BYTES_READ      READ_OPS     LARGE_READ_OPS      BYTES_WRITTEN     WRITE_OPS
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :      Map 1              0B             0                  0                87B             0
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : 
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : Scheme: S3A
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :   VERTICES      BYTES_READ      READ_OPS     LARGE_READ_OPS      BYTES_WRITTEN     WRITE_OPS
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :      Map 1              0B             6                  0               291B             2
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : 
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : org.apache.tez.common.counters.DAGCounter:
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    NUM_SUCCEEDED_TASKS: 1
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    TOTAL_LAUNCHED_TASKS: 1
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    RACK_LOCAL_TASKS: 1
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    AM_CPU_MILLISECONDS: 150
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    AM_GC_TIME_MILLIS: 0
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : File System Counters:
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    FILE_BYTES_READ: 0
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    FILE_BYTES_WRITTEN: 87
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    FILE_READ_OPS: 0
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    FILE_LARGE_READ_OPS: 0
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    FILE_WRITE_OPS: 0
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    S3A_BYTES_READ: 0
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    S3A_BYTES_WRITTEN: 291
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    S3A_READ_OPS: 6
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    S3A_LARGE_READ_OPS: 0
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    S3A_WRITE_OPS: 2
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : org.apache.tez.common.counters.TaskCounter:
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    TASK_DURATION_MILLIS: 1040
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    INPUT_RECORDS_PROCESSED: 4
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    INPUT_SPLIT_LENGTH_BYTES: 1
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    OUTPUT_RECORDS: 0
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  : HIVE:
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    CREATED_FILES: 1
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    DESERIALIZE_ERRORS: 0
2023-01-24T20:37:51,961 INFO  [Thread-66] jdbc.TestDriver: INFO  :    RECORDS_IN_Map_1: 3
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  :    RECORDS_OUT_1_default.t1_default: 1
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  :    RECORDS_OUT_INTERMEDIATE_Map_1: 0
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_FS_5: 1
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_MAP_0: 0
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_SEL_1: 1
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_SEL_3: 1
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_TS_0: 1
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_UDTF_2: 1
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  :    TOTAL_TABLE_ROWS_WRITTEN: 1
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  : org.apache.hadoop.hive.llap.counters.LlapWmCounters:
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  :    GUARANTEED_QUEUED_NS: 0
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  :    GUARANTEED_RUNNING_NS: 0
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  :    SPECULATIVE_QUEUED_NS: 99064
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  :    SPECULATIVE_RUNNING_NS: 1036942551
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  : org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable$LlapExecutorCounters:
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  :    EXECUTOR_CPU_NS: 104999582
2023-01-24T20:37:51,962 INFO  [Thread-66] jdbc.TestDriver: INFO  :    EXECUTOR_USER_NS: 80000000
2023-01-24T20:37:52,519 INFO  [Thread-66] jdbc.TestDriver: INFO  : Starting task [Stage-2:DEPENDENCY_COLLECTION] in serial mode
2023-01-24T20:37:52,519 INFO  [Thread-66] jdbc.TestDriver: INFO  : Starting task [Stage-0:MOVE] in serial mode
2023-01-24T20:37:52,519 INFO  [Thread-66] jdbc.TestDriver: INFO  : Loading data to table default.t1_default from s3a://qe-s3-bucket-weekly-km7n-dwx-managed/clusters/env-pgkm7n/warehouse-1674590807-pxxm/warehouse/tablespace/managed/hive/t1_default
2023-01-24T20:37:52,852 INFO  [Thread-66] jdbc.TestDriver: INFO  : Starting task [Stage-3:STATS] in serial mode
2023-01-24T20:37:52,852 INFO  [Thread-66] jdbc.TestDriver: INFO  : Executing stats task
2023-01-24T20:37:52,852 INFO  [Thread-66] jdbc.TestDriver: INFO  : Table default.t1_default stats: [numFiles=1, numRows=1, totalSize=116, rawDataSize=115, numFilesErasureCoded=0]
2023-01-24T20:37:52,852 INFO  [Thread-66] jdbc.TestDriver: INFO  : Table default.t1_default stats: [numFiles=1, numRows=1, totalSize=116, rawDataSize=115, numFilesErasureCoded=0]
2023-01-24T20:37:52,852 INFO  [Thread-66] jdbc.TestDriver: INFO  : Completed executing command(queryId=hive_20230124203750_1747a338-b515-440a-916b-7da63fd77e1d); Time taken: 2.325 seconds
2023-01-24T20:37:52,852 INFO  [Thread-66] jdbc.TestDriver: INFO  : OK
2023-01-24T20:37:52,996 INFO  [pool-9-thread-1] jdbc.TestDriver: No output to verify
2023-01-24T20:37:52,996 INFO  [pool-9-thread-1] jdbc.TestDriver: Query: insert into t1_default(b,dt) values (2,cast('2019-08-14' as DATE))
2023-01-24T20:37:53,055 INFO  [Thread-67] jdbc.TestDriver: INFO  : Compiling command(queryId=hive_20230124203753_700fcc2b-82f3-4d9a-a4a9-90a4b5a27f51): insert into t1_default(b,dt) values (2,cast('2019-08-14' as DATE))
2023-01-24T20:37:53,614 INFO  [Thread-67] jdbc.TestDriver: INFO  : Semantic Analysis Completed (retrial = false)
2023-01-24T20:37:53,614 INFO  [Thread-67] jdbc.TestDriver: INFO  : Created Hive schema: Schema(fieldSchemas:[FieldSchema(name:_col0, type:tinyint, comment:null), FieldSchema(name:_col1, type:smallint, comment:null), FieldSchema(name:_col2, type:int, comment:null), FieldSchema(name:_col3, type:bigint, comment:null), FieldSchema(name:_col4, type:double, comment:null), FieldSchema(name:_col5, type:double, comment:null), FieldSchema(name:_col6, type:varchar(25), comment:null), FieldSchema(name:_col7, type:decimal(38,18), comment:null), FieldSchema(name:_col8, type:varchar(5), comment:null), FieldSchema(name:_col9, type:varchar(25), comment:null), FieldSchema(name:_col10, type:char(25), comment:null), FieldSchema(name:_col11, type:timestamp, comment:null), FieldSchema(name:_col12, type:date, comment:null), FieldSchema(name:_col13, type:timestamp with local time zone, comment:null)], properties:null)
2023-01-24T20:37:53,614 INFO  [Thread-67] jdbc.TestDriver: INFO  : Completed compiling command(queryId=hive_20230124203753_700fcc2b-82f3-4d9a-a4a9-90a4b5a27f51); Time taken: 0.22 seconds
2023-01-24T20:37:53,614 INFO  [Thread-67] jdbc.TestDriver: INFO  : Executing command(queryId=hive_20230124203753_700fcc2b-82f3-4d9a-a4a9-90a4b5a27f51): insert into t1_default(b,dt) values (2,cast('2019-08-14' as DATE))
2023-01-24T20:37:53,614 INFO  [Thread-67] jdbc.TestDriver: INFO  : Compute 'qe-vw-dwx-hive-snwh' is active.
2023-01-24T20:37:53,614 INFO  [Thread-67] jdbc.TestDriver: INFO  : Query ID = hive_20230124203753_700fcc2b-82f3-4d9a-a4a9-90a4b5a27f51
2023-01-24T20:37:53,614 INFO  [Thread-67] jdbc.TestDriver: INFO  : Total jobs = 1
2023-01-24T20:37:53,614 INFO  [Thread-67] jdbc.TestDriver: INFO  : Launching Job 1 out of 1
2023-01-24T20:37:53,614 INFO  [Thread-67] jdbc.TestDriver: INFO  : Starting task [Stage-1:MAPRED] in serial mode
2023-01-24T20:37:53,614 INFO  [Thread-67] jdbc.TestDriver: INFO  : Subscribed to counters: [] for queryId: hive_20230124203753_700fcc2b-82f3-4d9a-a4a9-90a4b5a27f51
2023-01-24T20:37:53,614 INFO  [Thread-67] jdbc.TestDriver: INFO  : Tez session hasn't been created yet. Opening session
2023-01-24T20:37:53,614 INFO  [Thread-67] jdbc.TestDriver: INFO  : Dag name: insert into t1_default(b,dt) values...DATE)) (Stage-1)
2023-01-24T20:37:53,614 INFO  [Thread-67] jdbc.TestDriver: INFO  : HS2 Host: [hiveserver2-0], Query ID: [hive_20230124203753_700fcc2b-82f3-4d9a-a4a9-90a4b5a27f51], Dag ID: [dag_1674591732462_0001_64], DAG Session ID: [application_1674591732462_0001]
2023-01-24T20:37:53,614 INFO  [Thread-67] jdbc.TestDriver: INFO  : Status: Running (Executing on YARN cluster with App id application_1674591732462_0001)
2023-01-24T20:37:53,614 INFO  [Thread-67] jdbc.TestDriver: 
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : Status: DAG finished successfully in 0.91 seconds
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : DAG ID: dag_1674591732462_0001_64
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : 
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : Query Execution Summary
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : OPERATION                            DURATION
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : Compile Query                           0.22s
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : Prepare Plan                            0.04s
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : Get Query Coordinator (AM)              0.07s
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : Submit Plan                             0.01s
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : Start DAG                               0.00s
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : Run DAG                                 0.91s
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : 
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : Task Execution Summary
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  :   VERTICES      DURATION(ms)   CPU_TIME(ms)    GC_TIME(ms)   INPUT_RECORDS   OUTPUT_RECORDS
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  :      Map 1            402.00              0              0               3                0
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : 
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : LLAP IO Summary
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  :   VERTICES ROWGROUPS  META_HIT  META_MISS  DATA_HIT  DATA_MISS  TOTAL_IO
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  :      Map 1         0         0          0        0B         0B     0.00s
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : 
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : FileSystem Counters Summary
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : 
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : Scheme: FILE
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  :   VERTICES      BYTES_READ      READ_OPS     LARGE_READ_OPS      BYTES_WRITTEN     WRITE_OPS
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  :      Map 1              0B             0                  0                87B             0
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : 
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : Scheme: S3A
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  :   VERTICES      BYTES_READ      READ_OPS     LARGE_READ_OPS      BYTES_WRITTEN     WRITE_OPS
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  :      Map 1              0B             6                  0               291B             2
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : ----------------------------------------------------------------------------------------------
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : 
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  : org.apache.tez.common.counters.DAGCounter:
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  :    NUM_SUCCEEDED_TASKS: 1
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  :    TOTAL_LAUNCHED_TASKS: 1
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  :    RACK_LOCAL_TASKS: 1
2023-01-24T20:37:54,731 INFO  [Thread-67] jdbc.TestDriver: INFO  :    AM_CPU_MILLISECONDS: 350
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    AM_GC_TIME_MILLIS: 0
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  : File System Counters:
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    FILE_BYTES_READ: 0
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    FILE_BYTES_WRITTEN: 87
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    FILE_READ_OPS: 0
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    FILE_LARGE_READ_OPS: 0
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    FILE_WRITE_OPS: 0
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    S3A_BYTES_READ: 0
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    S3A_BYTES_WRITTEN: 291
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    S3A_READ_OPS: 6
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    S3A_LARGE_READ_OPS: 0
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    S3A_WRITE_OPS: 2
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  : org.apache.tez.common.counters.TaskCounter:
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    TASK_DURATION_MILLIS: 870
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    INPUT_RECORDS_PROCESSED: 4
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    INPUT_SPLIT_LENGTH_BYTES: 1
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    OUTPUT_RECORDS: 0
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  : HIVE:
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    CREATED_FILES: 1
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    DESERIALIZE_ERRORS: 0
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    RECORDS_IN_Map_1: 3
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    RECORDS_OUT_1_default.t1_default: 1
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    RECORDS_OUT_INTERMEDIATE_Map_1: 0
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_FS_5: 1
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_MAP_0: 0
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_SEL_1: 1
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_SEL_3: 1
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_TS_0: 1
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    RECORDS_OUT_OPERATOR_UDTF_2: 1
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    TOTAL_TABLE_ROWS_WRITTEN: 1
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  : org.apache.hadoop.hive.llap.counters.LlapWmCounters:
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    GUARANTEED_QUEUED_NS: 0
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    GUARANTEED_RUNNING_NS: 0
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    SPECULATIVE_QUEUED_NS: 85652
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    SPECULATIVE_RUNNING_NS: 867097700
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  : org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable$LlapExecutorCounters:
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    EXECUTOR_CPU_NS: 99146898
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  :    EXECUTOR_USER_NS: 60000000
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  : Starting task [Stage-2:DEPENDENCY_COLLECTION] in serial mode
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  : Starting task [Stage-0:MOVE] in serial mode
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  : Loading data to table default.t1_default from s3a://qe-s3-bucket-weekly-km7n-dwx-managed/clusters/env-pgkm7n/warehouse-1674590807-pxxm/warehouse/tablespace/managed/hive/t1_default
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  : Starting task [Stage-3:STATS] in serial mode
2023-01-24T20:37:54,732 INFO  [Thread-67] jdbc.TestDriver: INFO  : Executing stats task
2023-01-24T20:37:55,179 INFO  [Thread-67] jdbc.TestDriver: INFO  : Table default.t1_default stats: [numFiles=2, numRows=2, totalSize=232, rawDataSize=230, numFilesErasureCoded=0]
2023-01-24T20:37:55,179 INFO  [Thread-67] jdbc.TestDriver: INFO  : Table default.t1_default stats: [numFiles=2, numRows=2, totalSize=232, rawDataSize=230, numFilesErasureCoded=0]
2023-01-24T20:37:55,179 INFO  [Thread-67] jdbc.TestDriver: INFO  : Completed executing command(queryId=hive_20230124203753_700fcc2b-82f3-4d9a-a4a9-90a4b5a27f51); Time taken: 1.842 seconds
2023-01-24T20:37:55,179 INFO  [Thread-67] jdbc.TestDriver: INFO  : OK
2023-01-24T20:37:55,321 INFO  [pool-9-thread-1] jdbc.TestDriver: No output to verify
2023-01-24T20:37:55,321 INFO  [pool-9-thread-1] jdbc.TestDriver: Query: select * from t1_default ORDER BY t1_default.t
2023-01-24T20:37:55,382 INFO  [Thread-68] jdbc.TestDriver: INFO  : Compiling command(queryId=hive_20230124203755_a1e9f54e-09b8-48e3-9966-827c66f717cb): select * from t1_default ORDER BY t1_default.t
2023-01-24T20:37:55,943 INFO  [Thread-68] jdbc.TestDriver: INFO  : No Stats for default@t1_default, Columns: b, c, d, f, tz, i, bo, dt, s, t, si, v, dc, ts
2023-01-24T20:37:55,943 INFO  [Thread-68] jdbc.TestDriver: INFO  : Semantic Analysis Completed (retrial = false)
2023-01-24T20:37:55,943 INFO  [Thread-68] jdbc.TestDriver: INFO  : Created Hive schema: Schema(fieldSchemas:[FieldSchema(name:t1_default.t, type:tinyint, comment:null), FieldSchema(name:t1_default.si, type:smallint, comment:null), FieldSchema(name:t1_default.i, type:int, comment:null), FieldSchema(name:t1_default.b, type:bigint, comment:null), FieldSchema(name:t1_default.f, type:double, comment:null), FieldSchema(name:t1_default.d, type:double, comment:null), FieldSchema(name:t1_default.s, type:varchar(25), comment:null), FieldSchema(name:t1_default.dc, type:decimal(38,18), comment:null), FieldSchema(name:t1_default.bo, type:varchar(5), comment:null), FieldSchema(name:t1_default.v, type:varchar(25), comment:null), FieldSchema(name:t1_default.c, type:char(25), comment:null), FieldSchema(name:t1_default.ts, type:timestamp, comment:null), FieldSchema(name:t1_default.dt, type:date, comment:null), FieldSchema(name:t1_default.tz, type:timestamp with local time zone, comment:null)], properties:null)
2023-01-24T20:37:55,943 INFO  [Thread-68] jdbc.TestDriver: INFO  : Completed compiling command(queryId=hive_20230124203755_a1e9f54e-09b8-48e3-9966-827c66f717cb); Time taken: 0.172 seconds
2023-01-24T20:37:55,943 INFO  [Thread-68] jdbc.TestDriver: INFO  : Executing command(queryId=hive_20230124203755_a1e9f54e-09b8-48e3-9966-827c66f717cb): select * from t1_default ORDER BY t1_default.t
2023-01-24T20:37:55,943 INFO  [Thread-68] jdbc.TestDriver: INFO  : Compute 'qe-vw-dwx-hive-snwh' is active.
2023-01-24T20:37:55,943 INFO  [Thread-68] jdbc.TestDriver: INFO  : Query ID = hive_20230124203755_a1e9f54e-09b8-48e3-9966-827c66f717cb
2023-01-24T20:37:55,943 INFO  [Thread-68] jdbc.TestDriver: INFO  : Total jobs = 1
2023-01-24T20:37:55,943 INFO  [Thread-68] jdbc.TestDriver: INFO  : Launching Job 1 out of 1
2023-01-24T20:37:55,943 INFO  [Thread-68] jdbc.TestDriver: INFO  : Starting task [Stage-1:MAPRED] in serial mode
2023-01-24T20:37:55,943 INFO  [Thread-68] jdbc.TestDriver: INFO  : Subscribed to counters: [] for queryId: hive_20230124203755_a1e9f54e-09b8-48e3-9966-827c66f717cb
2023-01-24T20:37:55,943 INFO  [Thread-68] jdbc.TestDriver: INFO  : Tez session hasn't been created yet. Opening session
2023-01-24T20:37:55,943 INFO  [Thread-68] jdbc.TestDriver: INFO  : Dag name: select * from t1_default ORDE...t1_default.t (Stage-1)
2023-01-24T20:37:55,943 INFO  [Thread-68] jdbc.TestDriver: INFO  : HS2 Host: [hiveserver2-0], Query ID: [hive_20230124203755_a1e9f54e-09b8-48e3-9966-827c66f717cb], Dag ID: [dag_1674591732462_0001_65], DAG Session ID: [application_1674591732462_0001]
2023-01-24T20:37:55,943 INFO  [Thread-68] jdbc.TestDriver: INFO  : Status: Running (Executing on YARN cluster with App id application_1674591732462_0001)
2023-01-24T20:37:55,943 INFO  [Thread-68] jdbc.TestDriver: 
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: ERROR : Status: Failed
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: ERROR : Vertex failed, vertexName=Map 1, vertexId=vertex_1674591732462_0001_65_00, diagnostics=[Task failed, taskId=task_1674591732462_0001_65_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1674591732462_0001_65_00_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:164)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:704)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:663)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:543)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:189)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	... 15 more
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:470)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	... 26 more
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:150)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.RecordReaderWrapper.create(RecordReaderWrapper.java:72)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:461)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	... 27 more
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.convertTypeInfo(OrcInputFormat.java:2586)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.typeDescriptionsFromHiveTypeProperty(OrcInputFormat.java:2533)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getDesiredRowTypeDescr(OrcInputFormat.java:2656)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.<init>(LlapRecordReader.java:178)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.create(LlapRecordReader.java:135)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:122)
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: 	... 29 more
2023-01-24T20:37:56,409 INFO  [Thread-68] jdbc.TestDriver: ], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1674591732462_0001_65_00_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:164)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:704)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:663)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:543)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:189)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	... 15 more
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:470)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	... 26 more
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:150)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.RecordReaderWrapper.create(RecordReaderWrapper.java:72)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:461)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	... 27 more
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.convertTypeInfo(OrcInputFormat.java:2586)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.typeDescriptionsFromHiveTypeProperty(OrcInputFormat.java:2533)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getDesiredRowTypeDescr(OrcInputFormat.java:2656)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.<init>(LlapRecordReader.java:178)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.create(LlapRecordReader.java:135)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:122)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	... 29 more
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: ], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1674591732462_0001_65_00_000000_2:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:164)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:704)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:663)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:543)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:189)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	... 15 more
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
2023-01-24T20:37:56,410 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:470)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	... 26 more
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:150)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.RecordReaderWrapper.create(RecordReaderWrapper.java:72)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:461)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	... 27 more
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.convertTypeInfo(OrcInputFormat.java:2586)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.typeDescriptionsFromHiveTypeProperty(OrcInputFormat.java:2533)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getDesiredRowTypeDescr(OrcInputFormat.java:2656)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.<init>(LlapRecordReader.java:178)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.create(LlapRecordReader.java:135)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:122)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	... 29 more
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: ], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1674591732462_0001_65_00_000000_3:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:164)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:704)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:663)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:543)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:189)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	... 15 more
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:470)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	... 26 more
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:150)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.RecordReaderWrapper.create(RecordReaderWrapper.java:72)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:461)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	... 27 more
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.convertTypeInfo(OrcInputFormat.java:2586)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.typeDescriptionsFromHiveTypeProperty(OrcInputFormat.java:2533)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getDesiredRowTypeDescr(OrcInputFormat.java:2656)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.<init>(LlapRecordReader.java:178)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.create(LlapRecordReader.java:135)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:122)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	... 29 more
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: ]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1674591732462_0001_65_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: ERROR : Vertex killed, vertexName=Reducer 2, vertexId=vertex_1674591732462_0001_65_01, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to OTHER_VERTEX_FAILURE, failedTasks:0 killedTasks:1, Vertex vertex_1674591732462_0001_65_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: ERROR : DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: INFO  : org.apache.tez.common.counters.DAGCounter:
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: INFO  :    NUM_FAILED_TASKS: 4
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: INFO  :    TOTAL_LAUNCHED_TASKS: 4
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: INFO  :    AM_CPU_MILLISECONDS: 1010
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: INFO  :    AM_GC_TIME_MILLIS: 8
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: INFO  : org.apache.hadoop.hive.ql.exec.tez.HiveInputCounters:
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: INFO  :    GROUPED_INPUT_SPLITS_Map_1: 1
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: INFO  :    INPUT_DIRECTORIES_Map_1: 1
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: INFO  :    INPUT_FILES_Map_1: 2
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: INFO  :    RAW_INPUT_SPLITS_Map_1: 2
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: ERROR : FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1674591732462_0001_65_00, diagnostics=[Task failed, taskId=task_1674591732462_0001_65_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1674591732462_0001_65_00_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
2023-01-24T20:37:56,411 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:164)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:704)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:663)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:543)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:189)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	... 15 more
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:470)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	... 26 more
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:150)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.RecordReaderWrapper.create(RecordReaderWrapper.java:72)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:461)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	... 27 more
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.convertTypeInfo(OrcInputFormat.java:2586)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.typeDescriptionsFromHiveTypeProperty(OrcInputFormat.java:2533)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getDesiredRowTypeDescr(OrcInputFormat.java:2656)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.<init>(LlapRecordReader.java:178)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.create(LlapRecordReader.java:135)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:122)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	... 29 more
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: ], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1674591732462_0001_65_00_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:164)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:704)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:663)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:543)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:189)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	... 15 more
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:470)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	... 26 more
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:150)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.RecordReaderWrapper.create(RecordReaderWrapper.java:72)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:461)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	... 27 more
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.convertTypeInfo(OrcInputFormat.java:2586)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.typeDescriptionsFromHiveTypeProperty(OrcInputFormat.java:2533)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getDesiredRowTypeDescr(OrcInputFormat.java:2656)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.<init>(LlapRecordReader.java:178)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.create(LlapRecordReader.java:135)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:122)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	... 29 more
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: ], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1674591732462_0001_65_00_000000_2:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
2023-01-24T20:37:56,412 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:164)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:704)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:663)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:543)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:189)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	... 15 more
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:470)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	... 26 more
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:150)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.RecordReaderWrapper.create(RecordReaderWrapper.java:72)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:461)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	... 27 more
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.convertTypeInfo(OrcInputFormat.java:2586)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.typeDescriptionsFromHiveTypeProperty(OrcInputFormat.java:2533)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getDesiredRowTypeDescr(OrcInputFormat.java:2656)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.<init>(LlapRecordReader.java:178)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.create(LlapRecordReader.java:135)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:122)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	... 29 more
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: ], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1674591732462_0001_65_00_000000_3:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.security.AccessController.doPrivileged(Native Method)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at java.base/java.lang.Thread.run(Thread.java:829)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:164)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:704)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:663)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:543)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:189)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	... 15 more
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:470)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	... 26 more
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:150)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.RecordReaderWrapper.create(RecordReaderWrapper.java:72)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:461)
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	... 27 more
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: Caused by: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
2023-01-24T20:37:56,413 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.convertTypeInfo(OrcInputFormat.java:2586)
2023-01-24T20:37:56,414 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.typeDescriptionsFromHiveTypeProperty(OrcInputFormat.java:2533)
2023-01-24T20:37:56,414 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getDesiredRowTypeDescr(OrcInputFormat.java:2656)
2023-01-24T20:37:56,414 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.<init>(LlapRecordReader.java:178)
2023-01-24T20:37:56,414 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.create(LlapRecordReader.java:135)
2023-01-24T20:37:56,414 INFO  [Thread-68] jdbc.TestDriver: 	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:122)
2023-01-24T20:37:56,414 INFO  [Thread-68] jdbc.TestDriver: 	... 29 more
2023-01-24T20:37:56,414 INFO  [Thread-68] jdbc.TestDriver: ]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1674591732462_0001_65_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1674591732462_0001_65_01, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to OTHER_VERTEX_FAILURE, failedTasks:0 killedTasks:1, Vertex vertex_1674591732462_0001_65_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
2023-01-24T20:37:56,414 INFO  [Thread-68] jdbc.TestDriver: INFO  : Completed executing command(queryId=hive_20230124203755_a1e9f54e-09b8-48e3-9966-827c66f717cb); Time taken: 0.818 seconds
2023-01-24T20:37:56,414 INFO  [Thread-68] jdbc.TestDriver: INFO  : OK
2023-01-24T20:37:56,526 ERROR [pool-9-thread-1] jdbc.TestDriver: Error while compiling statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1674591732462_0001_65_00, diagnostics=[Task failed, taskId=task_1674591732462_0001_65_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1674591732462_0001_65_00_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:164)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)
	at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:704)
	at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:663)
	at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)
	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:543)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:189)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
	... 15 more
Caused by: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:470)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)
	... 26 more
Caused by: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:150)
	at org.apache.hadoop.hive.ql.io.RecordReaderWrapper.create(RecordReaderWrapper.java:72)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:461)
	... 27 more
Caused by: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.convertTypeInfo(OrcInputFormat.java:2586)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.typeDescriptionsFromHiveTypeProperty(OrcInputFormat.java:2533)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getDesiredRowTypeDescr(OrcInputFormat.java:2656)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.<init>(LlapRecordReader.java:178)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.create(LlapRecordReader.java:135)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:122)
	... 29 more
], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1674591732462_0001_65_00_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:164)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)
	at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:704)
	at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:663)
	at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)
	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:543)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:189)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
	... 15 more
Caused by: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:470)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)
	... 26 more
Caused by: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:150)
	at org.apache.hadoop.hive.ql.io.RecordReaderWrapper.create(RecordReaderWrapper.java:72)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:461)
	... 27 more
Caused by: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.convertTypeInfo(OrcInputFormat.java:2586)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.typeDescriptionsFromHiveTypeProperty(OrcInputFormat.java:2533)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getDesiredRowTypeDescr(OrcInputFormat.java:2656)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.<init>(LlapRecordReader.java:178)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.create(LlapRecordReader.java:135)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:122)
	... 29 more
], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1674591732462_0001_65_00_000000_2:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:164)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)
	at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:704)
	at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:663)
	at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)
	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:543)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:189)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
	... 15 more
Caused by: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:470)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)
	... 26 more
Caused by: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:150)
	at org.apache.hadoop.hive.ql.io.RecordReaderWrapper.create(RecordReaderWrapper.java:72)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:461)
	... 27 more
Caused by: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.convertTypeInfo(OrcInputFormat.java:2586)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.typeDescriptionsFromHiveTypeProperty(OrcInputFormat.java:2533)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getDesiredRowTypeDescr(OrcInputFormat.java:2656)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.<init>(LlapRecordReader.java:178)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.create(LlapRecordReader.java:135)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:122)
	... 29 more
], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1674591732462_0001_65_00_000000_3:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:164)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)
	at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:704)
	at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:663)
	at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)
	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:543)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:189)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
	... 15 more
Caused by: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:470)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)
	... 26 more
Caused by: java.io.IOException: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:150)
	at org.apache.hadoop.hive.ql.io.RecordReaderWrapper.create(RecordReaderWrapper.java:72)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:461)
	... 27 more
Caused by: java.lang.IllegalArgumentException: ORC doesn't handle primitive category TIMESTAMPLOCALTZ
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.convertTypeInfo(OrcInputFormat.java:2586)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.typeDescriptionsFromHiveTypeProperty(OrcInputFormat.java:2533)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getDesiredRowTypeDescr(OrcInputFormat.java:2656)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.<init>(LlapRecordReader.java:178)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.create(LlapRecordReader.java:135)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.getRecordReader(LlapInputFormat.java:122)
	... 29 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1674591732462_0001_65_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1674591732462_0001_65_01, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to OTHER_VERTEX_FAILURE, failedTasks:0 killedTasks:1, Vertex vertex_1674591732462_0001_65_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
2023-01-24T20:37:56,558 INFO  [pool-2-thread-1] jdbc.TestDriver: Ending Test at 2023-01-24 20:37:56,557
2023-01-24T20:37:56,558 INFO  [pool-2-thread-1] jdbc.TestDriver: TEST FAILED in 7 seconds.
2023-01-24T20:37:56,558 INFO  [pool-2-thread-1] jdbc.TestDriver: Annotations: [regression] {code}
 

SQL Test
{code:java}
>>> create table t1_default (
   t tinyint default 1Y,
   si smallint default 1S,
   i int default 1,
   b bigint default 1L,
    f double default double(5.7),
    d double,
    s varchar(25) default cast('col1' as varchar(25)),
    dc decimal(38,18),
    bo varchar(5),
    v varchar(25),
    c char(25) default cast('var1' as char(25)),
    ts timestamp DEFAULT TIMESTAMP'2016-02-22 12:45:07.000000000',
    dt date default cast('2015-03-12' as DATE),
    tz timestamp with local time zone DEFAULT TIMESTAMPLOCALTZ'2016-01-03 12:26:34 America/Los_Angeles')
    STORED AS TEXTFILE;

>>> insert into t1_default(t,si) values (2,5);
>>> insert into t1_default(b,dt) values (2,cast('2019-08-14' as DATE));
>>> select * from t1_default ORDER BY t1_default.t;
{code}",zratkai,dharmikt,Major,Resolved,Fixed,25/Jan/23 08:05,11/May/23 11:09
Bug,HIVE-26983,13521249,"404 Error when clicking the ""Getting Started Guide"" link in the website",[https://hive.apache.org/GettingStarted]  When we click this page then we are getting 404 not found page. Need to check and fix the issue.,maheshrajus,maheshrajus,Minor,Resolved,Fixed,25/Jan/23 09:21,16/Jun/23 10:04
Bug,HIVE-26988,13521539,Broken website links when opening results from search engines,"1. Some of the links are broken. 

2. The search engine has cached a few pages - 

Example: search engine points to [https://hive.apache.org/mailing_lists.html] but this page is moved to [https://hive.apache.org//community/mailinglists/] .

 ",simhadri-g,simhadri-g,Major,Resolved,Fixed,25/Jan/23 17:23,16/Jun/23 10:04
Bug,HIVE-26992,13521620,"Select count(*) query fails with error ""java.lang.ArrayIndexOutOfBoundsException: arraycopy: last source index 1024 out of bounds for int[256]""","{{java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:101)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:76)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:437)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
	... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:970)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:92)
	... 18 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: arraycopy: last source index 1024 out of bounds for int[256]
	at java.base/java.lang.System.arraycopy(Native Method)
	at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:116)
	at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:968)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:172)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.deliverVectorizedRowBatch(VectorMapOperator.java:809)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:842)
	... 19 more
], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1674594639075_0001_39_00_000002_1:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:101)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:76)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:437)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
	... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:970)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:92)
	... 18 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: arraycopy: last source index 1024 out of bounds for int[256]
	at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:116)
	at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:968)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:172)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.deliverVectorizedRowBatch(VectorMapOperator.java:809)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:842)
	... 19 more
], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1674594639075_0001_39_00_000002_2:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:101)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:76)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:437)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
	... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:970)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:92)
	... 18 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: arraycopy: last source index 1024 out of bounds for int[256]
	at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:116)
	at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:968)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:172)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.deliverVectorizedRowBatch(VectorMapOperator.java:809)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:842)
	... 19 more
], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1674594639075_0001_39_00_000002_3:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:101)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:76)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:437)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
	... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:970)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:92)
	... 18 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: arraycopy: last source index 1024 out of bounds for int[256]
	at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:116)
	at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:968)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:172)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.deliverVectorizedRowBatch(VectorMapOperator.java:809)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:842)
	... 19 more}}",InvisibleProgrammer,InvisibleProgrammer,Major,Resolved,Fixed,26/Jan/23 14:36,14/Feb/23 10:32
Bug,HIVE-27004,13522162,DateTimeFormatterBuilder cannot parse 'UTC+' in Java versions higher than 8,"Some of the unit tests related to _DateTimeFormatter_ were failing in Java versions greater than 8 while working in Java 8.

Example of a failing Unit Test : _org.apache.hadoop.hive.common.type.TestTimestampTZ#testComparision_

 
{code:java}
java.time.format.DateTimeParseException: Text '2017-04-14 18:00:00 UTC+08:00' could not be parsed, unparsed text found at index 23 at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049) at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874) at org.apache.hadoop.hive.common.type.TimestampTZUtil.parse(TimestampTZUtil.java:76) at org.apache.hadoop.hive.common.type.TimestampTZUtil.parse(TimestampTZUtil.java:64) at org.apache.hadoop.hive.common.type.TestTimestampTZ.testComparison(TestTimestampTZ.java:44) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method){code}
 

The {{appendZoneText(TextStyle)}} method of [DateTimeFormatteBuilder|https://github.com/apache/hive/blob/master/common/src/java/org/apache/hadoop/hive/common/type/TimestampTZUtil.java#L82] is not able to parse the {{+}} symbol in {{2017-04-14 18:00:00 UTC+08:00}} when running [the test|https://github.com/apache/hive/blob/master/common/src/test/org/apache/hadoop/hive/common/type/TestTimestampTZ.java#L37] in Java 11 , while it is working fine with Java 8.

According to the [doc|https://developer.android.com/reference/java/time/format/DateTimeFormatterBuilder#appendZoneText(java.time.format.TextStyle)] , {{appendZoneText(TextStyle)}} should be able to parse {{either the textual zone name, the zone ID or the offset}} and UTC+08:00 should come under offset as per the same [doc|https://developer.android.com/reference/java/time/format/DateTimeFormatterBuilder#appendOffset(java.lang.String,%20java.lang.String)].

It seems, however, that ""UTC"" was explicitly removed [when parsed as ZoneText for higher Java versions|https://github.com/openjdk/jdk/commit/5c3a01591c5c945926636fdc9f164d60b5b4f29e?diff=unified#diff-5fcf976db1c06e8f44a8671356d7e34fdfbf5b057baa852e7c3e015c8797c889R4263]

As a workaround, we can use {{appendZoneOrOffsetId()}} rather than {{appendZoneText().}}

This ensures the tests are passing for Java 8+ and based on my testing, I didn’t see any regression of the change.

Sample repro code - jdoodle.com/ia/D5e

 ",AnmolSun,AnmolSun,Minor,Resolved,Fixed,31/Jan/23 05:25,16/Jun/23 10:04
Bug,HIVE-27029,13523176,hive query fails with Filesystem closed error,"This Jira is raised to modify/fix the code which is done in part of *HIVE-26352.*

 

we should remove the finally block as this is causing the filesystem close errors.
{code:java}
String queueName, String userName) throws IOException, InterruptedException {
    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
    try {
        ugi.doAs((PrivilegedExceptionAction < Void > )() - >            {
                checkQueueAccessInternal(queueName, userName);
                return null;
            }        );
    } finally {
        {
            try {
                FileSystem.closeAllForUGI(ugi);
            } catch (IOException exception)            {
                LOG.error(""Could not clean up file-system handles for UGI: "" + ugi, exception);
            }        }
    }
} {code}
 

Caused by: java.io.IOException: Filesystem closed
at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:483) ~[hadoop-hdfs-client-3.1.1.7.1.8.11-3.jar:?]
at org.apache.hadoop.hdfs.DFSClient.getEZForPath(DFSClient.java:2771) ~[hadoop-hdfs-client-3.1.1.7.1.8.11-3.jar:?]
at org.apache.hadoop.hdfs.DistributedFileSystem$54.doCall(DistributedFileSystem.java:2796) ~[hadoop-hdfs-client-3.1.1.7.1.8.11-3.jar:?]
at org.apache.hadoop.hdfs.DistributedFileSystem$54.doCall(DistributedFileSystem.java:2793) ~[hadoop-hdfs-client-3.1.1.7.1.8.11-3.jar:?]
at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-3.1.1.7.1.8.11-3.jar:?]
at org.apache.hadoop.hdfs.DistributedFileSystem.getEZForPath(DistributedFileSystem.java:2812) ~[hadoop-hdfs-client-3.1.1.7.1.8.11-3.jar:?]
at org.apache.hadoop.hdfs.client.HdfsAdmin.getEncryptionZoneForPath(HdfsAdmin.java:374) ~[hadoop-hdfs-client-3.1.1.7.1.8.11-3.jar:?]
at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.getEncryptionZoneForPath(Hadoop23Shims.java:1384) ~[hive-exec-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1379) ~[hive-exec-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isPathEncrypted(SemanticAnalyzer.java:2484) ~[hive-exec-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
 

 ",maheshrajus,maheshrajus,Major,Resolved,Fixed,06/Feb/23 08:03,16/Jun/23 10:04
Bug,HIVE-27060,13523758,Exception in add partitions with SQL Server when number of parameters exceed 2100,"*[Description]* 

Add partitions with SQL Server db  throws {{SQLServerException}} when the number of parameters in the direct sql insert query exceed 2100.

*[Steps to reproduce]* 

Create stage table, load data that has 1000+ unique rows into stage table, create partition table and load data into the table from the stage table. data file is attached below.
{code:java}
0: jdbc:hive2://localhost:10000> create database mydb;
0: jdbc:hive2://localhost:10000> use mydb;
 
0: jdbc:hive2://localhost:10000> create table stage(sr int, st string, name string) row format delimited fields terminated by '\t' stored as textfile;
 
0: jdbc:hive2://localhost:10000> load data local inpath 'partdata1001' into table stage;
 
0: jdbc:hive2://localhost:10000> create table dynpart(num int, name string) partitioned by (category string) row format delimited fields terminated by '\t' stored as textfile;
 
0: jdbc:hive2://localhost:10000> insert into dynpart select * from stage;{code}
{{*It throws com.microsoft.sqlserver.jdbc.SQLServerException: The incoming request has too many parameters. The server supports a maximum of 2100 parameters. Reduce the number of parameters and resend the request.*}}

{{*[Exception Stack]*}}
{code:java}
javax.jdo.JDODataStoreException: Error executing SQL query ""insert into ""SERDES"" (""SERDE_ID"",""DESCRIPTION"",""DESERIALIZER_CLASS"",""NAME"",""SERDE_TYPE"",""SLIB"",""SERIALIZER_CLASS"") values (?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?),(?,?,?,?,?,?,?)"".
    at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:542) ~[datanucleus-api-jdo-5.2.8.jar:?]
    at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:456) ~[datanucleus-api-jdo-5.2.8.jar:?]
    at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:318) ~[datanucleus-api-jdo-5.2.8.jar:?]
    at org.apache.hadoop.hive.metastore.MetastoreDirectSqlUtils.executeWithArray(MetastoreDirectSqlUtils.java:69) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.MetastoreDirectSqlUtils.executeWithArray(MetastoreDirectSqlUtils.java:61) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.DirectSqlInsertPart.executeQuery(DirectSqlInsertPart.java:105) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.DirectSqlInsertPart.access$000(DirectSqlInsertPart.java:53) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.DirectSqlInsertPart$1.execute(DirectSqlInsertPart.java:131) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.DirectSqlInsertPart.insertInBatch(DirectSqlInsertPart.java:95) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.DirectSqlInsertPart.insertSerdeInBatch(DirectSqlInsertPart.java:134) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.DirectSqlInsertPart.addPartitions(DirectSqlInsertPart.java:808) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.addPartitions(MetaStoreDirectSql.java:534) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.ObjectStore$6.getSqlResult(ObjectStore.java:2647) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.ObjectStore$6.getSqlResult(ObjectStore.java:2644) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:4382) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.ObjectStore.addPartitions(ObjectStore.java:2667) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
    at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at com.sun.proxy.$Proxy29.addPartitions(Unknown Source) ~[?:?]
    at org.apache.hadoop.hive.metastore.HMSHandler.add_partitions_core(HMSHandler.java:4146) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.HMSHandler.add_partitions_req(HMSHandler.java:4408) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:146) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at com.sun.proxy.$Proxy31.add_partitions_req(Unknown Source) ~[?:?]
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.add_partitions(HiveMetaStoreClient.java:1111) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.add_partitions(SessionHiveMetaStoreClient.java:1083) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.add_partitions(SessionHiveMetaStoreClient.java:1029) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at com.sun.proxy.$Proxy32.add_partitions(Unknown Source) ~[?:?]
    at org.apache.hadoop.hive.metastore.SynchronizedMetaStoreClient.add_partitions(SynchronizedMetaStoreClient.java:85) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.metadata.Hive.addPartitionsToMetastore(Hive.java:2803) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.metadata.Hive.loadDynamicPartitions(Hive.java:3213) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.exec.MoveTask.handleDynParts(MoveTask.java:650) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:498) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:354) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:327) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:244) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:105) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:370) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:205) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:154) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:149) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:185) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:236) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:90) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:340) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_292]
    at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_292]
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) ~[hadoop-common-3.1.0.jar:?]
    at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:360) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_292]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_292]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_292]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_292]
    at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292]
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: The incoming request has too many parameters. The server supports a maximum of 2100 parameters. Reduce the number of parameters and resend the request.
    at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:258) ~[mssql-jdbc-6.2.1.jre8.jar:?]
    at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1535) ~[mssql-jdbc-6.2.1.jre8.jar:?]
    at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:467) ~[mssql-jdbc-6.2.1.jre8.jar:?]
    at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:409) ~[mssql-jdbc-6.2.1.jre8.jar:?]
    at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7151) ~[mssql-jdbc-6.2.1.jre8.jar:?]
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:2478) ~[mssql-jdbc-6.2.1.jre8.jar:?]
    at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:219) ~[mssql-jdbc-6.2.1.jre8.jar:?]
    at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:199) ~[mssql-jdbc-6.2.1.jre8.jar:?]
    at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeUpdate(SQLServerPreparedStatement.java:356) ~[mssql-jdbc-6.2.1.jre8.jar:?]
    at org.apache.hive.com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hive.com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeUpdate(HikariProxyPreparedStatement.java) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeUpdate(ParamLoggingPreparedStatement.java:393) ~[datanucleus-rdbms-5.2.10.jar:?]
    at org.datanucleus.store.rdbms.SQLController.executeStatementUpdate(SQLController.java:435) ~[datanucleus-rdbms-5.2.10.jar:?]
    at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:622) ~[datanucleus-rdbms-5.2.10.jar:?]
    at org.datanucleus.store.query.Query.executeQuery(Query.java:1975) ~[datanucleus-core-5.2.10.jar:?]
    at org.datanucleus.store.rdbms.query.SQLQuery.executeWithArray(SQLQuery.java:818) ~[datanucleus-rdbms-5.2.10.jar:?]
    at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:433) ~[datanucleus-api-jdo-5.2.8.jar:?]
    ... 66 more {code}",VenuReddy,VenuReddy,Major,Resolved,Fixed,08/Feb/23 11:35,16/Jun/23 10:04
Bug,HIVE-27061,13523947,Website deployment GitHub action should not trigger on pull requests,"The Website deployment GitHub action configured here:

[https://github.com/apache/hive-site/blob/a3132faf0f4a555434076cb8ad690ae2c2c8c371/.github/workflows/gh-pages.yml]

should not trigger on pull requests.

The issue can be seen here:

https://github.com/apache/hive-site/actions/runs/4127993132/jobs/7131893178

where the action was launched for https://github.com/apache/hive-site/pull/1",zabetak,zabetak,Major,Resolved,Fixed,09/Feb/23 10:50,16/Jun/23 10:04
Bug,HIVE-27063,13524082,LDAP+JWT auth forms not supported,"In HIVE-25875, support for multiple authentication forms was added for Hive Server. In HIVE-25575, support for JWT authentication was added. However, setting hive.server2.authentication=""JWT,LDAP"" will fail with the following validation error.


{noformat}
<12>1 2023-02-03T09:32:11.018Z hiveserver2-0 hiveserver2 1 0393cf91-48f7-49e3-b2b1-b983000d4cd6 [mdc@18060 class=""server.HiveServer2"" level=""WARN"" thread=""main""] Error starting HiveServer2 on attempt 2, will retry in 60000ms\rorg.apache.hive.service.ServiceException: Failed to Start HiveServer2
at org.apache.hive.service.CompositeService.start(CompositeService.java:80) 
at org.apache.hive.service.server.HiveServer2.start(HiveServer2.java:692) 
at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:1154) 
at org.apache.hive.service.server.HiveServer2.access$1400(HiveServer2.java:145) 
at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:1503) 
at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:1316) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r at org.apache.hadoop.util.RunJar.run(RunJar.java:318) 
at org.apache.hadoop.util.RunJar.main(RunJar.java:232)\rCaused by: java.lang.RuntimeException: Failed to init HttpServer 
at org.apache.hive.service.cli.thrift.ThriftHttpCLIService.initServer(ThriftHttpCLIService.java:239) 
at org.apache.hive.service.cli.thrift.ThriftCLIService.start(ThriftCLIService.java:235) 
at org.apache.hive.service.CompositeService.start(CompositeService.java:70) ... 11 more
Caused by: java.lang.Exception: The authentication types have conflicts: LDAP,JWT 
at org.apache.hive.service.auth.AuthType.verifyTypes(AuthType.java:69) 
at org.apache.hive.service.auth.AuthType.<init>(AuthType.java:43) 
org.apache.hive.service.cli.thrift.ThriftHttpServlet.<init>(ThriftHttpServlet.java:124) 
at org.apache.hive.service.cli.thrift.ThriftHttpCLIService.initServer(ThriftHttpCLIService.java:197) ... 13 more
{noformat}

We never fixed the AuthType.validateTypes() to support this.",ngangam,ngangam,Major,Resolved,Fixed,09/Feb/23 20:29,16/Jun/23 10:04
Bug,HIVE-27065,13524121,Exception in partition column statistics update with SQL Server db when histogram statistics is not enabled,"*[Description]* 

java.sql.BatchUpdateException thrown from insertIntoPartColStatTable() with SQL Server db when histogram statistics is not enabled.

*java.sql.BatchUpdateException: Implicit conversion from data type varchar to varbinary(max) is not allowed. Use the CONVERT function to run this query.*

 

*[Steps to reproduce]* 

Create stage table, load data into stage table, create partition table and load data into the table from the stage table.
{code:java}
0: jdbc:hive2://localhost:10000> create database mydb;
0: jdbc:hive2://localhost:10000> use mydb;
 
0: jdbc:hive2://localhost:10000> create table stage(sr int, st string, name string) row format delimited fields terminated by '\t' stored as textfile;
 
0: jdbc:hive2://localhost:10000> load data local inpath 'partdata' into table stage;
 
0: jdbc:hive2://localhost:10000> create table dynpart(num int, name string) partitioned by (category string) row format delimited fields terminated by '\t' stored as textfile;
 
0: jdbc:hive2://localhost:10000> insert into dynpart select * from stage; {code}
 

*[Exception Stack]*
{code:java}
2023-02-10T05:16:42,921 ERROR [HiveServer2-Background-Pool: Thread-112] metastore.DirectSqlUpdateStat: Unable to update Column stats for  dynpart
java.sql.BatchUpdateException: Implicit conversion from data type varchar to varbinary(max) is not allowed. Use the CONVERT function to run this query.
    at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2303) ~[mssql-jdbc-6.2.1.jre8.jar:?]
    at org.apache.hive.com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:127) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hive.com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.DirectSqlUpdateStat.insertIntoPartColStatTable(DirectSqlUpdateStat.java:281) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.DirectSqlUpdateStat.updatePartitionColumnStatistics(DirectSqlUpdateStat.java:612) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.updatePartitionColumnStatisticsBatch(MetaStoreDirectSql.java:3063) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.ObjectStore.updatePartitionColumnStatisticsInBatch(ObjectStore.java:9943) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
    at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at com.sun.proxy.$Proxy29.updatePartitionColumnStatisticsInBatch(Unknown Source) ~[?:?]
    at org.apache.hadoop.hive.metastore.HMSHandler.updatePartitionColStatsForOneBatch(HMSHandler.java:7068) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.HMSHandler.updatePartitionColStatsInBatch(HMSHandler.java:7121) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.HMSHandler.updatePartColumnStatsWithMerge(HMSHandler.java:9247) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.HMSHandler.set_aggr_stats_for(HMSHandler.java:9149) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:146) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at com.sun.proxy.$Proxy31.set_aggr_stats_for(Unknown Source) ~[?:?]
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.setPartitionColumnStatistics(HiveMetaStoreClient.java:3307) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.setPartitionColumnStatistics(SessionHiveMetaStoreClient.java:566) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at com.sun.proxy.$Proxy32.setPartitionColumnStatistics(Unknown Source) ~[?:?]
    at org.apache.hadoop.hive.ql.metadata.Hive.setPartitionColumnStatistics(Hive.java:5677) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.stats.ColStatsProcessor.persistColumnStats(ColStatsProcessor.java:221) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.stats.ColStatsProcessor.process(ColStatsProcessor.java:94) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.exec.StatsTask.execute(StatsTask.java:107) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:354) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:327) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:244) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:105) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:370) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:205) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:154) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:149) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:185) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:236) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:90) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:340) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_292]
    at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_292]
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) ~[hadoop-common-3.1.0.jar:?]
    at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:360) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_292]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_292]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_292]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_292]
    at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292] {code}
 ",VenuReddy,VenuReddy,Major,Resolved,Fixed,10/Feb/23 00:15,16/Jun/23 10:04
Bug,HIVE-27066,13524138,Fix mv_iceberg_partitioned_orc and mv_iceberg_partitioned_orc2,"These tests are flaky due to Total Size in the query output.
The test is recently committed, and is around MV's in iceberg. It missed 


{noformat}
--! qt:replace:/(\s+totalSize\s+)\S+(\s+)/$1#Masked#$2/ 
{noformat}

Which is done in most of the tests adding totalSize in the query output to avoid flakiness

 ",,ayushtkn,Minor,Resolved,Fixed,10/Feb/23 02:42,15/Feb/23 14:49
Bug,HIVE-27069,13524263,Incorrect results with bucket map join,"Attaching test.q file for repro.

Following query in testcase is expected to produce 9 records, but it is producing 5 records.
{code:java}
select * from DUP_TEST_TARGET T join (SELECT id , in_date , sample FROM (SELECT id , in_date , sample ,ROW_NUMBER()
OVER(PARTITION BY id ORDER BY in_date DESC ) AS ROW_NUMB  FROM DUP_TEST) OUTQUERY WHERE ROW_NUMB =1) as S ON T.id = S.id;{code}",Dayakar,nareshpr,Critical,Resolved,Fixed,10/Feb/23 17:27,21/Apr/23 14:24
Bug,HIVE-27071,13524333,"Select query with LIMIT clause can fail if there are marker files like ""_SUCCESS"" and ""_MANIFEST""","Spark clients creates marker files like ""_SUCCESS"" and ""_MANIFEST"" under the table/partition path at the end of a write operation. For example 'hdfs://name-node-host/table/partition/_SUCCESS'
Whenever Hive is trying to read that table with the LIMIT clause, it could to the following error:
{code:java}
ERROR : Vertex failed, vertexName=Map 1, vertexId=vertex_1676095298574_0017_2_00, diagnostics=[Vertex vertex_1676095298574_0017_2_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: trade initializer failed, vertex=vertex_1676095298574_0017_2_00 [Map 1], org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://name-node-host/table/partition/_MANIFEST
Input path does not exist: hdfs://name-node-host/table/partition/_SUCCESS at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:300)
at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:240)
at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:328)
at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:579) {code}
Hive execution engine should ignore these marker files while reading the table/partition data.",ayushtkn,hemanth619,Major,Resolved,Fixed,11/Feb/23 20:17,16/Jun/23 10:04
Bug,HIVE-27085,13524976,Revert Manual constructor from AbortCompactionResponseElement,,rkirtir,rkirtir,Major,Resolved,Fixed,16/Feb/23 08:47,16/Jun/23 10:04
Bug,HIVE-27088,13525138,Incorrect results when inner and outer joins with post join filters are merged,"When hive.merge.nway.joins is set to `true` and JOIN is performed with INNER and OUTER and a filter exists, normal results cannot be obtained.

For example:
{code:java}
-- Data preparation
create temporary table foo (id bigint, code string) stored as orc;
create temporary table bar (id bigint, code string) stored as orc;
create temporary table baz (id bigint) stored as orc;
INSERT INTO foo values
  (29999000052073, '01'),
  (29999000052107, '01'),
  (29999000052111, '01'),
  (29999000052112, '01'),
  (29999000052113, '01'),
  (29999000052114, '01'),
  (29999000052071, '01A'),
  (29999000052072, '01A'),
  (29999000052116, '01A'),
  (29999000052117, '01A'),
  (29999000052118, '01A'),
  (29999000052119, '01A'),
  (29999000052120, '01A'),
  (29999000052076, '06'),
  (29999000052074, '06A'),
  (29999000052075, '06A');INSERT INTO bar values
  (29999000052071, '01'),
  (29999000052072, '01'),
  (29999000052073, '01'),
  (29999000052116, '01'),
  (29999000052117, '01'),
  (29999000052071, '01A'),
  (29999000052072, '01A'),
  (29999000052073, '01A'),
  (29999000052116, '01AS'),
  (29999000052117, '01AS'),
  (29999000052071, '01B'),
  (29999000052072, '01B'),
  (29999000052073, '01B'),
  (29999000052116, '01BS'),
  (29999000052117, '01BS'),
  (29999000052071, '01C'),
  (29999000052072, '01C'),
  (29999000052073, '01C7'),
  (29999000052116, '01CS'),
  (29999000052117, '01CS'),
  (29999000052071, '01D'),
  (29999000052072, '01D'),
  (29999000052073, '01D'),
  (29999000052116, '01DS'),
  (29999000052117, '01DS');INSERT INTO baz values
  (29999000052071),
  (29999000052072),
  (29999000052073),
  (29999000052074),
  (29999000052075),
  (29999000052076),
  (29999000052107),
  (29999000052111),
  (29999000052112),
  (29999000052113),
  (29999000052114),
  (29999000052116),
  (29999000052117),
  (29999000052118),
  (29999000052119),
  (29999000052120);{code}
Normal works(set hive.merge.nway.joins=false):
{code:java}
hive> set hive.merge.nway.joins=false;
hive> SELECT
  a.id,
  b.code,
  c.id
FROM bar AS a
INNER JOIN foo AS b
ON a.id = b.id
  AND (a.code = '01AS' OR b.code = '01BS')
LEFT OUTER JOIN baz AS c
ON a.id = c.id;

OK
29999000052116  01A     29999000052116
29999000052117  01A     29999000052117 {code}
Abnormal works(set hive.merge.nway.joins=true):
{code:java}
hive> set hive.merge.nway.joins=true;
hive> SELECT
  a.id,
  b.code,
  c.id
FROM bar AS a
INNER JOIN foo AS b
ON a.id = b.id
  AND (a.code = '01AS' OR b.code = '01BS')
LEFT OUTER JOIN baz AS c
ON a.id = c.id;

OK 29999000052071  01A     NULL
29999000052072  01A     NULL
29999000052073  01      NULL
29999000052116  01A     NULL
29999000052117  01A     NULL
29999000052071  01A     NULL
29999000052072  01A     NULL
29999000052073  01      NULL
29999000052116  01A     29999000052116
29999000052117  01A     29999000052117
29999000052071  01A     NULL
29999000052072  01A     NULL
29999000052073  01      NULL
29999000052116  01A     NULL
29999000052117  01A     NULL
29999000052071  01A     NULL
29999000052072  01A     NULL
29999000052073  01      NULL
29999000052116  01A     NULL
29999000052117  01A     NULL
29999000052071  01A     NULL
29999000052072  01A     NULL
29999000052073  01      NULL
29999000052116  01A     NULL
29999000052117  01A     NULL   {code}
 

I think this is also related to the next ticket: https://issues.apache.org/jira/browse/HIVE-21322",ryu_kobayashi,ryu_kobayashi,Major,Resolved,Fixed,17/Feb/23 05:42,23/May/23 08:29
Bug,HIVE-27091,13525173,Add double quotes for tables in PartitionProjectionEvaluator,"When PartitionProjectionEvaluator requests partitions against PostgreSQL, there throws exception:
{noformat}
javax.jdo.JDODataStoreException: Error executing SQL query ""select ""SDS"".""LOCATION"",""PARTITIONS"".""CREATE_TIME"",""SDS"".""SD_ID"",""PARTITIONS"".""PART_ID"" from PARTITIONS left outer join SDS on PARTITIONS.""SD_ID"" = SDS.""SD_ID""   left outer join SERDES on SDS.""SERDE_ID"" = SERDES.""SERDE_ID"" where ""PART_ID"" in (92731,92732,92733,92734,92735,92736) order by ""PART_NAME"" asc"".
…
Caused by: org.postgresql.util.PSQLException: ERROR: relation ""partitions"" does not exist{noformat}",dengzh,dengzh,Major,Resolved,Fixed,17/Feb/23 09:42,16/Jun/23 10:04
Bug,HIVE-27103,13525977,Thrift compilation warnings on drop_dataconnector(),"We see compilation warnings when compiling Impala with hive_metastore.thrift
{code:java}
[WARNING:/home/quanlong/workspace/Impala/toolchain/cdp_components-36109364/hive-3.1.3000.7.2.17.0-75/standalone-metastore/src/main/thrift/hive_metastore.thrift:2397] No field key specified for ifNotExists, resulting protocol may have conflicts or not be backwards compatible!

[WARNING:/home/quanlong/workspace/Impala/toolchain/cdp_components-36109364/hive-3.1.3000.7.2.17.0-75/standalone-metastore/src/main/thrift/hive_metastore.thrift:2397] No field key specified for checkReferences, resulting protocol may have conflicts or not be backwards compatible!
{code}
A code snipper arround line 2397:
{code:java}
2395   void create_dataconnector(1:DataConnector connector) throws(1:AlreadyExistsException o1, 2:InvalidObjectException o2, 3:MetaException o3)
2396   DataConnector get_dataconnector_req(1:GetDataConnectorRequest request) throws(1:NoSuchObjectException o1, 2:MetaException o2)
2397   void drop_dataconnector(1:string name, bool ifNotExists, bool checkReferences) throws(1:NoSuchObjectException o1, 2:InvalidOperationException o2, 3:MetaException o3)
2398   list<string> get_dataconnectors() throws(1:MetaException o1)
{code}
The arguments of drop_dataconnector() should have indexes, but only the first argument 'name' has an index.",stigahuang,stigahuang,Major,Resolved,Fixed,24/Feb/23 02:05,16/Jun/23 10:04
Bug,HIVE-27105,13526113,Querying parquet table with zstd encryption is failing,"Steps to reproduce on the local with upstream or downstream code:
 * Start HiveServer2
 * Start beeline
 * Run
{code:java}
set hive.execution.engine=tez;{code}

 * Run

{code:java}
CREATE TABLE emp(id int, name string, department string, salary float) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS PARQUET TBLPROPERTIES (""parquet.compression""=""zstd"");  {code}

 * Run

{code:java}
Insert into emp VALUES (1, 'some name', 'some dept', 1.15);  {code}

 * Getting
java.lang.NoClassDefFoundError: com/github/luben/zstd/RecyclingBufferPool


Java call stack for the error:
{code:java}
java.lang.NoClassDefFoundError: com/github/luben/zstd/RecyclingBufferPool
E   	at org.apache.parquet.hadoop.codec.ZstandardCodec.createInputStream(ZstandardCodec.java:90)
E   	at org.apache.parquet.hadoop.codec.ZstandardCodec.createInputStream(ZstandardCodec.java:83)
E   	at org.apache.parquet.hadoop.CodecFactory$HeapBytesDecompressor.decompress(CodecFactory.java:111)
E   	at org.apache.parquet.hadoop.ColumnChunkPageReadStore$ColumnChunkPageReader.readDictionaryPage(ColumnChunkPageReadStore.java:236)
E   	at org.apache.hadoop.hive.ql.io.parquet.vector.BaseVectorizedColumnReader.<init>(BaseVectorizedColumnReader.java:137)
E   	at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedPrimitiveColumnReader.<init>(VectorizedPrimitiveColumnReader.java:58)
E   	at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.buildVectorizedParquetReader(VectorizedParquetRecordReader.java:515)
E   	at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:446)
E   	at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:406)
E   	at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.next(VectorizedParquetRecordReader.java:347)
E   	at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.next(VectorizedParquetRecordReader.java:95)
E   	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:376)
E   	at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:82)
E   	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:119)
E   	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:59)
E   	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:151)
E   	at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116)
E   	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68)
E   	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:437)
E   	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
E   	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
E   	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
E   	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
E   	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
E   	at java.security.AccessController.doPrivileged(Native Method)
E   	at javax.security.auth.Subject.doAs(Subject.java:422)
E   	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
E   	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
E   	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
E   	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
E   	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)
E   	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:69)
E   	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)
E   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
E   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
E   	at java.lang.Thread.run(Thread.java:748)
E   Caused by: java.lang.ClassNotFoundException: com.github.luben.zstd.RecyclingBufferPool
E   	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
E   	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
E   	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355)
E   	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
E   	... 36 more
E   , errorMessage=Cannot recover from this error:java.lang.NoClassDefFoundError: com/github/luben/zstd/RecyclingBufferPool
E   	at org.apache.parquet.hadoop.codec.ZstandardCodec.createInputStream(ZstandardCodec.java:90)
E   	at org.apache.parquet.hadoop.codec.ZstandardCodec.createInputStream(ZstandardCodec.java:83)
E   	at org.apache.parquet.hadoop.CodecFactory$HeapBytesDecompressor.decompress(CodecFactory.java:111)
E   	at org.apache.parquet.hadoop.ColumnChunkPageReadStore$ColumnChunkPageReader.readDictionaryPage(ColumnChunkPageReadStore.java:236)
E   	at org.apache.hadoop.hive.ql.io.parquet.vector.BaseVectorizedColumnReader.<init>(BaseVectorizedColumnReader.java:137)
E   	at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedPrimitiveColumnReader.<init>(VectorizedPrimitiveColumnReader.java:58)
E   	at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.buildVectorizedParquetReader(VectorizedParquetRecordReader.java:515)
E   	at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:446)
E   	at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:406)
E   	at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.next(VectorizedParquetRecordReader.java:347)
E   	at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.next(VectorizedParquetRecordReader.java:95)
E   	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:376)
E   	at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:82)
E   	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:119)
E   	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:59)
E   	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:151)
E   	at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116)
E   	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68)
E   	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:437)
E   	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
E   	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
E   	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
E   	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82)
E   	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69)
E   	at java.security.AccessController.doPrivileged(Native Method)
E   	at javax.security.auth.Subject.doAs(Subject.java:422)
E   	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
E   	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69)
E   	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39)
E   	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
E   	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)
E   	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:69)
E   	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)
E   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
E   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
E   	at java.lang.Thread.run(Thread.java:748)
E   Caused by: java.lang.ClassNotFoundException: com.github.luben.zstd.RecyclingBufferPool
E   	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
E   	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
E   	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355)
E   	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
E   	... 36 more{code}

This is what happens during the execution of this insert statement:
 # At some point Hive calls Tez execution on YARN cluster
 # Tez calls the code in Hive-Exec jar. Tez classpath does not have zstd-jni.jar. Tez classpath include Tez jars, Tez lib jars and hive-exec.
 # Hive code calls Parquet shaded in hive-exec.
 # Parquet is failing because it cannot find zstd-jni classes at runtime.",difin,difin,Major,Resolved,Fixed,24/Feb/23 19:22,03/Jul/23 12:19
Bug,HIVE-27116,13526717,HS2 need to send owner info for UDFs in the HivePrivilegeObject for authorization,UDFs (Functions) of HivePrivilegeObject should have owner info for authorization.,hemanth619,hemanth619,Major,Resolved,Fixed,01/Mar/23 16:57,16/Jun/23 10:04
Bug,HIVE-27128,13527712,"Exception ""Can't finish byte read from uncompressed stream DATA position"" when querying ORC table","Exception happening when querying an ORC table:
{code:java}
Caused by: java.io.EOFException: Can't finish byte read from uncompressed stream DATA position: 393216 length: 393216 range: 23 offset: 376832 position: 16384 limit: 16384
	at org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.commonReadByteArrays(TreeReaderFactory.java:1550)
	at org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.readOrcByteArrays(TreeReaderFactory.java:1566)
	at org.apache.orc.impl.TreeReaderFactory$StringDirectTreeReader.nextVector(TreeReaderFactory.java:1662)
	at org.apache.orc.impl.TreeReaderFactory$StringTreeReader.nextVector(TreeReaderFactory.java:1508)
	at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory$StringStreamReader.nextVector(EncodedTreeReaderFactory.java:305)
	at org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.decodeBatch(OrcEncodedDataConsumer.java:196)
	at org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.decodeBatch(OrcEncodedDataConsumer.java:66)
	at org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.consumeData(EncodedDataConsumer.java:122)
	at org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.consumeData(EncodedDataConsumer.java:42)
	at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:608)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:434)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:282)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:279)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:279)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:118)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer$CpuRecordingCallable.call(EncodedDataConsumer.java:88)
	at org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer$CpuRecordingCallable.call(EncodedDataConsumer.java:73) {code}
I created a q-test that reproduces this issue:

[https://github.com/difin/hive/commits/orc_read_err_qtest]

This issue happens in Hive starting from the commit that upgraded ORC version in Hive to ORC 1.6.7.",difin,difin,Critical,Resolved,Fixed,08/Mar/23 23:21,16/Jun/23 10:04
Bug,HIVE-27135,13528230,AcidUtils#getHdfsDirSnapshots() throws FNFE when a directory is removed in HDFS,"AcidUtils#getHdfsDirSnapshots() throws FileNotFoundException when a directory is removed in HDFS while fetching HDFS Snapshots.

Below testcode can be used to reproduce this issue.
{code:java}
 @Test
  public void testShouldNotThrowFNFEWhenHiveStagingDirectoryIsRemovedWhileFetchingHDFSSnapshots() throws Exception {
    MockFileSystem fs = new MockFileSystem(new HiveConf(),
        new MockFile(""mock:/tbl/part1/.hive-staging_dir/-ext-10002"", 500, new byte[0]),
        new MockFile(""mock:/tbl/part2/.hive-staging_dir"", 500, new byte[0]),
        new MockFile(""mock:/tbl/part1/_tmp_space.db"", 500, new byte[0]),
        new MockFile(""mock:/tbl/part1/delta_1_1/bucket-0000-0000"", 500, new byte[0]));
    Path path = new MockPath(fs, ""/tbl"");
    Path stageDir = new MockPath(fs, ""mock:/tbl/part1/.hive-staging_dir"");
    FileSystem mockFs = spy(fs);
    Mockito.doThrow(new FileNotFoundException("""")).when(mockFs).listLocatedStatus(eq(stageDir));
    try {
      Map<Path, AcidUtils.HdfsDirSnapshot> hdfsDirSnapshots = AcidUtils.getHdfsDirSnapshots(mockFs, path);
      Assert.assertEquals(1, hdfsDirSnapshots.size());
    }
    catch (FileNotFoundException fnf) {
      fail(""Should not throw FileNotFoundException when a directory is removed while fetching HDFSSnapshots"");
    }
  }{code}
This issue got fixed as a part of HIVE-26481 but here its not fixed completely. [Here|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java#L1541] FileUtils.listFiles() API which returns a RemoteIterator<LocatedFileStatus>. So while iterating over, it checks if it is a directory and recursive listing then it will try to list files from that directory but if that directory is removed by other thread/task then it throws FileNotFoundException. Here the directory which got removed is the .staging directory which needs to be excluded through by using passed filter.

 

So here we can use same logic written in _org.apache.hadoop.hive.ql.io.AcidUtils#getHdfsDirSnapshotsForCleaner()_ API to avoid FileNotFoundException.",Dayakar,Dayakar,Major,Resolved,Fixed,13/Mar/23 12:19,16/Jun/23 10:04
Bug,HIVE-27140,13528433,Set HADOOP_PROXY_USER cause  hiveMetaStoreClient close everytime,"In order to use proxy user to access hive metastore server with kerberos, I set HADOOP_PROXY_USER = proxy user.

this is the code of  HiveMetaStoreClient

!image-2023-03-14-20-51-32-317.png|width=606,height=393!

but when I run a spark job with hive metatsore client, the connection to  hive metatsore server always close 

!image-2023-03-14-20-53-40-132.png|width=687,height=375!

and I notice that some log before close :

{color:#ff0000}Mestastore configuration hive.metastore.token.signature changed from DelegationTokenForHiveMetaStoreServer to{color}

so we can find why the connection close always,  hive metatsore client will compare  currentMetaVars with hiveconf

!image-2023-03-14-20-56-00-757.png|width=903,height=463!

but if we set HADOOP_PROXY_USER and init  hive metatsore client, the  ConfVars.METASTORE_TOKEN_SIGNATURE always is set

!image-2023-03-14-21-00-18-049.png|width=905,height=592!

so I think this solutions can fix the bug:

ConfVars.METASTORE_TOKEN_SIGNATURE  is no need to check in hive metatsore client

!image-2023-03-14-21-03-52-764.png|width=905,height=606!

obviously that is no more close

!image-2023-03-14-21-56-27-408.png|width=903,height=452!

 ",chenruotao,chenruotao,Major,Resolved,Fixed,14/Mar/23 12:47,16/Jun/23 10:04
Bug,HIVE-27147,13528814,HS2 is not accessible to clients via zookeeper when hostname used is not FQDN,"HS2 is not accessible to clients via zookeeper when hostname used during registration is InetAddress.getHostName() with JDK 11. This issue is happening due to change in behavior on JDK 11 and it is OS specific - 

[https://stackoverflow.com/questions/61898627/inetaddress-getlocalhost-gethostname-different-behavior-between-jdk-11-and-j|http://example.com/]",VenuReddy,VenuReddy,Major,Resolved,Fixed,16/Mar/23 15:28,16/Jun/23 10:04
Bug,HIVE-27157,13529276,AssertionError when inferring return type for unix_timestamp function,"Any attempt to derive the return data type for the {{unix_timestamp}} function results into the following assertion error.
{noformat}
java.lang.AssertionError: typeName.allowsPrecScale(true, false): BIGINT
	at org.apache.calcite.sql.type.BasicSqlType.checkPrecScale(BasicSqlType.java:65)
	at org.apache.calcite.sql.type.BasicSqlType.<init>(BasicSqlType.java:81)
	at org.apache.calcite.sql.type.SqlTypeFactoryImpl.createSqlType(SqlTypeFactoryImpl.java:67)
	at org.apache.calcite.sql.fun.SqlAbstractTimeFunction.inferReturnType(SqlAbstractTimeFunction.java:78)
	at org.apache.calcite.rex.RexBuilder.deriveReturnType(RexBuilder.java:278)
{noformat}
due to a faulty implementation of type inference for the respective operators:
 * [https://github.com/apache/hive/blob/52360151dc43904217e812efde1069d6225e9570/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveUnixTimestampSqlOperator.java]
 * [https://github.com/apache/hive/blob/52360151dc43904217e812efde1069d6225e9570/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveToUnixTimestampSqlOperator.java]

Although at this stage in master it is not possible to reproduce the problem with an actual SQL query the buggy implementation must be fixed since slight changes in the code/CBO rules may lead to methods relying on {{{}SqlOperator.inferReturnType{}}}.

Note that in older versions of Hive it is possible to hit the AssertionError in various ways. For example in Hive 3.1.3 (and older), the error may come from [HiveRelDecorrelator|https://github.com/apache/hive/blob/4df4d75bf1e16fe0af75aad0b4179c34c07fc975/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveRelDecorrelator.java#L1933] in the presence of sub-queries.",zabetak,zabetak,Major,Resolved,Fixed,20/Mar/23 16:39,16/Jun/23 10:04
Bug,HIVE-27163,13529490,Column stats are not getting published after an insert query into an external table with custom location,"Test case details are below


*test.q*
{noformat}
set hive.stats.column.autogather=true;
set hive.stats.autogather=true;
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/test;
create external table test_custom(age int, name string) stored as orc location '/tmp/test';
insert into test_custom select 1, 'test';
desc formatted test_custom age;{noformat}

*test.q.out*

 

 
{noformat}
#### A masked pattern was here ####
PREHOOK: type: CREATETABLE
#### A masked pattern was here ####
PREHOOK: Output: database:default
PREHOOK: Output: default@test_custom
#### A masked pattern was here ####
POSTHOOK: type: CREATETABLE
#### A masked pattern was here ####
POSTHOOK: Output: database:default
POSTHOOK: Output: default@test_custom
PREHOOK: query: insert into test_custom select 1, 'test'
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@test_custom
POSTHOOK: query: insert into test_custom select 1, 'test'
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@test_custom
POSTHOOK: Lineage: test_custom.age SIMPLE []
POSTHOOK: Lineage: test_custom.name SIMPLE []
PREHOOK: query: desc formatted test_custom age
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@test_custom
POSTHOOK: query: desc formatted test_custom age
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@test_custom
col_name                age
data_type               int
min
max
num_nulls
distinct_count
avg_col_len
max_col_len
num_trues
num_falses
bit_vector
comment                 from deserializer{noformat}
As we can see from desc formatted output, column stats were not populated

 ",dengzh,tarak271,Major,Resolved,Fixed,21/Mar/23 17:24,16/Jun/23 10:04
Bug,HIVE-27164,13529497,Create Temp Txn Table As Select is failing at tablePath validation,"After HIVE-25303, every CTAS goes for  HiveMetaStore$HMSHandler#translate_table_dryrun() call to fetch table location for CTAS queries which fails with following exception for temp tables if MetastoreDefaultTransformer is set.
{code:java}
2023-03-17 16:41:23,390 INFO  org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer: [pool-6-thread-196]: Starting translation for CreateTable for processor HMSClient-@localhost with [EXTWRITE, EXTREAD, HIVEBUCKET2, HIVEFULLACIDREAD, HIVEFULLACIDWRITE, HIVECACHEINVALIDATE, HIVEMANAGESTATS, HIVEMANAGEDINSERTWRITE, HIVEMANAGEDINSERTREAD, HIVESQL, HIVEMQT, HIVEONLYMQTWRITE] on table test_temp
2023-03-17 16:41:23,392 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: [pool-6-thread-196]: MetaException(message:Illegal location for managed table, it has to be within database's managed location)
        at org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer.validateTablePaths(MetastoreDefaultTransformer.java:886)
        at org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer.transformCreateTable(MetastoreDefaultTransformer.java:666)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.translate_table_dryrun(HiveMetaStore.java:2164)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) {code}
I am able to repro this issue using attached testcase. [^mm_cttas.q]

 

There are multiple ways to fix this issue
 * Have temp txn table path under db's managed location path. This will help with encryption zone paths as well.
 * Skip location check for temp tables at MetastoreDefaultTransformer#validateTablePaths()",VenuReddy,nareshpr,Major,Resolved,Fixed,21/Mar/23 18:08,20/Apr/23 12:10
Bug,HIVE-27168,13529780,Use basename of the datatype when fetching partition metadata using partition filters,"While fetching partition metadata using partition filters, we use the column type of the table directly. However, char/varchar types can contain extra information such as length of the char/varchar column and hence it skips fetching partition metadata due to this extra information.

Solution: Use the basename of the column type while deciding on whether partition pruning can be done on the partitioned column.",sbadhya,sbadhya,Major,Resolved,Fixed,23/Mar/23 13:37,16/Jun/23 10:04
Bug,HIVE-27179,13530228,HS2 WebUI throws NPE when JspFactory loaded from jetty-runner,"In HIVE-17088{*},{*} we resolved a NPE thrown from HS2 WebUI by introducing 

javax.servlet.jsp-api. It works as expected when the javax.servlet.jsp-api jar prevails jetty-runner jar, but things can be different in some environments, it still throws NPE when opening the HS2 web:
{noformat}
java.lang.NullPointerException 
at org.apache.hive.generated.hiveserver2.hiveserver2_jsp._jspService(hiveserver2_jsp.java:286) 
at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:71) 
at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) 
at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1443) 
at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791) 
at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
...{noformat}
The jetty-runner JspFactory.getDefaultFactory() just returns null.",dengzh,dengzh,Major,Resolved,Fixed,27/Mar/23 10:45,16/Jun/23 10:04
Bug,HIVE-27201,13530935,Inconsistency between session Hive and thread-local Hive may cause HS2 deadlock,"The HiveServer2’s server handler can switch to process the operation from other session, in such case, the Hive cached in ThreadLocal is not the same as the Hive in SessionState, and can be referenced by another session. 

If the two handlers swap their sessions to process the DatabaseMetaData request, and the HiveMetastoreClientFactory obtains the Hive via Hive.get(), then there is a chance that the deadlock can happen.",dengzh,dengzh,Major,Resolved,Fixed,31/Mar/23 09:16,16/Jun/23 10:04
Bug,HIVE-27217,13531480,addWriteNotificationLogInBatch can silently fail,"Debugging an issue, I noticed that addWriteNotificationLogInBatch in Hive.java can fail silently if the TApplicationException thrown is not TApplicationException.UNKNOWN_METHOD or TApplicationException.WRONG_METHOD_NAME.

https://github.com/apache/hive/blob/40a7d689e51d02fa9b324553fd1810d0ad043080/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java#L3359-L3381

Failures to write in the notification log can be very difficult to debug, we should rethrow the exception so that the failure is very visible.",jfs,jfs,Major,Resolved,Fixed,04/Apr/23 16:50,16/Jun/23 10:03
Bug,HIVE-27223,13531697,Show Compactions failing with NPE,"{noformat}
java.lang.NullPointerException: null
	at java.io.DataOutputStream.writeBytes(DataOutputStream.java:274) ~[?:?]
	at org.apache.hadoop.hive.ql.ddl.process.show.compactions.ShowCompactionsOperation.writeRow(ShowCompactionsOperation.java:135) 
	at org.apache.hadoop.hive.ql.ddl.process.show.compactions.ShowCompactionsOperation.execute(ShowCompactionsOperation.java:57) 
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213) 
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) 
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:360) 
{noformat}
",ayushtkn,ayushtkn,Major,Resolved,Fixed,06/Apr/23 05:42,16/Jun/23 10:04
Bug,HIVE-27228,13531735,Add missing upgrade SQL statements after CQ_NUMBER_OF_BUCKETS column being introduced in HIVE-26719,"HIVE-26719 introduced CQ_NUMBER_OF_BUCKETS column in COMPACTION_QUEUE table and COMPLETED_COMPACTIONS table. However, the corresponding upgrade SQL statements is missing for these columns. Also CQ_NUMBER_OF_BUCKETS is not updated in the COMPACTIONS view in information schema.",sbadhya,sbadhya,Major,Resolved,Fixed,06/Apr/23 10:58,16/Jun/23 10:03
Bug,HIVE-27264,13532629,Literals in conjunction of two IN expression are considered not equals if type precision is different,"{code}
create table r_table (
  string_col varchar(30)
);


create table l_table (
  string_col varchar(14)
);

insert into r_table VALUES ('AAA111');
insert into l_table VALUES ('AAA111');
SELECT l_table.string_col from l_table, r_table
WHERE r_table.string_col = l_table.string_col AND l_table.string_col IN ('AAA111', 'BBB222') AND r_table.string_col IN ('AAA111', 'BBB222');
{code}
Should give one row
{code}
AAA111
{code}
but it returns empty rs

Workaround
{code}
set hive.optimize.point.lookup=false;
{code}

",kkasa,kkasa,Major,Resolved,Fixed,14/Apr/23 09:03,21/Apr/23 08:51
Bug,HIVE-27267,13532876,Incorrect results when doing bucket map join on decimal bucketed column with subquery,"The following queries when run on a Hive cluster produce no results - 
Repro queries - 
{code:java}
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.support.concurrency=true;
set hive.convert.join.bucket.mapjoin.tez=true;

drop table if exists test_external_source;
create external table test_external_source (date_col date, string_col string, decimal_col decimal(38,0)) stored as orc tblproperties ('external.table.purge'='true');
insert into table test_external_source values ('2022-08-30', 'pipeline', '50000000000000000005905545593'), ('2022-08-16', 'pipeline', '50000000000000000005905545593'), ('2022-09-01', 'pipeline', '50000000000000000006008686831'), ('2022-08-30', 'pipeline', '50000000000000000005992620837'), ('2022-09-01', 'pipeline', '50000000000000000005992620837'), ('2022-09-01', 'pipeline', '50000000000000000005992621067'), ('2022-08-30', 'pipeline', '50000000000000000005992621067');

drop table if exists test_external_target;
create external table test_external_target (date_col date, string_col string, decimal_col decimal(38,0)) stored as orc tblproperties ('external.table.purge'='true');
insert into table test_external_target values ('2017-05-17', 'pipeline', '50000000000000000000441610525'), ('2018-12-20', 'pipeline', '50000000000000000001048981030'), ('2020-06-30', 'pipeline', '50000000000000000002332575516'), ('2021-08-16', 'pipeline', '50000000000000000003897973989'), ('2017-06-06', 'pipeline', '50000000000000000000449148729'), ('2017-09-08', 'pipeline', '50000000000000000000525378314'), ('2022-08-30', 'pipeline', '50000000000000000005905545593'), ('2022-08-16', 'pipeline', '50000000000000000005905545593'), ('2018-05-03', 'pipeline', '50000000000000000000750826355'), ('2020-01-10', 'pipeline', '50000000000000000001816579677'), ('2021-11-01', 'pipeline', '50000000000000000004269423714'), ('2017-11-07', 'pipeline', '50000000000000000000585901787'), ('2019-10-15', 'pipeline', '50000000000000000001598843430'), ('2020-04-01', 'pipeline', '50000000000000000002035795461'), ('2020-02-24', 'pipeline', '50000000000000000001932600185'), ('2020-04-27', 'pipeline', '50000000000000000002108160849'), ('2016-07-05', 'pipeline', '50000000000000000000054405114'), ('2020-06-02', 'pipeline', '50000000000000000002234387967'), ('2020-08-21', 'pipeline', '50000000000000000002529168758'), ('2021-02-17', 'pipeline', '50000000000000000003158511687');

drop table if exists target_table;
drop table if exists source_table;
create table target_table(date_col date, string_col string, decimal_col decimal(38,0)) clustered by (decimal_col) into 7 buckets stored as orc tblproperties ('bucketing_version'='2', 'transactional'='true', 'transactional_properties'='default');
create table source_table(date_col date, string_col string, decimal_col decimal(38,0)) clustered by (decimal_col) into 7 buckets stored as orc tblproperties ('bucketing_version'='2', 'transactional'='true', 'transactional_properties'='default');

insert into table target_table select * from test_external_target;
insert into table source_table select * from test_external_source; {code}
Query which is under investigation - 
{code:java}
select * from target_table inner join (select distinct date_col, 'pipeline' string_col, decimal_col from source_table where coalesce(decimal_col,'') = '50000000000000000005905545593') s on s.date_col = target_table.date_col AND s.string_col = target_table.string_col AND s.decimal_col = target_table.decimal_col; {code}
Expected result of the query - 2 records
{code:java}
+------------------------+--------------------------+--------------------------------+-------------+---------------+--------------------------------+
| target_table.date_col  | target_table.string_col  |    target_table.decimal_col    | s.date_col  | s.string_col  |         s.decimal_col          |
+------------------------+--------------------------+--------------------------------+-------------+---------------+--------------------------------+
| 2022-08-16             | pipeline                 | 50000000000000000005905545593  | 2022-08-16  | pipeline      | 50000000000000000005905545593  |
| 2022-08-30             | pipeline                 | 50000000000000000005905545593  | 2022-08-30  | pipeline      | 50000000000000000005905545593  |
+------------------------+--------------------------+--------------------------------+-------------+---------------+--------------------------------+ {code}
Actual result of the query - No records
{code:java}
+------------------------+--------------------------+---------------------------+-------------+---------------+----------------+
| target_table.date_col  | target_table.string_col  | target_table.decimal_col  | s.date_col  | s.string_col  | s.decimal_col  |
+------------------------+--------------------------+---------------------------+-------------+---------------+----------------+
+------------------------+--------------------------+---------------------------+-------------+---------------+----------------+ {code}
The workaround which fetches the correct result here is to set the below config to false - 
{code:java}
set hive.convert.join.bucket.mapjoin.tez=false;{code}
Notes from investigation - 
1. The batch containing the 2 results are forwarded correctly to the map join operator. However, during the join comparision, the hash table is empty.
2. The problem seems to be that even though HashTableDummyOperator performs loading of hash table with the records, however the map join operator does not take into account all the hash tables from various instances of HashTableDummyOperator (due to multiple map tasks initiated by bucket map join) but rather uses only one hash table from one of the HashTableDummyOperator instance. In this case, the selected instance had an empty hash table hence no records were matched in the join operator.
3. If the table is unbucketed / 1-bucketed, then the results are correct. There is only 1 map task which is spawned which loads the records into the hash table. The workaround (setting *hive.convert.join.bucket.mapjoin.tez* to {*}false{*}) also has the same effect since there is 1 map task which loads the records into the hash table.
4. HashTableDummyOperator is created in the optimizer and is associated with the plan, hence suspecting there is a some issue in the optimizer code. Ideally, all hash tables from all instances of HashTableDummyOperator must be used by the map join operator.",seonggon,sbadhya,Major,Resolved,Fixed,17/Apr/23 12:06,19/May/23 07:14
Bug,HIVE-27268,13532938,Hive.getPartitionsByNames should not enforce SessionState to be available,"HIVE-24743, HIVE-24392 is enforcing to check for valid write Id list for ""Hive.getPartitionsByName"".

This breaks basic API integration. For a user who needs to get basic partition detail, he is forced to have SessionState.

Request in this ticket is to ensure that if SessionState.get() is null, it should return empty validWriteIdList.",henrib,henrib,Minor,Resolved,Fixed,17/Apr/23 19:01,16/Jun/23 10:03
Bug,HIVE-27271,13533176,"Client connection to HS2 fails when transportMode=http, ssl=true, sslTrustStore specified without trustStorePassword in the JDBC URL","*[Description]*

Client connection to HS2 fails with transportMode as http, ssl is enabled, sslTrustStore is specified without trustStorePassword in the JDBC URL. Where as with transportMode as binary, connection is successful without trustStorePassword in the connection URL.

trustStorePassword is not a necessary parameter in connection URL. Connection can be established without it.

From the javadocs [Link|https://docs.oracle.com/javase/7/docs/api/java/security/KeyStore.html#load(java.io.InputStream,%20char%5B%5D)] A password may be given to unlock the keystore (e.g. the keystore resides on a hardware token device), or to check the integrity of the keystore data. If a password is not given for integrity checking, then integrity checking is not performed.

 

At present, org.apache.hive.jdbc.HiveConnection#getHttpClient() access sslTrustStorePassword null reference and fails as shown below:

!image-2023-04-19-14-27-23-665.png!

 

*[Steps to reproduce]*
{code:java}
kvenureddy@192 apache-hive-3.1.3000.2023.0.15.0-SNAPSHOT-bin % bin/beeline -u ""jdbc:hive2://kvrtls-1.kvrtls.root.hwx.site:10001/default;ssl=true;sslTrustStore=/Users/kvenureddy/code/hive/cloudera/hive/packaging/target/apache-hive-3.1.3000.2023.0.15.0-SNAPSHOT-bin/apache-hive-3.1.3000.2023.0.15.0-SNAPSHOT-bin/cm-auto-global_truststore.jks;transportMode=http;httpPath=cliservice;""
Error: Could not open client transport with JDBC Uri: jdbc:hive2://kvrtls-1.kvrtls.root.hwx.site:10001/default;ssl=true;sslTrustStore=/Users/kvenureddy/code/hive/cloudera/hive/packaging/target/apache-hive-3.1.3000.2023.0.15.0-SNAPSHOT-bin/apache-hive-3.1.3000.2023.0.15.0-SNAPSHOT-bin/cm-auto-global_truststore.jks;transportMode=http;httpPath=cliservice;: Could not create an https connection to jdbc:hive2://kvrtls-1.kvrtls.root.hwx.site:10001/default;ssl=true;sslTrustStore=/Users/kvenureddy/code/hive/cloudera/hive/packaging/target/apache-hive-3.1.3000.2023.0.15.0-SNAPSHOT-bin/apache-hive-3.1.3000.2023.0.15.0-SNAPSHOT-bin/cm-auto-global_truststore.jks;transportMode=http;httpPath=cliservice;. null (state=08S01,code=0)
kvenureddy@192 apache-hive-3.1.3000.2023.0.15.0-SNAPSHOT-bin % 
{code}
 ",VenuReddy,VenuReddy,Major,Resolved,Fixed,19/Apr/23 09:05,16/Jun/23 10:04
Bug,HIVE-27272,13533238,Limit query causes error at qtests,"Sample query: 


{code:sql}
create table icebergtable (id int, name string) stored by iceberg stored as parquet;

insert into icebergtable values (1, 'Joe'), (2, 'Jack');

select * from icebergtable limit 1; 
{code}

{code}
Error: 

java.lang.IllegalStateException: Cannot get a query cache object while working outside of HS2, this is unexpected
        at org.apache.hadoop.hive.ql.exec.LimitOperator.initializeOp(LimitOperator.java:77) ~[hive-exec-3.1.3000.2023.0.15.0-SNAPSHOT.jar:3.1.3000.2023.0.15.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:374) ~[hive-exec-3.1.3000.2023.0.15.0-SNAPSHOT.jar:3.1.3000.2023.0.15.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:571) ~[hive-exec-3.1.3000.2023.0.15.0-SNAPSHOT.jar:3.1.3000.2023.0.15.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:523) ~[hive-exec-3.1.3000.2023.0.15.0-SNAPSHOT.jar:3.1.3000.2023.0.15.0-SNAPSHOT] 
{code}

Important note: check not only the positive but the negative tests as well!",InvisibleProgrammer,InvisibleProgrammer,Major,Resolved,Fixed,19/Apr/23 14:03,16/Jun/23 10:04
Bug,HIVE-27278,13533421,Simplify correlated queries with empty inputs,"The correlated query below will not produce any result no matter the content of the table.
{code:sql}
create table t1 (id int, val varchar(10)) stored as orc TBLPROPERTIES ('transactional'='true');
create table t2 (id int, val varchar(10)) stored as orc TBLPROPERTIES ('transactional'='true');

EXPLAIN CBO SELECT id FROM t1 WHERE NULL IN (SELECT NULL FROM t2 where t1.id = t2.id);
{code}
The CBO is able to derive that part of the query is empty and ends up with the following plan.
{noformat}
CBO PLAN:
HiveProject(id=[$0])
  LogicalCorrelate(correlation=[$cor0], joinType=[semi], requiredColumns=[{}])
    HiveTableScan(table=[[default, t1]], table:alias=[t1])
    HiveValues(tuples=[[]])
{noformat}
The presence of LogicalCorrelate is first redundant but also problematic since many parts of the optimizer assume that queries are decorrelated and do not know how to handle the LogicalCorrelate.

In the presence of views the same query can lead to the following exception during compilation.
{code:sql}
CREATE MATERIALIZED VIEW v1 AS SELECT id FROM t2;
EXPLAIN CBO SELECT id FROM t1 WHERE NULL IN (SELECT NULL FROM t2 where t1.id = t2.id);
{code}
{noformat}
org.apache.calcite.plan.RelOptPlanner$CannotPlanException: There are not enough rules to produce a node with desired properties: convention=HIVE, sort=[], dist=any. All the inputs have relevant nodes, however the cost is still infinite.
Root: rel#185:RelSubset#3.HIVE.[].any
Original rel:
HiveProject(id=[$0]): rowcount = 4.0, cumulative cost = {20.0 rows, 13.0 cpu, 0.0 io}, id = 178
  LogicalCorrelate(correlation=[$cor0], joinType=[semi], requiredColumns=[{}]): rowcount = 4.0, cumulative cost = {16.0 rows, 9.0 cpu, 0.0 io}, id = 176
    HiveTableScan(table=[[default, t1]], table:alias=[t1]): rowcount = 4.0, cumulative cost = {4.0 rows, 5.0 cpu, 0.0 io}, id = 111
    HiveValues(tuples=[[]]): rowcount = 1.0, cumulative cost = {1.0 rows, 1.0 cpu, 0.0 io}, id = 139

Sets:
Set#0, type: RecordType(INTEGER id, VARCHAR(10) val, BIGINT BLOCK__OFFSET__INSIDE__FILE, VARCHAR(2147483647) INPUT__FILE__NAME, RecordType(BIGINT writeid, INTEGER bucketid, BIGINT rowid) ROW__ID, BOOLEAN ROW__IS__DELETED)
	rel#180:RelSubset#0.HIVE.[].any, best=rel#111
		rel#111:HiveTableScan.HIVE.[].any(table=[default, t1],htColumns=[0, 1, 2, 3, 4, 5],insideView=false,plKey=default.t1;,table:alias=t1,tableScanTrait=null), rowcount=4.0, cumulative cost={4.0 rows, 5.0 cpu, 0.0 io}
Set#1, type: RecordType(NULL _o__c0)
	rel#181:RelSubset#1.HIVE.[].any, best=rel#139
		rel#139:HiveValues.HIVE.[].any(type=RecordType(NULL _o__c0),tuples=[]), rowcount=1.0, cumulative cost={1.0 rows, 1.0 cpu, 0.0 io}
Set#2, type: RecordType(INTEGER id, VARCHAR(10) val, BIGINT BLOCK__OFFSET__INSIDE__FILE, VARCHAR(2147483647) INPUT__FILE__NAME, RecordType(BIGINT writeid, INTEGER bucketid, BIGINT rowid) ROW__ID, BOOLEAN ROW__IS__DELETED)
	rel#183:RelSubset#2.NONE.[].any, best=null
		rel#182:LogicalCorrelate.NONE.[].any(left=RelSubset#180,right=RelSubset#181,correlation=$cor0,joinType=semi,requiredColumns={}), rowcount=4.0, cumulative cost={inf}
Set#3, type: RecordType(INTEGER id)
	rel#185:RelSubset#3.HIVE.[].any, best=null
		rel#184:HiveProject.HIVE.[].any(input=RelSubset#183,inputs=0,synthetic=false), rowcount=4.0, cumulative cost={inf}

Graphviz:
digraph G {
	root [style=filled,label=""Root""];
	subgraph cluster0{
		label=""Set 0 RecordType(INTEGER id, VARCHAR(10) val, BIGINT BLOCK__OFFSET__INSIDE__FILE, VARCHAR(2147483647) INPUT__FILE__NAME, RecordType(BIGINT writeid, INTEGER bucketid, BIGINT rowid) ROW__ID, BOOLEAN ROW__IS__DELETED)"";
		rel111 [label=""rel#111:HiveTableScan\ntable=[default, t1],htColumns=[0, 1, 2, 3, 4, 5],insideView=false,plKey=default.t1;,table:alias=t1,tableScanTrait=null\nrows=4.0, cost={4.0 rows, 5.0 cpu, 0.0 io}"",color=blue,shape=box]
		subset180 [label=""rel#180:RelSubset#0.HIVE.[].any""]
	}
	subgraph cluster1{
		label=""Set 1 RecordType(NULL _o__c0)"";
		rel139 [label=""rel#139:HiveValues\ntype=RecordType(NULL _o__c0),tuples=[]\nrows=1.0, cost={1.0 rows, 1.0 cpu, 0.0 io}"",color=blue,shape=box]
		subset181 [label=""rel#181:RelSubset#1.HIVE.[].any""]
	}
	subgraph cluster2{
		label=""Set 2 RecordType(INTEGER id, VARCHAR(10) val, BIGINT BLOCK__OFFSET__INSIDE__FILE, VARCHAR(2147483647) INPUT__FILE__NAME, RecordType(BIGINT writeid, INTEGER bucketid, BIGINT rowid) ROW__ID, BOOLEAN ROW__IS__DELETED)"";
		rel182 [label=""rel#182:LogicalCorrelate\nleft=RelSubset#180,right=RelSubset#181,correlation=$cor0,joinType=semi,requiredColumns={}\nrows=4.0, cost={inf}"",shape=box]
		subset183 [label=""rel#183:RelSubset#2.NONE.[].any""]
	}
	subgraph cluster3{
		label=""Set 3 RecordType(INTEGER id)"";
		rel184 [label=""rel#184:HiveProject\ninput=RelSubset#183,inputs=0,synthetic=false\nrows=4.0, cost={inf}"",shape=box]
		subset185 [label=""rel#185:RelSubset#3.HIVE.[].any""]
	}
	root -> subset185;
	subset180 -> rel111[color=blue];
	subset181 -> rel139[color=blue];
	subset183 -> rel182; rel182 -> subset180[label=""0""]; rel182 -> subset181[label=""1""];
	subset185 -> rel184; rel184 -> subset183;
}
	at org.apache.calcite.plan.volcano.RelSubset$CheapestPlanReplacer.visit(RelSubset.java:742) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.calcite.plan.volcano.RelSubset.buildCheapestPlan(RelSubset.java:365) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:520) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.applyMaterializedViewRewriting(CalcitePlanner.java:2058) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:1722) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:1591) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.calcite.tools.Frameworks.lambda$withPlanner$0(Frameworks.java:131) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.calcite.prepare.CalcitePrepareImpl.perform(CalcitePrepareImpl.java:914) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.calcite.tools.Frameworks.withPrepare(Frameworks.java:180) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.calcite.tools.Frameworks.withPlanner(Frameworks.java:126) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.logicalPlan(CalcitePlanner.java:1343) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:570) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12820) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:465) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:326) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:180) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:326) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:224) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:107) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:519) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:471) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:436) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:430) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:121) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:227) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:257) ~[hive-cli-4.0.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.cli.CliDriver.processCmd1(CliDriver.java:201) ~[hive-cli-4.0.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:127) ~[hive-cli-4.0.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:425) ~[hive-cli-4.0.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:356) ~[hive-cli-4.0.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:733) ~[hive-it-util-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:703) ~[hive-it-util-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:115) ~[hive-it-util-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:157) ~[hive-it-util-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver(TestMiniLlapLocalCliDriver.java:62) ~[test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_261]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_261]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_261]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_261]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) ~[junit-4.13.2.jar:4.13.2]
	at org.apache.hadoop.hive.cli.control.CliAdapter$2$1.evaluate(CliAdapter.java:135) ~[hive-it-util-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.Suite.runChild(Suite.java:128) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.Suite.runChild(Suite.java:27) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.2.jar:4.13.2]
	at org.apache.hadoop.hive.cli.control.CliAdapter$1$1.evaluate(CliAdapter.java:95) ~[hive-it-util-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.junit.rules.RunRules.evaluate(RunRules.java:20) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.2.jar:4.13.2]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) ~[surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) ~[surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) ~[surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) ~[surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) ~[surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) ~[surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) ~[surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) ~[surefire-booter-3.0.0-M4.jar:3.0.0-M4]
{noformat}
The goal of this ticket is to get rid of the redundant correlation to avoid compilation failures but also for unlocking further simplifications and improving plan readability.

The plan can be simplified further based on the following observations.

If the right side of the correlate is empty then the whole correlate is empty when joinType is SEMI/INNER. Moreover if correlate type is LEFT then we can also drop the correlate and use t1 padded with nulls for the right side. Lastly, if the type is ANTI then result is the entire t1 so the correlate can also be dropped. RIGHT and FULL correlations are invalid and should never appear in the plan.

If the left side of the correlate is empty the result is empty and the correlation can be dropped for every legal joinType (INNER/SEMI/ANTI/LEFT).",zabetak,zabetak,Major,Resolved,Fixed,20/Apr/23 15:28,16/Jun/23 10:03
Bug,HIVE-27289,13533912,Check for proxy hosts with subnet in IP results in exception ,"When running schematool for sysdb setup in a kerberized environment, the check to see if the user is a super user fails when the hadoop.proxyuser.hive.hosts contains a subnet in IPAddresses. (for example: 192.168.0.3/23)

{noformat}
exec /opt/cloudera/parcels/CDH-7.1.8-1.cdh7.1.8.p11.35002917/lib/hadoop/bin/hadoop jar /opt/cloudera/parcels/CDH-7.1.8-1.cdh7.1.8.p11.35002917/lib/hive/lib/hive-cli-3.1.3000.7.1.8.11-3.jar org.apache.hive.beeline.schematool.HiveSchemaTool -verbose -dbType hive -metaDbType mysql -initOrUpgradeSchema
WARNING: Use ""yarn jar"" to launch YARN applications.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-7.1.8-1.cdh7.1.8.p11.35002917/jars/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-7.1.8-1.cdh7.1.8.p11.35002917/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 93d863d8-cbe6-47fc-8817-49074841f9f1
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.hadoop.hive.common.StringInternUtils (file:/opt/cloudera/parcels/CDH-7.1.8-1.cdh7.1.8.p11.35002917/jars/hive-common-3.1.3000.7.1.8.11-3.jar) to field java.net.URI.string
WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hive.common.StringInternUtils
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
23/04/22 12:01:38 ERROR metastore.HiveMetaStoreAuthorizer: [main]: HiveMetaStoreAuthorizer.onEvent(): failed
java.lang.IllegalArgumentException: Could not parse []
	at org.apache.commons.net.util.SubnetUtils.toInteger(SubnetUtils.java:287) ~[commons-net-3.6.jar:3.6]
	at org.apache.commons.net.util.SubnetUtils.access$400(SubnetUtils.java:27) ~[commons-net-3.6.jar:3.6]
	at org.apache.commons.net.util.SubnetUtils$SubnetInfo.isInRange(SubnetUtils.java:125) ~[commons-net-3.6.jar:3.6]
	at org.apache.hadoop.util.MachineList.includes(MachineList.java:155) ~[hadoop-common-3.1.1.7.1.8.11-3.jar:?]
	at org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.checkUserHasHostProxyPrivileges(MetaStoreUtils.java:1347) ~[hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer.isSuperUser(HiveMetaStoreAuthorizer.java:495) ~[hive-exec-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer.skipAuthorization(HiveMetaStoreAuthorizer.java:558) ~[hive-exec-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer.onEvent(HiveMetaStoreAuthorizer.java:104) [hive-exec-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.firePreEvent(HiveMetaStore.java:4026) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_req(HiveMetaStore.java:1695) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at com.sun.proxy.$Proxy43.get_database_req(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1946) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1930) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:213) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at com.sun.proxy.$Proxy44.getDatabase(Unknown Source) [?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:2161) [hive-exec-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.service.cli.session.HiveSessionImpl.configureSession(HiveSessionImpl.java:296) [hive-service-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.service.cli.session.HiveSessionImpl.open(HiveSessionImpl.java:188) [hive-service-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.service.cli.session.SessionManager.createSession(SessionManager.java:480) [hive-service-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.service.cli.session.SessionManager.openSession(SessionManager.java:420) [hive-service-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.service.cli.CLIService.openSession(CLIService.java:190) [hive-service-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.service.cli.thrift.ThriftCLIService.getSessionHandle(ThriftCLIService.java:499) [hive-service-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.service.cli.thrift.ThriftCLIService.OpenSession(ThriftCLIService.java:341) [hive-service-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.jdbc.HiveConnection.openSession(HiveConnection.java:1006) [hive-jdbc-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:313) [hive-jdbc-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:107) [hive-jdbc-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at java.sql.DriverManager.getConnection(DriverManager.java:677) [java.sql:?]
	at java.sql.DriverManager.getConnection(DriverManager.java:228) [java.sql:?]
	at org.apache.hadoop.hive.metastore.tools.schematool.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:88) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.metastore.tools.schematool.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:103) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.metastore.CDHMetaStoreSchemaInfo.getMetaStoreSchemaVersion(CDHMetaStoreSchemaInfo.java:345) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.metastore.tools.schematool.SchemaToolTaskInitOrUpgrade.execute(SchemaToolTaskInitOrUpgrade.java:41) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.metastore.tools.schematool.MetastoreSchemaTool.run(MetastoreSchemaTool.java:454) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.beeline.schematool.HiveSchemaTool.main(HiveSchemaTool.java:138) [hive-beeline-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318) [hadoop-common-3.1.1.7.1.8.11-3.jar:?]
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232) [hadoop-common-3.1.1.7.1.8.11-3.jar:?]
23/04/22 12:01:38 ERROR metastore.HiveMetaStoreAuthorizer: [main]: HiveMetaStoreAuthorizer.onEvent(): failed
java.lang.IllegalArgumentException: Could not parse []
	at org.apache.commons.net.util.SubnetUtils.toInteger(SubnetUtils.java:287) ~[commons-net-3.6.jar:3.6]
	at org.apache.commons.net.util.SubnetUtils.access$400(SubnetUtils.java:27) ~[commons-net-3.6.jar:3.6]
	at org.apache.commons.net.util.SubnetUtils$SubnetInfo.isInRange(SubnetUtils.java:125) ~[commons-net-3.6.jar:3.6]
	at org.apache.hadoop.util.MachineList.includes(MachineList.java:155) ~[hadoop-common-3.1.1.7.1.8.11-3.jar:?]
	at org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.checkUserHasHostProxyPrivileges(MetaStoreUtils.java:1347) ~[hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer.isSuperUser(HiveMetaStoreAuthorizer.java:495) ~[hive-exec-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer.skipAuthorization(HiveMetaStoreAuthorizer.java:558) ~[hive-exec-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer.onEvent(HiveMetaStoreAuthorizer.java:104) [hive-exec-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.firePreEvent(HiveMetaStore.java:4026) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_req(HiveMetaStore.java:1695) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at com.sun.proxy.$Proxy43.get_database_req(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1946) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1930) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:213) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at com.sun.proxy.$Proxy44.getDatabase(Unknown Source) [?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:2161) [hive-exec-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.service.cli.session.HiveSessionImpl.configureSession(HiveSessionImpl.java:296) [hive-service-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.service.cli.session.HiveSessionImpl.open(HiveSessionImpl.java:188) [hive-service-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.service.cli.session.SessionManager.createSession(SessionManager.java:480) [hive-service-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.service.cli.session.SessionManager.openSession(SessionManager.java:420) [hive-service-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.service.cli.CLIService.openSession(CLIService.java:190) [hive-service-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.service.cli.thrift.ThriftCLIService.getSessionHandle(ThriftCLIService.java:499) [hive-service-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.service.cli.thrift.ThriftCLIService.OpenSession(ThriftCLIService.java:341) [hive-service-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.jdbc.HiveConnection.openSession(HiveConnection.java:1006) [hive-jdbc-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:313) [hive-jdbc-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:107) [hive-jdbc-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at java.sql.DriverManager.getConnection(DriverManager.java:677) [java.sql:?]
	at java.sql.DriverManager.getConnection(DriverManager.java:228) [java.sql:?]
	at org.apache.hadoop.hive.metastore.tools.schematool.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:88) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.metastore.tools.schematool.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:103) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.metastore.CDHMetaStoreSchemaInfo.getMetaStoreSchemaVersion(CDHMetaStoreSchemaInfo.java:345) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.metastore.tools.schematool.SchemaToolTaskInitOrUpgrade.execute(SchemaToolTaskInitOrUpgrade.java:41) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hadoop.hive.metastore.tools.schematool.MetastoreSchemaTool.run(MetastoreSchemaTool.java:454) [hive-standalone-metastore-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at org.apache.hive.beeline.schematool.HiveSchemaTool.main(HiveSchemaTool.java:138) [hive-beeline-3.1.3000.7.1.8.11-3.jar:3.1.3000.7.1.8.11-3]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318) [hadoop-common-3.1.1.7.1.8.11-3.jar:?]
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232) [hadoop-common-3.1.1.7.1.8.11-3.jar:?]
23/04/22 12:01:38 ERROR metastore.RetryingHMSHandler: [main]: MetaException(message:Could not parse [])
	at org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer.onEvent(HiveMetaStoreAuthorizer.java:110)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.firePreEvent(HiveMetaStore.java:4026)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_req(HiveMetaStore.java:1695)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy43.get_database_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1946)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1930)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:213)
	at com.sun.proxy.$Proxy44.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:2161)
	at org.apache.hive.service.cli.session.HiveSessionImpl.configureSession(HiveSessionImpl.java:296)
	at org.apache.hive.service.cli.session.HiveSessionImpl.open(HiveSessionImpl.java:188)
	at org.apache.hive.service.cli.session.SessionManager.createSession(SessionManager.java:480)
	at org.apache.hive.service.cli.session.SessionManager.openSession(SessionManager.java:420)
	at org.apache.hive.service.cli.CLIService.openSession(CLIService.java:190)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.getSessionHandle(ThriftCLIService.java:499)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.OpenSession(ThriftCLIService.java:341)
	at org.apache.hive.jdbc.HiveConnection.openSession(HiveConnection.java:1006)
	at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:313)
	at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:107)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:228)
	at org.apache.hadoop.hive.metastore.tools.schematool.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:88)
	at org.apache.hadoop.hive.metastore.tools.schematool.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:103)
	at org.apache.hadoop.hive.metastore.CDHMetaStoreSchemaInfo.getMetaStoreSchemaVersion(CDHMetaStoreSchemaInfo.java:345)
	at org.apache.hadoop.hive.metastore.tools.schematool.SchemaToolTaskInitOrUpgrade.execute(SchemaToolTaskInitOrUpgrade.java:41)
	at org.apache.hadoop.hive.metastore.tools.schematool.MetastoreSchemaTool.run(MetastoreSchemaTool.java:454)
	at org.apache.hive.beeline.schematool.HiveSchemaTool.main(HiveSchemaTool.java:138)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
{noformat}

The root cause being HiveMetaStoreAuthorizer.isSuperUser()
String        ipAddress = HiveMetaStore.HMSHandler.getIPAddress();

--> this is null is kerberized environments when running from something like schematool (and perhaps metatool as well). 
when you have a CIDR representation in the IPAddress, then SubnetInfo.isInRange("""") throws the above exception.

So in such cases, it is better to use the localhost address before resorting to """".
",ngangam,ngangam,Minor,Resolved,Fixed,25/Apr/23 02:14,16/Jun/23 10:04
Bug,HIVE-27292,13533974,Upgrade Zookeeper to 3.7.1,Upgrade Zookeper from 3.6.3 to 3.7.1 since 3.6.3 is in end of life. https://endoflife.date/zookeeper,amanraj2520,amanraj2520,Major,Resolved,Fixed,25/Apr/23 09:43,16/Jun/23 10:04
Bug,HIVE-27293,13533981,Vectorization: Incorrect results with nvl for ORC table,"Attached repro.q file and data file used to reproduce the issue.
{code:java}
Insert overwrite table etarget
select mt.*, floor(rand() * 1) as bdata_no from (select nvl(np.client_id,' '),nvl(np.id_enddate,cast(0 as decimal(10,0))),nvl(np.client_gender,' '),nvl(np.birthday,cast(0 as decimal(10,0))),nvl(np.nationality,' '),nvl(np.address_zipcode,' '),nvl(np.income,cast(0 as decimal(15,2))),nvl(np.address,' '),nvl(np.part_date,cast(0 as int)) from (select * from esource where part_date = 20230414) np) mt;
 {code}
Outcome:
{code:java}
select client_id,birthday,income from etarget; 
15678   0  0.00
67891  19313  -1.00
12345  0  0.00{code}
Expected Result :
{code:java}
select client_id,birthday,income from etarget; 
12345 19613 -1.00
67891 19313 -1.00 
15678 0  0.00{code}
Disabling hive.vectorized.use.vectorized.input.format produces correct output.",rameshkumar,rtrivedi12,Major,Resolved,Fixed,25/Apr/23 10:35,09/Jun/23 18:25
Bug,HIVE-27305,13534425,AssertionError in Calcite during planning for incremental rebuild of materialized view with aggregate on decimal column,"{code}
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.materializedview.rewriting.sql=false;

create table t1(a int, b decimal(7,2)) stored as orc TBLPROPERTIES ('transactional'='true');

insert into t1(a, b) values(1, 1);

create materialized view mat1 stored as orc TBLPROPERTIES ('transactional'='true') as
select t1.a, sum(t1.b) from t1
group by t1.a;

insert into t1(a,b) values(2, 5);

explain cbo alter materialized view mat1 rebuild;
{code}
{code}
java.lang.AssertionError: 
Cannot add expression of different type to set:
set type is RecordType(INTEGER $f0, DECIMAL(17, 2) $f1) NOT NULL
expression type is RecordType(INTEGER $f0, DECIMAL(18, 2) $f1) NOT NULL
set is rel#388:HiveAggregate.HIVE.[].any(input=HepRelVertex#387,group={0},agg#0=sum($1))
expression is HiveProject($f0=[$3], $f1=[CASE(IS NULL($1), $4, IS NULL($4), $1, +($4, $1))])
  HiveFilter(condition=[OR($2, IS NULL($2))])
    HiveJoin(condition=[IS NOT DISTINCT FROM($0, $3)], joinType=[right], algorithm=[none], cost=[not available])
      HiveProject(a=[$0], _c1=[$1], $f2=[true])
        HiveTableScan(table=[[default, mat1]], table:alias=[default.mat1])
      HiveAggregate(group=[{0}], agg#0=[sum($1)])
        HiveProject($f0=[$0], $f1=[$1])
          HiveFilter(condition=[<(1, $4.writeid)])
            HiveTableScan(table=[[default, t1]], table:alias=[t1])

	at org.apache.calcite.plan.RelOptUtil.verifyTypeEquivalence(RelOptUtil.java:380)
	at org.apache.calcite.plan.hep.HepRuleCall.transformTo(HepRuleCall.java:58)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:268)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:283)
	at org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveAggregateIncrementalRewritingRuleBase.onMatch(HiveAggregateIncrementalRewritingRuleBase.java:161)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.executeProgram(CalcitePlanner.java:2468)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.executeProgram(CalcitePlanner.java:2427)
	at org.apache.hadoop.hive.ql.ddl.view.materialized.alter.rebuild.AlterMaterializedViewRebuildAnalyzer$MVRebuildCalcitePlannerAction.applyIncrementalRebuild(AlterMaterializedViewRebuildAnalyzer.java:460)
	at org.apache.hadoop.hive.ql.ddl.view.materialized.alter.rebuild.AlterMaterializedViewRebuildAnalyzer$MVRebuildCalcitePlannerAction.applyAggregateInsertIncremental(AlterMaterializedViewRebuildAnalyzer.java:352)
	at org.apache.hadoop.hive.ql.ddl.view.materialized.alter.rebuild.AlterMaterializedViewRebuildAnalyzer$MVRebuildCalcitePlannerAction.applyRecordIncrementalRebuildPlan(AlterMaterializedViewRebuildAnalyzer.java:311)
	at org.apache.hadoop.hive.ql.ddl.view.materialized.alter.rebuild.AlterMaterializedViewRebuildAnalyzer$MVRebuildCalcitePlannerAction.applyMaterializedViewRewriting(AlterMaterializedViewRebuildAnalyzer.java:278)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:1722)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:1591)
	at org.apache.calcite.tools.Frameworks.lambda$withPlanner$0(Frameworks.java:131)
	at org.apache.calcite.prepare.CalcitePrepareImpl.perform(CalcitePrepareImpl.java:914)
	at org.apache.calcite.tools.Frameworks.withPrepare(Frameworks.java:180)
	at org.apache.calcite.tools.Frameworks.withPlanner(Frameworks.java:126)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.logicalPlan(CalcitePlanner.java:1343)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:570)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12824)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:465)
	at org.apache.hadoop.hive.ql.ddl.view.materialized.alter.rebuild.AlterMaterializedViewRebuildAnalyzer.analyzeInternal(AlterMaterializedViewRebuildAnalyzer.java:135)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:326)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:180)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:326)
	at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:224)
	at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:107)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:519)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:471)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:436)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:430)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:121)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:227)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:257)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd1(CliDriver.java:201)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:127)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:425)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:356)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:733)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:703)
	at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:115)
	at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:157)
	at org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver(TestMiniLlapLocalCliDriver.java:62)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.hadoop.hive.cli.control.CliAdapter$2$1.evaluate(CliAdapter.java:135)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.apache.hadoop.hive.cli.control.CliAdapter$1$1.evaluate(CliAdapter.java:95)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
{code}",kkasa,kkasa,Major,Resolved,Fixed,28/Apr/23 08:47,08/May/23 11:22
Bug,HIVE-27307,13534474,NPE when generating incremental rebuild plan of materialized view with empty Iceberg source table,"{code}
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

create external table tbl_ice(a int, b string, c int) stored by iceberg stored as orc tblproperties ('format-version'='1');
create external table tbl_ice_v2(d int, e string, f int) stored by iceberg stored as orc tblproperties ('format-version'='2');

insert into tbl_ice_v2 values (1, 'one v2', 50), (4, 'four v2', 53), (5, 'five v2', 54);

create materialized view mat1 as
select tbl_ice.b, tbl_ice.c, tbl_ice_v2.e from tbl_ice join tbl_ice_v2 on tbl_ice.a=tbl_ice_v2.d where tbl_ice.c > 52;

-- insert some new values to one of the source tables
insert into tbl_ice values (1, 'one', 50), (2, 'two', 51), (3, 'three', 52), (4, 'four', 53), (5, 'five', 54);

alter materialized view mat1 rebuild;
{code}
{code}
2023-04-28T07:34:17,949  WARN [1fb94a8e-8d75-4a1f-8f44-a5beaa8aafb6 Listener at 0.0.0.0/36857] rebuild.AlterMaterializedViewRebuildAnalyzer: Exception loading materialized views
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.metadata.Hive.getValidMaterializedViews(Hive.java:2298) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMaterializedViewForRebuild(Hive.java:2204) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.ddl.view.materialized.alter.rebuild.AlterMaterializedViewRebuildAnalyzer$MVRebuildCalcitePlannerAction.applyMaterializedViewRewriting(AlterMaterializedViewRebuildAnalyzer.java:215) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:1722) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:1591) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.calcite.tools.Frameworks.lambda$withPlanner$0(Frameworks.java:131) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.calcite.prepare.CalcitePrepareImpl.perform(CalcitePrepareImpl.java:914) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.calcite.tools.Frameworks.withPrepare(Frameworks.java:180) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.calcite.tools.Frameworks.withPlanner(Frameworks.java:126) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.logicalPlan(CalcitePlanner.java:1343) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:570) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12824) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:465) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.ddl.view.materialized.alter.rebuild.AlterMaterializedViewRebuildAnalyzer.analyzeInternal(AlterMaterializedViewRebuildAnalyzer.java:135) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:326) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:180) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:326) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:224) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:107) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:519) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:471) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:436) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:430) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:121) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:227) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:257) ~[hive-cli-4.0.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.cli.CliDriver.processCmd1(CliDriver.java:201) ~[hive-cli-4.0.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:127) ~[hive-cli-4.0.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:425) ~[hive-cli-4.0.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:356) ~[hive-cli-4.0.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:733) ~[hive-it-util-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:703) ~[hive-it-util-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:115) ~[hive-it-util-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:157) ~[hive-it-util-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.cli.TestIcebergLlapLocalCliDriver.testCliDriver(TestIcebergLlapLocalCliDriver.java:60) ~[test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_301]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_301]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_301]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_301]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) ~[junit-4.13.2.jar:4.13.2]
	at org.apache.hadoop.hive.cli.control.CliAdapter$2$1.evaluate(CliAdapter.java:135) ~[hive-it-util-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.Suite.runChild(Suite.java:128) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.Suite.runChild(Suite.java:27) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.2.jar:4.13.2]
	at org.apache.hadoop.hive.cli.control.CliAdapter$1$1.evaluate(CliAdapter.java:95) ~[hive-it-util-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.junit.rules.RunRules.evaluate(RunRules.java:20) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.2.jar:4.13.2]
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) ~[surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) ~[surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) ~[surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) ~[surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) ~[surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) ~[surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) ~[surefire-booter-3.0.0-M4.jar:3.0.0-M4]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) ~[surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: java.lang.NullPointerException
	at org.apache.iceberg.mr.hive.HiveIcebergStorageHandler.hasAppendsOnly(HiveIcebergStorageHandler.java:1303) ~[hive-iceberg-handler-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMaterializationInvalidationInfo(Hive.java:2151) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.metadata.Hive.getValidMaterializedViews(Hive.java:2247) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	... 73 more
{code}",kkasa,kkasa,Major,Resolved,Fixed,28/Apr/23 14:39,04/May/23 12:51
Bug,HIVE-27316,13534966,Select query on table with remote database returns NULL values with postgreSQL and Redshift data connectors,"*Brief Description:*

Few datatypes are not mapped from postgres/redshift to hive data types. Thus values for unmapped columns are shown as null.

 

*Steps to reproduce:*

*Redshift:*

1. create redshift connector, and create remote database with it.
{code:java}
create connector rscon1 type 'postgres' url 'jdbc:redshift://redshift.us-east-2.redshift.amazonaws.com:5439/dev?ssl=false&tcpKeepAlive=true' WITH DCPROPERTIES ('hive.sql.dbcp.username'='venu','hive.sql.dbcp.password'='Mypassword123','hive.sql.jdbc.driver'='com.amazon.redshift.jdbc.Driver','hive.sql.schema' = 'public');

create REMOTE database localdev1 using rscon1 with DBPROPERTIES(""connector.remoteDbName""=""dev"");
{code}
2. Create a test table and insert a row to redshit db through a jdbc client.
{code:java}
Class.forName(""com.amazon.redshift.jdbc.Driver"");
con = DriverManager.getConnection(
    ""jdbc:redshift://redshift.us-east-2.redshift.amazonaws.com:5439/dev?ssl=false&tcpKeepAlive=true"",
    ""venu"", ""Mypassword123"");

stmt = con.createStatement();
stmt.executeUpdate(""create table test (intvar int, int4var int4, integervar integer, smallintvar smallint, int2var int2, bigintvar bigint, int8var int8, boolvar bool, booleanvar boolean, floatvar float, float4var float4, realvar real, float8var float8, doubleprecisionvar double precision, numericvar numeric(8,3), charactervar character(14), ncharvar nchar(10), varcharvar varchar(30), charactervaryingvar character varying(20))"");
stmt.executeUpdate(""insert into test (intvar, int4var, integervar, smallintvar, int2var, bigintvar, int8var, boolvar, booleanvar, floatvar, float4var, realvar, float8var, doubleprecisionvar, numericvar, charactervar, ncharvar, varcharvar, charactervaryingvar) values (1, 10, 100, 2, 20, 3, 30, true, true, 1.234, 2.345, 3.456, 4.567, 5.678, 6.789, 'charactervar', 'ncharvar', 'varcharvar', 'charactervaryingvar')"");{code}
3. Execute select query on test table from beeline. NULL values are shown for the columns that are not mapped to hive data types.
{code:java}
0: jdbc:hive2://localhost:10000> use localdev1;
No rows affected (0.138 seconds)
0: jdbc:hive2://localhost:10000> select * from test;
+--------------+---------------+------------------+-------------------+---------------+-----------------+---------------+---------------+------------------+----------------+-----------------+---------------+-----------------+--------------------------+------------------+--------------------+----------------+------------------+---------------------------+
| test.intvar  | test.int4var  | test.integervar  | test.smallintvar  | test.int2var  | test.bigintvar  | test.int8var  | test.boolvar  | test.booleanvar  | test.floatvar  | test.float4var  | test.realvar  | test.float8var  | test.doubleprecisionvar  | test.numericvar  | test.charactervar  | test.ncharvar  | test.varcharvar  | test.charactervaryingvar  |
+--------------+---------------+------------------+-------------------+---------------+-----------------+---------------+---------------+------------------+----------------+-----------------+---------------+-----------------+--------------------------+------------------+--------------------+----------------+------------------+---------------------------+
| NULL         | NULL          | NULL             | NULL              | NULL          | 3               | 30            | NULL          | NULL             | NULL           | NULL            | NULL          | NULL            | NULL                     | 7                | NULL               | NULL           | varcharvar       | charactervaryingvar       |
+--------------+---------------+------------------+-------------------+---------------+-----------------+---------------+---------------+------------------+----------------+-----------------+---------------+-----------------+--------------------------+------------------+--------------------+----------------+------------------+---------------------------+
1 row selected (24.839 seconds)
0: jdbc:hive2://localhost:10000> 
{code}
 

*Postgres:*

1. create postgres connector, and create remote database with it.
{code:java}
create connector pscon1 type 'postgres' url 'jdbc:postgresql://postgres.us-east-2.rds.amazonaws.com:5432/postgres?ssl=false&tcpKeepAlive=true' WITH DCPROPERTIES ('hive.sql.dbcp.username'='venu','hive.sql.dbcp.password'='Mypassword123','hive.sql.jdbc.driver'='org.postgresql.Driver','hive.sql.schema' = 'public');

create REMOTE database localdevps1 using pscon1 with DBPROPERTIES(""connector.remoteDbName""=""postgres"");{code}
2. Create a test table and insert a row to postgre through a jdbc client.
{code:java}
Class.forName(""org.postgresql.Driver"");
con = DriverManager.getConnection(""jdbc:postgresql://postgres.us-east-2.rds.amazonaws.com:5432/postgres"",""venu"", ""Mypassword123"");

stmt = con.createStatement();
stmt.executeUpdate(""create table test (intvar int, int4var int4, integervar integer, smallintvar smallint, int2var int2, bigintvar bigint, int8var int8, boolvar bool, booleanvar boolean, floatvar float, float4var float4, realvar real, float8var float8, doubleprecisionvar double precision, numericvar numeric(8,3), charactervar character(14), ncharvar nchar(10), varcharvar varchar(30), charactervaryingvar character varying(20))"");
stmt.executeUpdate(""insert into test (intvar, int4var, integervar, smallintvar, int2var, bigintvar, int8var, boolvar, booleanvar, floatvar, float4var, realvar, float8var, doubleprecisionvar, numericvar, charactervar, ncharvar, varcharvar, charactervaryingvar) values (1, 10, 100, 2, 20, 3, 30, true, true, 1.234, 2.345, 3.456, 4.567, 5.678, 6.789, 'charactervar', 'ncharvar', 'varcharvar', 'charactervaryingvar')"");{code}
3. Execute select query on test table from beeline. NULL values are shown for the columns that are not mapped to hive data types.

 
{code:java}
0: jdbc:hive2://localhost:10000> use localdevps1;
0: jdbc:hive2://localhost:10000> select * from test;
+--------------+---------------+------------------+-------------------+---------------+-----------------+---------------+---------------+------------------+----------------+-----------------+---------------+-----------------+--------------------------+------------------+--------------------+----------------+------------------+---------------------------+
| test.intvar  | test.int4var  | test.integervar  | test.smallintvar  | test.int2var  | test.bigintvar  | test.int8var  | test.boolvar  | test.booleanvar  | test.floatvar  | test.float4var  | test.realvar  | test.float8var  | test.doubleprecisionvar  | test.numericvar  | test.charactervar  | test.ncharvar  | test.varcharvar  | test.charactervaryingvar  |
+--------------+---------------+------------------+-------------------+---------------+-----------------+---------------+---------------+------------------+----------------+-----------------+---------------+-----------------+--------------------------+------------------+--------------------+----------------+------------------+---------------------------+
| NULL         | NULL          | NULL             | NULL              | NULL          | 3               | 30            | NULL          | NULL             | NULL           | NULL            | NULL          | NULL            | NULL                     | 7                | charactervar       | ncharvar       | varcharvar       | charactervaryingvar       |
+--------------+---------------+------------------+-------------------+---------------+-----------------+---------------+---------------+------------------+----------------+-----------------+---------------+-----------------+--------------------------+------------------+--------------------+----------------+------------------+---------------------------+
1 row selected (69.075 seconds)
{code}
 ",VenuReddy,VenuReddy,Major,Resolved,Fixed,04/May/23 11:20,16/Jun/23 10:04
Bug,HIVE-27319,13535061,HMS server should throw InvalidObjectException in get_partitions_by_names() when the table is missing/dropped,"When the table object is dropped by a concurrent thread, the get_partitions_by_names_req() API is currently throwing a TApplicationException to the client. Instead, the HMS server should propagate the InvalidObjectException thrown by getTable() to the HMS client. By doing this, other services using HMS client will understand the exception better.",hemanth619,hemanth619,Major,Resolved,Fixed,04/May/23 23:33,22/May/23 16:51
Bug,HIVE-27327,13535390,Iceberg basic stats: Incorrect row count in snapshot summary leading to unoptimized plans,"In the absence of equality deletes, the total row count should be :
{noformat}
row_count = total-records - total-position-deletes{noformat}
 

 

Example:

After many inserts and deletes, there are only 46 records in a table.
{noformat}
>>select count(*) from llap_orders;
+------+
| _c0  |
+------+
| 46   |
+------+
1 row selected (7.22 seconds)

{noformat}
 

But the total records in snapshot summary indicate that there are 300 records

 
{noformat}
 {
    ""sequence-number"" : 19,
    ""snapshot-id"" : 4237525869561629328,
    ""parent-snapshot-id"" : 2572487769557272977,
    ""timestamp-ms"" : 1683553017982,
    ""summary"" : {
      ""operation"" : ""append"",
      ""added-data-files"" : ""5"",
      ""added-records"" : ""12"",
      ""added-files-size"" : ""3613"",
      ""changed-partition-count"" : ""5"",
      ""total-records"" : ""300"",
      ""total-files-size"" : ""164405"",
      ""total-data-files"" : ""100"",
      ""total-delete-files"" : ""73"",
      ""total-position-deletes"" : ""254"",
      ""total-equality-deletes"" : ""0""
    }{noformat}
 

As a result of this , the hive plans generated are unoptimized.
{noformat}
0: jdbc:hive2://simhadrigovindappa-2.simhadri> explain update llap_orders set itemid=7 where itemid=5;

INFO  : OK
+----------------------------------------------------+
|                      Explain                       |
+----------------------------------------------------+
| Vertex dependency in root stage                    |
| Reducer 2 <- Map 1 (SIMPLE_EDGE)                   |
| Reducer 3 <- Map 1 (SIMPLE_EDGE)                   |
|                                                    |
| Stage-4                                            |
|   Stats Work{}                                     |
|     Stage-0                                        |
|       Move Operator                                |
|         table:{""name:"":""db.llap_orders""}           |
|         Stage-3                                    |
|           Dependency Collection{}                  |
|             Stage-2                                |
|               Reducer 2 vectorized                 |
|               File Output Operator [FS_14]         |
|                 table:{""name:"":""db.llap_orders""}   |
|                 Select Operator [SEL_13] (rows=150 width=424) |
|                   Output:[""_col0"",""_col1"",""_col2"",""_col3"",""_col4"",""_col5"",""_col6"",""_col7"",""_col8"",""_col9""] |
|                 <-Map 1 [SIMPLE_EDGE]              |
|                   SHUFFLE [RS_4]                   |
|                     Select Operator [SEL_3] (rows=150 width=424) |
|                       Output:[""_col0"",""_col1"",""_col2"",""_col3"",""_col4"",""_col5"",""_col7"",""_col8"",""_col9""] |
|                       Select Operator [SEL_2] (rows=150 width=644) |
|                         Output:[""_col0"",""_col1"",""_col2"",""_col3"",""_col4"",""_col5"",""_col7"",""_col8"",""_col9"",""_col10"",""_col11"",""_col13"",""_col14"",""_col15""] |
|                         Filter Operator [FIL_9] (rows=150 width=220) |
|                           predicate:(itemid = 5)   |
|                           TableScan [TS_0] (rows=300 width=220) |
|                             db@llap_orders,llap_orders,Tbl:COMPLETE,Col:COMPLETE,Output:[""orderid"",""quantity"",""itemid"",""tradets"",""p1"",""p2""] |
|               Reducer 3 vectorized                 |
|               File Output Operator [FS_16]         |
|                 table:{""name:"":""db.llap_orders""}   |
|                 Select Operator [SEL_15]           |
|                   Output:[""_col0"",""_col1"",""_col2"",""_col3"",""_col4"",""_col5"",""_col4"",""_col5""] |
|                 <-Map 1 [SIMPLE_EDGE]              |
|                   SHUFFLE [RS_10]                  |
|                     PartitionCols:_col4, _col5     |
|                     Select Operator [SEL_7] (rows=150 width=220) |
|                       Output:[""_col0"",""_col1"",""_col2"",""_col3"",""_col4"",""_col5""] |
|                        Please refer to the previous Select Operator [SEL_2] |
|                                                    |
+----------------------------------------------------+
39 rows selected (0.104 seconds)
0: jdbc:hive2://simhadrigovindappa-2.simhadri>{noformat}
 

 ",simhadri-g,simhadri-g,Major,Resolved,Fixed,08/May/23 14:55,16/Jun/23 10:04
Bug,HIVE-27331,13535738,Iceberg: Rows are not deleted from table when execution mode is vectorized llap,Rows aren't getting deleted in case the iceberg table is of ORC format and using vectorisation & llap ,kkasa,ayushtkn,Major,Resolved,Fixed,11/May/23 07:24,16/May/23 13:24
Bug,HIVE-27337,13535887,create and alter queries should trim empty spaces in the request objects.,"If starting/trailing empty strings are being provided in the query (within the quotes),
Create table request (event) is currently being inserted into the notification log table. This would be a problem for listeners subscribed to the HMS notification log. So it would be ideal to trim these whitespaces and then insert it into the notification log. 

Alter table query is throwing an error [here|https://github.com/apache/hive/blob/master/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java#L174]. The ideal behavior should be to ignore the whitespaces and proceed with alter request.",hemanth619,hemanth619,Major,Resolved,Fixed,12/May/23 00:10,09/Jun/23 04:12
Bug,HIVE-27338,13535939,JDBC drivers are transitively inherited although declared optional,"The JDBC driver dependencies are declared optional (HIVE-25701) because we don't really need them and it is up to the end-user to decide which DBMS they will use and pick the appropriate driver for their use case. 

Also declaring them as [optional|https://maven.apache.org/guides/introduction/introduction-to-optional-and-excludes-dependencies.html] should stop their transitive propagation to 3rd-party projects that depend on Hive modules.

Currently the optional declaration is in the respective {{dependencyManagement}} sections of the project:
* https://github.com/apache/hive/blob/dd8a867386b605ecd7e8dbec476556bf24f54c6f/pom.xml#L1323
* https://github.com/apache/hive/blob/dd8a867386b605ecd7e8dbec476556bf24f54c6f/standalone-metastore/pom.xml#L352

However due to MNG-5227, the optional declaration in the {{dependencyManagement}} has no effect so JDBC dependencies are transitively propagated to every module that depends on some Hive module with a JDBC driver dependency. 

Till MNG-5227 is fixed we will need to include optional in every explicit declaration of the dependency.",zabetak,zabetak,Major,Resolved,Fixed,12/May/23 09:15,16/Jun/23 10:04
Bug,HIVE-27340,13536088,ThreadPool in HS2 over HTTP should respect the customized ThreadFactory,"In Jetty, ExecutorThreadPool will override the ThreadFactory of ThreadPoolExecutor even though the ThreadPoolExecutor has already initialized the ThreadFactory, 
{code:java}
_executor.setThreadFactory(this::newThread); {code}
Need to ignore such action as we have injected a ThreadFactory into the ThreadPoolExecutor.",dengzh,dengzh,Minor,Resolved,Fixed,14/May/23 01:43,16/Jun/23 10:04
Bug,HIVE-27344,13536203,ORC RecordReaderImpl throws NPE when close() is called from the constructor,"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl throws NullPointerException when the constructor of its super class org.apache.orc.impl.RecordReaderImpl calls this.close().

close() mutates the field 'batch', which is initialized after super class constructor is done. Therefore, we need a null check of field 'batch' in close(). ",seonggon,seonggon,Major,Resolved,Fixed,15/May/23 09:58,22/Jun/23 08:34
Bug,HIVE-27350,13536476,Unable to Create Table with 380+ Columns,"When we try to Iceberg table via Hive MetaStore with 388 Columns we get below exception:
{code:java}
org.apache.hadoop.hive.metastore.api.MetaException: Add request failed : INSERT INTO `COLUMNS_V2` (`CD_ID`,`COMMENT`,`COLUMN_NAME`,`TYPE_NAME`,`INTEGER_IDX`) VALUES (?,?,?,?,?) 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:54908) ~[hive-standalone-metastore-3.1.3.jar:3.1.3]	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:54876) ~[hive-standalone-metastore-3.1.3.jar:3.1.3]	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:54802) ~[hive-standalone-metastore-3.1.3.jar:3.1.3]	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[libthrift-0.9.3.jar:0.9.3]	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1556) ~[hive-standalone-metastore-3.1.3.jar:3.1.3]	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1542) ~[hive-standalone-metastore-3.1.3.jar:3.1.3]	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2867) ~[hive-standalone-metastore-3.1.3.jar:3.1.3]	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:837) ~[hive-standalone-metastore-3.1.3.jar:3.1.3]	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:822) ~[hive-standalone-metastore-3.1.3.jar:3.1.3]	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) ~[?:?]	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]	at java.lang.reflect.Method.invoke(Method.java:568) ~[?:?]	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:208) ~[hive-standalone-metastore-3.1.3.jar:3.1.3]	at jdk.proxy2.$Proxy20.createTable(Unknown Source) ~[?:?]	at org.apache.iceberg.hive.HiveTableOperations.lambda$persistTable$4(HiveTableOperations.java:405) ~[iceberg-spark-runtime-3.2_2.12-1.1.0.jar:?]	at org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:58) ~[iceberg-spark-runtime-3.2_2.12-1.1.0.jar:?]	at org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:51) ~[iceberg-spark-runtime-3.2_2.12-1.1.0.jar:?]	at org.apache.iceberg.hive.CachedClientPool.run(CachedClientPool.java:82) ~[iceberg-spark-runtime-3.2_2.12-1.1.0.jar:?]	at org.apache.iceberg.hive.HiveTableOperations.persistTable(HiveTableOperations.java:403) ~[iceberg-spark-runtime-3.2_2.12-1.1.0.jar:?]	at org.apache.iceberg.hive.HiveTableOperations.doCommit(HiveTableOperations.java:327) ~[iceberg-spark-runtime-3.2_2.12-1.1.0.jar:?]	at org.apache.iceberg.BaseMetastoreTableOperations.commit(BaseMetastoreTableOperations.java:135) ~[iceberg-spark-runtime-3.2_2.12-1.1.0.jar:?]	at org.apache.iceberg.BaseMetastoreCatalog$BaseMetastoreCatalogTableBuilder.create(BaseMetastoreCatalog.java:196) ~[iceberg-spark-runtime-3.2_2.12-1.1.0.jar:?]
{code}
When we reduce columns size 380, Table is created. Our metastore DB is Postgres.

Validated SD_PARAMS and SERDE_PARAMS table, both table has PARAM_VALUE as TEXT datatype only.

This is specific to COLUMNS_V2 table. Changed TYPE_NAME to TEXT type as well and Restarted Metastore but didn't work.

For reference Below is the Table Object(toString)
{code:java}
Table(tableName:credit_bureau, dbName:lz_agrinetwork, owner:karthikeyankarunanithi-NC23599, createTime:587637, lastAccessTime:587637, retention:2147483647, sd:StorageDescriptor(cols:[FieldSchema(name:creditBureauRating_num_dpd_0_instances_l3m_al, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l6m_al, type:double, comment:null), FieldSchema(name:computedCreditScore, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l6m_cc, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l12m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l12m_gl, type:double, comment:null), FieldSchema(name:creditBureauRating_balc_sanc_amt_ratio_total, type:double, comment:null), FieldSchema(name:creditBureauRating_count_60plus_12m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l6m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_count_sec_loans_opened_3m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l6m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_24m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_ever_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_count_HL_open, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_ever_twl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_6m_HL, type:double, comment:null), FieldSchema(name:creditBureauRating_count_OD_ever, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_ever_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l3m_bl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l3m, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_12m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_12m_GL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l6m_bl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_36m_HL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_ever_cvl, type:double, comment:null), FieldSchema(name:creditBureauRating_count_open_unsecured, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_6m_GL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_ever, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l12m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l12m_cc, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l12m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l24m, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_36m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_max_DPD_12m_CC, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l6m, type:double, comment:null), FieldSchema(name:creditBureauRating_balc_sanc_amt_ratio_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_Total_unsec_sanc_amount_6m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l12m_cvl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_active_overdue_loans_l24m, type:double, comment:null), FieldSchema(name:creditBureauRating_count_loans_closed_6m, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_12m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l24m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l12m, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_12m_HL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l12m_cc, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l6m_cc, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_active_l24m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l3m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l3m_cc, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_ever_twl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l6m_al, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l6m_pl, type:double, comment:null), FieldSchema(name:realmId, type:string, comment:null), FieldSchema(name:creditBureauRating_count_closed_loans, type:double, comment:null), FieldSchema(name:creditBureauRating_cc_highcredit_ratio, type:double, comment:null), FieldSchema(name:creditBureauRating_count_AL_open, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l12m_bl, type:double, comment:null), FieldSchema(name:creditBureauRating_count_active_closed_6m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_ever_cvl, type:double, comment:null), FieldSchema(name:userId, type:string, comment:null), FieldSchema(name:creditBureauRating_Ratio_opened_closed_loans_3m, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_6m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l6m_bl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l12m_twl, type:double, comment:null), FieldSchema(name:creditBureauRating_active_cc_limit, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l3m_pl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_12m_BL, type:double, comment:null), FieldSchema(name:creditBureauRating_cc_balc_highcredit_ratio, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l12m_cc, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l12m_al, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l6m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l12m_al, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_ever_BL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l12m_bl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l3m, type:double, comment:null), FieldSchema(name:creditBureauRating_max_emi_l24m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l6m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l12m_bl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_ever_AL, type:double, comment:null), FieldSchema(name:creditBureauRating_count_loans_closed_3m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l12m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_ratio_PL_open_total, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l3m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_12m_CC, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l12m_al, type:double, comment:null), FieldSchema(name:creditBureauRating_count_GL_ever, type:double, comment:null), FieldSchema(name:creditBureauRating_ratio_open_loans, type:double, comment:null), FieldSchema(name:creditBureauRating_count_sec_loans_opened_6m, type:double, comment:null), FieldSchema(name:creditBureauRating_active_od_bl_limit, type:double, comment:null), FieldSchema(name:creditBureauRating_Ratio_0plus_6m_12m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l6m_al, type:double, comment:null), FieldSchema(name:creditBureauRating_Ratio_opened_closed_loans_6m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l12m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l6m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_3m_AL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l3m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l6m_bl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_3m_BL, type:double, comment:null), FieldSchema(name:externalId, type:string, comment:null), FieldSchema(name:creditBureauRating_thick_thin_flag, type:string, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_6m, type:double, comment:null), FieldSchema(name:creditBureauRating_ratio_HL_open_total, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_15_instances_l3m, type:double, comment:null), FieldSchema(name:creditBureauRating_ratio_BL_open_total, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l12m_pl, type:double, comment:null), FieldSchema(name:creditBureauRating_suit_filed_ever_flag, type:boolean, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l6m_gl, type:double, comment:null), FieldSchema(name:creditBureauRating_balc_sanc_amt_ratio_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_3m_CVL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l3m_cvl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_loan_facilities_availed_l3m, type:double, comment:null), FieldSchema(name:creditBureauRating_cc_utilization_ratio, type:double, comment:null), FieldSchema(name:id, type:string, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_3m_TWL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l3m_twl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_24m_PL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l6m_hl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l12m_al, type:double, comment:null), FieldSchema(name:creditBureauRating_count_PL_open, type:double, comment:null), FieldSchema(name:creditBureauRating_count_CC_ever, type:double, comment:null), FieldSchema(name:creditBureauRating_report_date, type:string, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_ever_twl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l12m_twl, type:double, comment:null), FieldSchema(name:creditBureauRating_count_loans_doubtful, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l3m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_closed_l24m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l12m_cvl, type:double, comment:null), FieldSchema(name:creditBureauRating_settled_ever_flag, type:boolean, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l3m_cvl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_3m, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_6m_BL, type:double, comment:null), FieldSchema(name:creditBureauRating_count_LABD_open, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l12m_bl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_36m_CC, type:double, comment:null), FieldSchema(name:creditBureauRating_wrt_off_setld_ever_flag, type:boolean, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l3m_twl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l6m_hl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l12m_cc, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l12m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_ever_cvl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_36m_BL, type:double, comment:null), FieldSchema(name:creditBureauRating_count_loans_suit_filed, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_12m_AL, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_ever_CC, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_6m_AL, type:double, comment:null), FieldSchema(name:creditBureauRating_Total_sec_sanc_amount_36m, type:double, comment:null), FieldSchema(name:creditBureauRating_Total_sec_sanc_amount_24m, type:double, comment:null), FieldSchema(name:creditBureauRating_Total_sec_sanc_amount_12m, type:double, comment:null), FieldSchema(name:creditBureauRating_jlg_loan_bal_amt, type:double, comment:null), FieldSchema(name:creditBureauRating_count_unsecured, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l6m_gl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_36m_AL, type:double, comment:null), FieldSchema(name:creditBureauRating_count_BL_ever, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_ever_CVL, type:double, comment:null), FieldSchema(name:creditBureauRating_count_loans_closed_36m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l3m_pl, type:double, comment:null), FieldSchema(name:creditBureauRating_count_loans_closed_24m, type:double, comment:null), FieldSchema(name:creditBureauRating_count_loans_closed_12m, type:double, comment:null), FieldSchema(name:creditBureauRating_count_loans_wrt_off_sttld, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l3m_bl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l3m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l6m_gl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l6m_cc, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l6m_bl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_ever_TWL, type:double, comment:null), FieldSchema(name:creditBureauRating_count_AL_ever, type:double, comment:null), FieldSchema(name:creditBureauRating_Ratio_0plus_3m_ever, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l3m_cc, type:double, comment:null), FieldSchema(name:creditBureauRating_count_open_secured, type:double, comment:null), FieldSchema(name:creditBureauRating_balc_sanc_amt_ratio_PL, type:double, comment:null), FieldSchema(name:creditBureauRating_ratio_0plus_3m_12m, type:double, comment:null), FieldSchema(name:creditBureauRating_current_overdue, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l3m_al, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_ever, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l6m_al, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l6m_hl, type:double, comment:null), FieldSchema(name:creditBureauRating_count_credit_lines, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_6m_CC, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l12m_gl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_ever_HL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l3m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_36m_GL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_ever_pl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_ever_pl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_ever_al, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l12m_gl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l3m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l12m_hl, type:double, comment:null), FieldSchema(name:creditBureauRating_balc_sanc_amt_ratio_LABD, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_ever_GL, type:double, comment:null), FieldSchema(name:creditBureauRating_ratio_GL_open_total, type:double, comment:null), FieldSchema(name:creditBureauRating_Total_sec_sanc_amount_3m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_ever_sec, type:double, comment:null), FieldSchema(name:vintageWithNinjacart, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_36m, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_24m, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_12m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l6m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l6m_cc, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l12m_hl, type:double, comment:null), FieldSchema(name:creditBureauRating_count_loans, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_ever_bl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l6m_cvl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_24m_CVL, type:double, comment:null), FieldSchema(name:creditBureauRating_count_BL_open, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_ever_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_count_LABD_ever, type:double, comment:null), FieldSchema(name:creditBureauRating_active_od_limit, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l3m_al, type:double, comment:null), FieldSchema(name:creditBureauRating_count_CC_open, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_24m_HL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l6m_twl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l6m_cvl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_ever_al, type:double, comment:null), FieldSchema(name:creditBureauRating_count_loans_loss, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l3m_cc, type:double, comment:null), FieldSchema(name:creditBureauRating_Total_sanc_amount_12m, type:double, comment:null), FieldSchema(name:creditBureauRating_active_emi, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_ever_al, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_ever_cc, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l6m_pl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l6m_twl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_36m_TWL, type:double, comment:null), FieldSchema(name:creditBureauRating_updated_ts, type:string, comment:null), FieldSchema(name:creditBureauRating_Total_sanc_amount_36m, type:double, comment:null), FieldSchema(name:creditBureauRating_Total_sanc_amount_24m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_ever_cc, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l3m_bl, type:double, comment:null), FieldSchema(name:creditBureauRating_ratio_open_secured, type:double, comment:null), FieldSchema(name:creditBureauRating_Ratio_0plus_3m_6m, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_24m_GL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_ever_bl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_12m_CVL, type:double, comment:null), FieldSchema(name:creditBureauRating_count_loans_opened_3m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_ever_bl, type:double, comment:null), FieldSchema(name:creditBureauRating_balc_sanc_amt_ratio_CL, type:double, comment:null), FieldSchema(name:creditBureauRating_Total_sec_sanc_amount_6m, type:double, comment:null), FieldSchema(name:creditBureauRating_count_PL_ever, type:double, comment:null), FieldSchema(name:creditBureauRating_ratio_open_unsecured, type:double, comment:null), FieldSchema(name:creditBureauRating_Total_unsec_sanc_amount_36m, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_12m_TWL, type:double, comment:null), FieldSchema(name:creditBureauRating_count_GL_open, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_6m_CVL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l6m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_6m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_ever_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l3m_al, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_36m_CVL, type:double, comment:null), FieldSchema(name:creditBureauRating_balc_sanc_amt_ratio_BL, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_6m_TWL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_ever_cc, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l6m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l12m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l3m_bl, type:double, comment:null), FieldSchema(name:creditBureauRating_Total_unsec_sanc_amount_12m, type:double, comment:null), FieldSchema(name:creditBureauRating_Total_unsec_sanc_amount_24m, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_24m_TWL, type:double, comment:null), FieldSchema(name:creditBureauRating_balc_sanc_amt_ratio_AL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l6m_twl, type:double, comment:null), FieldSchema(name:source, type:string, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_ever_hl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_ever_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l12m_cvl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l6m_cvl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l6m_pl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l3m_gl, type:double, comment:null), FieldSchema(name:creditBureauRating_balc_sanc_amt_ratio_HL, type:double, comment:null), FieldSchema(name:creditBureauRating_max_cc_limit, type:double, comment:null), FieldSchema(name:creditBureauRating_count_loans_opened_6m, type:double, comment:null), FieldSchema(name:creditBureauRating_jlg_loan_active_emi, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l12m_twl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_ever_gl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_ever_gl, type:double, comment:null), FieldSchema(name:creditBureauRating_count_secured, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_3m_PL, type:double, comment:null), FieldSchema(name:creditBureauRating_Ratio_opened_closed_loans_12m, type:double, comment:null), FieldSchema(name:creditBureauRating_Ratio_opened_closed_loans_24m, type:double, comment:null), FieldSchema(name:creditBureauRating_balc_sanc_amt_ratio_GL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l3m_hl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l12m_pl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_ever_hl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l6m_cvl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l3m_hl, type:double, comment:null), FieldSchema(name:creditBureauRating_doubtful_ever_flag, type:boolean, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_ever_hl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l12m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_Ratio_opened_closed_loans_36m, type:double, comment:null), FieldSchema(name:creditBureauRating_ratio_secured_loans, type:double, comment:null), FieldSchema(name:creditBureauRating_count_write_off_loans, type:double, comment:null), FieldSchema(name:creditBureauRating_count_OD_open, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_ever_gl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l6m_twl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l3m_cc, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l12m_pl, type:double, comment:null), FieldSchema(name:creditBureauRating_count_settled_loans, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l3m_gl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l12m_pl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l6m_pl, type:double, comment:null), FieldSchema(name:creditBureauRating_write_off_ever_flag, type:boolean, comment:null), FieldSchema(name:creditBureauRating_count_open_loans, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_ever_gl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_ever_hl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l3m_pl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_delinquent_l24m, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_ever_PL, type:double, comment:null), FieldSchema(name:vintageBusinessName, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l3m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_write_off_settled_l24m, type:double, comment:null), FieldSchema(name:creditBureauRating_loss_ever_flag, type:boolean, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_3m_CC, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_6m_PL, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_24m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_count_HL_ever, type:double, comment:null), FieldSchema(name:creditBureauRating_Ratio_0plus_3m_12m, type:double, comment:null), FieldSchema(name:creditBureauRating_balc_sanc_amt_ratio_CVL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l12m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_ever_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_unsecured_sanction_flag, type:boolean, comment:null), FieldSchema(name:creditBureauRating_Total_sanc_amount_3m, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_36m_PL, type:double, comment:null), FieldSchema(name:creditBureauRating_balc_sanc_amt_ratio_TWL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l12m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l6m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_ratio_LABD_open_total, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_3m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l12m_cvl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_12m_PL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l3m_gl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l3m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l12m_twl, type:double, comment:null), FieldSchema(name:creditBureauRating_active_od_bl_flag, type:boolean, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_3m_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l3m_hl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_3m_GL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_ever_pl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_ever_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_Total_sanc_amount_6m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_1_instances_l3m, type:double, comment:null), FieldSchema(name:creditBureauRating_ratio_CC_open_total, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_ever_al, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l6m, type:double, comment:null), FieldSchema(name:creditBureauRating_count_sec_loans_opened_24m, type:double, comment:null), FieldSchema(name:creditBureauRating_count_sec_loans_opened_36m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l12m, type:double, comment:null), FieldSchema(name:creditBureauRating_count_sec_loans_opened_12m, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_3m_HL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l12m_hl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_ever_cc, type:double, comment:null), FieldSchema(name:creditBureauRating_Total_unsec_sanc_amount_3m, type:double, comment:null), FieldSchema(name:creditBureauRating_bureau_score, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_ever_bl, type:double, comment:null), FieldSchema(name:count_active_closed_6m, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_24m_CC, type:double, comment:null), FieldSchema(name:creditBureauRating_expiry_date, type:string, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l6m_gl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l12m_gl, type:double, comment:null), FieldSchema(name:creditBureauRating_count_loans_opened_36m, type:double, comment:null), FieldSchema(name:creditBureauRating_count_loans_opened_24m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_ever_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_write_off_l24m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l3m_cvl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l3m_cvl, type:double, comment:null), FieldSchema(name:creditBureauRating_total_sanc_amount_3m, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_24m_BL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_ever_pl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l3m_twl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l3m_gl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l3m_twl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_36m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_balc_sanc_amt_ratio_LAS, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l6m_hl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l12m_hl, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_ever_sec, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_ever, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_ever_cvl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_ever, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_ever, type:double, comment:null), FieldSchema(name:creditBureauRating_ratio_AL_open_total, type:double, comment:null), FieldSchema(name:creditBureauRating_Max_DPD_24m_AL, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_60_instances_l6m_unsec, type:double, comment:null), FieldSchema(name:creditBureauRating_count_loans_opened_12m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_90_instances_l3m, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_l3m_pl, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_0_instances_l3m_hl, type:double, comment:null), FieldSchema(name:creditBureauRating_bureau_vintage, type:double, comment:null), FieldSchema(name:creditBureauRating_num_dpd_30_instances_ever_twl, type:double, comment:null), FieldSchema(name:sys__processed_ts, type:timestamp, comment:null), FieldSchema(name:sys__event_id, type:string, comment:null)], location:abfs://lakehouseprod@ncetldata.dfs.core.windows.net/landingzone/lz_agrinetwork.db/credit_bureau, inputFormat:org.apache.hadoop.mapred.FileInputFormat, outputFormat:org.apache.hadoop.mapred.FileOutputFormat, compressed:false, numBuckets:0, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{}), bucketCols:null, sortCols:null, parameters:null), partitionKeys:[], parameters:{EXTERNAL=TRUE, write.format.default=parquet, write.metadata.previous-versions-max=5, metadata_location=abfs://lakehouseprod@ncetldata.dfs.core.windows.net/landingzone/lz_agrinetwork.db/credit_bureau/metadata/00000-d32267f0-61ba-4a3a-8571-b9aa24b7f921.metadata.json, write.metadata.delete-after-commit.enabled=true, default-partition-spec={""spec-id"":0,""fields"":[{""name"":""sys__processed_ts_hour"",""transform"":""hour"",""source-id"":387,""field-id"":1000}]}, snapshot-count=0, uuid=f491f592-d865-4fee-b69e-852bcd82cfca, table_type=ICEBERG}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, catName:hive, ownerType:USER){code}
 ",karthik1289,karthik1289,Blocker,Resolved,Fixed,17/May/23 01:32,17/May/23 11:16
Bug,HIVE-27365,13537154,Fix test acid_bloom_filter_orc_file_dump,"This test dumps orc file data and the table in the test has 2 delta directories with one orc file in each
Hive has a posthook which scans all the directories of a table and dumps all orc files to the output but the order of the directory list is not deterministic.

https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/hooks/PostExecOrcFileDump.java",kkasa,kokila19,Minor,Resolved,Fixed,22/May/23 11:10,23/May/23 08:59
Bug,HIVE-27366,13537160,Incorrect incremental rebuild mode shown of materialized view with Iceberg sources,"{code}
CREATE TABLE shtb_test1(KEY INT, VALUE STRING) PARTITIONED BY(ds STRING)
stored by iceberg stored as orc tblproperties ('format-version'='2');

CREATE MATERIALIZED VIEW shtb_test1_view1 stored by iceberg stored as orc tblproperties ('format-version'='1') AS
SELECT * FROM shtb_test1 where KEY > 1000 and KEY < 2000;

SHOW MATERIALIZED VIEWS;
{code}
{code}
# MV Name           	Rewriting Enabled   	Mode                	Incremental rebuild 
shtb_test1_view1    	Yes                 	Manual refresh      	Available           
{code}

It should be 
{code}
# MV Name           	Rewriting Enabled   	Mode                	Incremental rebuild 
shtb_test1_view1    	Yes                 	Manual refresh      	Available in presence of insert operations only
{code}
because deleted rows can not be identified in case of Iceberg source tables.",kkasa,kkasa,Major,Resolved,Fixed,22/May/23 11:31,01/Jun/23 08:28
Bug,HIVE-27373,13537646,Unable to pushdown partition filter with unquoted date literal type to metastore,"After HIVE-26787 and HIVE-26778 filter expressions with unquoted date literals and timestamp literals without type name are not supported when calling metastore api {{getPartitionsByExpr}}
Example
{code}
(j = 1990-11-12 or j = 1990-11-11 or j = 1990-11-10)
{code}",kkasa,kkasa,Major,Resolved,Fixed,25/May/23 11:30,07/Jun/23 13:08
Bug,HIVE-27374,13537674,Exception while getting kafka delegation tokens in Kerberos/SSL enabled clusters,"When Hiveserver2 is in a secure cluster (e.g., Kerberos) and Kafka brokers have Kerberos and SSL enabled (SASL_SSL) queries will fail while trying to obtain a delegation token.

To reproduce the problem create a cluster with Kerberos and SSL enabled and do the following:

{code:sql}
CREATE EXTERNAL TABLE person
(`msg` string)
STORED BY 'org.apache.hadoop.hive.kafka.KafkaStorageHandler'
TBLPROPERTIES
('kafka.topic' = 'person_topic', 'kafka.bootstrap.servers'='127.0.0.1:9093',
'kafka.consumer.sasl.kerberos.service.name'='kafka',
'kafka.consumer.security.protocol'='SASL_SSL',
'kafka.serde.class'='org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' );

SELECT COUNT(1) FROM person;
{code}

In an internal Hive fork the exception is the following:

{noformat}
2023-05-18 14:15:47,058 ERROR org.apache.hadoop.hive.ql.exec.tez.TezTask: [HiveServer2-Background-Pool: Thread-1430715]: Failed to execute tez graph.
java.lang.RuntimeException: Exception while getting kafka delegation tokens
        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getKafkaDelegationTokenForBrokers(DagUtils.java:386) ~[hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.collectKafkaDelegationTokenForTableDesc(DagUtils.java:349) ~[hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getKafkaCredentials(DagUtils.java:316) ~[hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.addCredentials(DagUtils.java:290) ~[hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:522) ~[hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:229) [hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213) [hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) [hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:357) [hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:330) [hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246) [hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:109) [hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:749) [hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:504) [hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:498) [hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:166) [hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:226) [hive-service-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:88) [hive-service-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:327) [hive-service-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_232]
        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_232]
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898) [hadoop-common-3.1.1.7.1.7.1000-141.jar:?]
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:345) [hive-service-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_232]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_232]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_232]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_232]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_232]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_232]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_232]
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=createDelegationToken, deadlineMs=1684390547054) timed out at 1684390547055 after 1 attempt(s)
        at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45) ~[kafka-clients-2.5.0.7.1.7.1000-141.jar:?]
        at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32) ~[kafka-clients-2.5.0.7.1.7.1000-141.jar:?]
        at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89) ~[kafka-clients-2.5.0.7.1.7.1000-141.jar:?]
        at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260) ~[kafka-clients-2.5.0.7.1.7.1000-141.jar:?]
        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getKafkaDelegationTokenForBrokers(DagUtils.java:384) ~[hive-exec-3.1.3000.7.1.7.1000-141.jar:3.1.3000.7.1.7.1000-141]
        ... 29 more
Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=createDelegationToken, deadlineMs=1684390547054) timed out at 1684390547055 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment.
{noformat}


I could also reproduce it with a unit test in current master and there the exception looks like below:
{noformat}
java.lang.RuntimeException: Exception while getting kafka delegation tokens
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getKafkaDelegationTokenForBrokers(DagUtils.java:387)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.collectKafkaDelegationTokenForTableDesc(DagUtils.java:350)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getKafkaCredentials(DagUtils.java:326)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.addCredentials(DagUtils.java:291)
	at org.apache.hadoop.hive.ql.exec.tez.TestDagUtilsKafkaCredentials$1.run(TestDagUtilsKafkaCredentials.java:151)
	at org.apache.hadoop.hive.ql.exec.tez.TestDagUtilsKafkaCredentials$1.run(TestDagUtilsKafkaCredentials.java:148)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.hive.ql.exec.tez.TestDagUtilsKafkaCredentials.testAddCredentialsForKafka(TestDagUtilsKafkaCredentials.java:148)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: createDelegationToken
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getKafkaDelegationTokenForBrokers(DagUtils.java:385)
	... 39 more
Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: createDelegationToken
{noformat}
",zabetak,zabetak,Major,Resolved,Fixed,25/May/23 13:35,23/Jun/23 07:22
Bug,HIVE-27375,13537748,SharedWorkOptimizer assigns a common cache key to MapJoin operators that should not share MapJoin tables,"When hive.optimize.shared.work.mapjoin.cache.reuse is set to true, SharedWorkOptimizer sometimes assigns a common cache key to MapJoin operators that should not share MapJoin tables. This bug occurs only for MapJoin operators with 3 or more parent operators.

Example:
MAPJOIN[575] (RS_83, GBY_66, RS_85)
MAPJOIN[585] (RS_212, RS_213, GBY_210)

In this example, both MAPJOIN[575] and MAPJOIN[585] have three parent operators. The current implementation assigns a common cache key to MAPJOIN[575] and MAPJOIN[585] because RS_83 are RS_212 are equivalent.

However, MAPJOIN[575] uses GBY_66 for its big table whereas MAPJOIN[585] uses GBY_210 for its big table. As a result, the MapJoin table loaded by one operator cannot be used by the other.
",seonggon,glapark,Major,Resolved,Fixed,26/May/23 05:04,16/Jun/23 10:04
Bug,HIVE-27392,13538184,Iceberg: Use String instead of Long for file length in HadoopInputFile,"Apply the workaround mentioned over here:

https://issues.apache.org/jira/browse/HADOOP-18724?focusedCommentId=17718087&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17718087

 

 ",ayushtkn,ayushtkn,Major,Resolved,Fixed,31/May/23 04:22,16/Jun/23 10:04
Bug,HIVE-27398,13538430,SHOW CREATE TABLE doesn't output backticks for CLUSTERED by Col names,"SHOW CREATE TABLE output uses backticks for all column names and partition column names but does not include backticks for CLUSTERED BY column names. This causes ParseException during table creation when any bucket column identifier matches reserved keywords 
{code:java}
CREATE TABLE `test_ts_reserved_keyword7`(
`member_id` varchar(8),
`plan_nr` varchar(11),
`timestamp` timestamp,
`shared_ind` varchar(1),
`user_id` varchar(8))
CLUSTERED BY (
member_nr,
plan_nr,
`timestamp`)
INTO 4 BUCKETS;

SHOW CREATE TABLE test_ts_reserved_keyword7;
CREATE TABLE `test_ts_reserved_keyword7`(
`member_id` varchar(8),
`plan_nr` varchar(11),
`timestamp` timestamp, 
`shared_ind` varchar(1), 
`user_id` varchar(8)) 
CLUSTERED BY (
member_id,
plan_nr,
timestamp)
INTO 4 BUCKETS
ROW FORMAT SERDE'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'STORED AS INPUTFORMAT'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'OUTPUTFORMAT'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';

This fails with ""Error while compiling statement: FAILED: ParseException line 13:0 cannot recognize input near 'timestamp' ')' 'INTO' in column name""{code}
 ",rtrivedi12,rtrivedi12,Minor,Resolved,Fixed,01/Jun/23 14:24,16/Jun/23 10:04
Bug,HIVE-27408,13538689,Parquet file opened for reading stats is never closed,"ParquetRecordWriterWrapper while closing the writer tries to collect the stats by creating a reader (opening the file). But it never closes the reader (never closes the file). This can leave the file open hence consuming memory and associated file handle resources.

[https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java#L143-L155]",sbadhya,sbadhya,Major,Resolved,Fixed,04/Jun/23 04:46,16/Jun/23 10:03
Bug,HIVE-27413,13538876,Bump jettison from 1.5.3 to 1.5.4,"PR from *[dependabot|https://github.com/apps/dependabot]*

Creating for tracking",,ayushtkn,Major,Resolved,Fixed,06/Jun/23 07:46,16/Jun/23 10:04
Bug,HIVE-27438,13539980,Audit leader election event failed in non-appendable filesystems,"If the underlying file system of the warehouse is S3 or others based which cannot support append operation, then auditing the leader info to the remote file could be failed. For example:

org.apache.hadoop.hive.metastore.HiveMetaStore: [Leader-Watcher-housekeeping1]: Error while writing the leader info, path: s3a://.../leader_housekeeping.json

java.lang.UnsupportedOperationException: Append is not supported by S3AFileSystem

As a result, the audit logs would be missing and the user cannot be able to see the history changes of leader any more.",dengzh,dengzh,Major,Resolved,Fixed,14/Jun/23 06:25,11/Jul/23 06:14
Bug,HIVE-27439,13540001,"Support space in Decimal(precision, scale) format","Now, TypeInfoUtils#parseType only support Decimal(precision,scale) instead of Decimal(precision, scale), for example, support decimal(10,2) instead of decimal(10, 2) with a blank before 2, however, users may create decimal in this format decimal(10, 2) and it will throw exception in 

```

Integer.parseInt(params[1]);

```

we need fix this issue.",xleesf,xleesf,Minor,Resolved,Fixed,14/Jun/23 07:47,23/Jun/23 12:51
Bug,HIVE-27441,13540095,"Iceberg: CTLT with source table as Hive managed table fails with ""The table must be stored using an ACID compliant format""","Iceberg CTLT with source table as Hive managed table fails with ""The table must be stored using an ACID compliant format""

As part of HIVE-26519 the support for creating Iceberg tables using CTLT was added.

 
{code:java}
CREATE TABLE `tpch`.`lineitem`(`l_orderkey` int,`l_partkey` int,`l_suppkey` int,`l_linenumber` int,`l_quantity` decimal(15,2),`l_extendedprice` decimal(15,2),`l_discount` decimal(15,2),`l_tax` decimal(15,2),`l_returnflag` char(1),`l_linestatus` char(1),`l_shipdate` date,`l_commitdate` date,`l_receiptdate` date,`l_shipinstruct` char(25),`l_shipmode` char(10),`l_comment` string)ROW FORMAT SERDE'org.apache.hadoop.hive.ql.io.orc.OrcSerde'STORED AS INPUTFORMAT'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'OUTPUTFORMAT'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'LOCATION's3a://qe-s3-bucket-weekly/cc-dwx-97hupz/warehouse/tablespace/managed/hive/tpch.db/lineitem'TBLPROPERTIES ('bucketing_version'='2','transactional'='true','transactional_properties'='default','transient_lastDdlTime'='1686741574'){code}
Iceberg query
{code:java}
CREATE TABLE lineitem LIKE tpch.lineitem
STORED BY ICEBERG;{code}
Error
{code:java}
Error while compiling statement: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:The table must be stored using an ACID compliant format (such as ORC): tpch_ioytc.lineitem) 


INFO  : Compiling command(queryId=hive_20230614191434_298d5e35-3ac9-4b6b-bf4f-132e8b4ad265): create table line like tpch.lineitem stored by iceberg
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Created Hive schema: Schema(fieldSchemas:null, properties:null)
INFO  : Completed compiling command(queryId=hive_20230614191434_298d5e35-3ac9-4b6b-bf4f-132e8b4ad265); Time taken: 0.03 seconds
INFO  : Executing command(queryId=hive_20230614191434_298d5e35-3ac9-4b6b-bf4f-132e8b4ad265): create table line like tpch.lineitem stored by iceberg
INFO  : Starting task [Stage-0:DDL] in serial mode
ERROR : Failed
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:The table must be stored using an ACID compliant format (such as ORC): tpch_ioytc.line)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1384) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1389) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.ddl.table.create.like.CreateTableLikeOperation.execute(CreateTableLikeOperation.java:88) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:360) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:333) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:250) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:111) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:810) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:547) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:541) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:166) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:235) ~[hive-service-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:92) ~[hive-service-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:340) ~[hive-service-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:?]
	at javax.security.auth.Subject.doAs(Subject.java:423) ~[?:?]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899) ~[hadoop-common-3.1.1.7.2.16.0-287.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:360) ~[hive-service-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: The table must be stored using an ACID compliant format (such as ORC): tpch_ioytc.line
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_req_result$create_table_req_resultStandardScheme.read(ThriftHiveMetastore.java) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_req_result$create_table_req_resultStandardScheme.read(ThriftHiveMetastore.java) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_req_result.read(ThriftHiveMetastore.java) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_create_table_req(ThriftHiveMetastore.java:2143) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.create_table_req(ThriftHiveMetastore.java:2130) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table(HiveMetaStoreClient.java:4461) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table(SessionHiveMetaStoreClient.java:180) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1403) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1373) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1364) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at jdk.internal.reflect.GeneratedMethodAccessor299.invoke(Unknown Source) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:216) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at com.sun.proxy.$Proxy56.createTable(Unknown Source) ~[?:?]
	at jdk.internal.reflect.GeneratedMethodAccessor299.invoke(Unknown Source) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:4365) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at com.sun.proxy.$Proxy56.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1373) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	... 27 more
ERROR : DDLTask failed, DDL Operation: class org.apache.hadoop.hive.ql.ddl.table.create.like.CreateTableLikeOperation
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:The table must be stored using an ACID compliant format (such as ORC): tpch_ioytc.line)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1384) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1389) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.ddl.table.create.like.CreateTableLikeOperation.execute(CreateTableLikeOperation.java:88) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:360) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:333) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:250) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:111) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:810) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:547) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:541) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:166) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:235) ~[hive-service-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:92) ~[hive-service-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:340) ~[hive-service-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:?]
	at javax.security.auth.Subject.doAs(Subject.java:423) ~[?:?]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899) ~[hadoop-common-3.1.1.7.2.16.0-287.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:360) ~[hive-service-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: The table must be stored using an ACID compliant format (such as ORC): tpch_ioytc.line
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_req_result$create_table_req_resultStandardScheme.read(ThriftHiveMetastore.java) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_req_result$create_table_req_resultStandardScheme.read(ThriftHiveMetastore.java) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_req_result.read(ThriftHiveMetastore.java) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_create_table_req(ThriftHiveMetastore.java:2143) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.create_table_req(ThriftHiveMetastore.java:2130) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table(HiveMetaStoreClient.java:4461) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table(SessionHiveMetaStoreClient.java:180) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1403) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1373) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:1364) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at jdk.internal.reflect.GeneratedMethodAccessor299.invoke(Unknown Source) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:216) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at com.sun.proxy.$Proxy56.createTable(Unknown Source) ~[?:?]
	at jdk.internal.reflect.GeneratedMethodAccessor299.invoke(Unknown Source) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:4365) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	at com.sun.proxy.$Proxy56.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1373) ~[hive-exec-3.1.3000.2023.0.15.0-153.jar:3.1.3000.2023.0.15.0-153]
	... 27 more
ERROR : FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:The table must be stored using an ACID compliant format (such as ORC): tpch_ioytc.line)
INFO  : Completed executing command(queryId=hive_20230614191434_298d5e35-3ac9-4b6b-bf4f-132e8b4ad265); Time taken: 0.023 seconds
INFO  : OK{code}
 ",ayushtkn,dharmikt,Major,Resolved,Fixed,14/Jun/23 19:19,22/Jun/23 04:14
Bug,HIVE-27447,13540470,Iceberg: Queries failing due to FileSystem close errors due to column statistics,"During fetching column statistics we are closing the filesystem in the HS2, FS is shared and cached, so closing it ain't a good idea.

HS2 Logs show
{noformat}
2023-06-16T16:57:09,515 DEBUG [45c1a38d-23c0-4e3a-96b7-c398bb215b2245c1a38d-23c0-4e3a-96b7-c398bb215b22 HiveServer2-Handler-Pool: Thread-114] fs.FileSystem: FileSystem.close() by method: org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:1511)); Key: (boroknagyz (auth:SIMPLE))@hdfs://localhost:20500; URI: hdfs://localhost:20500; Object Identity Hash: 14ec8672
2023-06-16T16:57:09,515 TRACE [45c1a38d-23c0-4e3a-96b7-c398bb215b2245c1a38d-23c0-4e3a-96b7-c398bb215b22 HiveServer2-Handler-Pool: Thread-114] fs.FileSystem: FileSystem.close() full stack trace:
java.lang.Throwable: null
        at org.apache.hadoop.fs.FileSystem.debugLogFileSystemClose(FileSystem.java:627) ~[hadoop-common-3.1.1.7.2.16.0-287.jar:?]
        at org.apache.hadoop.fs.FileSystem.close(FileSystem.java:2599) ~[hadoop-common-3.1.1.7.2.16.0-287.jar:?]
        at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:1511) ~[hadoop-hdfs-client-3.1.1.7.2.16.0-287.jar:?]
        at org.apache.iceberg.mr.hive.HiveIcebergStorageHandler.canProvideColStatistics(HiveIcebergStorageHandler.java:444) ~[hive-iceberg-handler-3.1.3000.2023.0.15.0-159.jar:3.1.3000.2023.0.15.0-159]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:1074) ~[hive-exec-3.1.3000.2023.0.15.0-159.jar:3.1.3000.2023.0.15.0-159]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:298) ~[hive-exec-3.1.3000.2023.0.15.0-159.jar:3.1.3000.2023.0.15.0-159]{noformat}",ayushtkn,ayushtkn,Major,Resolved,Fixed,17/Jun/23 05:08,20/Jun/23 04:19
Bug,HIVE-27449,13540542,Fix flaky test KafkaRecordIteratorTest,"KafkaRecordIteratorTest was disabled in [HIVE-23838|https://issues.apache.org/jira/browse/HIVE-23838],

Fix and enable it again",akshatm,akshatm,Major,Resolved,Fixed,19/Jun/23 02:32,22/Jun/23 04:10
Bug,HIVE-27452,13540636,Fix possible FNFE in HiveQueryLifeTimeHook::checkAndRollbackCTAS,"In case of a CTAS rollback, if the table directory is not created at all, then while getting the owner of the table directory we might get a FileNotFoundException.

Hence, check whether the directory exists before submitting a request to cleanup.",sbadhya,sbadhya,Major,Resolved,Fixed,19/Jun/23 14:13,22/Jun/23 03:36
Bug,HIVE-27466,13541574,Remove obsolete reference to category-X JSON license from NOTICE,"The JSON.org dependencies were removed as part of HIVE-15144. The respective mention in the NOTICE file is obsolete , misleading, and probably violates ASF policy.",zabetak,zabetak,Blocker,Resolved,Fixed,27/Jun/23 13:39,30/Jun/23 16:55
Bug,HIVE-27467,13541584,"NOTICE files use incorrect wording ""developed by"" instead of ""developed at""","The NOTICE files currently use the incorrect wording ""developed by The Apache Software Foundation"".

As it can be shown in the following links (and also from other JIRA tickets) the correct wording is ""developed at"". 

https://www.apache.org/legal/src-headers.html#notice
https://www.apache.org/legal/release-policy.html#notice-required",zabetak,zabetak,Blocker,Resolved,Fixed,27/Jun/23 14:36,30/Jun/23 16:56
Bug,HDFS-16881,13516263,Warn if AccessControlEnforcer runs for a long time to check permission,"AccessControlEnforcer is configurable.  If an external AccessControlEnforcer runs for a long time to check permission with the FSnamesystem lock, it will significantly slow down the entire Namenode.  In the JIRA, we will print a WARN message when it happens.",szetszwo,szetszwo,Major,Resolved,Fixed,03/Jan/23 06:40,06/Feb/23 21:09
Bug,HDFS-16884,13516561,Fix TestFsDatasetImpl#testConcurrentWriteAndDeleteBlock failed,"Since the default is async delete replica on the datanode, the replica may not be deleted during the execution of UT#testConcurrentWriteAndDeleteBlock, resulting in a mismatch between the number of replicas in each dataset obtained at the end and the expectation",haiyang Hu,haiyang Hu,Major,Resolved,Fixed,05/Jan/23 02:41,11/Jan/23 02:32
Bug,HDFS-16885,13517106,Fix TestHdfsConfigFields#testCompareConfigurationClassAgainstXml failed,"A new parameter ""dfs.namenode.access-control-enforcer-reporting-threshold-ms"" was introduced in HDFS-16881. 
However, this parameter was not added to hdfs-default.xml, cause run TestHdfsConfigFields#testCompareConfigurationClassAgainstXml failed.",haiyang Hu,haiyang Hu,Major,Resolved,Fixed,07/Jan/23 04:20,11/Jan/23 02:33
Bug,HDFS-16895,13520870,NamenodeHeartbeatService should use credentials of logged in user,"NamenodeHeartbeatService has been found to log the errors when querying protected Namenode JMX APIs. We have been able to work around this by running kinit with the DFS_ROUTER_KEYTAB_FILE_KEY and DFS_ROUTER_KERBEROS_PRINCIPAL_KEY on the router.

While investigating a solution, we found that doing the request as part of a  UserGroupInformation.getLoginUser.doAs() call doesn't require to kinit before.

The error logged is:
{noformat}
2022-08-16 21:35:00,265 ERROR org.apache.hadoop.hdfs.server.federation.router.FederationUtil: Cannot parse JMX output for Hadoop:service=NameNode,name=FSNamesystem* from server ltx1-yugiohnn03-ha1.grid.linkedin.com:50070
org.apache.hadoop.security.authentication.client.AuthenticationException: Error while authenticating with endpoint: http://ltx1-yugiohnn03-ha1.grid.linkedin.com:50070/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem*
	at sun.reflect.GeneratedConstructorAccessor55.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.wrapExceptionWithMessage(KerberosAuthenticator.java:232)
	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:219)
	at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:350)
	at org.apache.hadoop.hdfs.web.URLConnectionFactory.openConnection(URLConnectionFactory.java:186)
	at org.apache.hadoop.hdfs.server.federation.router.FederationUtil.getJmx(FederationUtil.java:82)
	at org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService.updateJMXParameters(NamenodeHeartbeatService.java:352)
	at org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService.getNamenodeStatusReport(NamenodeHeartbeatService.java:295)
	at org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService.updateState(NamenodeHeartbeatService.java:218)
	at org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService.periodicInvoke(NamenodeHeartbeatService.java:172)
	at org.apache.hadoop.hdfs.server.federation.router.PeriodicService$1.run(PeriodicService.java:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:360)
	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:204)
	... 15 more
Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
	at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)
	at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:122)
	at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)
	at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:224)
	at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)
	at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)
	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator$1.run(KerberosAuthenticator.java:336)
	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator$1.run(KerberosAuthenticator.java:310)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:310)
	... 16 more{noformat}
 ",hchaverri,hchaverri,Major,Resolved,Fixed,21/Jan/23 00:35,07/Feb/23 22:00
Bug,HDFS-16896,13521083,HDFS Client hedged read has increased failure rate than without hedged read,"When hedged read is enabled by HDFS client, we see an increased failure rate on reads.

*stacktrace*

 
{code:java}
Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1183972111-10.197.192.88-1590025572374:blk_17114848218_16043459722 file=/data/tracking/streaming/AdImpressionEvent/daily/2022/07/18/compaction_1/part-r-1914862.1658217125623.1362294472.orc
at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1077)
at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1060)
at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1039)
at org.apache.hadoop.hdfs.DFSInputStream.hedgedFetchBlockByteRange(DFSInputStream.java:1365)
at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1572)
at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1535)
at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:112)
at org.apache.hadoop.fs.RetryingInputStream.lambda$readFully$3(RetryingInputStream.java:172)
at org.apache.hadoop.fs.RetryPolicy.lambda$run$0(RetryPolicy.java:137)
at org.apache.hadoop.fs.NoOpRetryPolicy.run(NoOpRetryPolicy.java:36)
at org.apache.hadoop.fs.RetryPolicy.run(RetryPolicy.java:136)
at org.apache.hadoop.fs.RetryingInputStream.readFully(RetryingInputStream.java:168)
at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:112)
at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:112)
at io.trino.plugin.hive.orc.HdfsOrcDataSource.readInternal(HdfsOrcDataSource.java:76)
... 46 more
{code}
 ",mccormickt12,mccormickt12,Major,Resolved,Fixed,24/Jan/23 07:30,02/Mar/23 01:06
Bug,HDFS-16897,13521925,Fix abundant Broken pipe exception in BlockSender,"in our production cluster env , we found some exception in datanode logs，its frequently print below error
in HDFS-2054 we only ignored message starting with `Broken pipe`  which may not enough for the following case:
!https://user-images.githubusercontent.com/20748856/215264829-5f16dbc3-fea2-4883-a3d6-ded367564b8c.png!
this situation look like related to short-circuit read. in HDFS-4354 the error has been wrapped, so that our previous judgment conditions are invalid.
!https://user-images.githubusercontent.com/20748856/215314257-2064637b-ea46-42f5-b53f-a29e68bb50ea.png!
maybe we can improve it. ",fanluoo,fanluoo,Minor,Resolved,Fixed,29/Jan/23 12:17,08/Jun/23 20:26
Bug,HDFS-16910,13523391,Fix incorrectly initializing RandomAccessFile caused flush performance decreased for JN,"At present, after our cluster backport patch HDFS-15882, 
when set shouldSyncWritesAndSkipFsync to false, there will be flush performance degradation caused by JN.

*Root Cause*：
when setting shouldSyncWritesAndSkipFsync to false, the mode of init RandomAccessFile will be `rws`. 
even if fc.force(false) is executed when flushAndSync is executed (hopefully, only requires updates to the file's content to be written to storage and the metadata is not update), 
but since the mode of RandomAccessFile is `rws`, It will requires updates to both the file's content and its metadata to be written, 
there will be flush performance degradation caused by JN.


*Fix:*
Need to update RandomAccessFile's mode from `rws` to `rwd`:

rwd: Open for reading and writing, as with ""rw"", and also require that every update to the file's content be written synchronously to the underlying storage device.


{code:java}
if (shouldSyncWritesAndSkipFsync) {
rp = new RandomAccessFile(name, ""rwd"");
} else {
rp = new RandomAccessFile(name, ""rw"");
}
{code}

In this way, when flushAndSync is executed, 
if shouldSyncWritesAndSkipFsync is false and the mode of RandomAccessFile is 'rw', it will call fc.force(false) to execute, 
otherwise should use `rwd` to perform the operation.",haiyang Hu,haiyang Hu,Major,Resolved,Fixed,07/Feb/23 02:35,09/Feb/23 04:50
Bug,HDFS-16911,13523526,Distcp with snapshot diff to support Ozone filesystem.,"Currently in DistcpSync i.e the step which applies the diff b/w 2 provided snapshots as arguments to the distcp job with -diff option, only DistributedFilesystem and WebHDFS filesytems are supported.

 
{code:java}
// currently we require both the source and the target file system are
// DistributedFileSystem or (S)WebHdfsFileSystem.
if (!(srcFs instanceof DistributedFileSystem
        || srcFs instanceof WebHdfsFileSystem)) {
  throw new IllegalArgumentException(""Unsupported source file system: ""
      + srcFs.getScheme() + ""://. "" +
      ""Supported file systems: hdfs://, webhdfs:// and swebhdfs://."");
}
if (!(tgtFs instanceof DistributedFileSystem
    || tgtFs instanceof WebHdfsFileSystem)) {
  throw new IllegalArgumentException(""Unsupported target file system: ""
      + tgtFs.getScheme() + ""://. "" +
      ""Supported file systems: hdfs://, webhdfs:// and swebhdfs://."");
}{code}
As Ozone now supports snapshot feature after HDDS-6517, add support to use distcp with ozone snapshots.

 ",sadanand_shenoy,sadanand_shenoy,Major,Resolved,Fixed,07/Feb/23 15:29,10/Apr/23 21:53
Bug,HDFS-16923,13524814,The getListing RPC will throw NPE if the path does not exist,"The getListing RPC will throw NPE if the path does not exist. And the stack as bellow:
{code:java}
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RemoteException): org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListing(FSNamesystem.java:4195)
    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getListing(NameNodeRpcServer.java:1421)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getListing(ClientNamenodeProtocolServerSideTranslatorPB.java:783)
    at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:622)
    at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:590)
    at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574) {code}",xuzq_zander,xuzq_zander,Critical,Resolved,Fixed,15/Feb/23 10:41,04/Mar/23 00:33
Bug,HDFS-16925,13524909,Namenode audit log to only include IP address of client,"Fixes broken test: TestAuditLogger by restoring the behaviour for audit logs prior to HADOOP-18628

With HADOOP-18628 in place, we perform InetAddress#getHostName in addition to InetAddress#getHostAddress, to save host name with IPC Connection object. When we perform InetAddress#getHostName, toString() of InetAddress would automatically print \{hostName}/\{hostIPAddress} if hostname is already resolved:
{code:java}
/**
 * Converts this IP address to a {@code String}. The
 * string returned is of the form: hostname / literal IP
 * address.
 *
 * If the host name is unresolved, no reverse name service lookup
 * is performed. The hostname part will be represented by an empty string.
 *
 * @return  a string representation of this IP address.
 */
public String toString() {
    String hostName = holder().getHostName();
    return ((hostName != null) ? hostName : """")
        + ""/"" + getHostAddress();
}{code}
 

For namenode audit logs, this means that when dfs client makes filesystem updates, the audit logs would also print host name in the audit logs in addition to ip address.

In order to maintain the compatibility, the purpose of this Jira is to only let audit log retrieve IP address from InetAddress and print it.",vjasani,vjasani,Major,Resolved,Fixed,15/Feb/23 20:22,08/Jun/23 20:26
Bug,HDFS-16934,13526056,org.apache.hadoop.hdfs.tools.TestDFSAdmin#testAllDatanodesReconfig regression,"jenkins test failure as the logged output is in the wrong order for the assertions. HDFS-16624 flipped the order...without that this would have worked.

{code}

java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:87)
	at org.junit.Assert.assertTrue(Assert.java:42)
	at org.junit.Assert.assertTrue(Assert.java:53)
	at org.apache.hadoop.hdfs.tools.TestDFSAdmin.testAllDatanodesReconfig(TestDFSAdmin.java:1149)
{code}


Here the code is asserting about the contents of the output, 
{code}
    assertTrue(outs.get(0).startsWith(""Reconfiguring status for node""));
    assertTrue(""SUCCESS: Changed property dfs.datanode.peer.stats.enabled"".equals(outs.get(2))
        || ""SUCCESS: Changed property dfs.datanode.peer.stats.enabled"".equals(outs.get(1)));  // here
    assertTrue(""\tFrom: \""false\"""".equals(outs.get(3)) || ""\tFrom: \""false\"""".equals(outs.get(2)));
    assertTrue(""\tTo: \""true\"""".equals(outs.get(4)) || ""\tTo: \""true\"""".equals(outs.get(3)))
{code}

If you look at the log, the actual line is appearing in that list, just in a different place. race condition
{code}
2023-02-24 01:02:06,275 [Listener at localhost/41795] INFO  tools.TestDFSAdmin (TestDFSAdmin.java:testAllDatanodesReconfig(1146)) - dfsadmin -status -livenodes output:
2023-02-24 01:02:06,276 [Listener at localhost/41795] INFO  tools.TestDFSAdmin (TestDFSAdmin.java:lambda$testAllDatanodesReconfig$0(1147)) - Reconfiguring status for node [127.0.0.1:41795]: started at Fri Feb 24 01:02:03 GMT 2023 and finished at Fri Feb 24 01:02:03 GMT 2023.
2023-02-24 01:02:06,276 [Listener at localhost/41795] INFO  tools.TestDFSAdmin (TestDFSAdmin.java:lambda$testAllDatanodesReconfig$0(1147)) - Reconfiguring status for node [127.0.0.1:34007]: started at Fri Feb 24 01:02:03 GMT 2023SUCCESS: Changed property dfs.datanode.peer.stats.enabled
2023-02-24 01:02:06,277 [Listener at localhost/41795] INFO  tools.TestDFSAdmin (TestDFSAdmin.java:lambda$testAllDatanodesReconfig$0(1147)) - 	From: ""false""
2023-02-24 01:02:06,277 [Listener at localhost/41795] INFO  tools.TestDFSAdmin (TestDFSAdmin.java:lambda$testAllDatanodesReconfig$0(1147)) - 	To: ""true""
2023-02-24 01:02:06,277 [Listener at localhost/41795] INFO  tools.TestDFSAdmin (TestDFSAdmin.java:lambda$testAllDatanodesReconfig$0(1147)) -  and finished at Fri Feb 24 01:02:03 GMT 2023.
2023-02-24 01:02:06,277 [Listener at localhost/41795] INFO  tools.TestDFSAdmin (TestDFSAdmin.java:lambda$testAllDatanodesReconfig$0(1147)) - SUCCESS: Changed property dfs.datanode.peer.stats.enabled
{code}
we have a race condition in output generation and the assertions are clearly too brittle

for the 3.3.5 release I'm not going to make this a blocker. What i will do is propose that the asserts move to assertJ with an assertion that the collection ""containsExactlyInAnyOrder"" all the strings.

That will
1. not be brittle.
2. give nice errors on failure
",slfan1989,stevel@apache.org,Minor,Resolved,Fixed,24/Feb/23 12:13,06/Mar/23 15:47
Bug,HDFS-16935,13526060,TestFsDatasetImpl.testReportBadBlocks brittle,"jenkins failure as sleep() time not long enough
{code}
Failing for the past 1 build (Since #4 )
Took 7.4 sec.
Error Message
expected:<1> but was:<0>
Stacktrace
java.lang.AssertionError: expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:633)
{code}

assert is after a 3s sleep waiting for reports coming in.
{code}
      dataNode.reportBadBlocks(block, dataNode.getFSDataset()
          .getFsVolumeReferences().get(0));
      Thread.sleep(3000);                                           // 3s sleep
      BlockManagerTestUtil.updateState(cluster.getNamesystem()
          .getBlockManager());
      // Verify the bad block has been reported to namenode
      Assert.assertEquals(1, cluster.getNamesystem().getCorruptReplicaBlocks());  // here
{code}

LambdaTestUtils.eventually() should be used around this assert, maybe with an even shorter initial delay so on faster systems, test is faster.
",vjasani,stevel@apache.org,Minor,Resolved,Fixed,24/Feb/23 12:24,08/Jun/23 20:26
Bug,HDFS-16939,13526945,Fix the thread safety bug in LowRedundancyBlocks,"The remove method in LowRedundancyBlocks is not protected by synchronized. This method is private and is called by BlockManager. As a result, priorityQueues has the risk of being accessed concurrently by multiple threads.",zhangshuyan,zhangshuyan,Major,Resolved,Fixed,03/Mar/23 06:39,11/Mar/23 08:24
Bug,HDFS-16942,13527465,Send error to datanode if FBR is rejected due to bad lease,"When a datanode sends a FBR to the namenode, it requires a lease to send it. On a couple of busy clusters, we have seen an issue where the DN is somehow delayed in sending the FBR after requesting the least. Then the NN rejects the FBR and logs a message to that effect, but from the Datanodes point of view, it thinks the report was successful and does not try to send another report until the 6 hour default interval has passed.

If this happens to a few DNs, there can be missing and under replicated blocks, further adding to the cluster load. Even worse, I have see the DNs join the cluster with zero blocks, so it is not obvious the under replication is caused by lost a FBR, as all DNs appear to be up and running.

I believe we should propagate an error back to the DN if the FBR is rejected, that way, the DN can request a new lease and try again.",sodonnell,sodonnell,Major,Resolved,Fixed,07/Mar/23 14:25,15/Mar/23 04:34
Bug,HDFS-16946,13527825,RBF: top real owners metrics can't been parsed json string,"After HDFS-15447,  Add top real owners metrics for delegation tokens. But the metrics can't been parsed json string.

 RBFMetrics$getTopTokenRealOwners method just return `org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair@1`

!image-2023-03-09-22-24-39-833.png!",nishtha11shah,max2049,Minor,Resolved,Fixed,09/Mar/23 14:24,13/Jun/23 04:32
Bug,HDFS-16949,13528473,Update ReadTransferRate to ReadLatencyPerGB for effective percentile metrics,"HDFS-16917 added ReadTransferRate quantiles to calculate the rate which data is read per unit of time.

With percentiles the values are sorted in ascending order and hence for the transfer rate p90 gives us the value where 90 percent rates are lower (worse), p99 gives us the value where 99 percent values are lower (worse).

Note that value(p90) < p(99) thus p99 is a better transfer rate as compared to p90.

However as the percentile increases the value should become worse in order to know how good our system is.

Hence instead of calculating the data read transfer rate, we should calculate it's inverse. We will instead calculate the time taken for a GB of data to be read. ( seconds / GB )

After this the p90 value will give us 90 percentage of total values where the time taken is less than value(p90), similarly for p99 and others.

Also p(90) < p(99) and here p(99) will become a worse value (taking more time each byte) as compared to p(90)",rdingankar,rdingankar,Minor,Resolved,Fixed,14/Mar/23 16:35,10/Apr/23 16:58
Bug,HDFS-16954,13528773,RBF: The operation of renaming a multi-subcluster directory to a single-cluster directory should throw ioexception,"The operation of renaming a multi-subcluster directory to a single-cluster directory may cause inconsistent behavior of the file system. This operation should throw exception to be reasonable.

Examples are as follows:
1. add  hash_all mount point   `hdfs dfsrouteradmin -add /tmp/foo subcluster1,subcluster2  /tmp/foo -order HASH_ALL`
2. add   mount point   `hdfs dfsrouteradmin -add /user/foo subcluster1 /user/foo`
3. mkdir dir for all subcluster.  ` hdfs dfs -mkdir /tmp/foo/123 `

4. check dir and all subclusters will have dir `/tmp/foo/123`
`hdfs dfs -ls /tmp/foo/` : will show dir `/tmp/foo/123`;
`hdfs dfs -ls hdfs://subcluster1/tmp/foo/` : will show dir `hdfs://subcluster1/tmp/foo/123`;
`hdfs dfs -ls hdfs://subcluster2/tmp/foo/` : will show dir `hdfs://subcluster2/tmp/foo/123`;

5. rename `/tmp/foo/123` to `/user/foo/123`. The op will succeed. `hdfs dfs -mv /tmp/foo/123 /user/foo/123 `

6. check dir again, rbf cluster still show dir `/tmp/foo/123`
`hdfs dfs -ls /tmp/foo/` : will show dir `/tmp/foo/123`;
`hdfs dfs -ls hdfs://subcluster1/tmp/foo/` : will no dirs;
`hdfs dfs -ls hdfs://subcluster2/tmp/foo/` : will show dir `hdfs://subcluster2/tmp/foo/123`;

The step 5 should throw exception.
 

 

 

 ",max2049,max2049,Minor,Resolved,Fixed,16/Mar/23 10:39,20/Apr/23 11:30
Bug,HDFS-16972,13531451,Delete a snapshot may deleteCurrentFile,"We found one case the when deleting a snapshot (with ordered snapshot deletion disabled), it can incorrectly delete some files in the current state.",szetszwo,szetszwo,Major,Resolved,Fixed,04/Apr/23 13:52,12/Jun/23 15:58
Bug,HDFS-16975,13532227,FileWithSnapshotFeature.isCurrentFileDeleted is not reloaded from FSImage.,"FileWithSnapshotFeature.isCurrentFileDeleted is not serialized to FSImage.
If it is true for any files, it will becomes false after saving to a FSImage and loading it back.",szetszwo,szetszwo,Major,Resolved,Fixed,11/Apr/23 17:54,24/Apr/23 16:06
Bug,HDFS-16982,13532601,Use the right Quantiles Array for Inverse Quantiles snapshot ,"HDFS-16949 introduced InverseQuantiles. However during snapshot for Inverse Quantiles we were still trying to access values from previous snapshot based on the Quantile Array declared in MutableQuantiles. ( Quantile(.50, .050), Quantile(.75, .025), Quantile(.90, .010), Quantile(.95, .005), Quantile(.99, .001) )

For InverseQuantiles we wont have these values ( except for Quantile(.50, .050) ) thus except for 50 Percentile snapshot wont return any value for the remaining quantiles.

Fix is to use the correct Quantiles Array to retrieve values during snapshot. The new UTs verify this behavior.",rdingankar,rdingankar,Minor,Resolved,Fixed,14/Apr/23 05:42,18/Apr/23 17:49
Bug,HDFS-16983,13532882,Fix concat operation doesn't honor dfs.permissions.enabled,"In concat RPC, it will call FSDirConcatOp::verifySrcFiles() to check the source files. In this function, it would make permission check for srcs. Whether do the permission check should be decided by dfs.permissions.enabled configuration. And the 'pc' parameter is always not null.

So we should change 'if (pc != null)' to 'if (fsd.isPermissionEnabled())'.
{code:java}
// permission check for srcs
if (pc != null) {
  fsd.checkPathAccess(pc, iip, FsAction.READ); // read the file
  fsd.checkParentAccess(pc, iip, FsAction.WRITE); // for delete
} 

{code}",caozhiqiang,caozhiqiang,Major,Resolved,Fixed,17/Apr/23 12:23,05/Jun/23 11:13
Bug,HDFS-16985,13532966,Fix data missing issue when delete local block file.,"We encounterd several missing-block problem in our production cluster which  hdfs  running on AWS EC2 + EBS.

The root cause：
 # the block remains only 1 replication left and hasn't been reconstruction
 # DN checks block file existing when BlockSender construction
 # the EBS checking failed and throw FileNotFoundException (EBS may be in fault condition)
 # DN invalidateBlock and schedule block  async deletion
 # EBS already back to normal when DN do delete block
 # the block file be delete permanently and can't be recovered",smarthan,smarthan,Major,Resolved,Fixed,18/Apr/23 02:27,14/May/23 13:35
Bug,HDFS-16986,13533645,EC: Fix locationBudget in getListing(),"The current `locationBudget` is estimated using the `block_replication` in `FileStatus`, which is unreasonable on EC files, because it will count the number of locations of a EC block as 1. We should consider ErasureCodingPolicy of the files to keep the meaning of `locationBudget` consistent.",zhangshuyan,zhangshuyan,Major,Resolved,Fixed,22/Apr/23 16:10,24/Apr/23 10:54
Bug,HDFS-16996,13534327,Fix flaky testFsCloseAfterClusterShutdown in TestFileCreation,"{code:java}
[ERROR] testFsCloseAfterClusterShutdown(org.apache.hadoop.hdfs.TestFileCreation) Time elapsed: 1.725 s <<< FAILURE! java.lang.AssertionError: Test resulted in an unexpected exit: 1: Block report processor encountered fatal exception: java.lang.ClassCastException: org.apache.hadoop.fs.FsServerDefaults cannot be cast to java.lang.Boolean at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2166) at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2152) at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2145) at org.apache.hadoop.hdfs.TestFileCreation.testFsCloseAfterClusterShutdown(TestFileCreation.java:1198) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418) Caused by: 1: Block report processor encountered fatal exception: java.lang.ClassCastException: org.apache.hadoop.fs.FsServerDefaults cannot be cast to java.lang.Boolean at org.apache.hadoop.util.ExitUtil.terminate(ExitUtil.java:381) at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockReportProcessingThread.run(BlockManager.java:5451){code}
https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-5532/10/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt",nishtha11shah,umamaheswararao,Major,Resolved,Fixed,27/Apr/23 13:23,01/Jun/23 03:08
Bug,HDFS-16999,13535070,Fix wrong use of processFirstBlockReport(),"`processFirstBlockReport()` is used to process first block report from datanode. It does not calculating `toRemove` list because it believes that there is no metadata about the datanode in the namenode. However, If a datanode is re registered after restarting, its `blockReportCount` will be updated to 0. That is to say, the first block report after a datanode restarts will be processed by `processFirstBlockReport()`.  This is unreasonable because the metadata of the datanode already exists in namenode at this time, and if redundant replica metadata is not removed in time, the blocks with insufficient replicas cannot be reconstruct in time, which increases the risk of missing block. In summary, `processFirstBlockReport()` should only be used when the namenode restarts, not when the datanode restarts. ",zhangshuyan,zhangshuyan,Major,Resolved,Fixed,05/May/23 02:38,09/May/23 02:07
Bug,HDFS-17000,13535288,Potential infinite loop in TestDFSStripedOutputStreamUpdatePipeline.testDFSStripedOutputStreamUpdatePipeline,"The method {{TestDFSStripedOutputStreamUpdatePipeline.testDFSStripedOutputStreamUpdatePipeline}} contains the following line:
{code}
for (int i = 0; i < Long.MAX_VALUE; i++) {
{code}
[GitHub source link|https://github.com/apache/hadoop/blob/4ee92efb73a90ae7f909e96de242d216ad6878b2/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedOutputStreamUpdatePipeline.java#L48]

Because {{i}} is an {{int}} the condition {{i < Long.MAX_VALUE}} will always be true and {{i}} will simply overflow.",Marcono1234,Marcono1234,Major,Resolved,Fixed,07/May/23 01:22,31/May/23 10:42
Bug,HDFS-17003,13535373,Erasure Coding: invalidate wrong block after reporting bad blocks from datanode,"After receiving reportBadBlocks RPC from datanode, NameNode compute wrong block to invalidate. It is a dangerous behaviour and may cause data loss. Some logs in our production as below:

 

NameNode log:
{code:java}
2023-05-08 21:23:49,112 INFO org.apache.hadoop.hdfs.StateChange: *DIR* reportBadBlocks for block: BP-932824627-xxxx-1680179358678:blk_-9223372036848404320_1471186 on datanode: datanode1:50010

2023-05-08 21:23:49,183 INFO org.apache.hadoop.hdfs.StateChange: *DIR* reportBadBlocks for block: BP-932824627-xxxx-1680179358678:blk_-9223372036848404319_1471186 on datanode: datanode2:50010{code}
datanode1 log:
{code:java}
2023-05-08 21:23:49,088 WARN org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Reporting bad BP-932824627-xxxx-1680179358678:blk_-9223372036848404320_1471186 on /data7/hadoop/hdfs/datanode

2023-05-08 21:24:00,509 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Failed to delete replica blk_-9223372036848404319_1471186: ReplicaInfo not found.{code}
 

This phenomenon can be reproduced.",zhanghaobo,zhanghaobo,Critical,Resolved,Fixed,08/May/23 12:42,08/Jun/23 10:42
Bug,HDFS-17011,13536167,"Fix the metric of  ""HttpPort"" at DataNodeInfo ","Now, the ""HttpPort"" metric was getting from the conf `dfs.datanode.info.port`

but the conf seem to be useless, httpPort already named infoPort and was assigned (#Line1373)",wangzhaohui,wangzhaohui,Minor,Resolved,Fixed,15/May/23 06:28,08/Jun/23 20:26
Bug,HDFS-17017,13536644,Fix the issue of arguments number limit in report command in DFSAdmin.,"Currently, the DFSAdmin report command should support a maximum number of arguments of 7， such as :

hdfs dfsadmin [-report] [-live] [-dead] [-decommissioning] [-enteringmaintenance] [-inmaintenance] [-slownodes]
",haiyang Hu,haiyang Hu,Major,Resolved,Fixed,18/May/23 03:44,08/Jun/23 20:26
Bug,HDFS-17018,13536696,Improve dfsclient log format,Modify the log format.,15217812508@163.com,15217812508@163.com,Minor,Resolved,Fixed,18/May/23 07:52,22/May/23 12:21
Bug,HDFS-17019,13536733, Optimize the logic for reconfigure slow peer enable for Namenode,"The logic of Reconfigure slow peer enable for Namenode requires the following optimizations:

1.Make SlowPeerTracker slowPeerTracker volatile.

2.When starting the NameNode, if the parameter dfs.datanode.peer.stats.enabled is set to false, DatanodeManager#startSlowPeerCollector() will not call, as a result the Slow peers collection thread 'slowPeerCollectorDaemon' will not be started .

 If the parameter dfs.datanode.peer.stats.enabled is dynamically refreshed to true, the current logic will not call DatanodeManager#startSlowPeerCollector(), which to thread 'slowPeerCollectorDaemon' not started as expected, so we will optimize here",haiyang Hu,haiyang Hu,Major,Resolved,Fixed,18/May/23 10:44,08/Jun/23 02:42
Bug,HDFS-17022,13536870,Fix the exception message to print the Identifier pattern,"In case of an incorrect string passed as value, it would throw an exception, but the message doesn't print the identifier pattern.


{code:java}
java.lang.IllegalArgumentException: [] = [[a] must be {2}{code}
 instead of 
{code:java}
java.lang.IllegalArgumentException: [] = [[a] must be [a-zA-Z_][a-zA-Z0-9_\-]*{code}

Ref to original discussion: https://github.com/apache/hadoop/pull/5669#discussion_r1198937053",nishtha11shah,nishtha11shah,Minor,Resolved,Fixed,19/May/23 13:46,08/Jun/23 20:26
Bug,HDFS-17027,13537853,RBF: Add supports for observer.auto-msync-period when using routers,"None-RBF clients that use observer reads have the option to set *dfs.client.failover.observer.auto-msync-period.<nameservice>* . This config makes the client automatically do an msync, allowing clients to use the observer reads feature without any code change.

To use observer reads with RBF, clients set *dfs.client.rbf.observer.read.enable*. The way this flag is implemented does not allow clients to use the *auto-msync-period* config. So with RBF, clients either have to 
# Not use observer reads
# Use observer reads with the risk of stale reads
# Make code changes to explicitly call msync.

We should add support for *dfs.client.failover.observer.auto-msync-period.<nameservice>*. This can be done by adding a ProxyProvider, in a similar manner to the ObserverReadProxyProvider.


",simbadzina,simbadzina,Major,Resolved,Fixed,26/May/23 19:54,31/May/23 17:20
Bug,HDFS-17041,13539128,RBF: Fix putAll impl for mysql and file based state stores,"Only zookeeper based state store allows all records to be inserted even though only few of them already exists and ""errorIfExists"" is true, however file/fs as well as mysql based putAll fails the whole putAll operation immediately after encountering single record that already exists in the records and ""errorIfExists"" is provided true (which is the case while inserting records for the first time).

For all implementations, we should allow inserts of the records that do not already exist and report any record as failure that already exists, rather than failing the whole operation and not trying to insert valid records.",vjasani,vjasani,Major,Resolved,Fixed,07/Jun/23 17:12,13/Jun/23 16:56
Bug,HDFS-17045,13539692,File renamed from a snapshottable dir to a non-snapshottable dir cannot be deleted.,"HDFS-16972 added a [shouldDestroy|https://github.com/szetszwo/hadoop/blob/331e075115b4a35574622318b26f6d4731658d57/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeReference.java#L834-L845] method which caused the following bug.
h3. Background:
 - When {{FileSystem.rename(src, dst)}} from a snapshottable dir (src) to a snapshottable dir (dst), dstSnapshotId is set to the latest snapshot at dst. As a result, dst is NOT in dstSnapshotId because dstSnapshotId was already taken before rename.
 - snapshotToBeDeleted is the snapshot id of the current operation if the operation is {{{}FileSystem.deleteSnapshot{}}}. Otherwise, snapshotToBeDeleted is set to CURRENT_STATE_ID.
 - If (snapshotToBeDeleted > dstSnapshotId), dst is in snapshotToBeDeleted. The shouldDestroy method returns true to continue deletion.
 - If (snapshotToBeDeleted <= dstSnapshotId), dst must not be in snapshotToBeDeleted. The shouldDestroy method returns false to stop deletion.

All the above are correct for renaming within snapshottable directories.

h3. Bug:
 - If rename(src, dst) from a snapshottable dir (src) to a non-snapshottable dir (dst), dstSnapshotId becomes CURRENT_STATE_ID.
 - When {{FileSystem.delete(dst)}} happens, snapshotToBeDeleted is also set to CURRENT_STATE_ID.
 - In this case, snapshotToBeDeleted == dstSnapshotId, the shouldDestroy method will return false and it incorrectly stops the deletion.

Not that this bug may cause fsimage corruption and quota miscalculation since some files can be partially deleted.  Fortunately, this bug won't cause data loss.",szetszwo,szetszwo,Major,Resolved,Fixed,12/Jun/23 15:58,14/Jun/23 23:13
Bug,HDFS-17052,13540332,Improve BlockPlacementPolicyRackFaultTolerant to avoid choose nodes failed when no enough Rack.,"When writing EC data, if the number of racks matching the storageType is insufficient, more than one block are allowed to be written to the same rack

!write ec in same rack.png|width=962,height=604!

 

 

 

However, during EC block recovery, it is not possible to recover on the same rack, which deviates from the expected behavior.

!failed reconstruction ec in same rack-1.png|width=946,height=413!",zhtttylzz,zhtttylzz,Major,Resolved,Fixed,16/Jun/23 07:43,02/Jul/23 09:59
Bug,HBASE-27549,13516266,[hbase-thirdparty] Upgrade Netty to 4.1.86.Final,"Netty version - 4.1.86.Final has fix some CVEs.
CVE-2022-41915,
CVE-2022-41881

Upgrade to latest version.

",rajeshbabu,rajeshbabu,Major,Resolved,Fixed,03/Jan/23 06:55,03/Jan/23 09:24
Bug,HBASE-27554,13516657,Test failures on branch-2.4 with corrupted exclude list,"Nightly builds and PRs on branch-2.4 are failing with an invalid exclude list.

Executed unit test command:
{code:java}
/opt/maven/bin/mvn --batch-mode -Dmaven.repo.local=/home/jenkins/jenkins-home/workspace/Base-PreCommit-GitHub-PR_PR-4944/yetus-m2/hbase-branch-2.4-patch-0 --threads=4 -Djava.io.tmpdir=/home/jenkins/jenkins-home/workspace/Base-PreCommit-GitHub-PR_PR-4944/yetus-jdk8-hadoop2-check/src/target -DHBasePatchProcess -PrunAllTests -Dtest.exclude.pattern=**/replication.regionserver.TestMetaRegionReplicaReplicationEndpoint.java,**/client.TestMetaRegionLocationCache.java,**/master.balancer.TestStochasticLoadBalancerRegionReplicaWithRacks.java,**/replication.TestZKReplicationQueueStorageWARNING: All illegal access operations will be denied in a future release.java,**/replication.regionserver.TestBasicWALEntryStreamFSHLog.java -Dsurefire.firstPartForkCount=0.5C -Dsurefire.secondPartForkCount=0.5C clean test -fae {code}
The latest exclude list contains ""WARNING: All illegal access operations will be denied in a future release"" and maven treats this as a new parameter. As a result unit tests are failing on CI that rely on the exclude list.

[https://ci-hbase.apache.org/job/HBase-Find-Flaky-Tests/job/branch-2.4/lastSuccessfulBuild/artifact/output/excludes/*view*/]
{noformat}
**/replication.regionserver.TestMetaRegionReplicaReplicationEndpoint.java,**/client.TestMetaRegionLocationCache.java,**/master.balancer.TestStochasticLoadBalancerRegionReplicaWithRacks.java,**/replication.TestZKReplicationQueueStorageWARNING: All illegal access operations will be denied in a future release.java,**/replication.regionserver.TestBasicWALEntryStreamFSHLog.java {noformat}",psomogyi,psomogyi,Major,Resolved,Fixed,05/Jan/23 15:01,19/May/23 11:51
Bug,HBASE-27560,13517244,CatalogJanitor consistencyCheck cannot report the hole on last region if next table is disabled in meta,"When doing metaTableConsistencyCheck with ReportMakingVisitor, it will skip to check disabled table. When table before the disabled table in meta has a hole on last region,  it cannot be added to holes pairs.",shiningljz,shiningljz,Minor,Resolved,Fixed,09/Jan/23 13:14,29/Jan/23 02:31
Bug,HBASE-27561,13517299,hbase.master.port is ignored in processing of hbase.masters ,"Setting of hbase.master.port is ignored in default processing of hbase.masters. 

For example,

hbase.masters=localhost
hbase.master.port=8100

""Server at localhost:16000"" is in the failed servers list""

hbase.masters=localhost:8100
hbase.master.port=8100

and all is well. This is confusing and a minor configuration handling bug.",apurtell,apurtell,Minor,Resolved,Fixed,10/Jan/23 00:30,19/Jan/23 05:20
Bug,HBASE-27563,13517453,ChaosMonkey sometimes generates invalid boundaries for random item selection,"While running chaos monkey, I occasionally see `IndexOutOfBoundsException` originating from `PolicyBasedChaosMonkey#selectRandomItems`. Looks like we make some false assumptions about the arguments passes to the function. ",ndimiduk,ndimiduk,Minor,Resolved,Fixed,10/Jan/23 13:29,14/Jan/23 00:50
Bug,HBASE-27564,13517459,Add default encryption type for MiniKDC to fix failed tests on JDK11+,"An example of a failed test run with Hadoop2 and JDK17:

 
{code:java}
[INFO] Running org.apache.hadoop.hbase.coprocessor.TestSecureExport
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 56.87 s <<< FAILURE! - in org.apache.hadoop.hbase.coprocessor.TestSecureExport
[ERROR] org.apache.hadoop.hbase.coprocessor.TestSecureExport  Time elapsed: 56.862 s  <<< ERROR!
java.io.IOException: Failed on local exception: java.io.IOException: Couldn't setup connection for tianhang.tang/localhost@EXAMPLE.COM to localhost/127.0.0.1:53756; Host Details : local host is: ""Tangs-MacBook-Pro.local/10.2.175.4""; destination host is: ""localhost"":53756;
    at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:805)
    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1544)
    at org.apache.hadoop.ipc.Client.call(Client.java:1486)
    at org.apache.hadoop.ipc.Client.call(Client.java:1385)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
    at jdk.proxy2/jdk.proxy2.$Proxy34.getDatanodeReport(Unknown Source)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getDatanodeReport(ClientNamenodeProtocolTranslatorPB.java:653)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
    at jdk.proxy2/jdk.proxy2.$Proxy35.getDatanodeReport(Unknown Source)
    at org.apache.hadoop.hdfs.DFSClient.datanodeReport(DFSClient.java:2111)
    at org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2698)
    at org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2742)
    at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1723)
    at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:905)
    at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:798)
    at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster(HBaseTestingUtility.java:668)
    at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster(HBaseTestingUtility.java:641)
    at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:1130)
    at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:1105)
    at org.apache.hadoop.hbase.coprocessor.TestSecureExport.beforeClass(TestSecureExport.java:206)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.apache.hadoop.hbase.SystemExitRule$1.evaluate(SystemExitRule.java:38)
    at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
    at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.io.IOException: Couldn't setup connection for tianhang.tang/localhost@EXAMPLE.COM to localhost/127.0.0.1:53756
    at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:763)
    at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)
    at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:734)
    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:828)
    at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:423)
    at org.apache.hadoop.ipc.Client.getConnection(Client.java:1601)
    at org.apache.hadoop.ipc.Client.call(Client.java:1432)
    ... 41 more
Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Message stream modified (41) - Message stream modified)]
    at jdk.security.jgss/com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:228)
    at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:407)
    at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:629)
    at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:423)
    at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:815)
    at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:811)
    at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)
    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:810)
    ... 44 more
Caused by: GSSException: No valid credentials provided (Mechanism level: Message stream modified (41) - Message stream modified)
    at java.security.jgss/sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:778)
    at java.security.jgss/sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:266)
    at java.security.jgss/sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:196)
    at jdk.security.jgss/com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:209)
    ... 53 more
Caused by: KrbException: Message stream modified (41) - Message stream modified
    at java.security.jgss/sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:72)
    at java.security.jgss/sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:224)
    at java.security.jgss/sun.security.krb5.KrbTgsReq.sendAndGetCreds(KrbTgsReq.java:235)
    at java.security.jgss/sun.security.krb5.internal.CredentialsUtil.serviceCredsSingle(CredentialsUtil.java:477)
    at java.security.jgss/sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:340)
    at java.security.jgss/sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:314)
    at java.security.jgss/sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:169)
    at java.security.jgss/sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:493)
    at java.security.jgss/sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:700)
    ... 56 more
Caused by: KrbException: Identifier doesn't match expected value (906)
    at java.security.jgss/sun.security.krb5.internal.KDCRep.init(KDCRep.java:140)
    at java.security.jgss/sun.security.krb5.internal.TGSRep.init(TGSRep.java:65)
    at java.security.jgss/sun.security.krb5.internal.TGSRep.<init>(TGSRep.java:60)
    at java.security.jgss/sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:54)
    ... 64 more {code}
That's because hadoop-minikdc lower than 3.0 has compatibility issues with JDK11+, and we can find some useful infos in [KAFKA-7338|https://issues.apache.org/jira/browse/KAFKA-7338], FLINK-13516 and [SPARK-29957|https://issues.apache.org/jira/browse/SPARK-29957]: New encryption types of aes128-cts-hmac-sha256-128 and aes256-cts-hmac-sha384-192 (for Kerberos 5) enabled by default were added in Java 11.

Actually I'm not sure is it suitable to merge into master, because HBase has a rule that JDK11+ could only run with Hadoop3+. Is this just a design rule, or caused by some compatibility issues? If it is not a ""rule"", maybe we can try to find out the issues and fix them. Wish someone could give me some background infos.

 ",tangtianhang,tangtianhang,Major,Resolved,Fixed,10/Jan/23 14:43,21/Jan/23 00:10
Bug,HBASE-27566,13517523,Bump gitpython from 3.1.29 to 3.1.30 in /dev-support,,,zhangduo,Major,Resolved,Fixed,11/Jan/23 03:01,19/Jan/23 23:21
Bug,HBASE-27579,13520137,CatalogJanitor can cause data loss due to errors during cleanMergeRegion,"In CatalogJanitor.cleanMergeRegion, there is the following check:
{code:java}
HRegionFileSystem regionFs = null;
try {
  regionFs = HRegionFileSystem.openRegionFromFileSystem(this.services.getConfiguration(), fs,
    tabledir, mergedRegion, true);
} catch (IOException e) {
  LOG.warn(""Merged region does not exist: "" + mergedRegion.getEncodedName());
}

if (regionFs == null || !regionFs.hasReferences(htd)) {
 .. do the cleanup ..
} {code}
 

I think the assumption here is that an IOException would only be thrown if a region doesn't exist? We had a very poorly timed NameNode failover, during CatalogJanitor run, after a merge. The NameNode failover caused the openRegionFromFileSystem call to fail, which logged:
{code:java}
WARN org.apache.hadoop.hbase.master.janitor.CatalogJanitor: Merged region does not exist: 32c71224852c5a4b94a3ba271b4fcb15 {code}
This region did in fact exist and had not fully compacted, so there were still some lingering reference files.

The cleanup process moves the parent regions to the archive directory, but the default TTL for those files in the archive directory is only 5 minutes. After that they are cleaned up and the data is now unrecoverable.

This resulted in FileNotFoundExceptions trying to read or open this region. Our only course of action was to move the lingering reference files aside, so the data is unrecoverable.",bbeaudreault,bbeaudreault,Blocker,Resolved,Fixed,19/Jan/23 04:25,21/Jan/23 22:19
Bug,HBASE-27580,13520217,Reverse scan over rows with tags throw exceptions when using DataBlockEncoding,"This is easily reproducible, see test below. All you need to do is create a table with a DBE, write some puts with setTTL, then do a reverse scan. All 3 compressing DBE's fail. 

PREFIX throws an exception:
{code:java}
Caused by: java.lang.IndexOutOfBoundsException: index (0) must be less than size (0)
    at org.apache.hbase.thirdparty.com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:1355)
    at org.apache.hbase.thirdparty.com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:1337)
    at org.apache.hadoop.hbase.io.util.LRUDictionary$BidirectionalLRUMap.get(LRUDictionary.java:153)
    at org.apache.hadoop.hbase.io.util.LRUDictionary$BidirectionalLRUMap.access$000(LRUDictionary.java:79)
    at org.apache.hadoop.hbase.io.util.LRUDictionary.getEntry(LRUDictionary.java:43)
    at org.apache.hadoop.hbase.io.TagCompressionContext.uncompressTags(TagCompressionContext.java:152)
    at org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder$BufferedEncodedSeeker.decodeTags(BufferedDataBlockEncoder.java:819)
    at org.apache.hadoop.hbase.io.encoding.PrefixKeyDeltaEncoder$SeekerStateBufferedEncodedSeeker.decodeNext(PrefixKeyDeltaEncoder.java:209)
    at org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder$BufferedEncodedSeeker.seekToKeyInBlock(BufferedDataBlockEncoder.java:920) {code}
 

DIFF throws an exception:
{code:java}
Caused by: java.lang.ArrayIndexOutOfBoundsException: arraycopy: length -22 is negative
    at java.base/java.lang.System.arraycopy(Native Method)
    at org.apache.hadoop.hbase.util.ByteBufferUtils.copyFromBufferToArray(ByteBufferUtils.java:1140)
    at org.apache.hadoop.hbase.nio.SingleByteBuff.get(SingleByteBuff.java:213)
    at org.apache.hadoop.hbase.io.encoding.DiffKeyDeltaEncoder$DiffSeekerStateBufferedEncodedSeeker.decode(DiffKeyDeltaEncoder.java:431)
    at org.apache.hadoop.hbase.io.encoding.DiffKeyDeltaEncoder$DiffSeekerStateBufferedEncodedSeeker.decodeNext(DiffKeyDeltaEncoder.java:502)
    at org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder$BufferedEncodedSeeker.seekToKeyInBlock(BufferedDataBlockEncoder.java:920) {code}
 

FAST_DIFF throws a similar exception:
{code:java}
Caused by: java.lang.ArrayIndexOutOfBoundsException: arraycopy: length -22 is negative
    at java.base/java.lang.System.arraycopy(Native Method)
    at org.apache.hadoop.hbase.util.ByteBufferUtils.copyFromBufferToArray(ByteBufferUtils.java:1140)
    at org.apache.hadoop.hbase.nio.SingleByteBuff.get(SingleByteBuff.java:213)
    at org.apache.hadoop.hbase.io.encoding.FastDiffDeltaEncoder$FastDiffSeekerStateBufferedEncodedSeeker.decode(FastDiffDeltaEncoder.java:424)
    at org.apache.hadoop.hbase.io.encoding.FastDiffDeltaEncoder$FastDiffSeekerStateBufferedEncodedSeeker.decodeNext(FastDiffDeltaEncoder.java:490)
    at org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder$BufferedEncodedSeeker.seekToKeyInBlock(BufferedDataBlockEncoder.java:920) {code}
 

Reproduce with:
{code:java}
private static final Logger LOG = LoggerFactory.getLogger(TestTags.class);
@Test
public void testReverseScanWithDBE() throws IOException {
  byte[] family = Bytes.toBytes(""0"");

  Configuration conf = new Configuration(TEST_UTIL.getConfiguration());
  conf.setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 1);

  try (Connection connection = ConnectionFactory.createConnection(conf)) {

    for (DataBlockEncoding encoding : DataBlockEncoding.values()) {
      boolean isCompressing = encoding != DataBlockEncoding.NONE && encoding != DataBlockEncoding.ROW_INDEX_V1;
      try {
        testReverseScanWithDBE(connection, encoding, family);
        if (isCompressing) {
          Assert.fail(""Expected to throw exception for DBE "" + encoding);
        }
      } catch (Exception e) {
        LOG.info(""Got exception for DBE {}"", encoding, e);
        assertTrue(""Only expected compressing encodings to fail, but failed on "" + encoding, isCompressing);
      }
    }
  }
}

private void testReverseScanWithDBE(Connection conn, DataBlockEncoding encoding, byte[] family)
  throws IOException {
  LOG.info(""Running test with DBE={}"", encoding);
  TableName tableName = TableName.valueOf(TEST_NAME.getMethodName() + ""-"" + encoding);
  TEST_UTIL.createTable(
    TableDescriptorBuilder.newBuilder(tableName)
      .setColumnFamily(ColumnFamilyDescriptorBuilder.newBuilder(family)
        .setDataBlockEncoding(encoding)
        .build())
      .build(),
    null);

  Table table = conn.getTable(tableName);

  for (int i = 0; i < 10; i++) {
    table.put(new Put(Bytes.toBytes(i)).addColumn(family, Bytes.toBytes(0), new byte[10]).setTTL(600_000));
  }

  TEST_UTIL.flush(table.getName());

  Scan scan = new Scan();
  scan.setReversed(true);

  try (ResultScanner scanner = table.getScanner(scan)) {
    // should fail for compressing encodings
    scanner.next();
  }
} {code}",bbeaudreault,bbeaudreault,Major,Resolved,Fixed,19/Jan/23 16:07,05/Feb/23 19:36
Bug,HBASE-27585,13521071,Bump up jruby to 9.3.9.0 and related joni and jcodings to 2.1.43 and 1.0.57 respectively,"Bump up Jruby to 9.3.9.0 to ensure compliance which has multiple CVEs fixed related to openssl,snakeyaml etc.
 * rdoc has been updated to 6.3.3 to fix all known CVEs. ([#7396|https://github.com/jruby/jruby/issues/7396], [#7404|https://github.com/jruby/jruby/issues/7404])
 * rexml has been updated to 3.2.5 to fix all known CVEs. ([#7395|https://github.com/jruby/jruby/issues/7395], [#7405|https://github.com/jruby/jruby/issues/7405])
 * jruby-openssl has been updated to 0.14.0 to fix weak HMAC key hashing in bouncycastle, which itself is updated to 1.71. ([#7335|https://github.com/jruby/jruby/issues/7335], [#7385|https://github.com/jruby/jruby/issues/7385], [#7399|https://github.com/jruby/jruby/issues/7399])
 * psych has been updated to 3.3.4 to fix CVE-2022-38752 in the SnakeYAML library, which itself is updated to 1.33. ([#7386|https://github.com/jruby/jruby/issues/7386], [#7388|https://github.com/jruby/jruby/issues/7388], [#7400|https://github.com/jruby/jruby/issues/7400])
 * rubygems has been updated to 3.2.33 and bundler updated to 2.2.33 to address CVE-2021-43809. ([#7397|https://github.com/jruby/jruby/issues/7397], [#7401|https://github.com/jruby/jruby/issues/7401])",rajeshbabu,rajeshbabu,Major,Resolved,Fixed,24/Jan/23 05:45,03/Feb/23 06:16
Bug,HBASE-27586,13521072,Bump up commons-codec to 1.15,"commons-codec 1.15 has proper fix of few CVEs which may not effect in HBase but better to upgrade to ensure compliance.
Ex: While [a fix|https://github.com/apache/commons-codec/commit/48b615756d1d770091ea3322eefc08011ee8b113] was earlier made to {{commons-codec:commons-codec}} version 1.13, it was later found out to be incomplete. A [complete fix|https://github.com/apache/commons-codec/pull/29] exists in version 1.14 and that is the version users should upgrade to.",rajeshbabu,rajeshbabu,Major,Resolved,Fixed,24/Jan/23 05:50,29/Jan/23 04:03
Bug,HBASE-27589,13521183,Rename TestConnectionImplementation in hbase-it to fix javadoc failure,"discussed during release of 2.5.3RC0 [https://lists.apache.org/thread/b34lgz3yy8vkv4fbbxj4mvtjyjrp4m6p]

I found a javadoc problem introduced by HBASE-27498 with the same test class shared the same package prefix such that the javadoc failed with a unclear java.lang.NullPointerException. 

- hbase-it/src/test/java/org/apache/hadoop/hbase/client/TestConnectionImplementation.java 
- hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestConnectionImplementation.java

after renaming hbase-it/src/test/java/org/apache/hadoop/hbase/client/TestConnectionImplementation.java to hbase-it/src/test/java/org/apache/hadoop/hbase/client/TestConnectionImplementationCacheMasterState.java should fix this issue.",taklwu,taklwu,Blocker,Resolved,Fixed,24/Jan/23 22:41,26/Jan/23 06:56
Bug,HBASE-27592,13521499,Update hadoop netty version for hadoop-2.0 profile,"In HBASE-27299 we updated our minimum hadoop 2 dependency in branch-2 to 2.10.2, but we left our hadoop.netty.version at 3.6.2.Final.  According to [https://github.com/apache/hadoop/blob/rel/release-2.10.2/hadoop-project/pom.xml#L721,] the correct version is 3.10.6.Final.

This is causing a NoSuchMethodError in our newly backported TestRemoteRestore tests. Bumping to 3.10.6.Final locally fixes the tests.",bbeaudreault,bbeaudreault,Major,Resolved,Fixed,25/Jan/23 14:51,27/Jan/23 06:07
Bug,HBASE-27599,13521964,Cleanup wrong dependency in dependencyManagement,"In HBASE-27340 added a hadoop-3.0 profile in hbase-shaded-testing-util pom, but missed the version of hadoop-common test jar: [HBASE-27340|https://github.com/apache/hbase/commit/1b5403cf7d906a22c20bd4f025566a2a5f895d44]
 !screenshot-2.png! 

This will cause build failure:
 !screenshot-1.png! 

But I still don't know why this problem does not necessarily arise...",tangtianhang,tangtianhang,Major,Resolved,Fixed,30/Jan/23 03:08,05/Feb/23 19:07
Bug,HBASE-27600,13521973,Make the behavior of hadoop profile consistent,"In HBASE-27599 I found an issue that should cause build failure, but in fact both the community and my local build jobs succeed, which is not as expected.

The reason is, the activation condition in hbase-shaded-testing-util module is different with other modules, and it will not activate by default.",tangtianhang,tangtianhang,Major,Resolved,Fixed,30/Jan/23 03:51,05/Feb/23 19:07
Bug,HBASE-27602,13522211,Remove the impact of operating env on testHFileCleaning,"TestHFileCleaner#testHFileCleaning failed on my local, with this error:

{code:java}
2023-01-31T19:05:14,453 ERROR [hfile_cleaner-dir-scan-pool-0] mob.ManualMobMaintHFileCleaner(87): Failed to determine mob status of 'hdfs://localhost:59285/user/tianhang.tang/test-data/01a0d8a0-6306-7725-b63a-5debcab8cf57/archive/someHFileThatWouldBeAUUID.1675163114186', keeping it just in case.
java.lang.IllegalArgumentException: Illegal character <.> at 8. Namespaces may only contain 'alphanumeric characters' from any language and digits: tianhang.tang
        at org.apache.hadoop.hbase.TableName.isLegalNamespaceName(TableName.java:221) ~[classes/:?]
        at org.apache.hadoop.hbase.TableName.isLegalNamespaceName(TableName.java:196) ~[classes/:?]
        at org.apache.hadoop.hbase.TableName.<init>(TableName.java:316) ~[classes/:?]
        at org.apache.hadoop.hbase.TableName.createTableNameIfNecessary(TableName.java:349) ~[classes/:?]
        at org.apache.hadoop.hbase.TableName.valueOf(TableName.java:387) ~[classes/:?]
        at org.apache.hadoop.hbase.util.CommonFSUtils.getTableName(CommonFSUtils.java:440) ~[classes/:?]
        at org.apache.hadoop.hbase.mob.ManualMobMaintHFileCleaner.isFileDeletable(ManualMobMaintHFileCleaner.java:62) ~[classes/:?]
        at org.apache.hbase.thirdparty.com.google.common.collect.Iterators$5.computeNext(Iterators.java:673) ~[hbase-shaded-miscellaneous-4.1.4.jar:4.1.4]
        at org.apache.hbase.thirdparty.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:146) ~[hbase-shaded-miscellaneous-4.1.4.jar:4.1.4]
        at org.apache.hbase.thirdparty.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:141) ~[hbase-shaded-miscellaneous-4.1.4.jar:4.1.4]
        at org.apache.hbase.thirdparty.com.google.common.collect.Iterators$ConcatenatedIterator.hasNext(Iterators.java:1391) ~[hbase-shaded-miscellaneous-4.1.4.jar:4.1.4]
        at org.apache.hadoop.hbase.master.cleaner.HFileCleaner.deleteFiles(HFileCleaner.java:194) ~[classes/:?]
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteFiles(CleanerChore.java:391) ~[classes/:?]
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.lambda$traverseAndDelete$1(CleanerChore.java:483) ~[classes/:?]
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.deleteAction(CleanerChore.java:565) ~[classes/:?]
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.traverseAndDelete(CleanerChore.java:483) ~[classes/:?]
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.lambda$chore$0(CleanerChore.java:258) ~[classes/:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
        at java.lang.Thread.run(Thread.java:833) ~[?:?]
{code}

That's because the mocked dir is too simple, need to add a whole namespace/table/region path.
",tangtianhang,tangtianhang,Major,Resolved,Fixed,31/Jan/23 11:07,12/Feb/23 19:42
Bug,HBASE-27608,13522799,Use lowercase image reference name in our docker file,"https://ci-hbase.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-4966/5/consoleFull

The build failed with

{noformat}
19:46:22  #1 [internal] load build definition from Dockerfile
19:46:22  #1 transferring dockerfile: 7.66kB done
19:46:22  #1 DONE 0.1s
19:46:22  
19:46:22  #2 [internal] load .dockerignore
19:46:22  #2 transferring context: 2B done
19:46:22  #2 DONE 0.1s
19:46:22  Dockerfile:117
19:46:22  --------------------
19:46:22   115 |     #
19:46:22   116 |     
19:46:22   117 | >>> FROM BASE_IMAGE
19:46:22   118 |     SHELL [""/bin/bash"", ""-o"", ""pipefail"", ""-c""]
19:46:22   119 |     
19:46:22  --------------------
19:46:22  ERROR: failed to solve: failed to parse stage name ""BASE_IMAGE"": invalid reference format: repository name must be lowercase
{noformat}

Changing BASE_IMAGE to base_image can fix the problem.",zhangduo,zhangduo,Major,Resolved,Fixed,02/Feb/23 15:07,04/Feb/23 06:31
Bug,HBASE-27619,13523518,Bulkload fails when trying to bulkload files with invalid names after HBASE-26707,"HBASE-26707 has introduced changes to reduce renames on bulkload when using FILE based SFT. However, if the bulkloading file has an invalid hfile name, or has been split in the bulkload process, we don't do any renaming when FILE based SFT is enabled, and we place the file name as ""it is"" in the store dir. This later fails the validations performed by StoreFileReader when it tries to open the file.

This jira adds extra validation for the bulkloading file name format in HRegion.bulkLoadHFiles and also extends TestLoadIncrementalHFiles to run the same test suite with FILE based SFT enabled.",wchevreuil,wchevreuil,Major,Resolved,Fixed,07/Feb/23 14:29,11/Feb/23 00:50
Bug,HBASE-27621,13523746,Also clear the Dictionary when resetting when reading compressed WAL file,"After trying several times, now I can reproduce a critical problem when reading compressed WAL file in replication.

The problem is about how we construct the LRUDictionary when reset the WALEntryStream. In the current design, we will not reconstruct the LRUDictionary when reseting, but when reading again, we will call addEntry directly to add 'new' word into the dict, which will mess up the dict and cause data corruption.

I've implemented a UT to simulate reading partial WAL entry in replication, with the current code base, after reseting and reading again, we will stuck there for ever.

-The fix is to always use findEntry when constructing the dict when reading, so we will not mess things up.-
It turns out that the above solution does not work.
Another possible fix is to always reconstruct the dict after reseting, we will also clear the dict and reconstruct it again. But it is less efficient as we need to read from the beginning to the position we want to seek to, instead of seek to the position directly, especially when tailing the WAL file which is currently being written.

And notice that, the UT can only reproduce the problem in local file system, on HDFS, the available method is implemented so if there is not enough data, we will throw EOFException earlier before parsing cells with the compression decoder, so we will not add  duplicated word to dict. But in real world, it is possible that even if there are enough data to read, we could hit an IOException while reading and lead to the same problem described above.

And while fixing, I also found another problem that in TagConressionContext and CompressionContext, we use the result of InputStream incorrectly, as we just cast it to byte and test whether it is -1 to determine whether the field is in the dict. The return value of InputStream.read is an int, and it will return -1 if reaches EOF, but here we will consider it as not in dict... We should throw EOFException instead.

I'm not sure whether fix this can also fix HBASE-27073 but let's have a try first.",zhangduo,zhangduo,Critical,Resolved,Fixed,08/Feb/23 09:53,12/Feb/23 19:42
Bug,HBASE-27622,13523753,Bump cryptography from 3.3.2 to 39.0.1 in /dev-support/git-jira-release-audit,,,zhangduo,Major,Resolved,Fixed,08/Feb/23 11:04,09/Feb/23 19:10
Bug,HBASE-27624,13523883,Cannot Specify Namespace via the hbase.table Option in Spark Connector,"When using the old mapping format and specifying the HBase table via the 
_hbase.table_ option, the connector passes the namespaced string to HBase, and we get


{noformat}
Caused by: java.lang.IllegalArgumentException: Illegal character code:58, <:> at 7. User-space table qualifiers may only contain 'alphanumeric characters' and digits: staplesHbaseNamespace:staplesHbaseTableName
at org.apache.hadoop.hbase.TableName.isLegalTableQualifierName(TableName.java:187)
at org.apache.hadoop.hbase.TableName.isLegalTableQualifierName(TableName.java:138)
at org.apache.hadoop.hbase.TableName.<init>(TableName.java:320)
at org.apache.hadoop.hbase.TableName.createTableNameIfNecessary(TableName.java:354)
at org.apache.hadoop.hbase.TableName.valueOf(TableName.java:484){noformat}

This seems to be related to the changes in HBASE-24276",stoty,stoty,Major,Resolved,Fixed,09/Feb/23 06:16,01/Mar/23 14:37
Bug,HBASE-27628,13524007,Spotless fix in RELEASENOTES.md,"There is a whitespace vilotation in RELEASENOTES.md on branch-2.4 and branch-2.5 causing the pre-commit and nightly builds to fail.

{noformat}
[ERROR] Failed to execute goal com.diffplug.spotless:spotless-maven-plugin:2.27.2:check (default-cli) on project hbase: The following files had format violations:
[ERROR]     RELEASENOTES.md
[ERROR]         @@ -85,7 +85,7 @@
[ERROR]          
[ERROR]          *Â·[HBASE-27529](https://issues.apache.org/jira/browse/HBASE-27529)Â·|Â·*Major*Â·|Â·**ProvideÂ·RSÂ·coprocÂ·abilityÂ·toÂ·attachÂ·WALÂ·extendedÂ·attributesÂ·toÂ·mutationsÂ·atÂ·replicationÂ·sink**
[ERROR]          
[ERROR]         -NewÂ·regionserverÂ·coprocÂ·endpointsÂ·thatÂ·canÂ·beÂ·usedÂ·byÂ·coprocÂ·atÂ·theÂ·replicationÂ·sinkÂ·clusterÂ·ifÂ·WALÂ·hasÂ·extendedÂ·attributes.Â·
[ERROR]         +NewÂ·regionserverÂ·coprocÂ·endpointsÂ·thatÂ·canÂ·beÂ·usedÂ·byÂ·coprocÂ·atÂ·theÂ·replicationÂ·sinkÂ·clusterÂ·ifÂ·WALÂ·hasÂ·extendedÂ·attributes.
[ERROR]          UsingÂ·theÂ·newÂ·endpoints,Â·WALÂ·extendedÂ·attributesÂ·canÂ·beÂ·transferredÂ·toÂ·MutationÂ·attributesÂ·atÂ·theÂ·replicationÂ·sinkÂ·cluster.
{noformat}",psomogyi,psomogyi,Trivial,Resolved,Fixed,09/Feb/23 15:58,10/Feb/23 19:43
Bug,HBASE-27630,13524066,hbase-spark bulkload stage directory limited to hdfs only,"It's impossible to set up the staging directory for bulkload operation in spark-hbase connector to any other filesystem different from hdfs. That might be a problem for deployments where hbase.rootdir points to cloud storage. In this case, an additional copy task from hdfs to cloud storage would be required before loading hfiles to hbase.",sergey.soldatov,sergey.soldatov,Major,Resolved,Fixed,09/Feb/23 18:36,14/Feb/23 12:33
Bug,HBASE-27636,13524473,"The ""CREATE_TIME_TS"" value of the hfile generated by the HFileOutputFormat2 class is 0","When HFileOutputFormat2 is used to create an hfile, the CREATE_TIME_TS of hfileinfo will not be assigned when creating an hfileContext, resulting in the value of  lastMajorCompactionAge  is the timestamp of the current time.
{code:java}
##HFileOutputFormat2.class
HFileContextBuilder contextBuilder = new HFileContextBuilder()
                            .withCompression(compression)
                            .withChecksumType(HStore.getChecksumType(conf))
                            .withBytesPerCheckSum(HStore.getBytesPerChecksum(conf))
                            .withBlockSize(blockSize);

##get lastMajorCompactionTs metric

  lastMajorCompactionTs = this.region.getOldestHfileTs(true);
...
  long now = EnvironmentEdgeManager.currentTime();
  return now - lastMajorCompactionTs;
...
##
public long getOldestHfileTs(boolean majorCompactionOnly) throws IOException {
  long result = Long.MAX_VALUE;
  for (HStore store : stores.values()) {
    Collection<HStoreFile> storeFiles = store.getStorefiles();
   ...
    for (HStoreFile file : storeFiles) {
      StoreFileReader sfReader = file.getReader();
     ...
      result = Math.min(result, reader.getFileContext().getFileCreateTime());
    }
  }
  return result == Long.MAX_VALUE ? 0 : result;
}{code}
 

 ",selina.yan,selina.yan,Major,Resolved,Fixed,13/Feb/23 12:36,19/Feb/23 05:39
Bug,HBASE-27637,13524493,Zero length value would cause value compressor read nothing and not advance the position of the InputStream,"This is a code sniff from the discussion of HBASE-27073

{code}
  public static void main(String[] args) throws Exception {
    CompressionContext ctx =
      new CompressionContext(LRUDictionary.class, false, false, true, Compression.Algorithm.GZ);
    ValueCompressor compressor = ctx.getValueCompressor();
    byte[] compressed = compressor.compress(new byte[0], 0, 0);
    System.out.println(""compressed length: "" + compressed.length);
    ByteArrayInputStream bis = new ByteArrayInputStream(compressed);
    int read = compressor.decompress(bis, compressed.length, new byte[0], 0, 0);
    System.out.println(""read length: "" + read);
    System.out.println(""position: "" + (compressed.length - bis.available()));
{code}

And the output is
{noformat}
compressed length: 20
read length: 0
position: 0
{noformat}

So it turns out that, when compressing, an empty array will still generate some output bytes but while reading, we will skip reading anything if we find the output length is zero, so next time when we read from the stream, we will start at a wrong position...",zhangduo,zhangduo,Critical,Resolved,Fixed,13/Feb/23 15:13,16/Feb/23 05:49
Bug,HBASE-27644,13524992,Should not return false when WALKey has no following KVs while reading WAL file,"In the current implementation

{code}
      if (!walKey.hasFollowingKvCount() || 0 == walKey.getFollowingKvCount()) {
        LOG.trace(""WALKey has no KVs that follow it; trying the next one. current offset={}"",
          this.inputStream.getPos());
        seekOnFs(originalPosition);
        return false;
      }
{code}

Here we just return false, seek back to the original position. I think the intention here is that it means the data is not available yet and we should try to read them next time.

But this class is not only used for replication, it is also used by splitting, return false will make the reader.next return null, and WALSplitter will think the WAL file has been fully read and complete the splitting task. If there are still other WAL entries in the file, we will miss reading them and cause data loss.

And in fact, the following kv count is a field in a pb message, so it is impossible that now it is 0 but later it will become a value greater than 0, as we use writeDelimited to write the message, there is a size in front of the message, if we read it successfully, we can make sure the message is complete/ So seeking back in replication is also an useless operation.

So here we propose we still need to return true here, so the upper layer are free to skip or not, but they still need to read other entries.",zhangduo,zhangduo,Critical,Resolved,Fixed,16/Feb/23 09:47,27/Feb/23 05:21
Bug,HBASE-27648,13525034,CopyOnWriteArrayMap does not honor contract of ConcurrentMap.putIfAbsent,"I've been tracking down a meta cache issue and noticed this. I'm not sure if this is actually causing an issue, but if you enable TRACE logs on MetaCache it causes all calls to cacheLocation to go down the ""Merged cached locations"" path instead of the ""Cached location"" path for inserting new entries. This is just misleading, but also means going through the more expensive RegionLocations.mergeLocations. Reading the code, I don't think that would cause an actual issue, but hard to be sure.",bbeaudreault,bbeaudreault,Major,Resolved,Fixed,16/Feb/23 14:32,19/Feb/23 05:39
Bug,HBASE-27649,13525081,WALPlayer does not properly dedupe overridden cell versions,"If you do 2 Puts to a cell with different values but the same timestamp, the latest one will win. This is because in the memstore we use a sequenceId as a tie breaker for duplicate timestamps. When the data is flushed to a StoreFile, the deduplication will occur and eventually the sequenceId will be dropped.

Those 2 Puts would have been added to the WAL, and if you use WALPlayer to replay those WALs (as anyone could do, but also as backup/restore does for incremental restores) it will not properly do the same thing. It's unclear which of the duplicate cells you will get, when you should always get the latest.

Our WAL encoder doesn't include the sequenceIds in the WALEntry cells. Instead the WALKey has a getSequenceId() which contains the same sequenceId the cells used to have. In WALCellMapper we don't pass those along, nor in CellSerialization, and thus CellSortReducer is not able to use the sequenceId to dedupe.

I think we just need to translate the WALKey.getSequenceId() into the output Cells in WALCellMapper, then update CellSerialization to include them as well. At that point CellSortReducer should work as expected, and we should get the correct cell values in the hfiles.

One open question is whether we should clear out the sequenceId before flushing to the hfile. I don't think so?",bbeaudreault,bbeaudreault,Major,Resolved,Fixed,16/Feb/23 20:20,27/Feb/23 05:21
Bug,HBASE-27650,13525124,Merging empty regions corrupts meta cache,"Let's say you have three regions with start keys A, B, C and all are cached in the meta cache. Region B is empty and not getting any requests, and all 3 regions are merged together. The new merged region has start key A.

A user submits a request for row C1, which would previously have gone to region C. That region no longer exists, so the MetaCache returns region C, the request goes out to the server which throws NotServingRegionException. That region C is now removed from the cache, and meta is scanned. The meta scan returns the newly merged region A, which is cached into the MetaCache.

So now we have a MetaCache where A has been updated with the newly merged RegionInfo, B still exists with the old/deleted RegionInfo, and C has been removed.

A user submits a request for row C1 again. This _should_ go to region A, but we do cache.floorEntry(C1) which returns the old but still cached region B. We have checks in MetaCache which validate the RegionInfo.getEndKey() against the requested row, and that validation fails because C1 is beyond the endkey of the old region. The cached region B result is ignored and cache returns null. Meta is scanned, and returns the new region A, which is cached again.

Requests to rows C1+ will still succeed... but they will always require a meta scan because the meta cache will always return that old region B which is invalid and doesn't contain the C1+ rows.

Currently, the only way this will ever resolve is if a request is sent to region B, which will cause a NotServingRegionException which will finally clear region B from the cache. At that point, requests for C1+ will properly get resolved to region A in the cache.

I've created a reproducible test case here: [https://gist.github.com/bbeaudreault/c82ff9f8ad0b9424eb987483ede35c12]

This problem affects both AsyncTable and branch-2's Table.

 ",bbeaudreault,bbeaudreault,Major,Resolved,Fixed,17/Feb/23 03:19,01/Mar/23 05:46
Bug,HBASE-27651,13525179,hbase-daemon.sh foreground_start should propagate SIGHUP and SIGTERM,"Running HBase in a container environment, launched using {{{}bin/hbase-daemon.sh foreground_start ...{}}}. Signaling the container for config refresh or for graceful termination doesn't make it through to the JVM process. Rather, the {{trap}} function is unconditionally sending {{{}kill -9{}}}.",ndimiduk,ndimiduk,Minor,Resolved,Fixed,17/Feb/23 10:14,21/Mar/23 19:50
Bug,HBASE-27652,13525193,Client-side lock contention around Configuration when using read replica regions,Since upgrading to 2.5.1 our client-side application has noticed lock contention.,ndimiduk,ndimiduk,Major,Resolved,Fixed,17/Feb/23 11:40,16/Mar/23 20:35
Bug,HBASE-27654,13525312,IndexBlockEncoding is missing in HFileContextBuilder copy constructor,"{{IndexBlockEncoding}} is missing in {{HFileContextBuilder}} copy constructor, so if we want to construct a new {{HFileContext}}  by an existing {{HFileContext}}, we would missing the {{ndexBlockEncoding}} setting. ",comnetwork,comnetwork,Major,Resolved,Fixed,19/Feb/23 03:11,21/Feb/23 19:11
Bug,HBASE-27661,13525833,Set size of systable queue in UT,"TestSlowLogAccessor#testHigherSlowLogs failed in my CI pipeline.

!image-2023-02-23-14-52-38-867.png|width=654,height=409!

!image-2023-02-23-14-53-02-177.png|width=702,height=328!

SlowLogPersistentService use an [EvictingQueue|https://github.com/apache/hbase/blob/22dbb7afc383b9b9a8678f2c9eca1dca31784615/hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/SlowLogPersistentService.java#L58] to handle slowLogs. and default size is 1000.

So when the write speed is fast, this UT cannot guarantee that more than 2000 slowLogs will be stored in the table (they will be evicted in the queue).",tangtianhang,tangtianhang,Major,Resolved,Fixed,23/Feb/23 06:52,26/Feb/23 19:42
Bug,HBASE-27668,13526195,PB's parseDelimitedFrom can successfully return when there are not enough bytes,"Found this when writing some UTs for parsing partial header and trailer, WALHeader.parseDelimitedFrom can return successfully when there are only two bytes in the stream(only the length, actually).

So I know why in the past we have a followingKvCount == 0 check in ProtobufLogReader, as we just want to prevent the partial PB message.

This is a very critial problem, for me I think we should provide our own implementation of parseDelimitedFrom for some critical usages, for example, when reading WAL entries. If there are not enough data, we just throw exception out instead of returning a partial PB message.",zhangduo,zhangduo,Critical,Resolved,Fixed,26/Feb/23 06:58,01/Mar/23 05:46
Bug,HBASE-27671,13526317,Client should not be able to restore/clone a snapshot after it's TTL has expired,"Steps:

precondition : base.master.cleaner.snapshot.interval to 5 min in hbase-site.xml

1. create a table t1 , put some data

2. create a snapshot 'snapt1' with TTL 1 mins

let the TTL expries 

3. disable and drop table t1

4. restore snapshot t1

Actual : restore snapshot successful 

Expected : restore operation should fail and throw specified snapshot TTL expried cant restore

Note : its can consider as improvement point ",nihaljain.cs,Ashok shetty,Minor,Resolved,Fixed,27/Feb/23 10:41,22/Mar/23 04:29
Bug,HBASE-27673,13526338,Fix mTLS client hostname verification,"The exception what I get:

{noformat}
23/02/22 15:18:06 ERROR tls.HBaseTrustManager: Failed to verify host address: 127.0.0.1
javax.net.ssl.SSLPeerUnverifiedException: Certificate for <127.0.0.1> doesn't match any of the subject alternative names: [***]
	at org.apache.hadoop.hbase.io.crypto.tls.HBaseHostnameVerifier.matchIPAddress(HBaseHostnameVerifier.java:144)
	at org.apache.hadoop.hbase.io.crypto.tls.HBaseHostnameVerifier.verify(HBaseHostnameVerifier.java:117)
	at org.apache.hadoop.hbase.io.crypto.tls.HBaseTrustManager.performHostVerification(HBaseTrustManager.java:143)
	at org.apache.hadoop.hbase.io.crypto.tls.HBaseTrustManager.checkClientTrusted(HBaseTrustManager.java:97)
	...
23/02/22 15:18:06 ERROR tls.HBaseTrustManager: Failed to verify hostname: localhost
javax.net.ssl.SSLPeerUnverifiedException: Certificate for <localhost> doesn't match any of the subject alternative names: [***]
	at org.apache.hadoop.hbase.io.crypto.tls.HBaseHostnameVerifier.matchDNSName(HBaseHostnameVerifier.java:159)
	at org.apache.hadoop.hbase.io.crypto.tls.HBaseHostnameVerifier.verify(HBaseHostnameVerifier.java:119)
	at org.apache.hadoop.hbase.io.crypto.tls.HBaseTrustManager.performHostVerification(HBaseTrustManager.java:171)
	at org.apache.hadoop.hbase.io.crypto.tls.HBaseTrustManager.checkClientTrusted(HBaseTrustManager.java:97)
	...
23/02/22 15:18:06 WARN ipc.NettyRpcServer: Connection /100.100.124.2:47109; caught unexpected downstream exception.
org.apache.hbase.thirdparty.io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: Failed to verify both host address and host name
	at org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:499)
	at org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at org.apache.hbase.thirdparty.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:800)
	at org.apache.hbase.thirdparty.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:499)
	at org.apache.hbase.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:397)
	at org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at org.apache.hbase.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.apache.hbase.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:750)
Caused by: javax.net.ssl.SSLHandshakeException: Failed to verify both host address and host name
	at sun.security.ssl.Alert.createSSLException(Alert.java:131)
	at sun.security.ssl.TransportContext.fatal(TransportContext.java:324)
	at sun.security.ssl.TransportContext.fatal(TransportContext.java:267)
	at sun.security.ssl.TransportContext.fatal(TransportContext.java:262)
	at sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkClientCerts(CertificateMessage.java:700)
	at sun.security.ssl.CertificateMessage$T12CertificateConsumer.onCertificate(CertificateMessage.java:411)
	at sun.security.ssl.CertificateMessage$T12CertificateConsumer.consume(CertificateMessage.java:375)
	at sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:377)
	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:444)
	at sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:981)
	at sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:968)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.security.ssl.SSLEngineImpl$DelegatedTask.run(SSLEngineImpl.java:915)
	at org.apache.hbase.thirdparty.io.netty.handler.ssl.SslHandler.runDelegatedTasks(SslHandler.java:1549)
	at org.apache.hbase.thirdparty.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1395)
	at org.apache.hbase.thirdparty.io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1236)
	at org.apache.hbase.thirdparty.io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1285)
	at org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:529)
	at org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:468)
	... 15 more
Caused by: java.security.cert.CertificateException: Failed to verify both host address and host name
	at org.apache.hadoop.hbase.io.crypto.tls.HBaseTrustManager.performHostVerification(HBaseTrustManager.java:175)
	at org.apache.hadoop.hbase.io.crypto.tls.HBaseTrustManager.checkClientTrusted(HBaseTrustManager.java:97)
	at sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkClientCerts(CertificateMessage.java:682)
	... 29 more
Caused by: javax.net.ssl.SSLPeerUnverifiedException: Certificate for <localhost> doesn't match any of the subject alternative names: [***]
	at org.apache.hadoop.hbase.io.crypto.tls.HBaseHostnameVerifier.matchDNSName(HBaseHostnameVerifier.java:159)
	at org.apache.hadoop.hbase.io.crypto.tls.HBaseHostnameVerifier.verify(HBaseHostnameVerifier.java:119)
	at org.apache.hadoop.hbase.io.crypto.tls.HBaseTrustManager.performHostVerification(HBaseTrustManager.java:171)
	... 31 more
{noformat}

The connection was made from {{100.100.124.2:47109}}, however it tried to verify the certificate with {{localhost/127.0.0.1}}.",meszibalu,meszibalu,Major,Resolved,Fixed,27/Feb/23 13:09,02/Mar/23 06:24
Bug,HBASE-27684,13526924,Client metrics for user region lock related behaviors.,,vli02us,vli02us,Major,Resolved,Fixed,03/Mar/23 01:11,23/Mar/23 04:21
Bug,HBASE-27688,13527158,"HFile splitting occurs during bulkload, the CREATE_TIME_TS of hfileinfo is 0","If HFile splitting occurs during bulkload, the CREATE_TIME_TS of hfileinfo =0,When HFile is copied after splitting, CREATE_TIME_TS of the original file is not copied。
{code:java}
##BulkLoadHFilesTool.class 
/**
 * Copy half of an HFile into a new HFile.
 */
private static void copyHFileHalf(Configuration conf, Path inFile, Path outFile,
  Reference reference, ColumnFamilyDescriptor familyDescriptor) throws IOException {
  FileSystem fs = inFile.getFileSystem(conf);
  CacheConfig cacheConf = CacheConfig.DISABLED;
  HalfStoreFileReader halfReader = null;
  StoreFileWriter halfWriter = null;
  try {
    。。。
    HFileContext hFileContext = new HFileContextBuilder().withCompression(compression)
      .withChecksumType(StoreUtils.getChecksumType(conf))
      .withBytesPerCheckSum(StoreUtils.getBytesPerChecksum(conf)).withBlockSize(blocksize)
      .withDataBlockEncoding(familyDescriptor.getDataBlockEncoding()).withIncludesTags(true)
      .build();
// TODO .withCreateTime(EnvironmentEdgeManager.currentTime())      

halfWriter = new StoreFileWriter.Builder(conf, cacheConf, fs).withFilePath(outFile)
      .withBloomType(bloomFilterType).withFileContext(hFileContext).build();
    HFileScanner scanner = halfReader.getScanner(false, false, false);
    scanner.seekTo();
    do {
      halfWriter.append(scanner.getCell());
    } while (scanner.next());

    for (Map.Entry<byte[], byte[]> entry : fileInfo.entrySet()) {
      if (shouldCopyHFileMetaKey(entry.getKey())) {
        halfWriter.appendFileInfo(entry.getKey(), entry.getValue());
      }
    }
  } finally {
    。。。
  }
} 


##get lastMajorCompactionTs metric

  lastMajorCompactionTs = this.region.getOldestHfileTs(true);
...
  long now = EnvironmentEdgeManager.currentTime();
  return now - lastMajorCompactionTs;
...
##
public long getOldestHfileTs(boolean majorCompactionOnly) throws IOException {
  long result = Long.MAX_VALUE;
  for (HStore store : stores.values()) {
    Collection<HStoreFile> storeFiles = store.getStorefiles();
   ...
    for (HStoreFile file : storeFiles) {
      StoreFileReader sfReader = file.getReader();
     ...
      result = Math.min(result, reader.getFileContext().getFileCreateTime());
    }
  }
  return result == Long.MAX_VALUE ? 0 : result;
}{code}
 ",alanlemma,alanlemma,Major,Resolved,Fixed,06/Mar/23 02:45,16/Mar/23 06:34
Bug,HBASE-27690,13527427,Fix a misspell in TestRegionStateStore,,tangtianhang,tangtianhang,Minor,Resolved,Fixed,07/Mar/23 09:56,09/Mar/23 19:21
Bug,HBASE-27701,13527994,ZStdCodec codec implementation class documentation typo,As mentioned in the [user@hbase.apache.org|mailto:user@hbase.apache.org] mailing list I noticed a small typo in the documentation on compression for Zstd. The codec implementation class in the documentation is listed as {{org.apache.hadoop.hbase.io.compress.zstd.ZStdCodec}} while the actual class is written with a lower case S: {{ZStdCodec.}},frensjan,frensjan,Minor,Resolved,Fixed,10/Mar/23 15:03,14/Mar/23 19:22
Bug,HBASE-27704,13528043,Quotas can drastically overflow configured limit,"The original implementation did not allow exceeding quota. For example, you specify a limit of 10 resource/sec and consume 20 resources, it takes 1.1 seconds to be able submit another request. This was covered by the [testOverconsumption in TestRateLimiter|https://github.com/apache/hbase/blame/587b0b4f20bdc0415b6541023e611b69c87dba15/hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestRateLimiter.java#L97]. As an incidental part of HBASE-13686, that logic was changed. There is no mention of the reasoning behind the change in the issue comments or review board, I think it was missed. The goal of that issue was to add different refill strategies, but it also modified the over consumption. The testOverconsumption was [split out for both refill strategies|https://github.com/apache/hbase/blame/master/hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestRateLimiter.java#L104-L159], but the core reasoning was lost. The comment says:
{code:java}
// 10 resources are available, but we need to consume 20 resources109    
// Verify that we have to wait at least 1.1sec to have 1 resource available {code}
But the actual test was updated to only require a new resource after 100ms. This is incorrect. 

The problem is, when consuming if you go negative it sets to 0 [here|https://github.com/apache/hbase/blame/master/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RateLimiter.java#L187-L191]. Additionally, when refilling the new logic does a Math.max(0, available + refillAmount): [here|https://github.com/apache/hbase/blame/master/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RateLimiter.java#L159-L163]. So it's really impossible to get below 0, which is impractical for a rate limiter. 

With this setup it's very easy to drastically overconsume the rate limiter. See attached screenshot, which shows two humps. The first one has the current logic, the second hump has my fix which removes both of those problems. The rate limit was set to 500mb/s, but I was easily able to go over 700 mb/s without the fix.",bbeaudreault,bbeaudreault,Major,Resolved,Fixed,10/Mar/23 22:17,05/Apr/23 01:20
Bug,HBASE-27708,13528239,CPU hot-spot resolving User subject,"Even with OpenTelemetry tracing disabled, we see contention related to populating the string representation of the User principle on the client side. Can HBase connection cache this?",ndimiduk,ndimiduk,Major,Resolved,Fixed,13/Mar/23 13:20,17/Mar/23 19:32
Bug,HBASE-27712,13528336,Remove unused params in region metrics,"Histogram metrics in region have been removed in HBASE-17017, but there are some time cost params still left.

Need to remove them.",tangtianhang,tangtianhang,Major,Resolved,Fixed,14/Mar/23 03:57,02/Jun/23 19:01
Bug,HBASE-27714,13528401,WALEntryStreamTestBase creates a new HBTU in startCluster method which causes all sub classes are testing default configurations,"Should be a typo...

Let's fix this...",zhangduo,zhangduo,Major,Resolved,Fixed,14/Mar/23 10:47,16/Mar/23 06:34
Bug,HBASE-27716,13528403,Fix TestWALOpenAfterDNRollingStart,"Just a test issue, should use NoEOF reader.",zhangduo,zhangduo,Major,Resolved,Fixed,14/Mar/23 10:49,16/Mar/23 06:35
Bug,HBASE-27718,13528462,The regionStateNode only need remove once in regionOffline,"The regionStateNode only need remove once in regionOffline when delete a table.

In

org.apache.hadoop.hbase.master.assignment.AssignmentManager#deleteTable

 

why need delete twice? i think one is enougth",chaijunjie,chaijunjie,Minor,Resolved,Fixed,14/Mar/23 15:14,20/Mar/23 19:45
Bug,HBASE-27726,13528891,ruby shell not handled SyntaxError exceptions properly,"hbase:002:0> create 't2', 'cf'
2023-03-14 04:54:50,061 INFO  [main] client.HBaseAdmin: Operation: CREATE, Table Name: default:t2, procId: 2140 completed
Created table t2
Took 1.1503 seconds
=> Hbase::Table - t2
hbase:003:0> alter 't2', NAME ⇒ 'cf', VERSIONS ⇒ 5
SyntaxError: (hbase):3: syntax error, unexpected tIDENTIFIER
alter 't2', NAME ⇒ 'cf', VERSIONS ⇒ 5
                 ^~~
                      eval at org/jruby/RubyKernel.java:1091
                  evaluate at uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/irb/workspace.rb:85
                  evaluate at uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/irb/context.rb:385
                eval_input at uri:classloader:/irb/hirb.rb:115
             signal_status at uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/irb.rb:647
                eval_input at uri:classloader:/irb/hirb.rb:112
  each_top_level_statement at uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/irb/ruby-lex.rb:246
                      loop at org/jruby/RubyKernel.java:1507
  each_top_level_statement at uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/irb/ruby-lex.rb:232
                     catch at org/jruby/RubyKernel.java:1237
  each_top_level_statement at uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/irb/ruby-lex.rb:231
                eval_input at uri:classloader:/irb/hirb.rb:111
                       run at uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/irb.rb:428
                     catch at org/jruby/RubyKernel.java:1237
                       run at uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/irb.rb:427
<main> at classpath:/jar-bootstrap.rb:226",rishabhm7,chiruburri,Minor,Resolved,Fixed,17/Mar/23 03:34,30/Mar/23 06:39
Bug,HBASE-27729,13528934,Missed one parameter when logging exception in StoreFileListFile,"At line 265
{code}
      LOG.debug(""Failed to delete old track file {}, ignoring the exception"", e);
{code}

Should add 'trackFiles[nextTrackFile]' before 'e'.",slfan1989,zhangduo,Major,Resolved,Fixed,17/Mar/23 09:45,20/Mar/23 19:45
Bug,HBASE-27732,13529082,NPE in TestBasicWALEntryStreamFSHLog.testEOFExceptionInOldWALsDirectory,"https://ci-hbase.apache.org/job/HBase-Flaky-Tests/job/master/7985/testReport/junit/org.apache.hadoop.hbase.replication.regionserver/TestBasicWALEntryStreamFSHLog/testEOFExceptionInOldWALsDirectory_1__isCompressionEnabled_true_/
{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hbase.wal.AbstractFSWALProvider.findArchivedLog(AbstractFSWALProvider.java:488)
	at org.apache.hadoop.hbase.replication.regionserver.TestBasicWALEntryStream.testEOFExceptionInOldWALsDirectory(TestBasicWALEntryStream.java:771)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.apache.hadoop.hbase.SystemExitRule$1.evaluate(SystemExitRule.java:39)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:750)
{noformat}",zhangduo,zhangduo,Major,Resolved,Fixed,18/Mar/23 13:55,05/May/23 23:43
Bug,HBASE-27751,13529993,[hbase-operator-tools] TestMissingTableDescriptorGenerator fails with HBase 2.5.3,"hbase-operator-tools fails to compile against hbase 2.5.3 with following test failures.
{code:java}
[INFO] Running org.apache.hbase.TestMissingTableDescriptorGenerator
[ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 30.149 s <<< FAILURE! - in org.apache.hbase.TestMissingTableDescriptorGenerator
[ERROR] testTableinfoGeneratedWhenNoTableSpecified(org.apache.hbase.TestMissingTableDescriptorGenerator)  Time elapsed: 16.734 s  <<< ERROR!
java.lang.IllegalArgumentException: hdfs://localhost:51882/user/nihaljain/test-data/de8af727-6c02-7a95-9beb-027d18fc6603/data/default/test-1/.tabledesc/.tableinfo.0000000001.639
	at org.apache.hbase.TestMissingTableDescriptorGenerator.testTableinfoGeneratedWhenNoTableSpecified(TestMissingTableDescriptorGenerator.java:145)

[ERROR] shouldGenerateTableInfoBasedOnFileSystem(org.apache.hbase.TestMissingTableDescriptorGenerator)  Time elapsed: 6.794 s  <<< ERROR!
java.lang.IllegalArgumentException: hdfs://localhost:51961/user/nihaljain/test-data/5ade0aa1-cb9a-a1da-b700-fe808eeda3b9/data/default/test-1/.tabledesc/.tableinfo.0000000001.666
	at org.apache.hbase.TestMissingTableDescriptorGenerator.shouldGenerateTableInfoBasedOnFileSystem(TestMissingTableDescriptorGenerator.java:120)

[ERROR] shouldGenerateTableInfoBasedOnCachedTableDescriptor(org.apache.hbase.TestMissingTableDescriptorGenerator)  Time elapsed: 6.621 s  <<< ERROR!
java.lang.IllegalArgumentException: hdfs://localhost:52022/user/nihaljain/test-data/d858258b-6ba1-8e4f-c118-4e30d8a5136f/data/default/test-1/.tabledesc/.tableinfo.0000000001.666
	at org.apache.hbase.TestMissingTableDescriptorGenerator.shouldGenerateTableInfoBasedOnCachedTableDescriptor(TestMissingTableDescriptorGenerator.java:107)
{code}
Steps to reproduce, run following against hbase-operator-tools master:
{code:java}
mvn clean install -Dhbase.version=2.5.3 -Dhbase-thirdparty.version=4.1.4 {code}
The goal is to allow hbase-operator-tools to compile with hbase 2.5.3 without any failures",nihaljain.cs,nihaljain.cs,Minor,Resolved,Fixed,24/Mar/23 15:29,31/Mar/23 23:14
Bug,HBASE-27754,13530028,[HBCK2] generateMissingTableDescriptorFile should throw write permission error and fail,"Try running hbck2 generateMissingTableDescriptorFile with a user not having permissions to write to HDFS. 

*Actual* 
The tool completes with success message, while it actually does not really generate/write the files, as it does not even have permissions.

*Expected* 
Tool should throw error and should not log task is success 'Table descriptor written successfully. Orphan table xxxx fixed.'

*Debug dump* 

Upon enabling debug logging, we can see incorrect behaviour.
{code:java}
2023-03-24T19:03:16,890 DEBUG [IPC Parameter Sending Thread #0] ipc.Client: IPC Client (199657303) connection to hostname/ip_address:port_num from root sending #31 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
2023-03-24T19:03:16,893 DEBUG [IPC Client (199657303) connection to hostname/ip_address:port_num from root] ipc.Client: IPC Client (199657303) connection to hostname/ip_address:port_num from root got value #31
2023-03-24T19:03:16,894 DEBUG [main] ipc.ProtobufRpcEngine: Call: getFileInfo took 4ms
2023-03-24T19:03:16,894 DEBUG [main] hdfs.DFSClient: /apps/hbase/data/data/default/ittable-2090120905/.tmp/.tableinfo.0000000010: masked={ masked: rw-r--r--, unmasked: rw-rw-rw- }
2023-03-24T19:03:16,895 DEBUG [IPC Parameter Sending Thread #0] ipc.Client: IPC Client (199657303) connection to hostname/ip_address:port_num from root sending #32 org.apache.hadoop.hdfs.protocol.ClientProtocol.create
2023-03-24T19:03:16,897 DEBUG [IPC Client (199657303) connection to hostname/ip_address:port_num from root] ipc.Client: IPC Client (199657303) connection to hostname/ip_address:port_num from root got value #32
2023-03-24T19:03:16,898 DEBUG [main] retry.RetryInvocationHandler: Exception while invoking call #32 ClientNamenodeProtocolTranslatorPB.create over null. Not retrying because try once and fail.
org.apache.hadoop.ipc.RemoteException: Permission denied: user=root, access=WRITE, inode=""/apps/hbase/data/data/default/ittable-2090120905/.tmp"":hdfs:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1896)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1880)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1839)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2513)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2457)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:791)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:478)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1031)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:959)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2963)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1587) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.ipc.Client.call(Client.java:1533) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.ipc.Client.call(Client.java:1430) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-hadoop_version.jar:?]
	at com.sun.proxy.$Proxy25.create(Unknown Source) ~[?:?]
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:372) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:java_version]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:java_version]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:java_version]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:java_version]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) ~[hadoop-common-hadoop_version.jar:?]
	at com.sun.proxy.$Proxy26.create(Unknown Source) ~[?:?]
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1222) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1201) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1139) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:534) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:545) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:472) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1125) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1105) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:994) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hbase.HBCKFsTableDescriptors.writeTD(HBCKFsTableDescriptors.java:391) ~[hbase-hbck2-hbase_op_tools_version.jar:hbase_op_tools_version]
	at org.apache.hbase.HBCKFsTableDescriptors.writeTableDescriptor(HBCKFsTableDescriptors.java:365) ~[hbase-hbck2-hbase_op_tools_version.jar:hbase_op_tools_version]
	at org.apache.hbase.HBCKFsTableDescriptors.createTableDescriptorForTableDirectory(HBCKFsTableDescriptors.java:439) ~[hbase-hbck2-hbase_op_tools_version.jar:hbase_op_tools_version]
	at org.apache.hbase.HBCKFsTableDescriptors.createTableDescriptor(HBCKFsTableDescriptors.java:411) ~[hbase-hbck2-hbase_op_tools_version.jar:hbase_op_tools_version]
	at org.apache.hbase.MissingTableDescriptorGenerator.generateTableDescriptorFileIfMissing(MissingTableDescriptorGenerator.java:93) ~[hbase-hbck2-hbase_op_tools_version.jar:hbase_op_tools_version]
	at org.apache.hbase.HBCK2.doCommandLine(HBCK2.java:1034) ~[hbase-hbck2-hbase_op_tools_version.jar:hbase_op_tools_version]
	at org.apache.hbase.HBCK2.run(HBCK2.java:830) ~[hbase-hbck2-hbase_op_tools_version.jar:hbase_op_tools_version]
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hbase.HBCK2.main(HBCK2.java:1145) ~[hbase-hbck2-hbase_op_tools_version.jar:hbase_op_tools_version]
2023-03-24T19:03:16,902 DEBUG [main] hbase.HBCKFsTableDescriptors: Failed write and/or rename; retrying
org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode=""/apps/hbase/data/data/default/ittable-2090120905/.tmp"":hdfs:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1896)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1880)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1839)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2513)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2457)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:791)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:478)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1031)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:959)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2963)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:java_version]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:java_version]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:java_version]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:java_version]
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:281) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1222) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1201) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1139) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:534) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:545) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:472) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1125) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1105) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:994) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hbase.HBCKFsTableDescriptors.writeTD(HBCKFsTableDescriptors.java:391) ~[hbase-hbck2-hbase_op_tools_version.jar:hbase_op_tools_version]
	at org.apache.hbase.HBCKFsTableDescriptors.writeTableDescriptor(HBCKFsTableDescriptors.java:365) ~[hbase-hbck2-hbase_op_tools_version.jar:hbase_op_tools_version]
	at org.apache.hbase.HBCKFsTableDescriptors.createTableDescriptorForTableDirectory(HBCKFsTableDescriptors.java:439) ~[hbase-hbck2-hbase_op_tools_version.jar:hbase_op_tools_version]
	at org.apache.hbase.HBCKFsTableDescriptors.createTableDescriptor(HBCKFsTableDescriptors.java:411) ~[hbase-hbck2-hbase_op_tools_version.jar:hbase_op_tools_version]
	at org.apache.hbase.MissingTableDescriptorGenerator.generateTableDescriptorFileIfMissing(MissingTableDescriptorGenerator.java:93) ~[hbase-hbck2-hbase_op_tools_version.jar:hbase_op_tools_version]
	at org.apache.hbase.HBCK2.doCommandLine(HBCK2.java:1034) ~[hbase-hbck2-hbase_op_tools_version.jar:hbase_op_tools_version]
	at org.apache.hbase.HBCK2.run(HBCK2.java:830) ~[hbase-hbck2-hbase_op_tools_version.jar:hbase_op_tools_version]
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hbase.HBCK2.main(HBCK2.java:1145) ~[hbase-hbck2-hbase_op_tools_version.jar:hbase_op_tools_version]
Caused by: org.apache.hadoop.ipc.RemoteException: Permission denied: user=root, access=WRITE, inode=""/apps/hbase/data/data/default/ittable-2090120905/.tmp"":hdfs:hdfs:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1896)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1880)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1839)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2513)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2457)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:791)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:478)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1031)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:959)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2963)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1587) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.ipc.Client.call(Client.java:1533) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.ipc.Client.call(Client.java:1430) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-hadoop_version.jar:?]
	at com.sun.proxy.$Proxy25.create(Unknown Source) ~[?:?]
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:372) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:java_version]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:java_version]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:java_version]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:java_version]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-hadoop_version.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) ~[hadoop-common-hadoop_version.jar:?]
	at com.sun.proxy.$Proxy26.create(Unknown Source) ~[?:?]
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276) ~[hadoop-hdfs-client-hadoop_version.jar:?]
	... 21 more
2023-03-24T19:03:16,907 DEBUG [IPC Parameter Sending Thread #0] ipc.Client: IPC Client (199657303) connection to hostname/ip_address:port_num from root sending #33 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
2023-03-24T19:03:16,908 DEBUG [IPC Client (199657303) connection to hostname/ip_address:port_num from root] ipc.Client: IPC Client (199657303) connection to hostname/ip_address:port_num from root got value #33
2023-03-24T19:03:16,908 DEBUG [main] ipc.ProtobufRpcEngine: Call: getFileInfo took 2ms
2023-03-24T19:03:16,908 WARN  [main] hbase.HBCKFsTableDescriptors: Failed cleanup of hdfs://hostname:port_num/apps/hbase/data/data/default/ittable-2090120905/.tmp/.tableinfo.0000000010
2023-03-24T19:03:16,909 INFO  [main] hbase.MissingTableDescriptorGenerator: Table descriptor written successfully. Orphan table ittable-2090120905 fixed.
{code}
 ",nihaljain.cs,nihaljain.cs,Major,Resolved,Fixed,24/Mar/23 19:19,28/Mar/23 11:09
Bug,HBASE-27768,13530821,Race conditions in BlockingRpcConnection,"We've been experiencing strange timeouts since upgrading to hbase2 client. We use BlockingRpcConnection for now until we migrate our auth stack to native TLS. In diagnosing the timeouts, I noticed a few issues in this class:
 # Most importantly, there is a race condition which can result in a case where a BlockingRpcConnection instance has 2 reader threads running. In this case, both are competing for the socket and it causes weird timeouts and in some cases corrupted response (i.e. InvalidProtocolBufferException)
 # The waitForWork loop does not properly handle interruption. When it gets interrupted, if the above race condition occurs, the waitForWork loop ends up forever being in a tight loop. The ""wait()"" call instantly throws InterruptedException, and we set interrupted state back and restart the loop. So no waiting is occurring anymore.

The race condition is somewhat rare, only occurring in certain failure scenarios on our highest volume clients. But when it happens, a low level of errors will forever be thrown for the affected server connection until the client is bounced.",bbeaudreault,bbeaudreault,Major,Resolved,Fixed,30/Mar/23 14:47,11/Apr/23 19:52
Bug,HBASE-27778,13531449,Incorrect  ReplicationSourceWALReader. totalBufferUsed may cause replication hang up,"When we read a new WAL Entry in {{ReplicationSourceWALReader.readWALEntries}}, we add {{ReplicationSourceWALReader.totalBufferUsed}} by the size of new entry in   {{ReplicationSourceWALReader.addEntryToBatch}}, but the whole {{WALEntryBatch}} may not be put to the {{ReplicationSourceWALReader.entryBatchQueue}} because of exception(eg. exception thrown by {{WALEntryFilter.filter}} for following WAL Entry), and the {{ReplicationSourceWALReader.totalBufferUsed}} is not decreased in this case. Because the  {{ReplicationSourceWALReader.totalBufferUsed}}  is actually scoped to {{ReplicationSourceManager}}, after a long run, replication to all peers may hang up.",comnetwork,comnetwork,Major,Resolved,Fixed,04/Apr/23 13:42,08/Apr/23 04:23
Bug,HBASE-27782,13531744,"During SSL handshake error, netty complains that exceptionCaught() was not handled","I was chaos testing the new native TLS, forcing a certificate to expire and fail handshake. The handshake failure properly causes submitted requests to fail, but I see the following ""unhandled exception"" like message:
{code:java}
WARN  o.a.h.t.i.n.c.DefaultChannelPipeline - An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.
org.apache.hbase.thirdparty.io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: Received fatal alert: certificate_expired
        at org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:499)
        at org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
        at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
        at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
        at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
        at org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
        at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
        at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
        at org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
        at org.apache.hbase.thirdparty.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
        at org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
        at org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
        at org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
        at org.apache.hbase.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
        at org.apache.hbase.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: javax.net.ssl.SSLHandshakeException: Received fatal alert: certificate_expired
        at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:131)
        at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:117)
        at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:358)
        at java.base/sun.security.ssl.Alert$AlertConsumer.consume(Alert.java:293)
        at java.base/sun.security.ssl.TransportContext.dispatch(TransportContext.java:204)
        at java.base/sun.security.ssl.SSLTransport.decode(SSLTransport.java:172)
        at java.base/sun.security.ssl.SSLEngineImpl.decode(SSLEngineImpl.java:736)
        at java.base/sun.security.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:691)
        at java.base/sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:506)
        at java.base/sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:482)
        at java.base/javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:679)
        at org.apache.hbase.thirdparty.io.netty.handler.ssl.SslHandler$SslEngineType$3.unwrap(SslHandler.java:296)
        at org.apache.hbase.thirdparty.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1343)
        at org.apache.hbase.thirdparty.io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1236)
        at org.apache.hbase.thirdparty.io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1285)
        at org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:529)
        at org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:468)
        ... 17 common frames omitted{code}",zhangduo,bbeaudreault,Major,Resolved,Fixed,06/Apr/23 13:09,23/Jun/23 18:44
Bug,HBASE-27793,13532564,Make HBCK be able to report unknown servers,Currently hbck is not reporting unknown servers it would be helpful to report those as inconsistencies so that directly hbck2 schedulerecoveries  option can be used to recover on unknown servers otherwise the taking the action for inconsistencies reported due to unknown servers may corrupt if not done properly.,rajeshbabu,rajeshbabu,Major,Resolved,Fixed,13/Apr/23 19:23,02/Jun/23 06:47
Bug,HBASE-27796,13532691,Improve MemcachedBlockCache,"MemcachedBlockCache is apparently not used, given the bugs I have found, but provides some useful functionality that we are investigating at $dayjob. Fix it.

 - Track and log better stats

- Fix use of memcached API such that we are not accidentally immediately expiring keys.

- Use a default retention period of 30 days, the max per memcached spec

- Use set instead of add to store keys

- Gracefully handle failures to cache and read timeouts

- Add unit tests using jmemcached as a test dependency",apurtell,apurtell,Major,Resolved,Fixed,14/Apr/23 17:57,19/Apr/23 05:37
Bug,HBASE-27801,13532843,Remove redundant avro.version property from Kafka connector,"<avro.version>1.7.7</avro.version>

is defined both in the main connectors pom, and in the kafka module.
This is not useful.",stoty,stoty,Minor,Resolved,Fixed,17/Apr/23 08:13,17/Apr/23 08:55
Bug,HBASE-27807,13533498,PressureAwareCompactionThroughputController#tune log the opposite of the actual scenario,"In [https://github.com/apache/hbase/commit/07c71d630cd293ab55ac85e9bfa06033598134c4] ,    the condition of the if statement is incorrectly modified.

!image-2023-04-21-14-28-19-766.png!

 ",haosen chen,haosen chen,Trivial,Resolved,Fixed,21/Apr/23 06:26,24/Apr/23 17:41
Bug,HBASE-27810,13534028,HBCK throws RejectedExecutionException when closing ZooKeeper resources,"HBCK throws RejectedExecutionException at the end of run, because the order of closing ZooKeeper resources has been swapped in HBASE-27426.

In ZKWatcher.java close() method first it shuts down the zkEventProcessor and when it fully shut down, it closes the RecoverableZooKeeper (the ZK client). The watcher receives the close event which cannot be submitted to the event processor and throws exception.

I think we need to check whether the executor is able to receive jobs before submitting.",andor,andor,Major,Resolved,Fixed,25/Apr/23 15:49,02/May/23 05:35
Bug,HBASE-27822,13534449,TestFromClientSide5.testAppendWithoutWAL is flaky,"Not sure whether it is also flaky on branch-2.x, but at least on master, when we call Table.getScanner and get the returned ResultScanner, it does not mean that we have finished sending the request to region server, as the actual implementation is asynchronous. If we want a stable result, we need to at least get one row from the scanner.",zhangduo,zhangduo,Major,Resolved,Fixed,28/Apr/23 11:26,05/May/23 19:51
Bug,HBASE-27823,13534820,NPE in ClaimReplicationQueuesProcedure when running TestAssignmentManager.testAssignSocketTimeout,"{quote}
2023-05-03T05:56:10,094 ERROR [PEWorker-18 {}] procedure2.ProcedureExecutor(1669): CODE-BUG: Uncaught runtime exception: pid=8, ppid=5, state=RUNNABLE, hasLock=true; org.apache.hadoop.hbase.master.replication.ClaimReplicationQueuesProcedure
java.lang.NullPointerException: null
	at org.apache.hadoop.hbase.master.replication.ClaimReplicationQueuesProcedure.execute(ClaimReplicationQueuesProcedure.java:83) ~[classes/:?]
	at org.apache.hadoop.hbase.master.replication.ClaimReplicationQueuesProcedure.execute(ClaimReplicationQueuesProcedure.java:48) ~[classes/:?]
	at org.apache.hadoop.hbase.procedure2.Procedure.doExecute(Procedure.java:919) ~[hbase-procedure-3.0.0-alpha-4-SNAPSHOT.jar:3.0.0-alpha-4-SNAPSHOT]
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execProcedure(ProcedureExecutor.java:1650) ~[hbase-procedure-3.0.0-alpha-4-SNAPSHOT.jar:3.0.0-alpha-4-SNAPSHOT]
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.executeProcedure(ProcedureExecutor.java:1396) ~[hbase-procedure-3.0.0-alpha-4-SNAPSHOT.jar:3.0.0-alpha-4-SNAPSHOT]
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.access$1000(ProcedureExecutor.java:75) ~[hbase-procedure-3.0.0-alpha-4-SNAPSHOT.jar:3.0.0-alpha-4-SNAPSHOT]
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor$WorkerThread.runProcedure(ProcedureExecutor.java:1962) ~[hbase-procedure-3.0.0-alpha-4-SNAPSHOT.jar:3.0.0-alpha-4-SNAPSHOT]
	at org.apache.hadoop.hbase.trace.TraceUtil.trace(TraceUtil.java:216) ~[hbase-common-3.0.0-alpha-4-SNAPSHOT.jar:3.0.0-alpha-4-SNAPSHOT]
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor$WorkerThread.run(ProcedureExecutor.java:1989) ~[hbase-procedure-3.0.0-alpha-4-SNAPSHOT.jar:3.0.0-alpha-4-SNAPSHOT]
{quote}

We mock lots of things in this test so not sure this is a test issue or a real issue.

Need to dig more.

This should be a possible reason that why this test is flaky.",zhangduo,zhangduo,Major,Resolved,Fixed,03/May/23 12:07,05/May/23 19:51
Bug,HBASE-27824,13534822,NPE in MetricsMasterWrapperImpl.isRunning,"{quote}
2023-05-03T05:54:09,312 ERROR [HBase-Metrics2-1 {}] impl.MetricsSourceAdapter(202): Error getting metrics from source Master,sub=Server
java.lang.NullPointerException: null
	at org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.isRunning(MetricsMasterWrapperImpl.java:143) ~[classes/:?]
	at org.apache.hadoop.hbase.master.MetricsMasterSourceImpl.getMetrics(MetricsMasterSourceImpl.java:92) ~[hbase-hadoop-compat-3.0.0-alpha-4-SNAPSHOT.jar:?]
	at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:200) ~[hadoop-common-3.2.4.jar:?]
	at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:183) ~[hadoop-common-3.2.4.jar:?]
	at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:156) ~[hadoop-common-3.2.4.jar:?]
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333) ~[?:1.8.0_362]
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319) ~[?:1.8.0_362]
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522) ~[?:1.8.0_362]
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:98) ~[hadoop-common-3.2.4.jar:?]
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:72) ~[hadoop-common-3.2.4.jar:?]
	at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222) ~[hadoop-common-3.2.4.jar:?]
	at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:101) ~[hadoop-common-3.2.4.jar:?]
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268) ~[hadoop-common-3.2.4.jar:?]
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:239) ~[hadoop-common-3.2.4.jar:?]
	at sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_362]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_362]
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:324) ~[hadoop-common-3.2.4.jar:?]
	at com.sun.proxy.$Proxy31.postStart(Unknown Source) ~[?:?]
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:193) ~[hadoop-common-3.2.4.jar:?]
	at org.apache.hadoop.metrics2.impl.JmxCacheBuster$JmxCacheBusterRunnable.run(JmxCacheBuster.java:102) ~[hbase-hadoop-compat-3.0.0-alpha-4-SNAPSHOT.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_362]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_362]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_362]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_362]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_362]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_362]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_362]
{quote}

Found this in the output when running TestAssignmentManager.testAssignSocketTimeout. It should not be related to the failure of the test but anyway, throwing NPE is not good so we need to check whether this is a test issue or a real issue in our code.",zhangduo,zhangduo,Major,Resolved,Fixed,03/May/23 12:09,05/May/23 19:52
Bug,HBASE-27851,13535534,TestListTablesByState is silently failing due to a surefire bug,"Surefire version 3.0.0-M6 has a bug where tests end up being removed from the test results if they fail with a long exception message. See:
 
https://issues.apache.org/jira/browse/SUREFIRE-2079
 
org.apache.hadoop.hbase.master.TestListTablesByState is currently failing in CI due to an error. However, it does not show up in the Test Results because of the surefire bug.
 
If you download the raw test_logs from the build artifacts, you will find the files:
/home/jenkins/jenkins-home/workspace/HBase_Nightly_master/output-jdk8-hadoop3/archiver/hbase-server/target/surefire-reports/org.apache.hadoop.hbase.master.TestListTablesByState.txt
 
which contains:
{{-----------------------------------------------------------------------}}

{{Test set: org.apache.hadoop.hbase.master.TestListTablesByState}}

{{-----------------------------------------------------------------------}}

{{Tests run: 0, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.929 s - in org.apache.hadoop.hbase.master.TestListTablesByState}}

 
and /home/jenkins/jenkins-home/workspace/HBase_Nightly_master/output-jdk8-hadoop3/archiver/hbase-server/target/surefire-reports/org.apache.hadoop.hbase.master.TestListTablesByState-output.txt
 
which contains exceptions like:
{{...}}

{{2023-05-04T11:41:56,262 INFO  [RPCClient-NioEventLoopGroup-4-3 {}] client.RawAsyncHBaseAdmin$TableProcedureBiConsumer(2603): Operation: CREATE, Table Name: default:test failed with org.apache.hadoop.hbase.DoNotRetryIOException: Table test should have at least one column family.}}

{{        at}}

{{...}}

 
I found this while testing the final surfire 3.0.0 version which fixes the bug and the test then shows up as failing.",jonathan.albrecht,jonathan.albrecht,Minor,Resolved,Fixed,09/May/23 15:45,12/May/23 19:22
Bug,HBASE-27857,13535785,HBaseClassTestRule: system exit not restored if test times out may cause test to hang,"HBaseClassTestRule applies a timeout and a system exit rule to tests. The timeout rule throws an exception if it hits the timeout threshold. Since the timeout rule is applied after the system exit rule, the system exit rule does not see the exception and does not re-enable the default system exit behavior which can cause maven to hang on some tests. I saw the hang happen when certain tests timed out on s390x but it could happen on any platform.
 
If the org.apache.hadoop.hbase.TestTimeout.infiniteLoop test is enabled and run it will generate a *-jvmRun1.dump file which shows that the org.apache.hadoop.hbase.TestSecurityManager is still enabled:
 
{quote}# Created at 2023-04-27T15:51:58.947
org.apache.hadoop.hbase.SystemExitRule$SystemExitInTestException
        at org.apache.hadoop.hbase.TestSecurityManager.checkExit(TestSecurityManager.java:32)
        at java.base/java.lang.Runtime.exit(Runtime.java:114)
        at java.base/java.lang.System.exit(System.java:1752)
        at org.apache.maven.surefire.booter.ForkedBooter.acknowledgedExit(ForkedBooter.java:381)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:178)
        at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:507)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:495)
...{quote}
 ",jonathan.albrecht,jonathan.albrecht,Minor,Resolved,Fixed,11/May/23 12:05,14/May/23 05:52
Bug,HBASE-27859,13535824,HMaster.getCompactionState can happen NPE when region state is closed,"Following steps to reproduce:

1 create table 
{code:java}
create 'hbase_region_test', 'info', SPLITS => ['3', '7']  {code}
 

2 write data
{code:java}
//代码占位符
put 'hbase_region_test', '10010', 'info:name', 'Tom'
put 'hbase_region_test', '20010', 'info:name', 'Tom'
put 'hbase_region_test', '30010', 'info:name', 'Tom'
put 'hbase_region_test', '40010', 'info:name', 'Tom'
put 'hbase_region_test', '50010', 'info:name', 'Tom'
put 'hbase_region_test', '60010', 'info:name', 'Tom'
put 'hbase_region_test', '70010', 'info:name', 'Tom'
put 'hbase_region_test', '80010', 'info:name', 'Tom'
put 'hbase_region_test', '90010', 'info:name', 'Tom' {code}
 

3 closed a region of hbase_region_test by unassign 

 

4 calling method HMaster.getCompactionState:    

At this step, we can trigger this method to be called by opening hbase UI page about 'hbase_region_test' table detailes and getting the compaction state of 'hbase_region_test' samply

 

5 HMaster print NPE logs about HMaster.getCompactionState",guluo,guluo,Major,Resolved,Fixed,11/May/23 15:11,07/Jul/23 05:49
Bug,HBASE-27860,13535898,Fix build error against Hadoop 3.3.5,"Build with hadoop 3.3.5 will fail with following messages.

Some packages are not included in shaded jar.
{code:java}
$ mvn clean install -DskipTests -Phadoop-3.0 -Dhadoop-three.version=3.3.5
...
[INFO] --- exec:1.6.0:exec (check-jar-contents-for-stuff-with-hadoop) @ hbase-shaded-with-hadoop-check-invariants ---
[ERROR] Found artifact with unexpected contents: '/home/yamasakisua/hbase/hbase-shaded/hbase-shaded-client/target/hbase-shaded-client-2.4.17.jar'
    Please check the following and either correct the build or update
    the allowed list with reasoning.    com/
    com/sun/
    com/sun/jersey/
    com/sun/jersey/json/ {code}",yamasakisua,yamasakisua,Major,Resolved,Fixed,12/May/23 01:25,14/May/23 05:52
Bug,HBASE-27865,13536130,TestThriftServerCmdLine fails with org.apache.hadoop.hbase.SystemExitRule$SystemExitInTestException,"https://ci-hbase.apache.org/job/HBase%20Nightly/job/master/839/testReport/junit/org.apache.hadoop.hbase.thrift/TestThriftServerCmdLine/health_checks___yetus_jdk11_hadoop3_checks___testRunThriftServer_13_/

{noformat}
org.apache.hadoop.hbase.SystemExitRule$SystemExitInTestException
	at org.apache.hadoop.hbase.TestSecurityManager.checkExit(TestSecurityManager.java:32)
	at java.base/java.lang.Runtime.exit(Runtime.java:114)
	at java.base/java.lang.System.exit(System.java:1752)
	at org.apache.hadoop.hbase.thrift.ThriftServer$2.run(ThriftServer.java:850)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:361)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1742)
	at org.apache.hadoop.hbase.thrift.ThriftServer.run(ThriftServer.java:836)
	at org.apache.hadoop.hbase.thrift.ThriftServerRunner.run(ThriftServerRunner.java:54)
{noformat}",zhangduo,zhangduo,Major,Resolved,Fixed,14/May/23 15:37,16/May/23 05:50
Bug,HBASE-27867,13536318,Close the L1 victim handler race,"When we evict a block from L1 and move it to L2 there is a brief window of time where we have removed the block from the L1 map and yet the victim handler has not completed execution. Some read-your-write use cases can be significantly impacted even though the window is small. Imagine a use case where PRELOAD_DATA_ON_OPEN and CACHE_DATA_ON_WRITE are both enabled to warm cache ahead of access and any miss can be problematic. Perhaps a particularly latency sensitive case with HFiles backed by S3. 

Victim handling can be made atomic with respect to the unmapping operation with ConcurrentHashMap#computeIfPresent. The upside is there will be no L1+L2 misses during the transition. The downside is if the victim handler takes a long time to execute -- currently they are all very fast, so only a theoretical risk -- then other removals or insertions in L1 can block until it completes. ",apurtell,apurtell,Minor,Resolved,Fixed,15/May/23 21:14,27/May/23 06:07
Bug,HBASE-27871,13536581,Meta replication stuck forever if wal it's still reading gets rolled and deleted,"This affects branch-2 based releases only (in master, HBASE-26416 refactored region replication to not rely on the replication framework anymore).

Per the original [meta region replicas design|https://docs.google.com/document/d/1jJWVc-idHhhgL4KDRpjMsQJKCl_NRaCLGiH3Wqwd3O8/edit], we use most of the replication framework for communicating changes in the primary replica back to the secondary ones, but we skip storing the queue state in ZK. In the event of a region replication crash, we should let the related replication source thread be interrupted, so that 
RegionReplicaReplicationEndpoint would set a new source from the scratch and make sure to update the secondary replicas.
 
We have run into a situation in one of our customers' cluster where the region replica source faced a long lag (probably because the RSes hosting the secondary replicas were busy and slower in processing the region replication entries), so that the current wal got rolled and eventually deleted whilst the replication source reader was still referring it. In such cases, ReplicationSourceReader only sees the IOException and keeps retrying the read indefinitely, but since the file is now gone, it will just get stuck there forever. In the particular case of FNFE (which I believe would only happen for region replication), we should just raise an exception and let RegionReplicaReplicationEndpoint handle it to reset the region replication source.
 
 ",wchevreuil,wchevreuil,Major,Resolved,Fixed,17/May/23 14:44,21/Jun/23 03:30
Bug,HBASE-27872,13536649,xerial's snappy-java requires GLIBC >= 2.32,"We need to add a native library load check with a helpful error message if it fails to load due to a too old glibc or similar reason, and disable the unit test if the native library fails to load. 

{noformat}
TestHFileCompressionSnappy.test:54->HFileTestBase.doTest:72 »
UnsatisfiedLink /home/apurtell/src/hbase/hbase-compression/hbase-compression-snappy/target/snappy-1.1.9-6406bd03-44b4-4586-a91b-79d213c14062-libsnappyjava.so:
/lib/aarch64-linux-gnu/libc.so.6: version `GLIBC_2.32'
{noformat}

In this case a Debian test VM has ""Debian GLIBC 2.31-13+deb11u6"" installed, which is too old. ",apurtell,apurtell,Minor,Resolved,Fixed,18/May/23 04:54,27/May/23 06:07
Bug,HBASE-27874,13536754,Problem in flakey generated report causes pre-commit run to fail,"Have noticed the UT pre-commit run failed on this [latest PR for branch-2|https://ci-hbase.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-5241/3/artifact/yetus-jdk8-hadoop2-check/output/patch-unit-hbase-server.txt] with the below:
{noformat}
Thu May 18 10:37:32 AM UTC 2023
cd /home/jenkins/jenkins-home/workspace/Base-PreCommit-GitHub-PR_PR-5241/yetus-jdk8-hadoop2-check/src/hbase-server
/opt/maven/bin/mvn --batch-mode -Dmaven.repo.local=/home/jenkins/jenkins-home/workspace/Base-PreCommit-GitHub-PR_PR-5241/yetus-m2/hbase-branch-2-patch-1 --threads=4 -Djava.io.tmpdir=/home/jenkins/jenkins-home/workspace/Base-PreCommit-GitHub-PR_PR-5241/yetus-jdk8-hadoop2-check/src/target -DHBasePatchProcess -PrunAllTests -Dtest.exclude.pattern=**/regionserver.TestMetricsRegionServer.java,**/master.procedure.TestSnapshotProcedureRSCrashes.java,**/security.access.TestAccessController.java,**/conf.TestConfigurationManagerWARNING: package jdk.internal.util.random not in java.base.java,**/io.hfile.bucket.TestPrefetchPersistence.java,**/client.TestFromClientSide3.java,**/replication.TestReplicationMetricsforUI.java,**/io.hfile.bucket.TestBucketCache.java,**/replication.regionserver.TestReplicationValueCompressedWAL.java,**/master.procedure.TestHBCKSCP.java,**/http.TestInfoServersACL.java,**/io.hfile.bucket.TestBucketCachePersister.java,**/replication.TestReplicationKillSlaveRS.java,**/regionserver.TestClearRegionBlockCache.java,**/master.TestUnknownServers.java,**/replication.TestReplicationKillSlaveRSWithSeparateOldWALs.java,**/quotas.TestClusterScopeQuotaThrottle.java,**/io.hfile.TestBlockEvictionOnRegionMovement.java,**/replication.regionserver.TestMetaRegionReplicaReplicationEndpoint.java,**/regionserver.TestRegionReplicas.java,**/coprocessor.TestCoprocessorEndpointTracing.java,**/master.region.TestMasterRegionCompaction.java,**/io.hfile.TestPrefetchRSClose.java -Dsurefire.firstPartForkCount=0.5C -Dsurefire.secondPartForkCount=0.5C clean test -fae
....
------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  0.861 s (Wall Clock)
[INFO] Finished at: 2023-05-18T10:37:34Z
[INFO] ------------------------------------------------------------------------
[ERROR] Unknown lifecycle phase ""jdk.internal.util.random"". You must specify a valid lifecycle phase or a goal in the format <plugin-prefix>:<goal> or <plugin-group-id>:<plugin-artifact-id>[:<plugin-version>]:<goal>. Available lifecycle phases are: validate, initialize, generate-sources, process-sources, generate-resources, process-resources, compile, process-classes, generate-test-sources, process-test-sources, generate-test-resources, process-test-resources, test-compile, process-test-classes, test, prepare-package, package, pre-integration-test, integration-test, post-integration-test, verify, install, deploy, pre-clean, clean, post-clean, pre-site, site, post-site, site-deploy. -> [Help 1]
[ERROR] 
{noformat}
Note the ""{+}**/conf.TestConfigurationManagerWARNING: package jdk.internal.util.random not in java.base.java{+}"" passed as one of the supposedly flakey tests. Looking around our build scripts, I figured we pull the list of flakey from the ""{+}excludes{+}"" artifact generated by the latest ""find flakey"" build. It seems the [latest branch-2 run|https://ci-hbase.apache.org/job/HBase-Find-Flaky-Tests/job/branch-2/1063/artifact/output/excludes] generated this artifact with the wrong name already:

{noformat}
**/replication.TestReplicationMetricsforUI.java,**/conf.TestConfigurationManagerWARNING: package jdk.internal.util.random not in java.base.java,**/master.region.TestMasterRegionCompaction.java,**/regionserver.TestRegionReplicas.java,**/replication.regionserver.TestReplicationValueCompressedWAL.java,**/coprocessor.TestCoprocessorEndpointTracing.java,**/quotas.TestClusterScopeQuotaThrottle.java,**/replication.TestReplicationKillSlaveRSWithSeparateOldWALs.java,**/client.TestFromClientSide3.java,**/io.hfile.TestBlockEvictionOnRegionMovement.java,**/io.hfile.bucket.TestPrefetchPersistence.java,**/regionserver.TestMetricsRegionServer.java,**/io.hfile.bucket.TestBucketCachePersister.java,**/regionserver.TestClearRegionBlockCache.java,**/master.procedure.TestHBCKSCP.java,**/replication.regionserver.TestMetaRegionReplicaReplicationEndpoint.java,**/security.access.TestAccessController.java,**/io.hfile.bucket.TestBucketCache.java,**/io.hfile.TestPrefetchRSClose.java,**/replication.TestReplicationKillSlaveRS.java,**/master.TestUnknownServers.java,**/http.TestInfoServersACL.java
{noformat}

Digging deeper, found that the ""find flakey"" build checks the UT output of latest nightly and flakey builds, to parse it and generate the report. In some of the builds, such as [this one|https://ci-hbase.apache.org/job/HBase%20Nightly/job/branch-2/812/artifact/output-jdk11-hadoop3/patch-unit-root.txt], results can be malformed, merging test names and WARNING messages in same line:

{noformat}
[INFO] Running org.apache.hadoop.hbase.conf.TestConfigurationManagerWARNING: package jdk.internal.util.random not in java.base

[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.145 s - in org.apache.hadoop.hbase.conf.TestConfigurationManager
{noformat}

Thought about modifying the python script that generates the flakey report to also consider this malformed pattern when parsing test names.",wchevreuil,wchevreuil,Major,Resolved,Fixed,18/May/23 14:57,23/May/23 04:30
Bug,HBASE-27894,13537864,create-release is broken by recent gitbox changes,"The error looks like:
{noformat}
...
GIT_BRANCH [branch-2.5]: 
-:1: parser error : Space required after the Public Identifier
<!DOCTYPE HTML PUBLIC ""-//IETF//DTD HTML 2.0//EN"">
{noformat}
but is misleading. What is going on is create-release tries to retrieve the top level POM from the specified branch using a gitbox URL but gitbox is returning an HTML redirect, which is not what is expected.
{noformat}
++ read -r -p 'GIT_BRANCH [branch-2.5]: ' REPLY
GIT_BRANCH [branch-2.5]: 
++ local RETVAL=branch-2.5
++ '[' -z branch-2.5 ']'
++ echo branch-2.5
+ GIT_BRANCH=branch-2.5
+ export GIT_BRANCH
+ local version
++ curl -s 'https://gitbox.apache.org/repos/asf?p=hbase.git;a=blob_plain;f=pom.xml;hb=refs/heads/branch-2.5'
++ parse_version
++ xmllint --xpath '//*[local-name()='\''project'\'']/*[local-name()='\''version'\'']/text()' -
-:1: parser error : Space required after the Public Identifier
<!DOCTYPE HTML PUBLIC ""-//IETF//DTD HTML 2.0//EN"">
{noformat}
{noformat}
$ curl -s 'https://gitbox.apache.org/repos/asf?p=hbase.git;a=blob_plain;f=pom.xml;hb=refs/heads/branch-2.5'
<!DOCTYPE HTML PUBLIC ""-//IETF//DTD HTML 2.0//EN"">
<html><head>
<title>302 Found</title>
</head><body>
<h1>Found</h1>
<p>The document has moved <a href=""https://raw.githubusercontent.com/apache/hbase/refs/heads/branch-2.5/pom.xml"">here</a>.</p>
</body></html>
{noformat}
The solution is to retrieve content using github URLs (via raw.githubusercontent.com) instead of gitbox URLs.",apurtell,apurtell,Major,Resolved,Fixed,27/May/23 00:38,17/Jun/23 19:21
Bug,HBASE-27923,13539372,NettyRpcServer may hange if it should skip initial sasl handshake,"{{NettyRpcServer}} may hange if  it should skip initial sasl handshake when server does not enable security  and client enables security,  I think this problem is caused by two reasons：
* For Server:
  The type of the response is  {{RpcResponse}}, but for {{NettyRpcServerPreambleHandler}},when it  send {{RpcResponse}} ,{{NettyRpcServerResponseEncoder}} does not exist, so {{RpcResponse}} messages cannot be sent.

* For Client
  When {{NettyHBaseSaslRpcClientHandler}} receives {{SaslUtil.SWITCH_TO_SIMPLE_AUTH}}, it does not remove {{SaslChallengeDecoder}} and {{NettyHBaseSaslRpcClientHandler}}, so the latter responses are considered to be incorrect.
",comnetwork,comnetwork,Major,Resolved,Fixed,09/Jun/23 06:09,13/Jun/23 05:54
Bug,HBASE-27924,13539619,Remove duplicate code for NettyHBaseSaslRpcServerHandler and make the sentByte metrics more accurate,"{{NettyHBaseSaslRpcServerHandler.doResponse}}  and  {{ServerRpcConnection.doRawSaslReply}} are very similar, I think we could replace {{NettyHBaseSaslRpcServerHandler.doResponse}}  with {{ServerRpcConnection.doRawSaslReply}}： 

{code:java}
private void doResponse(ChannelHandlerContext ctx, SaslStatus status, Writable rv,
    String errorClass, String error) throws IOException {
    // In my testing, have noticed that sasl messages are usually
    // in the ballpark of 100-200. That's why the initial capacity is 256.
    ByteBuf resp = ctx.alloc().buffer(256);
    try (ByteBufOutputStream out = new ByteBufOutputStream(resp)) {
      out.writeInt(status.state); // write status
      if (status == SaslStatus.SUCCESS) {
        rv.write(out);
      } else {
        WritableUtils.writeString(out, errorClass);
        WritableUtils.writeString(out, error);
      }
    }
    NettyFutureUtils.safeWriteAndFlush(ctx, resp);
  }
{code}


{code:java}
protected final void doRawSaslReply(SaslStatus status, Writable rv, String errorClass,
    String error) throws IOException {
    BufferChain bc;
    // In my testing, have noticed that sasl messages are usually
    // in the ballpark of 100-200. That's why the initial capacity is 256.
    try (ByteBufferOutputStream saslResponse = new ByteBufferOutputStream(256);
      DataOutputStream out = new DataOutputStream(saslResponse)) {
      out.writeInt(status.state); // write status
      if (status == SaslStatus.SUCCESS) {
        rv.write(out);
      } else {
        WritableUtils.writeString(out, errorClass);
        WritableUtils.writeString(out, error);
      }
      bc = new BufferChain(saslResponse.getByteBuffer());
    }
    doRespond(() -> bc);
  }
{code}

At the same time, {{NettyHBaseSaslRpcServerHandler.doResponse}}  sends ByteBuf directly , not the unified {{RpcResponse}} , so  it would not handled by the logic in  {{NettyRpcServerResponseEncoder.write}}, which would update the {{MetricsHBaseServer.sentBytes}}.  Using   {{ServerRpcConnection.doRawSaslReply}} uniformly would make the {{MetricsHBaseServer.sentBytes}} more accurate.",comnetwork,comnetwork,Major,Resolved,Fixed,12/Jun/23 08:17,17/Jun/23 19:21
Bug,HBASE-27936,13540244,NPE in StoreFileReader.passesGeneralRowPrefixBloomFilter(),"When executing itbll, we encountered the following NPE exception：
{code:java}
java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.StoreFileReader.passesGeneralRowPrefixBloomFilter(StoreFileReader.java:352)
        at org.apache.hadoop.hbase.regionserver.StoreFileReader.passesBloomFilter(StoreFileReader.java:265)
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.shouldUseScanner(StoreFileScanner.java:483)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.selectScannersFrom(StoreScanner.java:467)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:320)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:289)
        at org.apache.hadoop.hbase.regionserver.compactions.Compactor.createScanner(Compactor.java:544)
        at org.apache.hadoop.hbase.regionserver.compactions.Compactor$1.createScanner(Compactor.java:269)
        at org.apache.hadoop.hbase.regionserver.compactions.Compactor.compact(Compactor.java:358)
        at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:64)
        at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:122)
        at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1176)
        at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:2407)
        at org.apache.hadoop.hbase.regionserver.CompactSplit$CompactionRunner.doCompaction(CompactSplit.java:667)
        at org.apache.hadoop.hbase.regionserver.CompactSplit$CompactionRunner.run(CompactSplit.java:716)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
{code}


",zhangduo,heliangjun,Major,Resolved,Fixed,15/Jun/23 14:16,23/Jun/23 18:44
Bug,HBASE-27940,13540468,Midkey metadata in root index block would always be ignored by BlockIndexReader.readMultiLevelIndexRoot,"After HBASE-27053， checksum is removed from the {{HFileBlock}} {{ByteBuff}}  in {{FSReaderImpl.readBlockDataInternal}}  once the checksum is verified, so {{HFileBlock.buf}} does not include checksum, but for {{BlockIndexReader.readMultiLevelIndexRoot}}, after read root index entries , it still subtracts the checksum to check if the midkey metadat exists,  the midkey metadata would always be ignored: 

{code:java}
public void readMultiLevelIndexRoot(HFileBlock blk, final int numEntries) throws IOException {
      DataInputStream in = readRootIndex(blk, numEntries);
      // after reading the root index the checksum bytes have to
      // be subtracted to know if the mid key exists.
      int checkSumBytes = blk.totalChecksumBytes();
      if ((in.available() - checkSumBytes) < MID_KEY_METADATA_SIZE) {
        // No mid-key metadata available.
        return;
      }
      midLeafBlockOffset = in.readLong();
      midLeafBlockOnDiskSize = in.readInt();
      midKeyEntry = in.readInt();
 }
{code}
",comnetwork,comnetwork,Major,Resolved,Fixed,17/Jun/23 03:10,18/Jun/23 20:15
Bug,HBASE-27942,13540771,The description about hbase.hstore.comactionThreshold is not accurate,"In document hbase-default.xml， The  description about [hbase.hstore.comactionThreshold|http://hbase.hstore.comactionthreshold/] is not accurate，as follow：

!2023-06-20_221700.png!

Actually，hbase will run compaction when the number of store files greater than or equal to [hbase.hstore.comactionThreshold|http://hbase.hstore.comactionthreshold/], as follow

!2023-06-20_221821.png!

 ",guluo,guluo,Minor,Resolved,Fixed,20/Jun/23 14:24,06/Jul/23 21:27
Bug,HBASE-27950,13541180,ClientSideRegionScanner does not adhere to RegionScanner.nextRaw contract,A follow-on for HBASE-26630 so that we can fix the actual logic in ClientSideRegionScanner so that it works for all ReadTypes and/or when used outside of the context of TableSnapshotInputFormat,hgromer,bbeaudreault,Minor,Resolved,Fixed,23/Jun/23 13:45,24/Jun/23 19:55
Bug,HBASE-27951,13541222,Use ADMIN_QOS in MasterRpcServices for regionserver operational dependencies,"Analysis of a recent production incident is not yet complete but an item of note is an apparent deadlock. Imagine you are gracefully draining a regionserver by way of a flurry of moveRegion requests. The handler for moveRegion submits a TRSP and then waits on its future without timeout. Imagine that there are sufficient number of moveRegion requests to tie up the normal priority master RPC pool. Now imagine that all of those requests are waiting on TRSPs pending on a regionserver that is concurrently bounced or maybe it fails. The TRSPs are blocked in REGION_STATE_TRANSITION_CLOSE because the target regionserver terminated before responding to the close requests, blocking the moveRegion requests, blocking the RPC handlers. The regionserver restarts and tries to check in, but cannot report to the master because there are no free normal priority handlers to handle it. It seems not correct to have the regionserver operational dependencies (regionServerStartup, regionServerReport, and reportFatalRSError) contending with normal priority requests.

They should be made ADMIN_QOS priority to avoid this case. ",apurtell,apurtell,Major,Resolved,Fixed,23/Jun/23 22:50,30/Jun/23 00:58
Bug,GROOVY-10894,13516376,Duplicate annotations on trait FieldHelper,"The latest nightly (included with GRECLIPSE 4.9.0.v202301012233-e2206) has introduced a regression that appears to be the same bug as GROOVY-10553; JSR-303 annotations applied to a trait property are listed twice in the bytecode on both the field and the {{$get/$set}} methods, resulting in an {{AnnotationFormatError}} at runtime. Compiling with groovyc 4.0.7 produces clean bytecode, and rebuilding in Eclipse reintroduces the duplicate.

Sample:

{code}
trait LocationRelated {
  @NotNull @Valid Location location
}
{code}

In {{FieldHelper}} disassembly:
{code}
  public abstract com.example.Location com_example_LocationRelated__location$set(com.example.Location);
    descriptor: (Lcom/example/Location;)Lcom/example/Location;
    flags: (0x0401) ACC_PUBLIC, ACC_ABSTRACT
    RuntimeVisibleTypeAnnotations:
      0: #11(): METHOD_RETURN
        javax.validation.constraints.NotNull
      1: #12(): METHOD_RETURN
        javax.validation.Valid
      2: #11(): METHOD_RETURN
        javax.validation.constraints.NotNull
      3: #12(): METHOD_RETURN
        javax.validation.Valid
      4: #11(): METHOD_FORMAL_PARAMETER, param_index=0
        javax.validation.constraints.NotNull
      5: #12(): METHOD_FORMAL_PARAMETER, param_index=0
        javax.validation.Valid
      6: #11(): METHOD_FORMAL_PARAMETER, param_index=0
        javax.validation.constraints.NotNull
      7: #12(): METHOD_FORMAL_PARAMETER, param_index=0
        javax.validation.Valid
{code}

(As a side issue, is there a particular reason these methods aren't marked synthetic? I'm not certain that would prevent the problem's actually surfacing in this case, but based on my reading of the Hibernate Validator code it might have. Not sure whether that's a positive or a negative, though.)",emilles,chrylis,Major,Resolved,Fixed,03/Jan/23 20:19,04/Jan/23 22:06
Bug,GROOVY-10897,13516723,Static Type Checking selects wrong method for abstract method call check,"Consider following code:
{code:java}
@CompileStatic
class E extends BImpl {
    @Override
    void a() {
        super.a();
    }
    static void main(args) { }
}
interface A {
    void a()
}
interface B extends A {
    @Override
    void a()
}
class AImpl implements A {
    @Override
    void a() {
    }
}
class BImpl extends AImpl implements B {
} {code}
Compiling it will fail with the error 
{{[Static type checking] - Abstract method a() cannot be called directly}}

However, when running it _without_ {{@CompileStatic}} or when writing similar code in Java, it will select the non-abstract method from {{AImpl}} instead.

It seems like this is a problem in the code to choose the best method here: [https://github.com/apache/groovy/blob/ea6ba7c6fcfefe3d8abdfbb6e20a44b1ebb8823e/src/main/java/org/codehaus/groovy/transform/stc/StaticTypeCheckingSupport.java#L1091]

The distance from {{BImpl}} to {{B}} to the interface is less than (1) the distance to {{AImpl}} (4), therefore the abstract method is chosen as best method.

This originally happened when extending {{AbstractSet}} and calling {{{}super.addAll(...){}}}.",emilles,sirywell,Major,Closed,Fixed,05/Jan/23 21:02,10/Feb/23 21:40
Bug,GROOVY-10902,13518307,Dynamic constants for primitive types get default value in Java,"If we define a constant that is resolved during runtime in a Groovy class and reference it in a Java class, it's value gets a static default value in the Java class. This seems to be a regression in Groovy 3.0.10 as earlier versions don't have the problem. Also affects all 4.0.x versions that I tested.

Example:
{code:groovy}
// G.groovy
class G {
    public static final int DYNAMIC_CONSTANT = (9.9).intValue()
} {code}
{code:java}
// J.java
public class J {
    public static void main(String[] args) {
        System.out.println(G.DYNAMIC_CONSTANT);
    }
} {code}
*Groovy 4.0.7*
Prints 0 with the following bytecode for J#main
{code:java}
0: getstatic     #2 // Field java/lang/System.out:Ljava/io/PrintStream;
3: iconst_0
4: invokevirtual #4 // Method java/io/PrintStream.println:(I)V
7: return{code}
*Groovy 3.0.9*
Correctly prints 9 with the following bytecode:
{code:java}
0: getstatic     #2 // Field java/lang/System.out:Ljava/io/PrintStream;
3: getstatic     #3 // Field G.DYNAMIC_CONSTANT:I
6: invokevirtual #4 // Method java/io/PrintStream.println:(I)V
9: return {code}
 

 ",emilles,okoskine,Major,Closed,Fixed,13/Jan/23 10:12,18/Apr/23 18:47
Bug,GROOVY-10904,13518428,STC doesn't find property accessor on nested class,"The STC appears unable to see the property accessor for a nested class.

{code}
@CompileStatic
class Example {
  static class Profile {
    String name
  }

  Map<String, Profile> PROFILES = [new Profile()]
      .stream()
      .collect(toMap(Profile::getName, identity()))
}
{code}

{code}
Groovy:Failed to find class method 'getName(Example$Profile)' or instance method 'getName()' for the type: Example$Profile
{code}",emilles,chrylis,Major,Resolved,Fixed,13/Jan/23 22:53,14/Jan/23 23:27
Bug,GROOVY-10911,13520832,General error during instruction selection: AIOOBE,"I am attempting to upgrade a large application from Groovy 2.4.15.  The application compiles fine with Groovy 2.4.15 and 2.4.21.  The compiler throws an ArrayIndexOutOfBoundsException using 2.5.20 and 3.0.14.  The output contains no clues as to which class was being processed at the time, so I have no idea how to narrow down the cause.

The stack trace from 3.0.14 is:
{code}
startup failed:
General error during canonicalization: Index 1 out of bounds for length 1

java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 1
at org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.extractGenericsConnections(StaticTypeCheckingSupport.java:1786)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.extractGenericsConnections(StaticTypeCheckingSupport.java:1765)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.extractGenericsConnections(StaticTypeCheckingSupport.java:1726)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.extractGenericsConnections(StaticTypeCheckingSupport.java:1738)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.inferenceCheck(StaticTypeCheckingSupport.java:1496)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.typeCheckMethodsWithGenerics(StaticTypeCheckingSupport.java:1462)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.typeCheckMethodsWithGenerics(StaticTypeCheckingSupport.java:1419)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.typeCheckMethodsWithGenericsOrFail(StaticTypeCheckingVisitor.java:5732)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitConstructorCallExpression(StaticTypeCheckingVisitor.java:2316)
at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitConstructorCallExpression(StaticCompilationVisitor.java:436)
at org.codehaus.groovy.ast.expr.ConstructorCallExpression.visit(ConstructorCallExpression.java:44)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitBinaryExpression(StaticTypeCheckingVisitor.java:773)
at org.codehaus.groovy.ast.CodeVisitorSupport.visitDeclarationExpression(CodeVisitorSupport.java:335)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitDeclarationExpression(ClassCodeVisitorSupport.java:150)
at org.codehaus.groovy.ast.expr.DeclarationExpression.visit(DeclarationExpression.java:89)
at org.codehaus.groovy.ast.CodeVisitorSupport.visitExpressionStatement(CodeVisitorSupport.java:117)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitExpressionStatement(ClassCodeVisitorSupport.java:200)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitExpressionStatement(StaticTypeCheckingVisitor.java:2203)
at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:86)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:164)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitBlockStatement(StaticTypeCheckingVisitor.java:4048)
at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
at org.codehaus.groovy.ast.CodeVisitorSupport.visitClosureExpression(CodeVisitorSupport.java:239)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitClosureExpression(StaticTypeCheckingVisitor.java:2438)
at org.codehaus.groovy.ast.expr.ClosureExpression.visit(ClosureExpression.java:46)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethodCallArguments(StaticTypeCheckingVisitor.java:2846)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethodCallExpression(StaticTypeCheckingVisitor.java:3583)
at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitMethodCallExpression(StaticCompilationVisitor.java:421)
at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:76)
at org.codehaus.groovy.ast.CodeVisitorSupport.visitExpressionStatement(CodeVisitorSupport.java:117)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitExpressionStatement(ClassCodeVisitorSupport.java:200)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitExpressionStatement(StaticTypeCheckingVisitor.java:2203)
at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:86)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:164)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitBlockStatement(StaticTypeCheckingVisitor.java:4048)
at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:138)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:111)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitConstructorOrMethod(StaticTypeCheckingVisitor.java:2192)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:106)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.startMethodInference(StaticTypeCheckingVisitor.java:2671)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethod(StaticTypeCheckingVisitor.java:2639)
at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitConstructorOrMethod(StaticCompilationVisitor.java:236)
at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitMethod(StaticCompilationVisitor.java:251)
at org.codehaus.groovy.ast.ClassNode.visitMethods(ClassNode.java:1094)
at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1087)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:52)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitClass(StaticTypeCheckingVisitor.java:416)
at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitClass(StaticCompilationVisitor.java:197)
at org.codehaus.groovy.transform.sc.StaticCompileTransformation.visit(StaticCompileTransformation.java:67)
at org.codehaus.groovy.transform.ASTTransformationVisitor.visitClass(ASTTransformationVisitor.java:143)
at org.codehaus.groovy.transform.ASTTransformationVisitor.lambda$addPhaseOperations$2(ASTTransformationVisitor.java:221)
at org.codehaus.groovy.control.CompilationUnit$IPrimaryClassNodeOperation.doPhaseOperation(CompilationUnit.java:943)
at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:672)
at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:636)
at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:611)
at org.gradle.api.internal.tasks.compile.ApiGroovyCompiler.execute(ApiGroovyCompiler.java:268)
at org.gradle.api.internal.tasks.compile.ApiGroovyCompiler.execute(ApiGroovyCompiler.java:68)
at org.gradle.api.internal.tasks.compile.GroovyCompilerFactory$DaemonSideCompiler.execute(GroovyCompilerFactory.java:87)
at org.gradle.api.internal.tasks.compile.GroovyCompilerFactory$DaemonSideCompiler.execute(GroovyCompilerFactory.java:75)
at org.gradle.api.internal.tasks.compile.daemon.AbstractDaemonCompiler$CompilerWorkAction.execute(AbstractDaemonCompiler.java:113)
at org.gradle.workers.internal.DefaultWorkerServer.execute(DefaultWorkerServer.java:47)
at org.gradle.workers.internal.AbstractClassLoaderWorker$1.create(AbstractClassLoaderWorker.java:46)
at org.gradle.workers.internal.AbstractClassLoaderWorker$1.create(AbstractClassLoaderWorker.java:36)
at org.gradle.internal.classloader.ClassLoaderUtils.executeInClassloader(ClassLoaderUtils.java:98)
at org.gradle.workers.internal.AbstractClassLoaderWorker.executeInClassLoader(AbstractClassLoaderWorker.java:36)
at org.gradle.workers.internal.IsolatedClassloaderWorker.execute(IsolatedClassloaderWorker.java:54)
at org.gradle.workers.internal.WorkerDaemonServer.execute(WorkerDaemonServer.java:56)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.base/java.lang.reflect.Method.invoke(Method.java:566)
at org.gradle.process.internal.worker.request.WorkerAction.run(WorkerAction.java:118)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.base/java.lang.reflect.Method.invoke(Method.java:566)
at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)
at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
at java.base/java.lang.Thread.run(Thread.java:829)
{code}

The stack trace from 2.5.20 is similar:
{code}
startup failed:
General error during instruction selection: Index 1 out of bounds for length 1

java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 1
at org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.extractGenericsConnections(StaticTypeCheckingSupport.java:1856)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.extractGenericsConnections(StaticTypeCheckingSupport.java:1835)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.extractGenericsConnections(StaticTypeCheckingSupport.java:1791)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.extractGenericsConnections(StaticTypeCheckingSupport.java:1803)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.inferenceCheck(StaticTypeCheckingSupport.java:1562)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.typeCheckMethodsWithGenerics(StaticTypeCheckingSupport.java:1528)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.typeCheckMethodsWithGenerics(StaticTypeCheckingSupport.java:1474)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.typeCheckMethodsWithGenericsOrFail(StaticTypeCheckingVisitor.java:5664)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitConstructorCallExpression(StaticTypeCheckingVisitor.java:2359)
at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitConstructorCallExpression(StaticCompilationVisitor.java:439)
at org.codehaus.groovy.ast.expr.ConstructorCallExpression.visit(ConstructorCallExpression.java:44)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitBinaryExpression(StaticTypeCheckingVisitor.java:809)
at org.codehaus.groovy.ast.CodeVisitorSupport.visitDeclarationExpression(CodeVisitorSupport.java:296)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitDeclarationExpression(ClassCodeVisitorSupport.java:107)
at org.codehaus.groovy.ast.expr.DeclarationExpression.visit(DeclarationExpression.java:89)
at org.codehaus.groovy.ast.CodeVisitorSupport.visitExpressionStatement(CodeVisitorSupport.java:120)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitExpressionStatement(ClassCodeVisitorSupport.java:203)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitExpressionStatement(StaticTypeCheckingVisitor.java:2237)
at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:86)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:167)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitBlockStatement(StaticTypeCheckingVisitor.java:4004)
at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
at org.codehaus.groovy.ast.CodeVisitorSupport.visitClosureExpression(CodeVisitorSupport.java:225)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitClosureExpression(StaticTypeCheckingVisitor.java:2497)
at org.codehaus.groovy.ast.expr.ClosureExpression.visit(ClosureExpression.java:46)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethodCallArguments(StaticTypeCheckingVisitor.java:2916)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethodCallExpression(StaticTypeCheckingVisitor.java:3651)
at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitMethodCallExpression(StaticCompilationVisitor.java:423)
at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:68)
at org.codehaus.groovy.ast.CodeVisitorSupport.visitExpressionStatement(CodeVisitorSupport.java:120)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitExpressionStatement(ClassCodeVisitorSupport.java:203)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitExpressionStatement(StaticTypeCheckingVisitor.java:2237)
at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:86)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:167)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitBlockStatement(StaticTypeCheckingVisitor.java:4004)
at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitConstructorOrMethod(StaticTypeCheckingVisitor.java:2226)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:125)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.startMethodInference(StaticTypeCheckingVisitor.java:2744)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethod(StaticTypeCheckingVisitor.java:2712)
at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitConstructorOrMethod(StaticCompilationVisitor.java:235)
at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitMethod(StaticCompilationVisitor.java:250)
at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1074)
at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:51)
at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitClass(StaticTypeCheckingVisitor.java:386)
at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitClass(StaticCompilationVisitor.java:200)
at org.codehaus.groovy.transform.sc.StaticCompileTransformation.visit(StaticCompileTransformation.java:65)
at org.codehaus.groovy.transform.ASTTransformationVisitor.visitClass(ASTTransformationVisitor.java:146)
at org.codehaus.groovy.transform.ASTTransformationVisitor$2.call(ASTTransformationVisitor.java:228)
at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1055)
at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:623)
at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:602)
at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:579)
at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:558)
at org.gradle.api.internal.tasks.compile.ApiGroovyCompiler.execute(ApiGroovyCompiler.java:268)
at org.gradle.api.internal.tasks.compile.ApiGroovyCompiler.execute(ApiGroovyCompiler.java:68)
at org.gradle.api.internal.tasks.compile.GroovyCompilerFactory$DaemonSideCompiler.execute(GroovyCompilerFactory.java:87)
at org.gradle.api.internal.tasks.compile.GroovyCompilerFactory$DaemonSideCompiler.execute(GroovyCompilerFactory.java:75)
at org.gradle.api.internal.tasks.compile.daemon.AbstractDaemonCompiler$CompilerWorkAction.execute(AbstractDaemonCompiler.java:113)
at org.gradle.workers.internal.DefaultWorkerServer.execute(DefaultWorkerServer.java:47)
at org.gradle.workers.internal.AbstractClassLoaderWorker$1.create(AbstractClassLoaderWorker.java:46)
at org.gradle.workers.internal.AbstractClassLoaderWorker$1.create(AbstractClassLoaderWorker.java:36)
at org.gradle.internal.classloader.ClassLoaderUtils.executeInClassloader(ClassLoaderUtils.java:98)
at org.gradle.workers.internal.AbstractClassLoaderWorker.executeInClassLoader(AbstractClassLoaderWorker.java:36)
at org.gradle.workers.internal.IsolatedClassloaderWorker.execute(IsolatedClassloaderWorker.java:54)
at org.gradle.workers.internal.WorkerDaemonServer.execute(WorkerDaemonServer.java:56)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.base/java.lang.reflect.Method.invoke(Method.java:566)
at org.gradle.process.internal.worker.request.WorkerAction.run(WorkerAction.java:118)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.base/java.lang.reflect.Method.invoke(Method.java:566)
at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)
at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
at java.base/java.lang.Thread.run(Thread.java:829)
{code}",emilles,jasongarrett,Major,Closed,Fixed,20/Jan/23 15:42,10/Feb/23 21:31
Bug,GROOVY-10915,13521156,SC: class that provides isCase but not isNotCase,"Consider the following:
{code:groovy}
class C {
  boolean isCase(value) {
    System.out.println(""C isCase""); true
  }
}
@groovy.transform.CompileStatic // comment out and C#isCase is called for all 3
void test() {
  assert 0 in new C()
  assert !!(0 in new C())
  assert !(0 !in new C())
}
test()
{code}
""x in c"" and ""!(x in c)"" will use C's {{isCase}} method. However ""x !in c"" will use {{DGM.isNotCase(c,x)}} which static dispatches to {{{}DGM.isCase{}}}. The isNotCase extension methods added in Groovy 4 should probably use invokeMethod to dynamic dispatch to make use of the isCase implemented by C.

IMO it would be much simpler to ditch ""isNotCase"" and have ""a not in b"" work identically to ""not(a in b)"" so one cannot implement incongruent ""in"" and ""!in"" handling.  Otherwise, expected behavior could be restored by disabling GROOVY-10383 optimization when declaring class of isCase and isNotCase differ.",emilles,emilles,Major,Resolved,Fixed,24/Jan/23 18:48,09/May/23 13:06
Bug,GROOVY-10918,13521641,Memory leak: local variable values are not discarded,"When I run the code below with 3 statements with closures inside the method the local var (parameter) values (all 4 of them) are staying in memory.
I don't see the same problem if I run corresponding Java code with lambdas.

Run TestOOM.groovy with 
-Xmx600M -XX:+HeapDumpOnOutOfMemoryError

Notice groovy fails:
Y
Z
java.lang.OutOfMemoryError: Java heap space
Dumping heap to java_pid147612.hprof ...
Heap dump file created [497819587 bytes in 0.136 secs]
Caught: java.lang.OutOfMemoryError: Java heap space
java.lang.OutOfMemoryError: Java heap space
    at test.TestOOM.test(TestOOM.groovy:31)
    at test.TestOOM.run(TestOOM.groovy:41)

But Java version does not.

It looks like all the values of s are still in memory (see screenshot), even though previous values should be discarded.",emilles,arysin,Major,Closed,Fixed,26/Jan/23 17:38,13/Mar/23 12:14
Bug,GROOVY-10919,13521668,"@MapConstructor Breaks in Groovy4 when combined with @TupleConstructor, fine in Groovy3","@MapConstructor Breaks in Groovy4 when combined with @TupleConstructor, fine in Groovy3

 

Example in Groovy4
{code:java}
import groovy.transform.*
@MapConstructor(includeFields = true)
@TupleConstructor(includeFields = true)
@ToString(includeNames = true, includeFields = true)
class Foo {    
  private final float w = 1    
  private final int x    
  private int y = 1    
  private final int z
}
println new Foo(x:2, z: 3){code}

outputs the incorrect value
{code:java}
Foo(x:2, y:0, z:3){code}


In Groovy3 it outputs the correct 
{code:java}
Foo(x:2, y:1, z:3){code}
Also commenting out @TupleConstructor in Groovy4 causes the output to be the correct value. 

 ",emilles,aibolit,Major,Closed,Fixed,26/Jan/23 21:39,10/Feb/23 21:31
Bug,GROOVY-10920,13521815,Compiler exception with void expression as while condition with CompileStatic,"Under static compilation, using a void expression as the condition (or part of the condition) of a while statement causes the compiler to throw exceptions.

This code:
{code:java}
package example

import groovy.transform.CompileStatic

@CompileStatic
class WhileVoid {
	void isSo() {
		null
	}

	void myBuggyConstruct() {
		while(isSo()) {
			null
		}
	}
}
{code}
results in this exception from the compiler:
{code:java}
startup failed:
General error during instruction selection: ASM reporting processing error for example.WhileVoid#myBuggyConstruct with signature void myBuggyConstruct() in WhileVoid.groovy:11. /Users/jasongarrett/scratch/scratch/src/main/groovy/example/WhileVoid.groovy

groovy.lang.GroovyRuntimeException: ASM reporting processing error for example.WhileVoid#myBuggyConstruct with signature void myBuggyConstruct() in WhileVoid.groovy:11. /Users/jasongarrett/scratch/scratch/src/main/groovy/example/WhileVoid.groovy
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:432)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:106)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:557)
	at org.codehaus.groovy.ast.ClassNode.visitMethods(ClassNode.java:1094)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1087)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:52)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:273)
	at org.codehaus.groovy.control.CompilationUnit$3.call(CompilationUnit.java:798)
	at org.codehaus.groovy.control.CompilationUnit$IPrimaryClassNodeOperation.doPhaseOperation(CompilationUnit.java:943)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:672)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:636)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:611)
	at org.gradle.api.internal.tasks.compile.ApiGroovyCompiler.execute(ApiGroovyCompiler.java:277)
	at org.gradle.api.internal.tasks.compile.ApiGroovyCompiler.execute(ApiGroovyCompiler.java:67)
	at org.gradle.api.internal.tasks.compile.GroovyCompilerFactory$DaemonSideCompiler.execute(GroovyCompilerFactory.java:98)
	at org.gradle.api.internal.tasks.compile.GroovyCompilerFactory$DaemonSideCompiler.execute(GroovyCompilerFactory.java:77)
	at org.gradle.api.internal.tasks.compile.daemon.AbstractDaemonCompiler$CompilerWorkAction.execute(AbstractDaemonCompiler.java:135)
	at org.gradle.workers.internal.DefaultWorkerServer.execute(DefaultWorkerServer.java:63)
	at org.gradle.workers.internal.AbstractClassLoaderWorker$1.create(AbstractClassLoaderWorker.java:49)
	at org.gradle.workers.internal.AbstractClassLoaderWorker$1.create(AbstractClassLoaderWorker.java:43)
	at org.gradle.internal.classloader.ClassLoaderUtils.executeInClassloader(ClassLoaderUtils.java:97)
	at org.gradle.workers.internal.AbstractClassLoaderWorker.executeInClassLoader(AbstractClassLoaderWorker.java:43)
	at org.gradle.workers.internal.IsolatedClassloaderWorker.run(IsolatedClassloaderWorker.java:49)
	at org.gradle.workers.internal.IsolatedClassloaderWorker.run(IsolatedClassloaderWorker.java:30)
	at org.gradle.workers.internal.WorkerDaemonServer.run(WorkerDaemonServer.java:85)
	at org.gradle.workers.internal.WorkerDaemonServer.run(WorkerDaemonServer.java:55)
	at org.gradle.process.internal.worker.request.WorkerAction$1.call(WorkerAction.java:138)
	at org.gradle.process.internal.worker.child.WorkerLogEventListener.withWorkerLoggingProtocol(WorkerLogEventListener.java:41)
	at org.gradle.process.internal.worker.request.WorkerAction.run(WorkerAction.java:135)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:414)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.ArrayIndexOutOfBoundsException: Index 0 out of bounds for length 0
	at groovyjarjarasm.asm.Frame.merge(Frame.java:1268)
	at groovyjarjarasm.asm.Frame.merge(Frame.java:1233)
	at groovyjarjarasm.asm.MethodWriter.computeAllFrames(MethodWriter.java:1611)
	at groovyjarjarasm.asm.MethodWriter.visitMaxs(MethodWriter.java:1547)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:413)
Caused by: java.lang.ArrayIndexOutOfBoundsException: Index 0 out of bounds for length 0
{code}
This example, with a slightly more complex condition for the while loop:
{code:java}
package example

import groovy.transform.CompileStatic

@CompileStatic
class WhileNotVoid {
	List list = []

	void isSo() {
		null
	}

	void myBuggyConstruct() {
		while(!list.empty && !isSo()) {
			null
		}
	}
}
{code}
results in a different compiler exception:
{code:java}
BUG! exception in phase 'class generation' in source unit '/Users/jasongarrett/scratch/scratch/src/main/groovy/example/WhileNotVoid.groovy' operand stack contains 2 elements, but we expected only 0
{code}",emilles,jasongarrett,Minor,Resolved,Fixed,27/Jan/23 23:01,30/Apr/23 01:17
Bug,GROOVY-10922,13522115,stack overflow calling implicit getter/setter from override,"In the following situation, under static compilation:
 * An interface declares a getter and/or setter
 * An implementing class declares a property of the corresponding name and type, so the implicit getter and/or setter implement the interface
 * A subclass overrides the getter/setter and calls the super getter/setter

the subclass getter/setter will recurse instead of calling the superclass implicit method.

This appears to have been introduced in 3.0.14.

Running the main method in this example will reproduce the stack overflow.
{code:java}
package example

import groovy.transform.CompileStatic

@CompileStatic
class SuperImplicitSetter {
   interface FooHaving {
      String getFoo()
      void setFoo(String foo)
   }

   static class Foo implements FooHaving {
      String foo  // implements interface
   }

   static class Bar extends Foo {
      String bar

      @Override
      void setFoo(String foo) {
         super.setFoo(foo)  // recurses instead of calling implicit Foo.setFoo(String)
         bar = foo
      }
   }

   static void main(String[] args) {
      Bar bar = new Bar()
      bar.setFoo(""bar"")  // stack overflow
      println bar.foo
   }
}
 {code}",emilles,jasongarrett,Major,Closed,Fixed,30/Jan/23 20:07,10/Feb/23 21:31
Bug,GROOVY-10928,13523307,Invalid stub code for interface field,"When compiling this interface:

{code:groovy}
@CompileStatic
interface Service {
  TypeReference<Map<String, Object>> STATE_TYPE_MAP = new TypeReference<Map<String, Object>>(){}
}
{code}

the stub generator produces code that does not compile under Java 11:

{code:java}
@groovy.transform.CompileStatic() public interface Service
<T> {
;
com.fasterxml.jackson.core.type.TypeReference<java.util.Map<java.lang.String, java.lang.Object>> STATE_TYPE_MAP;
static { STATE_TYPE_MAP = null; }
{code}

{code}
[ERROR] Service.java:[6,112] = expected
[ERROR] Service.java:[7,8] initializers not allowed in interfaces
{code}
",emilles,chrylis,Major,Closed,Fixed,06/Feb/23 21:16,13/Mar/23 12:14
Bug,GROOVY-10929,13523832,Method closure somehow doesn't match argument,"I do not understand at all how this is happening, but when I pass class literals to a method closure whose parameter is a {{Class}}, I'm getting {{MissingMethodException}}. This seems to have something to do with ASTTs, because it only happens when {{TupleConstructor}} is involved. In my business code, {{TupleConstructor}} on a top-level containing class triggers the error on a static nested class; I have a repro, but I'm only able to make it trigger on the specific class involved.

In any case, in my business code I'm using {{@CompileStatic}}, but that doesn't seem to be protecting me against runtime dynamic dispatch.

{code:groovy}
@Grab(group='software.amazon.awssdk', module='dynamodb-enhanced', version='2.19.25')
import software.amazon.awssdk.enhanced.dynamodb.mapper.annotations.*
import software.amazon.awssdk.enhanced.dynamodb.TableSchema
import java.util.function.Function
import groovy.transform.*

@DynamoDbBean @TupleConstructor(defaults=false)
class TopLevel {
    String id

    @DynamoDbPartitionKey
    String getId() { id }

    @DynamoDbBean
    @ToString
    static class Nested {
        String id

        @DynamoDbPartitionKey
        String getId() { id }
    }
}

// https://javadoc.io/static/software.amazon.awssdk/dynamodb-enhanced/2.19.25/software/amazon/awssdk/enhanced/dynamodb/TableSchema.html#fromClass-java.lang.Class-
Function<Class<?>, TableSchema<?>> func = TableSchema.&fromClass
func.apply(TopLevel) // XXX
func.apply(TopLevel.Nested)
{code}

{code}
Condition failed with Exception:

func.apply(TopLevel)
|    |     |
|    |     class com.example.TopLevel
|    groovy.lang.MissingMethodException: No signature of method: org.codehaus.groovy.runtime.MethodClosure.fromClass() is applicable for argument types: (Class) values: [class com.example.TopLevel]
|    Possible solutions: getClass(), metaClass(groovy.lang.Closure)
|    	at com.example.BugRepl.top-level(BugRepl.groovy:19)
org.codehaus.groovy.runtime.MethodClosure@6a472566
{code}

I'm tagging this as critical because it's a ""can't-possibly-happen"" bug (at least in static mode) that has apparently been latent in an internal library for months and is still present as of 4.0.8.",emilles,chrylis,Critical,Closed,Fixed,08/Feb/23 20:34,30/May/23 21:13
Bug,GROOVY-10930,13524205,STC accepts program although constructor reference is invalid,"I have the following program

{code}
import java.util.function.*;

class Bar {}

public class Test {

    public static void main(String[] args) {
      m(Bar::new);
    }

    static <X> void m(Function<String, X> d) {
      d.apply(""dfa"")
    }
}
{code}

h3. Actual behavior

The compiler accepts the program, but notice that the constructor reference is invalid. So, at runtime, we get:

{code}
Exception in thread ""main"" groovy.lang.GroovyRuntimeException: Could not find matching constructor for: Bar(String)
        at groovy.lang.MetaClassImpl.invokeConstructor(MetaClassImpl.java:1908)
        at groovy.lang.MetaClassImpl.invokeConstructor(MetaClassImpl.java:1677)
        at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
        at Test.ctorRef$main$0(test.groovy)
        at Test.m(test.groovy:13)
        at Test.main(test.groovy:9)
{code}

h3. Expected behavior

The compiler should have rejected this example program.

Tested against master (commit: 2c40df2827b8d1bc941bd6ebdc386b3f9bc3bf3b)",emilles,theosot,Major,Resolved,Fixed,10/Feb/23 10:46,12/Apr/23 14:13
Bug,GROOVY-10933,13524539,Bootstrap Method Error on non-void lambda for Consumer,"When a method that returns a value is used as a {{Consumer}}, a runtime {{Error}} is produced.

{code:groovy}
class BugRepro extends Specification {
    def 'repro'() {
        expect:
        new Demo().run()
    }
}

@CompileStatic
class Demo {
    List<String> strings = []

    void run() {
        Optional.of('hello')
            .ifPresent(strings::add)
    }
}
{code}

{code}
Condition failed with Exception:

new Demo().run()
|          |
|          java.lang.BootstrapMethodError: bootstrap method initialization exception
|          	at java.base/java.lang.invoke.BootstrapMethodInvoker.invoke(BootstrapMethodInvoker.java:194)
|          	at java.base/java.lang.invoke.CallSite.makeSite(CallSite.java:307)
|          	at java.base/java.lang.invoke.MethodHandleNatives.linkCallSiteImpl(MethodHandleNatives.java:258)
|          	at java.base/java.lang.invoke.MethodHandleNatives.linkCallSite(MethodHandleNatives.java:248)
|          	at Demo.run(BugRepro.groovy:18)
|          	at BugRepro.repro(BugRepro.groovy:9)
|          Caused by: java.lang.invoke.LambdaConversionException: Type mismatch for lambda expected return: boolean is not convertible to void
|          	at java.base/java.lang.invoke.AbstractValidatingLambdaMetafactory.checkDescriptor(AbstractValidatingLambdaMetafactory.java:317)
|          	at java.base/java.lang.invoke.AbstractValidatingLambdaMetafactory.validateMetafactoryArgs(AbstractValidatingLambdaMetafactory.java:294)
|          	at java.base/java.lang.invoke.LambdaMetafactory.metafactory(LambdaMetafactory.java:328)
|          	at java.base/java.lang.invoke.BootstrapMethodInvoker.invoke(BootstrapMethodInvoker.java:127)
|          	... 5 more
<Demo@18e8473e strings=[]>

	at BugRepro.repro(BugRepro.groovy:9)
Caused by: java.lang.BootstrapMethodError: bootstrap method initialization exception
	at java.base/java.lang.invoke.BootstrapMethodInvoker.invoke(BootstrapMethodInvoker.java:194)
	at java.base/java.lang.invoke.CallSite.makeSite(CallSite.java:307)
	at java.base/java.lang.invoke.MethodHandleNatives.linkCallSiteImpl(MethodHandleNatives.java:258)
	at java.base/java.lang.invoke.MethodHandleNatives.linkCallSite(MethodHandleNatives.java:248)
	at Demo.run(BugRepro.groovy:18)
	... 1 more
Caused by: java.lang.invoke.LambdaConversionException: Type mismatch for lambda expected return: boolean is not convertible to void
	at java.base/java.lang.invoke.AbstractValidatingLambdaMetafactory.checkDescriptor(AbstractValidatingLambdaMetafactory.java:317)
	at java.base/java.lang.invoke.AbstractValidatingLambdaMetafactory.validateMetafactoryArgs(AbstractValidatingLambdaMetafactory.java:294)
	at java.base/java.lang.invoke.LambdaMetafactory.metafactory(LambdaMetafactory.java:328)
	at java.base/java.lang.invoke.BootstrapMethodInvoker.invoke(BootstrapMethodInvoker.java:127)
	... 5 more
{code}",emilles,chrylis,Major,Closed,Fixed,13/Feb/23 21:19,13/Mar/23 12:14
Bug,GROOVY-10935,13524541,MissingPropertyException truncates nested class name,"When a nested class is the target of an invalid property expression, the resulting error message has the wrong class name:

{code:groovy}
class Outer {
  static class Inner {}
}

new Outer.Inner().missing
{code}

results in

{code}
groovy.lang.MissingPropertyException: No such property: missing for class: com.example.Outer
{code}",emilles,chrylis,Minor,Closed,Fixed,13/Feb/23 21:23,13/Mar/23 12:15
Bug,GROOVY-10937,13524743,record + lombok + groovy == Cannot specify duplicate annotation on the same member : lombok.NonNull,"I've bumped into a strange looking compilation error in a project using JDK 17 record, Lombok's @NotNull and Groovy for tests. It took me a while to find a construction which causes that, as:
 # Compilation error occurs in a Groovy class not related to the problematic Java record.
 # Groovy class must have (non related) inner interface or class.
 # Record has must have two (or more) fields with @NotNull annotation from Lombok.
 # Some (other) Groovy class has to refer to that record.

For example:
{code:java}
// Java
public record SampleRecordWithNonNullFromLombok(
        @NonNull
        String nonNullField,
        @NonNull        //Two or more @NulNull annotated fields are required to trigger the problem
        String nonNullField2
) {}

//Groovy
class ClassWithReferenceToRecord {
    private SampleRecordWithNonNullFromLombok sampleRecord
}

class ReproducerClass {  //it fails to compile

    interface RequiredToReproduceProblem {
    }
}{code}
The error is:
{code:java}
[ERROR] Failed to execute goal org.codehaus.gmavenplus:gmavenplus-plugin:1.13.1:compileTests (default) on project record-lombok-groovy: Error occurred while calling a method on a Groovy class from classpath.: InvocationTargetException: startup failed:
[ERROR] .../ReproducerClass.groovy: -1: Cannot specify duplicate annotation on the same member : lombok.NonNull
[ERROR]  @ line -1, column -1.
[ERROR] 1 error
{code}
I created a shrank version of a reproducer:

[https://github.com/szpak/code-examples-and-poc/tree/master/record-lombok-groovy]

 

It might be a problem with the way how Lombok generates .class for Java record, however, I start here as Groovy compilation is the final reason.

I have observed that error occasionally with Groovy 3(.0.14), but it occurs every time with 4(0.9).",emilles,szpak,Minor,Closed,Fixed,15/Feb/23 00:20,13/Mar/23 12:15
Bug,GROOVY-10939,13524986, Non-static method java.lang.Object#hashCode cannot be called from static context,"Updating Micronaut Framework 4.0.x branch to 4.0.9 we got the following issue:
https://github.com/micronaut-projects/micronaut-core/pull/8784#discussion_r1108171939
{code}
[Static type checking] - Non-static method java.lang.Object#hashCode cannot be called from static context @ line 99, column 16. return visitor.getClass().hashCode(){code}",emilles,sdelamo,Major,Closed,Fixed,16/Feb/23 09:37,15/Jun/23 14:30
Bug,GROOVY-10955,13526551,@Builder doesn't work on records,"For this code:
{code}
import groovy.transform.builder.*
@Builder
record Developer(Integer id, String first, String last, String email, List<String> skills) { }
Developer.builder().id(2).build()
{code}
The code fails in the {{build}} method. It is meant to create a new Developer but instead creates a DeveloperBuilder instance and then throws a cast exception:
{noformat}
org.codehaus.groovy.runtime.typehandling.GroovyCastException: Cannot cast object 'Developer$DeveloperBuilder@5ef26266' with class 'Developer$DeveloperBuilder' to class 'Developer'
        ...
	at Developer$DeveloperBuilder.build(ConsoleScript25)
{noformat}
I wasn't necessarily expecting it to work. It could be made to work or we could explicitly disable it for records.

Similarly, this code fails:
{code}
@Builder(builderStrategy=InitializerStrategy)
record Developer(Integer id, String first, String last, String email, List<String> skills) { }
Developer.createInitializer().id(2).build()
{code}
with this more obscure error:
{noformat}
java.lang.ArrayIndexOutOfBoundsException: Internal compiler error while compiling ConsoleScript26
Method: org.codehaus.groovy.ast.MethodNode@7cd420f9[Developer$DeveloperInitializer id(java.lang.Integer) from Developer$DeveloperInitializer]
Line -1, expecting casting to Developer$DeveloperInitializer<groovy.transform.builder.InitializerStrategy$SET, T1, T2, T3, T4> but operand stack is empty
        ...
	at org.codehaus.groovy.classgen.asm.OperandStack.doConvertAndCast(OperandStack.java:340)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeReturn(StatementWriter.java:593)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitReturnStatement(AsmClassGenerator.java:822)
...
{noformat}
I would probably just used the named args style rather than a builder, e.g.:
{code}
var dev1 = new Developer(id: 1, first: 'Dan', last: 'Vega', email: 'danvega@gmail.com', skills: ['Java', 'Spring'])
assert dev1.with{ [id, first, last, email, skills] } ==
//  [1, 'Dan', 'Vega', 'danvega@gmail.com', ['Java', 'Spring']]
{code}
But we should either support or disable one or more of the @Builder strategies.

Builder can also be written on constructors. That does work for the default strategy but again not for the InitializerStrategy. Here is a working example:
{code}
import groovy.transform.builder.*
record Developer(Integer id, String first, String last, String email, List<String> skills) {
    @Builder
    Developer(Integer id, String full, String email, List<String> skills) {
        this(id, full.split(' ')[0], full.split(' ')[1], email, skills)
    }
}

var dev1 = new Developer(id: 1, first: 'Dan', last: 'Vega', email: 'danvega@gmail.com', skills: ['Java', 'Spring'])

assert dev1.with{ [id, first, last, email, skills] } ==
  [1, 'Dan', 'Vega', 'danvega@gmail.com', ['Java', 'Spring']]

var dev2 = Developer.builder().id(2).full('Paul King').email('paulk@apache.org').skills(['Java', 'Groovy']).build()

assert dev2.with{ [id, first, last, email, skills] } ==
  [2, 'Paul', 'King', 'paulk@apache.org', ['Java', 'Groovy']]
{code}",paulk,paulk,Major,Closed,Fixed,28/Feb/23 15:38,13/Mar/23 12:15
Bug,GROOVY-10962,13527285,"Instance ""isser"" prevents resolving of static ""getter""","While upgrading the Groovy-Version in our application, we noticed, like others, that the priority of issers and getters have changed. This has already been confirmed as intended behavior, but there seems to be some odd behavior with static and instance accessors.

When having a class with a static get-method that also has an instance is-method, trying to access the static property via the class name an exception is raised: *groovy.lang.MissingPropertyException: No such property: x for class: mypackage.MyClass  Possible solutions: x*

You can recreate this rather easily:

 
{code:java}
class Test
{
  static String getTestProperty { ""test"" }
  boolean isTestProperty { true }
}
Test.testProperty // => groovy.lang.MissingPropertyException
{code}
 

Given that the property is called from a static context, one would assume, that the instance method isTestProperty would not be in the way. However it is. Removing the method, makes that static getTestProperty available again via Test.testProperty.

Is this also intended behavior?",emilles,pmarkus,Major,Closed,Fixed,06/Mar/23 14:07,16/May/23 13:29
Bug,GROOVY-10963,13527311,AIOOBE with Java lambda syntax,"This bug may have something to do with the overload-selection challenges discussed previously, but I don't remember an actual internal exception before.

When using Vavr {{Try}}, several of the methods have overloads that accept either {{Runnable}} or {{Consumer<T>}}. Using a Groovy-syntax lambda with an explicit parameter can result in erroneous selection of {{Runnable}} (covered elsewhere), but using a _Java_-syntax lambda apparently results in the same erroneous selection with a crash instead of a meaningful error.

{code}
Try.<Val> success(someValue)
  .andThen((Val result) -> { println result }) // should be unambiguously Consumer<Val>
{code}

produces
{code}
[ERROR] Failed to execute goal org.codehaus.gmavenplus:gmavenplus-plugin:1.13.1:compile (default) on project azimuth-server: Error occurred while calling a method on a Groovy class from classpath.: InvocationTargetException: startup failed:
[ERROR] General error during instruction selection: Index 0 out of bounds for length 0
[ERROR] 
[ERROR] java.lang.ArrayIndexOutOfBoundsException: Index 0 out of bounds for length 0
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticTypesLambdaWriter.createParametersWithExactType(StaticTypesLambdaWriter.java:296)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticTypesLambdaWriter.addSyntheticLambdaMethodNode(StaticTypesLambdaWriter.java:274)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticTypesLambdaWriter.createLambdaClass(StaticTypesLambdaWriter.java:251)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticTypesLambdaWriter.lambda$getOrAddLambdaClass$2(StaticTypesLambdaWriter.java:216)
[ERROR]         at java.base/java.util.HashMap.computeIfAbsent(HashMap.java:1133)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticTypesLambdaWriter.getOrAddLambdaClass(StaticTypesLambdaWriter.java:215)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticTypesLambdaWriter.writeLambda(StaticTypesLambdaWriter.java:118)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitLambdaExpression(AsmClassGenerator.java:873)
[ERROR]         at org.codehaus.groovy.ast.expr.LambdaExpression.visit(LambdaExpression.java:46)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.visitArgument(StaticInvocationWriter.java:512)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.loadArguments(StaticInvocationWriter.java:459)
[ERROR]         at org.codehaus.groovy.classgen.asm.InvocationWriter.writeDirectMethodCall(InvocationWriter.java:213)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.writeDirectMethodCall(StaticInvocationWriter.java:385)
[ERROR]         at org.codehaus.groovy.classgen.asm.InvocationWriter.makeDirectCall(InvocationWriter.java:311)
[ERROR]         at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:131)
[ERROR]         at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:125)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.makeCall(StaticInvocationWriter.java:647)
[ERROR]         at org.codehaus.groovy.classgen.asm.InvocationWriter.writeInvokeMethod(InvocationWriter.java:454)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.writeInvokeMethod(StaticInvocationWriter.java:135)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethodCallExpression(AsmClassGenerator.java:988)
[ERROR]         at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:77)
[ERROR]         at org.codehaus.groovy.classgen.asm.StatementWriter.writeExpressionStatement(StatementWriter.java:613)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitExpressionStatement(AsmClassGenerator.java:827)
[ERROR]         at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:41)
[ERROR]         at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:95)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticTypesStatementWriter.writeBlockStatement(StaticTypesStatementWriter.java:77)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:752)
[ERROR]         at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:70)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:611)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:546)
[ERROR]         at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:110)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:695)
[ERROR]         at org.codehaus.groovy.ast.ClassNode.visitMethods(ClassNode.java:1131)
[ERROR]         at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1124)
[ERROR]         at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:52)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:365)
[ERROR]         at org.codehaus.groovy.control.CompilationUnit$3.call(CompilationUnit.java:797)
[ERROR]         at org.codehaus.groovy.control.CompilationUnit$IPrimaryClassNodeOperation.doPhaseOperation(CompilationUnit.java:937)
[ERROR]         at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:692)
[ERROR]         at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:666)
[ERROR]         at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:647)
[ERROR]         at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[ERROR]         at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[ERROR]         at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[ERROR]         at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[ERROR]         at org.codehaus.gmavenplus.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:212)
[ERROR]         at org.codehaus.gmavenplus.mojo.AbstractCompileMojo.doCompile(AbstractCompileMojo.java:334)
[ERROR]         at org.codehaus.gmavenplus.mojo.CompileMojo.execute(CompileMojo.java:70)
{code}

This may be a duplicate if the correct resolution is ""enhance the overload-selection logic to pick the most suitable one at an earlier stage"".",emilles,chrylis,Major,Resolved,Fixed,06/Mar/23 16:27,14/Jun/23 15:17
Bug,GROOVY-10971,13527826,STC is looking for the wrong constructor reference,"I have the following program

{code}
class Foo {
  Foo(String d) {}
}

class Main {
  static final void test() {
        def x = java.util.stream.Collectors.groupingBy(Main::m) // works;
        def y = java.util.stream.Collectors.groupingBy(Foo::new) // fails;
  }

  static Foo m(String x) { return null; }
}
{code}

h3. Actual behavior

{code}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
test.groovy: 8: [Static type checking] - Cannot find matching constructor Foo(java.lang.Object)
 @ line 8, column 56.
   .stream.Collectors.groupingBy(Foo::new)
                                 ^

1 error
{code}

h3. Expected behavior

Compile successfully

Tested against master (commit: 46722e7147ed529fe8817660617ac6af98ad6f54)",emilles,theosot,Major,Resolved,Fixed,09/Mar/23 14:28,12/Apr/23 20:27
Bug,GROOVY-10972,13527849,Issue in resolving overloaded method reference,"I have the following program

{code}
import java.util.function.Function;
import java.util.LinkedList;

class Foo {}


class Test {

    public static void main(String[] args) {
        LinkedList<Foo> x = new LinkedList();
        x.add(new Foo());

        Function<Integer, Foo> z = x::remove;
        Foo k = z.apply(0); // ClassCastException boolean cannot be cast to Foo.
    }

}
{code}

h3. Actual behavior

The compiler accepts the code, but the following exception is triggered at runtime.

{code}
Exception in thread ""main"" org.codehaus.groovy.runtime.typehandling.GroovyCastException: Cannot cast object 'false' with class 'java.lang.Boolean' to class 'Foo'
        at org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.continueCastOnSAM(DefaultTypeTransformation.java:425)
        at org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.continueCastOnNumber(DefaultTypeTransformation.java:336)
        at org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.castToType(DefaultTypeTransformation.java:256)
        at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
        at Test.main(test.groovy:14)
{code}

There problem happens because there are two overloaded methods inside LinkedList:
  * E remove(int x)
  * boolean remove(Object d)

The compiler seems to resolve the second method (i.e., boolean remove (Object)) although its return type does not match with Function<Integer, Foo>.

Based only on the argument type, both methods are applicable. If the second ""remove"" method is more specific than the first one, then a compiler error should be raised at compile-time, because boolean does not match Foo. If this is not the case, then the program should call the first ""remove"" method.",emilles,theosot,Minor,Resolved,Fixed,09/Mar/23 15:51,15/Jun/23 15:23
Bug,GROOVY-10978,13528996,LUB of Type<T extends Something> and Type<? extends Something>,"Follow up to GROOVY-10756.  Blending wildcard and unresolved parameterized type is incomplete.

Consider the following:
{code:groovy}
            interface Handle {
                int getCount()
            }
            class HandleContainer<H extends Handle> {
                H handle
            }
            interface Input {
                HandleContainer<? extends Handle> getResult(key)
            }
            interface State {
                def <X extends Handle> HandleContainer<X> getResult(key)
            }

            void test(Input input, State state) {
                def container = state.getResult('k') ?: input.getResult('k') // HERE: HandleContainer<# extends Handle> and HandleContainer<? extends Handle>
                Handle handle = container.handle
                Integer count = handle.count
                assert count == 1
            }

            Handle h = {->1}
            def c = new HandleContainer(handle: h)
            test({k->c} as Input, {k->c} as State)
{code}",emilles,emilles,Major,Closed,Fixed,17/Mar/23 16:34,05/Apr/23 02:44
Bug,GROOVY-10981,13530036,STC: type inference of variable expression with superclass field and access method,"Consider the following:
{code:groovy}
abstract class A {
  protected Object thing = 'field'
  String getThing() { 'property' }
}
@groovy.transform.CompileStatic
class C extends A {
  void test() {
    print thing.toUpperCase()
  }
}
new C().test()
{code}

If "".toUpperCase()"" is removed, the script prints ""property"".  However the STC type recorded comes from the field.",emilles,emilles,Major,Closed,Fixed,24/Mar/23 21:28,10/Apr/23 20:40
Bug,GROOVY-10994,13530414,Fail to resolve method reference with generics,"I have the following program

{code}
import java.util.List;
import java.util.function.Predicate;


class Main {
  static final <T> void test() {

    List<T> x = null;
    Predicate<? super T> y = x::add;

  }
}
{code}

h3. Actual behavior

{code}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
Main.groovy: 9: Failed to find method 'add(T)' for the type: java.util.List<T> @ line 9, column 30.
       Predicate<? super T> y = x::add;
                                ^

1 error
{code}

h3. Expected behavior

Compile successfully

Tested against master (commit: a29ce1ce64d565526b70e145ace665dd0617ec9b)",emilles,theosot,Major,Resolved,Fixed,28/Mar/23 11:45,11/Apr/23 22:02
Bug,GROOVY-10995,13530624,Classes are compiled in Groovy 3 and Java but fails in Groovy 4,"We use Groovy 3. And I tried to migrate the existing application to use Groovy 4. 

I have a compilation issue in the code and the same behavior in IntelliJ IDEA, because it uses Groovy 4 syntax by default.

What I have:
{code:java}
// A simple interface with generic parameter
// This is Java class compiled by Java
interface MyInterface<T> {
} {code}
{code:java}
// A simple annotation that expects an MyInterface implemenations
// This is Java class compiled by Java
@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
@interface MyAnnotation {
    Class<? extends MyInterface<?>> value()
} {code}
{code:java}
//A class that implements MyInterface and ignores type raw type using warnings
//This is Groovy class located at the same package with the next code
public class MyClass implements MyInterface { 
}
{code}
{code:java}
// A method that uses MyAnnotation with MyClass parameter
//This is Groovy code that fails and located at the same package with MyClass
@MyAnnotation(value = com.example.MyClass)
static void main(String[] args) { 
}{code}
This code compiles in Groovy 3 and compiles in Java but fails to compile in Groovy 4.0.10

The code starts to compile in Groovy 4 If I change MyClass implementation to:
{code:java}
public class MyClass implements MyInterface<Object> {
} {code}
I think it is a bug, please, validate and fix it.",,mkamalov,Major,Closed,Fixed,29/Mar/23 13:08,29/Mar/23 14:14
Bug,GROOVY-10996,13530645,Compilation issue in Groovy 4,"We migrate the application to Groovy 4 and got an issue:

1) We use GroovyClassLoader to load *.groovy written classes to a classloader

2) Before the migration we used Groovy 3.0.15.

3) After the migration to 4.0.10 I've got the following error:
{code:java}
Only classes and closures can be used for attribute 'value' in @org.openl.rules.ruleservice.core.annotations.ServiceExtraMethod
 @ line 18, column 30.
       @ServiceExtraMethod(value = org.openl.generated.services.ServiceExtraMethodHandler2Impl.class)
                                ^ {code}
4) If the annotation definition changed from @ServiceExtraMethod(value = org.openl.generated.services.ServiceExtraMethodHandler2Impl.class) to @ServiceExtraMethod(value = org.openl.generated.services.ServiceExtraMethodHandler2Impl) then the code is compiled successfully.

Why does the compilation start to fail with .class? I can't reproduce the same issue on IntelliJ IDEA.

Can you suggest what to try? Is it expected for Groovy to stop supporting .class in Groovy 4?",emilles,mkamalov,Major,Resolved,Fixed,29/Mar/23 15:28,18/Apr/23 17:28
Bug,GROOVY-11001,13531769,AIOOBE when using method reference in nested lambda,"When using (1) the method-reference operator (but not the method-closure operator) (2) inside a nested lambda, the STC throws an AIOOBE.

{code:groovy}
@Grapes([
    @Grab(group = 'software.amazon.awssdk', module = 'utils', version = '2.18.31'),
    @Grab(group = 'io.vavr', module = 'vavr', version = '0.10.3'),
])
@CompileStatic
class Bug {
    Object bug() {
        Try.success('1234')
            .flatMap {
                Try.success(it)
                    .map(StringInputStream::new)
            }
    }
}
{code}

{code}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
General error during instruction selection: Internal compiler error while compiling Bug.groovy
Method: org.codehaus.groovy.ast.MethodNode@782bf610[software.amazon.awssdk.utils.StringInputStream ctorRef$doCall$0(java.lang.String) from Bug$_bug_closure1]
Line -1, expecting casting to software.amazon.awssdk.utils.StringInputStream but operand stack is empty

java.lang.ArrayIndexOutOfBoundsException: Internal compiler error while compiling Bug.groovy
Method: org.codehaus.groovy.ast.MethodNode@782bf610[software.amazon.awssdk.utils.StringInputStream ctorRef$doCall$0(java.lang.String) from Bug$_bug_closure1]
Line -1, expecting casting to software.amazon.awssdk.utils.StringInputStream but operand stack is empty
        at org.codehaus.groovy.classgen.asm.OperandStack.throwExceptionForNoStackElement(OperandStack.java:335)
        at org.codehaus.groovy.classgen.asm.OperandStack.doConvertAndCast(OperandStack.java:340)
        at org.codehaus.groovy.classgen.asm.OperandStack.doGroovyCast(OperandStack.java:305)
        at org.codehaus.groovy.classgen.asm.StatementWriter.writeReturn(StatementWriter.java:593)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitReturnStatement(AsmClassGenerator.java:812)
        at org.codehaus.groovy.ast.stmt.ReturnStatement.visit(ReturnStatement.java:73)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:601)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:546)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:110)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:685)
        at org.codehaus.groovy.ast.ClassNode.visitMethods(ClassNode.java:1142)
        at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1124)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:52)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:365)
        at org.codehaus.groovy.control.CompilationUnit$3.call(CompilationUnit.java:797)
        at org.codehaus.groovy.control.CompilationUnit$3.call(CompilationUnit.java:814)
        at org.codehaus.groovy.control.CompilationUnit$IPrimaryClassNodeOperation.doPhaseOperation(CompilationUnit.java:937)
        at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:692)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:666)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:647)
        at org.codehaus.groovy.tools.FileSystemCompiler.compile(FileSystemCompiler.java:311)
        at org.codehaus.groovy.tools.FileSystemCompiler.doCompilation(FileSystemCompiler.java:240)
        at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompile(FileSystemCompiler.java:165)
        at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompileWithErrorHandling(FileSystemCompiler.java:205)
        at org.codehaus.groovy.tools.FileSystemCompiler.main(FileSystemCompiler.java:189)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:568)
        at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:109)
        at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:132)
{code}",emilles,chrylis,Major,Resolved,Fixed,06/Apr/23 18:02,24/Apr/23 14:27
Bug,GROOVY-11003,13531867,Cannot find method with variable arguments when using fully qualified class names,"I have the following program

{code}
class Main {
  static final void test(java.util.List<Integer> p) {
    m(1, p);
  }

  static <T> void m(Integer x, java.util.List<T>... items) { }
}
{code}

h3. Actual behavior

{code}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
test.groovy: 3: [Static type checking] - Cannot find matching method Main#m(int, java.util.List<java.lang.Integer>). Please check if the declared type is correct and if the method exists.
 @ line 3, column 5.
       m(1, p);
       ^

1 error
{code}

h3. Expected behavior

Compile successfully

Tested against master (commit: a4e0d6de9cc2d8ecb48b48df501e63ec1735d837)",emilles,theosot,Major,Resolved,Fixed,07/Apr/23 13:07,11/Apr/23 16:16
Bug,GROOVY-11005,13531910,CompileStatic: Cannot set default-visible property in superclass that has a getter without a setter,"If a class's ancestors declare a default-visibility property and a getter for that property:
{code:java}
class HasProperty {
    Object foo

    Object getFoo() {
       return foo
    }
} {code}
Then the class, if it is CompileStatic, cannot set that property.

 

This fails to compile:
{code:java}
import groovy.transform.CompileStatic

@CompileStatic
class SetsProperty extends HasProperty {
    void doStuff() {
       foo = ""example""
    }
} {code}
Compiler output:
{noformat}
SetsProperty.groovy: 8: [Static type checking] - Cannot set read-only property: foo
 @ line 8, column 3.
           foo = ""example""{noformat}
 

It also fails to compile with ""this.@"":
{code:java}
import groovy.transform.CompileStatic

@CompileStatic
class SetsProperty extends HasProperty {
    void doStuff() {
       this.@foo = ""example""
    }
} {code}
Results in:
{noformat}
SetsProperty.groovy: 8: [Static type checking] - The field HasProperty.foo is not accessible
 @ line 8, column 9.
           this.@foo = ""example""{noformat}
 

{{These do compile if the class is not CompileStatic, or if the property is explicitly declared ""public"".}}

 

 ",emilles,jasongarrett,Minor,Resolved,Fixed,07/Apr/23 21:02,11/Apr/23 00:06
Bug,GROOVY-11007,13532088,STC: instanceof guard for property loses type information,"Consider the following:
{code:groovy}
interface I {
    CharSequence getCharSequence()
}

void accept(CharSequence cs) { }

void test(I i) {
    i.with {
        if (charSequence instanceof String) {
            charSequence.toUpperCase()
            accept(charSequence)
        }
    }
}

test({ -> 'works' } as I)
{code}


Reports ""Cannot find matching method script#accept(java.lang.Object)"".",emilles,emilles,Major,Resolved,Fixed,10/Apr/23 20:39,10/Apr/23 22:11
Bug,GROOVY-11009,13532193,STC resolves wrong method reference,"This is probably a regression

I have the following program

{code}
import java.util.function.*;

class Main {
  static final void test() {
    final Function<Double, Double> x = Main::clone;
  }

  public static <T> T clone(T x) { return x; }

}
{code}

h3. Actual behavior

{code}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
test.groovy: 5: [Static type checking] - Invalid return type: java.lang.Object is not convertible to java.lang.Double
 @ line 5, column 40.
       final Function<Double, Double> x = Main::clone;
                                          ^

1 error
{code}

h3. Expected behavior

Compile successfully

h3. Notes
Tested against master (commit: c4ee3ce0661eec7d633fc81281d79c8889b3dc66)

Test case adapted from:

{code}
import java.util.function.*;

class Main {
  static final void test() {
    Function<Double, Double> x = org.apache.commons.lang3.ObjectUtils::clone;
  }
}
{code}",emilles,theosot,Minor,Resolved,Fixed,11/Apr/23 13:35,18/Apr/23 21:21
Bug,GROOVY-11011,13532529,SC: array instanceof guard and length or subscript,"Consider the following:
{code:groovy}
@groovy.transform.CompileStatic
class FileTreeBuilder {
    def methodMissing(String name, Object args) {
        if (args instanceof Object[] && args.length == 1) {
            def arg = args[0]
            if (arg instanceof Closure) {
                dir(name, arg)
            } else if (arg instanceof CharSequence) {
                file(name, arg.toString())
            } else if (arg instanceof byte[]) {
                file(name, arg)
            } else if (arg instanceof File) {
                file(name, arg)
            }
        }
    }
}
{code}

STC gives no errors for this, but there are 2 class format issues.  ""args.length"" is reporting bad type on operand stack and ""if (arg instanceof CharSequence)"" and subsequent guards are reporting improper class name ""Ljava/lang/Object;"" on the operand stack.",emilles,emilles,Major,Resolved,Fixed,13/Apr/23 15:32,13/Apr/23 17:12
Bug,GROOVY-11012,13532538,Wrong type is inferred when calling method defined in the parent class,"I have the following program

{code}
interface Bar<V> {
  default V get(Object x, Object y) { return null; }
}

class Foo<V> implements Bar<V> {
  static <V> Foo<V> create() { return null; }
}

class Main {
  static final void test() {
    Number x = Foo.<Number>create().get(null, null);
  }

}
{code}

h3. Actual behavior

{code}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
test.groovy: 11: [Static type checking] - Cannot assign value of type java.lang.Object to variable of type java.lang.Number
 @ line 11, column 16.
       Number x = Foo.<Number>create().get(null, null);
                  ^

1 error
{code}

h3. Expected behavior

Compile successfully

h3. Notes

Tested against master (commit: 5bcd83e0fde722971f97d479f1a4d6da6cd4aa5f)

Test cased adapt from the following program that uses the Guava lib

{code}
import com.google.common.collect.HashBasedTable;

class Main {
  static final void test() {
    Number x = HashBasedTable.<Number, Number, Number>create().get(null, null);
  }
}
{code}",emilles,theosot,Minor,Resolved,Fixed,13/Apr/23 16:19,30/Apr/23 16:29
Bug,GROOVY-11013,13532671,Fail to substitute type when having SAM type with generics,"I have the following program

{code}
import java.util.*;

@FunctionalInterface
interface Foo<T> {
  int m(List<T> x);
}

class Main {
  static final void test() {
    Foo<String> p = (List<String> x) -> 1;
  }
}
{code}

h3. Actual behavior

{code}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
Main.groovy: 11: [Static type checking] - Expected type java.util.List<T> for lambda parameter: x
 @ line 11, column 22.
       Foo<String> p = (List<String> x) -> 1;
                        ^

1 error
{code}

h3. Expected behavior

Compile successfully

h3. Notes

Tested against master (commit: cdc57551a7cce33ec7031b40753f25c869dcf0cc)

Test case adapted from the following program that uses the apache-commons-collection4 lib

{code}

class Main {
  static final void test() {
    org.apache.commons.collections4.sequence.ReplacementsHandler<Number> f = (int x, List<java.io.IOException> y, List<Number> z) -> { } ;
  }
}
{code}",emilles,theosot,Minor,Resolved,Fixed,14/Apr/23 14:09,26/May/23 22:42
Bug,GROOVY-11019,13532949,joint compilation:  public static final string does not compile,"I haven't managed to reproduce this in a simple example, but as I'm upgrading my application from groovy 3 to 4 I have run into this situation.  I have a class that defines a public static final string like this:
{code:java}
class PublicStaticString { 
   public static final String NONE = ""None"" 
}
{code}
Its java stub fails to compile with:
{noformat}
error: no suitable constructor found for String(Object)
public static final java.lang.String NONE = new java.lang.String((java.lang.Object)null);{noformat}
 

I can remove the ""public"" from the declaration and the class/stub will compile, but now any java class that references that field will fail to compile with:
{noformat}
error: NONE has private access in PublicStaticString
         selectNoneButton.setText(PublicStaticString.NONE);{noformat}
The only way I know to fix this is to convert the java class to groovy.",emilles,jasongarrett,Major,Resolved,Fixed,17/Apr/23 21:14,18/Apr/23 19:54
Bug,GROOVY-11020,13533178,NullPointerException in instruction selection when calling parameterized function whose type parameter has an upper bound corresponding to a SAM type,"I have the following code

{code}
import java.util.*;
import java.util.function.*;

class Main {
  static final void test(ArrayDeque<String> y) {
    
    m(y::addFirst);
  }

  static <C extends Consumer<String>> void m(C cl) {}
}
{code}

h3. Actual behavior

The compiler crashes with the following stacktrace:

{code}
>>> a serious error occurred: BUG! exception in phase 'instruction selection' in source unit 'Main.groovy' unexpected NullPointerException
>>> stacktrace:
BUG! exception in phase 'instruction selection' in source unit 'Main.groovy' unexpected NullPointerException
        at org.codehaus.groovy.control.CompilationUnit$IPrimaryClassNodeOperation.doPhaseOperation(CompilationUnit.java:953)
        at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:694)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:668)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:649)
        at org.codehaus.groovy.tools.FileSystemCompiler.compile(FileSystemCompiler.java:311)
        at org.codehaus.groovy.tools.FileSystemCompiler.doCompilation(FileSystemCompiler.java:240)
        at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompile(FileSystemCompiler.java:165)
        at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompileWithErrorHandling(FileSystemCompiler.java:205)
        at org.codehaus.groovy.tools.FileSystemCompiler.main(FileSystemCompiler.java:189)
Caused by: java.lang.NullPointerException
        at org.codehaus.groovy.ast.tools.GenericsUtils.parameterizeSAM(GenericsUtils.java:974)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.inferClosureParameterTypes(StaticTypeCheckingVisitor.java:3110)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethodCallArguments(StaticTypeCheckingVisitor.java:2904)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitStaticMethodCallExpression(StaticTypeCheckingVisitor.java:2819)
        at org.codehaus.groovy.ast.expr.StaticMethodCallExpression.visit(StaticMethodCallExpression.java:44)
        at org.codehaus.groovy.ast.CodeVisitorSupport.visitExpressionStatement(CodeVisitorSupport.java:117)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitExpressionStatement(ClassCodeVisitorSupport.java:212)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitExpressionStatement(StaticTypeCheckingVisitor.java:2217)
        at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:41)
        at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:86)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:175)
        at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:72)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:139)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:118)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.lambda$visitConstructorOrMethod$28(StaticTypeCheckingVisitor.java:2680)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.doWithTypeCheckingExtensions(StaticTypeCheckingVisitor.java:463)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitConstructorOrMethod(StaticTypeCheckingVisitor.java:2680)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:110)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.startMethodInference(StaticTypeCheckingVisitor.java:2663)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethod(StaticTypeCheckingVisitor.java:2642)
        at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitConstructorOrMethod(StaticCompilationVisitor.java:189)
        at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitMethod(StaticCompilationVisitor.java:204)
        at org.codehaus.groovy.ast.ClassNode.visitMethods(ClassNode.java:1144)
        at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1137)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:52)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.lambda$visitClass$3(StaticTypeCheckingVisitor.java:499)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.doWithTypeCheckingExtensions(StaticTypeCheckingVisitor.java:463)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitClass(StaticTypeCheckingVisitor.java:499)
        at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitClass(StaticCompilationVisitor.java:151)
        at org.codehaus.groovy.transform.sc.StaticCompileTransformation.visit(StaticCompileTransformation.java:68)
        at org.codehaus.groovy.control.customizers.ASTTransformationCustomizer.call(ASTTransformationCustomizer.groovy:303)
        at org.codehaus.groovy.control.CompilationUnit$IPrimaryClassNodeOperation.doPhaseOperation(CompilationUnit.java:948)
        ... 8 more
{code}

h3. Expected behavior 

Compile successfully

h3. Notes

Tested against master (commit: 61f445cf4b1099eb25f770cc0685efb10783b8f1)

Test case adapted from a program that uses the apache-commons-collections lib:

{code}
import java.util.*;
import java.util.function.*;

class Main {
  static final void test(Iterator<String> x,
                         ArrayDeque<String> y) {
    
    org.apache.commons.collections4.CollectionUtils.forAllButLastDo(x, y::addFirst);
  }

}
{code}",emilles,theosot,Minor,Resolved,Fixed,19/Apr/23 09:15,29/May/23 01:24
Bug,GROOVY-11023,13533863,SC: IncompatibleClassChangeError when closure direct-targets extension method,"Consider the following:
{code:groovy}
@groovy.transform.CompileStatic
void test() {
    def c = { p, q = p.toString() -> '' + p + q }
    assert c('foo', 'bar') == 'foobar'
    assert c('foo') == 'foofoo'
}
test()
{code}

Default argument creates delegate method that does not have static compilation.  Running script throws {{java.lang.IncompatibleClassChangeError}}.",emilles,emilles,Major,Resolved,Fixed,24/Apr/23 14:27,27/Apr/23 18:25
Bug,GROOVY-11024,13533881,STC NPE when using Collectors.toMap() with a generic result,"I'm not sure what the most specific culprit is, but this reproduces the problem reliably. (The original use case was using {{DynamoDbTable#index(String)}}, which also returns a generic result.)

{code:groovy}
import static java.util.function.Function.identity
import static java.util.stream.Collectors.toMap

import groovy.transform.CompileStatic

@CompileStatic
class Bug {
    Bug() {
        ['a'].stream().collect(toMap(identity(), List::of))
    }
}
{code}

{code}
[ERROR] Caused by: java.lang.NullPointerException: Cannot load from object array because the return value of ""org.codehaus.groovy.ast.ClassNode.getGenericsTypes()"" is null
[ERROR]         at org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.isAssignableTo(StaticTypeCheckingSupport.java:496)
[ERROR]         at org.codehaus.groovy.ast.tools.ParameterUtils.parametersMatch(ParameterUtils.java:74)
[ERROR]         at org.codehaus.groovy.ast.tools.ParameterUtils.parametersCompatible(ParameterUtils.java:53)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticTypesMethodReferenceExpressionWriter.lambda$findMethodRefMethod$2(StaticTypesMethodReferenceExpressionWriter.java:334)
[ERROR]         at java.base/java.util.Collection.removeIf(Collection.java:576)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticTypesMethodReferenceExpressionWriter.findMethodRefMethod(StaticTypesMethodReferenceExpressionWriter.java:319)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticTypesMethodReferenceExpressionWriter.writeMethodReferenceExpression(StaticTypesMethodReferenceExpressionWriter.java:104)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethodReferenceExpression(AsmClassGenerator.java:925)
[ERROR]         at org.codehaus.groovy.ast.expr.MethodReferenceExpression.visit(MethodReferenceExpression.java:34)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.visitArgument(StaticInvocationWriter.java:513)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.loadArguments(StaticInvocationWriter.java:460)
[ERROR]         at org.codehaus.groovy.classgen.asm.InvocationWriter.writeDirectMethodCall(InvocationWriter.java:213)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.writeDirectMethodCall(StaticInvocationWriter.java:386)
[ERROR]         at org.codehaus.groovy.classgen.asm.InvocationWriter.makeDirectCall(InvocationWriter.java:311)
[ERROR]         at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:131)
[ERROR]         at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:125)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.makeCall(StaticInvocationWriter.java:648)
[ERROR]         at org.codehaus.groovy.classgen.asm.InvocationWriter.writeInvokeMethod(InvocationWriter.java:454)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.writeInvokeMethod(StaticInvocationWriter.java:136)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethodCallExpression(AsmClassGenerator.java:988)
[ERROR]         at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:77)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.visitArgument(StaticInvocationWriter.java:513)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.loadArguments(StaticInvocationWriter.java:460)
[ERROR]         at org.codehaus.groovy.classgen.asm.InvocationWriter.writeDirectMethodCall(InvocationWriter.java:213)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.writeDirectMethodCall(StaticInvocationWriter.java:386)
[ERROR]         at org.codehaus.groovy.classgen.asm.InvocationWriter.makeDirectCall(InvocationWriter.java:311)
[ERROR]         at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:131)
[ERROR]         at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:125)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.makeCall(StaticInvocationWriter.java:648)
[ERROR]         at org.codehaus.groovy.classgen.asm.InvocationWriter.writeInvokeMethod(InvocationWriter.java:454)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticInvocationWriter.writeInvokeMethod(StaticInvocationWriter.java:136)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethodCallExpression(AsmClassGenerator.java:988)
[ERROR]         at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:77)
[ERROR]         at org.codehaus.groovy.classgen.asm.StatementWriter.writeExpressionStatement(StatementWriter.java:613)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitExpressionStatement(AsmClassGenerator.java:827)
[ERROR]         at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:41)
[ERROR]         at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:95)
[ERROR]         at org.codehaus.groovy.classgen.asm.sc.StaticTypesStatementWriter.writeBlockStatement(StaticTypesStatementWriter.java:77)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:752)
[ERROR]         at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:72)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:611)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:546)
[ERROR]         at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructor(ClassCodeVisitorSupport.java:105)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructor(AsmClassGenerator.java:689)
[ERROR]         at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1121)
[ERROR]         at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:52)
[ERROR]         at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:365)
[ERROR]         ... 41 more

{code}",emilles,chrylis,Major,Resolved,Fixed,24/Apr/23 17:40,01/May/23 19:52
Bug,GROOVY-11026,13534180,NPE when checking compatibility of method reference,"I have the following program

{code}
import java.util.function.*;
import java.util.List;

class Main {
  static final void test() {
        List<String> x = null;
        m(x::set);
  }

  static final <T, R> void m(BiFunction<T, R, R> y) {}

}
{code}

h3. Actual behavior

{code}
BUG! exception in phase 'instruction selection' in source unit 'test.groovy' unexpected NullPointerException
        at org.codehaus.groovy.control.CompilationUnit$IPrimaryClassNodeOperation.doPhaseOperation(CompilationUnit.java:953)
        at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:694)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:668)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:649)
        at org.codehaus.groovy.tools.FileSystemCompiler.compile(FileSystemCompiler.java:311)
        at org.codehaus.groovy.tools.FileSystemCompiler.doCompilation(FileSystemCompiler.java:240)
        at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompile(FileSystemCompiler.java:165)
        at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompileWithErrorHandling(FileSystemCompiler.java:205)
        at org.codehaus.groovy.tools.FileSystemCompiler.main(FileSystemCompiler.java:189)
Caused by: java.lang.NullPointerException
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingSupport.checkCompatibleAssignmentTypes(StaticTypeCheckingSupport.java:725)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.lambda$visitMethodPointerExpression$25(StaticTypeCheckingVisitor.java:2504)
        at java.base/java.util.Optional.ifPresent(Optional.java:183)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethodPointerExpression(StaticTypeCheckingVisitor.java:2496)
        at org.codehaus.groovy.ast.CodeVisitorSupport.visitMethodReferenceExpression(CodeVisitorSupport.java:319)
        at org.codehaus.groovy.ast.expr.MethodReferenceExpression.visit(MethodReferenceExpression.java:34)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethodCallArguments(StaticTypeCheckingVisitor.java:2912)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitStaticMethodCallExpression(StaticTypeCheckingVisitor.java:2820)
        at org.codehaus.groovy.ast.expr.StaticMethodCallExpression.visit(StaticMethodCallExpression.java:44)
        at org.codehaus.groovy.ast.CodeVisitorSupport.visitExpressionStatement(CodeVisitorSupport.java:117)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitExpressionStatement(ClassCodeVisitorSupport.java:212)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitExpressionStatement(StaticTypeCheckingVisitor.java:2217)
        at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:41)
        at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:86)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:175)
        at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:72)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:139)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:118)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.lambda$visitConstructorOrMethod$28(StaticTypeCheckingVisitor.java:2681)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.doWithTypeCheckingExtensions(StaticTypeCheckingVisitor.java:463)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitConstructorOrMethod(StaticTypeCheckingVisitor.java:2681)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:110)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.startMethodInference(StaticTypeCheckingVisitor.java:2664)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitMethod(StaticTypeCheckingVisitor.java:2643)
        at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitConstructorOrMethod(StaticCompilationVisitor.java:189)
        at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitMethod(StaticCompilationVisitor.java:204)
        at org.codehaus.groovy.ast.ClassNode.visitMethods(ClassNode.java:1144)
        at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1137)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:52)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.lambda$visitClass$3(StaticTypeCheckingVisitor.java:499)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.doWithTypeCheckingExtensions(StaticTypeCheckingVisitor.java:463)
        at org.codehaus.groovy.transform.stc.StaticTypeCheckingVisitor.visitClass(StaticTypeCheckingVisitor.java:499)
        at org.codehaus.groovy.transform.sc.StaticCompilationVisitor.visitClass(StaticCompilationVisitor.java:151)
        at org.codehaus.groovy.transform.sc.StaticCompileTransformation.visit(StaticCompileTransformation.java:68)
        at org.codehaus.groovy.control.customizers.ASTTransformationCustomizer.call(ASTTransformationCustomizer.groovy:303)
        at org.codehaus.groovy.control.CompilationUnit$IPrimaryClassNodeOperation.doPhaseOperation(CompilationUnit.java:948)
        ... 8 more
{code}

h3. Compile succssfully

h3. Notes

Tested against master (commit: 8346e3406ed51dd071a2d31792d3fbad543585a7)",emilles,theosot,Minor,Resolved,Fixed,26/Apr/23 15:40,29/Apr/23 15:28
Bug,GROOVY-11028,13534231,STC: empty map (or list) literal does not type check like emptyMap(),"Consider the following:
{code:groovy}
@groovy.transform.TypeChecked
void test() {
  List<Integer> list = [].withDefault { 0 }
    // Cannot assign ListWithDefault<? extends Object> to List<Integer>
  Map<String,Integer> map = [:].withDefault { 1 }
    // Cannot assign Map<Object, ? extends Object> to Map<String, Integer>
}
{code}",emilles,emilles,Major,Resolved,Fixed,26/Apr/23 22:00,06/Jun/23 13:31
Bug,GROOVY-11029,13534343,SC: super property is not accessible by a subclass that defines a getter and setter,"If a class defines a default-visibility property and a subclass declares a getter and a setter for that property, the subclass cannot access super's property.
{code:java}
import groovy.transform.CompileStatic
import spock.lang.Specification

class Foo {
    Object myThing
}

@CompileStatic  // everything works without CompileStatic
class Bar extends Foo {
    void setMyThing(Object list) {
        super.myThing = list
    }
    
    Object getMyThing() {
        return super.myThing as List  // groovy.lang.MissingPropertyException: No such property: myThing for class: Bar
//        return super.@myThing as List  // [Static type checking] - The field Foo.myThing is not accessible
    }
}

class BarTest extends Specification {
    def ""myThing""() {
        when:
            Bar bar = new Bar()
            List list = []
            bar.myThing = list
        then:
            bar.myThing.is(list)
    }
}{code}
[Live Demo|https://gwc-experiment.appspot.com/?g=groovy_4_0&codez=eJxtUUFuwjAQPNevWHGCQ8M9EhJq1Z5KWwluiENwNolLYlv2Qogq_l7bGEgKK9mSM5OZ3VnRaGUISqPUoUvIZNIWyjTJq2q0qHFJGQnOxJllteK7pM5kmSw1clEI7mAlGeN1Zi28KwW_DFx9bX-QEzTdqhKyZCfG5gNFgOkU8ICmI0-AVpmdhVZQpfYEQ_Oz9ktmAI-EMu_7HJTIwSItzkbj6FsLS5NI8WX3Gk0Su4FZwAN4Cne_5fIm1lcwSHsj_wllFj6cUhgmJhjCWQhrHf5tlGNT93bkqH1OKXwqJ8Er0BFKLxGBSx3CpKkflTnFR87zR9brmCl1GoFXyHeOsYFnWFUIhcA694lduxYWpCLIOEfX5rbGmMSJ3aJeodO-xD3YdcwkxwJGUXE0SKqtUKbXly-_uq07M5DY-td4csWfwhR-Hw5eb27f3Q-PFuaL7hx65ETYcVj_Zao_zwXo-w]

A workaround is to declare a getter and setter in the superclass.",emilles,jasongarrett,Major,Resolved,Fixed,27/Apr/23 16:45,04/May/23 17:56
Bug,GROOVY-11030,13534348,CompileStatic: string concatenation in method parameter default value results in NoSuchMethodError,"If a method parameter's default value is defined by concatenating a String literal to a String variable, calling the method throws java.lang.NoSuchMethodError.
{code:java}
import groovy.transform.CompileStatic
import spock.lang.Specification

@CompileStatic  // works without CompileStatic
class MyClass {
    String makePlural(String word, String plural = word + ""s"") {  // java.lang.NoSuchMethodError: 'java.lang.String java.lang.String.plus(java.lang.CharSequence)'
        return plural
    }
}

class Test extends Specification {
    def test() {
        setup:
            MyClass myObj = new MyClass()
        expect:
            myObj.makePlural(""word"") == ""words""
    }
}    {code}
[Live demo.|https://gwc-experiment.appspot.com/?g=groovy_4_0&codez=eJxdUMFugzAMvecrLC4FbaL3SkiTqh3bTWI_kAUDKZAwJ5Siqv--kIZ2zBcn79nP9pNdr8lCRVqfp9QSV6bU1KV73fWyxdxyKwWT9yrTa9GkLVdVmvcoZCmFo7Vi7G1VD7DdwqipMTBKW-vBwlpPtNwYOEx7n68MXOSWpKqg4w1-tgPxNg6IEypeF7r3FGQehReITJTA1Q888TO_L3fU-SDqA7rRxTuRph1snmxQ-g-kTtrET3Rfc8rxZ0AlMNn4FecgtAOpsIdHb-zGwkVfaCzgxaIqDKwsCkcWWIJ1RXESgDmMk-x3j-8cizXd9PF9ctcqHBcsTh6VeHEj7LrTd6R_XIxmp5xJWQb-aaJl6zn9AvUzpMc]

A workaround is to do the concatenation in a GString:
{code:java}
String makePlural(String word, String plural = ""${word}s"") { {code}",emilles,jasongarrett,Minor,Resolved,Fixed,27/Apr/23 17:57,27/Apr/23 19:19
Bug,GROOVY-11036,13534428,StackOverflowError in cyclic interface inheritance,"I have the following program

{code}
interface Bar {}
interface Foo extends Foo, Bar {}
{code}

h3. Actual behavior

{code}
>>> a serious error occurred: null
>>> stacktrace:
java.lang.StackOverflowError
	at org.codehaus.groovy.ast.ClassNode.getText(ClassNode.java:1606)
	at org.codehaus.groovy.ast.ClassNode.hashCode(ClassNode.java:734)
	at org.codehaus.groovy.ast.ClassNode.hashCode(ClassNode.java:734)
	at java.base/java.util.HashMap.hash(HashMap.java:339)
	at java.base/java.util.HashMap.put(HashMap.java:607)
	at java.base/java.util.HashSet.add(HashSet.java:220)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:466)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
	at org.codehaus.groovy.ast.ClassNode.getAllInterfaces(ClassNode.java:470)
{code}

h3. Expected behavior

Reject due to cyclic inheritance.

Tested against master (commit: ed4b33c8d5ac71ee4963d6bb391850593c59e5b1)",emilles,theosot,Major,Resolved,Fixed,28/Apr/23 08:57,01/May/23 17:27
Bug,GROOVY-11041,13534647,Incorrect property handling for records,"While it is not normal to override record component getters, it is perfectly allowable.

 

The following record is getting both an ""x()"" method and a ""getX()"" method. The supplied ""x()"" method isn't being associated correctly with the property.
{code:java}
record Foo(String x, String y) {
    String x() {
        'z'
    }
} {code}",paulk,paulk,Major,Resolved,Fixed,02/May/23 05:54,09/May/23 13:59
Bug,GROOVY-11044,13535041,SC: property access within closure produces cast exception,"Consider the following:
{code:groovy}
class Bar {
  @groovy.beans.Bindable
  String baz
  String other
}
class Foo {
  Bar bar
  @groovy.beans.Bindable
  String foo
  @groovy.transform.CompileStatic
  void postConstruct() {
    bar = new Bar()
    bar.with {
      addPropertyChangeListener('baz') { event ->
        other = 'value' // ClassCastException: class Foo cannot be cast to class Bar
        print 'changed'
      }
    }
    print 'ready;'
  }
}

Foo foo = new Foo()
foo.postConstruct()
foo.getBar().setBaz('xxx')
{code}",emilles,emilles,Major,Resolved,Fixed,04/May/23 17:54,04/May/23 23:51
Bug,GROOVY-11051,13535692,Static compiler does not coerce Groovy truth from closures,"The static compiler will accept a closure returning a non-boolean value as fulfilling a {{Predicate}}, but it does not coerce the return value using Groovy truth.

{code}
@CompileStatic
class Bug {
    static void main(String[] args) {
        println new AtomicReference<Object>(null).stream()
            .filter( { it.get() } as Predicate<AtomicReference<?>>)
      // or .filter(AtomicReference::get as Predicate<AtomicReference<?>>)
            .findAny()
    }
}
{code}

Expected result: {{Optional.empty}}
Actual result: NullPointerException on unboxing the {{null}} return value

Using an explicit {{as boolean}} inside the lambda works, but this doesn't work with method references. Coercion should work even without {{as Predicate}} if the compiler sees the functional return type is boolean/Boolean.",emilles,chrylis,Major,Resolved,Fixed,10/May/23 20:01,30/May/23 17:48
Bug,GROOVY-11053,13535815,Method ambiguity error when having methods with primitive arrays and varargs,"I have the following program (it uses the apache-commons-lang3 library)

{code}
class Main {
  static final void test() {
    final byte[] vetting = new byte[1];
    final byte[] grafting = org.apache.commons.lang3.ArrayUtils.removeAll(vetting); // works 
    final byte[] grafting = org.apache.commons.lang3.ArrayUtils.removeAll(vetting, 0); // fails 
  }
}
{code}

h3. Actual behavior

{code}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
groovy38.groovy: 4: [Static type checking] - Reference to method is ambiguous. Cannot choose between [byte[] org.apache.commons.lang3.ArrayUtils#removeAll(byte[], int[]), long[] org.apache.commons.lang3.ArrayUtils#removeAll(long[], int[]), float[] org.apache.commons.lang3.ArrayUtils#removeAll(float[], int[]), double[] org.apache.commons.lang3.ArrayUtils#removeAll(double[], int[]), int[] org.apache.commons.lang3.ArrayUtils#removeAll(int[], int[]), short[] org.apache.commons.lang3.ArrayUtils#removeAll(short[], int[])]
 @ line 4, column 29.
       final byte[] grafting = org.apache.commons.lang3.ArrayUtils.removeAll(vetting, 0);
                               ^

1 error
{code}

h3. Expected behavior

Compile successfully

h3. Tested against master (commit: 7a4b2679c77aea8cd9dc00fa2d3b71e25e97fbd2)",emilles,theosot,Minor,Resolved,Fixed,11/May/23 13:43,29/May/23 20:23
Bug,GROOVY-11056,13536153,modified behavior of variable resolution within Closure ,"Groovy stopped seeing statically imported instances when called within Closure within Closure. And instead it thinks it is some method call.

Below is the example to repro. At the moment it affects all our users written test scripts and we rolled back to 3.0.15 where behavior is still as expected.

Tried various closure resolution strategies to no result 
{code:java}
import static org.testingisdocumenting.webtau.persona.ReproPersonaList.Admin

class ClosureResolveRepro {
    static void main(String[] args) {
        def LocalAdmin = new ReproPersona(""local-admin"")

        wrapper {
            LocalAdmin { // <-- no issue here as it can correctly see LocalAdmin
            }
        }

        wrapper {
            Admin { // <--- issue is here, for some reason it doesn't see statically imported instance. It works as expected in 3.0.15
            }
        }
    }

    private static void wrapper(Closure code) { // this wrapper is required to reproduce 
        code.run()
    }
}
 {code}
{code:java}
person: local-adminException in thread ""main"" groovy.lang.MissingMethodException: No signature of method: org.testingisdocumenting.webtau.persona.ClosureResolveRepro$_main_closure2.Admin() is applicable for argument types: (org.testingisdocumenting.webtau.persona.ClosureResolveRepro$_main_closure2$_closure4) values: [org.testingisdocumenting.webtau.persona.ClosureResolveRepro$_main_closure2$_closure4@40f08448]Possible solutions: find(), dump(), find(), dump(), any(), any() at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:380) at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1035) at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:73) at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:51) at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:171) at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:185) at org.testingisdocumenting.webtau.persona.ClosureResolveRepro$_main_closure2.doCall(ClosureResolveRepro.groovy:31) at org.testingisdocumenting.webtau.persona.ClosureResolveRepro$_main_closure2.doCall(ClosureResolveRepro.groovy) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107) at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323) at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:274) at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1035) at groovy.lang.Closure.call(Closure.java:412) at groovy.lang.Closure.call(Closure.java:406) at groovy.lang.Closure.run(Closure.java:493) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107) at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323) at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:274) at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1035) at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:38) at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47) at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:53) at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:130) at org.testingisdocumenting.webtau.persona.ClosureResolveRepro.wrapper(ClosureResolveRepro.groovy:37) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107) at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrapNoCoerce.invoke(StaticMetaMethodSite.java:149) at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.callStatic(StaticMetaMethodSite.java:100) at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:55) at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:217) at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:231) at org.testingisdocumenting.webtau.persona.ClosureResolveRepro.main(ClosureResolveRepro.groovy:30)
{code}
 
{code:java}
class ReproPersona {
    private String id

    ReproPersona(String id) {
        this.id = id
    }

    void call(Closure code) {
        println ""person: $id""
    }
} {code}
{code:java}
class ReproPersonaList {
    public static def Admin = new ReproPersona(""admin"")
} {code}
 ",emilles,ninside,Major,Resolved,Fixed,15/May/23 02:55,16/May/23 17:10
Bug,GROOVY-11057,13536166,STC Generic type matching failure,"With Groovy version 4.0.12 this code will no longer compile:

 
{code:java}
import groovy.transform.CompileStatic
import org.junit.jupiter.api.Test
import org.mockito.Mockito

@CompileStatic
interface Configuration {
  Map<String, Object> getSettings()
}

@CompileStatic
class GenericMapStubbing {

  @Test
  void stubSettings() {
    def configuration = Mockito.mock(Configuration).tap {
      Mockito.when(it.getSettings()).thenReturn([:])
    }

    assert configuration.settings.isEmpty()
  }
} {code}
Failure message:

 

18: [Static type checking] - Cannot find matching method org.mockito.stubbing.OngoingStubbing#thenReturn(java.util.LinkedHashMap<#K, #V>). Please check if the declared type is correct and if the method exists.
 @ line 16, column 7.
         Mockito.when(it.getSettings()).thenReturn([:])",emilles,jhunovis,Major,Resolved,Fixed,15/May/23 06:18,06/Jun/23 13:30
Bug,GROOVY-11060,13536323,SC internal error when spreading inside list literal,"Spreading a list inside brackets causes a {{SpreadExpression should not be visited here}} during class generation. (The business instance of this is trying to use a list of URL patterns for Spring {{antMatchers}}.)

{code}
@CompileStatic
class Bug {
    public static final List<String> A = ['a1', 'a2']

    static void varargs(String... strings) {
        println Arrays.toString(strings)
    }

    public static void main(String... args) {
        varargs([
            *A
        ] as String[])
    }
}
{code}",emilles,chrylis,Major,Resolved,Fixed,15/May/23 22:12,22/May/23 17:10
Bug,GROOVY-11062,13536508,Annotation collection isn't properly handling enum annotation attribute values,"The easiest way to reproduce is to change the definition of `Immutable`.

 Change the line:

{code}
@TupleConstructor(defaults = false)
{code}

to:

{code}
@TupleConstructor(defaultsMode = DefaultsMode.OFF)
{code}

The first error is to do with stub generation:
{noformat}
subprojects/groovy-macro/build/tmp/compileGroovy/groovy-java-stubs/org/codehaus/groovy/macro/matcher/MatchingConstraints.java:3: error: incompatible types: String cannot be converted to DefaultsMode
@groovy.transform.CompileStatic() @groovy.transform.ToString(includeSuperProperties=true, cache=true) @groovy.transform.EqualsAndHashCode(cache=true) @groovy.transform.ImmutableBase() @groovy.transform.Final() @groovy.transform.ImmutableOptions(knownImmutableClasses={org.codehaus.groovy.macro.matcher.internal.ConstraintPredicate.class}) @groovy.transform.PropertyOptions(propertyHandler=groovy.transform.options.ImmutablePropertyHandler.class) @groovy.transform.TupleConstructor(defaultsMode=""OFF"") @groovy.transform.MapConstructor(noArg=true, includeSuperProperties=true, includeFields=true) @groovy.transform.KnownImmutable() public class MatchingConstraints
                                                                                                                                                                                                                                                                                                                                                                        
              ^
1 error
startup failed:
Compilation failed; see the compiler error output for details.
{noformat}

Once that is fixed, the next error surfaces which is once the `compileTestGroovy` task is executed:
{noformat}
General error during instruction selection: Cannot generate bytecode for constant: OFF of type: groovy.transform.DefaultsMode

org.codehaus.groovy.classgen.ClassGeneratorException: Cannot generate bytecode for constant: OFF of type: groovy.transform.DefaultsMode
        at org.codehaus.groovy.classgen.asm.OperandStack.pushConstant(OperandStack.java:513)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstantExpression(AsmClassGenerator.java:897)
        at org.codehaus.groovy.ast.expr.ConstantExpression.visit(ConstantExpression.java:82)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitMapExpression(AsmClassGenerator.java:1615)
        at org.codehaus.groovy.ast.expr.MapExpression.visit(MapExpression.java:54)
...
{noformat}

",paulk,paulk,Major,Resolved,Fixed,17/May/23 07:57,22/May/23 00:40
Bug,GROOVY-11063,13536597,NPE in ListHashMap,"Stacktrace:
{code:java}
java.lang.NullPointerException
at org.codehaus.groovy.util.ListHashMap.get(ListHashMap.java:120)
at org.codehaus.groovy.ast.NodeMetaDataHandler.getNodeMetaData(NodeMetaDataHandler.java:44){code}
 

It's fixed on master branch with commit 923f680d769f236a543e622121c7fa3b4badcd56, but wasn't backported to 4.x branch.",emilles,weih,Major,Resolved,Fixed,17/May/23 16:32,18/May/23 13:18
Bug,GROOVY-11068,13537373,VerifyError when using generic container in closure,"The following code produces a {{VerifyError}}. It appears to this amateur eye that the code in the closure actually performs an {{invokedynamic}} on the nested method reference and _then_ calls the Iterable's {{forEach}}, when I would expect it to push a reference to the method handle somehow and then let that be passed to {{forEach}}.
{code:groovy}
@Grab('org.apache.pdfbox:pdfbox:2.0.28')
import org.apache.pdfbox.pdmodel.*
@Grab('io.vavr:vavr:0.10.4')
import io.vavr.control.Try
@CompileStatic
@POJO
class Bug {
    public static void main(String... args) {
        PDDocument doc = new PDDocument()
        extraPages().forEach {
            it.forEach(doc::addPage)
        }
    }
    static Try<Iterable<PDPage>> extraPages() {
        return Try.success([null]) // value present only to cause the iteration to proceed; NPE is the expected outcome here
    }
}
{code}

{code}
java.lang.VerifyError: Bad type on operand stack
Exception Details:
  Location:
    Bug$_main_closure1.doCall(Ljava/lang/Object;)Ljava/lang/Void; @11: invokedynamic
  Reason:
    Type 'java/lang/Object' (current frame, stack[1]) is not assignable to 'org/apache/pdfbox/pdmodel/PDDocument'
  Current Frame:
    bci: @11
    flags: { }
    locals: { 'Bug$_main_closure1', 'java/lang/Object' }
    stack: { 'java/lang/Iterable', 'java/lang/Object' }
  Bytecode:
    0000000: 2bc0 0024 2ab4 001c b600 2aba 003f 0000
    0000010: b900 4302 0001 ba00 5000 00b0          

	at Bug.main(Bug.groovy:16)
{code}

{code}
  public java.lang.Void doCall(java.lang.Object);
    descriptor: (Ljava/lang/Object;)Ljava/lang/Void;
    flags: (0x0001) ACC_PUBLIC
    Code:
      stack=2, locals=2, args_size=2
         0: aload_1
         1: checkcast     #36                 // class java/lang/Iterable
         4: aload_0
         5: getfield      #28                 // Field doc:Lgroovy/lang/Reference;
         8: invokevirtual #42                 // Method groovy/lang/Reference.get:()Ljava/lang/Object;
        11: invokedynamic #63,  0             // InvokeDynamic #0:accept:(Lorg/apache/pdfbox/pdmodel/PDDocument;)Ljava/util/function/Consumer;
        16: invokeinterface #67,  2           // InterfaceMethod java/lang/Iterable.forEach:(Ljava/util/function/Consumer;)V
        21: aconst_null
        22: invokedynamic #80,  0             // InvokeDynamic #1:cast:(Ljava/lang/Object;)Ljava/lang/Void;
        27: areturn
      LineNumberTable:
        line 17: 0
      LocalVariableTable:
        Start  Length  Slot  Name   Signature
            0      28     0  this   LBug$_main_closure1;
            0      28     1    it   Ljava/lang/Object;
    MethodParameters:
      Name                           Flags
      it
{code}",emilles,chrylis,Major,Resolved,Fixed,23/May/23 17:00,03/Jun/23 15:49
Bug,GROOVY-11071,13537463,Compiler error related to getAt and interface methods,"*Error message:*
{code:none}
BUG! exception in phase 'class generation' in source unit '...Reproducer.groovy' At line 11 column 21
On receiver: myMap with message: getAt and arguments: new Reproducer$Key()
This method should not have been called. {code}
*Simple example reproducing the error:*
{code:groovy}
import groovy.transform.CompileStatic
import org.junit.jupiter.api.Test

@CompileStatic
class Reproducer {

    @Test
    void reproduce() {
        MyMap myMap = null
        try {
            println(myMap[new Key()].toString())
        } catch (NullPointerException ignored) {
            // Expected
        }
    }

    static class Key {}

    static interface SomeMap<SELF extends SomeMap<SELF, K, V>, K, V> {
        V getAt(K key);
    }

    static interface MyMap extends SomeMap<MyMap, Key, Object> {}
}
 {code}",emilles,hussdl,Major,Resolved,Fixed,24/May/23 10:02,24/May/23 20:12
Bug,GROOVY-11072,13537759,Unable to compile: Expected type for closure parameter ,"This code is unable to compile, throwing the exception:
{code:java}
Main.groovy: 15: 22: [Static type checking] - Expected type java.util.List<T> for closure parameter: list
 @ line 22, column 17.
                   List<String> list ->
                   ^ 

Main.groovy: 17: [Static type checking] - Expected type java.lang.Object for closure parameter: tm
 @ line 17, column 21.
                       TestModel tm ->
                       ^

Main.groovy: 18: [Static type checking] - No such property: id for class: java.lang.Object
 @ line 18, column 33.
                           println(tm.id)
                                   ^{code}
Code:
{code:java}
@CompileStatic
class Main {
    static void main(String[] args) {
        TestTable table = new TestTable()
        table.getAll({
            List<TestModel> list ->
                list.each {
                    TestModel tm ->
                        println(tm.id)
                }
        })
    }
}{code}
This is a simplified version of my code which worked fine in 3.0.11, it failed first on 3.0.12 including 4.x. 

I have prepared a git project with the minimum code at:

[https://gitlab.com/intellisrc/groovy-compile-error2]

In _build.gradle_ you can change between 3.0.11 and 4.0.12 versions

 ",emilles,lepe,Major,Resolved,Fixed,26/May/23 07:06,26/May/23 22:42
Bug,GROOVY-11073,13537787,Cannot infer type of parameterized method when dealing with arrays,"I have the following code

{code}
class Main {
  static final void test() {
    double[] x = Tuple.tuple(org.codehaus.groovy.runtime.ArrayGroovyMethods.max((double[][]) null, { -> 1 })).getV1();
  }
}
{code}

h3. Actual behavior

{code}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
Main.groovy: 3: [Static type checking] - Cannot assign value of type (java.io.Serializable or java.lang.Cloneable) to variable of type double[]
 @ line 3, column 18.
       double[] x = Tuple.tuple(org.codehaus.groovy.runtime.ArrayGroovyMethods.max((double[][]) null, { -> 1 })).getV1();
                    ^

1 error
{code}

h3. Expected behavior

Compile successully

Tested against master (commit: 73c0f12ab35427bc3e7fd76929b482df61e1b80d)",emilles,theosot,Minor,Resolved,Fixed,26/May/23 09:56,31/May/23 16:15
Bug,GROOVY-11075,13538139,The method pointer operator suppresses checked exceptions and results in a MethodMissingException,"The method pointer operator suppresses checked exceptions and results in a MethodMissingException instead in groovy 4.0.12. This issue seems to first appear in groovy 3.0.16, which is the earliest version I am able to reproduce. Example code
{code:java}
class Foo {
    static void bar(String str) {
        throw new IOException()
        println str
    }
}
def baz = Foo.&bar

baz('hi') {code}
This results in a MethodMissingException despite the IO Exception being thrown. This seems to be the case for any checked exception.

 
{code:java}
class Foo {
    static void bar(String str) {
        throw new IOException()
        println str
    }
}

Foo.bar(""hi"") {code}
When calling the method directly I get an IO Exception as expected ",emilles,astrogg,Major,Resolved,Fixed,30/May/23 16:52,30/May/23 21:34
Bug,GROOVY-11076,13538550,@ClosureParams for StringGroovyMethods.replaceFirst and StringGroovyMethods.replaceAll wrong,"The {{@ClosureParams}} for {{StringGroovyMethods.replaceFirst}} and {{StringGroovyMethods.replaceAll}} seems to be wrong. It says {{​&#123;""List<String>"", ""String[]""&#125;​}}, so without static compilation IntelliJ shows a warning and with static compilation an error for the {{String it}} parts of
{code:groovy}
def pascalCasedWorkflowName = workflowName
    .replaceAll(/-\w/) { String it -> it[1].toUpperCase() }
    .replaceFirst(/^\w/) { String it -> it[0].toUpperCase() }
{code}
while it compiles and works perfectly fine. I'd guess it should be {{​&#123;""String"", ""List<String>"", ""String[]""&#125;​}}",emilles,vampire,Major,Resolved,Fixed,02/Jun/23 09:07,12/Jun/23 15:13
Bug,GROOVY-11079,13538650,GroovyCastException on closure implementing Consumer,"I'm having trouble making a minimal repro for this one, but here's the actual code. Libraries are PDFBox (PDDocument) and Vavr. The bytecode problem seems to be that although 31 pushes {{null}} onto the stack, it's popped before being cast to {{Void}}, leaving the {{String}} on top. Shouldn't these be reversed?

{code}
loadOriginal(ep.payload)
  .onSuccess { sps.envelope.filename = it._1 }  // exception thrown at the end of this closure
  .<PDDocument> map(Tuple2::_2)
{code}

{code}
org.codehaus.groovy.runtime.typehandling.GroovyCastException: Cannot cast object 'dummy.pdf' with class 'java.lang.String' to class 'java.lang.Void'
	at org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.continueCastOnSAM(DefaultTypeTransformation.java:424)
	at org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.continueCastOnNumber(DefaultTypeTransformation.java:335)
	at org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.castToType(DefaultTypeTransformation.java:255)
	at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
	at com.example.EsignFinisherImpl$_notifySignatureApplied_closure2$_closure11.doCall(EsignFinisherImpl.groovy:70)
{code}

{code}
  public java.lang.Void doCall(java.lang.Object);
    descriptor: (Ljava/lang/Object;)Ljava/lang/Void;
    flags: (0x0001) ACC_PUBLIC
    Code:
      stack=3, locals=3, args_size=2
         0: aload_1
         1: checkcast     #35                 // class io/vavr/Tuple2
         4: getfield      #38                 // Field io/vavr/Tuple2._1:Ljava/lang/Object;
         7: dup
         8: astore_2
         9: aload_0
        10: getfield      #27                 // Field sps:Lgroovy/lang/Reference;
        13: invokevirtual #44                 // Method groovy/lang/Reference.get:()Ljava/lang/Object;
        16: checkcast     #46                 // class SignaturePageSpec
        19: invokevirtual #50                 // Method SignaturePageSpec.getEnvelope:()LSignaturePageSpec$Envelope;
        22: aload_2
        23: invokedynamic #64,  0             // InvokeDynamic #0:cast:(Ljava/lang/Object;)Ljava/lang/String;
        28: invokevirtual #70                 // Method SignaturePageSpec$Envelope.setFilename:(Ljava/lang/String;)V
        31: aconst_null
        32: pop
        33: invokedynamic #72,  0             // InvokeDynamic #0:cast:(Ljava/lang/Object;)Ljava/lang/Void;
        38: areturn
{code}",emilles,chrylis,Major,Resolved,Fixed,02/Jun/23 21:00,09/Jun/23 19:44
Bug,GROOVY-11080,13538831,Static type checking regression with implicit generics of inline maps,"In 4.0.12, generic types of inline map definitions are no longer correctly inferred in situations where they would be under 4.0.11. For instance, the following:
{code:java}
@CompileStatic
class TypeCheckingTest {
    static final Map<String, Map<String, String>> testMap = [:]

    static test() {
        testMap.put(""test"", [:])
    }
}{code}
Fails to compile in 4.0.12, but compiles in 4.0.11. The error produced is:
{code:java}
10: [Static type checking] - Cannot find matching method java.util.Map#put(java.lang.String, java.util.LinkedHashMap<#K, #V>). Please check if the declared type is correct and if the method exists.
 @ line 10, column 9.
           testMap.put(""test"", [:])
           ^1 error {code}",emilles,lukebemish,Major,Resolved,Fixed,05/Jun/23 22:10,06/Jun/23 13:38
Bug,GROOVY-11083,13538960,STC: closure param's default not type-checked (method target),"Consider the following:
{code:groovy}
void setFoo(java.util.function.Consumer<Number> c) {}
@groovy.transform.TypeChecked
void test(Date d) {
  foo = { n = d -> }
}
{code}

The default value expression ""d"" is not compatible with the Number parameter ""n"".  There is no type-checking error for the call case.  Direct assignment is checked: ""java.util.function.Consumer<Number> c = \{ n = new Date() -> \}"" and ""def bar = \{ Number n = d -> \}""",emilles,emilles,Minor,Resolved,Fixed,06/Jun/23 18:36,07/Jun/23 14:12
Bug,GROOVY-11085,13539106,STC: assignment of closure with untyped parameter to SAM-type (setter target),"Consider the following:
{code:groovy}
void setStrategy(Predicate<Long> tester) {
  assert tester.test(1L)
}
strategy = { n -> n instanceof Long }
{code}

Compiler error: ""Incompatible generic argument types. Cannot assign Predicate<Object> to Predicate<Long>""",emilles,emilles,Minor,Resolved,Fixed,07/Jun/23 14:10,07/Jun/23 14:32
Bug,GROOVY-11088,13539300,Missing symbol for package-private member under joint compilation,"Consider the following:
{code:groovy}
class C {
  @PackageScope static final String X = ""x""
}
{code}

Reference to ""C.X"" from a java class in the same package produces error:
{code}
Main.java:4: error: cannot find symbol
                            System.err.println(C.X);
                                                ^
      symbol:   variable X
      location: class C
    1 error
{code}

This has to do with stubgen running on an early compile phase.  In this case, there are no members to generate, only modifiers to adjust so we should be able to sniff this out and handle it special.",emilles,emilles,Major,Resolved,Fixed,08/Jun/23 14:59,08/Jun/23 18:08
Bug,GROOVY-11089,13539336,STC: closure param type inference for array and list,"Consider the following:
{code:groovy}
@groovy.transform.TypeChecked
void test() {
  (new String[]{'a','b'}).with { a, b -> } // throws MissingMethodException
  ['a','b'].with { a, b -> } // param count error
}
test()
{code}

1. STC thinks it can spread array elements across closure parameters, but this does not work.
2. STC thinks it cannot spread list elements across closure parameters, which does work (if length check is lifted).",emilles,emilles,Major,Resolved,Fixed,08/Jun/23 21:10,12/Jun/23 15:14
Bug,GROOVY-11104,13540521,Static import of property accessor for type in same unit,"Consider the following:
{code:groovy}
import static Type.setP as store
class Type {
  static p
}
@groovy.transform.CompileStatic
void test() {
  store('') // not recognized if Type is in same unit; property methods not there yet
}
test()
{code}",emilles,emilles,Major,Resolved,Fixed,18/Jun/23 14:11,18/Jun/23 15:39
Bug,GROOVY-11107,13540830,EnumSet varargs constructor is not recognized (broken) in runtime,"EnumSet.of() has 6 overloaded variants, the one with varargs is throwing GroovyRuntimeException, claiming it can not find a proper constructor. But it is working for jdk 8 with any Groovy version: 2.4, 2.5, 3.0 and 4.0, broken for jdk 11 and 17 with 2.5 and 3.0 and working fine for groovy 4.0 with any jdk.

It is a runtime exception and the code needs to access the field:

 
{code:java}
package test
enum Test1 {
    ONE,
    TWO,
    THREE,
    FOUR,
    FIVE,
    SIX
    
    public static EnumSet<Test1> test1 = EnumSet.of(ONE, TWO, THREE, FOUR, FIVE, SIX)
    //public static EnumSet<Test1> test2 = EnumSet.of(ONE, TWO, THREE, FOUR) + EnumSet.of(FIVE, SIX) // workaround to not use varargs, works for any combinations
}
class Application {
    static void main(String[] args) {
        Test1.test1
   }
}
{code}
It was working fine in groovy 2.4 and all groovy version using jdk 8. But in groovy 2.5 and 3.0, using JDK 11 and 17 it is throwing an exception:
{code:java}
Exception in thread ""main"" java.lang.ExceptionInInitializerError
    at java.base/jdk.internal.misc.Unsafe.ensureClassInitialized0(Native Method)
    at java.base/jdk.internal.misc.Unsafe.ensureClassInitialized(Unsafe.java:1042)
    at java.base/jdk.internal.reflect.UnsafeFieldAccessorFactory.newFieldAccessor(UnsafeFieldAccessorFactory.java:43)
    at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:186)
    at java.base/java.lang.reflect.Field.acquireFieldAccessor(Field.java:1105)
    at java.base/java.lang.reflect.Field.getFieldAccessor(Field.java:1086)
    at java.base/java.lang.reflect.Field.get(Field.java:418)
    at org.codehaus.groovy.reflection.CachedField.getProperty(CachedField.java:70)
    at groovy.lang.MetaClassImpl.getProperty(MetaClassImpl.java:1890)
    at groovy.lang.MetaClassImpl.getProperty(MetaClassImpl.java:3827)
    at org.codehaus.groovy.runtime.callsite.ClassMetaClassGetPropertySite.getProperty(ClassMetaClassGetPropertySite.java:50)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callGetProperty(AbstractCallSite.java:329)
    at test.Application.main(Application.groovy:20)
Caused by: org.codehaus.groovy.runtime.typehandling.GroovyCastException: Cannot cast object '[FOUR, ONE, TWO, SIX, FIVE, THREE]' with class 'java.util.ImmutableCollections$SetN' to class 'java.util.EnumSet' due to: groovy.lang.GroovyRuntimeException: Could not find matching constructor for: java.util.EnumSet(test.Test1, test.Test1, test.Test1, test.Test1, test.Test1, test.Test1)
    at org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.continueCastOnSAM(DefaultTypeTransformation.java:402)
    at org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.continueCastOnNumber(DefaultTypeTransformation.java:315)
    at org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.continueCastOnCollection(DefaultTypeTransformation.java:273)
    at org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.castToType(DefaultTypeTransformation.java:230)
    at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.castToType(ScriptBytecodeAdapter.java:615)
    at test.Test1.<clinit>(Application.groovy:14)
    ... 13 more
    Suppressed: groovy.lang.GroovyRuntimeException: Could not find matching constructor for: java.util.EnumSet(SetN)
        at groovy.lang.MetaClassImpl.invokeConstructor(MetaClassImpl.java:1835)
        at groovy.lang.MetaClassImpl.invokeConstructor(MetaClassImpl.java:1605)
        at org.codehaus.groovy.runtime.InvokerHelper.invokeConstructorOf(InvokerHelper.java:1067)
        at org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.continueCastOnSAM(DefaultTypeTransformation.java:378)
        ... 18 more{code}
Although it is working fine in groovy 4 in all JDK: 8, 11 and 17.

 
||Groovy\JDK||JDK8||jdk-11.0.19+7||jdk-17.0.7+7||
|2.4.21|{color:#00875a}V{color}|N/A|N/A|
|2.5.22|{color:#00875a}V{color}|{color:#de350b}X{color}|{color:#de350b}X{color}|
|3.0.17|{color:#00875a}V{color}|{color:#de350b}X{color}|{color:#de350b}X{color}|
|4.0.12|{color:#00875a}V{color}|{color:#00875a}V{color}|{color:#00875a}V{color}|

Also, static compilation of enum is also fixing the problem. I am using CentOS 7 and AdoptNet(Temurin) Java, see versions of groovy and java in the table above.

 ",emilles,ntn.immortal,Major,Resolved,Fixed,21/Jun/23 01:16,26/Jun/23 17:39
Bug,GROOVY-11112,13541355,Calling SwingWorker publish() method aborts with groovy.lang.MissingMethodException,"Hello,

 

with Groovy 3.0.15 and Java *17* I get the following exception when I call a SwingWorker publish() method

 
{code:java}
groovy.lang.MissingMethodException: No signature of method: TestWorker2.publish() is applicable for argument types: (Integer) values: [0]
java.util.concurrent.ExecutionException: groovy.lang.MissingMethodException: No signature of method: TestWorker2.publish() is applicable for argument types: (Integer) values: [0]
    at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
    at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
    at java.desktop/javax.swing.SwingWorker.get(SwingWorker.java:613)
    at java_util_concurrent_Future$get.callCurrent(Unknown Source)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:171)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:176)
    at TestWorker2.done(TestWorker2.groovy:32)
    at java.desktop/javax.swing.SwingWorker$5.run(SwingWorker.java:750)
    at java.desktop/javax.swing.SwingWorker$DoSubmitAccumulativeRunnable.run(SwingWorker.java:848)
    at java.desktop/sun.swing.AccumulativeRunnable.run(AccumulativeRunnable.java:112)
    at java.desktop/javax.swing.SwingWorker$DoSubmitAccumulativeRunnable.actionPerformed(SwingWorker.java:858)
    at java.desktop/javax.swing.Timer.fireActionPerformed(Timer.java:311)
    at java.desktop/javax.swing.Timer$DoPostEvent.run(Timer.java:243)
    at java.desktop/java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:318)
    at java.desktop/java.awt.EventQueue.dispatchEventImpl(EventQueue.java:771)
    at java.desktop/java.awt.EventQueue$4.run(EventQueue.java:722)
    at java.desktop/java.awt.EventQueue$4.run(EventQueue.java:716)
    at java.base/java.security.AccessController.doPrivileged(AccessController.java:399)
    at java.base/java.security.ProtectionDomain$JavaSecurityAccessImpl.doIntersectionPrivilege(ProtectionDomain.java:86)
    at java.desktop/java.awt.EventQueue.dispatchEvent(EventQueue.java:741)
    at java.desktop/java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:203)
    at java.desktop/java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:124)
    at java.desktop/java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:113)
    at java.desktop/java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:109)
    at java.desktop/java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:101)
    at java.desktop/java.awt.EventDispatchThread.run(EventDispatchThread.java:90)
Caused by: groovy.lang.MissingMethodException: No signature of method: publish() is applicable for argument types: (Integer) values: [0]
    at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:72)
    at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:82)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:171)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:185)
    at TestWorker2.doInBackground(TestWorker2.groovy:16)
    at TestWorker2.doInBackground(TestWorker2.groovy)
    at java.desktop/javax.swing.SwingWorker$1.call(SwingWorker.java:304)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at java.desktop/javax.swing.SwingWorker.run(SwingWorker.java:343)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:833) {code}
 

 

The code example is the following:

TestWorker2Test.groovy

 
{code:java}
import java.awt.EventQueue

class TestWorker2Test {
	
	static void main(String[] args) {
		
		EventQueue.invokeLater(new Runnable() {
			public void run() {
				try {
					println ""TestWorkerTest2""
					TestWorker2 tw2 = new TestWorker2()
					tw2.execute()
				} catch (Exception e) {
					e.printStackTrace();
				}
			}
		})
		
	}
}

 {code}
 

TestWorker2.groovy
{code:java}
import javax.swing.SwingWorker


class TestWorker2 extends SwingWorker<Void, Integer> {
    
        @Override
        protected Void doInBackground() throws Exception {
            
            for(int i=0; i<10; i++) {
                publish(i)
            }
            return null
        }
        
        @Override
        protected void process(List<Integer> chunks) {
            // Get Info
            for (Integer number : chunks) {
                println""Execute 'process chunks' in TestWorker 2! Number:$number""
            }
        }
        
        @Override
        protected void done() {
            try {
                get()
                println ""Background thread has been processed.""
            } catch (Exception ex) {
                println ""Error!""
                println ex.getMessage()
                ex.printStackTrace()
            }
        }
        
} {code}
When I run the above code with Groovy 3.0.15 and Java *11* everything works fine, only after switching to Java *17* I get the above exception.

 

What has changed with Java 17 and how can I solve the problem? Thanks in advance!

 ",,dschum,Major,Closed,Fixed,26/Jun/23 07:16,27/Jun/23 13:51
Bug,FLINK-30545,13516269,The SchemaManager doesn't check 'NOT NULL' specification when committing AddColumn change,"Currently, table store doesn't support adding column with 'NOT NULL' specification, but it doesn't check this condition.",yzl,yzl,Major,Closed,Fixed,03/Jan/23 07:12,04/Jan/23 01:45
Bug,FLINK-30552,13516319,Pulsar connector shouldn't assert the BatchMessageId size.,"Pulsar will try to assert the batch message id size in {{MessageIdUtils}}, but the batch size is  determined by the producer in batch mode. So we can promise the size could be 1.

And the next message id calculation should be calculated by using BatchMessageId.",syhily,syhily,Major,Closed,Fixed,03/Jan/23 12:13,11/Jan/23 07:57
Bug,FLINK-30555,13516434,Hive cluster can not read oss/s3 tables,"FLINK-29964 add oss support for Hive, but only valid in the case of standalone Hive, the distributed Hive compute engine cannot access.
We should add more FileSystems.initialize to Hive connector",lzljs3620320,lzljs3620320,Major,Closed,Fixed,04/Jan/23 07:36,05/Jan/23 12:08
Bug,FLINK-30558,13516448,The metric 'numRestarts' reported in SchedulerBase will be overridden by metric 'fullRestarts',"The method SchedulerBase#registerJobMetrics register metrics 'numRestarts' and 'fullRestarts' with the same metric object, as discussed in FLINK-30246, that will result in the loss of the metric 'numRestarts'.
{code:java}
metrics.gauge(MetricNames.NUM_RESTARTS, numberOfRestarts); 
metrics.gauge(MetricNames.FULL_RESTARTS, numberOfRestarts);{code}
I have verified this problem via rest api /jobs/:jobid/metrics, and the response shows below, we can find that the metric 'numRestarts' is missing.
{noformat}
[{""id"":""numberOfFailedCheckpoints""},{""id"":""cancellingTime""},{""id"":""lastCheckpointSize""},{""id"":""totalNumberOfCheckpoints""},{""id"":""lastCheckpointExternalPath""},{""id"":""lastCheckpointRestoreTimestamp""},{""id"":""failingTime""},{""id"":""runningTime""},{""id"":""uptime""},{""id"":""restartingTime""},{""id"":""initializingTime""},{""id"":""numberOfInProgressCheckpoints""},{""id"":""downtime""},{""id"":""lastCheckpointProcessedData""},{""id"":""numberOfCompletedCheckpoints""},{""id"":""deployingTime""},{""id"":""lastCheckpointFullSize""},{""id"":""fullRestarts""},{""id"":""createdTime""},{""id"":""lastCheckpointDuration""},{""id"":""lastCheckpointPersistedData""}]{noformat}",xiasun,xiasun,Major,Closed,Fixed,04/Jan/23 10:14,05/Jan/23 06:59
Bug,FLINK-30559,13516450,May get wrong result for `if` expression if it's string data type,"Can be reproduced by the folowing code in `org.apache.flink.table.planner.runtime.batch.sql.CalcITCase`

 
{code:java}
checkResult(""SELECT if(b > 10, 'ua', c) from Table3"", data3) {code}
The actual result is [co, He, He, ...].

Seems it will only get the first two characters.

 ",luoyuxia,luoyuxia,Major,Resolved,Fixed,04/Jan/23 10:31,06/Jun/23 02:55
Bug,FLINK-30561,13516470,ChangelogStreamHandleReaderWithCache cause FileNotFoundException,"When a job with state changelog enabled continues to restart, the following exceptions may occur :
{code:java}
java.lang.RuntimeException: java.io.FileNotFoundException: /data1/hadoop/yarn/nm-local-dir/usercache/hadoop-rt/appcache/application_1671689962742_1333392/dstl-cache-file/dstl6215344559415829831.tmp (No such file or directory)
    at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321)
    at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:87)
    at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.hasNext(StateChangelogHandleStreamHandleReader.java:69)
    at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.readBackendHandle(ChangelogBackendRestoreOperation.java:107)
    at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:78)
    at org.apache.flink.state.changelog.ChangelogStateBackend.restore(ChangelogStateBackend.java:94)
    at org.apache.flink.state.changelog.AbstractChangelogStateBackend.createKeyedStateBackend(AbstractChangelogStateBackend.java:136)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:336)
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:353)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:165)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:265)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:726)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:702)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:669)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: /data1/hadoop/yarn/nm-local-dir/usercache/hadoop-rt/appcache/application_1671689962742_1333392/dstl-cache-file/dstl6215344559415829831.tmp (No such file or directory)
    at java.io.FileInputStream.open0(Native Method)
    at java.io.FileInputStream.open(FileInputStream.java:195)
    at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138)
    at org.apache.flink.changelog.fs.ChangelogStreamHandleReaderWithCache.openAndSeek(ChangelogStreamHandleReaderWithCache.java:158)
    at org.apache.flink.changelog.fs.ChangelogStreamHandleReaderWithCache.openAndSeek(ChangelogStreamHandleReaderWithCache.java:95)
    at org.apache.flink.changelog.fs.StateChangeIteratorImpl.read(StateChangeIteratorImpl.java:42)
    at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85)
    ... 21 more {code}
*Problem causes：*
 # *_ChangelogStreamHandleReaderWithCache_* use RefCountedFile manager local cache file. The reference count is incremented when the input stream is opened from the cache file, and decremented by one when the input stream is closed. So the input stream must be closed and only once.
 # _*StateChangelogHandleStreamHandleReader#getChanges()*_ may cause the input stream to be closed twice. This happens when changeIterator.read(tuple2.f0, tuple2.f1) throws an exception (for example, when the task is canceled for other reasons during the restore process) the current state change iterator will be closed twice.

{code:java}
private void advance() {
    while (!current.hasNext() && handleIterator.hasNext()) {
        try {
            current.close();
            Tuple2<StreamStateHandle, Long> tuple2 = handleIterator.next();
            LOG.debug(""read at {} from {}"", tuple2.f1, tuple2.f0);
            current = changeIterator.read(tuple2.f0, tuple2.f1);
        } catch (Exception e) {
            ExceptionUtils.rethrow(e);
        }
    }
}

@Override
public void close() throws Exception {
    current.close();
}{code}
So we should make sure current state change iterator only be closed once. I suggest to make the following changes to _*StateChangelogHandleStreamHandleReader*_ :
{code:java}
private boolean currentClosed = false;

private void advance() {
    while (!current.hasNext() && handleIterator.hasNext()) {
        try {
            current.close();
            currentClosed = true;

            Tuple2<StreamStateHandle, Long> tuple2 = handleIterator.next();
            LOG.debug(""read at {} from {}"", tuple2.f1, tuple2.f0);
            current = changeIterator.read(tuple2.f0, tuple2.f1);
            currentClosed = false;
        } catch (Exception e) {
            ExceptionUtils.rethrow(e);
        }
    }
}

@Override
public void close() throws Exception {
    if (!currentClosed) {
        current.close();
    }
}{code}
 

cc [~yuanmei] , [~roman] .",Feifan Wang,Feifan Wang,Major,Closed,Fixed,04/Jan/23 12:49,10/Feb/23 12:21
Bug,FLINK-30564,13516540,Select from a new table with Kafka LogStore crashes with UnknownTopicOrPartitionException,"Selecting from newly created table that uses Kafka as a Log Store creates a job that crash-loops with {{UnknownTopicOrPartitionException: This server does not host this topic-partition}} exception. This happens because neither {{CREATE TABLE}} nor {{SELECT FROM}} create the underlying topic. 

Steps to reproduce:
{noformat}
CREATE TABLE word_count (
    word STRING PRIMARY KEY NOT ENFORCED,
    cnt BIGINT
) WITH (
    'connector' = 'table-store',
    'path' = 's3://my-bucket/table-store',
    'log.system' = 'kafka',
    'kafka.bootstrap.servers' = 'broker:9092',
    'kafka.topic' = 'word_count_log',
    'auto-create' = 'true',
    'log.changelog-mode' = 'all',
    'log.consistency' = 'transactional'
);

SELECT * FROM word_count; {noformat}
 

JM logs:
{noformat}
flink          | 2023-01-04 23:27:24,292 ERROR org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext [] - Exception while handling result from async call in SourceCoordinator-Source: word_count[1]. Triggering job failover.
flink          | org.apache.flink.util.FlinkRuntimeException: Failed to list subscribed topic partitions due to
flink          |     at org.apache.flink.table.store.shaded.connector.kafka.source.enumerator.KafkaSourceEnumerator.checkPartitionChanges(KafkaSourceEnumerator.java:234) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
flink          |     at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$null$1(ExecutorNotifier.java:83) ~[flink-dist-1.16.0.jar:1.16.0]
flink          |     at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:40) [flink-dist-1.16.0.jar:1.16.0]
flink          |     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_352]
flink          |     at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_352]
flink          |     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_352]
flink          |     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_352]
flink          |     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_352]
flink          |     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_352]
flink          |     at java.lang.Thread.run(Thread.java:750) [?:1.8.0_352]
flink          | Caused by: java.lang.RuntimeException: Failed to get metadata for topics [word_count_log].
flink          |     at org.apache.flink.table.store.shaded.connector.kafka.source.enumerator.subscriber.KafkaSubscriberUtils.getTopicMetadata(KafkaSubscriberUtils.java:47) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
flink          |     at org.apache.flink.table.store.shaded.connector.kafka.source.enumerator.subscriber.TopicListSubscriber.getSubscribedTopicPartitions(TopicListSubscriber.java:52) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
flink          |     at org.apache.flink.table.store.shaded.connector.kafka.source.enumerator.KafkaSourceEnumerator.getSubscribedTopicPartitions(KafkaSourceEnumerator.java:219) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
flink          |     at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$notifyReadyAsync$2(ExecutorNotifier.java:80) ~[flink-dist-1.16.0.jar:1.16.0]
flink          |     ... 7 more
flink          | Caused by: java.util.concurrent.ExecutionException: org.apache.flink.table.store.shaded.org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. {noformat}",,Gerrrr,Major,Closed,Fixed,04/Jan/23 23:34,29/Mar/23 01:44
Bug,FLINK-30569,13516587,File Format can not change with data file exists,"# Set file format to orc
# Write records.
# Set file format to parquet.
# Write records
# Read -> throw exception...

We should support change file format.",lzljs3620320,lzljs3620320,Major,Closed,Fixed,05/Jan/23 06:56,06/Jan/23 01:44
Bug,FLINK-30573,13516624,Table Store dedicated compact job may skip some records when checkpoint interval is long,"Currently the sink for Table Store dedicated compact job only receives records about what buckets are changed, instead of what files are changed. If the writer is kept open, new files of this bucket may be skipped.",TsReaper,TsReaper,Major,Closed,Fixed,05/Jan/23 09:53,06/Jan/23 08:27
Bug,FLINK-30577,13516721,OpenShift FlinkSessionJob artifact write error on non-default namespaces,"[~tagarr] has pointed out an issue with using the /opt/flink/artifacts filesystem on OpenShift in non-default namespaces.  The OpenShift permissions don't allow write to /opt.  
```
org.apache.flink.util.FlinkRuntimeException: Failed to create the dir: /opt/flink/artifacts/jim/basic-session-deployment-only-example/basic-session-job-only-example
```
A few ways to solve the problem are:
1. Remove the comment on line 34 here in [flink-conf.yaml|https://github.com/apache/flink-kubernetes-operator/blob/main/helm/flink-kubernetes-operator/conf/flink-conf.yaml#L34] and change it to: /tmp/flink/artifacts

2. Append this after line 143 here in [values.yaml|https://github.com/apache/flink-kubernetes-operator/blob/main/helm/flink-kubernetes-operator/values.yaml#L142]:
kubernetes.operator.user.artifacts.base.dir: /tmp/flink/artifacts

3.  Changing it in line 142 of [KubernetesOperatorConfigOptions.java|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/KubernetesOperatorConfigOptions.java#L142] like this:
.defaultValue(""/tmp/flink/artifacts"") 
and then rebuilding the operator image.


",tagarr,jbusche,Major,Closed,Fixed,05/Jan/23 20:45,30/Jan/23 09:06
Bug,FLINK-30586,13517006,Fix calcCodeGen failed if calc with like condition contains double quotation mark,"If I write a sql like ""SELECT * FROM MyTable WHERE b LIKE '%""%'"" in Flink-1.16 as

'like' condition contains double quotation mark, it will cause code gen failed because wrong code generated by codeGen. 

!code-gen-1.png!

 

!code-gen-2.png!

 ",337361684@qq.com,337361684@qq.com,Major,Closed,Fixed,06/Jan/23 08:23,11/Jan/23 08:57
Bug,FLINK-30594,13517046,Update Java version because of JDK bug in the operator,"The following JDK bug is found during operator usage: https://bugs.openjdk.org/browse/JDK-8221218

This has been resolved in 11.0.18 which should be used in the operator base image.",gaborgsomogyi,gaborgsomogyi,Critical,Closed,Fixed,06/Jan/23 15:07,06/Feb/23 12:51
Bug,FLINK-30596,13517089,"Multiple POST /jars/:jarid/run requests with the same jobId, runs duplicate jobs","Analysis from [~trohrmann]:

{quote}
The problem is the following: When submitting a job, then the {{Dispatcher}} will wait for the termination of a previous {{JobMaster}}. This is done to enable the proper cleanup of the job resources. In the initial submission case, there is no previous {{JobMaster}} with the same {{jobId}}. The problem is now that Flink schedules the [{{persistAndRunJob}}|https://github.com/apache/flink/blob/5f924bc84227a3a6c67b44e82c45fe444393f577/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L571] action, which runs the newly submitted job, as [an asynchronous task|https://github.com/apache/flink/blob/5f924bc84227a3a6c67b44e82c45fe444393f577/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L1312-L1318]. This is done to ensure that the action is run on the {{Dispatcher}}'s main thread since the termination future can be run on a different thread. Due to this behaviour, there can be other tasks enqueued in the {{Dispatcher}}'s work queue which are executed before. Such a task could be another job submission which wouldn't see that there is already a job submitted with the same {{jobId}} since [we only do this in {{runJob}}|https://github.com/apache/flink/blob/5f924bc84227a3a6c67b44e82c45fe444393f577/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L602] which is called by {{persistAndRunJob}}. This is the reason why you don't see a duplicate job submission exception for the second job submission. Even worse, this will eventually [lead to an invalid state|https://github.com/apache/flink/blob/5f924bc84227a3a6c67b44e82c45fe444393f577/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L611-L615] and fail the whole cluster entrypoint.
{quote}

The following fix to the {{Dispatcher}} seems to fix the issue, but before submitting a PR, I wanted to post this for possible follow up discussions:

{code:language=java}
private CompletableFuture<Void> waitForTerminatingJob(
            JobID jobId, JobGraph jobGraph, ThrowingConsumer<JobGraph, ?> action) {
        ...
        return FutureUtils.thenAcceptAsyncIfNotDone(
                jobManagerTerminationFuture,
                getMainThreadExecutor(),
                FunctionUtils.uncheckedConsumer(
                    (ignored) -> {
                        jobManagerRunnerTerminationFutures.remove(jobId);
                        action.accept(jobGraph);
                    }));
    }
{code}",morezaei00,morezaei00,Critical,Closed,Fixed,06/Jan/23 21:57,03/Jul/23 09:31
Bug,FLINK-30598,13517158,Wrong code generated for WatermarkGenerator because of inconsistent source type info when deserialized from exec plan,"When compile from an exist exec plan which contains watermark declaration and it referred the metadata column, the generated code for WatermarkGenerator maybe wrong

because currently `DynamicTableSourceSpec`.getTableSource passes the user defined schema to `SourceAbilitySpec` to perform optimization like projection/watermark pushdown, while optimization path from sql use a fixed reorder form: ""PHYSICAL COLUMNS + METADATA COLUMNS"", this may cause the problem.

a repro-case:

{code}
@Test
    public void testWatermarkPushDownWithMetadata() throws Exception {
        // to verify FLINK-: the case declares metadata field first, without fix it will get a
        // wrong code generated by WatermarkGeneratorCodeGenerator which reference the incorrect
        // varchar column as the watermark field.
        createTestValuesSourceTable(
                ""MyTable"",
                JavaScalaConversionUtil.toJava(TestData.data3WithTimestamp()),
                new String[] {
                    ""ts timestamp(3) metadata"",
                    ""a int"",
                    ""b bigint"",
                    ""c varchar"",
                    ""watermark for ts as ts - interval '5' second""
                },
                new HashMap<String, String>() {
                    {
                        put(""enable-watermark-push-down"", ""true"");
                        put(""readable-metadata"", ""ts:timestamp(3)"");
                    }
                });

        File sinkPath =
                createTestCsvSinkTable(
                        ""MySink"", ""a int"", ""b bigint"", ""c varchar"", ""ts timestamp(3)"");

        compileSqlAndExecutePlan(""insert into MySink select a, b, c, ts from MyTable where b = 3"")
                .await();

        assertResult(
                Arrays.asList(
                        ""4,3,Hello world, how are you?,"" + toLocalDateTime(4000L),
                        ""5,3,I am fine.,"" + toLocalDateTime(5000L),
                        ""6,3,Luke Skywalker,"" + toLocalDateTime(6000L)),
                sinkPath);
    }
{code}

the wrong code snippet(`row.getString(3)` should be a TimestampData):
{code}
public Long currentWatermark(org.apache.flink.table.data.RowData row) throws Exception {
          
          org.apache.flink.table.data.binary.BinaryStringData field$19;
          boolean isNull$19;
          org.apache.flink.table.data.binary.BinaryStringData field$21;
          boolean isNull$22;
          org.apache.flink.table.data.TimestampData result$23;
          boolean isNull$24;
          org.apache.flink.table.data.TimestampData result$25;
          
          isNull$19 = row.isNullAt(3);
          field$19 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
          if (!isNull$19) {
            field$19 = ((org.apache.flink.table.data.binary.BinaryStringData) row.getString(3));
          }
{code}

 ",lincoln.86xy,lincoln.86xy,Major,Resolved,Fixed,08/Jan/23 14:21,06/Jul/23 09:45
Bug,FLINK-30603,13517189,CompactActionITCase in table store is unstable,"https://github.com/apache/flink-table-store/actions/runs/3927960511/jobs/6715071149

Error:  Failures: 
Error:    CompactActionITCase.testStreamingCompact:193 expected:<[+I 1|100|15|20221208, +I 1|100|15|20221209]> but was:<[+I 1|100|15|20221208]>
[INFO] 
Error:  Tests run: 221, Failures: 1, Errors: 0, Skipped: 4",TsReaper,zjureel,Major,Closed,Fixed,09/Jan/23 06:24,29/Mar/23 01:59
Bug,FLINK-30605,13517237,'Streaming File Sink end-to-end test'  fails with UnsupportedOperationException,"We have a test failure in {{Streaming File Sink end-to-end test}} being caused by an {{UnsupportedOperationException}} ({{{}StreamingFileSink{}}} is not supported as a type):
{code:java}
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Unsupported sink type: StreamingFileSink
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:98)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:851)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:245)
	at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1095)
	at org.apache.flink.client.cli.CliFrontend.lambda$mainInternal$9(CliFrontend.java:1189)
	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
	at org.apache.flink.client.cli.CliFrontend.mainInternal(CliFrontend.java:1189)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1157)
Caused by: java.lang.UnsupportedOperationException: Unsupported sink type: StreamingFileSink
	at org.apache.flink.connector.file.sink.FileSinkProgram.main(FileSinkProgram.java:88)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
	... 9 more {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44597&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=2154",,mapohl,Blocker,Resolved,Fixed,09/Jan/23 12:16,09/Jan/23 13:01
Bug,FLINK-30607,13517248,Table.to_pandas doesn't support Map type,"It seems that the Table#to_pandas method in PyFlink doesn't support Map type. It throws the following exception.
{code:java}
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame.
: java.lang.UnsupportedOperationException: Python vectorized UDF doesn't support logical type MAP<INT, INT> currently.
    at org.apache.flink.table.runtime.arrow.ArrowUtils$LogicalTypeToArrowTypeConverter.defaultMethod(ArrowUtils.java:743)
    at org.apache.flink.table.runtime.arrow.ArrowUtils$LogicalTypeToArrowTypeConverter.defaultMethod(ArrowUtils.java:617)
    at org.apache.flink.table.types.logical.utils.LogicalTypeDefaultVisitor.visit(LogicalTypeDefaultVisitor.java:167)
    at org.apache.flink.table.types.logical.MapType.accept(MapType.java:115)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.toArrowField(ArrowUtils.java:189)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.lambda$toArrowSchema$0(ArrowUtils.java:180)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.toArrowSchema(ArrowUtils.java:181)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame(ArrowUtils.java:483)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
    at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
    at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748) {code}
This can be reproduced with the following code.
{code:java}
env = StreamExecutionEnvironment.get_execution_environment()
t_env = StreamTableEnvironment.create(env)
table = t_env.from_descriptor(
    TableDescriptor.for_connector(""datagen"")
    .schema(
        Schema.new_builder()
        .column(""val"", DataTypes.MAP(DataTypes.INT(), DataTypes.INT()))
        .build()
    )
    .option(""number-of-rows"", ""10"")
    .build()
)
df = table.to_pandas()
print(df) {code}",dianfu,xuannan,Major,Closed,Fixed,09/Jan/23 14:18,29/Mar/23 04:15
Bug,FLINK-30611,13517369,Expire snapshot should be reentrant,"At present, if the file is incomplete, expire will throw an exception.
However, the snapshot in expire may be incomplete. It can be interrupted and killed suddenly.
Therefore, we should ensure the safety of expire, make it reentrant, and avoid throwing exceptions.",TsReaper,lzljs3620320,Blocker,Closed,Fixed,10/Jan/23 03:22,12/Jan/23 12:49
Bug,FLINK-30616,13517411,Don't support batchMessageId when restore from checkpoint,"I have a non-partition topic: 
 * the producer for the topic sends batch messages to the topic(to improve the speed of producers)
 * the flink job consumes this topic by Exclusive subscription type

When the flink task manager is restarted for some reason, an exception is thrown when restored from the checkpoint:
{code:java}
java.lang.RuntimeException: One or more fetchers have encountered the exception
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.pollNext(PulsarOrderedSourceReader.java:106) ~[?:?]
at org.apache.flink.streaming.api.operators.SourceOperator.emitNextNotReading(SourceOperator.java:403) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:387) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:542) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) ~[flink-dist-1.16.0.jar:1.16.0]
at java.lang.Thread.run(Unknown Source) ~[?:?]
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-files-1.16.0.jar:1.16.0]
at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]
at java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
... 1 more
Caused by: java.lang.IllegalArgumentException: We only support normal message id currently. This batch size is %d [83]
at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:160) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.connector.pulsar.source.enumerator.cursor.MessageIdUtils.unwrapMessageId(MessageIdUtils.java:65) ~[?:?]
at org.apache.flink.connector.pulsar.source.enumerator.cursor.MessageIdUtils.nextMessageId(MessageIdUtils.java:44) ~[?:?]
at org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.beforeCreatingConsumer(PulsarOrderedPartitionSplitReader.java:92) ~[?:?]
at org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.handleSplitsChanges(PulsarPartitionSplitReaderBase.java:171) ~[?:?]
at org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.handleSplitsChanges(PulsarOrderedPartitionSplitReader.java:51) ~[?:?]
at org.apache.flink.connector.base.source.reader.fetcher.AddSplitsTask.run(AddSplitsTask.java:51) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-files-1.16.0.jar:1.16.0]
at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]
at java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
... 1 more{code}
some important logs in the task manager:
{code:java}
2023-01-09 14:51:01,645 DEBUG org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(1/1) and restoring with state from alternative (1/1).2023-01-09 14:51:01,664 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [PulsarPartitionSplit{partition=persistent://ethereum-prod/raw/transactions|[0-65535]}]2023-01-09 14:51:01,740 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 02023-01-09 14:51:01,741 DEBUG org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Prepare to run AddSplitsTask: [[PulsarPartitionSplit{partition=persistent://ethereum-prod/raw/transactions|[0-65535]}]]2023-01-09 14:51:01,741 DEBUG org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Enqueued task AddSplitsTask: [[PulsarPartitionSplit{partition=persistent://ethereum-prod/raw/transactions|[0-65535]}]]2023-01-09 14:51:01,741 DEBUG org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Cleaned wakeup flag.2023-01-09 14:51:01,741 DEBUG org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Prepare to run AddSplitsTask: [[PulsarPartitionSplit{partition=persistent://ethereum-prod/raw/transactions|[0-65535]}]]2023-01-09 14:51:01,742 DEBUG org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase [] - Handle split changes SplitAddition:[[PulsarPartitionSplit{partition=persistent://ethereum-prod/raw/transactions|[0-65535]}]]2023-01-09 14:51:01,742 INFO  org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader [] - Reset subscription position by the checkpoint 25551:17912:-1:82023-01-09 14:51:01,743 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records {code}
I don't know if it is a feature or a bug, but this means that we can't restore from a batch message id checkpoint. I would like to know what to do better. [~Tison] 

 ",,songv,Major,Closed,Fixed,10/Jan/23 07:53,10/Jan/23 08:57
Bug,FLINK-30618,13517434,flink-connector-pulsar not retrievable from Apache's Snapshot Maven repository,"The build failure was caused by {{flink-connector-pulsar}} not being retrievable from the Apache Snapshot Maven repository:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44633&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=10132

{code}
Jan 10 02:03:24 [WARNING] The requested profile ""skip-webui-build"" could not be activated because it does not exist.
Jan 10 02:03:24 [ERROR] Failed to execute goal on project flink-python: Could not resolve dependencies for project org.apache.flink:flink-python:jar:1.17-SNAPSHOT: Could not find artifact org.apache.flink:flink-sql-connector-pulsar:jar:4.0-SNAPSHOT in apache.snapshots (https://repository.apache.org/snapshots) -> [Help 1]
Jan 10 02:03:24 [ERROR] 
Jan 10 02:03:24 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Jan 10 02:03:24 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
Jan 10 02:03:24 [ERROR] 
Jan 10 02:03:24 [ERROR] For more information about the errors and possible solutions, please read the following articles:
Jan 10 02:03:24 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
Jan 10 02:03:24 [ERROR] 
Jan 10 02:03:24 [ERROR] After correcting the problems, you can resume the build with the command
Jan 10 02:03:24 [ERROR]   mvn <goals> -rf :flink-python
{code}",martijnvisser,mapohl,Critical,Closed,Fixed,10/Jan/23 10:43,26/Jan/23 08:27
Bug,FLINK-30623,13517474,Performance regression in checkpointSingleInput.UNALIGNED on 04.01.2023,"Performance regression
checkpointSingleInput.UNALIGNED median=338.1445195 recent_median=67.6453005
checkpointSingleInput.UNALIGNED_1 median=213.230041 recent_median=39.830277
deployAllTasks.STREAMING median=168.533106 recent_median=159.8534395
stateBackends.MEMORY median=3229.0248875 recent_median=2985.782919
tupleKeyBy median=4155.684199 recent_median=3987.5812305

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=checkpointSingleInput.UNALIGNED&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=checkpointSingleInput.UNALIGNED_1&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=8&ben=deployAllTasks.STREAMING&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=6&ben=stateBackends.MEMORY&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=6&ben=tupleKeyBy&extr=on&quarts=on&equid=off&env=2&revs=200",fanrui,martijnvisser,Blocker,Closed,Fixed,10/Jan/23 17:02,31/Jan/23 14:09
Bug,FLINK-30624,13517476,Performance regression in stateBackends.FS / FS_ASYNC / MEMORY on 05.01.2023,"stateBackends.FS median=4147.01197 recent_median=3957.8419495
stateBackends.FS_ASYNC median=4148.8160595 recent_median=3973.418166
stateBackends.MEMORY median=4114.406091 recent_median=3935.8805775

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=stateBackends.FS&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=stateBackends.FS_ASYNC&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=stateBackends.MEMORY&extr=on&quarts=on&equid=off&env=2&revs=200

",wanglijie,martijnvisser,Critical,Closed,Fixed,10/Jan/23 17:05,01/Feb/23 06:13
Bug,FLINK-30628,13517528,Kerberos in HiveCatalog is not work,We should read kerberos keytab from catalog options and doAs for hive metastore client.,lzljs3620320,lzljs3620320,Major,Closed,Fixed,11/Jan/23 04:21,19/Mar/23 05:39
Bug,FLINK-30629,13517561,ClientHeartbeatTest.testJobRunningIfClientReportHeartbeat is unstable,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44690&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=10819

{code:java}
Jan 11 04:32:39 [ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 21.02 s <<< FAILURE! - in org.apache.flink.client.ClientHeartbeatTest
Jan 11 04:32:39 [ERROR] org.apache.flink.client.ClientHeartbeatTest.testJobRunningIfClientReportHeartbeat  Time elapsed: 9.157 s  <<< ERROR!
Jan 11 04:32:39 java.lang.IllegalStateException: MiniCluster is not yet running or has already been shut down.
Jan 11 04:32:39 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
Jan 11 04:32:39 	at org.apache.flink.runtime.minicluster.MiniCluster.getDispatcherGatewayFuture(MiniCluster.java:1044)
Jan 11 04:32:39 	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:917)
Jan 11 04:32:39 	at org.apache.flink.runtime.minicluster.MiniCluster.getJobStatus(MiniCluster.java:841)
Jan 11 04:32:39 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobStatus(MiniClusterJobClient.java:91)
Jan 11 04:32:39 	at org.apache.flink.client.ClientHeartbeatTest.testJobRunningIfClientReportHeartbeat(ClientHeartbeatTest.java:79)
{code}
",Jiangang,xtsong,Critical,Resolved,Fixed,11/Jan/23 08:33,27/Jun/23 07:43
Bug,FLINK-30637,13517615,"In linux-aarch64 environment, using “is” judgment to match the window type of overwindow have returned incorrect matching results","In  linux-arch64 environment, “window_type is OverWindow.ROW_UNBOUNDED_FOLLOWING” in  in the PandasBatchOverWindowAggregateFunctionOperation class of the pyflink source code has returned the wrong result.

For example, when window_type is 6, it represents the window type of ‘ROW_UNBOUNDED_FOLLOWING’, but “window_type is OverWindow.ROW_UNBOUNDED_FOLLOWING” return false because the memory address of window_type has changed. It will lead to the wrong type of window, such as row sliding window, so, the wrong input data of python udf have been assembled and wrong results of that have appeared.

 

Specifically, the pyflink unit testcase is ‘test_over_window_aggregate_function’ in ‘pyflink\table\tests\test_pandas_udaf.py’. It performance incorrectly when I execute it by pytest on linux-aarch64 system. I cut this unit use case to the following code and executed it in the flink standalone mode of aarch64 system, and got the same error result:

 
{code:java}
import unittest
from pyflink.table import DataTypes, TableEnvironment, EnvironmentSettings
from pyflink.table.udf import udaf, AggregateFunction


class MaxAdd(AggregateFunction, unittest.TestCase):

    def open(self, function_context):
        mg = function_context.get_metric_group()
        self.counter = mg.add_group(""key"", ""value"").counter(""my_counter"")
        self.counter_sum = 0

    def get_value(self, accumulator):
        # counter
        self.counter.inc(10)
        self.counter_sum += 10
        return accumulator[0]

    def create_accumulator(self):
        return []

    def accumulate(self, accumulator, *args):
        result = 0
        for arg in args:
            result += arg.max()
        accumulator.append(result)


@udaf(result_type=DataTypes.FLOAT(), func_type=""pandas"")
def mean_udaf(v):
    import logging
    logging.error(""debug"")
    logging.error(v)
    return v.mean()


t_env = TableEnvironment.create(
    EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build())
t_env.get_config().get_configuration().set_string(""parallelism.default"", ""2"")
t_env.get_config().get_configuration().set_string(
    ""python.fn-execution.bundle.size"", ""1"")

import datetime

t = t_env.from_elements(
    [
        (1, 2, 3, datetime.datetime(2018, 3, 11, 3, 10, 0, 0)),
        (1, 3, 1, datetime.datetime(2018, 3, 11, 3, 10, 0, 0)),
        (1, 8, 5, datetime.datetime(2018, 3, 11, 4, 20, 0, 0))
    ],
    DataTypes.ROW(
        [DataTypes.FIELD(""a"", DataTypes.TINYINT()),
         DataTypes.FIELD(""b"", DataTypes.SMALLINT()),
         DataTypes.FIELD(""c"", DataTypes.INT()),
         DataTypes.FIELD(""rowtime"", DataTypes.TIMESTAMP(3))]))

# sink
t_env.execute_sql(""""""
        CREATE TABLE mySink (
          c INT,
          d FLOAT 
        ) WITH (
          'connector' = 'print'
        )
    """""")

t_env.create_temporary_system_function(""mean_udaf"", mean_udaf)
t_env.register_function(""max_add"", udaf(MaxAdd(),
                                        result_type=DataTypes.INT(),
                                        func_type=""pandas""))
t_env.register_table(""T"", t)
t_env.execute_sql(""""""
        insert into mySink
        select
         max_add(b, c)
         over (PARTITION BY a ORDER BY rowtime
         ROWS BETWEEN UNBOUNDED preceding AND 0 FOLLOWING),
         mean_udaf(b)
         over (PARTITION BY a ORDER BY rowtime
         ROWS BETWEEN 1 PRECEDING AND UNBOUNDED FOLLOWING)
        from T
    """""").wait()
'''
assert_equals(actual,
              [""5,4.3333335"",
               ""13,5.5"",
               ""6,4.3333335""])
'''{code}
The expected results are [""5,4.3333335"", ""13,5.5"", ""6,4.3333335""], but actual results are List(5,2.0, 13,5.5, 4,2.5). For ‘mean_udaf’ and ‘OverWindow.UNBOUNDED FOLLOWING’ in the code, by adding the error log, I found that when window_type is 6 and 'OverWindow.ROW_UNBOUNDED_FOLLOWING' also represents 6, the following code from pyflink source code returned false.
{code:java}
// pyflink\fn_execution\operations.py (line 273)
elif window_type is OverWindow.ROW_UNBOUNDED_FOLLOWING:
    # row unbounded following window
    window_start = window.lower_boundary
    for j in range(input_cnt):
        start = max(j + window_start, 0)
        series_slices = [s.iloc[start: input_cnt] for s in input_series]
        result.append(func(series_slices)){code}
And it It finally chose row sliding window to assemble input data of mean_udaf:
{code:java}
// pyflink\fn_execution\operations.py (line 280) 
else:
    # row sliding window
    window_start = window.lower_boundary
    window_end = window.upper_boundary
    for j in range(input_cnt):
        start = max(j + window_start, 0)
        end = min(j + window_end + 1, input_cnt)
        series_slices = [s.iloc[start: end] for s in input_series]
        result.append(func(series_slices)){code}
Obviously, that's not right. The right choice will be made in x86 environment.

The reason is window_ type‘s memory address  is different from ‘OverWindow.ROW_ UNBOUNDED_ FOLLOWING’ in linux-aarch64 environment. On the contrary, they are the same in the linux-x86 environment. The reason why the memory address is different is unknown yet. But I observed that window_type comes from 'serialized_fn.windows':
{code:java}
def __init__(self, spec): 
super(PandasBatchOverWindowAggregateFunctionOperation, self).__init__(spec) 
self.windows = [window for window in self.spec.serialized_fn.windows]
{code}
Perhaps grpc, protobuf dependencies or serialization operations in the arrch environment have affected the memory address of the int variables, I'll explore the underlying reasons later.

 

Solution and suggestion:

Since the window selections need to compare the values of two integer variables(window_type, OverWindow.ROW_ UNBOUNDED_ FOLLOWING), I recommend replacing ‘is’ with ‘==’ at the window type matching. That can also prevents erroneous results caused by python small integer object pool failure which may also affects the memory address. And this modification has been verified to perform correctly on both x86 and aarch64 environments, either this unit test case or the case I cut.

 

 

 

 

 ",xinchen147,xinchen147,Major,Resolved,Fixed,11/Jan/23 12:55,29/Jan/23 01:54
Bug,FLINK-30640,13517755,Unstable test in CliClientITCase,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44743&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4]

 

The failed test can work normally in my local environment.",lsy,yzl,Major,Closed,Fixed,12/Jan/23 06:44,11/Mar/23 05:08
Bug,FLINK-30641,13517757,docs_404_check fail,"{code:java}
Cloning into 'flink-connector-rabbitmq'...
Note: switching to '325b6ba8d866bb02204736229aa54ade304119a3'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by switching back to a branch.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -c with the switch command. Example:

  git switch -c <new-branch-name>

Or undo this operation with:

  git switch -

Turn off this advice by setting config variable advice.detachedHead to false

Start building sites … 
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:45:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:46:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:53:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:54:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:62:21"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:63:21"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:103:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:104:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/datastream/dynamodb"": ""/home/vsts/work/1/s/docs/content/docs/connectors/datastream/overview.md:43:22"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/datastream/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/datastream/overview.md:44:34"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/datastream/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/datastream/overview.md:45:35"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/dynamodb"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/overview.md:70:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/overview.md:76:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/overview.md:82:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:45:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:46:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:53:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:54:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:62:21"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:63:21"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:103:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:104:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/datastream/dynamodb"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/datastream/overview.md:42:22"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/datastream/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/datastream/overview.md:43:34"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/datastream/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/datastream/overview.md:44:35"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/dynamodb"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/overview.md:70:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/overview.md:76:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/overview.md:82:20"": page not found
Total in 16618 ms
Error: Error building site: logged 28 error(s)
Error building the docs
##[error]Bash exited with code '1'. {code}",dannycranmer,wanglijie,Blocker,Resolved,Fixed,12/Jan/23 07:12,12/Jan/23 10:38
Bug,FLINK-30646,13517803,Table Store Hive catalog throws ClassNotFoundException when custom hive-site.xml is presented,"For Hive 2.3.9, if a custom {{hive-site.xml}} is presented in {{$HIVE_HOME/conf}}, when creating Table Store Hive catalog in Flink, the following exception will be thrown.

{code}
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2273) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2367) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2393) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.loadFilterHooks(HiveMetaStoreClient.java:250) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:145) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_352]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_352]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_352]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_352]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.hive.HiveCatalog.createClient(HiveCatalog.java:415) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.hive.HiveCatalog.<init>(HiveCatalog.java:82) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.hive.HiveCatalogFactory.create(HiveCatalogFactory.java:51) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.file.catalog.CatalogFactory.createCatalog(CatalogFactory.java:106) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:66) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:57) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:31) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:435) ~[flink-table-api-java-uber-1.16.0.jar:1.16.0]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1426) ~[flink-table-api-java-uber-1.16.0.jar:1.16.0]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1172) ~[flink-table-api-java-uber-1.16.0.jar:1.16.0]
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:206) ~[flink-sql-client-1.16.0.jar:1.16.0]
	... 10 more
{code}

This is because {{hive-default.xml.template}} contains a property named {{hive.metastore.filter.hook}}. Its default value is {{org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl}}. However all Hive packages in Table Store are shaded, so the class loader cannot find the original class.

we need to remove relocation of Hive classes.",TsReaper,TsReaper,Major,Closed,Fixed,12/Jan/23 10:35,13/Jan/23 02:15
Bug,FLINK-30657,13517938,Remove Shared and Key_Shared related tests in Pulsar connector,"As the [FLINK-30413|https://issues.apache.org/jira/browse/FLINK-30413] issue talked, we have dropped the Shared and Key_Shared supported in upcoming flink-connector-pulsar 4.0 release. The flaky tests of Shared and Key_Shared still matters the old Flink build.

Cause these tests are useless now, we can just disable them without any fix.",syhily,syhily,Critical,Resolved,Fixed,13/Jan/23 01:07,31/Jan/23 10:51
Bug,FLINK-30669,13517966,Update recent job status in FlinkDeployment resource object.,"User jar has code as  -
{code:java}
main() {
 init env
 pipelines.foreach{
  env.fromSource(pipeline.getSource())
     .map(pipeline.transform())
     .sinkTo(pipeline.getSink())
  env.execute(pipeline.getName())
 }
}{code}
and below configuration -
{code:java}
execution.runtime-mode: ""BATCH""
execution.attached: ""true""
$internal.pipeline.job-id: """" {code}
When this single jar executed in Application Mode by using flink-kubernetes-operator, multiple jobs are submitted sequentially and as per design only one of the JobStatus is always associated with FlinkDeployment k8s resource, this job status is periodically updated by operator. To update job status in k8s resource, it fetches all of the job status from job-manager rest endpoint and pick the first one and update that one. Problem is, job status list returned by job-manager rest api is not sorted on time.

!image-2023-01-13-10-04-32-891.png|width=883,height=489!
!image-2023-01-13-09-54-54-280.png|width=353,height=284!

As you can see in above example, job autoscaling-3 is first one in the rest response and same updated in FlinkDeployment resource, but FlinkDeployment should have status of job autoscaling-19 because that is the last job finished.",,kmozaid,Major,Closed,Fixed,13/Jan/23 04:26,31/Jan/23 08:24
Bug,FLINK-30677,13518319,SqlGatewayServiceStatementITCase.testFlinkSqlStatements fails,"We're observing a test instability with {{SqlGatewayServiceStatementITCase.testFlinkSqlStatements}} in the following builds:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44775&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=14251]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44775&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=14608
{code:java}
Jan 13 02:46:10 [ERROR] Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 27.279 s <<< FAILURE! - in org.apache.flink.table.gateway.service.SqlGatewayServiceStatementITCase
Jan 13 02:46:10 [ERROR] org.apache.flink.table.gateway.service.SqlGatewayServiceStatementITCase.testFlinkSqlStatements(String)[5]  Time elapsed: 1.573 s  <<< FAILURE!
Jan 13 02:46:10 org.opentest4j.AssertionFailedError: 
Jan 13 02:46:10 
Jan 13 02:46:10 expected: 
Jan 13 02:46:10   ""# table.q - CREATE/DROP/SHOW/ALTER/DESCRIBE TABLE
Jan 13 02:46:10   #
Jan 13 02:46:10   # Licensed to the Apache Software Foundation (ASF) under one or more
Jan 13 02:46:10   # contributor license agreements.  See the NOTICE file distributed with
Jan 13 02:46:10   # this work for additional information regarding copyright ownership.
Jan 13 02:46:10   # The ASF licenses this file to you under the Apache License, Version 2.0
Jan 13 02:46:10   # (the ""License""); you may not use this file except in compliance with
Jan 13 02:46:10   # the License.  You may obtain a copy of the License at
Jan 13 02:46:10   #
Jan 13 02:46:10   # http://www.apache.org/licenses/LICENSE-2.0
Jan 13 02:46:10   #
Jan 13 02:46:10   # Unless required by applicable law or agreed to in writing, software
Jan 13 02:46:10   # distributed under the License is distributed on an ""AS IS"" BASIS,
Jan 13 02:46:10   # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
Jan 13 02:46:10   # See the License for the specific language governing permissions and
Jan 13 02:46:10   # limitations under the License.
[...] {code}",Paul Lin,mapohl,Critical,Closed,Fixed,13/Jan/23 10:50,19/Jan/23 13:42
Bug,FLINK-30679,13518324,Can not load the data of hive dim table when project-push-down is introduced,"维表project push down优化引入：
https://issues.apache.org/jira/browse/FLINK-29138

hive维表的两个问题：
https://issues.apache.org/jira/browse/FLINK-29992
https://issues.apache.org/jira/browse/FLINK-30679

 

 

 

Can not load the data of hive dim table when project-push-down is introduced.

 

hive-exec  version: 2.3.4

flink version: 1.14.6

flink-hive-connector: the latest code for release-1.14 branch

 

vectorize read:

 
{code:java}
java.lang.ArrayIndexOutOfBoundsException: 3
    at org.apache.flink.connectors.hive.read.HiveTableInputFormat.useOrcVectorizedRead(HiveTableInputFormat.java:276) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.connectors.hive.read.HiveTableInputFormat.open(HiveTableInputFormat.java:129) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader.open(HiveInputFormatPartitionReader.java:86) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.table.filesystem.FileSystemLookupFunction.checkCacheReload(FileSystemLookupFunction.java:132) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6]
    at org.apache.flink.table.filesystem.FileSystemLookupFunction.eval(FileSystemLookupFunction.java:105) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6]
    at LookupFunction$26.flatMap(Unknown Source) ~[?:?] {code}
 

 

mapreduce read:

 
{code:java}
java.lang.ArrayIndexOutOfBoundsException: 3
    at org.apache.flink.connectors.hive.read.HiveMapredSplitReader.lambda$new$0(HiveMapredSplitReader.java:139) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250) ~[?:1.8.0_301]
    at java.util.Spliterators$IntArraySpliterator.forEachRemaining(Spliterators.java:1032) ~[?:1.8.0_301]
    at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693) ~[?:1.8.0_301]
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_301]
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_301]
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546) ~[?:1.8.0_301]
    at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260) ~[?:1.8.0_301]
    at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:438) ~[?:1.8.0_301]
    at org.apache.flink.connectors.hive.read.HiveMapredSplitReader.<init>(HiveMapredSplitReader.java:141) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.connectors.hive.read.HiveTableInputFormat.open(HiveTableInputFormat.java:157) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader.open(HiveInputFormatPartitionReader.java:86) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.table.filesystem.FileSystemLookupFunction.checkCacheReload(FileSystemLookupFunction.java:132) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6]
    at org.apache.flink.table.filesystem.FileSystemLookupFunction.eval(FileSystemLookupFunction.java:105) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6]
    at LookupFunction$26.flatMap(Unknown Source) ~[?:?]
    at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:81) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6]
    at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:34) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6] {code}
 

 

The sql :

 
{code:java}
CREATE TABLE kafkaTableSource (
    name string,
    age int,
    sex string,
    address string,
    ptime AS PROCTIME()
) WITH (
    'connector' = 'kafka',
    'topic' = 'hehuiyuan1',
    'scan.startup.mode' = 'latest-offset',
    'properties.bootstrap.servers' = 'localhost:9092',
    'properties.client.id' = 'test-consumer-group',
    'properties.group.id' = 'test-consumer-group',
    'format' = 'csv'
);

CREATE TABLE printsink (
    name string,
    age int,
    sex string,
    address string,
    score bigint,
    dt string
) WITH (
    'connector' = 'print'
);

CREATE CATALOG myhive
WITH (
        'type' = 'hive',
        'default-database' = 'hhy',
        'hive-version' = '2.0.0',
        'hadoop-conf-dir'='/Users/hehuiyuan/soft/hadoop/hadoop-2.7.3/etc/hadoop'
);

USE CATALOG myhive;
USE hhy;

set table.sql-dialect=hive;
CREATE TABLE IF NOT EXISTS tmp_flink_test_text (
    name STRING,
    age INT,
    score BIGINT
) PARTITIONED BY (dt STRING) STORED AS TEXTFILE TBLPROPERTIES (
    'streaming-source.enable' = 'false',
    'streaming-source.partition.include' = 'all',
    'lookup.join.cache.ttl' = '5 min'
);
set table.sql-dialect=default;

USE CATALOG default_catalog;
INSERT INTO default_catalog.default_database.printsink
SELECT s.name, s.age, s.sex, s.address, r.score, r.dt
FROM default_catalog.default_database.kafkaTableSource  as s
JOIN myhive.hhy.tmp_flink_test_text FOR SYSTEM_TIME AS OF s.ptime  AS r
ON r.name = s.name;
 {code}
 

 ",hehuiyuan,hehuiyuan,Critical,Closed,Fixed,13/Jan/23 11:58,14/Feb/23 10:43
Bug,FLINK-30710,13519847,Fix invalid field id for nested type in spark catalog,"Current user can create table by spark sql, but the field id will start from 0 for nested type which causes exception",zjureel,zjureel,Major,Closed,Fixed,17/Jan/23 05:46,17/Jan/23 07:52
Bug,FLINK-30727,13519992,JoinReorderITCase.testBushyTreeJoinReorder failed due to IOException,"IOException due to timeout occurring while requesting exclusive NetworkBuffer caused JoinReorderITCase.testBushyTreeJoinReorder to fail:
{code}
[...]
Jan 18 01:11:27 Caused by: java.io.IOException: Timeout triggered when requesting exclusive buffers: The total number of network buffers is currently set to 2048 of 32768 bytes each. You can increase this number by setting the configuration keys 'taskmanager.memory.network.fraction', 'taskmanager.memory.network.min', and 'taskmanager.memory.network.max',  or you may increase the timeout which is 30000ms by setting the key 'taskmanager.network.memory.exclusive-buffers-request-timeout-ms'.
Jan 18 01:11:27 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.internalRequestMemorySegments(NetworkBufferPool.java:256)
Jan 18 01:11:27 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.requestPooledMemorySegmentsBlocking(NetworkBufferPool.java:179)
Jan 18 01:11:27 	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.reserveSegments(LocalBufferPool.java:262)
Jan 18 01:11:27 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setupChannels(SingleInputGate.java:517)
Jan 18 01:11:27 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setup(SingleInputGate.java:277)
Jan 18 01:11:27 	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.setup(InputGateWithMetrics.java:105)
Jan 18 01:11:27 	at org.apache.flink.runtime.taskmanager.Task.setupPartitionsAndGates(Task.java:962)
Jan 18 01:11:27 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:648)
Jan 18 01:11:27 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:556)
Jan 18 01:11:27 	at java.lang.Thread.run(Thread.java:748)
{code}
Same build, 2 failures:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44987&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=14300
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44987&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=14362",337361684@qq.com,mapohl,Critical,Resolved,Fixed,18/Jan/23 07:25,06/Feb/23 09:03
Bug,FLINK-30730,13520000,StringIndexer cannot handle null values correctly,"When training data contains null values, StringIndexer throws a exception. The reason is this method [1]: null values are neither String type nor Number type.

In StringIndexerModel, null values are also not handled correctly when performing transformation.

 

[1] [https://github.com/apache/flink-ml/blob/966cedd7bbab4e12d8d8b37dbd582146714e68a6/flink-ml-lib/src/main/java/org/apache/flink/ml/feature/stringindexer/StringIndexer.java#L164]",hongfanxo,hongfanxo,Major,Resolved,Fixed,18/Jan/23 07:58,08/Feb/23 02:39
Bug,FLINK-30740,13520061,INSERT to Kafka does not work when Kafka config auto.create.topics.enabled is set to false,"If I use Kafka as the log system and set {{auto.create.topics.enabled}} to false in Kafka INSERTs do not work.

Steps to reproduce:

# Start a Kafka broker and set {{auto.create.topics.enabled}} to false
# Issue the following statements
{code:sql}
CREATE CATALOG table_store_catalog WITH (
   'type'='table-store',
   'warehouse'=<path to object store>
);

USE CATALOG table_store_catalog;

CREATE TABLE word_count (
      word STRING PRIMARY KEY NOT ENFORCED,
      cnt BIGINT
 ) WITH (
   'log.system' = 'kafka',
   'kafka.bootstrap.servers' = <address to broker>,
   'kafka.topic' = 'test-topic,
   'log.consistency' = 'eventual'
 );
 
 INSERT INTO word_count VALUES ('foo', 1);
{code} 

The task manager logs show:
{code}
flink-sandbox-taskmanager-1  | 2023-01-18 12:46:17,085 WARN  org.apache.flink.table.store.shaded.org.apache.kafka.clients.NetworkClient [] - [Producer clientId=producer-1] Error while fetching metadata with correlation id 544 : {test-topic=UNKNOWN_TOPIC_OR_PARTITION}
{code}

The INSERT job on the task manager fails with
{code:java}
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.lang.NullPointerException
	at org.apache.flink.table.store.kafka.KafkaSinkFunction.lambda$open$0(KafkaSinkFunction.java:75)
	at org.apache.flink.table.store.shaded.org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:982)
	at org.apache.flink.table.store.shaded.org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:885)
	at org.apache.flink.table.store.shaded.streaming.connectors.kafka.internals.FlinkKafkaInternalProducer.send(FlinkKafkaInternalProducer.java:142)
	at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:926)
	at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:101)
	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:245)
	at org.apache.flink.table.store.connector.sink.StoreWriteOperator.processElement(StoreWriteOperator.java:134)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.lang.Thread.run(Thread.java:750)
	Suppressed: org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaException: Failed to send data to Kafka: Pending record count must be zero at this point: 1
		at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1428)
		at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:976)
		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
		at org.apache.flink.table.store.connector.sink.StoreWriteOperator.close(StoreWriteOperator.java:166)
		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:163)
		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:125)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:997)
		at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
		at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
		at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:916)
		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:930)
		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:930)
		... 3 more
	Caused by: java.lang.IllegalStateException: Pending record count must be zero at this point: 1
		at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.flush(FlinkKafkaProducer.java:1111)
		at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:936)
		... 15 more
{code}

Apparently, the Kafka topic is created when the first record is written to the Kafka topic, although I found code to create a Kafka topic explicitly on table creation:
https://github.com/apache/flink-table-store/blob/f201b507fef88501c4beb4c62807bef818e31be5/flink-table-store-kafka/src/main/java/org/apache/flink/table/store/kafka/KafkaLogStoreFactory.java#L123

Topic creation should not rely on enabling auto topic creation in Kafka, because users might opt to disable auto topic creation to prevent unexpected costs when a fully-managed Kafka service is used. For example see the Confluent Cloud documentation:
https://docs.confluent.io/cloud/current/clusters/broker-config.html#enable-automatic-topic-creation  

IMO, when a table is created, the corresponding Kafka topic should be explicitly created.",,cadonna,Major,Closed,Fixed,18/Jan/23 13:21,29/Mar/23 01:55
Bug,FLINK-30749,13520088,Delegation token provider enabled flag documentation is wrong,,gaborgsomogyi,gaborgsomogyi,Major,Closed,Fixed,18/Jan/23 15:25,19/Jan/23 15:26
Bug,FLINK-30750,13520135,CompactActionITCase.testBatchCompact in table store is unstable,"https://github.com/apache/flink-table-store/actions/runs/3954989166/jobs/6772877033


2023-01-17T11:45:17.9511390Z [INFO] Results:
2023-01-17T11:45:17.9511641Z [INFO] 
2023-01-17T11:45:17.9511838Z [ERROR] Errors: 
2023-01-17T11:45:17.9512585Z [ERROR]   CompactActionITCase.testBatchCompact » JobExecution Job execution failed.
2023-01-17T11:45:17.9512964Z [INFO] 
2023-01-17T11:45:17.9513223Z [ERROR] Tests run: 224, Failures: 0, Errors: 1, Skipped: 4




Besides above error, there's another exception as followed

https://github.com/apache/flink-table-store/actions/runs/3964547232/jobs/6793496230
Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout
	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$assignResource$4(DefaultExecutionDeployer.java:227)
	... 37 more
Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	... 35 more
Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout
	at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerImpl.lambda$schedulePendingRequestBulkWithTimestampCheck$0(PhysicalSlotRequestBulkCheckerImpl.java:86)
	... 28 more
Caused by: java.util.concurrent.TimeoutException: Timeout has occurred: 300000 ms
	... 29 more

",yzl,zjureel,Major,Closed,Fixed,19/Jan/23 03:29,02/Feb/23 03:22
Bug,FLINK-30751,13520142,Remove references to disableDataSync in RocksDB documentation,"The EmbeddedRocksDBStateBackend allows configuration using some predefined options via the .setPredefinedOptions method. The documentation for PredefinedOptions ([link|https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/contrib/streaming/state/PredefinedOptions.html]) mentions disableDataSync is called for {{FLASH_SSD_OPTIMIZED}} and {{{}SPINNING_DISK_OPTIMIZED{}}}.

 

But this option was removed several years ago in RocksDB 5.3.0 ([link|https://github.com/facebook/rocksdb/blob/main/HISTORY.md#530-2017-03-08]), and according to the code [PredefinedOptions.java|https://github.com/apache/flink/blob/0bbc7b1e9fed89b8c3e8ec67b7b0dad5999c2c01/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/PredefinedOptions.java#L72], it is no longer actually set in Flink.

We should remove references to disableDataSync in PredefinedOptions.java, and in state_backend.py, so that it does not appear in the documentation.",dchristle,dchristle,Minor,Resolved,Fixed,19/Jan/23 05:53,05/Jun/23 07:08
Bug,FLINK-30754,13520168,Fix ExceptionThrowingDelegationTokenProvider/Receiver multi-threaded test issues,,gaborgsomogyi,gaborgsomogyi,Major,Closed,Fixed,19/Jan/23 09:50,01/Feb/23 13:45
Bug,FLINK-30785,13521245,RocksDB Memory Management end-to-end test failed due to unexpected exception,"We see a test instability with {{RocksDB Memory Management end-to-end test}}. The test failed because an exception was detected in the logs:
{code}
2023-01-25T02:47:38.7172354Z Jan 25 02:47:38 Checking for errors...
2023-01-25T02:47:39.1661969Z Jan 25 02:47:39 No errors in log files.
2023-01-25T02:47:39.1662430Z Jan 25 02:47:39 Checking for exceptions...
2023-01-25T02:47:39.2893767Z Jan 25 02:47:39 Found exception in log files; printing first 500 lines; see full logs for details:
[...]
2023-01-25T02:47:39.5674568Z Jan 25 02:47:39 Checking for non-empty .out files...
2023-01-25T02:47:39.5675055Z Jan 25 02:47:39 No non-empty .out files.
2023-01-25T02:47:39.5675352Z Jan 25 02:47:39 
2023-01-25T02:47:39.5676104Z Jan 25 02:47:39 [FAIL] 'RocksDB Memory Management end-to-end test' failed after 1 minutes and 50 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files
{code}

The only exception being reported in the Flink logs is due to a warning:
{code}
2023-01-25 02:47:38,242 WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 1 for job 421e4c00ef175b3b133d63cbfe9bca8b. (0 consecutive failed attempts so far)
org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint Coordinator is suspending.
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.stopCheckpointScheduler(CheckpointCoordinator.java:1970) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorDeActivator.jobStatusChanges(CheckpointCoordinatorDeActivator.java:46) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifyJobStatusChange(DefaultExecutionGraph.java:1578) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.transitionState(DefaultExecutionGraph.java:1173) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.transitionState(DefaultExecutionGraph.java:1145) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.cancel(DefaultExecutionGraph.java:973) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SchedulerBase.cancel(SchedulerBase.java:671) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:461) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_352]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_352]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_352]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_352]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_352]
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45185&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5117",roman,mapohl,Critical,Closed,Fixed,25/Jan/23 09:09,10/Feb/23 09:40
Bug,FLINK-30787,13521356,dmesg fails to save data to file due to permissions,"We're not collecting the {{dmesg}} output due to a permission issue in any build:
{code}
2023-01-12T10:10:25.1598207Z dmesg: read kernel buffer failed: Operation not permitted
{code}",,mapohl,Major,Resolved,Fixed,25/Jan/23 11:39,30/Jan/23 11:51
Bug,FLINK-30791,13521536,Codespeed machine is not responding,"Neither speedcenter: [http://codespeed.dak8s.net:8000/]

nor jenkins: [http://codespeed.dak8s.net:8080|http://codespeed.dak8s.net:8080/]

are responding. Both services were hosted on the same EC2 machine from Ververica's AWS account.",jingge,pnowojski,Blocker,Closed,Fixed,25/Jan/23 16:43,25/Jan/23 20:06
Bug,FLINK-30803,13521780,PyFlink mishandles script dependencies,"h2. Summary

Since Flink 1.15, PyFlink is unable to run scripts that import scripts under other directories. For instance, if _main.py_ imports {_}job/word_count.py{_}, PyFlink will fail due to not finding the _job_ directory.

The issue seems to have started after a [refactoring of _PythonDriver_|https://github.com/apache/flink/commit/330aae0c6e0811f50888d17830f10f7a29efe7d7] to address FLINK-26847. The path to the Python script is removed, which forces PyFlink to use the copy in its temporary directory. When files are copied to this directory, the original directory structure is not maintained and ends up breaking the imports.
h2. Testing

To confirm the regression, I ran the attached application in both Flink 1.14.6 and 1.15.3 clusters.
h3. Flink 1.14.6

Application was able to start after being submitted via CLI:

 
{code:java}
% ./bin/flink run --python ~/sandbox/word_count_split/main.py
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/.../flink-1.14.6/lib/flink-dist_2.12-1.14.6.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Job has been submitted with JobID 6f7be21072384ca3a314af10860c4ba8 {code}
 
h3. Flink 1.15.3

Application did not start due to not finding the _job_ directory:

 
{code:java}
% ./bin/flink run --python ~/sandbox/word_count_split/main.py
Traceback (most recent call last):
  File ""/usr/lib64/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib64/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/tmp/pyflink/40c649c3-24af-46ef-ae27-e0019cb55769/3673dd18-adff-40e0-bb11-06a3f00ba29c/main.py"", line 5, in <module>
    from job.word_count import word_count
ModuleNotFoundError: No module named 'job'
org.apache.flink.client.program.ProgramAbortException: java.lang.RuntimeException: Python process exits with code: 1
        at org.apache.flink.client.python.PythonDriver.main(PythonDriver.java:140)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:841)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:240)
        at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1085)
        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1163)
        at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1163)
Caused by: java.lang.RuntimeException: Python process exits with code: 1
        at org.apache.flink.client.python.PythonDriver.main(PythonDriver.java:130)
        ... 13 more {code}
 

 ",dianfu,nuafonso,Major,Closed,Fixed,27/Jan/23 15:04,02/Feb/23 06:13
Bug,FLINK-30808,13521831,MultipleInputITCase failed with AdaptiveBatch Scheduler,"MultipleInputITCase#testRelatedInputs failed with AdaptiveBatch Scheduler.
{code:java}
java.lang.UnsupportedOperationException: Forward partitioning does not allow change of parallelism. Upstream operation: Calc[10]-14 parallelism: 1, downstream operation: HashJoin[15]-20 parallelism: 3 You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global. {code}",JunRuiLi,JunRuiLi,Major,Closed,Fixed,28/Jan/23 03:43,29/Jan/23 08:09
Bug,FLINK-30811,13521842,Fix sql gateway can not stop job correctly,,qingyue,fsk119,Major,Closed,Fixed,28/Jan/23 08:01,07/Feb/23 02:32
Bug,FLINK-30814,13521895,The parallelism&maxParallelism of sort after a global partitioning is not forced to be 1,"The parallelism&maxParallelism of sort after a global partitioning is not forced to be 1. The may lead to the parallelism to be changed by adaptive batch scheduler, which is unexpected.",godfrey,zhuzh,Major,Closed,Fixed,29/Jan/23 03:16,30/Jan/23 01:55
Bug,FLINK-30815,13521897,BatchTestBase/BatchAbstractTestBase are using JUnit4 while some child tests are using JUnit5,"BatchTestBase/BatchAbstractTestBase are using Junit4, while some child tests (e.g. DynamicFilteringITCase) are using JUnit5. This may break some assumption and hide some problems.
For example, the child test will create a MiniCluster by itself, instead of using the MiniCluster(TM=1, slots=3)  created in BatchAbstractTestBase. The created MiniCluster may  have more slots and hide resource deadlock issues.",tanyuxin,zhuzh,Major,Closed,Fixed,29/Jan/23 04:24,11/May/23 08:12
Bug,FLINK-30825,13522052,Ingress creation failed on K8s with version < 1.19,"Currently flink-kubernetes-operator only use io.fabric8.kubernetes.api.model.networking.v1.Ingress, but this is only available in and after K8s 1.19 ([https://kubernetes.io/docs/reference/using-api/deprecation-guide/] use the *networking.k8s.io/v1* API version, available since v1.19.)

With K8s before 1.19, we can use Ingress in *extensions/v1beta1* or {*}networking.k8s.io/v1beta1{*}, to avoid Ingress creation failure.",,idealities,Minor,Closed,Fixed,30/Jan/23 11:52,09/Feb/23 08:56
Bug,FLINK-30826,13522060,HashAggITCase.testLeadLag is failing,"With jdk8 it is green.
At the same time it is constantly failing for jdk11 like
{noformat}
[ERROR] org.apache.flink.table.planner.runtime.batch.sql.agg.HashAggITCase.testLeadLag  Time elapsed: 0.336 s  <<< FAILURE!
java.lang.AssertionError: 

Results do not match for query:
  
SELECT
  a,
  b, LEAD(b, 1) over (order by a)  AS bLead, LAG(b, 1) over (order by a)  AS bLag,
  c, LEAD(c, 1) over (order by a)  AS cLead, LAG(c, 1) over (order by a)  AS cLag,
  d, LEAD(d, 1) over (order by a)  AS dLead, LAG(d, 1) over (order by a)  AS dLag,
  e, LEAD(e, 1) over (order by a)  AS eLead, LAG(e, 1) over (order by a)  AS eLag,
  f, LEAD(f, 1) over (order by a)  AS fLead, LAG(f, 1) over (order by a)  AS fLag,
  g, LEAD(g, 1) over (order by a)  AS gLead, LAG(g, 1) over (order by a)  AS gLag,
  h, LEAD(h, 1) over (order by a)  AS hLead, LAG(h, 1) over (order by a)  AS hLag,
  i, LEAD(i, 1) over (order by a)  AS iLead, LAG(i, 1) over (order by a)  AS iLag,
  j, LEAD(j, 1) over (order by a)  AS jLead, LAG(j, 1) over (order by a)  AS jLag,
  k, LEAD(k, 1) over (order by a)  AS kLead, LAG(k, 1) over (order by a)  AS kLag,
  l, LEAD(l, 1) over (order by a)  AS lLead, LAG(l, 1) over (order by a)  AS lLag,
  m, LEAD(m, 1) over (order by a)  AS mLead, LAG(m, 1) over (order by a)  AS mLag,
  n, LEAD(n, 1) over (order by a)  AS nLead, LAG(n, 1) over (order by a)  AS nLag

FROM UnnamedTable$18
order by a


Results
 == Correct Result - 3 ==                                                                                                                                                                                                                                                                                                  == Actual Result - 3 ==
!+I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char                , char                , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null]   +I[Alice, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99, null, null]
!+I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char                , null, char                , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99]   +I[Alice, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99, null, null]
!+I[Alice, null, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99]                                     +I[Alice, null, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char                , char                , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99]
        
Plan:
  == Abstract Syntax Tree ==
LogicalSort(sort0=[$0], dir0=[ASC-nulls-first])
+- LogicalProject(inputs=[0..1], exprs=[[LEAD($1, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($1, 1) OVER (ORDER BY $0 NULLS FIRST), $2, LEAD($2, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($2, 1) OVER (ORDER BY $0 NULLS FIRST), $3, LEAD($3, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($3, 1) OVER (ORDER BY $0 NULLS FIRST), $4, LEAD($4, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($4, 1) OVER (ORDER BY $0 NULLS FIRST), $5, LEAD($5, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($5, 1) OVER (ORDER BY $0 NULLS FIRST), $6, LEAD($6, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($6, 1) OVER (ORDER BY $0 NULLS FIRST), $7, LEAD($7, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($7, 1) OVER (ORDER BY $0 NULLS FIRST), $8, LEAD($8, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($8, 1) OVER (ORDER BY $0 NULLS FIRST), $9, LEAD($9, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($9, 1) OVER (ORDER BY $0 NULLS FIRST), $10, LEAD($10, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($10, 1) OVER (ORDER BY $0 NULLS FIRST), $11, LEAD($11, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($11, 1) OVER (ORDER BY $0 NULLS FIRST), $12, LEAD($12, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($12, 1) OVER (ORDER BY $0 NULLS FIRST), $13, LEAD($13, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($13, 1) OVER (ORDER BY $0 NULLS FIRST)]])
   +- LogicalUnion(all=[true])
      :- LogicalProject(exprs=[[CAST(_UTF-16LE'Alice':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", CAST(1:TINYINT):TINYINT, CAST(1:SMALLINT):SMALLINT, CAST(2):INTEGER, CAST(9223:BIGINT):BIGINT, CAST(-2.3E0:FLOAT):FLOAT, CAST(9.9E0:DOUBLE):DOUBLE, CAST(true):BOOLEAN, CAST(_UTF-16LE'varchar':VARCHAR(20) CHARACTER SET ""UTF-16LE""):VARCHAR(20) CHARACTER SET ""UTF-16LE"", CAST(_UTF-16LE'char                '):CHAR(20) CHARACTER SET ""UTF-16LE"", CAST(2021-08-03):DATE, CAST(20:08:17):TIME(0), CAST(2021-08-03 20:08:29:TIMESTAMP(6)):TIMESTAMP(6), CAST(9.99E0:DECIMAL(3, 2)):DECIMAL(3, 2)]])
      :  +- LogicalValues(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])
      :- LogicalProject(exprs=[[CAST(_UTF-16LE'Alice':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", null:TINYINT, null:SMALLINT, null:INTEGER, null:BIGINT, null:FLOAT, null:DOUBLE, null:BOOLEAN, null:VARCHAR(20) CHARACTER SET ""UTF-16LE"", null:CHAR(20) CHARACTER SET ""UTF-16LE"", null:DATE, null:TIME(0), null:TIMESTAMP(6), null:DECIMAL(3, 2)]])
      :  +- LogicalValues(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])
      +- LogicalProject(exprs=[[CAST(_UTF-16LE'Alice':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", CAST(1:TINYINT):TINYINT, CAST(1:SMALLINT):SMALLINT, CAST(2):INTEGER, CAST(9223:BIGINT):BIGINT, CAST(-2.3E0:FLOAT):FLOAT, CAST(9.9E0:DOUBLE):DOUBLE, CAST(true):BOOLEAN, CAST(_UTF-16LE'varchar':VARCHAR(20) CHARACTER SET ""UTF-16LE""):VARCHAR(20) CHARACTER SET ""UTF-16LE"", CAST(_UTF-16LE'char                '):CHAR(20) CHARACTER SET ""UTF-16LE"", CAST(2021-08-03):DATE, CAST(20:08:17):TIME(0), CAST(2021-08-03 20:08:29:TIMESTAMP(6)):TIMESTAMP(6), CAST(9.99E0:DECIMAL(3, 2)):DECIMAL(3, 2)]])
         +- LogicalValues(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])

== Optimized Logical Plan ==
Sort(orderBy=[a ASC])
+- Calc(select=[a, b, w0$o0 AS bLead, w0$o1 AS bLag, c, w0$o2 AS cLead, w0$o3 AS cLag, d, w0$o4 AS dLead, w0$o5 AS dLag, e, w0$o6 AS eLead, w0$o7 AS eLag, f, w0$o8 AS fLead, w0$o9 AS fLag, g, w0$o10 AS gLead, w0$o11 AS gLag, h, w0$o12 AS hLead, w0$o13 AS hLag, i, w0$o14 AS iLead, w0$o15 AS iLag, j, w0$o16 AS jLead, w0$o17 AS jLag, k, w0$o18 AS kLead, w0$o19 AS kLag, l, w0$o20 AS lLead, w0$o21 AS lLag, m, w0$o22 AS mLead, w0$o23 AS mLag, n, w0$o24 AS nLead, w0$o25 AS nLag])
   +- OverAggregate(orderBy=[a ASC], window#0=[LEAD(b, 1) AS w0$o0, LAG(b, 1) AS w0$o1, LEAD(c, 1) AS w0$o2, LAG(c, 1) AS w0$o3, LEAD(d, 1) AS w0$o4, LAG(d, 1) AS w0$o5, LEAD(e, 1) AS w0$o6, LAG(e, 1) AS w0$o7, LEAD(f, 1) AS w0$o8, LAG(f, 1) AS w0$o9, LEAD(g, 1) AS w0$o10, LAG(g, 1) AS w0$o11, LEAD(h, 1) AS w0$o12, LAG(h, 1) AS w0$o13, LEAD(i, 1) AS w0$o14, LAG(i, 1) AS w0$o15, LEAD(j, 1) AS w0$o16, LAG(j, 1) AS w0$o17, LEAD(k, 1) AS w0$o18, LAG(k, 1) AS w0$o19, LEAD(l, 1) AS w0$o20, LAG(l, 1) AS w0$o21, LEAD(m, 1) AS w0$o22, LAG(m, 1) AS w0$o23, LEAD(n, 1) AS w0$o24, LAG(n, 1) AS w0$o25 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[a, b, c, d, e, f, g, h, i, j, k, l, m, n, w0$o0, w0$o1, w0$o2, w0$o3, w0$o4, w0$o5, w0$o6, w0$o7, w0$o8, w0$o9, w0$o10, w0$o11, w0$o12, w0$o13, w0$o14, w0$o15, w0$o16, w0$o17, w0$o18, w0$o19, w0$o20, w0$o21, w0$o22, w0$o23, w0$o24, w0$o25])
      +- Sort(orderBy=[a ASC])
         +- Exchange(distribution=[single])
            +- Union(all=[true], union=[a, b, c, d, e, f, g, h, i, j, k, l, m, n])
               :- Calc(select=[CAST('Alice' AS VARCHAR(2147483647)) AS a, CAST(1 AS TINYINT) AS b, CAST(1 AS SMALLINT) AS c, CAST(2 AS INTEGER) AS d, CAST(9223 AS BIGINT) AS e, CAST(-2.3E0 AS FLOAT) AS f, CAST(9.9E0 AS DOUBLE) AS g, CAST(true AS BOOLEAN) AS h, CAST('varchar' AS VARCHAR(20)) AS i, CAST('char                ' AS CHAR(20)) AS j, CAST(2021-08-03 AS DATE) AS k, CAST(20:08:17 AS TIME(0)) AS l, CAST(2021-08-03 20:08:29 AS TIMESTAMP(6)) AS m, CAST(9.99E0 AS DECIMAL(3, 2)) AS n])
               :  +- Values(tuples=[[{ 0 }]], values=[ZERO])
               :- Calc(select=[CAST('Alice' AS VARCHAR(2147483647)) AS a, null:TINYINT AS b, null:SMALLINT AS c, null:INTEGER AS d, null:BIGINT AS e, null:FLOAT AS f, null:DOUBLE AS g, null:BOOLEAN AS h, null:VARCHAR(20) AS i, null:CHAR(20) AS j, null:DATE AS k, null:TIME(0) AS l, null:TIMESTAMP(6) AS m, null:DECIMAL(3, 2) AS n])
               :  +- Values(tuples=[[{ 0 }]], values=[ZERO])
               +- Calc(select=[CAST('Alice' AS VARCHAR(2147483647)) AS a, CAST(1 AS TINYINT) AS b, CAST(1 AS SMALLINT) AS c, CAST(2 AS INTEGER) AS d, CAST(9223 AS BIGINT) AS e, CAST(-2.3E0 AS FLOAT) AS f, CAST(9.9E0 AS DOUBLE) AS g, CAST(true AS BOOLEAN) AS h, CAST('varchar' AS VARCHAR(20)) AS i, CAST('char                ' AS CHAR(20)) AS j, CAST(2021-08-03 AS DATE) AS k, CAST(20:08:17 AS TIME(0)) AS l, CAST(2021-08-03 20:08:29 AS TIMESTAMP(6)) AS m, CAST(9.99E0 AS DECIMAL(3, 2)) AS n])
                  +- Values(tuples=[[{ 0 }]], values=[ZERO])

       
	at org.junit.Assert.fail(Assert.java:89)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.$anonfun$check$1(BatchTestBase.scala:152)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.$anonfun$check$1$adapted(BatchTestBase.scala:145)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:145)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:106)
	at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.testLeadLag(AggregateITCaseBase.scala:958)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
	at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)

{noformat}",,Sergey Nuyanzin,Major,Resolved,Fixed,30/Jan/23 12:25,31/Jan/23 13:53
Bug,FLINK-30828,13522075,SortAggITCase.testLeadLag failed,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45389&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12560

{code}
Jan 30 11:03:32 [ERROR] Tests run: 72, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 37.42 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase
Jan 30 11:03:32 [ERROR] org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.testLeadLag  Time elapsed: 0.547 s  <<< FAILURE!
Jan 30 11:03:32 java.lang.AssertionError: 
Jan 30 11:03:32 
Jan 30 11:03:32 Results do not match for query:
Jan 30 11:03:32   
Jan 30 11:03:32 SELECT
Jan 30 11:03:32   a,
Jan 30 11:03:32   b, LEAD(b, 1) over (order by a)  AS bLead, LAG(b, 1) over (order by a)  AS bLag,
Jan 30 11:03:32   c, LEAD(c, 1) over (order by a)  AS cLead, LAG(c, 1) over (order by a)  AS cLag,
Jan 30 11:03:32   d, LEAD(d, 1) over (order by a)  AS dLead, LAG(d, 1) over (order by a)  AS dLag,
Jan 30 11:03:32   e, LEAD(e, 1) over (order by a)  AS eLead, LAG(e, 1) over (order by a)  AS eLag,
Jan 30 11:03:32   f, LEAD(f, 1) over (order by a)  AS fLead, LAG(f, 1) over (order by a)  AS fLag,
Jan 30 11:03:32   g, LEAD(g, 1) over (order by a)  AS gLead, LAG(g, 1) over (order by a)  AS gLag,
Jan 30 11:03:32   h, LEAD(h, 1) over (order by a)  AS hLead, LAG(h, 1) over (order by a)  AS hLag,
Jan 30 11:03:32   i, LEAD(i, 1) over (order by a)  AS iLead, LAG(i, 1) over (order by a)  AS iLag,
Jan 30 11:03:32   j, LEAD(j, 1) over (order by a)  AS jLead, LAG(j, 1) over (order by a)  AS jLag,
Jan 30 11:03:32   k, LEAD(k, 1) over (order by a)  AS kLead, LAG(k, 1) over (order by a)  AS kLag,
Jan 30 11:03:32   l, LEAD(l, 1) over (order by a)  AS lLead, LAG(l, 1) over (order by a)  AS lLag,
Jan 30 11:03:32   m, LEAD(m, 1) over (order by a)  AS mLead, LAG(m, 1) over (order by a)  AS mLag,
Jan 30 11:03:32   n, LEAD(n, 1) over (order by a)  AS nLead, LAG(n, 1) over (order by a)  AS nLag
Jan 30 11:03:32 
Jan 30 11:03:32 FROM UnnamedTable$230
Jan 30 11:03:32 order by a
Jan 30 11:03:32 
Jan 30 11:03:32 
Jan 30 11:03:32 Results
Jan 30 11:03:32  == Correct Result - 3 ==                                                                                                                                                                                                                                                                                                  == Actual Result - 3 ==
Jan 30 11:03:32  +I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char                , char                , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null]   +I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char                , char                , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null]
Jan 30 11:03:32  +I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char                , null, char                , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99]   +I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char                , null, char                , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99]
Jan 30 11:03:32 !+I[Alice, null, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99]                                     +I[Alice, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99, null]
Jan 30 11:03:32         
Jan 30 11:03:32 Plan:
Jan 30 11:03:32   == Abstract Syntax Tree ==
Jan 30 11:03:32 LogicalSort(sort0=[$0], dir0=[ASC-nulls-first])
Jan 30 11:03:32 +- LogicalProject(inputs=[0..1], exprs=[[LEAD($1, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($1, 1) OVER (ORDER BY $0 NULLS FIRST), $2, LEAD($2, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($2, 1) OVER (ORDER BY $0 NULLS FIRST), $3, LEAD($3, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($3, 1) OVER (ORDER BY $0 NULLS FIRST), $4, LEAD($4, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($4, 1) OVER (ORDER BY $0 NULLS FIRST), $5, LEAD($5, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($5, 1) OVER (ORDER BY $0 NULLS FIRST), $6, LEAD($6, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($6, 1) OVER (ORDER BY $0 NULLS FIRST), $7, LEAD($7, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($7, 1) OVER (ORDER BY $0 NULLS FIRST), $8, LEAD($8, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($8, 1) OVER (ORDER BY $0 NULLS FIRST), $9, LEAD($9, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($9, 1) OVER (ORDER BY $0 NULLS FIRST), $10, LEAD($10, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($10, 1) OVER (ORDER BY $0 NULLS FIRST), $11, LEAD($11, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($11, 1) OVER (ORDER BY $0 NULLS FIRST), $12, LEAD($12, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($12, 1) OVER (ORDER BY $0 NULLS FIRST), $13, LEAD($13, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($13, 1) OVER (ORDER BY $0 NULLS FIRST)]])
Jan 30 11:03:32    +- LogicalUnion(all=[true])
[...]
{code}",zhuzh,mapohl,Critical,Closed,Fixed,30/Jan/23 13:30,31/Jan/23 10:27
Bug,FLINK-30841,13522174,Incorrect calc merge generate wrong plan,"currently we have a `FlinkCalcMergeRuleTest`, take one test as example:
{code:java}
  @Test
  def testCalcMergeWithNonDeterministicExpr1(): Unit = {
    val sqlQuery = ""SELECT a, a1 FROM (SELECT a, random_udf(a) AS a1 FROM MyTable) t WHERE a1 > 10""
    util.verifyRelPlan(sqlQuery)
  }
{code}
the current final optimized plan will be wrong:
{code:java}
Calc(select=[a, random_udf(b) AS a1], where=[(random_udf(b) > 10)])
+- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
{code}
the merged calc contains two `random_udf` call, users may encounter the result satisfied by where predicate (>10) but the selected column <= 10, that's counter-intuitive for users

the expected plan is:
{code:java}
Calc(select=[a, a1], where=[(a1 > 10)])
+- Calc(select=[a, random_udf(b) AS a1])
   +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
{code}",lincoln.86xy,lincoln.86xy,Major,Closed,Fixed,31/Jan/23 07:00,03/Feb/23 13:38
Bug,FLINK-30844,13522181,TaskTest.testInterruptibleSharedLockInInvokeAndCancel causes a JVM shutdown with exit code 239,"We're experiencing a fatal crash in {{TaskTest}}:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45440&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8334
{code}
[...]
Jan 31 01:03:12 [ERROR] Process Exit Code: 239
Jan 31 01:03:12 [ERROR] Crashed tests:
Jan 31 01:03:12 [ERROR] org.apache.flink.runtime.taskmanager.TaskTest
Jan 31 01:03:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
Jan 31 01:03:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$700(ForkStarter.java:121)
Jan 31 01:03:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:393)
Jan 31 01:03:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:370)
Jan 31 01:03:12 [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Jan 31 01:03:12 [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Jan 31 01:03:12 [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Jan 31 01:03:12 [ERROR] at java.lang.Thread.run(Thread.java:748)
Jan 31 01:03:12 [ERROR] -> [Help 1]
Jan 31 01:03:12 [ERROR] 
Jan 31 01:03:12 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Jan 31 01:03:12 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
Jan 31 01:03:12 [ERROR] 
Jan 31 01:03:12 [ERROR] For more information about the errors and possible solutions, please read the following articles:
Jan 31 01:03:12 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
Jan 31 01:03:12 [ERROR] 
Jan 31 01:03:12 [ERROR] After correcting the problems, you can resume the build with the command
Jan 31 01:03:12 [ERROR]   mvn <goals> -rf :flink-runtime
{code}",akalashnikov,mapohl,Major,Closed,Fixed,31/Jan/23 07:50,26/May/23 08:17
Bug,FLINK-30845,13522182,Params in jarURI end up in file name,"*Context*

Jar files for jobs are submitted to the operator by supplying a URI to the .jar file. This URI can be a file system path or a URI to some HTTP resource. If a HTTP URI is given, the file will be fetched using the {{{}HttpArtifactFetcher{}}}. 

There are cases where the supplied URI will contain additional params. For example if pre-signed S3 URLs are used.

Example:
{code:java}
https://some-domain.example.com/some.jar?some=params{code}
*Problem*

When the HttpArtifactFetcher determines the name of the .jar file it does also use the params as part of the file name. In the example from above the resulting file name would be:  {{some.jar?some=params}}

Submitting this job to Flink will result in an error as it will be checked for the file name to end with {{.jar}}

*Possible Solution*
In the {{HttpArtifactFetcher}} it would be enough to replace:
{code:java}
String fileName = FilenameUtils.getName(url.getFile());{code}
with
{code:java}
String fileName = FilenameUtils.getName(url.getPath());{code}",fabiowanner,fabiowanner,Minor,Closed,Fixed,31/Jan/23 07:55,07/Feb/23 11:29
Bug,FLINK-30846,13522183,SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource fails,"{{SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource}} is timing out
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45440&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8599
{code}
Jan 31 02:02:28 ""ForkJoinPool-1-worker-25"" #27 daemon prio=5 os_prio=0 tid=0x00007fcf74f2b800 nid=0x5476 waiting on condition [0x00007fce2b078000]
Jan 31 02:02:28    java.lang.Thread.State: WAITING (parking)
Jan 31 02:02:28 	at sun.misc.Unsafe.park(Native Method)
Jan 31 02:02:28 	- parking to wait for  <0x00000000a22933e0> (a java.util.concurrent.CompletableFuture$Signaller)
Jan 31 02:02:28 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Jan 31 02:02:28 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Jan 31 02:02:28 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
Jan 31 02:02:28 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Jan 31 02:02:28 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Jan 31 02:02:28 	at org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.executeJob(SpeculativeSchedulerITCase.java:216)
Jan 31 02:02:28 	at org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource(SpeculativeSchedulerITCase.java:162)
{code}",JunRuiLi,mapohl,Critical,Closed,Fixed,31/Jan/23 07:57,06/Feb/23 07:17
Bug,FLINK-30852,13522217,TaskTest.testCleanupWhenSwitchToInitializationFails reports AssertionError but doesn't fail,"While investigating FLINK-30844, I noticed that {{TaskTest.testCleanup}} reports an AssertionError in the logs but doesn't fail:
{code}
00:59:01,886 [                main] ERROR org.apache.flink.runtime.taskmanager.Task                    [] - Error while canceling task Test Task (1/1)#0.
java.lang.AssertionError: This should not be called
        at org.junit.Assert.fail(Assert.java:89) ~[junit-4.13.2.jar:4.13.2]
        at org.apache.flink.runtime.taskmanager.TaskTest$TestInvokableCorrect.cancel(TaskTest.java:1304) ~[test-classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.cancelInvokable(Task.java:1529) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:796) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.TaskTest.testCleanupWhenSwitchToInitializationFails(TaskTest.java:184) ~[test-classes/:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
[...]
{code}

[~akalashnikov] is this expected?

The affected build is https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45440&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8",akalash,mapohl,Critical,Resolved,Fixed,31/Jan/23 12:00,21/Apr/23 11:50
Bug,FLINK-30857,13522296,Create table does not create topic with multiple partitions," 
{code:java}
CREATE CATALOG table_store_catalog WITH (
    'type'='table-store',
    'warehouse'='s3://my-bucket/table-store'
 );
USE CATALOG table_store_catalog;
SET 'execution.checkpointing.interval' = '10 s';
CREATE TABLE word_count_kafka (
     word STRING PRIMARY KEY NOT ENFORCED,
     cnt BIGINT
 ) WITH (
     'log.system' = 'kafka',
     'kafka.bootstrap.servers' = 'broker:9092',
     'kafka.topic' = 'word_count_log',
     'bucket'='4'
 );
{code}
 

The created topic has only one partition
{code:java}
Topic: word_count_log    TopicId: udeJwBIkRsSybkf1EerphA    PartitionCount: 1    ReplicationFactor: 1    Configs:
    Topic: word_count_log    Partition: 0    Leader: 1    Replicas: 1    Isr: 1{code}",,vicky_papavas,Major,Closed,Fixed,31/Jan/23 20:01,29/Mar/23 02:04
Bug,FLINK-30858,13522370,Kubernetes operator does not update reconciled generation,"Kubernetes manages the generation field as part of the spec metadata. It will be increased when changes are made to the resource. The counterpart in status is ""observed generation"", provided by a controller. By comparing the two, the client can determine that the controller has processed the spec and in conjunction with other status information conclude that a change has been reconciled.

The Flink operator currently tracks the generation as part of reconciled and stable specs but these cannot be used as ""observed generation"" to perform the check. The value isn't updated in cases where operator determines that there are no changes to the spec that require deployment. This can be reproduced through PUT/replace with the same spec or a change in upgrade mode.

The operator should provide the observed spec, which in conjunction with deployment state can then be used by clients to determine that the spec has been reconciled.",gyfora,thw,Major,Closed,Fixed,01/Feb/23 00:50,04/Feb/23 02:10
Bug,FLINK-30861,13522413,Table Store Hive Catalog throws java.lang.ClassNotFoundException: org.apache.hadoop.hive.common.ValidWriteIdList under certain environment,Table Store Hive Catalog throws {{java.lang.ClassNotFoundException: org.apache.hadoop.hive.common.ValidWriteIdList}} under certain environment. We need to package {{hive-storage-api}} dependency.,TsReaper,TsReaper,Major,Closed,Fixed,01/Feb/23 07:16,01/Feb/23 08:05
Bug,FLINK-30864,13522456,Optional pattern at the start of a group pattern not working,"The optional pattern at the start of a group pattern turns out be ""not optional"", e.g.
{code:java}
Pattern.<String>begin(""A"").next(Pattern.<String>begin(""B"").optional().next(""C"")).next(""D"")
{code}
cannot match sequence ""a1 c1 d1"".",Juntao Hu,Juntao Hu,Major,Closed,Fixed,01/Feb/23 11:25,17/Feb/23 07:44
Bug,FLINK-30868,13522467,Revert to use LongSerializer for seralization in the TimeIndicatorTypeInfo,,fsk119,fsk119,Major,Closed,Fixed,01/Feb/23 12:22,06/Feb/23 07:13
Bug,FLINK-30870,13522480,Performance regressions notifications in Slack are cut off,"Example from today at https://apache-flink.slack.com/archives/C0471S0DFJ9/p1675253720571659

{code}
Performance regression
mapRebalanceMapSink.F27_UNBOUNDED median=17231.7398765 recent_median=16165.0549395
multiInputOneIdleMapSink median=11254.5329375 recent_median=10727.7280915
calculateRegionToRestart.BATCH median=12.881527 recent_median=12.096391
partitionRelease.BATCH median=23.2130145 recent_median=21.4858475
checkpointMultiInput median=2.6094395 recent_median=2.477736
checkpointSingleInput.UNALIGNED median=339.229515 recent_median=67.2695295
checkpointSingleInput.UNALIGNED_1 median=215.2789775 recent_median=40.1294965
fireProcessingTimers median=50.9977185 recent_median=44.0925955
globalWindow median=5459.689767 recent_median=5045.436655
<http…
{code}

As you can see, the last part is cut off with {{<htt...}} listed. I'm not sure if this is because there are more regressions, but they aren't posted to Slack (could be) or if this is just a mistake in the output and the {{globalWindow}} was the last regression to be reported. It would be great if this could be validated. ",Yanfei Lei,martijnvisser,Major,Closed,Fixed,01/Feb/23 13:15,10/Feb/23 07:41
Bug,FLINK-30875,13522705,Fix usages of legacy AdaptiveBatchScheduler configuration,"In FLINK-30686, we deprecated the JobManagerOptions's AdaptiveBatchScheduler configuration. However, these configuration items still have some calls. And we should change these calls to new configuration.",JunRuiLi,JunRuiLi,Critical,Closed,Fixed,02/Feb/23 07:43,03/Feb/23 10:28
Bug,FLINK-30876,13522711,Fix ResetTransformationProcessor don't reset the transformation of ExecNode in BatchExecMultiInput.rootNode,"Now, ResetTransformationProcessor don't reset the transformation of ExecNode in BatchExecMultiInput.rootNode. This may cause error while creating StreamGraph for BatchExecMultiInput due to different id of rootNode and inputNode.",337361684@qq.com,337361684@qq.com,Major,Closed,Fixed,02/Feb/23 08:21,06/Feb/23 03:23
Bug,FLINK-30878,13522716,KubernetesHighAvailabilityRecoverFromSavepointITCase fails due to a deadlock,"We're seeing a test failure in {{KubernetesHighAvailabilityRecoverFromSavepointITCase}} due to a deadlock:
{code:java}
2023-02-01T18:53:35.5540322Z ""ForkJoinPool-1-worker-1"" #14 daemon prio=5 os_prio=0 tid=0x00007f68ecb18000 nid=0x43dd1 waiting on condition [0x00007f68c1711000]
2023-02-01T18:53:35.5540900Z    java.lang.Thread.State: TIMED_WAITING (parking)
2023-02-01T18:53:35.5541272Z 	at sun.misc.Unsafe.park(Native Method)
2023-02-01T18:53:35.5541932Z 	- parking to wait for  <0x00000000d14d7b60> (a java.util.concurrent.CompletableFuture$Signaller)
2023-02-01T18:53:35.5542496Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2023-02-01T18:53:35.5543088Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1709)
2023-02-01T18:53:35.5543672Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
2023-02-01T18:53:35.5544240Z 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1788)
2023-02-01T18:53:35.5544801Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2023-02-01T18:53:35.5545632Z 	at org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase.testRecoverFromSavepoint(KubernetesHighAvailabilityRecoverFromSavepointITCase.java:113)
2023-02-01T18:53:35.5546409Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) {code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45565&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=61916]

The build failure happens on 1.16. I'm adding 1.17 and 1.15 as fixVersions as well because it might be due to some recent changes which were introduced with FLINK-30462 and/or FLINK-30474",mapohl,mapohl,Blocker,Resolved,Fixed,02/Feb/23 08:30,02/Feb/23 11:03
Bug,FLINK-30881,13522764,Crictl/Minikube version mismatch causes errors in k8s setup,"We observed constant failures in the e2e k8s tests with permission issues. This was initially accidentally reported through FLINK-29671. But FLINK-29671 actually covers a different instability.

Here are the build failures initially reported in FLINK-29671:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45548&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4972]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4900]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=af885ea8-6b05-5dc2-4a37-eab9c0d1ab09&t=f779a55a-0ffe-5bbc-8824-3a79333d4559&l=5597]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45587&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4818]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45587&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=f8a6d3eb-38cf-5cca-9a99-d0badeb5fe62&l=5852]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45591&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4915]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45598&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4921]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45603&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4991]

{code:java}
Feb 01 11:00:45 Starting minikube ...
Feb 01 11:00:45 * minikube v1.29.0 on Ubuntu 20.04
Feb 01 11:00:45 * Using the none driver based on existing profile
Feb 01 11:00:45 * Starting control plane node minikube in cluster minikube
Feb 01 11:00:45 * Restarting existing none bare metal machine for ""minikube"" ...
Feb 01 11:00:45 * OS release is Ubuntu 20.04.5 LTS
Feb 01 11:01:22 
X Exiting due to RUNTIME_ENABLE: Temporary Error: sudo /usr/local/bin/crictl version: exit status 1
stdout:
[...]  
Feb 01 11:01:22 
E0201 11:01:22.809164  241870 root.go:80] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
Feb 01 11:01:22 
X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
* Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
* Related issue: https://github.com/kubernetes/minikube/issues/9165
[...]{code}",mapohl,mapohl,Blocker,Resolved,Fixed,02/Feb/23 11:12,03/Feb/23 08:55
Bug,FLINK-30885,13522782,Optional group pattern starts with non-optional looping pattern get wrong result on followed-by,"{code:java}
Pattern.begin(""A"")
  .followedBy(
    Pattern.begin(""B"").oneOrMore().greedy().consecutive()
      .next(""C""))
  .optional()
  .next(""D""){code}
This can match ""a1 e1 d1"", which is not the expected behavior.",Juntao Hu,Juntao Hu,Major,Closed,Fixed,02/Feb/23 12:31,17/Feb/23 07:43
Bug,FLINK-30887,13522790,CI workflow for externalized connectors doesn't cache Flink's binary download,The current CI workflow for PRs that runs on externalized connectors doesn't cache Flink binary downloads properly. We should fix this. ,martijnvisser,martijnvisser,Major,Closed,Fixed,02/Feb/23 13:25,09/Feb/23 16:05
Bug,FLINK-30888,13522845,Flink Doc(zh version) has misspelled words ,"[https://github.com/apache/flink/blob/master/docs/content.zh/docs/dev/datastream/overview.md]

Iterative streaming 程序实现了 +{color:#de350b}setp function{color}+ 并将其嵌入到 {{IterativeStream}} 。由于 DataStream 程序可能永远不会完成，因此没有最大迭代次数。相反，你需要指定流的哪一部分反馈给迭代，哪一部分使用[旁路输出](\{{< ref ""docs/dev/datastream/side_output"" >}})或{{{}过滤器{}}}转发到下游。

==> step function

 ",,marlin,Minor,Closed,Fixed,03/Feb/23 00:42,17/Apr/23 03:59
Bug,FLINK-30889,13522861,Fix the key of configuration SPECULATIVE_ENABLED.,"In FLINK-30686 we refined the adaptive batch configuration. However, the key of a configuration item `SPECULATIVE_ENABLED` has been modified incorrectly. According to flip-283, it should be modified to `execution.batch.speculative.enabled`.",JunRuiLi,JunRuiLi,Critical,Closed,Fixed,03/Feb/23 03:41,03/Feb/23 09:33
Bug,FLINK-30895,13522930,SlotSharingSlotAllocator may waste slots,"The allocated evenly distributes slots across slot sharing groups independent of how many slots the vertices in that group actually need.

This can cause slots to be unused.",chesnay,chesnay,Major,Closed,Fixed,03/Feb/23 10:30,20/Feb/23 11:20
Bug,FLINK-30901,13523123,The jobVertex's parallelismConfigured is incorrect when chaining with source operators,"When creating OperatorChainInfo in StreamingJobGenerator, the chained source are not included in OperatorChainInfo#chainedNodes, because they are not added to OperatorChainInfo via #addNodeToChain().

This will affect jobVertex which has a MultiInput operator chained with sources. The vertex's parallelismConfigured will be false even if the chained sources have a parallelism configured. ",JunRuiLi,JunRuiLi,Major,Closed,Fixed,06/Feb/23 03:21,07/Feb/23 11:17
Bug,FLINK-30903,13523148,The max parallelism used in adaptive batch scheduler doesn't fallbacks to default parallelism,"In FLINK-30684 we mark the vertices which use the default parallelism, and in AdaptiveBatchScheduler we allow users to use parallelism.default as the max parallelism if they don't configure the configuration item ""execution.batch.adaptive.auto-parallelism.max-parallelism"". This issue will fix the fallback logic.

 ",JunRuiLi,JunRuiLi,Critical,Closed,Fixed,06/Feb/23 06:06,07/Feb/23 08:09
Bug,FLINK-30908,13523165,Fatal error in ResourceManager caused YARNSessionFIFOSecuredITCase.testDetachedMode to fail,"There's a build failure in {{YARNSessionFIFOSecuredITCase.testDetachedMode}} which is caused by a fatal error in the ResourceManager:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45720&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=29869

{code}
Feb 05 02:41:58 java.io.InterruptedIOException: Interrupted waiting to send RPC request to server
Feb 05 02:41:58 java.io.InterruptedIOException: Interrupted waiting to send RPC request to server
Feb 05 02:41:58 	at org.apache.hadoop.ipc.Client.call(Client.java:1480) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.ipc.Client.call(Client.java:1422) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at com.sun.proxy.$Proxy31.allocate(Unknown Source) ~[?:?]
Feb 05 02:41:58 	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77) ~[hadoop-yarn-common-3.2.3.jar:?]
Feb 05 02:41:58 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
Feb 05 02:41:58 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
Feb 05 02:41:58 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
Feb 05 02:41:58 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
Feb 05 02:41:58 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at com.sun.proxy.$Proxy32.allocate(Unknown Source) ~[?:?]
Feb 05 02:41:58 	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:325) ~[hadoop-yarn-client-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread.run(AMRMClientAsyncImpl.java:311) [hadoop-yarn-client-3.2.3.jar:?]
Feb 05 02:41:58 Caused by: java.lang.InterruptedException
Feb 05 02:41:58 	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404) ~[?:1.8.0_292]
Feb 05 02:41:58 	at java.util.concurrent.FutureTask.get(FutureTask.java:191) ~[?:1.8.0_292]
Feb 05 02:41:58 	at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1180) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.ipc.Client.call(Client.java:1475) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	... 17 more
{code}",xtsong,mapohl,Critical,Closed,Fixed,06/Feb/23 07:28,03/May/23 15:03
Bug,FLINK-30910,13523169,ApplicationDispatcherBootstrapTest.testApplicationIsStoppedWhenStoppingBootstrap fails with assertion,"A build failure in {{ApplicationDispatcherBootstrapTest.testApplicationIsStoppedWhenStoppingBootstrap}}:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45722&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9831

{code}
Feb 05 01:13:44 [ERROR] Tests run: 30, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 5.174 s <<< FAILURE! - in org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest
Feb 05 01:13:44 [ERROR] org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest.testApplicationIsStoppedWhenStoppingBootstrap  Time elapsed: 2.026 s  <<< FAILURE!
Feb 05 01:13:44 org.opentest4j.AssertionFailedError: 
Feb 05 01:13:44 
Feb 05 01:13:44 Expecting value to be true but was false
Feb 05 01:13:44 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Feb 05 01:13:44 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Feb 05 01:13:44 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Feb 05 01:13:44 	at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest.testApplicationIsStoppedWhenStoppingBootstrap(ApplicationDispatcherBootstrapTest.java:361)
[...]
{code}",mapohl,mapohl,Critical,Resolved,Fixed,06/Feb/23 07:56,08/Feb/23 07:51
Bug,FLINK-30917,13523189,The user configured max parallelism does not take effect when using adaptive batch scheduler,"Currently, the adaptive batch scheduler only respects the global maximum parallelism(which is configured by option {{parallelism.default}} or {{execution.batch.adaptive.auto-parallelism.max-parallelism}}, see FLINK-30686 for details) when deciding parallelism for job vertices, the maximum parallelism of vertices configured by the user through {{setMaxParallelism}} will not be respected.

In this ticket, we will change the behavior so that the user-configured max parallelism also be respected.",wanglijie,wanglijie,Major,Closed,Fixed,06/Feb/23 08:43,15/Feb/23 06:19
Bug,FLINK-30921,13523205,"Too many CI failed due to ""Could not connect to azure.archive.ubuntu.com""","!image-2023-02-06-17-59-20-019.png!

 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45762&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14]

 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45766&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a",mapohl,fanrui,Critical,Resolved,Fixed,06/Feb/23 09:59,03/May/23 08:49
Bug,FLINK-30922,13523211,SQL validate fail in parsing writable metadata,"When i tried an simple demo sql with writing metadata to the kafka in flink sql client
{code:java}
CREATE TABLE KafkaTable (
  `user_id` BIGINT,
  `item_id` BIGINT,
  `behavior` STRING,
  `ts` TIMESTAMP(3) METADATA FROM 'timestamp'
) WITH (
  'connector' = 'kafka',
  'topic' = 'user_behavior',
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.group.id' = 'testGroup',
  'scan.startup.mode' = 'earliest-offset',
  'format' = 'csv'
)

INSERT INTO KafkaTable(user_id, ts) SELECT '1', CURRENT_TIMESTAMP; {code}
 

it will be throw an error
{code:java}
org.apache.flink.table.client.gateway.SqlExecutionException: Failed to parse statement: INSERT INTO KafkaTable(user_id, ts) SELECT '1', CURRENT_TIMESTAMP;
        at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:174) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.cli.SqlCommandParserImpl.parseCommand(SqlCommandParserImpl.java:45) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.cli.SqlMultiLineParser.parse(SqlMultiLineParser.java:71) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.jline.reader.impl.LineReaderImpl.acceptLine(LineReaderImpl.java:2964) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.jline.reader.impl.LineReaderImpl$$Lambda$364/1900307803.apply(Unknown Source) ~[?:?]
        at org.jline.reader.impl.LineReaderImpl$1.apply(LineReaderImpl.java:3778) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.jline.reader.impl.LineReaderImpl.readLine(LineReaderImpl.java:679) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:295) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:280) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:228) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161) [flink-sql-client-1.16.1.jar:1.16.1]
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 33 to line 1, column 34: Unknown target column 'ts'
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:186) ~[?:?]
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113) ~[?:?]
        at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:261) ~[?:?]
        at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[?:?]
        at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172) ~[flink-sql-client-1.16.1.jar:1.16.1]
        ... 13 more
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 33 to line 1, column 34: Unknown target column 'ts'
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_41]
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_41]
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_41]
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422) ~[?:1.8.0_41]
        at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467) ~[?:?]
        at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883) ~[?:?]
        at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.newValidationError(PreValidateReWriter.scala:401) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.validateField(PreValidateReWriter.scala:389) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.$anonfun$appendPartitionAndNullsProjects$3(PreValidateReWriter.scala:172) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$$$Lambda$610/614335089.apply(Unknown Source) ~[?:?]
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike$$Lambda$329/456314134.apply(Unknown Source) ~[?:?]
        at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike.map(TraversableLike.scala:233) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike.map$(TraversableLike.scala:226) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.appendPartitionAndNullsProjects(PreValidateReWriter.scala:164) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.rewriteInsert(PreValidateReWriter.scala:71) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.visit(PreValidateReWriter.scala:61) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.visit(PreValidateReWriter.scala:50) ~[?:?]
        at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) ~[?:?]
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:118) ~[?:?]
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113) ~[?:?]
        at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:261) ~[?:?]
        at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[?:?]
        at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172) ~[flink-sql-client-1.16.1.jar:1.16.1]
        ... 13 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Unknown target column 'ts'
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_41]
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_41]
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_41]
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422) ~[?:1.8.0_41]
        at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467) ~[?:?]
        at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:560) ~[?:?]
        at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883) ~[?:?]
        at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.newValidationError(PreValidateReWriter.scala:401) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.validateField(PreValidateReWriter.scala:389) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.$anonfun$appendPartitionAndNullsProjects$3(PreValidateReWriter.scala:172) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$$$Lambda$610/614335089.apply(Unknown Source) ~[?:?]
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike$$Lambda$329/456314134.apply(Unknown Source) ~[?:?]
        at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike.map(TraversableLike.scala:233) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike.map$(TraversableLike.scala:226) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.appendPartitionAndNullsProjects(PreValidateReWriter.scala:164) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.rewriteInsert(PreValidateReWriter.scala:71) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.visit(PreValidateReWriter.scala:61) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.visit(PreValidateReWriter.scala:50) ~[?:?]
        at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) ~[?:?]
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:118) ~[?:?]
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113) ~[?:?]
        at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:261) ~[?:?]
        at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[?:?]
        at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172) ~[flink-sql-client-1.16.1.jar:1.16.1]
        ... 13 more{code}
 ",csq,tanjialiang,Major,Closed,Fixed,06/Feb/23 10:35,07/Mar/23 01:27
Bug,FLINK-30927,13523230,"Several tests started generate output with two non-abstract methods  have the same parameter types, declaring type and return type","e.g. org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase#testUserDefinedFunctions

 

it seems during code splitter it starts generating some methods with same signature

 

{noformat}

org.codehaus.janino.InternalCompilerException: Compiling ""MatchRecognizePatternProcessFunction$77"": Two non-abstract methods ""default void MatchRecognizePatternProcessFunction$77.processMatch_0(java.util.Map, org.apache.flink.cep.functions.PatternProcessFunction$Context, org.apache.flink.util.Collector) throws java.lang.Exception"" have the same parameter types, declaring type and return type

{noformat}

 

Probably could be a side effect of https://issues.apache.org/jira/browse/FLINK-27246",KristoffSC,Sergey Nuyanzin,Major,Closed,Fixed,06/Feb/23 12:45,09/Feb/23 09:33
Bug,FLINK-30933,13523406,Result of join inside iterationBody loses max watermark,"Currently if we execute a join inside an iteration body, the following program produces empty output. (In which the right result should be a list with \{2}.
{code:java}
public class Test {

    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        DataStream<Tuple2<Long, Integer>> input1 =
                env.fromElements(Tuple2.of(1L, 1), Tuple2.of(2L, 2));

        DataStream<Tuple2<Long, Long>> input2 =
                env.fromElements(Tuple2.of(1L, 2L), Tuple2.of(2L, 3L));

        DataStream<Tuple2<Long, Long>> iterationJoin =
                Iterations.iterateBoundedStreamsUntilTermination(
                                DataStreamList.of(input1),
                                ReplayableDataStreamList.replay(input2),
                                IterationConfig.newBuilder()
                                        .setOperatorLifeCycle(
                                                IterationConfig.OperatorLifeCycle.PER_ROUND)
                                        .build(),
                                new MyIterationBody())
                        .get(0);

        DataStream<Long> left = iterationJoin.map(x -> x.f0);
        DataStream<Long> right = iterationJoin.map(x -> x.f1);
        DataStream<Long> result =
                left.join(right)
                        .where(x -> x)
                        .equalTo(x -> x)
                        .window(EndOfStreamWindows.get())
                        .apply((JoinFunction<Long, Long, Long>) (l1, l2) -> l1);

        List<Long> collectedResult = IteratorUtils.toList(result.executeAndCollect());
        List<Long> expectedResult = Arrays.asList(2L);
        compareResultCollections(expectedResult, collectedResult, Long::compareTo);
    }

    private static class MyIterationBody implements IterationBody {
        @Override
        public IterationBodyResult process(
                DataStreamList variableStreams, DataStreamList dataStreams) {
            DataStream<Tuple2<Long, Integer>> input1 = variableStreams.get(0);
            DataStream<Tuple2<Long, Long>> input2 = dataStreams.get(0);

            DataStream<Long> terminationCriteria = input1.flatMap(new TerminateOnMaxIter(1));

            DataStream<Tuple2<Long, Long>> res =
                    input1.join(input2)
                            .where(x -> x.f0)
                            .equalTo(x -> x.f0)
                            .window(EndOfStreamWindows.get())
                            .apply(
                                    new JoinFunction<
                                            Tuple2<Long, Integer>,
                                            Tuple2<Long, Long>,
                                            Tuple2<Long, Long>>() {
                                        @Override
                                        public Tuple2<Long, Long> join(
                                                Tuple2<Long, Integer> longIntegerTuple2,
                                                Tuple2<Long, Long> longLongTuple2)
                                                throws Exception {
                                            return longLongTuple2;
                                        }
                                    });

            return new IterationBodyResult(
                    DataStreamList.of(input1), DataStreamList.of(res), terminationCriteria);
        }
    }
}
{code}
 

There are two possible reasons:
 * The timer in `HeadOperator` is not a daemon process and it does not exit even flink job finishes.
 * The max watermark from the iteration body is missed.

 

 ",zhangzp,zhangzp,Major,Closed,Fixed,07/Feb/23 06:06,19/Apr/23 01:38
Bug,FLINK-30940,13523491,InterruptedException in ExecutorImplITCase which doesn't fail the test,"We're experiencing a test failure {{CliClientITCase.testSqlStatements}} which might be caused by an {{InterruptedException}}:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45828&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=46490

{code}
Exception in thread ""worker"" org.apache.flink.table.client.gateway.SqlExecutionException: Interrupted to fetch results.
	at org.apache.flink.table.client.gateway.ExecutorImpl.lambda$fetchUtilResultsReady$4(ExecutorImpl.java:375)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getFetchResultResponse(ExecutorImpl.java:398)
	at org.apache.flink.table.client.gateway.ExecutorImpl.fetchUtilResultsReady(ExecutorImpl.java:368)
	at org.apache.flink.table.client.gateway.ExecutorImpl.executeStatement(ExecutorImpl.java:234)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterruptExecution$7(ExecutorImplITCase.java:507)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterrupting$12(ExecutorImplITCase.java:607)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:347)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getFetchResultResponse(ExecutorImpl.java:396)
	... 5 more
Exception in thread ""worker"" org.apache.flink.table.client.gateway.SqlExecutionException: Interrupted to fetch results.
	at org.apache.flink.table.client.gateway.ExecutorImpl.lambda$fetchUtilResultsReady$4(ExecutorImpl.java:375)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getFetchResultResponse(ExecutorImpl.java:398)
	at org.apache.flink.table.client.gateway.ExecutorImpl.fetchUtilResultsReady(ExecutorImpl.java:368)
	at org.apache.flink.table.client.gateway.ExecutorImpl.executeStatement(ExecutorImpl.java:234)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterruptFetching$8(ExecutorImplITCase.java:515)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterrupting$12(ExecutorImplITCase.java:607)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:347)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getFetchResultResponse(ExecutorImpl.java:396)
	... 5 more
Exception in thread ""worker"" org.apache.flink.table.client.gateway.SqlExecutionException: Interrupted to get response.
	at org.apache.flink.table.client.gateway.ExecutorImpl.lambda$executeStatement$2(ExecutorImpl.java:228)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getResponse(ExecutorImpl.java:429)
	at org.apache.flink.table.client.gateway.ExecutorImpl.executeStatement(ExecutorImpl.java:210)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterruptSubmitting$6(ExecutorImplITCase.java:502)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterrupting$12(ExecutorImplITCase.java:607)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:347)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getResponse(ExecutorImpl.java:424)
	... 4 more
{code}",fsk119,mapohl,Critical,Closed,Fixed,07/Feb/23 11:50,09/Feb/23 12:01
Bug,FLINK-30942,13523499,Fix the bug that the decided parallelism by adaptive batch scheduler may be larger than the max parallelism,"Currently, when using the adaptive batch scheduler, the vertex parallelism decided by  forward group may be larger than the global max parallelism(which is configured by option {{parallelism.default}} or {{execution.batch.adaptive.auto-parallelism.max-parallelism}}, see FLINK-30686 for details), which will cause the following exception:

{code:java}
Caused by: java.lang.IllegalArgumentException: Vertex's parallelism should be smaller than or equal to vertex's max parallelism.
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
	at org.apache.flink.runtime.scheduler.DefaultVertexParallelismInfo.setParallelism(DefaultVertexParallelismInfo.java:95)
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.setParallelism(ExecutionJobVertex.java:317)
	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.changeJobVertexParallelism(AdaptiveBatchScheduler.java:385)
	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.initializeVerticesIfPossible(AdaptiveBatchScheduler.java:284)
	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.onTaskFinished(AdaptiveBatchScheduler.java:183)
	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:745)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
	... 30 more
{code}

Following code can reproduce the above exception:

{code:java}
final Configuration configuration = new Configuration();
configuration.setString(RestOptions.BIND_PORT, ""0"");
configuration.setLong(JobManagerOptions.SLOT_REQUEST_TIMEOUT, 5000L);
configuration.setInteger(
        BatchExecutionOptions.ADAPTIVE_AUTO_PARALLELISM_MAX_PARALLELISM, 2);
configuration.set(
        BatchExecutionOptions.ADAPTIVE_AUTO_PARALLELISM_AVG_DATA_VOLUME_PER_TASK,
        MemorySize.parse(""150kb""));
configuration.set(TaskManagerOptions.MEMORY_SEGMENT_SIZE, MemorySize.parse(""4kb""));
configuration.set(TaskManagerOptions.NUM_TASK_SLOTS, 1);

final StreamExecutionEnvironment env =
        StreamExecutionEnvironment.createLocalEnvironment(configuration);
env.setRuntimeMode(RuntimeExecutionMode.BATCH);
env.setParallelism(4);

final DataStream<Long> source =
        env.fromSequence(0, NUMBERS_TO_PRODUCE - 1)
                .setParallelism(4)
                .name(""source"")
                .slotSharingGroup(""group1"");

source.forward().map(new NumberCounter()).name(""map"").slotSharingGroup(""group2"");
env.execute();
{code}

",wanglijie,wanglijie,Major,Closed,Fixed,07/Feb/23 12:23,17/Feb/23 05:54
Bug,FLINK-30944,13523524,ExecutionGraphPartitionReleaseTest leaks threads,{{ExecutionGraphPartitionReleaseTest}} leaks threads through {{ExecutionGraphPartitionReleaseTest.scheduledExecutorService}}. The {{ScheduledExecutorService}} is instantiated but never shut down.,Weijie Guo,mapohl,Major,Resolved,Fixed,07/Feb/23 15:08,13/Feb/23 08:30
Bug,FLINK-30945,13523528,FTS does not support multiple writers into the same table and topic,"When creating two different streaming jobs that INSERT INTO the same table and kafka topic, the second job is never able to make progress as the transaction gets constantly aborted due to the producer getting fenced.

FTS should set the transactionalIdPrefix to avoid transactions of different jobs clashing.
{code:java}
2023-02-06 17:13:36,088 WARN org.apache.flink.runtime.taskmanager.Task [] - Writer -> Global Committer -> Sink: end (1/1)#0 (8cf4197af9716623c3c19e7fa3d7c071_b5c8d46f3e7b141acf271f12622e752b_0_0) switched from RUNNING to FAILED with failure cause: org.apache.flink.util.FlinkRuntimeException: Committing one of transactions failed, logging first encountered failure at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.notifyCheckpointComplete(TwoPhaseCommitSinkFunction.java:323) at org.apache.flink.table.store.connector.sink.StoreWriteOperator.notifyCheckpointComplete(StoreWriteOperator.java:175) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.notifyCheckpointComplete(StreamOperatorWrapper.java:104) at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.notifyCheckpointComplete(RegularOperatorChain.java:145) at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpoint(SubtaskCheckpointCoordinatorImpl.java:479) at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:413) at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:1412) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$15(StreamTask.java:1353) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$18(StreamTask.java:1392) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:383) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:345) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780) at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935) at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) at java.lang.Thread.run(Thread.java:750) Caused by: org.apache.flink.table.store.shaded.org.apache.kafka.common.errors.ProducerFencedException: There is a newer producer with the same transactionalId which fences the current one. {code}
Sample queries:
 
 
{code:java}
CREATE CATALOG table_store_catalog WITH (
    'type'='table-store',
    'warehouse'='s3://my-bucket/table-store'
 );
USE CATALOG table_store_catalog;
SET 'execution.checkpointing.interval' = '10 s';
CREATE TABLE word_count_kafka (
     word STRING PRIMARY KEY NOT ENFORCED,
     cnt BIGINT
 ) WITH (
     'log.system' = 'kafka',
     'kafka.bootstrap.servers' = 'broker:9092',
     'kafka.topic' = 'word_count_log'
 );
CREATE TEMPORARY TABLE word_table (
     word STRING
 ) WITH (
     'connector' = 'datagen',
     'fields.word.length' = '1'
 );
{code}
 

And the two INSERT jobs:
{code:java}
INSERT INTO word_count_kafka SELECT word, COUNT(*) FROM word_table GROUP BY word;{code}",,vicky_papavas,Major,Closed,Fixed,07/Feb/23 15:43,29/Mar/23 03:06
Bug,FLINK-30958,13523715,Rest API doc generation failure caused by JobClientHeartbeatHeaders,`JobClientHeartbeatHeaders` should override `operationId` since `getHttpMethod` returns `POST`. Otherwise `UnsupportedOperationException` is thrown at `OpenApiSpecGenerator` when generating the REST API doc.,qingyue,qingyue,Major,Closed,Fixed,08/Feb/23 06:34,08/Feb/23 10:56
Bug,FLINK-30964,13523761,flink-mirror repo sync release branches failed,"we use https://github.com/flink-ci/git-repo-sync/blob/master/sync_repo.sh to sync the master the latest 2 release-X branches  from apache/flink to flink-ci/flink-mirror.  
 but the scripts https://github.com/flink-ci/git-repo-sync/blob/master/sync_repo.sh exists a wrong condition judgement which lead the release-1.17 sync fail.",leonard,leonard,Critical,Resolved,Fixed,08/Feb/23 12:38,08/Feb/23 13:40
Bug,FLINK-30965,13523765,git-repo-sync doesn't pick up the 2nd-most-recently published Flink version (even though it's still supported) after a new release branch is cut,"I noticed that we're always synchronizing the most-recent release branches (see [flink-ci/git-repo-sync:git-repo-synch:27|https://github.com/flink-ci/git-repo-sync/blob/7c0c2ed4b8f1cdf343e75021ca89e9dcc9421b93/sync_repo.sh#L27]). That means that we stop running CI on the Flink version that is soon to be deprecated after a new release branch is cut, e.g.: 1.16 & 1.15 are supported and we cut {{release-1.17}} to start the release process for 1.17. From this point onwards, we are synchronizing 1.17 and 1.16 but skip 1.15 eventhough 1.15 is still supported.",mapohl,mapohl,Major,Resolved,Fixed,08/Feb/23 13:18,16/Feb/23 09:39
Bug,FLINK-30966,13523766,Flink SQL IF FUNCTION logic error,"my data is 
{code:java}
//
{ ""before"": { ""status"": ""sent"" }, ""after"": { ""status"": ""succeed"" }, ""op"": ""u"", ""ts_ms"": 1671926400225, ""transaction"": null } {code}
my sql is 

 
{code:java}
CREATE TABLE t
(
    before ROW (

        status varchar (32)

        ),
    after ROW (

        status varchar (32)

        ),
    ts_ms                bigint,
    op                   string,
    kafka_timestamp      timestamp METADATA FROM 'timestamp',
--     @formatter:off
    proctime AS PROCTIME()
--     @formatter:on
) WITH (
    'connector' = 'kafka',
--     'topic' = '',
    'topic' = 'test',
    'properties.bootstrap.servers' = ' ',
    'properties.group.id' = '',
    'format' = 'json',
    'scan.topic-partition-discovery.interval' = '60s',
    'scan.startup.mode' = 'earliest-offset',
    'json.ignore-parse-errors' = 'true'
 );
create table p
(
    status                  STRING ,
    before_status                  STRING ,
    after_status                  STRING ,
    metadata_operation      STRING COMMENT '源记录操作类型',
    dt                      STRING
)WITH (
    'connector' = 'print'
 );
INSERT INTO p
SELECT
       IF(op <> 'd', after.status, before.status),
        before.status,
        after.status,
       op                                         AS metadata_operation,
       DATE_FORMAT(kafka_timestamp, 'yyyy-MM-dd') AS dt
FROM t;

 {code}
 my local env output is 

 

 
{code:java}
+I[null, sent, succeed, u, 2023-02-08] {code}
 

 my produtionc env output is 
{code:java}
+I[sent, sent, succeed, u, 2023-02-08]  {code}
why?  
This look like a bug.

 ",csq,hiscat,Major,Resolved,Fixed,08/Feb/23 13:20,06/Jun/23 02:56
Bug,FLINK-30969,13523860,"Pyflink table example throws ""module 'pandas' has no attribute 'Int8Dtype'""","After apache-beam is upgraded to 2.43.0 in 1.17, running `python pyflink/examples/table/basic_operations.py` will throw error:
{code:java}
Traceback (most recent call last):
  File ""pyflink/examples/table/basic_operations.py"", line 484, in <module>
    basic_operations()
  File ""pyflink/examples/table/basic_operations.py"", line 29, in basic_operations
    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/table/table_environment.py"", line 121, in create
    return TableEnvironment(j_tenv)
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/table/table_environment.py"", line 100, in __init__
    self._open()
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/table/table_environment.py"", line 1640, in _open
    startup_loopback_server()
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/table/table_environment.py"", line 1631, in startup_loopback_server
    from pyflink.fn_execution.beam.beam_worker_pool_service import \
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/fn_execution/beam/beam_worker_pool_service.py"", line 31, in <module>
    from apache_beam.options.pipeline_options import DebugOptions
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/apache_beam/__init__.py"", line 92, in <module>
    from apache_beam import coders
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>
    from apache_beam.coders.coders import *
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/apache_beam/coders/coders.py"", line 59, in <module>
    from apache_beam.coders import coder_impl
  File ""apache_beam/coders/coder_impl.py"", line 63, in init apache_beam.coders.coder_impl
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/apache_beam/typehints/__init__.py"", line 31, in <module>
    from apache_beam.typehints.pandas_type_compatibility import *
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/apache_beam/typehints/pandas_type_compatibility.py"", line 81, in <module>
    (pd.Int8Dtype(), Optional[np.int8]),
AttributeError: module 'pandas' has no attribute 'Int8Dtype' {code}",Juntao Hu,Juntao Hu,Major,Closed,Fixed,09/Feb/23 02:52,13/Feb/23 03:43
Bug,FLINK-30971,13523871,Modify the default value of parameter 'table.exec.local-hash-agg.adaptive.sampling-threshold',"In our test environment, we set the default parallelism to  1 and got the most appropriate default value of parameter 'table.exec.local-hash-agg.adaptive.sampling-threshold'  is 5000000. However, for these batch jobs with high parallelism in produce environment,  the amount of data in single parallelism is almost less than 5000000. Therefore, after testing, we found that set to 500000 can get better results.",337361684@qq.com,337361684@qq.com,Major,Closed,Fixed,09/Feb/23 03:55,02/Mar/23 09:52
Bug,FLINK-30972,13523877,"E2e tests always fail in phase ""Prepare E2E run""","{code:java}
Installing required software
Reading package lists...
Building dependency tree...
Reading state information...
bc is already the newest version (1.07.1-2build1).
bc set to manually installed.
libapr1 is already the newest version (1.6.5-1ubuntu1).
libapr1 set to manually installed.
0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.
--2023-02-09 04:38:47--  http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.deb
Resolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...
Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2023-02-09 04:38:47 ERROR 404: Not Found.


WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

Reading package lists...
E: Unsupported file ./libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.deb given on commandline
##[error]Bash exited with code '100'.
Finishing: Prepare E2E run
{code}",renqs,wanglijie,Blocker,Resolved,Fixed,09/Feb/23 05:27,06/Mar/23 09:57
Bug,FLINK-30976,13523892,docs_404_check fails occasionally,"We've seen the docs_404_check failing in nightly builds (only the cron stage but not the ci stage):
{code}
Re-run Hugo with the flag --panicOnWarning to get a better error message.
ERROR 2023/02/09 01:27:27 ""docs/connectors/datastream/pulsar.md"": Invalid use of artifact shortcode. Unknown flag `4.0.0-SNAPSHOT`
ERROR 2023/02/09 01:27:34 ""docs/connectors/datastream/pulsar.md"": Invalid use of artifact shortcode. Unknown flag `4.0.0-SNAPSHOT`
Error: Error building site: logged 2 error(s)
Total in 12945 ms
Error building the docs
{code}
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45909&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&t=ddd6d61a-af16-5d03-2b9a-76a279badf98&l=133
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45906&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&t=ddd6d61a-af16-5d03-2b9a-76a279badf98&l=132",mapohl,mapohl,Major,Resolved,Fixed,09/Feb/23 06:48,09/Feb/23 15:03
Bug,FLINK-30977,13523893,flink tumbling window stream converting to pandas dataframe not work,"I want to know if tumbling window supported to convert to pandas?
{code:java}
code... #create env

kafka_src = """"""
CREATE TABLE if not exists `kafka_src` (
...
`event_time` as CAST(`end_time` as TIMESTAMP(3)),
WATERMARK FOR event_time as event_time - INTERVAL '5' SECOND
)
with (
'connector' = 'kafka',
'topic' = 'topic',
'properties.bootstrap.servers' = '***',
'properties.group.id' = '***',
'scan.startup.mode' = 'earliest-offset',
'value.format' = 'debezium-json'
);
""""""  
  
t_env.execute_sql(kafka_src)
table = st_env.sql_query(""SELECT columns,`event_time`  \
    FROM TABLE(TUMBLE(TABLE table_name, DESCRIPTOR(event_time), INTERVAL '1' MINUTES))"")

table.execute().print()  #could print the result

df = table.to_pandas()

#schema is correct!
schema = DataTypes.ROW([DataTypes.FIELD(""column1"", DataTypes.STRING()),
                        .......
                            ])
table = st_env.from_pandas(df,schema=schema)
st_env.create_temporary_view(""view_table"",table)

st_env.sql_query(""select * from view_table"").execute().print() # Not work!Can't print the result {code}
Tumbling window stream from kafka source convert to pandas dataframe and it can't print the result.The schema is right.I have tested in another job with using batch stream from jdbc source.It can print the result.The only different thing is the input stream.Is tumbling windows supported to convert to Pandas?",,Joekwal,Major,Closed,Fixed,09/Feb/23 06:50,02/Mar/23 09:04
Bug,FLINK-30978,13523894,ExecutorImplITCase.testInterruptExecution hangs waiting for SQL gateway service closing,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45921&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=44674,fsk119,renqs,Major,Closed,Fixed,09/Feb/23 06:52,06/Mar/23 10:17
Bug,FLINK-30981,13523900,explain_sql throws java method not exist,"Execute `t_env.explainSql(""ANY VALID SQL"")` will throw error:
{code:java}
Traceback (most recent call last):
  File ""ISSUE/FLINK-25622.py"", line 42, in <module>
    main()
  File ""ISSUE/FLINK-25622.py"", line 34, in main
    print(t_env.explain_sql(
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/table/table_environment.py"", line 799, in explain_sql
    return self._j_tenv.explainSql(stmt, j_extra_details)
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1322, in __call__
    return_value = get_return_value(
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/util/exceptions.py"", line 146, in deco
    return f(*a, **kw)
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/py4j/protocol.py"", line 330, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o11.explainSql. Trace:
org.apache.flink.api.python.shaded.py4j.Py4JException: Method explainSql([class java.lang.String, class [Lorg.apache.flink.table.api.ExplainDetail;]) does not exist
    at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)
    at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)
    at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:274)
    at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
    at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.base/java.lang.Thread.run(Thread.java:829) {code}
[30668|https://issues.apache.org/jira/browse/FLINK-30668] changed TableEnvironment#explainSql to an interface default method, while both TableEnvironmentInternal and TableEnvironmentImpl not overwriting it, it triggers a bug in py4j, see [https://github.com/py4j/py4j/issues/506] .",Juntao Hu,Juntao Hu,Major,Closed,Fixed,09/Feb/23 07:19,13/Feb/23 03:38
Bug,FLINK-30983,13523913,the security.ssl.algorithms configuration does not take effect in rest ssl,"The security.ssl.algorithms configuration does not take effect in rest ssl.

 

SSLUtils#createRestNettySSLContext does not call SslContextBuilder#ciphers as  SSLUtils#createInternalNettySSLContext.

!image-2023-02-09-15-58-36-254.png!

 

!image-2023-02-09-15-58-43-963.png!",tanyuxin,lyssg,Minor,Closed,Fixed,09/Feb/23 07:59,08/Mar/23 03:36
Bug,FLINK-30989,13523959,Configuration table.exec.spill-compression.block-size not take effect in batch job,"h1. Description

I tried to config table.exec.spill-compression.block-size in TableEnv in my job and failed. I  attached to TaskManager and found conf passed to constructor of [BinaryExternalSorter|https://github.com/apache/flink/blob/release-1.16.1/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sort/BinaryExternalSorter.java#L204] is empty:

!image-2023-02-09-19-37-44-927.png|width=306,height=185!
h1. How to reproduce

A simple code to reproduce this problem:
{code:java}
// App.java

package test.flink403;

import static org.apache.flink.configuration.ExecutionOptions.RUNTIME_MODE;
import static org.apache.flink.table.api.config.ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_BLOCK_SIZE;

import org.apache.flink.api.common.RuntimeExecutionMode;
import org.apache.flink.configuration.AlgorithmOptions;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.table.api.config.ExecutionConfigOptions;

import java.util.Arrays; public class App {

  public static void main(String argc[]) throws Exception {

    Configuration config = new Configuration();
    config.set(RUNTIME_MODE, RuntimeExecutionMode.BATCH);
    config.set(ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_ENABLED, true);
    config.set(AlgorithmOptions.HASH_JOIN_BLOOM_FILTERS, true);
    config.setString(TABLE_EXEC_SPILL_COMPRESSION_BLOCK_SIZE.key(), ""32 m""); // <---- cannot take effect
    config.set(AlgorithmOptions.SORT_SPILLING_THRESHOLD, Float.valueOf(0.5f));
    final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(1, config);

    final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);
    tableEnv.getConfig().set(""table.exec.spill-compression.block-size"", ""32 m""); // <---- cannot take effect
    final DataStream<Order> orderA =
        env.fromCollection(
            Arrays.asList(
                new Order(1L, ""beer"", 3),
                new Order(1L, ""diaper"", 4),
                new Order(3L, ""rubber"", 2)));

    final Table tableA = tableEnv.fromDataStream(orderA);

    final Table result =
        tableEnv.sqlQuery(
            ""SELECT * FROM ""
                + tableA
                + "" ""
                + "" order by user"");

    tableEnv.toDataStream(result, Order.class).print();
    env.execute();
  }
}

// ---------------------------------------------------------------
// Order.java
package test.flink403;

public class Order {
  public Long user;
  public String product;
  public int amount;

  // for POJO detection in DataStream API
  public Order() {}

  // for structured type detection in Table API
  public Order(Long user, String product, int amount) {
    this.user = user;
    this.product = product;
    this.amount = amount;
  }

  @Override
  public String toString() {
    return ""Order{""
        + ""user=""
        + user
        + "", product='""
        + product
        + '\''
        + "", amount=""
        + amount
        + '}';
  }
}{code}
 

I think it is because [SortOperator|https://github.com/apache/flink/blob/release-1.16.1/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sort/SortOperator.java#L88] try to get conf from JobConfiguration, which should be set in JobGraph. 
Following are the Classes use the same method to get conf from JobConfiguration:
 * BinaryExternalSorter
 ** ExecutionConfigOptions.TABLE_EXEC_SORT_ASYNC_MERGE_ENABLED
 ** ExecutionConfigOptions.TABLE_EXEC_SORT_MAX_NUM_FILE_HANDLES
 ** ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_ENABLED
 ** ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_BLOCK_SIZE
 * BinaryHashTable，BaseHybridHashTable
 ** ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_ENABLED
 ** ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_BLOCK_SIZE
 * SortDataInput
 ** AlgorithmOptions.SORT_SPILLING_THRESHOLD
 ** AlgorithmOptions.SPILLING_MAX_FAN
 ** AlgorithmOptions.USE_LARGE_RECORDS_HANDLER",lsy,shenjiaqi,Major,Closed,Fixed,09/Feb/23 11:59,07/Apr/23 08:07
Bug,FLINK-31008,13524166,[Flink][Table Store] The Split allocation of the same bucket in ContinuousFileSplitEnumerator may be out of order,"There are two places in {{ContinuousFileSplitEnumerator}} that add {{FileStoreSourceSplit}} to {{{}bucketSplits{}}}: {{addSplitsBack}} and {{{}processDiscoveredSplits{}}}. {{processDiscoveredSplits}} will continuously check for new splits and add them to the queue.  At this time, the order of the splits is in order.
{code:java}
private void addSplits(Collection<FileStoreSourceSplit> splits) {
    splits.forEach(this::addSplit);
}

private void addSplit(FileStoreSourceSplit split) {
    bucketSplits
            .computeIfAbsent(((DataSplit) split.split()).bucket(), i -> new LinkedList<>())
            .add(split);
}{code}
However, when the task failover, the splits that have been allocated before will be returned. At this time, these returned splits are also added to the end of the queue, which leads to disorder in the allocation of splits.

 

I think these returned splits should be added to the head of the queue to ensure the order of allocation.",Ming Li,Ming Li,Major,Closed,Fixed,10/Feb/23 07:27,13/Feb/23 11:31
Bug,FLINK-31012,13524187,Update docs for files table in table store,Update docs to add partition,zjureel,zjureel,Major,Closed,Fixed,10/Feb/23 09:44,10/Feb/23 10:42
Bug,FLINK-31013,13524188,Session window aggregation cannot trigger window using event time,"{code:sql}
-- test against Flink 1.16.0

create catalog fscat with (
    'type' = 'table-store',
    'warehouse' = 'file:///tmp/fscat'
);


use catalog fscat;
create table events (
  `id` int, 
  `type` string, 
  `date` TIMESTAMP(3), 
  watermark for `date` AS `date`);
  
insert into events 
values (1, 'T1', to_timestamp('2018-01-24', 'yyyy-MM-dd')), 
(2, 'T1', to_timestamp('2018-01-26', 'yyyy-MM-dd')), 
(1, 'T2', to_timestamp('2018-01-28', 'yyyy-MM-dd')), 
(1, 'T2', to_timestamp('2018-01-28', 'yyyy-MM-dd'));  

-- no output
select `id`,
    `type`, 
    COUNT(1) as event_cnt, 
    session_start(`date`, interval '1' DAY) as ss, 
    session_end(`date`, interval '1' DAY) as se 
from events group by `id`, `type`, session(`date`, interval '1' DAY); 

-- explain plan
== Abstract Syntax Tree ==
LogicalProject(id=[$0], type=[$1], event_cnt=[$3], ss=[SESSION_START($2)], se=[SESSION_END($2)])
+- LogicalAggregate(group=[{0, 1, 2}], event_cnt=[COUNT()])
   +- LogicalProject(id=[$0], type=[$1], $f2=[$SESSION($2, 86400000:INTERVAL DAY)])
      +- LogicalWatermarkAssigner(rowtime=[date], watermark=[$2])
         +- LogicalTableScan(table=[[fscat, default, events]])


== Optimized Physical Plan ==
Calc(select=[id, type, event_cnt, w$start AS ss, w$end AS se])
+- GroupWindowAggregate(groupBy=[id, type], window=[SessionGroupWindow('w$, date, 86400000)], properties=[w$start, w$end, w$rowtime, w$proctime], select=[id, type, COUNT(*) AS event_cnt, start('w$) AS w$start, end('w$) AS w$end, rowtime('w$) AS w$rowtime, proctime('w$) AS w$proctime])
   +- Exchange(distribution=[hash[id, type]])
      +- TableSourceScan(table=[[fscat, default, events, watermark=[date]]], fields=[id, type, date])


== Optimized Execution Plan ==
Calc(select=[id, type, event_cnt, w$start AS ss, w$end AS se])
+- GroupWindowAggregate(groupBy=[id, type], window=[SessionGroupWindow('w$, date, 86400000)], properties=[w$start, w$end, w$rowtime, w$proctime], select=[id, type, COUNT(*) AS event_cnt, start('w$) AS w$start, end('w$) AS w$end, rowtime('w$) AS w$rowtime, proctime('w$) AS w$proctime])
   +- Exchange(distribution=[hash[id, type]])
      +- TableSourceScan(table=[[fscat, default, events, watermark=[date]]], fields=[id, type, date])

-- however, if switch to filesystem source, the window can be triggered normally

CREATE TEMPORARY TABLE `fscat`.`default`.`event_file_source` (
  `id` INT,
  `type` VARCHAR(2147483647),
  `date` TIMESTAMP(3),
  WATERMARK FOR `date` AS `date`
) WITH (
  'format' = 'csv',
  'path' = '/tmp/events.csv',
  'source.monitor-interval' = '1 min',
  'connector' = 'filesystem'
);

// cat events.csv                                
1,T1,2018-01-24 00:00:00.000
2,T1,2018-01-26 00:00:00.000
1,T2,2018-01-28 00:00:00.000
1,T2,2018-01-28 00:00:00.000


-- same query using filesystem source
select `id`, `type`, COUNT(1) as event_cnt, session_start(`date`, interval '1' DAY) as ss, session_end(`date`, interval '1' DAY) as se from event_file_source group by `id`, `type`, session(`date`, interval '1' DAY);

-- output

          id                           type            event_cnt                      ss                      se
           1                             T1                    1 2018-01-24 00:00:00.000 2018-01-25 00:00:00.000
           2                             T1                    1 2018-01-26 00:00:00.000 2018-01-27 00:00:00.000{code}",,qingyue,Major,Closed,Fixed,10/Feb/23 09:52,29/Mar/23 02:00
Bug,FLINK-31017,13524233,Early-started partial match timeout not yield completed matches,"Pattern example:
{code:java}
Pattern.begin(""A"").where(startsWith(""a"")).oneOrMore().consecutive().greedy()
    .followedBy(""B"")
    .where(count(""A"") > 2 ? startsWith(""b"") : startsWith(""c""))
    .within(Time.seconds(3));{code}
Sequence example, currently without any output:

a1 a2 a3 a4 c1

When match[a3, a4, c1] completes, partial match[a1, a2, a3, a4] is earlier, so NFA#processMatchesAccordingToSkipStrategy() won't give any result, which is the expected behavior. However, when partial match[a1, a2, a3, a4] is timed-out, completed match[a3, a4, c1] should be ""freed"" from NFAState to output.",Juntao Hu,Juntao Hu,Major,Closed,Fixed,10/Feb/23 14:35,17/Feb/23 08:29
Bug,FLINK-31026,13524391,KBinsDiscretizer gives wrong bin edges when all values are same.,"Current implements gives bin edges of \{Double.MIN_VALUE, Double.MAX_VALUE} when all values are same.
However, this bin cannot cover negative values and 0.",hongfanxo,hongfanxo,Major,Resolved,Fixed,13/Feb/23 04:02,16/Feb/23 11:38
Bug,FLINK-31029,13524405,KBinsDiscretizer gives wrong bin edges in 'quantile' strategy when input data contains only 2 distinct values,"When one input column contains only 2 distinct values and their counts are same, KBinsDiscretizer transforms this column to all 0s using `quantile` strategy. An example of such column is `[0, 0, 0, 1, 1, 1]`.

When the 2 distinct values have different counts, the transformed values are also all 0s, which cannot distinguish them.",zhangzp,hongfanxo,Major,Resolved,Fixed,13/Feb/23 06:23,01/Apr/23 02:52
Bug,FLINK-31036,13524431,StateCheckpointedITCase timed out,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10608

{code}
""Legacy Source Thread - Source: Custom Source -> Filter (6/12)#69980"" #13718026 prio=5 os_prio=0 tid=0x00007f05f44f0800 nid=0x128157 waiting on condition [0x00007f059feef000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f0a974e8> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking(LocalBufferPool.java:384)
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilderBlocking(LocalBufferPool.java:356)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewBufferBuilderFromPool(BufferWritingResultPartition.java:414)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewUnicastBufferBuilder(BufferWritingResultPartition.java:390)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.appendUnicastDataForRecordContinuation(BufferWritingResultPartition.java:328)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.emitRecord(BufferWritingResultPartition.java:161)
	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:107)
	at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:55)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:105)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:91)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:45)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:59)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:31)
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:39)
	at org.apache.flink.streaming.runtime.io.RecordProcessorUtils$$Lambda$1311/1256184070.accept(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	- locked <0x00000000d55035c0> (a java.lang.Object)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.test.checkpointing.StateCheckpointedITCase$StringGeneratingSourceFunction.run(StateCheckpointedITCase.java:178)
	- locked <0x00000000d55035c0> (a java.lang.Object)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
{code}",fanrui,mapohl,Major,Closed,Fixed,13/Feb/23 09:22,27/Feb/23 10:33
Bug,FLINK-31038,13524447,Avoid accessing non-TableStore tables in HiveCatalog.listTables,"In HiveCatalog.listTables, in the current implementation, getTable will be called for each TableName. However, the environment here may not be able to access non-TableStore tables.
We can avoid access non-TableStore tables by judging whether it is a TableStore table in advance.",lzljs3620320,lzljs3620320,Major,Closed,Fixed,13/Feb/23 10:23,13/Feb/23 12:17
Bug,FLINK-31039,13524456,ChangelogWithKeyFileStoreTableITCase in table store is not stable,"FAILURE! - in org.apache.flink.table.store.connector.ChangelogWithKeyFileStoreTableITCase
Error:  testFullCompactionChangelogProducerStreamingRandom  Time elapsed: 600.077 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 600000 milliseconds
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sleepBeforeRetry(CollectResultFetcher.java:244)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:114)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
	at org.apache.flink.table.store.connector.ChangelogWithKeyFileStoreTableITCase.checkFullCompactionTestResult(ChangelogWithKeyFileStoreTableITCase.java:395)
	at org.apache.flink.table.store.connector.ChangelogWithKeyFileStoreTableITCase.testFullCompactionChangelogProducerRandom(ChangelogWithKeyFileStoreTableITCase.java:343)
	at org.apache.flink.table.store.connector.ChangelogWithKeyFileStoreTableITCase.testFullCompactionChangelogProducerStreamingRandom(ChangelogWithKeyFileStoreTableITCase.java:300)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:750)

[INFO] 
[INFO] Results:
[INFO] 
Error:  Errors: 
Error:    ChangelogWithKeyFileStoreTableITCase.testFullCompactionChangelogProducerStreamingRandom:300->testFullCompactionChangelogProducerRandom:343->checkFullCompactionTestResult:395 » TestTimedOut

https://github.com/apache/flink-table-store/actions/runs/4161755735/jobs/7200106408",,zjureel,Major,Closed,Fixed,13/Feb/23 10:52,29/Mar/23 02:03
Bug,FLINK-31041,13524459,Build up of pending global failures causes JM instability,"h4. Context

When a job creates multiple sources that use the {{SourceCoordinator}} (FLIP-27), there is a failure race condition that result in a ""leak"" of ExecutionVertextVersion due to a ""queue"" of pending global failures. 

This results in the Job Manager becoming unresponsive.
h4. !flink-31041-heap-dump.png!
h4. Reproduction Steps

This can be reproduced by a job that creates multiple sources that fail in the {{{}SplitEnumerator{}}}. We observed this with multiple {{KafkaSource's}} trying to load a non-existent cert from the file system and throwing FNFE. Thus, here is a simple job to reproduce (BE WARNED: running this locally will lock up your IDE):
{code:java}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setParallelism(1);
env.setRestartStrategy(new RestartStrategies.FailureRateRestartStrategyConfiguration(10000, Time.of(10, TimeUnit.SECONDS), Time.of(10, TimeUnit.SECONDS)));

KafkaSource<String> source = KafkaSource.<String>builder()
        .setProperty(""security.protocol"", ""SASL_SSL"")
        // SSL configurations
        // Configure the path of truststore (CA) provided by the server
        .setProperty(""ssl.truststore.location"", ""/path/to/kafka.client.truststore.jks"")
        .setProperty(""ssl.truststore.password"", ""test1234"")
        // Configure the path of keystore (private key) if client authentication is required
        .setProperty(""ssl.keystore.location"", ""/path/to/kafka.client.keystore.jks"")
        .setProperty(""ssl.keystore.password"", ""test1234"")
        // SASL configurations
        // Set SASL mechanism as SCRAM-SHA-256
        .setProperty(""sasl.mechanism"", ""SCRAM-SHA-256"")
        // Set JAAS configurations
        .setProperty(""sasl.jaas.config"", ""org.apache.kafka.common.security.scram.ScramLoginModule required username=\""username\"" password=\""password\"";"")
        .setBootstrapServers(""http://localhost:3456"")
        .setTopics(""input-topic"")
        .setGroupId(""my-group"")
        .setStartingOffsets(OffsetsInitializer.earliest())
        .setValueOnlyDeserializer(new SimpleStringSchema())
        .build();

List<SingleOutputStreamOperator<String>> sources = IntStream.range(0, 32)
        .mapToObj(i -> env
                .fromSource(source, WatermarkStrategy.noWatermarks(), ""Kafka Source "" + i).uid(""source-"" + i)
                .keyBy(s -> s.charAt(0))
                .map(s -> s))
        .collect(Collectors.toList());

env.fromSource(source, WatermarkStrategy.noWatermarks(), ""Kafka Source"").uid(""source"")
        .keyBy(s -> s.charAt(0))
        .union(sources.toArray(new SingleOutputStreamOperator[] {}))
        .print();

env.execute(""test job""); {code}
h4. Root Cause

We can see that the {{OperatorCoordinatorHolder}} already has a [debounce mechanism|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/operators/coordination/OperatorCoordinatorHolder.java#L609], however the {{DefaultScheduler}} does not. We need a debounce mechanism in the {{DefaultScheduler}} since it handles many {{{}OperatorCoordinatorHolder{}}}.
h4. Fix

I have managed to fix this, I will open a PR, but would need feedback from people who understand this code better than me!

 

 ",huwh,dannycranmer,Critical,Resolved,Fixed,13/Feb/23 11:09,23/Feb/23 08:48
Bug,FLINK-31042,13524460,AfterMatchSkipStrategy not working on notFollowedBy ended pattern,"Pattern: begin(""A"", SkipToNext()).oneOrMore().allowCombinations().followedBy(""C"").notFollowedBy(""B"").within(Time.milliseconds(10L))

Sequence: <a1, 1L> <a2, 2L> <a3, 3L> <c1, 4L> will produce

[a1, a2, a3, c1]

[a1, a2, c1]

[a1, c1]

[a2, a3, c1]

[a2, c1]

[a3, c1]

Using SkipPastLastEvent() also produce the same result.",Juntao Hu,Juntao Hu,Major,Closed,Fixed,13/Feb/23 11:17,17/Feb/23 08:28
Bug,FLINK-31043,13524463,KeyError exception is thrown in CachedMapState,"Have seen the following exception in a PyFlink job which runs in Flink 1.15. It happens occasionally and may indicate a bug of the state cache of MapState:
{code:java}
Caused by: java.lang.RuntimeException: Error received from SDK harness for instruction 131: Traceback (most recent call last):
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 289, in _execute
    response = task()
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 362, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 607, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 644, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1000, in process_bundle
    element.data)
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 228, in process_encoded
    self.output(decoded_value)
  File ""apache_beam/runners/worker/operations.py"", line 357, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 359, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 221, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 158, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
  File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 170, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/table/operations.py"", line 417, in finish_bundle
    return self.group_agg_function.finish_bundle()
  File ""pyflink/fn_execution/table/aggregate_fast.pyx"", line 597, in pyflink.fn_execution.table.aggregate_fast.GroupTableAggFunction.finish_bundle
  File ""pyflink/fn_execution/table/aggregate_fast.pyx"", line 652, in pyflink.fn_execution.table.aggregate_fast.GroupTableAggFunction.finish_bundle
  File ""pyflink/fn_execution/table/aggregate_fast.pyx"", line 389, in pyflink.fn_execution.table.aggregate_fast.SimpleTableAggsHandleFunction.emit_value
  File ""/tmp/pyflink/17360444-8c0b-46a5-90a4-689c376ea4ed/0e2967b5-181c-4663-bd7a-267d47509cf5/whms_dws_stock_python_sps_1_output.py"", line 29, in emit_value
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/table/state_data_view.py"", line 147, in get
    return self._map_state.get(key)
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 915, in get
    return self.get_internal_state().get(key)
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 773, in get
    self._state_key, map_key, self._map_key_encoder, self._map_value_decoder)
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 418, in blocking_get
    cached_map_state.put(map_key, (exists, value))
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 319, in put
    super(CachedMapState, self).put(key, exists_and_value)
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 68, in put
    self._on_evict(name, value)
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 305, in on_evict
    self._cached_keys.remove(key)
KeyError: 'SPAREPARTS_M11F010L4L1_01'
{code}",hxb,dianfu,Major,Closed,Fixed,13/Feb/23 11:50,13/Feb/23 12:13
Bug,FLINK-31048,13524506,Java doc of PulsarSourceBuilder was not updated in time,"The java doc of `PulsarSourceBuilder` was not updated in time.

IIUC, 
{code:java}
setDeserializationSchema(PulsarDeserializationSchema.flinkSchema(new SimpleStringSchema())) {code}
should be replaced by
{code:java}
setDeserializationSchema(new SimpleStringSchema()) {code}
BTW, I also checked `pulsar.md` and luckily it was correct.",Weijie Guo,Weijie Guo,Major,Closed,Fixed,13/Feb/23 17:00,14/Feb/23 03:04
Bug,FLINK-31055,13524605,The dynamic flag of stream graph does not take effect when translating the transformations,"Currently, the dynamic flag of stream graph is not set when [translate transformations|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraphGenerator.java#L324]. However, the dynamic flag will be used ([here|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraph.java#L696]) when translating, we should set the dynamic flag before the translating.",wanglijie,wanglijie,Major,Closed,Fixed,14/Feb/23 07:12,16/Feb/23 15:16
Bug,FLINK-31063,13524649,Prevent duplicate reading when restoring from a checkpoint.,"Exact-once semantics may not be guaranteed at present on partial reads.
We use a number fetchSize to limit the records count for every fetch loop but we didn't record the offset into the split state. When resuming the split reader from a partially completed split, we may re-read some data.

We should record the current reading offset into split state.
Skip this offset when restoring to prevent duplicate reading.",jiabao.sun,jiabao.sun,Major,Closed,Fixed,14/Feb/23 10:48,20/Feb/23 10:22
Bug,FLINK-31067,13524680,Add pull request template for flink-connector-pulsar,,Weijie Guo,Weijie Guo,Major,Closed,Fixed,14/Feb/23 14:52,15/Feb/23 06:20
Bug,FLINK-31077,13524769,Trigger checkpoint failed but it were shown as COMPLETED by rest API,"Currently, we can trigger a checkpoint and poll the status of the checkpoint until it is finished by rest according to FLINK-27101. However, even if the checkpoint status returned by rest is completed, it does not mean that the checkpoint is really completed. If an exception occurs after marking the pendingCheckpoint completed([here|https://github.com/apache/flink/blob/bf0ad52cbcb052961c54c94c7013f5ac0110ef8a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L1309]), the checkpoint is not written to the HA service and we can not failover from this checkpoint.",JunRuiLi,JunRuiLi,Major,Closed,Fixed,15/Feb/23 06:09,27/Feb/23 07:09
Bug,FLINK-31081,13524792,Update Jira links in How To Contribute guide,"FLINK-30007 added a description in [the community docs|https://flink.apache.org/community.html#issue-tracker] on how to get access to the Flink Jira. But several other locations mentioned and link Jira as well (especially in the how to contribute sections). Newcomers might be miss the paragraph in community and wonder how they could get a working Jira account.

This issue is about replacing all the Jira links (where it's useful) by a reference to the issue track section in the community docs (https://flink.apache.org/community.html#issue-tracker). That way, they are redirected to the information on how they can gain access to the Flink jira board.

Locations that needs to be updated are (not exclusively!):
* https://flink.apache.org/contributing/contribute-code.html
* https://flink.apache.org/gettinghelp.html
*  https://flink.apache.org/contributing/how-to-contribute.html",ericbrzezenski,mapohl,Major,Resolved,Fixed,15/Feb/23 08:13,27/Mar/23 10:51
Bug,FLINK-31082,13524793,Setting maven property 'flink.resueForks' to false in table planner module ,"This issue is created to alleviate the OOM problem mentioned in issue: https://issues.apache.org/jira/browse/FLINK-18356

Setting maven property 'flink.resueForks' to false in table planner module can only reduce the frequency of oom, but can't solve this problem. To completely solve this problem, we need to identify the specific reasons, but this is a time-consuming work.",337361684@qq.com,337361684@qq.com,Major,Closed,Fixed,15/Feb/23 08:36,01/Mar/23 12:26
Bug,FLINK-31083,13524805,Python ProcessFunction with OutputTag cannot be reused,"{code:java}
output_tag = OutputTag(""side"", Types.STRING())

def udf(i):
    yield output_tag, i

ds1.map(udf).get_side_output(output_tag)
ds2.map(udf){code}
raises TypeError: cannot pickle '_thread.RLock' object",Juntao Hu,Juntao Hu,Major,Closed,Fixed,15/Feb/23 09:30,15/Feb/23 11:28
Bug,FLINK-31091,13524862,SQL interval related queries stop working via SQL client,"I put blocker since it works in 1.16.x and stopped working in 1.17 after a certain commit

Any interval related query run via SQL Client is failing with 

{noformat}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.runtime.rest.util.RestClientException: [Internal server error. Could not map response to JSON.]
	at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:536)
	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:516)
	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

{noformat}

example of query
{code:sql}
SELECT INTERVAL '2' DAY;
SELECT 1, INTERVAL '2' YEAR;
{code}

based on tests it stopped working after this commit 
https://issues.apache.org/jira/browse/FLINK-29945

More traces from logs
{noformat}
org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException: Unable to serialize logical type 'INTERVAL MONTH NOT NULL'. Please check the documentation for supported types. (through reference chain: java.util.Coll
ections$UnmodifiableRandomAccessList[1]->org.apache.flink.table.gateway.rest.serde.ColumnInfo[""logicalType""])
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:392) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:351) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.std.StdSerializer.wrapAndThrow(StdSerializer.java:316) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:782) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:178) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:119) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serialize(IndexedListSerializer.java:79) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serialize(IndexedListSerializer.java:18) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.SerializerProvider.defaultSerializeField(SerializerProvider.java:1166) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.ResultInfoSerializer.serialize(ResultInfoSerializer.java:82) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.ResultInfoSerializer.serialize(ResultInfoSerializer.java:47) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.SerializerProvider.defaultSerializeField(SerializerProvider.java:1166) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.FetchResultsResponseBodySerializer.serialize(FetchResultsResponseBodySerializer.java:60) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.FetchResultsResponseBodySerializer.serialize(FetchResultsResponseBodySerializer.java:31) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:480) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:319) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4568) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:3804) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.util.HandlerUtils.sendResponse(HandlerUtils.java:92) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.lambda$respondToRequest$1(AbstractSqlGatewayRestHandler.java:93) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670) ~[?:1.8.0_362]
        at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:683) ~[?:1.8.0_362]
        at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:2010) ~[?:1.8.0_362]
        at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.respondToRequest(AbstractSqlGatewayRestHandler.java:91) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.respondToRequest(AbstractSqlGatewayRestHandler.java:52) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.AbstractHandler.respondAsLeader(AbstractHandler.java:196) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.lambda$channelRead0$0(LeaderRetrievalHandler.java:83) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]

        at java.util.Optional.ifPresent(Optional.java:159) [?:1.8.0_362]
        at org.apache.flink.util.OptionalConsumer.ifPresent(OptionalConsumer.java:45) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:80) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:49) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:115) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:94) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:55) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:208) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:69) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:336) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:308) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:750) [?:1.8.0_362]
Caused by: java.lang.UnsupportedOperationException: Unable to serialize logical type 'INTERVAL MONTH NOT NULL'. Please check the documentation for supported types.
        at org.apache.flink.table.gateway.rest.serde.LogicalTypeJsonSerializer.serializeInternal(LogicalTypeJsonSerializer.java:174) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.LogicalTypeJsonSerializer.serialize(LogicalTypeJsonSerializer.java:100) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.LogicalTypeJsonSerializer.serialize(LogicalTypeJsonSerializer.java:51) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:728) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:774) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        ... 67 more

{noformat}
//cc [~fsk119]",Sergey Nuyanzin,Sergey Nuyanzin,Blocker,Resolved,Fixed,15/Feb/23 14:37,22/Feb/23 15:23
Bug,FLINK-31092,13524864,Hive ITCases fail with OutOfMemoryError,"We're experiencing an OutOfMemoryError where the heap space reaches the upper limit:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46161&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=23142

{code}
Feb 15 05:05:14 [INFO] Running org.apache.flink.table.catalog.hive.HiveCatalogITCase
Feb 15 05:05:17 [INFO] java.lang.OutOfMemoryError: Java heap space
Feb 15 05:05:17 [INFO] Dumping heap to java_pid9669.hprof ...
Feb 15 05:05:28 [INFO] Heap dump file created [1957090051 bytes in 11.718 secs]
java.lang.OutOfMemoryError: Java heap space
	at org.apache.maven.surefire.booter.ForkedBooter.cancelPingScheduler(ForkedBooter.java:209)
	at org.apache.maven.surefire.booter.ForkedBooter.acknowledgedExit(ForkedBooter.java:419)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:186)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}",fsk119,mapohl,Blocker,Closed,Fixed,15/Feb/23 14:55,06/Mar/23 11:04
Bug,FLINK-31097,13524938,mysql catalog datatype mapping error tinyint mapping boolean ,"mysql ddl 
`auto_borrow` tinyint(1) DEFAULT '0',
 
flink sql client
mysql catalog query 
  `auto_borrow` BOOLEAN,
 
so weird.",,hiscat,Major,Closed,Fixed,16/Feb/23 03:16,16/Feb/23 03:23
Bug,FLINK-31099,13524963,Chained WindowOperator throws NPE in PyFlink ThreadMode,"Test case
{code:python}
config = Configuration()
config.set_string(""python.execution-mode"", ""process"")
env = StreamExecutionEnvironment.get_execution_environment(config)

class MyTimestampAssigner(TimestampAssigner, ABC):
    def extract_timestamp(self, value: tuple, record_timestamp: int) -> int:
        return value[0]

ds = env.from_collection(
    [(1676461680000, ""a1"", ""b1"", 1), (1676461680000, ""a1"", ""b1"", 1),
     (1676461680000, ""a2"", ""b2"", 1), (1676461680000, ""a1"", ""b2"", 1),
     (1676461740000, ""a1"", ""b1"", 1), (1676461740000, ""a2"", ""b2"", 1)]
).assign_timestamps_and_watermarks(
    WatermarkStrategy.for_monotonous_timestamps().with_timestamp_assigner(MyTimestampAssigner())
)
ds.key_by(
    lambda x: (x[0], x[1], x[2])
).window(
    TumblingEventTimeWindows.of(Time.minutes(1))
).reduce(
    lambda x, y: (x[0], x[1], x[2], x[3] + y[3]),
    output_type=Types.TUPLE([Types.LONG(), Types.STRING(), Types.STRING(), Types.INT()])
# ).filter(
#     lambda x: x[1] == ""a1""
).map(
    lambda x: (x[0], x[1], x[3]),
    output_type=Types.TUPLE([Types.LONG(), Types.STRING(), Types.INT()])
).print()
env.execute()
{code}",hxbks2ks,hxbks2ks,Major,Resolved,Fixed,16/Feb/23 07:22,16/Feb/23 09:31
Bug,FLINK-31109,13525087,Fails with proxy user not supported even when security.kerberos.fetch.delegation-token is set to false,"With
{code:java}
security.kerberos.fetch.delegation-token: false
{code}
and delegation tokens obtained through our internal service which sets both HADOOP_TOKEN_FILE_LOCATION to pick up the DTs and also sets the HADOOP_PROXY_USER which fails with the below error
{code:java}
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/export/home/vsowrira/flink-1.18-SNAPSHOT/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/export/apps/hadoop/hadoop-bin_2100503/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
org.apache.flink.runtime.security.modules.SecurityModule$SecurityInstallException: Unable to set the Hadoop login user
	at org.apache.flink.runtime.security.modules.HadoopModule.install(HadoopModule.java:106)
	at org.apache.flink.runtime.security.SecurityUtils.installModules(SecurityUtils.java:76)
	at org.apache.flink.runtime.security.SecurityUtils.install(SecurityUtils.java:57)
	at org.apache.flink.client.cli.CliFrontend.mainInternal(CliFrontend.java:1188)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1157)
Caused by: java.lang.UnsupportedOperationException: Proxy user is not supported
	at org.apache.flink.runtime.security.token.hadoop.KerberosLoginProvider.throwProxyUserNotSupported(KerberosLoginProvider.java:137)
	at org.apache.flink.runtime.security.token.hadoop.KerberosLoginProvider.isLoginPossible(KerberosLoginProvider.java:81)
	at org.apache.flink.runtime.security.modules.HadoopModule.install(HadoopModule.java:73)
	... 4 more
{code}

This seems to have gotten changed after [480e6edf|https://github.com/apache/flink/commit/480e6edf9732f8334ef7576080fdbfc98051cb28] ([FLINK-28330][runtime][security] Remove old delegation token framework code)",vsowrirajan,vsowrirajan,Blocker,Closed,Fixed,16/Feb/23 21:23,27/Feb/23 19:06
Bug,FLINK-31114,13525139,Batch job fails with IllegalStateException when using adaptive batch scheduler,"This is caused by FLINK-30942. Currently, if two job vertices have the same input and the same parallelism(even the parallelism is -1), they will share partitions. However after FLINK-30942, the scheduler may change the job vertices' parallelism before scheduling, resulting in two job vertices having the same parallelism in  compilation phase (in which case will share partitions), but different parallelism in the scheduling phase, and then cause the following exception:

{code:java}
Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Consumers must have the same max parallelism.
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:975)
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
        ... 37 more
Caused by: java.lang.IllegalStateException: Consumers must have the same max parallelism.
        at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
        at org.apache.flink.runtime.executiongraph.IntermediateResult.getConsumersMaxParallelism(IntermediateResult.java:219)
        at org.apache.flink.runtime.executiongraph.Execution.getPartitionMaxParallelism(Execution.java:501)
        at org.apache.flink.runtime.executiongraph.Execution.registerProducedPartitions(Execution.java:472)
        at org.apache.flink.runtime.executiongraph.Execution.registerProducedPartitions(Execution.java:431)
        at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$registerProducedPartitions$5(DefaultExecutionDeployer.java:277)
        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
        ... 38 more
{code}

Putting the following test into {{AdaptiveBatchSchedulerITCase}} can reproduce the problem：

{code:java}
    @Test
    void testDifferentConsumerParallelism() throws Exception {
        final Configuration configuration = createConfiguration();
        final StreamExecutionEnvironment env =
                StreamExecutionEnvironment.createLocalEnvironment(configuration);
        env.setRuntimeMode(RuntimeExecutionMode.BATCH);
        env.setParallelism(8);

        final DataStream<Long> source1 =
                env.fromSequence(0, NUMBERS_TO_PRODUCE - 1)
                        .setParallelism(8)
                        .name(""source1"")
                        .slotSharingGroup(""group1"");

        final DataStream<Long> source2 =
                env.fromSequence(0, NUMBERS_TO_PRODUCE - 1)
                        .setParallelism(8)
                        .name(""source2"")
                        .slotSharingGroup(""group2"");

        source1.forward()
                .union(source2)
                .map(new NumberCounter())
                .name(""map1"")
                .slotSharingGroup(""group3"");

        source2.map(new NumberCounter()).name(""map2"").slotSharingGroup(""group4"");

        env.execute();
    }
{code}

",wanglijie,wanglijie,Blocker,Closed,Fixed,17/Feb/23 05:44,10/May/23 02:25
Bug,FLINK-31120,13525178,ConcurrentModificationException occurred in StringFunctionsITCase.test,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46255&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12334

{code}
Feb 17 04:51:25 [ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 10.725 s <<< FAILURE! - in org.apache.flink.table.planner.functions.StringFunctionsITCase
Feb 17 04:51:25 [ERROR] org.apache.flink.table.planner.functions.StringFunctionsITCase.test(TestCase)[4] Time elapsed: 4.367 s <<< ERROR!
Feb 17 04:51:25 org.apache.flink.table.api.TableException: Failed to execute sql
Feb 17 04:51:25 at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:974)
Feb 17 04:51:25 at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1422)
Feb 17 04:51:25 at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:476)
Feb 17 04:51:25 at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$ResultTestItem.test(BuiltInFunctionTestBase.java:354)
Feb 17 04:51:25 at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$TestSetSpec.lambda$getTestCase$4(BuiltInFunctionTestBase.java:320)
Feb 17 04:51:25 at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$TestCase.execute(BuiltInFunctionTestBase.java:113)
Feb 17 04:51:25 at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase.test(BuiltInFunctionTestBase.java:93)
Feb 17 04:51:25 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",csq,mapohl,Blocker,Closed,Fixed,17/Feb/23 10:09,23/Feb/23 07:24
Bug,FLINK-31131,13525404,The INITIALIZING of ExecutionState is missed in the state_machine doc,"[https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/internals/job_scheduling/#jobmanager-data-structures]

 

The INITIALIZING of ExecutionState is missed in the state_machine doc, it should be between DEPLOYING and RUNNING.

 

!image-2023-02-20-17-39-22-557.png!

 ",Wencong Liu,fanrui,Major,Closed,Fixed,20/Feb/23 09:40,24/Apr/23 02:07
Bug,FLINK-31132,13525408,compact without setting parallelism does not follow the configured sink parallelism for HiveTableSink,"If the parallelism of compact operator was not set, it should use the sink parallelism and disable parallelism inference when using adaptive batch scheduler to avoid take much time to finish compaction.",Weijie Guo,fsk119,Major,Closed,Fixed,20/Feb/23 10:03,22/Feb/23 14:45
Bug,FLINK-31133,13525410,PartiallyFinishedSourcesITCase hangs if a checkpoint fails,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b

This build ran into a timeout. Based on the stacktraces reported, it was either caused by [SnapshotMigrationTestBase.restoreAndExecute|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=13475]:
{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007f23d800b800 nid=0x60cdd waiting on condition [0x00007f23e1c0d000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.test.checkpointing.utils.SnapshotMigrationTestBase.restoreAndExecute(SnapshotMigrationTestBase.java:382)
	at org.apache.flink.test.migration.TypeSerializerSnapshotMigrationITCase.testSnapshot(TypeSerializerSnapshotMigrationITCase.java:172)
	at sun.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
[...]
{code}

or [PartiallyFinishedSourcesITCase.test|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10401]:
{code}
2023-02-20T07:13:05.6084711Z ""main"" #1 prio=5 os_prio=0 tid=0x00007fd35c00b800 nid=0x8c8a waiting on condition [0x00007fd363d0f000]
2023-02-20T07:13:05.6085149Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2023-02-20T07:13:05.6085487Z 	at java.lang.Thread.sleep(Native Method)
2023-02-20T07:13:05.6085925Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
2023-02-20T07:13:05.6086512Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:138)
2023-02-20T07:13:05.6087103Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForSubtasksToFinish(CommonTestUtils.java:291)
2023-02-20T07:13:05.6087730Z 	at org.apache.flink.runtime.operators.lifecycle.TestJobExecutor.waitForSubtasksToFinish(TestJobExecutor.java:226)
2023-02-20T07:13:05.6088410Z 	at org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.test(PartiallyFinishedSourcesITCase.java:138)
2023-02-20T07:13:05.6088957Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}

Still, it sounds odd: Based on a code analysis it's quite unlikely that those two caused the issue. The former one has a 5 min timeout (see related code in [SnapshotMigrationTestBase:382|https://github.com/apache/flink/blob/release-1.15/flink-tests/src/test/java/org/apache/flink/test/checkpointing/utils/SnapshotMigrationTestBase.java#L382]). For the other one, we found it being not responsible in the past when some other concurrent test caused the issue (see FLINK-30261).

An investigation on where we lose the time for the timeout revealed that {{AdaptiveSchedulerITCase}} took 2980s to finish (see [build logs|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5265]).
{code}
2023-02-20T03:43:55.4546050Z Feb 20 03:43:55 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2023-02-20T03:43:58.0448506Z Feb 20 03:43:58 [INFO] Running org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
2023-02-20T04:33:38.6824634Z Feb 20 04:33:38 [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2,980.445 s - in org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
{code}",roman,mapohl,Major,Resolved,Fixed,20/Feb/23 10:08,08/Mar/23 17:55
Bug,FLINK-31136,13525414,SQL Client Gateway mode should not read read execution config,,fsk119,fsk119,Major,Closed,Fixed,20/Feb/23 10:21,23/Feb/23 03:24
Bug,FLINK-31137,13525415,SQL Client doesn't print results for SHOW CREATE TABLE/DESC in hive dialect,,fsk119,fsk119,Major,Closed,Fixed,20/Feb/23 10:23,21/Feb/23 12:03
Bug,FLINK-31142,13525444,Some queries lead to abrupt sql client close,"Although the behavior has been changed in 1.17.0, I'm not sure whether it is a blocker or not, since in both cases it is invalid query.
I put it to blocker just because of regression.

The difference in the behavior is that before 1.17.0
a query like 
{code:sql}
select /* multiline comment;
{code}
fails to execute and sql client prompts to submit another query.

In 1.17.0 it  shuts down the session failing with 
{noformat}
Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Could not read from command line.
	at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:205)
	at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:168)
	at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:113)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:169)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:118)
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:228)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:179)
Caused by: org.apache.flink.sql.parser.impl.TokenMgrError: Lexical error at line 1, column 29.  Encountered: <EOF> after : """"
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImplTokenManager.getNextToken(FlinkSqlParserImplTokenManager.java:26752)
	at org.apache.flink.table.client.cli.parser.SqlCommandParserImpl$TokenIterator.scan(SqlCommandParserImpl.java:89)
	at org.apache.flink.table.client.cli.parser.SqlCommandParserImpl$TokenIterator.next(SqlCommandParserImpl.java:81)
	at org.apache.flink.table.client.cli.parser.SqlCommandParserImpl.checkIncompleteStatement(SqlCommandParserImpl.java:141)
	at org.apache.flink.table.client.cli.parser.SqlCommandParserImpl.getCommand(SqlCommandParserImpl.java:111)
	at org.apache.flink.table.client.cli.parser.SqlCommandParserImpl.parseStatement(SqlCommandParserImpl.java:52)
	at org.apache.flink.table.client.cli.parser.SqlMultiLineParser.parse(SqlMultiLineParser.java:82)
	at org.jline.reader.impl.LineReaderImpl.acceptLine(LineReaderImpl.java:2964)
	at org.jline.reader.impl.LineReaderImpl$1.apply(LineReaderImpl.java:3778)
	at org.jline.reader.impl.LineReaderImpl.readLine(LineReaderImpl.java:679)
	at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:183)
	... 6 more

Shutting down the session...
done.

{noformat}

",,Sergey Nuyanzin,Blocker,Resolved,Fixed,20/Feb/23 14:46,26/Feb/23 14:00
Bug,FLINK-31144,13525451,Slow scheduling on large-scale batch jobs ,"When executing a complex job graph at high parallelism `DefaultPreferredLocationsRetriever.getPreferredLocationsBasedOnInputs` can get slow and cause long pauses where the JobManager becomes unresponsive and all the taskmanagers just wait. I've attached a VisualVM snapshot to illustrate the problem.[^flink-1.17-snapshot-1676473798013.nps]

At Spotify we have complex jobs where this issue can cause batch ""pause"" of 40+ minutes and make the overall execution 30% slower or more.
More importantly this prevent us from running said jobs on larger cluster as adding resources to the cluster worsen the issue.

We have successfully tested a modified Flink version where `DefaultPreferredLocationsRetriever.getPreferredLocationsBasedOnInputs` was completely commented and simply returns an empty collection and confirmed it solves the issue.

In the same spirit as a recent change ([https://github.com/apache/flink/blob/43f419d0eccba86ecc8040fa6f521148f1e358ff/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/DefaultPreferredLocationsRetriever.java#L98-L102)] there could be a mechanism in place to detect when Flink run into this specific issue and just skip the call to `getInputLocationFutures`  [https://github.com/apache/flink/blob/43f419d0eccba86ecc8040fa6f521148f1e358ff/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/DefaultPreferredLocationsRetriever.java#L105-L108.]

I'm not familiar enough with the internals of Flink to propose a more advanced fix, however it seems like a configurable threshold on the number of consumer vertices above which the preferred location is not computed would do. If this  solution is good enough, I'd be happy to submit a PR.",JunRuiLi,jto,Major,Closed,Fixed,20/Feb/23 16:59,03/Apr/23 09:02
Bug,FLINK-31162,13525502,Avoid setting private tokens to AM container context when kerberos delegation token fetch is disabled,"In our internal env, we have enabled [Consistent Reads from HDFS Observer NameNode|https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/ObserverNameNode.html]. With this, some of the _ObserverReadProxyProvider_ implementation clone the delegation token for HA service and mark those tokens private so that they won't be accessible through _ugi.getCredentials()._

But Flink internally uses _currUsr.getTokens()_ [here|https://github.com/apache/flink/blob/release-1.16.1/flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java#L222] to get the current user credentials tokens to be set in AM context for submitting the YARN app to RM.

This fails with the following error:
{code:java}
Unable to add the application to the delegation token renewer.
java.io.IOException: Failed to renew token: Kind: HDFS_DELEGATION_TOKEN, Service: test01-ha4.abc:9000, Ident: (HDFS_DELEGATION_TOKEN token 151335106 for john)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:495)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$900(DelegationTokenRenewer.java:79)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:939)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:916)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby. Visit https://s.apache.org/sbnn-error
at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:108)
at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:2044)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1451)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewDelegationToken(FSNamesystem.java:5348)
at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewDelegationToken(NameNodeRpcServer.java:733)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:1056)
at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:525)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:495)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1038)
at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1003)
at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:931)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1905)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2856)

at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1499)
at org.apache.hadoop.ipc.Client.call(Client.java:1445)
at org.apache.hadoop.ipc.Client.call(Client.java:1342)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
at com.sun.proxy.$Proxy87.renewDelegationToken(Unknown Source)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewDelegationToken(ClientNamenodeProtocolTranslatorPB.java:986)
at sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
at com.sun.proxy.$Proxy88.renewDelegationToken(Unknown Source)
at org.apache.hadoop.hdfs.DFSClient$Renewer.renew(DFSClient.java:761)
at org.apache.hadoop.security.token.Token.renew(Token.java:466)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:629)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:626)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1905)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.renewToken(DelegationTokenRenewer.java:625)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:481)
... 6 more
{code}
Based on the [code comment here in HAUtilClient.java|https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/HAUtilClient.java#L128], it seems like the user credentials should be obtained using _ugi.getCredentials()_ instead of {_}ugi.getTokens(){_}. Also Spark seems to use _ugi.getCredentials()_ [here|https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L348] to set the credentials obtained to AM.",vsowrirajan,vsowrirajan,Major,Closed,Fixed,21/Feb/23 06:20,23/Feb/23 12:12
Bug,FLINK-31165,13525522,Over Agg: The window rank function without order by error in top N query," 
{code:java}
val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

val tableEnv = StreamTableEnvironment.create(env)


val td = TableDescriptor.forConnector(""datagen"").option(""rows-per-second"", ""10"")
  .option(""number-of-rows"", ""10"")
  .schema(Schema
    .newBuilder()
    .column(""NAME"", DataTypes.VARCHAR(2147483647))
    .column(""ROLLNO"", DataTypes.DECIMAL(5, 0))
    .column(""DOB"", DataTypes.DATE())
    .column(""CLASS"", DataTypes.DECIMAL(2, 0))
    .column(""SUBJECT"", DataTypes.VARCHAR(2147483647))
    .build())
  .build()

val table = tableEnv.from(td)


tableEnv.createTemporaryView(""temp_table"", table)

val newTable = tableEnv.sqlQuery(""select temp_table.*,cast('2022-01-01' as date) SRC_NO from temp_table"")

tableEnv.createTemporaryView(""temp_table2"", newTable)


val newTable2 = tableEnv.sqlQuery(""select * from (select NAME,ROLLNO,row_number() over (partition by NAME ORDER BY SRC_NO) AS rownum  from temp_table2 a) where rownum <= 1"")

tableEnv.toChangelogStream(newTable2).print()

env.execute()
 {code}
 

 

I am getting the below error if I run the above code.

I have already provided an order by column.

If I change the order by column to some other column, such as ""SUBJECT"", then the job runs fine.

 

 
{code:java}
Exception in thread ""main"" java.lang.RuntimeException: Error while applying rule FlinkLogicalOverAggregateConverter(in:NONE,out:LOGICAL), args [rel#245:LogicalWindow.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#244,window#0=window(partition {0} rows between UNBOUNDED PRECEDING and CURRENT ROW aggs [ROW_NUMBER()]))]
    at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:256)
    at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:58)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:510)
    at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:62)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:187)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:185)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:189)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:184)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:108)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:176)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83)
    at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
    at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:315)
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:195)
    at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:224)
    at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:219)
    at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.toChangelogStream(StreamTableEnvironmentImpl.scala:160)
    at org.example.OverAggregateBug$.main(OverAggregateBug.scala:39)
    at org.example.OverAggregateBug.main(OverAggregateBug.scala)
Caused by: org.apache.flink.table.api.ValidationException: Over Agg: The window rank function without order by. please re-check the over window statement.
    at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalOverAggregateConverter.$anonfun$convert$2(FlinkLogicalOverAggregate.scala:95)
    at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalOverAggregateConverter.$anonfun$convert$2$adapted(FlinkLogicalOverAggregate.scala:92)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalOverAggregateConverter.$anonfun$convert$1(FlinkLogicalOverAggregate.scala:92)
    at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalOverAggregateConverter.$anonfun$convert$1$adapted(FlinkLogicalOverAggregate.scala:89)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalOverAggregateConverter.convert(FlinkLogicalOverAggregate.scala:89)
    at org.apache.calcite.rel.convert.ConverterRule.onMatch(ConverterRule.java:167)
    at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:229)
    ... 27 more {code}
 

 ",qingyue,rohankrao,Major,Resolved,Fixed,21/Feb/23 08:54,10/Apr/23 13:06
Bug,FLINK-31166,13525538,array_contains does NOT work when haystack elements are not nullable and needle is nullable,"{{ARRAY_CONTAINS}} works ok for the case when both haystack elements and needle are not nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1], 0);{code}
it works ok when both haystack elements and needle are nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1, NULL], CAST(NULL AS INT));{code}
it works ok when haystack elements are nullable and needle is not nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1, NULL], 1);{code}
and it does NOT work when haystack elements are not nullable and needle is nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1], CAST(NULL AS INT));{code}
 

!image-2023-02-22-09-56-59-257.png!

 

!image-2023-02-21-18-41-19-385.png!",jackylau,jackylau,Major,Resolved,Fixed,21/Feb/23 10:41,09/Mar/23 23:43
Bug,FLINK-31169,13525560,KubernetesResourceManagerDriverTest.testOnPodDeleted fails fatally due to 239 exit code,"master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46341&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=27329

{code}
[...]
Feb 21 04:44:11 [ERROR] Process Exit Code: 239
Feb 21 04:44:11 [ERROR] Crashed tests:
Feb 21 04:44:11 [ERROR] org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest
Feb 21 04:44:11 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
[...]
{code}

{code}
[...]
Test org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.testOnPodDeleted[testOnPodDeleted()] is running.
--------------------------------------------------------------------------------
04:43:57,681 [ForkJoinPool-4-worker-1] INFO  org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Recovered 0 pods from previous attempts, current attempt id is 1.
04:43:57,701 [testing-rpc-main-thread] INFO  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Enabled external resources: []
04:43:57,705 [testing-rpc-main-thread] INFO  org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Creating new TaskManager pod with name testing-flink-cluster-taskmanager-1-1 and resource <704,0.0>.
04:43:57,708 [testing-rpc-main-thread] INFO  org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Received new TaskManager pod: testing-flink-cluster-taskmanager-1-1
04:43:57,708 [testing-rpc-main-thread] INFO  org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Pod testing-flink-cluster-taskmanager-1-1 is created.
04:43:57,708 [testing-rpc-main-thread] WARN  org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Pod testing-flink-cluster-taskmanager-1-1 is terminated before being scheduled.
04:43:57,709 [testing-rpc-main-thread] ERROR org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Error completing resource request.
org.apache.flink.util.FlinkException: Pod is terminated.
        at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.onPodTerminated(KubernetesResourceManagerDriver.java:379) ~[classes/:?]
        at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.lambda$handlePodEventsInMainThread$2(KubernetesResourceManagerDriver.java:347) ~[classes/:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
04:43:57,724 [testing-rpc-main-thread] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'testing-rpc-main-thread' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.lang.RuntimeException: org.apache.flink.util.FlinkException: Pod is terminated.
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:838) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_292]
        at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.onPodTerminated(KubernetesResourceManagerDriver.java:379) ~[classes/:?]
        at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.lambda$handlePodEventsInMainThread$2(KubernetesResourceManagerDriver.java:347) ~[classes/:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.lang.RuntimeException: org.apache.flink.util.FlinkException: Pod is terminated.
        at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321) ~[flink-core-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.lambda$requestResource$1(KubernetesResourceManagerDriver.java:233) ~[classes/:?]
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_292]
        ... 12 more
Caused by: org.apache.flink.util.FlinkException: Pod is terminated.
        ... 9 more
[...]
{code}",xtsong,mapohl,Blocker,Closed,Fixed,21/Feb/23 12:57,22/Feb/23 10:06
Bug,FLINK-31173,13525668,Fix several bugs in flink-ml-iteration module,"In flink-ml-iteration, there are several bugs as follows:
 # TailOperator should have one input operator. We have added a Tail operator to increment the epoch watermark at each iteration. We have made an assumption that each Tail operator have only one input and did not align the epoch watermarks from different inputs. This assumption might not be true if the input is an `union`.
 # ProxyOperatorStateBackend does not correctly initialize the state descriptor.",zhangzp,zhangzp,Major,Resolved,Fixed,22/Feb/23 02:31,20/Apr/23 09:48
Bug,FLINK-31182,13525719,CompiledPlan cannot deserialize BridgingSqlFunction with MissingTypeStrategy,"This issue is reported from the [user mail list|https://lists.apache.org/thread/y6fgzyx330omhkr40376knw8k4oczz3s].

The stacktrace is 
{code:java}
Unable to find source-code formatter for language: text. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yamlCaused by: org.apache.flink.table.api.TableException: Could not resolve internal system function '$UNNEST_ROWS$1'. This is a bug, please file an issue.
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeInternalFunction(RexNodeJsonDeserializer.java:392)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeSqlOperator(RexNodeJsonDeserializer.java:337)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeCall(RexNodeJsonDeserializer.java:307)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:146)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:128)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:115) {code}
The root cause is that although ModuleManager can resolve '$UNNEST_ROWS$1', the output type strategy is ""Missing""; as a result, FunctionCatalogOperatorTable#convertToBridgingSqlFunction returns empty.
!screenshot-1.png|width=675,height=295!",twalthr,qingyue,Major,Closed,Fixed,22/Feb/23 10:17,06/Mar/23 14:33
Bug,FLINK-31183,13525726,Flink Kinesis EFO Consumer can fail to stop gracefully,"*Background*

When stopping a Flink job using the stop-with-savepoint API the EFO Kinesis source can fail to close gracefully.

 

Sample stack trace
{code:java}
2023-02-16 20:45:40
org.apache.flink.runtime.checkpoint.CheckpointException: Task has failed.
	at org.apache.flink.runtime.messages.checkpoint.SerializedCheckpointException.unwrap(SerializedCheckpointException.java:51)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:1013)
	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$declineCheckpoint$2(ExecutionGraphHandler.java:103)
	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Task name with subtask : Source: vas_source_stream (38/48)#0 Failure reason: Task has failed.
	at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1395)
	at org.apache.flink.runtime.taskmanager.Task.lambda$triggerCheckpointBarrier$3(Task.java:1338)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:343)
Caused by: java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: event executor terminated
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1063)
	... 3 more
Caused by: java.util.concurrent.RejectedExecutionException: event executor terminated
	at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:923)
	at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:350)
	at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:343)
	at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:825)
	at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:815)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.nrs.HandlerPublisher$ChannelSubscription.cancel(HandlerPublisher.java:502)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.DelegatingSubscription.cancel(DelegatingSubscription.java:37)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.http2.Http2ResetSendingSubscription.cancel(Http2ResetSendingSubscription.java:41)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.DelegatingSubscription.cancel(DelegatingSubscription.java:37)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.ResponseHandler$OnCancelSubscription.cancel(ResponseHandler.java:409)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.FlatteningSubscriber$1.cancel(FlatteningSubscriber.java:98)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.FlatteningSubscriber.handleStateUpdate(FlatteningSubscriber.java:170)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.FlatteningSubscriber.access$100(FlatteningSubscriber.java:29)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.FlatteningSubscriber$1.request(FlatteningSubscriber.java:93)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.DelegatingSubscription.request(DelegatingSubscription.java:32)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.DelegatingSubscription.request(DelegatingSubscription.java:32)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.DelegatingSubscription.request(DelegatingSubscription.java:32)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber$FanOutShardSubscription.requestRecord(FanOutShardSubscriber.java:401)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.consumeAllRecordsFromKinesisShard(FanOutShardSubscriber.java:355)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.subscribeToShardAndConsumeRecords(FanOutShardSubscriber.java:189)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.runWithBackoff(FanOutRecordPublisher.java:169)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.run(FanOutRecordPublisher.java:124)
	at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.run(ShardConsumer.java:114)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
 {code}",dannycranmer,dannycranmer,Major,Resolved,Fixed,22/Feb/23 10:36,23/Feb/23 17:06
Bug,FLINK-31185,13525738,Python BroadcastProcessFunction not support side output,,Juntao Hu,Juntao Hu,Major,Closed,Fixed,22/Feb/23 12:01,06/Mar/23 10:37
Bug,FLINK-31187,13525768,Standalone HA mode does not work if dynamic properties are supplied,"With FLINK-30518 '--host $(POD_IP)' has been added to the arguments of the JMs which fixes the issue with HA on standalone mode, but it always gets appended to the end of the final JM arguments: https://github.com/usamj/flink-kubernetes-operator/blob/72ec9d384def3091ce50c2a3e2a06cded3b572e6/flink-kubernetes-standalone/src/main/java/org/apache/flink/kubernetes/operator/kubeclient/decorators/CmdStandaloneJobManagerDecorator.java#L107

But this will not be parsed properly in case any dynamic properties were set in the arguments, e.g.:
{code:java}
 Program Arguments:
   --configDir
   /opt/flink/conf
   -D
   jobmanager.memory.off-heap.size=134217728b
   -D
   jobmanager.memory.jvm-overhead.min=201326592b
   -D
   jobmanager.memory.jvm-metaspace.size=268435456b
   -D
   jobmanager.memory.heap.size=469762048b
   -D
   jobmanager.memory.jvm-overhead.max=201326592b
   --job-classname
   org.apache.flink.streaming.examples.statemachine.StateMachineExample
   --test
   test
   --host
   172.17.0.11{code}
You can verify this bug by using the YAML I've attached and in the JM logs you can see this line: 
{code:java}
Remoting started; listening on addresses :[akka.tcp://flink@flink-example-statemachine.flink:6123]{code}
Without any program arguments supplied this would correctly be:
{code:java}
Remoting started; listening on addresses :[akka.tcp://flink@172.17.0.8:6123]{code}

I believe this could be easily fixed by appending the --host parameter before JobSpec.args and if a committer can assign this ticket to me I can create a PR for this.",mateczagany,mateczagany,Major,Closed,Fixed,22/Feb/23 18:09,18/Jun/23 15:29
Bug,FLINK-31191,13525823,VectorIndexer should check whether doublesByColumn is null before snapshot,"Currently VectorIndexer would lead to NPE when doing checkpoint. It should check whether `doublesByColumn` is null before calling snapshot.

 

logview: [https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039]

details:
 
 
[735|https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039#step:4:736]Caused by: java.lang.NullPointerException 
[736|https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039#step:4:737] at org.apache.flink.ml.feature.vectorindexer.VectorIndexer$ComputeDistinctDoublesOperator.convertToListArray(VectorIndexer.java:232) 
[737|https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039#step:4:738] at org.apache.flink.ml.feature.vectorindexer.VectorIndexer$ComputeDistinctDoublesOperator.snapshotState(VectorIndexer.java:228) 
[738|https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039#step:4:739] at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:222) 
[739|https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039#step:4:740] ... 33 more",zhangzp,zhangzp,Major,Resolved,Fixed,23/Feb/23 06:04,01/Mar/23 10:10
Bug,FLINK-31195,13525841,FullChangelogStoreSinkWrite bucket writer conflicts with rescale,"At present, this operator relies on ListState, Flink distributes data according to round-robin when rescaling, which may be different from the distribution rules of our bucket after rescaling.

We need to change the mode of UnionListState, broadcast to each node, and finally decide whether it belongs to the task.",,lzljs3620320,Major,Closed,Fixed,23/Feb/23 08:22,29/Mar/23 03:07
Bug,FLINK-31204,13525970,HiveCatalogITCase fails due to avro conflict in table store,"Test fails in IDEA

	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	... 4 more
Caused by: java.lang.NoSuchMethodError: org.apache.avro.Schema.isNullable()Z
	at org.apache.flink.table.store.format.avro.AvroSchemaConverter.nullableSchema(AvroSchemaConverter.java:203)
	at org.apache.flink.table.store.format.avro.AvroSchemaConverter.convertToSchema(AvroSchemaConverter.java:172)
	at org.apache.flink.table.store.format.avro.AvroSchemaConverter.convertToSchema(AvroSchemaConverter.java:147)
	at org.apache.flink.table.store.format.avro.AvroSchemaConverter.convertToSchema(AvroSchemaConverter.java:147)
	at org.apache.flink.table.store.format.avro.AvroSchemaConverter.convertToSchema(AvroSchemaConverter.java:55)
	at org.apache.flink.table.store.format.avro.AvroFileFormat$AvroGenericRecordBulkFormat.<init>(AvroFileFormat.java:95)
	at org.apache.flink.table.store.format.avro.AvroFileFormat.createReaderFactory(AvroFileFormat.java:80)
	at org.apache.flink.table.store.format.FileFormat.createReaderFactory(FileFormat.java:71)
	at org.apache.flink.table.store.format.FileFormat.createReaderFactory(FileFormat.java:67)
	at org.apache.flink.table.store.file.manifest.ManifestList$Factory.create(ManifestList.java:130)
	at org.apache.flink.table.store.file.operation.AbstractFileStoreScan.<init>(AbstractFileStoreScan.java:95)
	at org.apache.flink.table.store.file.operation.KeyValueFileStoreScan.<init>(KeyValueFileStoreScan.java:57)
	at org.apache.flink.table.store.file.KeyValueFileStore.newScan(KeyValueFileStore.java:118)
	at org.apache.flink.table.store.file.KeyValueFileStore.newScan(KeyValueFileStore.java:71)
	at org.apache.flink.table.store.file.KeyValueFileStore.newScan(KeyValueFileStore.java:38)
	at org.apache.flink.table.store.file.AbstractFileStore.newCommit(AbstractFileStore.java:116)
	at org.apache.flink.table.store.file.AbstractFileStore.newCommit(AbstractFileStore.java:43)
	at org.apache.flink.table.store.table.AbstractFileStoreTable.newCommit(AbstractFileStoreTable.java:121)
	at org.apache.flink.table.store.connector.sink.FileStoreSink.lambda$createCommitterFactory$63124b4e$1(FileStoreSink.java:69)
	at org.apache.flink.table.store.connector.sink.CommitterOperator.initializeState(CommitterOperator.java:104)
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:122)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:283)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:726)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:702)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:669)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
	at java.lang.Thread.run(Thread.java:750)
",zjureel,zjureel,Major,Closed,Fixed,24/Feb/23 00:25,24/Feb/23 04:00
Bug,FLINK-31206,13525993,Broken links on flink.apache.org,"Previously page link https://flink.apache.org/contribute/code-style-and-quality/preamble/ is broken, new link is https://flink.apache.org/how-to-contribute/code-style-and-quality-preamble/.

Shall we set up a redirection or just let those broken links wait for maintainers fixing?

cc [~martijnvisser]",martijnvisser,tison,Minor,Closed,Fixed,24/Feb/23 04:02,24/Feb/23 11:09
Bug,FLINK-31236,13526288,Limit pushdown should not open useless RecordReader,,lzljs3620320,lzljs3620320,Major,Closed,Fixed,27/Feb/23 08:26,27/Feb/23 10:21
Bug,FLINK-31237,13526289,Fix possible bug of array_distinct,"as talked here [https://github.com/apache/flink/pull/19623,] we should use builtin expressions/functions. because the sql semantic is different from  java equals",,jackylau,Major,Resolved,Fixed,27/Feb/23 08:40,27/Feb/23 14:10
Bug,FLINK-31255,13526478,OperatorUtils#createWrappedOperatorConfig should update input and sideOutput serializers,"Currently we use operator wrapper to enable using normal operators in iterations. However, the operatorConfig is not correctly unwrapped. For example, the following code fails because of wrong type serializer.

 
{code:java}
@Test
public void testIterationWithMapPartition() throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    DataStream<Long> input =
        env.fromParallelCollection(new NumberSequenceIterator(0L, 5L), Types.LONG);
    DataStreamList result =
        Iterations.iterateBoundedStreamsUntilTermination(
            DataStreamList.of(input),
            ReplayableDataStreamList.notReplay(input),
            IterationConfig.newBuilder()
                .setOperatorLifeCycle(OperatorLifeCycle.PER_ROUND)
                .build(),
            new IterationBodyWithMapPartition());

    List<Integer> counts = IteratorUtils.toList(result.get(0).executeAndCollect());
    System.out.println(counts.size());
}

private static class IterationBodyWithMapPartition implements IterationBody {

    @Override
    public IterationBodyResult process(
        DataStreamList variableStreams, DataStreamList dataStreams) {
        DataStream<Long> input = variableStreams.get(0);

        DataStream<Long> mapPartitionResult =
            DataStreamUtils.mapPartition(
                input,
                new MapPartitionFunction <Long, Long>() {
                    @Override
                    public void mapPartition(Iterable <Long> iterable, Collector <Long> collector)
                        throws Exception {
                        for (Long iter: iterable) {
                            collector.collect(iter);
                        }
                    }
                });

        DataStream<Integer> terminationCriteria =
            mapPartitionResult.<Long>flatMap(new TerminateOnMaxIter(2)).returns(Types.INT);

        return new IterationBodyResult(
            DataStreamList.of(mapPartitionResult), variableStreams, terminationCriteria);
    }
} {code}
The error stack is:

Caused by: java.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.flink.iteration.IterationRecord
    at org.apache.flink.iteration.typeinfo.IterationRecordSerializer.serialize(IterationRecordSerializer.java:34)
    at org.apache.flink.iteration.datacache.nonkeyed.FileSegmentWriter.addRecord(FileSegmentWriter.java:79)
    at org.apache.flink.iteration.datacache.nonkeyed.DataCacheWriter.addRecord(DataCacheWriter.java:107)
    at org.apache.flink.iteration.datacache.nonkeyed.ListStateWithCache.add(ListStateWithCache.java:148)
    at org.apache.flink.ml.common.datastream.DataStreamUtils$MapPartitionOperator.processElement(DataStreamUtils.java:445)
    at org.apache.flink.iteration.operator.perround.OneInputPerRoundWrapperOperator.processElement(OneInputPerRoundWrapperOperator.java:69)
    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
    at java.lang.Thread.run(Thread.java:748)",Jiang Xin,zhangzp,Major,Closed,Fixed,28/Feb/23 08:15,18/Apr/23 05:50
Bug,FLINK-31258,13526515,can not get kerberos keytab in flink operator,"env:

flink k8s operator 1.4

flink 1.14.6 :

the conf
{code:java}
  flinkConfiguration:
    security.kerberos.login.keytab=/path/your/user.keytab 
   security.kerberos.login.principal=your@HADOOP.COM  {code}
and I get an exception:

 
{code:java}
Status:
  Cluster Info:
  Error:                          {""type"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException"",""message"":""org.apache.flink.client.deployment.ClusterDeploymentException: Could not create Kubernetes cluster \""basic-example\""."",""throwableList"":[{""type"":""org.apache.flink.client.deployment.ClusterDeploymentException"",""message"":""Could not create Kubernetes cluster \""basic-example\"".""},{""type"":""org.apache.flink.configuration.IllegalConfigurationException"",""message"":""Kerberos login configuration is invalid: keytab [/path/your/user.keytab] doesn't exist!""}]} {code}
 

 ",mbalassi,zhangjun,Major,Closed,Fixed,28/Feb/23 12:02,02/Mar/23 16:07
Bug,FLINK-31270,13526606,Fix flink jar name in docs for table store,,zjureel,zjureel,Major,Closed,Fixed,01/Mar/23 04:30,01/Mar/23 05:46
Bug,FLINK-31272,13526622,Duplicate operators appear in the StreamGraph for Python DataStream API jobs,"For the following job:
{code}
import argparse
import json
import sys
import time
from typing import Iterable, cast

from pyflink.common import Types, Time, Encoder
from pyflink.datastream import StreamExecutionEnvironment, ProcessWindowFunction, EmbeddedRocksDBStateBackend, \
    PredefinedOptions, FileSystemCheckpointStorage, CheckpointingMode, ExternalizedCheckpointCleanup
from pyflink.datastream.connectors.file_system import FileSink, RollingPolicy, OutputFileConfig
from pyflink.datastream.state import ReducingState, ReducingStateDescriptor
from pyflink.datastream.window import TimeWindow, Trigger, TriggerResult, T, TumblingProcessingTimeWindows, \
    ProcessingTimeTrigger


class CountWithProcessTimeoutTrigger(ProcessingTimeTrigger):

    def __init__(self, window_size: int):
        self._window_size = window_size
        self._count_state_descriptor = ReducingStateDescriptor(
            ""count"", lambda a, b: a + b, Types.LONG())

    @staticmethod
    def of(window_size: int) -> 'CountWithProcessTimeoutTrigger':
        return CountWithProcessTimeoutTrigger(window_size)

    def on_element(self,
                   element: T,
                   timestamp: int,
                   window: TimeWindow,
                   ctx: 'Trigger.TriggerContext') -> TriggerResult:
        count_state = cast(ReducingState, ctx.get_partitioned_state(self._count_state_descriptor))
        count_state.add(1)
        # print(""element arrive:"", element, ""count_state:"", count_state.get(), window.max_timestamp(),
        #       ctx.get_current_watermark())

        if count_state.get() >= self._window_size:  # 必须fire&purge！！！！
            print(""fire element count"", element, count_state.get(), window.max_timestamp(),
                  ctx.get_current_watermark())
            count_state.clear()
            return TriggerResult.FIRE_AND_PURGE
        if timestamp >= window.end:
            count_state.clear()
            return TriggerResult.FIRE_AND_PURGE
        else:
            return TriggerResult.CONTINUE

    def on_processing_time(self,
                           timestamp: int,
                           window: TimeWindow,
                           ctx: Trigger.TriggerContext) -> TriggerResult:
        if timestamp >= window.end:
            return TriggerResult.CONTINUE
        else:
            print(""fire with process_time:"", timestamp)
            count_state = cast(ReducingState, ctx.get_partitioned_state(self._count_state_descriptor))
            count_state.clear()
            return TriggerResult.FIRE_AND_PURGE

    def on_event_time(self,
                      timestamp: int,
                      window: TimeWindow,
                      ctx: 'Trigger.TriggerContext') -> TriggerResult:
        return TriggerResult.CONTINUE

    def clear(self,
              window: TimeWindow,
              ctx: 'Trigger.TriggerContext') -> None:
        count_state = ctx.get_partitioned_state(self._count_state_descriptor)
        count_state.clear()


def to_dict_map(v):
    time.sleep(1)
    dict_value = json.loads(v)
    return dict_value


def get_group_key(value, keys):
    group_key_values = []
    for key in keys:
        one_key_value = 'null'
        if key in value:
            list_value = value[key]
            if list_value:
                one_key_value = str(list_value[0])
        group_key_values.append(one_key_value)
    group_key = '_'.join(group_key_values)
    # print(""group_key="", group_key)
    return group_key


class CountWindowProcessFunction(ProcessWindowFunction[dict, dict, str, TimeWindow]):

    def __init__(self, uf):
        self._user_function = uf

    def process(self,
                key: str,
                context: ProcessWindowFunction.Context[TimeWindow],
                elements: Iterable[dict]) -> Iterable[dict]:
        result_list = self._user_function.process_after_group_by_function(elements)
        return result_list


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--output',
        dest='output',
        required=False,
        help='Output file to write results to.')

    argv = sys.argv[1:]
    known_args, _ = parser.parse_known_args(argv)
    output_path = known_args.output

    env = StreamExecutionEnvironment.get_execution_environment()
    # write all the data to one file
    env.set_parallelism(1)

    # process time
    env.get_config().set_auto_watermark_interval(0)
    state_backend = EmbeddedRocksDBStateBackend(True)
    state_backend.set_predefined_options(PredefinedOptions.FLASH_SSD_OPTIMIZED)
    env.set_state_backend(state_backend)
    config = env.get_checkpoint_config()
    # config.set_checkpoint_storage(FileSystemCheckpointStorage(""hdfs://ha-nn-uri/tmp/checkpoint/""))
    config.set_checkpoint_storage(FileSystemCheckpointStorage(""file:///Users/10030122/Downloads/pyflink_checkpoint/""))
    config.set_checkpointing_mode(CheckpointingMode.AT_LEAST_ONCE)
    config.set_checkpoint_interval(5000)
    config.set_externalized_checkpoint_cleanup(ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION)

    # define the source
    data_stream1 = env.from_collection(['{""user_id"": [""0""], ""goods_id"": [0,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [1,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [2,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [3,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [4,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [5,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [6,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [7,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [8,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [9,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [10,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [11,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [12,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [13,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [14,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [15,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [16,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [17,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [18,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [19,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [20,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [21,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [22,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [23,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [24,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [25,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [26,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [27,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [28,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [29,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [30,0]}'])

    data_stream2 = env.from_collection(['{""user_id"": [""0""], ""goods_id"": [0,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [1,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [2,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [3,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [4,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [5,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [6,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [7,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [8,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [9,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [10,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [11,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [12,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [13,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [14,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [15,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [16,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [17,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [18,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [19,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [20,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [21,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [22,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [23,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [24,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [25,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [26,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [27,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [28,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [29,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [30,0]}'])

    # group_keys = ['user_id', 'goods_id']
    group_keys = ['user_id']

    sink_to_file_flag = True

    data_stream = data_stream1.union(data_stream2)

    # user_function = __import__(""UserFunction"")

    ds = data_stream.map(lambda v: to_dict_map(v)) \
        .filter(lambda v: v) \
        .map(lambda v: v) \
        .key_by(lambda v: get_group_key(v, group_keys)) \
        .window(TumblingProcessingTimeWindows.of(Time.seconds(12))) \
        .process(CountWindowProcessFunction(lambda v: v), Types.STRING())

    ds = ds.map(lambda v: v, Types.PRIMITIVE_ARRAY(Types.BYTE()))

    base_path = ""/tmp/1.txt""
    encoder = Encoder.simple_string_encoder()
    file_sink_builder = FileSink.for_row_format(base_path, encoder)
    file_sink = file_sink_builder \
        .with_bucket_check_interval(1000) \
        .with_rolling_policy(RollingPolicy.on_checkpoint_rolling_policy()) \
        .with_output_file_config(
        OutputFileConfig.builder().with_part_prefix(""pre"").with_part_suffix(""suf"").build()) \
        .build()
    ds.sink_to(file_sink)

    # submit for execution
    env.execute()
{code}

The stream graph is as following:
{code}
{
  ""nodes"" : [ {
    ""id"" : 1,
    ""type"" : ""Source: Collection Source"",
    ""pact"" : ""Data Source"",
    ""contents"" : ""Source: Collection Source"",
    ""parallelism"" : 1
  }, {
    ""id"" : 2,
    ""type"" : ""Source: Collection Source"",
    ""pact"" : ""Data Source"",
    ""contents"" : ""Source: Collection Source"",
    ""parallelism"" : 1
  }, {
    ""id"" : 9,
    ""type"" : ""TumblingProcessingTimeWindows"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Window(TumblingProcessingTimeWindows(12000, 0), ProcessingTimeTrigger, CountWindowProcessFunction)"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 15,
      ""ship_strategy"" : ""HASH"",
      ""side"" : ""second""
    } ]
  }, {
    ""id"" : 10,
    ""type"" : ""Map"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Map"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 9,
      ""ship_strategy"" : ""FORWARD"",
      ""side"" : ""second""
    } ]
  }, {
    ""id"" : 15,
    ""type"" : ""Map, Filter, Map, _stream_key_by_map_operator"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Map, Filter, Map, _stream_key_by_map_operator"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 1,
      ""ship_strategy"" : ""FORWARD"",
      ""side"" : ""second""
    }, {
      ""id"" : 2,
      ""ship_strategy"" : ""FORWARD"",
      ""side"" : ""second""
    } ]
  }, {
    ""id"" : 16,
    ""type"" : ""TumblingProcessingTimeWindows, Map"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Window(TumblingProcessingTimeWindows(12000, 0), ProcessingTimeTrigger, CountWindowProcessFunction)"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 15,
      ""ship_strategy"" : ""HASH"",
      ""side"" : ""second""
    } ]
  }, {
    ""id"" : 18,
    ""type"" : ""Sink: Writer"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Sink: Writer"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 10,
      ""ship_strategy"" : ""FORWARD"",
      ""side"" : ""second""
    } ]
  }, {
    ""id"" : 20,
    ""type"" : ""Sink: Committer"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Sink: Committer"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 18,
      ""ship_strategy"" : ""FORWARD"",
      ""side"" : ""second""
    } ]
  } ]
}
{code}

The plan is incorrect as we can see that TumblingProcessingTimeWindows appears twice.",dianfu,dianfu,Major,Closed,Fixed,01/Mar/23 07:19,08/Mar/23 17:54
Bug,FLINK-31273,13526624,Left join with IS_NULL filter be wrongly pushed down and get wrong join results,"Left join with IS_NULL filter be wrongly pushed down and get wrong join results. The sql is:
{code:java}
SELECT * FROM MyTable1 LEFT JOIN MyTable2 ON a1 = a2 WHERE a2 IS NULL AND a1 < 10

The wrongly plan is:

LogicalProject(a1=[$0], b1=[$1], c1=[$2], b2=[$3], c2=[$4], a2=[$5])
+- LogicalFilter(condition=[IS NULL($5)])
   +- LogicalJoin(condition=[=($0, $5)], joinType=[left])
      :- LogicalValues(tuples=[[]])
      +- LogicalTableScan(table=[[default_catalog, default_database, MyTable2]]) {code}",337361684@qq.com,337361684@qq.com,Critical,Closed,Fixed,01/Mar/23 07:27,28/Jun/23 14:35
Bug,FLINK-31277,13526639,JM Deployment recovery logic inconsistent with health related restart ,"The current JM Deployment logic that restarts missing deployments strictly requires HA metadata event for stateless deployments.

This is inconsistent with how the cluster health check related restarts work which can cause the operator to delete an unhealthy deployment and potentially leave it missing if the first deploy attempt fails.",gyfora,gyfora,Major,Closed,Fixed,01/Mar/23 08:13,02/Mar/23 08:42
Bug,FLINK-31279,13526652,Fix MULTIPLY(TIMES) function doesn't support interval types,"{code:java}
// code placeholder
Flink SQL> select 2 * interval '3'  day;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.planner.codegen.CodeGenException: Interval expression type expected. {code}",jackylau,jackylau,Major,Closed,Fixed,01/Mar/23 09:01,08/Mar/23 09:33
Bug,FLINK-31283,13526697,Correct the description of building from source with scala version,"After FLINK-20845, Flink dropped the support of Scala 2.11. However, the doc of ""building from source"" still has the description on scala-2.11.",yunta,yunta,Major,Resolved,Fixed,01/Mar/23 14:14,07/Mar/23 06:19
Bug,FLINK-31286,13526778,Python processes are still alive when shutting down a session cluster directly without stopping the jobs,"Reproduce steps:
1) start a standalone cluster: ./bin/start_cluster.sh
2) submit a PyFlink job which contains Python UDFs
3) stop the cluster: ./bin/stop_cluster.sh
4) Check if Python process still exists: ps aux | grep -i beam_boot

!image-2023-03-02-10-55-34-863.png!",dianfu,dianfu,Major,Closed,Fixed,02/Mar/23 02:57,08/Mar/23 17:54
Bug,FLINK-31288,13526788,Disable overdraft buffer for batch shuffle,"Only pipelined / pipelined-bounded partition needs overdraft buffer. More specifically, there is no reason to request more buffers for non-pipelined (i.e. batch) shuffle. The reasons are as follows:
 # For BoundedBlockingShuffle, each full buffer will be directly released.
 # For SortMergeShuffle, the maximum capacity of buffer pool is 4 * numSubpartitions. It is efficient enough to spill this part of memory to disk.
 # For Hybrid Shuffle, the buffer pool is unbounded. If it can't get a normal buffer, it also can't get an overdraft buffer.",Weijie Guo,Weijie Guo,Blocker,Closed,Fixed,02/Mar/23 03:49,03/Mar/23 14:50
Bug,FLINK-31293,13526796,Request memory segment from LocalBufferPool may hanging forever.,"In our TPC-DS test, we found that in the case of fierce competition in network memory, some tasks may hanging forever.

From the thread dump information, we can see that the task is waiting for the {{LocalBufferPool}} to become available. It is strange that other tasks have finished and released network memory already. Undoubtedly, this is an unexpected behavior, which implies that there must be a bug in the {{LocalBufferPool}} or {{{}NetworkBufferPool{}}}.

!image-2023-03-02-12-23-50-572.png|width=650,height=153!

By dumping the heap memory, we can find a strange phenomenon that there are available buffers in the {{{}LocalBufferPool{}}}, but it was considered to be un-available. Another thing to note is that it now holds an overdraft buffer.

!image-2023-03-02-12-28-48-437.png|width=520,height=200!

!image-2023-03-02-12-29-03-003.png|width=438,height=84!

TL;DR: This problem occurred in multi-thread race related to the introduction of overdraft buffer.

Suppose we have two threads, called A and B. For simplicity, {{LocalBufferPool}} is called {{LocalPool}} and {{NetworkBufferPool}} is called {{{}GlobalPool{}}}.

Thread A continuously request buffers blocking from the {{{}LocalPool{}}}.
Thread B continuously return buffers to {{{}GlobalPool{}}}.
 # If thread A takes the last available buffer of {{{}LocalPool{}}}, but {{GlobalPool}} does not have a buffer at this time, it will register a callback function with {{{}GlobalPool{}}}.
 # Thread B returns one buffer to {{{}GlobalPool{}}}, but has not started to trigger the callback.
 # Thread A continues to request buffer. Because the {{availableMemorySegments}} of {{LocalPool}} is empty, it requests the overdraftBuffer instead. But there is already a buffer in the {{{}GlobalPool{}}}, it successfully gets the buffer.
 # Thread B triggers the callback. Since there is no buffer in {{GlobalPool}} now, the callback is re-registered.
 # Thread A continues to request buffer. Because there is no buffer in {{{}GlobalPool{}}}, it will block on {{{}CompletableFuture#get{}}}.
 # Thread B continues to return a buffer and triggers the recently registered callback. As a result, {{LocalPool}} puts the buffer into {{{}availableMemorySegments{}}}. Unfortunately, the current logic of {{shouldBeAvailable}} method is: if there is an overdraft buffer, {{LocalPool}} is considered as un-available.
 # Thread A will keep blocking forever.",Weijie Guo,Weijie Guo,Critical,Closed,Fixed,02/Mar/23 04:21,04/Apr/23 02:37
Bug,FLINK-31294,13526800,CommitterOperator forgot to close Committer when closing.,"{{CommitterOperator}} does not close the {{Committer}} when it closes, which may lead to resource leaks.",Ming Li,Ming Li,Major,Closed,Fixed,02/Mar/23 05:05,03/Mar/23 11:18
Bug,FLINK-31297,13526828,FineGrainedSlotManagerTest.testTaskManagerRegistrationDeductPendingTaskManager unstable when running it a single time,"We noticed a weird test-instability in {{FineGrainedSlotManagerTest.testTaskManagerRegistrationDeductPendingTaskManager}} when switching to sequential test execution (see FLINK-31278). I couldn't reproduce it in 1.16, therefore, marking it as a blocker for now. But it feels to be more of a test code issue.
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46671&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9695

{code}
Mar 01 15:20:17 [ERROR] org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.testTaskManagerRegistrationDeductPendingTaskManager  Time elapsed: 0.746 s  <<< FAILURE!
Mar 01 15:20:17 java.lang.AssertionError: 
Mar 01 15:20:17 
Mar 01 15:20:17 Expected size: 1 but was: 0 in:
Mar 01 15:20:17 []
Mar 01 15:20:17 	at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.testTaskManagerRegistrationDeductPendingTaskManager(FineGrainedSlotManagerTest.java:209)
Mar 01 15:20:17 
{code}",huwh,mapohl,Critical,Closed,Fixed,02/Mar/23 09:51,06/Mar/23 15:03
Bug,FLINK-31298,13526830,ConnectionUtilsTest.testFindConnectingAddressWhenGetLocalHostThrows swallows IllegalArgumentException,"FLINK-24156 introduced {{NetUtils.acceptWithoutTimeout}} which caused the test to print a the stacktrace of an {{IllegalArgumentException}}:
{code}
Exception in thread ""Thread-0"" java.lang.IllegalArgumentException: serverSocket SO_TIMEOUT option must be 0
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
	at org.apache.flink.util.NetUtils.acceptWithoutTimeout(NetUtils.java:139)
	at org.apache.flink.runtime.net.ConnectionUtilsTest$1.run(ConnectionUtilsTest.java:83)
	at java.lang.Thread.run(Thread.java:750)
{code}

This is also shown in the Maven output of CI runs and might cause confusion. The test should be fixed.",Wencong Liu,mapohl,Major,Closed,Fixed,02/Mar/23 10:01,09/Mar/23 06:01
Bug,FLINK-31299,13526857,PendingRecords metric might not be available,"The Kafka pendingRecords metric is only initialized on receiving the first record. For empty topics or checkpointed topics without any incoming data, the metric won't appear.

We need to handle this case in the autoscaler and allow downscaling.",mxm,mxm,Major,Resolved,Fixed,02/Mar/23 12:57,06/Mar/23 16:34
Bug,FLINK-31300,13526876,TRY_CAST fails for constructed types,"In case of problems with cast it is expected to return {{null}}

however for arrays, maps it fails

example of failing queries
{code:sql}
select try_cast(array['a'] as array<int>);
select try_cast(map['a', '1'] as map<int, int>);
{code}

 {noformat}
[ERROR] Could not execute SQL statement. Reason:
java.lang.NumberFormatException: For input string: 'a'. Invalid character found.
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.numberFormatExceptionFor(BinaryStringDataUtil.java:585)
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.toInt(BinaryStringDataUtil.java:518)
	at StreamExecCalc$15.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
{noformat}",Sergey Nuyanzin,Sergey Nuyanzin,Major,Resolved,Fixed,02/Mar/23 15:20,09/Mar/23 22:14
Bug,FLINK-31305,13526920,KafkaWriter doesn't wait for errors for in-flight records before completing flush,"The KafkaWriter flushing needs to wait for all in-flight records to send successfully. This can be achieved by tracking requests and returning a response from the registered callback from the producer#send() logic.

There is potential for data loss since the checkpoint does not accurately reflect that all records have been sent successfully, to preserve at least once semantics.",mason6345,mason6345,Blocker,Closed,Fixed,02/Mar/23 23:47,17/Apr/23 07:16
Bug,FLINK-31315,13527011,FlinkActionsE2eTest.testMergeInto is unstable,"{code:java}
Error:  Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 320.272 s <<< FAILURE! - in org.apache.flink.table.store.tests.FlinkActionsE2eTest
82Error:  testMergeInto  Time elapsed: 111.826 s  <<< FAILURE!
83org.opentest4j.AssertionFailedError: 
84Result is still unexpected after 60 retries.
85Expected: {3, v_3, creation, 02-27=1, 2, v_2, creation, 02-27=1, 6, v_6, creation, 02-28=1, 1, v_1, creation, 02-27=1, 8, v_8, insert, 02-29=1, 11, v_11, insert, 02-29=1, 7, Seven, matched_upsert, 02-28=1, 5, v_5, creation, 02-28=1, 10, v_10, creation, 02-28=1, 9, v_9, creation, 02-28=1}
86Actual: {4, v_4, creation, 02-27=1, 8, v_8, creation, 02-28=1, 3, v_3, creation, 02-27=1, 7, v_7, creation, 02-28=1, 2, v_2, creation, 02-27=1, 6, v_6, creation, 02-28=1, 1, v_1, creation, 02-27=1, 5, v_5, creation, 02-28=1, 10, v_10, creation, 02-28=1, 9, v_9, creation, 02-28=1}
87	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:39)
88	at org.junit.jupiter.api.Assertions.fail(Assertions.java:134)
89	at org.apache.flink.table.store.tests.E2eTestBase.checkResult(E2eTestBase.java:261)
90	at org.apache.flink.table.store.tests.FlinkActionsE2eTest.testMergeInto(FlinkActionsE2eTest.java:355) {code}",,lzljs3620320,Major,Closed,Fixed,03/Mar/23 15:45,19/Mar/23 05:36
Bug,FLINK-31319,13527030,Kafka new source partitionDiscoveryIntervalMs=0 cause bounded source can not quit,"As kafka option description, partitionDiscoveryIntervalMs <=0 means disabled.

!image-2023-03-04-01-37-29-360.png|width=781,height=147!

just like start kafka enumerator:

!image-2023-03-04-01-39-20-352.png|width=465,height=311!

but inner 
handlePartitionSplitChanges use error if condition( < 0):

!image-2023-03-04-01-40-44-124.png|width=576,height=237!
 
it will cause noMoreNewPartitionSplits can not be set to true. 
!image-2023-03-04-01-41-55-664.png|width=522,height=610!

Finally cause bounded source can not signalNoMoreSplits, so it will not quit.

Besides，Both ends of the if condition should be mutually exclusive.",taoran,taoran,Critical,Resolved,Fixed,03/Mar/23 17:44,12/Apr/23 17:06
Bug,FLINK-31323,13527067,Fix unstable E2E test for flink actions,"Currently, the flink actions use Data Stream API to do insert job making batch configuration invalid. We should use Table API instead.",yzl,yzl,Major,Closed,Fixed,04/Mar/23 07:44,07/Mar/23 02:33
Bug,FLINK-31324,13527091,Broken SingleThreadFetcherManager constructor API,"FLINK-28853 changed the default constructor of {{SingleThreadFetcherManager}}. Though the {{SingleThreadFetcherManager}} is annotated as {{Internal}}, it actually acts as some-degree public API, which is widely used in many connector projects:
[flink-cdc-connector|https://github.com/ververica/flink-cdc-connectors/blob/release-2.3.0/flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/reader/MySqlSourceReader.java#L93], [flink-connector-mongodb|https://github.com/apache/flink-connector-mongodb/blob/main/flink-connector-mongodb/src/main/java/org/apache/flink/connector/mongodb/source/reader/MongoSourceReader.java#L58] and so on.

Once flink-1.17 is released, all these existing connectors are broken and cannot be used in new release version, and will throw exceptions like:

{code:java}
java.lang.NoSuchMethodError: org.apache.flink.connector.base.source.reader.fetcher.SingleThreadFetcherManager.<init>(Lorg/apache/flink/connector/base/source/reader/synchronization/FutureCompletingBlockingQueue;Ljava/util/function/Supplier;)V
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSourceReader.<init>(MySqlSourceReader.java:91) ~[flink-sql-connector-mysql-cdc-2.3.0.jar:2.3.0]
	at com.ververica.cdc.connectors.mysql.source.MySqlSource.createReader(MySqlSource.java:159) ~[flink-sql-connector-mysql-cdc-2.3.0.jar:2.3.0]
	at org.apache.flink.streaming.api.operators.SourceOperator.initReader(SourceOperator.java:312) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.init(SourceOperatorStreamTask.java:94) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:699) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_362]
{code}


Thus, I suggest to make the original SingleThreadFetcherManager constructor as depreacted instead of removing it.",yunta,yunta,Blocker,Resolved,Fixed,04/Mar/23 15:40,06/Mar/23 10:13
Bug,FLINK-31326,13527137,Disabled source scaling breaks downstream scaling if source busyTimeMsPerSecond is 0,"In case of 'scaling.sources.enabled'='false' the 'TARGET_DATA_RATE' of the source vertex will be calculated as '(1000 / busyTimeMsPerSecond) * numRecordsOutPerSecond' which currently on the main branch results in an infinite value if 'busyTimeMsPerSecond' is 0. This will also affect downstream operators.

I'm not that familiar with the autoscaler code, but it's in my opinion it's quite unexpected to have such behavioral changes by setting 'scaling.sources.enabled' to false.

 

With PR #543 for FLINK-30575 (https://github.com/apache/flink-kubernetes-operator/pull/543) scaling will happen even with 'busyTimeMsPerSecond' being 0, but it will result in unreasonably high parallelism numbers for downstream operators because 'TARGET_DATA_RATE' will be very high where 0 'busyTimeMsPerSecond' will be replaced with 1e-10.


Metrics from the operator logs (source=e5a72f353fc1e6bbf3bd96a41384998c, sink=51312116a3e504bccb3874fc80d5055e)

'scaling.sources.enabled'='true':
{code:java}
 jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.PARALLELISM.Current: 1.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.MAX_PARALLELISM.Current: 1.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_PROCESSING_RATE.Current: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_PROCESSING_RATE.Average: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.CATCH_UP_DATA_RATE.Current: 0.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.SCALE_UP_RATE_THRESHOLD.Current: 5.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.SCALE_DOWN_RATE_THRESHOLD.Current: 10.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.OUTPUT_RATIO.Current: 2.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.OUTPUT_RATIO.Average: 2.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_OUTPUT_RATE.Current: Infinity
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_OUTPUT_RATE.Average: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TARGET_DATA_RATE.Current: 3.8666666666666667
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TARGET_DATA_RATE.Average: 3.8833333333333333

jobVertexID.51312116a3e504bccb3874fc80d5055e.PARALLELISM.Current: 4.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.MAX_PARALLELISM.Current: 12.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.TRUE_PROCESSING_RATE.Current: 4.827299209321681
jobVertexID.51312116a3e504bccb3874fc80d5055e.TRUE_PROCESSING_RATE.Average: 4.848351269098938
jobVertexID.51312116a3e504bccb3874fc80d5055e.CATCH_UP_DATA_RATE.Current: 0.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.SCALE_UP_RATE_THRESHOLD.Current: 10.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.SCALE_DOWN_RATE_THRESHOLD.Current: 21.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.TARGET_DATA_RATE.Current: 7.733333333333333
jobVertexID.51312116a3e504bccb3874fc80d5055e.TARGET_DATA_RATE.Average: 7.766666666666667{code}

'scaling.sources.enabled'='false':
{code:java}
 jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.PARALLELISM.Current: 1.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.MAX_PARALLELISM.Current: 1.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_PROCESSING_RATE.Current: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_PROCESSING_RATE.Average: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.CATCH_UP_DATA_RATE.Current: 0.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.SCALE_UP_RATE_THRESHOLD.Current: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.SCALE_DOWN_RATE_THRESHOLD.Current: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.OUTPUT_RATIO.Current: 2.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.OUTPUT_RATIO.Average: 2.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_OUTPUT_RATE.Current: Infinity
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_OUTPUT_RATE.Average: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TARGET_DATA_RATE.Current: Infinity
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TARGET_DATA_RATE.Average: NaN

jobVertexID.51312116a3e504bccb3874fc80d5055e.PARALLELISM.Current: 4.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.MAX_PARALLELISM.Current: 12.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.TRUE_PROCESSING_RATE.Current: 5.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.TRUE_PROCESSING_RATE.Average: 4.980555555555556
jobVertexID.51312116a3e504bccb3874fc80d5055e.CATCH_UP_DATA_RATE.Current: 0.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.SCALE_UP_RATE_THRESHOLD.Current: NaN
jobVertexID.51312116a3e504bccb3874fc80d5055e.SCALE_DOWN_RATE_THRESHOLD.Current: NaN
jobVertexID.51312116a3e504bccb3874fc80d5055e.TARGET_DATA_RATE.Current: Infinity
jobVertexID.51312116a3e504bccb3874fc80d5055e.TARGET_DATA_RATE.Average: NaN{code}
 

My guess is 'scaling.sources.enabled' exists to support connectors where `pendingRecords` is not available, but setting this to false also negatively impacts existing Kafka sources for example, and users cannot anticipate this from the documentation.

 

I think it would be worth it to include this in the docs, or if anyone has any suggested solutions I would gladly look into implementing it.",mxm,mateczagany,Major,Resolved,Fixed,05/Mar/23 15:20,09/Mar/23 15:05
Bug,FLINK-31329,13527173,Fix Parquet stats extractor,"Some bugs in Parquet stats extractor:
 # Decimal Supports
 # Timestamp Supports
 # Null nullCounts supports",lzljs3620320,lzljs3620320,Major,Closed,Fixed,06/Mar/23 05:43,06/Mar/23 09:01
Bug,FLINK-31331,13527201,Flink 1.16 should implement new LookupFunction,"Only implements new LookupFunction, retry lookup join can work.",lzljs3620320,lzljs3620320,Major,Closed,Fixed,06/Mar/23 08:33,06/Mar/23 09:45
Bug,FLINK-31337,13527223,EmbeddedDataStreamBatchTests.test_keyed_co_broadcast_side_output,"Same build, multiple times:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24566
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=24235
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=24545
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=24481
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=24757

{code}
Mar 04 01:21:35 pyflink/datastream/tests/test_data_stream.py:743: 
Mar 04 01:21:35 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
Mar 04 01:21:35 pyflink/datastream/tests/test_data_stream.py:63: in assert_equals_sorted
Mar 04 01:21:35     self.assertEqual(expected, actual)
Mar 04 01:21:35 E   AssertionError: Lists differ: ['0', '1', '2', '4', '5', '5', '6', '6'] != ['0', '1', '2', '3', '5', '5', '6', '6']
Mar 04 01:21:35 E   
Mar 04 01:21:35 E   First differing element 3:
Mar 04 01:21:35 E   '4'
Mar 04 01:21:35 E   '3'
Mar 04 01:21:35 E   
Mar 04 01:21:35 E   - ['0', '1', '2', '4', '5', '5', '6', '6']
Mar 04 01:21:35 E   ?                  ^
Mar 04 01:21:35 E   
Mar 04 01:21:35 E   + ['0', '1', '2', '3', '5', '5', '6', '6']
Mar 04 01:21:35 E   ?   
{code}",Juntao Hu,mapohl,Blocker,Closed,Fixed,06/Mar/23 09:39,06/Mar/23 15:41
Bug,FLINK-31345,13527286,Trim autoscaler configMap to not exceed 1mb size limit,"When the {{autoscaler-<deployment_name>}} ConfigMap which is used to persist scaling decisions and metrics becomes too large, the following error is thrown consistently:

{noformat}
io.fabric8.kubernetes.client.KubernetesClientException: Operation: [replace]  for kind: [ConfigMap]  with name: [deployment]  in namespace: [namespace]  failed.
    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:159)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.lambda$replace$0(HasMetadataOperation.java:169)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.replace(HasMetadataOperation.java:172)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.replace(HasMetadataOperation.java:113)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.replace(HasMetadataOperation.java:41)
    at io.fabric8.kubernetes.client.extension.ResourceAdapter.replace(ResourceAdapter.java:252)
    at org.apache.flink.kubernetes.operator.autoscaler.AutoScalerInfo.replaceInKubernetes(AutoScalerInfo.java:167)
    at org.apache.flink.kubernetes.operator.autoscaler.JobAutoScalerImpl.scale(JobAutoScalerImpl.java:113)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:178)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:130)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:56)
    at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:145)
    at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:103)
    at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
    at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:102)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:139)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:119)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:89)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:62)
    at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:406)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: stream was reset: NO_ERROR
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:514)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:551)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleUpdate(OperationSupport.java:347)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleUpdate(BaseOperation.java:680)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.lambda$replace$0(HasMetadataOperation.java:167)
    ... 21 more
Caused by: okhttp3.internal.http2.StreamResetException: stream was reset: NO_ERROR
    at okhttp3.internal.http2.Http2Stream.checkOutNotClosed$okhttp(Http2Stream.kt:646)
    at okhttp3.internal.http2.Http2Stream$FramingSink.emitFrame(Http2Stream.kt:557)
    at okhttp3.internal.http2.Http2Stream$FramingSink.write(Http2Stream.kt:532)
    at okio.ForwardingSink.write(ForwardingSink.kt:29)
    at okhttp3.internal.connection.Exchange$RequestBodySink.write(Exchange.kt:218)
    at okio.RealBufferedSink.emitCompleteSegments(RealBufferedSink.kt:255)
    at okio.RealBufferedSink.write(RealBufferedSink.kt:185)
    at okhttp3.RequestBody$Companion$toRequestBody$2.writeTo(RequestBody.kt:152)
    at okhttp3.internal.http.CallServerInterceptor.intercept(CallServerInterceptor.kt:59)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:34)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at org.apache.flink.kubernetes.operator.metrics.KubernetesClientMetrics.intercept(KubernetesClientMetrics.java:130)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientBuilderImpl$InteceptorAdapter.intercept(OkHttpClientBuilderImpl.java:70)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientBuilderImpl$InteceptorAdapter.intercept(OkHttpClientBuilderImpl.java:70)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientBuilderImpl$InteceptorAdapter.intercept(OkHttpClientBuilderImpl.java:70)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)
    at okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)
    ... 3 more
    Suppressed: okhttp3.internal.http2.StreamResetException: stream was reset: NO_ERROR
        ... 31 more
 {noformat}

We should trim the ConfigMap to not exceed the size limit.",gyfora,mxm,Major,Closed,Fixed,06/Mar/23 14:08,17/Mar/23 12:09
Bug,FLINK-31346,13527294,Batch shuffle IO scheduler does not throw TimeoutException if numRequestedBuffers is greater than 0,"We currently rely on throw exception to trigger downstream task failover to avoid read buffer request deadlock. But if {{numRequestedBuffers}} is greater than 0, IO scheduler does not throw {{TimeoutException}}. This will cause a deadlock.
",Weijie Guo,Weijie Guo,Major,Closed,Fixed,06/Mar/23 14:49,09/Mar/23 04:21
Bug,FLINK-31347,13527303,AdaptiveSchedulerClusterITCase.testAutomaticScaleUp times out,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46850&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10451

{code}
Mar 06 14:11:24 ""main"" #1 prio=5 os_prio=0 tid=0x00007f482800b800 nid=0x6eee waiting on condition [0x00007f48325cd000]
Mar 06 14:11:24    java.lang.Thread.State: TIMED_WAITING (sleeping)
Mar 06 14:11:24 	at java.lang.Thread.sleep(Native Method)
Mar 06 14:11:24 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:151)
Mar 06 14:11:24 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:144)
Mar 06 14:11:24 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.waitUntilParallelismForVertexReached(AdaptiveSchedulerClusterITCase.java:265)
Mar 06 14:11:24 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.testAutomaticScaleUp(AdaptiveSchedulerClusterITCase.java:153)
[...]
{code}",dmvk,mapohl,Critical,Resolved,Fixed,06/Mar/23 16:05,07/Mar/23 13:41
Bug,FLINK-31348,13527310,Documentation fails to build due to unclosed shortcodes,"After migration to HUGO and using Hugo version 0.111.0 or higher, there are a bunch of unclosed shortcodes which prevent the documentation from being served locally.

 

Example:
{code:java}
docker run -v $(pwd):/src -p 1313:1313 jakejarvis/hugo-extended:latest server --buildDrafts --buildFuture --bind 0.0.0.0
 
...

Error: Error building site: ""/src/content.zh/docs/connectors/datastream/formats/parquet.md:111:1"": failed to extract shortcode: unclosed shortcode ""tabs"" {code}
 
This is caused by the new Hugo 0.111.0 version https://github.com/gohugoio/hugo/releases/tag/v0.111.0 which includes ""Throw an error when shortcode is expected to be closed 7d78a49 @bep #10675""",dmvk,dmvk,Critical,Closed,Fixed,06/Mar/23 16:27,07/Mar/23 12:52
Bug,FLINK-31351,13527328,HiveServer2EndpointITCase.testExecuteStatementInSyncModeWithRuntimeException2 times out on CI,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46872&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=24908]

 
{code:java}
Mar 06 18:28:56 ""ForkJoinPool-1-worker-25"" #27 daemon prio=5 os_prio=0 tid=0x00007ff4b1832000 nid=0x21b2 waiting on condition [0x00007ff3a8c3e000]
Mar 06 18:28:56    java.lang.Thread.State: TIMED_WAITING (sleeping)
Mar 06 18:28:56 	at java.lang.Thread.sleep(Native Method)
Mar 06 18:28:56 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.waitUntilJobIsRunning(HiveServer2EndpointITCase.java:1004)
Mar 06 18:28:56 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.lambda$testExecuteStatementInSyncModeWithRuntimeException2$37(HiveServer2EndpointITCase.java:711)
Mar 06 18:28:56 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase$$Lambda$2018/2127600974.accept(Unknown Source)
Mar 06 18:28:56 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.runExecuteStatementInSyncModeWithRuntimeException(HiveServer2EndpointITCase.java:999)
Mar 06 18:28:56 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.testExecuteStatementInSyncModeWithRuntimeException2(HiveServer2EndpointITCase.java:709)
Mar 06 18:28:56 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 06 18:28:56 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 06 18:28:56 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 06 18:28:56 	at java.lang.reflect.Method.invoke(Method.java:498)
 {code}",fsk119,dmvk,Blocker,Closed,Fixed,06/Mar/23 18:53,09/Mar/23 08:19
Bug,FLINK-31354,13527392,NettyClientServerSslTest.testValidSslConnectionAdvanced timed out,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46883&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8242

{code}
Test testValidSslConnectionAdvanced[SSL provider = JDK](org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest) is running.
--------------------------------------------------------------------------------
05:15:10,904 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyConfig        [] - NettyConfig [server address: localhost/127.0.0.1, server port: 42935, ssl enabled: true, memory segment size (bytes): 1024, transport type: AUTO, number of server threads: 1 (manual), number of client threads>
05:15:10,916 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Transport type 'auto': using EPOLL.
05:15:12,149 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful initialization (took 1245 ms). Listening on SocketAddress /127.0.0.1:42935.
05:15:12,150 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Transport type 'auto': using EPOLL.
05:15:13,249 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful initialization (took 1099 ms).
05:15:14,588 [                main] ERROR org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest [] - 
--------------------------------------------------------------------------------
Test testValidSslConnectionAdvanced[SSL provider = JDK](org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest) failed with:
org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandshakeTimeoutException: handshake timed out after 1000ms
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler$7.run(SslHandler.java:2115)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:153)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:748)
{code}",mapohl,mapohl,Critical,Resolved,Fixed,07/Mar/23 07:54,22/May/23 16:21
Bug,FLINK-31359,13527486,HsResultPartitionTest fails with fatal error,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46910&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8512

{code}
Mar 07 13:20:39 [ERROR] Process Exit Code: 239
Mar 07 13:20:39 [ERROR] Crashed tests:
Mar 07 13:20:39 [ERROR] org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionTest
[...]
{code}",Weijie Guo,mapohl,Critical,Closed,Fixed,07/Mar/23 16:51,08/Mar/23 09:11
Bug,FLINK-31363,13527557,KafkaSink failed to commit transactions under EXACTLY_ONCE semantics,"When KafkaSink starts Exactly once and no data is written to the topic during a checkpoint, the transaction commit exception is triggered, with the following exception.

[Transiting to fatal error state due to org.apache.kafka.common.errors.InvalidTxnStateException: The producer attempted a transactional operation in an invalid state.]",tzulitai,lightzhao,Blocker,Closed,Fixed,08/Mar/23 02:55,12/Apr/23 17:13
Bug,FLINK-31374,13527711,ProxyStreamPartitioner should implement ConfigurableStreamPartitioner,"In flink-ml-iterations module, we use ProxyStreamPartitioner to wrap StreamPartitioner to deal with records in iterations.

 

However, it did not implement ConfigurableStreamPartitioner interface. Thus that maxParallelism information is lost.",Jiang Xin,zhangzp,Major,Resolved,Fixed,08/Mar/23 23:13,13/Apr/23 03:29
Bug,FLINK-31378,13527794,Documentation fails to build due to lack of package,"In [Project Configuration Section|[https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/configuration/overview/#running-and-packaging],] it shows that ""If you want to run your job by simply executing the main class, you will need {{flink-runtime}} in your classpath"". 

However, when I just add flink-runtime in my classPath, an error is thrown like this:""
No ExecutorFactory found to execute the application"".

It seems that flink-clients is also needed to supply an excutor through Java Service Load.

Could you please add this in official article for beginners like me?

 ",Wencong Liu,loserwang1024,Not a Priority,Closed,Fixed,09/Mar/23 11:26,13/Mar/23 07:19
Bug,FLINK-31386,13527845,Fix the potential deadlock issue of blocking shuffle,"Currently, the SortMergeResultPartition may allocate more network buffers than the guaranteed size of the LocalBufferPool. As a result, some result partitions may need to wait other result partitions to release the over-allocated network buffers to continue. However, the result partitions which have allocated more than guaranteed buffers relies on the processing of input data to trigger data spilling and buffer recycling. The input data further relies on batch reading buffers used by the SortMergeResultPartitionReadScheduler which may already taken by those blocked result partitions which are waiting for buffers. Then deadlock occurs. We can easily fix this deadlock by reserving the guaranteed buffers on initializing.",kevin.cyj,kevin.cyj,Blocker,Closed,Fixed,09/Mar/23 15:35,23/Apr/23 10:01
Bug,FLINK-31387,13527850,StreamTaskCancellationTest.testCancelTaskShouldPreventAdditionalProcessingTimeTimersFromBeingFired failed with an assertion,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46994&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9253

{code}
Mar 09 14:04:42 [ERROR] org.apache.flink.streaming.runtime.tasks.StreamTaskCancellationTest.testCancelTaskShouldPreventAdditionalProcessingTimeTimersFromBeingFired  Time elapsed: 0.018 s  <<< FAILURE!
Mar 09 14:04:42 java.lang.AssertionError: 
Mar 09 14:04:42 
Mar 09 14:04:42 Expecting AtomicInteger(0) to have value:
Mar 09 14:04:42   10
Mar 09 14:04:42 but did not.
Mar 09 14:04:42 	at org.apache.flink.streaming.runtime.tasks.StreamTaskCancellationTest.testCancelTaskShouldPreventAdditionalTimersFromBeingFired(StreamTaskCancellationTest.java:305)
Mar 09 14:04:42 	at org.apache.flink.streaming.runtime.tasks.StreamTaskCancellationTest.testCancelTaskShouldPreventAdditionalProcessingTimeTimersFromBeingFired(StreamTaskCancellationTest.java:281)
Mar 09 14:04:42 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",dmvk,mapohl,Critical,Resolved,Fixed,09/Mar/23 16:04,10/Mar/23 10:32
Bug,FLINK-31389,13527901,Fix spark jar name in docs for table store,,zhuangchong,zhuangchong,Major,Closed,Fixed,10/Mar/23 01:28,10/Mar/23 04:38
Bug,FLINK-31393,13527922,HsFileDataManager use an incorrect default timeout,"For batch shuffle(i.e. hybrid shuffle & sort-merge shuffle), If there is a fierce contention of the batch shuffle read memory, it will throw a {{TimeoutException}} to fail downstream task to release memory. But for hybrid shuffle, It uses an incorrect default timeout(5ms), this will make the job very easy to fail.",Weijie Guo,Weijie Guo,Critical,Closed,Fixed,10/Mar/23 06:08,12/Mar/23 15:02
Bug,FLINK-31394,13527931,Fix spark jar name in the create release script for table store,,zhuangchong,zhuangchong,Major,Closed,Fixed,10/Mar/23 06:54,10/Mar/23 08:14
Bug,FLINK-31396,13527942,Occasional inaccurate timeout time calculation with System.nanotime in batch read buffer pool,"When running TPC-DS tests, I encountered the read buffer request timeout because of configuring too less read buffers. But I found the timeout time may be less than 5m occasionally, 5m is the expected time. 
I read the docs of System.nanotime, the docs say that  t1 < t0 should not be used, because of the possibility of numerical overflow. I tested the System.currentTimeMillis and it can work as expected.",tanyuxin,tanyuxin,Minor,Closed,Fixed,10/Mar/23 08:29,10/Mar/23 14:40
Bug,FLINK-31401,13527999,testTransformationSetParallelism fails on 10 core machines,"StreamingJobGraphGenerator#testTransformationSetParallelism fails if it is run in an environment where the default parallelism is 10:

{noformat}
org.opentest4j.AssertionFailedError: 
expected: 3
 but was: 2
Expected :3
Actual   :2
{noformat}

The fix is trivial, we need to make an implicit assumption in the test about paralellisms explicit.",mbalassi,mbalassi,Minor,Closed,Fixed,10/Mar/23 15:30,12/Mar/23 18:52
Bug,FLINK-31403,13528058,CliClientITCase print unexpected border when printing explain results,"The CliClientITCase fails in the https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47010&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=16287.

After comparing the actual output and expected output, the differences are the length of the border when printing explain results.
",fsk119,fsk119,Major,Closed,Fixed,11/Mar/23 05:05,13/Mar/23 07:55
Bug,FLINK-31414,13528186,exceptions in the alignment timer are ignored,"Alignment timer task in alternating aligned checkpoint run as a future task in mailbox thread, causing the exceptions ([SingleCheckpointBarrierHandler#registerAlignmentTimer()|https://github.com/apache/flink/blob/65ab8e820a3714d2134dfb4c9772a10c998bd45a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/io/checkpointing/SingleCheckpointBarrierHandler.java#L327]) to be ignored. These exceptions should have failed the task, but now this will cause the same checkpoint to fire twice initInputsCheckpoints in my test.

 
{code:java}
 switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: unable to send request to worker
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.enqueue(ChannelStateWriterImpl.java:247)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.addInputData(ChannelStateWriterImpl.java:161)
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.prepareSnapshot(StreamTaskNetworkInput.java:103)
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.prepareSnapshot(StreamOneInputProcessor.java:83)
        at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.prepareSnapshot(StreamMultipleInputProcessor.java:122)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.prepareInputSnapshot(StreamTask.java:518)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.prepareInflightDataSnapshot(SubtaskCheckpointCoordinatorImpl.java:655)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.initInputsCheckpoint(SubtaskCheckpointCoordinatorImpl.java:515)
        at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$ControllerImpl.initInputsCheckpoint(SingleCheckpointBarrierHandler.java:516)
        at org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCollectingBarriers.alignmentTimeout(AlternatingCollectingBarriers.java:46)
        at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlternatingAlignedBarrierHandlerState.barrierReceived(AbstractAlternatingAlignedBarrierHandlerState.java:54)
        at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.lambda$processBarrier$2(SingleCheckpointBarrierHandler.java:234)
        at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.markCheckpointAlignedAndTransformState(SingleCheckpointBarrierHandler.java:262)
        at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:231)
        at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:181)
        at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:159)
        at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
        at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:85)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:542)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780)
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
        at java.lang.Thread.run(Thread.java:748)
        Suppressed: java.io.IOException: java.lang.IllegalStateException: writer not found for request start 17
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.close(ChannelStateWriteRequestExecutorImpl.java:175)
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.close(ChannelStateWriterImpl.java:235)
                at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:564)
                at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.close(SubtaskCheckpointCoordinatorImpl.java:551)
                at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:255)
                at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
                at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
                at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:943)
                at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:917)
                at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
                at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
                ... 3 more
        Caused by: java.lang.IllegalStateException: writer not found for request start 17
                at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatchInternal(ChannelStateWriteRequestDispatcherImpl.java:75)
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatch(ChannelStateWriteRequestDispatcherImpl.java:62)
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.loop(ChannelStateWriteRequestExecutorImpl.java:96)
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:75)
                ... 1 more
Caused by: java.lang.IllegalStateException: not running
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.ensureRunning(ChannelStateWriteRequestExecutorImpl.java:152)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.submitInternal(ChannelStateWriteRequestExecutorImpl.java:144)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.submit(ChannelStateWriteRequestExecutorImpl.java:128)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.enqueue(ChannelStateWriterImpl.java:244)
        ... 27 more
        [CIRCULAR REFERENCE:java.lang.IllegalStateException: writer not found for request start 17] {code}
 

 

see : [BarrierAlignmentUtil#createRegisterTimerCallback()|https://github.com/apache/flink/blob/65ab8e820a3714d2134dfb4c9772a10c998bd45a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/io/checkpointing/BarrierAlignmentUtil.java#L50]

 ",Feifan Wang,Feifan Wang,Major,Closed,Fixed,13/Mar/23 07:23,15/Mar/23 10:54
Bug,FLINK-31417,13528211,Hadoop version unknown when TrinoPageSourceBase.getNextPage,"Exception thrown when quering flink-table-store by trino
{code:java}
2023-03-13T11:46:36.694+0800    ERROR   SplitRunner-11-113      io.trino.execution.executor.TaskExecutor        Error processing Split 20230313_034504_00000_jdcet.1.0.0-11 {} (start = 3.599627617710298E10, wall = 89264 ms, cpu = 0 ms, wait = 1 ms, calls = 1)java.lang.NoClassDefFoundError: Could not initialize class org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderUtils        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:257)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:649)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createRecordReader(OrcReaderFactory.java:284)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:98)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:56)        at org.apache.flink.table.store.file.utils.FileUtils.createFormatReader(FileUtils.java:108)        at org.apache.flink.table.store.file.io.KeyValueDataFileRecordReader.<init>(KeyValueDataFileRecordReader.java:55)        at org.apache.flink.table.store.file.io.KeyValueFileReaderFactory.createRecordReader(KeyValueFileReaderFactory.java:95)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.lambda$readerForRun$1(MergeTreeReaders.java:89)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.create(ConcatRecordReader.java:50)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForRun(MergeTreeReaders.java:92)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForSection(MergeTreeReaders.java:74)        at org.apache.flink.table.store.file.operation.KeyValueFileStoreRead.lambda$createReader$2(KeyValueFileStoreRead.java:195)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.create(ConcatRecordReader.java:50)        at org.apache.flink.table.store.file.operation.KeyValueFileStoreRead.createReader(KeyValueFileStoreRead.java:204)        at org.apache.flink.table.store.table.source.KeyValueTableRead.createReader(KeyValueTableRead.java:44)        at org.apache.flink.table.store.trino.TrinoPageSourceProvider.createPageSource(TrinoPageSourceProvider.java:76)        at org.apache.flink.table.store.trino.TrinoPageSourceProvider.lambda$createPageSource$0(TrinoPageSourceProvider.java:52)        at org.apache.flink.table.store.trino.ClassLoaderUtils.runWithContextClassLoader(ClassLoaderUtils.java:30)        at org.apache.flink.table.store.trino.TrinoPageSourceProvider.createPageSource(TrinoPageSourceProvider.java:51)        at io.trino.split.PageSourceManager.createPageSource(PageSourceManager.java:68)        at io.trino.operator.TableScanOperator.getOutput(TableScanOperator.java:308)        at io.trino.operator.Driver.processInternal(Driver.java:388)        at io.trino.operator.Driver.lambda$processFor$9(Driver.java:292)        at io.trino.operator.Driver.tryWithLock(Driver.java:685)        at io.trino.operator.Driver.processFor(Driver.java:285)        at io.trino.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1076)        at io.trino.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:163)        at io.trino.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:488)        at io.trino.$gen.Trino_366_0____20230313_034413_2.run(Unknown Source)        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)        at java.base/java.lang.Thread.run(Thread.java:834)  2023-03-13T11:46:36.775+0800    ERROR   remote-task-callback-2  io.trino.execution.scheduler.PipelinedStageExecution    Pipelined stage execution for stage 20230313_034504_00000_jdcet.1 failedjava.lang.ExceptionInInitializerError        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:257)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:649)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createRecordReader(OrcReaderFactory.java:284)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:98)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:56)        at org.apache.flink.table.store.file.utils.FileUtils.createFormatReader(FileUtils.java:108)        at org.apache.flink.table.store.file.io.KeyValueDataFileRecordReader.<init>(KeyValueDataFileRecordReader.java:55)        at org.apache.flink.table.store.file.io.KeyValueFileReaderFactory.createRecordReader(KeyValueFileReaderFactory.java:95)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.lambda$readerForRun$1(MergeTreeReaders.java:89)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.create(ConcatRecordReader.java:50)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForRun(MergeTreeReaders.java:92)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForSection(MergeTreeReaders.java:74)        at org.apache.flink.table.store.file.operation.KeyValueFileStoreRead.lambda$createReader$2(KeyValueFileStoreRead.java:195)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.readBatch(ConcatRecordReader.java:65)        at org.apache.flink.table.store.file.mergetree.DropDeleteReader.readBatch(DropDeleteReader.java:44)        at org.apache.flink.table.store.table.source.KeyValueTableRead$RowDataRecordReader.readBatch(KeyValueTableRead.java:61)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.nextPage(TrinoPageSourceBase.java:120)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.getNextPage(TrinoPageSourceBase.java:113)        at io.trino.operator.TableScanOperator.getOutput(TableScanOperator.java:311)        at io.trino.operator.Driver.processInternal(Driver.java:388)        at io.trino.operator.Driver.lambda$processFor$9(Driver.java:292)        at io.trino.operator.Driver.tryWithLock(Driver.java:685)        at io.trino.operator.Driver.processFor(Driver.java:285)        at io.trino.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1076)        at io.trino.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:163)        at io.trino.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:488)        at io.trino.$gen.Trino_366_0____20230313_034413_2.run(Unknown Source)        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)        at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.lang.NumberFormatException: For input string: ""Unknown""        at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)        at java.base/java.lang.Integer.parseInt(Integer.java:652)        at java.base/java.lang.Integer.parseInt(Integer.java:770)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.HadoopShimsFactory.get(HadoopShimsFactory.java:53)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderUtils.<clinit>(RecordReaderUtils.java:47)        ... 30 more  2023-03-13T11:46:36.777+0800    ERROR   stage-scheduler io.trino.execution.scheduler.SqlQueryScheduler  Failure in distributed stage for query 20230313_034504_00000_jdcetjava.lang.ExceptionInInitializerError        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:257)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:649)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createRecordReader(OrcReaderFactory.java:284)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:98)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:56)        at org.apache.flink.table.store.file.utils.FileUtils.createFormatReader(FileUtils.java:108)        at org.apache.flink.table.store.file.io.KeyValueDataFileRecordReader.<init>(KeyValueDataFileRecordReader.java:55)        at org.apache.flink.table.store.file.io.KeyValueFileReaderFactory.createRecordReader(KeyValueFileReaderFactory.java:95)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.lambda$readerForRun$1(MergeTreeReaders.java:89)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.create(ConcatRecordReader.java:50)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForRun(MergeTreeReaders.java:92)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForSection(MergeTreeReaders.java:74)        at org.apache.flink.table.store.file.operation.KeyValueFileStoreRead.lambda$createReader$2(KeyValueFileStoreRead.java:195)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.readBatch(ConcatRecordReader.java:65)        at org.apache.flink.table.store.file.mergetree.DropDeleteReader.readBatch(DropDeleteReader.java:44)        at org.apache.flink.table.store.table.source.KeyValueTableRead$RowDataRecordReader.readBatch(KeyValueTableRead.java:61)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.nextPage(TrinoPageSourceBase.java:120)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.getNextPage(TrinoPageSourceBase.java:113)        at io.trino.operator.TableScanOperator.getOutput(TableScanOperator.java:311)        at io.trino.operator.Driver.processInternal(Driver.java:388)        at io.trino.operator.Driver.lambda$processFor$9(Driver.java:292)        at io.trino.operator.Driver.tryWithLock(Driver.java:685)        at io.trino.operator.Driver.processFor(Driver.java:285)        at io.trino.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1076)        at io.trino.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:163)        at io.trino.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:488)        at io.trino.$gen.Trino_366_0____20230313_034413_2.run(Unknown Source)        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)        at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.lang.NumberFormatException: For input string: ""Unknown""        at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)        at java.base/java.lang.Integer.parseInt(Integer.java:652)        at java.base/java.lang.Integer.parseInt(Integer.java:770)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.HadoopShimsFactory.get(HadoopShimsFactory.java:53)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderUtils.<clinit>(RecordReaderUtils.java:47)        ... 30 more  2023-03-13T11:46:36.784+0800    ERROR   stage-scheduler io.trino.execution.StageStateMachine    Stage 20230313_034504_00000_jdcet.1 failedjava.lang.ExceptionInInitializerError        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:257)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:649)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createRecordReader(OrcReaderFactory.java:284)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:98)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:56)        at org.apache.flink.table.store.file.utils.FileUtils.createFormatReader(FileUtils.java:108)        at org.apache.flink.table.store.file.io.KeyValueDataFileRecordReader.<init>(KeyValueDataFileRecordReader.java:55)        at org.apache.flink.table.store.file.io.KeyValueFileReaderFactory.createRecordReader(KeyValueFileReaderFactory.java:95)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.lambda$readerForRun$1(MergeTreeReaders.java:89)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.create(ConcatRecordReader.java:50)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForRun(MergeTreeReaders.java:92)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForSection(MergeTreeReaders.java:74)        at org.apache.flink.table.store.file.operation.KeyValueFileStoreRead.lambda$createReader$2(KeyValueFileStoreRead.java:195)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.readBatch(ConcatRecordReader.java:65)        at org.apache.flink.table.store.file.mergetree.DropDeleteReader.readBatch(DropDeleteReader.java:44)        at org.apache.flink.table.store.table.source.KeyValueTableRead$RowDataRecordReader.readBatch(KeyValueTableRead.java:61)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.nextPage(TrinoPageSourceBase.java:120)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.getNextPage(TrinoPageSourceBase.java:113)        at io.trino.operator.TableScanOperator.getOutput(TableScanOperator.java:311)        at io.trino.operator.Driver.processInternal(Driver.java:388)        at io.trino.operator.Driver.lambda$processFor$9(Driver.java:292)        at io.trino.operator.Driver.tryWithLock(Driver.java:685)        at io.trino.operator.Driver.processFor(Driver.java:285)        at io.trino.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1076)        at io.trino.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:163)        at io.trino.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:488)        at io.trino.$gen.Trino_366_0____20230313_034413_2.run(Unknown Source)        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)        at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.lang.NumberFormatException: For input string: ""Unknown""        at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)        at java.base/java.lang.Integer.parseInt(Integer.java:652)        at java.base/java.lang.Integer.parseInt(Integer.java:770)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.HadoopShimsFactory.get(HadoopShimsFactory.java:53)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderUtils.<clinit>(RecordReaderUtils.java:47)        ... 30 more {code}
Seems the common-version-info.properties file in flink-shaded-hadoop-2-uber-2.8.3-10.0.jar is not found by the classloader. The stacks tell that the call is from TrinoPageSourceBase.getNextPage, where the classloader of the current thread is AppClassloader, rather than PluginClassloader.

Can we fix it by using runWithContextClassLoader to run TrinoPageSourceBase.getNextPage with TrinoPageSourceBase.class.getClassloader?

 ",,nonggia,Major,Closed,Fixed,13/Mar/23 10:07,19/Mar/23 05:35
Bug,FLINK-31418,13528215,SortMergeResultPartitionReadSchedulerTest.testRequestBufferTimeout failed on CI,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47077&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8756]

Error message:
{code:java}
Mar 13 05:22:10 [ERROR] Failures: 
Mar 13 05:22:10 [ERROR]   SortMergeResultPartitionReadSchedulerTest.testRequestBufferTimeout:278 
Mar 13 05:22:10 Expecting value to be true but was false{code}",tanyuxin,renqs,Critical,Closed,Fixed,13/Mar/23 10:17,16/May/23 11:46
Bug,FLINK-31420,13528219,ThreadInfoRequestCoordinatorTest.testShutDown failed on CI,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47076&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8394]
{code:java}
Mar 13 03:58:48 [ERROR] Failures: 
Mar 13 03:58:48 [ERROR]   ThreadInfoRequestCoordinatorTest.testShutDown:207 
Mar 13 03:58:48 Expecting
Mar 13 03:58:48   <CompletableFuture[Failed with the following stack trace:
Mar 13 03:58:48 java.lang.RuntimeException: Discarded
Mar 13 03:58:48 	at org.apache.flink.runtime.webmonitor.stats.TaskStatsRequestCoordinator$PendingStatsRequest.discard(TaskStatsRequestCoordinator.java:266)
Mar 13 03:58:48 	at org.apache.flink.runtime.webmonitor.stats.TaskStatsRequestCoordinator.handleFailedResponse(TaskStatsRequestCoordinator.java:114)
Mar 13 03:58:48 	at org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinator.lambda$requestThreadInfo$1(ThreadInfoRequestCoordinator.java:152)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
Mar 13 03:58:48 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Mar 13 03:58:48 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Mar 13 03:58:48 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
Mar 13 03:58:48 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
Mar 13 03:58:48 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Mar 13 03:58:48 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Mar 13 03:58:48 	at java.lang.Thread.run(Thread.java:748)
Mar 13 03:58:48 Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Request timeout.
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:925)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:913)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
Mar 13 03:58:48 	at org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinatorTest.lambda$createMockTaskManagerGateway$2(ThreadInfoRequestCoordinatorTest.java:259)
Mar 13 03:58:48 	... 6 more
Mar 13 03:58:48 Caused by: java.util.concurrent.TimeoutException: Request timeout.
Mar 13 03:58:48 	... 7 more
Mar 13 03:58:48 ]>
Mar 13 03:58:48 not to be done.
Mar 13 03:58:48 Be aware that the state of the future in this message might not reflect the one at the time when the assertion was performed as it is evaluated later on {code}",Weijie Guo,renqs,Major,Closed,Fixed,13/Mar/23 10:29,15/Mar/23 14:46
Bug,FLINK-31437,13528341,Wrong key 'lookup.cache.caching-missing-key' in connector documentation,"'lookup.cache.caching-missing-key' change should be configured as 'lookup.partial-cache.caching-missing-key'.
An error occurred when I configured a dimension table.
The configuration given by the official website is not available.
!image-2023-03-14-05-45-06-230.png!
!image-2023-03-14-05-45-44-616.png!",JunRuiLi,gaara,Major,Resolved,Fixed,14/Mar/23 05:46,15/Mar/23 09:44
Bug,FLINK-31442,13528356,Scala suffix checker fails for release-1.15,"{code:bash}
08:53:07,511 ERROR org.apache.flink.tools.ci.suffixcheck.ScalaSuffixChecker     [] - Violations found:
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-formats/flink-sequence-file/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-hadoop-compatibility/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-connector-hbase-1.4/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-connector-hbase-2.2/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-hcatalog/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-connector-hive/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-table/flink-sql-client/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-tests/pom.xml'.
	Scala-free module 'flink-hcatalog' is referenced with scala suffix in 'flink-connectors/flink-hcatalog/pom.xml'.
	Scala-free module 'flink-sql-connector-hive-1.2.2' is referenced with scala suffix in 'flink-connectors/flink-sql-connector-hive-1.2.2/pom.xml'.
	Scala-free module 'flink-sql-connector-hive-2.2.0' is referenced with scala suffix in 'flink-connectors/flink-sql-connector-hive-2.2.0/pom.xml'.
	Scala-free module 'flink-sql-connector-hive-2.3.6' is referenced with scala suffix in 'flink-connectors/flink-sql-connector-hive-2.3.6/pom.xml'.
	Scala-free module 'flink-sql-connector-hive-3.1.2' is referenced with scala suffix in 'flink-connectors/flink-sql-connector-hive-3.1.2/pom.xml'.
==============================================================================

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47102&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=25873",martijnvisser,martijnvisser,Critical,Closed,Fixed,14/Mar/23 07:53,14/Mar/23 10:20
Bug,FLINK-31460,13528542,Fix hive catalog and connector jar name in the create release script for table store,,zhuangchong,zhuangchong,Major,Closed,Fixed,15/Mar/23 01:58,15/Mar/23 11:59
Bug,FLINK-31477,13528710,NestedLoopJoinTest.testLeftOuterJoinWithFilter failed on azure ,"{noformat}
 Failures: 
Mar 15 15:52:32 [ERROR]   NestedLoopJoinTest.testLeftOuterJoinWithFilter1:37 optimized exec plan expected:<...[InnerJoin], where=[[true], select=[a, e, f], build=[left])
Mar 15 15:52:32    :- Exchange(distribution=[broadcast])
Mar 15 15:52:32    :  +- Calc(select=[a], where=[(a = 10)])
Mar 15 15:52:32    :     +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable1, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
Mar 15 15:52:32    +- Calc(select=[e, f], where=[(d = 10])])
Mar 15 15:52:32       +- LegacyT...> but was:<...[InnerJoin], where=[[(a = d)], select=[a, d, e, f], build=[left])
Mar 15 15:52:32    :- Exchange(distribution=[broadcast])
Mar 15 15:52:32    :  +- Calc(select=[a], where=[SEARCH(a, Sarg[10])])
Mar 15 15:52:32    :     +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable1, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
Mar 15 15:52:32    +- Calc(select=[d, e, f], where=[SEARCH(d, Sarg[10]])])
Mar 15 15:52:32       +- LegacyT...>{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47202&view=logs&j=086353db-23b2-5446-2315-18e660618ef2&t=6cd785f3-2a2e-58a8-8e69-b4a03be28843",337361684@qq.com,leonard,Major,Resolved,Fixed,16/Mar/23 01:40,20/Mar/23 07:23
Bug,FLINK-31478,13528716,"TypeError: a bytes-like object is required, not 'JavaList' is thrown when ds.execute_and_collect() is called on a KeyedStream","{code}
################################################################################
#  Licensed to the Apache Software Foundation (ASF) under one
#  or more contributor license agreements.  See the NOTICE file
#  distributed with this work for additional information
#  regarding copyright ownership.  The ASF licenses this file
#  to you under the Apache License, Version 2.0 (the
#  ""License""); you may not use this file except in compliance
#  with the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
# limitations under the License.
################################################################################
import argparse
import logging
import sys

from pyflink.common import WatermarkStrategy, Encoder, Types
from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode
from pyflink.datastream.connectors.file_system import (FileSource, StreamFormat, FileSink,
                                                       OutputFileConfig, RollingPolicy)


word_count_data = [""To be, or not to be,--that is the question:--"",
                   ""Whether 'tis nobler in the mind to suffer"",
                   ""The slings and arrows of outrageous fortune"",
                   ""Or to take arms against a sea of troubles,"",
                   ""And by opposing end them?--To die,--to sleep,--"",
                   ""No more; and by a sleep to say we end"",
                   ""The heartache, and the thousand natural shocks"",
                   ""That flesh is heir to,--'tis a consummation"",
                   ""Devoutly to be wish'd. To die,--to sleep;--"",
                   ""To sleep! perchance to dream:--ay, there's the rub;"",
                   ""For in that sleep of death what dreams may come,"",
                   ""When we have shuffled off this mortal coil,"",
                   ""Must give us pause: there's the respect"",
                   ""That makes calamity of so long life;"",
                   ""For who would bear the whips and scorns of time,"",
                   ""The oppressor's wrong, the proud man's contumely,"",
                   ""The pangs of despis'd love, the law's delay,"",
                   ""The insolence of office, and the spurns"",
                   ""That patient merit of the unworthy takes,"",
                   ""When he himself might his quietus make"",
                   ""With a bare bodkin? who would these fardels bear,"",
                   ""To grunt and sweat under a weary life,"",
                   ""But that the dread of something after death,--"",
                   ""The undiscover'd country, from whose bourn"",
                   ""No traveller returns,--puzzles the will,"",
                   ""And makes us rather bear those ills we have"",
                   ""Than fly to others that we know not of?"",
                   ""Thus conscience does make cowards of us all;"",
                   ""And thus the native hue of resolution"",
                   ""Is sicklied o'er with the pale cast of thought;"",
                   ""And enterprises of great pith and moment,"",
                   ""With this regard, their currents turn awry,"",
                   ""And lose the name of action.--Soft you now!"",
                   ""The fair Ophelia!--Nymph, in thy orisons"",
                   ""Be all my sins remember'd.""]


def word_count(input_path, output_path):
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_runtime_mode(RuntimeExecutionMode.BATCH)
    # write all the data to one file
    env.set_parallelism(1)

    # define the source
    if input_path is not None:
        ds = env.from_source(
            source=FileSource.for_record_stream_format(StreamFormat.text_line_format(),
                                                       input_path)
                             .process_static_file_set().build(),
            watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(),
            source_name=""file_source""
        )
    else:
        print(""Executing word_count example with default input data set."")
        print(""Use --input to specify file input."")
        ds = env.from_collection(word_count_data)

    def split(line):
        yield from line.split()

    # compute word count
    ds = ds.flat_map(split) \
           .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \
           .key_by(lambda i: i[0])
           # .reduce(lambda i, j: (i[0], i[1] + j[1]))

    # define the sink
    if output_path is not None:
        ds.sink_to(
            sink=FileSink.for_row_format(
                base_path=output_path,
                encoder=Encoder.simple_string_encoder())
            .with_output_file_config(
                OutputFileConfig.builder()
                .with_part_prefix(""prefix"")
                .with_part_suffix("".ext"")
                .build())
            .with_rolling_policy(RollingPolicy.default_rolling_policy())
            .build()
        )
    else:
        print(""Printing result to stdout. Use --output to specify output path."")
        a = list(ds.execute_and_collect())


if __name__ == '__main__':
    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=""%(message)s"")

    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--input',
        dest='input',
        required=False,
        help='Input file to process.')
    parser.add_argument(
        '--output',
        dest='output',
        required=False,
        help='Output file to write results to.')

    argv = sys.argv[1:]
    known_args, _ = parser.parse_known_args(argv)

    word_count(known_args.input, known_args.output)
{code}

For the above job, the following exception will be thrown:
{code}
Traceback (most recent call last):
  File ""/Users/dianfu/code/src/workspace/pyflink-examples/udf/test_udf_perf.py"", line 131, in <module>
    word_count(known_args.input, known_args.output)
  File ""/Users/dianfu/code/src/workspace/pyflink-examples/udf/test_udf_perf.py"", line 110, in word_count
    a = list(ds.execute_and_collect())
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/datastream/data_stream.py"", line 2920, in __next__
    return self.next()
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/datastream/data_stream.py"", line 2931, in next
    return convert_to_python_obj(self._j_closeable_iterator.next(), self._type_info)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/datastream/utils.py"", line 72, in convert_to_python_obj
    fields.append(pickled_bytes_to_python_converter(data, field_type))
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/datastream/utils.py"", line 91, in pickled_bytes_to_python_converter
    data = pickle.loads(data)
TypeError: a bytes-like object is required, not 'JavaList'
{code}

See more details on https://apache-flink.slack.com/archives/C03G7LJTS2G/p1678894062180649",dianfu,dianfu,Major,Closed,Fixed,16/Mar/23 02:44,17/Mar/23 15:16
Bug,FLINK-31479,13528720,Close blocking iterators in tests,Several blocking iterators are not closed in `ContinuousFileStoreITCase`,,dianer17,Major,Closed,Fixed,16/Mar/23 03:10,23/Mar/23 05:32
Bug,FLINK-31485,13528760,Connecting to Kafka and Avro Schema Registry fails with ClassNotFoundException,"When running the SQL Client and using flink-sql-connector-kafka, flink-sql-avro and flink-sql-avro-confluent-registry and trying to query Schema Registry, the job will fail with

{code:bash}
[ERROR] Could not execute SQL statement. Reason:
java.lang.ClassNotFoundException: com.google.common.base.Ticker
{code}",martijnvisser,martijnvisser,Blocker,Closed,Fixed,16/Mar/23 09:04,17/Mar/23 07:56
Bug,FLINK-31486,13528790,Using KeySelector in IterationBody causes ClassCastException,"We have the following code which uses CoGroup along with KeySelector in an IterationBody. When we submit to Flink Session cluster, the exception raises.
{code:java}
public static void main(String[] args) throws Exception {
    Configuration config = new Configuration();
    config.set(HeartbeatManagerOptions.HEARTBEAT_TIMEOUT, 5000000L);
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(config);
    env.setStateBackend(new EmbeddedRocksDBStateBackend());
    env.getConfig().enableObjectReuse();
    env.setRestartStrategy(RestartStrategies.noRestart());
    env.setParallelism(1);
    env.getCheckpointConfig().disableCheckpointing();

    StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

    int num = 400;
    int types = num / 10;

    Random rand = new Random(0);
    long[] randoms = new long[types];
    for (int i = 0; i < types; i++) {
        randoms[i] = rand.nextInt(types);
    }

    SourceFunction<Row> rowGenerator =
            new SourceFunction<Row>() {
                @Override
                public final void run(SourceContext<Row> ctx) throws Exception {
                    int cnt = 0;
                    while (cnt < num) {
                        ctx.collect(
                                Row.of(
                                        randoms[cnt % (types)],
                                        randoms[cnt % (types)],
                                        new DenseVector(10)));
                        cnt++;
                    }
                }

                @Override
                public void cancel() {}
            };

    Table trainDataTable =
            tEnv.fromDataStream(
                    env.addSource(rowGenerator, ""sourceOp-"" + 1)
                            .returns(
                                    Types.ROW(
                                            Types.LONG,
                                            Types.LONG,
                                            DenseVectorTypeInfo.INSTANCE)));

    testCoGroupWithIteration(tEnv, trainDataTable);
}

public static void testCoGroupWithIteration(StreamTableEnvironment tEnv, Table trainDataTable)
        throws Exception {
    DataStream<Row> data1 = tEnv.toDataStream(trainDataTable);
    DataStream<Row> data2 = tEnv.toDataStream(trainDataTable);
    DataStreamList coResult =
            Iterations.iterateBoundedStreamsUntilTermination(
                    DataStreamList.of(data1),
                    ReplayableDataStreamList.notReplay(data2),
                    IterationConfig.newBuilder().build(),
                    new TrainIterationBody());

    List<Integer> counts = IteratorUtils.toList(coResult.get(0).executeAndCollect());
    System.out.println(counts.size());
}

private static class TrainIterationBody implements IterationBody {

    @Override
    public IterationBodyResult process(
            DataStreamList variableStreams, DataStreamList dataStreams) {

        DataStreamList feedbackVariableStream =
                IterationBody.forEachRound(
                        dataStreams,
                        input -> {
                            DataStream<Row> dataStream1 = variableStreams.get(0);
                            DataStream<Row> dataStream2 = dataStreams.get(0);

                            DataStream<Row> coResult =
                                    dataStream1
                                            .coGroup(dataStream2)
                                            .where(
                                                    (KeySelector<Row, Long>)
                                                            t2 -> t2.getFieldAs(0))
                                            .equalTo(
                                                    (KeySelector<Row, Long>)
                                                            t2 -> t2.getFieldAs(1))
                                            .window(EndOfStreamWindows.get())
                                            .apply(
                                                    new RichCoGroupFunction<Row, Row, Row>() {
                                                        @Override
                                                        public void coGroup(
                                                                Iterable<Row> iterable,
                                                                Iterable<Row> iterable1,
                                                                Collector<Row> collector) {
                                                            for (Row row : iterable1) {
                                                                collector.collect(row);
                                                            }
                                                        }
                                                    });
                            return DataStreamList.of(coResult);
                        });

        DataStream<Integer> terminationCriteria =
                feedbackVariableStream
                        .get(0)
                        .flatMap(new TerminateOnMaxIter(2))
                        .returns(Types.INT);

        return new IterationBodyResult(
                feedbackVariableStream, feedbackVariableStream, terminationCriteria);
    }
} {code}
The exception is as below. Note that the exception can not be reproduced in the unittest with MiniCluster since all classes are in the Java classpath.
{code:java}
Caused by: org.apache.flink.streaming.runtime.tasks.StreamTaskException: Could not instantiate state partitioner. at org.apache.flink.streaming.api.graph.StreamConfig.getStatePartitioner(StreamConfig.java:662) at org.apache.flink.iteration.operator.OperatorUtils.createWrappedOperatorConfig(OperatorUtils.java:96) at org.apache.flink.iteration.operator.perround.AbstractPerRoundWrapperOperator.getWrappedOperator(AbstractPerRoundWrapperOperator.java:168) at org.apache.flink.iteration.operator.perround.AbstractPerRoundWrapperOperator.getWrappedOperator(AbstractPerRoundWrapperOperator.java:146) at org.apache.flink.iteration.operator.perround.OneInputPerRoundWrapperOperator.processElement(OneInputPerRoundWrapperOperator.java:68) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753) at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) at java.lang.Thread.run(Thread.java:748) 

Caused by: java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.flink.streaming.api.datastream.CoGroupedStreams$UnionKeySelector.keySelector1 of type org.apache.flink.api.java.functions.KeySelector in instance of org.apache.flink.streaming.api.datastream.CoGroupedStreams$UnionKeySelector at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2302) at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1432) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2409) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2327) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2185) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1665) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2403) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2327) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2185) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1665) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:501) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:459) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589) at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:543) at org.apache.flink.streaming.api.graph.StreamConfig.getStatePartitioner(StreamConfig.java:659) ... 17 more  {code}
 ",Jiang Xin,Jiang Xin,Major,Resolved,Fixed,16/Mar/23 12:10,21/Mar/23 03:06
Bug,FLINK-31489,13528822,OperationManagerTest.testCloseOperation failed because it couldn't find a submitted operation,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47218&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=13386

{code}
Mar 16 02:49:52 [ERROR] Tests run: 7, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 30.433 s <<< FAILURE! - in org.apache.flink.table.gateway.service.operation.OperationManagerTest
Mar 16 02:49:52 [ERROR] org.apache.flink.table.gateway.service.operation.OperationManagerTest.testCloseOperation  Time elapsed: 0.042 s  <<< ERROR!
Mar 16 02:49:52 org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the 1734d6cf-cf52-40c5-804f-809e48a9818a.
Mar 16 02:49:52 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:487)
Mar 16 02:49:52 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:518)
Mar 16 02:49:52 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:482)
Mar 16 02:49:52 	at org.apache.flink.table.gateway.service.operation.OperationManager.awaitOperationTermination(OperationManager.java:149)
Mar 16 02:49:52 	at org.apache.flink.table.gateway.service.operation.OperationManagerTest.testCloseOperation(OperationManagerTest.java:199)
Mar 16 02:49:52 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",lincoln.86xy,mapohl,Major,Resolved,Fixed,16/Mar/23 15:56,17/Mar/23 11:36
Bug,FLINK-31492,13528830,AWS Firehose Connector misclassifies IAM permission exceptions as retryable,"The AWS Firehose connector uses an exception classification mechanism to decide if errors writing requests to AWS Firehose are fatal (i.e. non-retryable) or not (i.e. retryable).
{code:java}
private boolean isRetryable(Throwable err) {
    if (!FIREHOSE_FATAL_EXCEPTION_CLASSIFIER.isFatal(err, getFatalExceptionCons())) {
        return false;
    }
    if (failOnError) {
        getFatalExceptionCons()
                .accept(new KinesisFirehoseException.KinesisFirehoseFailFastException(err));
        return false;
    }

    return true;
} {code}
([github|https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws-kinesis-firehose/src/main/java/org/apache/flink/connector/firehose/sink/KinesisFirehoseSinkWriter.java#L252])

This exception classification mechanism compares an exception's actual type with known, fatal exception types (by using Flink's [FatalExceptionClassifier.withExceptionClassifier|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/sink/throwable/FatalExceptionClassifier.java#L60]).  An exception is considered fatal if it is assignable to a given known fatal exception ([code|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/util/ExceptionUtils.java#L479]).

The AWS Firehose SDK throws fatal IAM permission exceptions as [FirehoseException|https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/firehose/model/FirehoseException.html]s, e.g.
{code:java}
software.amazon.awssdk.services.firehose.model.FirehoseException: User: arn:aws:sts::000000000000:assumed-role/example-role/kiam-kiam is not authorized to perform: firehose:PutRecordBatch on resource: arn:aws:firehose:us-east-1:000000000000:deliverystream/example-stream because no identity-based policy allows the firehose:PutRecordBatch action{code}
At the same time, certain subtypes of FirehoseException are retryable and non-fatal (e.g.[https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/firehose/model/LimitExceededException.html]).

The AWS Firehose connector currently wrongly classifies the fatal IAM permission exception as non-fatal. However, the current exception classification mechanism does not easily handle a case where a super-type should be considered fatal, but its child type shouldn't.

To address this issue, AWS services and the AWS SDK use error codes (see e.g. [Firehose's error codes|https://docs.aws.amazon.com/firehose/latest/APIReference/CommonErrors.html] or [S3's error codes|https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ErrorCodeList], see API docs [here|https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/awscore/exception/AwsErrorDetails.html#errorCode()] and [here|https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/awscore/exception/AwsServiceException.html#awsErrorDetails()]) to uniquely identify error conditions and to be used to handle errors by type.

The AWS Firehose connector (and other AWS connectors) currently log to debug when retrying fully failed records ([code|https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws-kinesis-firehose/src/main/java/org/apache/flink/connector/firehose/sink/KinesisFirehoseSinkWriter.java#L213]). This makes it difficult for users to root cause the above issue without enabling debug logs.

 

 

 ",samuelsiebenmann,samuelsiebenmann,Major,Resolved,Fixed,16/Mar/23 16:40,28/Mar/23 17:11
Bug,FLINK-31503,13528980,"""org.apache.beam.sdk.options.PipelineOptionsRegistrar: Provider org.apache.beam.sdk.options.DefaultPipelineOptionsRegistrar not a subtype"" is thrown when executing Python UDFs in SQL Client ","The following exception will be thrown when executing SQL statements containing Python UDFs in SQL Client:
{code}
Caused by: java.util.ServiceConfigurationError: org.apache.beam.sdk.options.PipelineOptionsRegistrar: Provider org.apache.beam.sdk.options.DefaultPipelineOptionsRegistrar not a subtype
        at java.util.ServiceLoader.fail(ServiceLoader.java:239)
        at java.util.ServiceLoader.access$300(ServiceLoader.java:185)
        at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:376)
        at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
        at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
        at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableCollection$Builder.addAll(ImmutableCollection.java:415)
        at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableSet$Builder.addAll(ImmutableSet.java:507)
        at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableSortedSet$Builder.addAll(ImmutableSortedSet.java:528)
        at org.apache.beam.sdk.util.common.ReflectHelpers.loadServicesOrdered(ReflectHelpers.java:199)
        at org.apache.beam.sdk.options.PipelineOptionsFactory$Cache.initializeRegistry(PipelineOptionsFactory.java:2089)
        at org.apache.beam.sdk.options.PipelineOptionsFactory$Cache.<init>(PipelineOptionsFactory.java:2083)
        at org.apache.beam.sdk.options.PipelineOptionsFactory$Cache.<init>(PipelineOptionsFactory.java:2047)
        at org.apache.beam.sdk.options.PipelineOptionsFactory.resetCache(PipelineOptionsFactory.java:581)
        at org.apache.beam.sdk.options.PipelineOptionsFactory.<clinit>(PipelineOptionsFactory.java:547)
        at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.open(BeamPythonFunctionRunner.java:241)
{code}",dianfu,dianfu,Major,Closed,Fixed,17/Mar/23 14:10,19/Mar/23 06:49
Bug,FLINK-31519,13529159,The watermark alignment docs is outdated after FLIP-217 finished,"With FLIP-217 finished, the watermark alignment limitation has been resolved, the *beta* tag and *warning note* can be safely removed.

[1] https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/datastream/event-time/generating_watermarks/#watermark-alignment-_beta_",mason6345,leonard,Major,Closed,Fixed,20/Mar/23 03:16,30/May/23 13:49
Bug,FLINK-31527,13529202,ChangelogRescalingITCase.test failed due to FileNotFoundException,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47369&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10238

{code}
Mar 20 01:31:54 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 15.209 s <<< FAILURE! - in org.apache.flink.test.state.ChangelogRescalingITCase
Mar 20 01:31:54 [ERROR] ChangelogRescalingITCase.test  Time elapsed: 8.492 s  <<< FAILURE!
Mar 20 01:31:54 org.opentest4j.AssertionFailedError: Graph is in globally terminal state (FAILED)
Mar 20 01:31:54 	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:42)
Mar 20 01:31:54 	at org.junit.jupiter.api.Assertions.fail(Assertions.java:147)
Mar 20 01:31:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.lambda$waitForAllTaskRunning$3(CommonTestUtils.java:213)
Mar 20 01:31:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:150)
Mar 20 01:31:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:144)
Mar 20 01:31:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForAllTaskRunning(CommonTestUtils.java:208)
Mar 20 01:31:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForAllTaskRunning(CommonTestUtils.java:181)
Mar 20 01:31:54 	at org.apache.flink.test.state.ChangelogRescalingITCase.test(ChangelogRescalingITCase.java:163)
Mar 20 01:31:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 20 01:31:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 20 01:31:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 20 01:31:54 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 20 01:31:54 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Mar 20 01:31:54 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Mar 20 01:31:54 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Mar 20 01:31:54 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
{code}",roman,mapohl,Major,Closed,Fixed,20/Mar/23 07:56,22/Mar/23 17:00
Bug,FLINK-31531,13529351,Test HiveCatalogHiveMetadataTest.testCreateTableWithConstraint failed on azure,"{noformat}
Mar 21 01:11:13 [ERROR] Failures: 
Mar 21 01:11:13 [ERROR]   HiveCatalogHiveMetadataTest.testCreateTableWithConstraints:295 
Mar 21 01:11:13 Expecting value to be true but was false
Mar 21 01:11:13 [INFO] 
Mar 21 01:11:13 [ERROR] Tests run: 370, Failures: 1, Errors: 0, Skipped: 0
{noformat}
 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47400&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f",luoyuxia,leonard,Major,Resolved,Fixed,21/Mar/23 04:50,22/Mar/23 08:02
Bug,FLINK-31541,13529430,Get metrics in Flink WEB UI error,"When i get a metrics from a operator which name contains '[' or ']', it will be return 400 from rest response.

The reason is we can not submit an GET request which params contains '[' or ']', it is invaild in REST.

 

!image-2023-03-21-20-28-56-348.png!",huwh,tanjialiang,Major,Closed,Fixed,21/Mar/23 12:32,29/Mar/23 08:54
Bug,FLINK-31557,13529531,Metric viewUpdater and reporter task in a SingleThreadScheduledExecutor lead to inaccurate PerSecond related metrics,"Currently, metric viewUpdater and reporterTask share the same SingleThreadScheduledExecutor, and customized reporters may have unpredictable logic, such as unreasonable network timeout settings, which can affect viewUpdater's calculation of PerSecond related metrics. For example, a real online problem we encountered, the network timeout of the reporter is set to 10 seconds, and the reporting interval is 15 seconds. When the server is unavailable, the thread is blocked for 10s, resulting in 66.7% (5/3x) higher PerSecond related metrics.

Is it possible to optimize here, such as whether it can be changed to a ScheduledThreadPool executor?",huwh,LiuZeshan,Minor,Closed,Fixed,22/Mar/23 03:06,30/Mar/23 13:57
Bug,FLINK-31561,13529585,"flink-master-regression-check is failing since March 15, 2023","[flink-master-benchmarks-regression-check|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/] is failing since March 15, 2023:
{code}
Started by timer
hudson.plugins.git.GitException: Command ""git fetch --tags --progress --prune origin +refs/heads/cutoff:refs/remotes/origin/cutoff"" returned status code 128:
stdout: 
stderr: fatal: Couldn't find remote ref refs/heads/cutoff

	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandIn(CliGitAPIImpl.java:2372)
	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandWithCredentials(CliGitAPIImpl.java:1985)
	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.access$500(CliGitAPIImpl.java:80)
	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$1.execute(CliGitAPIImpl.java:563)
	at jenkins.plugins.git.GitSCMFileSystem$BuilderImpl.build(GitSCMFileSystem.java:348)
	at jenkins.scm.api.SCMFileSystem.of(SCMFileSystem.java:197)
	at jenkins.scm.api.SCMFileSystem.of(SCMFileSystem.java:173)
	at org.jenkinsci.plugins.workflow.cps.CpsScmFlowDefinition.create(CpsScmFlowDefinition.java:115)
	at org.jenkinsci.plugins.workflow.cps.CpsScmFlowDefinition.create(CpsScmFlowDefinition.java:69)
	at org.jenkinsci.plugins.workflow.job.WorkflowRun.run(WorkflowRun.java:299)
	at hudson.model.ResourceController.execute(ResourceController.java:97)
	at hudson.model.Executor.run(Executor.java:429)
Finished: FAILURE
{code}

As a consequence, no regressions are reported in Slack",Yanfei Lei,mapohl,Critical,Closed,Fixed,22/Mar/23 10:18,27/Mar/23 11:29
Bug,FLINK-31588,13529747,The unaligned checkpoint type is wrong at subtask level,"FLINK-20488 supported show checkpoint type for each subtask, and it based on received `CheckpointOptions` and it's right.

However, FLINK-27251 supported timeout aligned to unaligned checkpoint barrier in the output buffers. It means the received `CheckpointOptions` can be converted from aligned checkpoint to unaligned checkpoint.

So, the unaligned checkpoint type may be wrong at subtask level. For example, as shown in the figure below, Unaligned checkpoint type is false, but it is actually Unaligned checkpoint (persisted data > 0).

 

!image-2023-03-23-18-45-01-535.png|width=1879,height=797!

 ",fanrui,fanrui,Major,Closed,Fixed,23/Mar/23 10:45,25/Apr/23 02:23
Bug,FLINK-31609,13529972,Fatal error in ResourceManager caused YARNSessionFIFOSecuredITCase.testDetachedMode to fail,"This looks like FLINK-30908. I created a follow-up ticket because we reached a new minor version.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47547&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461

{code}
Mar 24 09:32:29 2023-03-24 09:31:50,001 ERROR org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl [] - Exception on heartbeat
Mar 24 09:32:29 java.io.InterruptedIOException: Interrupted waiting to send RPC request to server
Mar 24 09:32:29 java.io.InterruptedIOException: Interrupted waiting to send RPC request to server
Mar 24 09:32:29 	at org.apache.hadoop.ipc.Client.call(Client.java:1461) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.ipc.Client.call(Client.java:1403) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at com.sun.proxy.$Proxy33.allocate(Unknown Source) ~[?:?]
Mar 24 09:32:29 	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77) ~[hadoop-yarn-common-2.10.2.jar:?]
Mar 24 09:32:29 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
Mar 24 09:32:29 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
Mar 24 09:32:29 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
Mar 24 09:32:29 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
Mar 24 09:32:29 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:433) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:166) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:158) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:96) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:362) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at com.sun.proxy.$Proxy34.allocate(Unknown Source) ~[?:?]
Mar 24 09:32:29 	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:297) ~[hadoop-yarn-client-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread.run(AMRMClientAsyncImpl.java:274) [hadoop-yarn-client-2.10.2.jar:?]
Mar 24 09:32:29 Caused by: java.lang.InterruptedException
Mar 24 09:32:29 	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404) ~[?:1.8.0_292]
Mar 24 09:32:29 	at java.util.concurrent.FutureTask.get(FutureTask.java:191) ~[?:1.8.0_292]
Mar 24 09:32:29 	at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1177) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.ipc.Client.call(Client.java:1456) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	... 17 more
{code}",ferenc-csaky,mapohl,Critical,Closed,Fixed,24/Mar/23 12:33,15/May/23 15:09
Bug,FLINK-31612,13530050,ClassNotFoundException when using GCS path as HA directory,"Hi,

When I am trying to run Flink job in HA mode with GCS path as a HA directory (eg: [gs://flame-poc/ha]) or while starting a job from checkpoints in GCS I am getting following exception:
{code:java}
Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback not found
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2712) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.<init>(Groups.java:107) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.<init>(Groups.java:102) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:338) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575) ~[?:?]
	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getUgiUserName(GoogleHadoopFileSystemBase.java:1226) ~[?:?]
	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:858) ~[?:?]
	at org.apache.flink.fs.gs.org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.listStatus(HadoopFileSystem.java:170) ~[?:?] {code}
{*}Observations{*}:

While using File system as a HA path and GCS as checkpointing directory the job is able to write checkpoints to GCS checkpoint path. 

After debugging what I found was all the *org.apache.hadoop* paths are shaded to {*}org.apache.flink.fs.shaded.hadoop3.org{*}{*}.apache.hadoop{*}. Ideally the code should look for  {*}org.apache.flink.fs.shaded.hadoop3.org{*}{*}.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback{*} instead of  *org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.*
I think it is not getting shaded over here due to reflection being used here:
[https://github.com/apache/hadoop/blob/branch-3.3.4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/Groups.java#L108]

As a workaround I rebuilt *flink-gs-fs-hadoop* plugin removing this relocation and it worked for me.
{code:java}
<relocation>
<pattern>org.apache.hadoop</pattern>
<shadedPattern>org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop</shadedPattern>
</relocation> {code}",martijnvisser,mohit.aggarwal,Critical,Closed,Fixed,25/Mar/23 08:13,28/Mar/23 14:32
Bug,FLINK-31620,13530148,ReducingUpsertWriter does not flush the wrapped writer,"According to {{SinkWriter#flush}} [javadoc|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/connector/sink2/SinkWriter.java#L43-L47], the writer must flush its records to guarantee AT_LEAST_ONCE.

{{upsert-kafka}}'s {{ReducingUpsertWriter}} inserts buffered records into the wrapped writer, but does not flush it:
1. SinkWriter#flush implementation - https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriter.java#L88-L91.
2. The actual flush code - https://github.com/apache/flink/blob/f3c653ed2e4264315ed83a5b4b2494a7dcc41474/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriter.java#L143-L150.
",Gerrrr,Gerrrr,Blocker,Closed,Fixed,26/Mar/23 22:33,12/Apr/23 17:11
Bug,FLINK-31623,13530191,Fix DataStreamUtils#sample with approximate uniform sampling,"Current implementation employs two-level sampling method.

However, when data instances are imbalanced distributed among partitions (subtasks), the probabilities of instances to be sampled are different in different partitions (subtasks), i.e., not a uniform sampling.

 

In addition, one side-effect of current implementation is: one subtask has a memory footprint of `2 * numSamples * sizeof(element)`, which could cause unexpected OOM in some situations.",,hongfanxo,Major,Resolved,Fixed,27/Mar/23 07:25,03/Apr/23 07:16
Bug,FLINK-31626,13530203,HsSubpartitionFileReaderImpl should recycle skipped read buffers.,"In FLINK-30189, {{HsSubpartitionFileReaderImpl}} will skip the buffer has been consumed from memory to avoid double-consumption. But these buffers were not returned to the {{BatchShuffleReadBufferPool}}, resulting in read buffer leaks. In addition, all loaded buffers should also be recycled after data view released.",Weijie Guo,Weijie Guo,Major,Closed,Fixed,27/Mar/23 08:45,31/Mar/23 09:42
Bug,FLINK-31627,13530212,docker-build.sh build fails on Linux machines,"Building the Flink website on Linux fails due to how Docker is used as a Daemon running under root in Linux (see [this blog|https://jtreminio.com/blog/running-docker-containers-as-current-host-user/#ok-so-what-actually-works] for more details).

Building the website will fail when copying the artifacts because they are owned by the root user. 

{code}
./docker-build.sh build                                                                                     
latest: Pulling from jakejarvis/hugo-extended                                                                                                
Digest: sha256:7d7eb41d7949b5ed338c27926098b84e152e7e1d8ad8f1955c29b383a2336548                                                              
Status: Image is up to date for jakejarvis/hugo-extended:latest                                                                              
docker.io/jakejarvis/hugo-extended:latest                                                                                                    
Start building sites …                                                                                                                       
hugo v0.111.3-5d4eb5154e1fed125ca8e9b5a0315c4180dab192+extended linux/amd64 BuildDate=2023-03-12T11:40:50Z VendorInfo=docker                 
Error: Error building site: open /src/target/news/2014/08/26/release-0.6.html: permission denied                                             
Total in 153 ms                                                                                                                              
mv: cannot move 'docs/target/2014' to 'content/2014': Permission denied                                                                      
mv: cannot move 'docs/target/2015' to 'content/2015': Permission denied                                                                      
mv: cannot move 'docs/target/2016' to 'content/2016': Permission denied                                                                      
mv: cannot move 'docs/target/2017' to 'content/2017': Permission denied                                                                      
mv: cannot move 'docs/target/2018' to 'content/2018': Permission denied                                                                      
mv: cannot move 'docs/target/2019' to 'content/2019': Permission denied                                                                      
mv: cannot move 'docs/target/2020' to 'content/2020': Permission denied                                                                      
mv: cannot move 'docs/target/2021' to 'content/2021': Permission denied                                                                      
mv: cannot move 'docs/target/2022' to 'content/2022': Permission denied                                                                                                                                                                                                                   
mv: cannot move 'docs/target/2023' to 'content/2023': Permission denied                                                                      
mv: cannot move 'docs/target/categories' to 'content/categories': Permission denied
[...]
{code}",mapohl,mapohl,Major,Resolved,Fixed,27/Mar/23 09:16,29/Mar/23 14:33
Bug,FLINK-31628,13530214,ArrayIndexOutOfBoundsException in watermark processing,"After upgrading a job from Flink 1.16.1 to 1.17.0, my task managers throw the following exception:

 

 
{code:java}
java.lang.ArrayIndexOutOfBoundsException: Index -2147483648 out of bounds for length 5
	at org.apache.flink.streaming.runtime.watermarkstatus.HeapPriorityQueue.removeInternal(HeapPriorityQueue.java:155)
	at org.apache.flink.streaming.runtime.watermarkstatus.HeapPriorityQueue.remove(HeapPriorityQueue.java:100)
	at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve$InputChannelStatus.removeFrom(StatusWatermarkValve.java:300)
	at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve$InputChannelStatus.access$200(StatusWatermarkValve.java:266)
	at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve.markWatermarkUnaligned(StatusWatermarkValve.java:222)
	at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve.inputWatermarkStatus(StatusWatermarkValve.java:140)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:153)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:85)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.base/java.lang.Thread.run(Unknown Source){code}
I never saw this before. The job has multiple Kafka inputs, but doesn't use watermark alignment.

 

 

Initially reported [on Slack|https://apache-flink.slack.com/archives/C03G7LJTS2G/p1679908171461309], where a relation to FLINK-28853 was suspected.",wanglijie,michael.helmling,Major,Closed,Fixed,27/Mar/23 09:25,13/Apr/23 08:06
Bug,FLINK-31629,13530229,Trying to access closed classloader when submit query to restSqlGateway via SqlClient,"When I attempted to resubmit the same SQL job(Using HiveCatalog) to SqlGateway through SqlClient, I encountered the error shown in the figure.

!screenshot-1.png|width=649,height=263!

 ",Weijie Guo,Weijie Guo,Major,Closed,Fixed,27/Mar/23 10:53,21/Apr/23 03:36
Bug,FLINK-31632,13530316,watermark aligned idle source can't resume," 
{code:java}
WatermarkStrategy<String> watermarkStrategy = WatermarkStrategy
        .<String>forBoundedOutOfOrderness(Duration.ofMillis(0))
        .withTimestampAssigner((element, recordTimestamp) -> Long.parseLong(element))
        .withWatermarkAlignment(""group"", Duration.ofMillis(10), Duration.ofSeconds(2))
        .withIdleness(Duration.ofSeconds(10)); 
DataStreamSource<String> s1 = env.fromSource(kafkaSource(""topic1""), watermarkStrategy, ""S1"");
DataStreamSource<String> s2 = env.fromSource(kafkaSource(""topic2""), watermarkStrategy, ""S2"");{code}
send ""0"" to kafka topic1 and topic2

 

After 10s, source1 and source2 is idle，and logs are

 
{code:java}
09:44:30,403 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ -1 (1970-01-01 07:59:59.999) from subTaskId=0
09:44:30,404 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ -1 (1970-01-01 07:59:59.999) from subTaskId=0
09:44:32,019 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=9 to subTaskIds=[0]
09:44:32,019 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=9 to subTaskIds=[0]
09:44:32,417 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ -1 (1970-01-01 07:59:59.999) from subTaskId=0
09:44:32,418 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ -1 (1970-01-01 07:59:59.999) from subTaskId=0
09:44:34,028 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=9 to subTaskIds=[0]
09:44:34,028 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=9 to subTaskIds=[0]
09:44:34,423 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ 9223372036854775807 (292278994-08-17 15:12:55.807) from subTaskId=0
09:44:34,424 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ 9223372036854775807 (292278994-08-17 15:12:55.807) from subTaskId=0
09:44:36,023 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=-9223372036854775799 to subTaskIds=[0]
09:44:36,023 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=-9223372036854775799 to subTaskIds=[0]
09:44:36,433 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ 9223372036854775807 (292278994-08-17 15:12:55.807) from subTaskId=0
09:44:36,433 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ 9223372036854775807 (292278994-08-17 15:12:55.807) from subTaskId=0 {code}
send message to topic1 or topic2 now, the message can't be consumed。

 

the reason is: 

when a source is marked idle, the lastEmittedWatermark = Long.MAX_VALUE and 
currentMaxDesiredWatermark = Long.MAX_VALUE + maxAllowedWatermarkDrift in org.apache.flink.streaming.api.operators.SourceOperator.
currentMaxDesiredWatermark is negative and always less than lastEmittedWatermark
operatingMode always is WAITING_FOR_ALIGNMENT",haishui,haishui,Critical,Closed,Fixed,28/Mar/23 02:07,26/Apr/23 02:57
Bug,FLINK-31652,13530560,Flink should handle the delete event if the pod was deleted while pending,"I found that in kubernetes deployment, if the taskmanager pod is deleted in 'Pending' phase, the flink job will get stuck and keep waiting for the pod scheduled. We can reproduce this issue with the 'kubectl delete pod' command to delete the pod when it is in the pending phase.
 
The cause reason is that the pod status will not be updated in time in this case, so the KubernetesResourceManagerDriver won't detect the pod is terminated, and I also verified this by logging the pod status in KubernetesPod#isTerminated(), and it shows as follows.
{code:java}
public boolean isTerminated() {
    log.info(""pod status: "" + getInternalResource().getStatus());
    if (getInternalResource().getStatus() != null) {
        final boolean podFailed =
                PodPhase.Failed.name().equals(getInternalResource().getStatus().getPhase());
        final boolean containersFailed =
                getInternalResource().getStatus().getContainerStatuses().stream()
                        .anyMatch(
                                e ->
                                        e.getState() != null
                                                && e.getState().getTerminated() != null);
        return containersFailed || podFailed;
    }
    return false;
} {code}
In the case, this function will return false because `containersFailed` and `podFailed` are both false.
{code:java}
PodStatus(conditions=[PodCondition(lastProbeTime=null, lastTransitionTime=2023-03-28T12:35:10Z, reason=Unschedulable, status=False, type=PodScheduled, additionalProperties={})], containerStatuses=[], ephemeralContainerStatuses=[], hostIP=null, initContainerStatuses=[], message=null, nominatedNodeName=null, phase=Pending, podIP=null, podIPs=[], qosClass=Guaranteed, reason=null, startTime=null, additionalProperties={}){code}
 
 ",xiasun,xiasun,Major,Closed,Fixed,29/Mar/23 06:57,03/Apr/23 08:05
Bug,FLINK-31657,13530599,ConfigurationInfo generates incorrect openapi schema,"ConfigurationInfo extends ArrayList, and the schema generator picks up List#isEmpty as a property.
This results in an invalid schema, as arrays cant have properties.",chesnay,chesnay,Major,Closed,Fixed,29/Mar/23 11:08,30/Mar/23 08:31
Bug,FLINK-31660,13530662,flink-connectors-kafka ITCases are not runnable in the IDE,"The following exception is thrown when trying to run {{KafkaChangelogTableITCase}} or {{KafkaTableITCase}}
{code:java}
java.lang.NoClassDefFoundError: org/apache/flink/table/shaded/com/jayway/jsonpath/spi/json/JsonProvider    at java.base/java.lang.Class.getDeclaredMethods0(Native Method)
    at java.base/java.lang.Class.privateGetDeclaredMethods(Class.java:3166)
    at java.base/java.lang.Class.getMethodsRecursive(Class.java:3307)
    at java.base/java.lang.Class.getMethod0(Class.java:3293)
    at java.base/java.lang.Class.getMethod(Class.java:2106)
    at org.apache.calcite.linq4j.tree.Types.lookupMethod(Types.java:309)
    at org.apache.calcite.util.BuiltInMethod.<init>(BuiltInMethod.java:670)
    at org.apache.calcite.util.BuiltInMethod.<clinit>(BuiltInMethod.java:357)
    at org.apache.calcite.rel.metadata.BuiltInMetadata$PercentageOriginalRows.<clinit>(BuiltInMetadata.java:344)
    at org.apache.calcite.rel.metadata.RelMdPercentageOriginalRows$RelMdPercentageOriginalRowsHandler.getDef(RelMdPercentageOriginalRows.java:231)
    at org.apache.calcite.rel.metadata.ReflectiveRelMetadataProvider.reflectiveSource(ReflectiveRelMetadataProvider.java:134)
    at org.apache.calcite.rel.metadata.RelMdPercentageOriginalRows.<clinit>(RelMdPercentageOriginalRows.java:42)
    at org.apache.calcite.rel.metadata.DefaultRelMetadataProvider.<init>(DefaultRelMetadataProvider.java:42)
    at org.apache.calcite.rel.metadata.DefaultRelMetadataProvider.<clinit>(DefaultRelMetadataProvider.java:28)
    at org.apache.calcite.plan.RelOptCluster.<init>(RelOptCluster.java:97)
    at org.apache.calcite.plan.RelOptCluster.create(RelOptCluster.java:106)
    at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$.create(FlinkRelOptClusterFactory.scala:36)
    at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory.create(FlinkRelOptClusterFactory.scala)
    at org.apache.flink.table.planner.delegation.PlannerContext.<init>(PlannerContext.java:132)
    at org.apache.flink.table.planner.delegation.PlannerBase.<init>(PlannerBase.scala:121)
    at org.apache.flink.table.planner.delegation.StreamPlanner.<init>(StreamPlanner.scala:65)
    at org.apache.flink.table.planner.delegation.DefaultPlannerFactory.create(DefaultPlannerFactory.java:65)
    at org.apache.flink.table.planner.loader.DelegatePlannerFactory.create(DelegatePlannerFactory.java:36)
    at org.apache.flink.table.factories.PlannerFactoryUtil.createPlanner(PlannerFactoryUtil.java:58)
    at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:127)
    at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:122)
    at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:94)
    at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.setup(KafkaTableTestBase.java:93)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: java.lang.ClassNotFoundException: Class 'org.apache.flink.table.shaded.com.jayway.jsonpath.spi.json.JsonProvider' not found. Perhaps you forgot to add the module 'flink-table-runtime' to the classpath?
    at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:123)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
    ... 61 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.shaded.com.jayway.jsonpath.spi.json.JsonProvider
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
    at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromOwnerOnly(ComponentClassLoader.java:164)
    at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentFirst(ComponentClassLoader.java:158)
    at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:104)
    ... 62 more {code}
This is a similar problem to https://issues.apache.org/jira/browse/FLINK-25525 ",nateab,nateab,Major,Closed,Fixed,29/Mar/23 17:27,08/May/23 14:08
Bug,FLINK-31670,13530769,ElasticSearch connector's document was not incorrect linked to external repo,"In the [doc|https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/elasticsearch/], It still use ""flink-version"" for flink-connector-elastiacsearch instead of the version in the external repository.

<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-elasticsearch6</artifactId>
    <version>1.18-SNAPSHOT</version>
</dependency>",Weijie Guo,Weijie Guo,Major,Closed,Fixed,30/Mar/23 09:06,11/Apr/23 13:15
Bug,FLINK-31675,13530819,Deadlock in AWS Connectors following content-length AWS SDK exception,"Connector calls to AWS services can hang on a canceled future following a content-length mismatch that isn't handled gracefully by the SDK:

 
{code:java}
org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.FutureCancelledException: java.io.IOException: Response had content-length of 31 bytes, but only received 0 bytes before the connection was closed.
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.NettyRequestExecutor.lambda$null$3(NettyRequestExecutor.java:136)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.PromiseTask.run(PromiseTask.java:106)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
at org.apache.flink.kinesis.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
at org.apache.flink.kinesis.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: Response had content-length of 31 bytes, but only received 0 bytes before the connection was closed.
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.ResponseHandler.validateResponseContentLength(ResponseHandler.java:163)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.ResponseHandler.access$700(ResponseHandler.java:75)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.ResponseHandler$PublisherAdapter$1.onComplete(ResponseHandler.java:369)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.nrs.HandlerPublisher.complete(HandlerPublisher.java:447)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.nrs.HandlerPublisher.channelInactive(HandlerPublisher.java:430)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
at org.apache.flink.kinesis.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
at org.apache.flink.kinesis.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
at org.apache.flink.kinesis.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
at org.apache.flink.kinesis.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
at org.apache.flink.kinesis.shaded.io.netty.handler.codec.http2.AbstractHttp2StreamChannel$Http2ChannelUnsafe$2.run(AbstractHttp2StreamChannel.java:737)
... 6 more {code}
Related AWS SDK issue: [https://github.com/aws/aws-sdk-java-v2/issues/3335]

AWS SDK fix: [https://github.com/aws/aws-sdk-java-v2/pull/3855/files]

 

This mishandled exception creates a deadlock situation that prevents the connectors from making any progress.

We should update the AWS SDK v2 to 2.20.32: [https://github.com/aws/aws-sdk-java-v2/commit/eb5619e24e4eaca6f80effa1c43c0cd409cdd53e]",antoniovespoli,antoniovespoli,Major,Resolved,Fixed,30/Mar/23 14:34,26/Apr/23 14:33
Bug,FLINK-31681,13530923,Network connection timeout between operators should trigger either network re-connection or job failover,"If a network connection error occurs between two operators, the upstream operator may log the following error message in the method PartitionRequestQueue#handleException and subsequently close the connection. When this happens, the Flink job may become stuck without completing or failing. 

To avoid this issue, we can either allow the upstream operator to reconnect with the downstream operator, or enable job failover so that users can take corrective action promptly.

org.apache.flink.runtime.io.network.netty.PartitionRequestQueue - Encountered error while consuming partitions org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors#NativeIOException: writeAccess(...) failed: Connection timed out.",,lindong,Major,Closed,Fixed,31/Mar/23 08:18,12/Apr/23 08:45
Bug,FLINK-31683,13530941,Align the outdated Chinese filesystem connector docs,"The current Chinese doc of the file system SQL connector is outdated from Flink-1.15, we should fix it to avoid misunderstanding.",yunta,yunta,Major,Resolved,Fixed,31/Mar/23 09:27,31/Mar/23 13:55
Bug,FLINK-31688,13531121,Broken links in docs for Azure Table Storage,"The doc page of https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/dataset/formats/azure_table_storage/ has a broken links, we should fix it.",tanyuxin,tanyuxin,Major,Closed,Fixed,02/Apr/23 13:40,03/Apr/23 05:39
Bug,FLINK-31690,13531142,The current key is not set for KeyedCoProcessOperator,See https://apache-flink.slack.com/archives/C03G7LJTS2G/p1680294701254239 for more details.,dianfu,dianfu,Major,Closed,Fixed,03/Apr/23 02:32,03/Apr/23 05:44
Bug,FLINK-31707,13531260,Constant string cannot be used as input arguments of Pandas UDAF,"It will throw exceptions as following when using constant strings in Pandas UDAF:
{code}
E                       raise ValueError(""field_type %s is not supported."" % field_type)
E                   ValueError: field_type type_name: CHAR
E                   char_info {
E                     length: 3
E                   }
E                    is not supported.
{code}",dianfu,dianfu,Major,Closed,Fixed,03/Apr/23 11:39,04/Apr/23 05:26
Bug,FLINK-31708,13531261,RuntimeException/KryoException thrown when deserializing an empty protobuf record,"h1. Problem description

I am using protobuf defined Class in Flink job. When the application runs on production, the job throws following Exception:
{code:java}
java.lang.RuntimeException: Could not create class com.MYClass <==== generated by protobuf
        at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:76)
        at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:40)
        at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:813)
        at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:346)
        at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:205)
        at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:46)
        at org.apache.flink.runtime.plugable.NonReusingDeserializationDelegate.read(NonReusingDeserializationDelegate.java:55)
        at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.getNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:141)
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:121)
        at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:185)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:319)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:494)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:478)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)
        at java.lang.Thread.run(Thread.java:748)
Caused by: com.esotericsoftware.kryo.KryoException: java.io.EOFException: No more bytes left.
        at org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes(NoFetchingInput.java:127)
        at com.esotericsoftware.kryo.io.Input.readBytes(Input.java:332)
        at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:73)
        ... 16 common frames omitted
 {code}
h1. How to reproduce

I think this is similar to another issue: FLINK-29347.

Follwing is an example to reproduce the problem:
{code:java}
package com.test;

import com.test.ProtobufGeneratedClass;

import com.google.protobuf.Message;
import com.twitter.chill.protobuf.ProtobufSerializer;
import lombok.extern.slf4j.Slf4j;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.restartstrategy.RestartStrategies;
import org.apache.flink.api.common.state.MapStateDescriptor;
import org.apache.flink.api.common.time.Time;
import org.apache.flink.api.java.utils.MultipleParameterTool;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.contrib.streaming.state.RocksDBStateBackend;
import org.apache.flink.streaming.api.CheckpointingMode;
import org.apache.flink.streaming.api.datastream.BroadcastStream;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.co.KeyedBroadcastProcessFunction;
import org.apache.flink.streaming.api.functions.sink.SinkFunction;
import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;
import org.apache.flink.util.Collector;
import org.apache.flink.util.OutputTag;

import java.util.Random;
@Slf4j
public class app {
  public static final OutputTag<ProtobufGeneratedClass> OUTPUT_TAG_1 =
      new OutputTag<ProtobufGeneratedClass>(""output-tag-1"") {
  };

  public static final OutputTag<ProtobufGeneratedClass> OUTPUT_TAG_2 =
      new OutputTag<ProtobufGeneratedClass>(""output-tag-2"") {
  };

  public static final OutputTag<ProtobufGeneratedClass> OUTPUT_TAG_3 =
      new OutputTag<ProtobufGeneratedClass>(""output-tag-3"") {
  };

  public static class MySourceFunction extends RichParallelSourceFunction<ProtobufGeneratedClass> {
    Random rnd = new Random();
    private final String name;

    private boolean running = true;

    private MySourceFunction(String name) {
      this.name = name;
    }

    @Override
    public void run(SourceContext<ProtobufGeneratedClass> sourceContext) throws Exception {
      final int index = getRuntimeContext().getIndexOfThisSubtask();
      int counter = 0;

      while (running) {
        synchronized (sourceContext.getCheckpointLock()) {
          ++counter;
          ProtobufGeneratedClass.Builder builder = ProtobufGeneratedClass.newBuilder();
          if (rnd.nextBoolean()) {

            builder.addGraphIds(rnd.nextInt(10));
            byte[] bytes;
            if (rnd.nextInt(10) == 1) {
              // make sure record is large enough to reproduce the problem
              // in which case, SpillingAdaptiveSpanningRecordDeserializer#spanningWrapper may be activated
              bytes = new byte[rnd.nextInt(5000000)];
            } else if (rnd.nextInt(10) == 2) {
              bytes = new byte[rnd.nextInt(50000)];
            } else {
              bytes = new byte[rnd.nextInt(50)];
            }
            builder.addUserTagNames(new String(bytes));
          } else {
				// create an empty record by do nothing.
          }
          sourceContext.collect(builder.build());
          Thread.sleep(5);
        }
      }
    }

    @Override
    public void cancel() {
      running = false;
    }
  }

  public static void main(String[] args) throws Exception {

    final int SHARD_NUM = 64;
    final MultipleParameterTool params = MultipleParameterTool.fromArgs(args);

    // set up the execution environment
    Configuration config = new Configuration();
    config.setInteger(""state.checkpoints.num-retained"", 5);
    config.setInteger(""taskmanager.numberOfTaskSlots"", 1);
    config.setInteger(""local.number-taskmanager"", 4);
    StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(3, config);

    RocksDBStateBackend rocksDBStateBackend =
        new RocksDBStateBackend(""file:///Users/shenjiaqi/Workspace/state/checkpoints/"", true);

    env.setParallelism(3);
    env.setStateBackend(rocksDBStateBackend);
    env.getCheckpointConfig().setCheckpointTimeout(100000);
    env.enableCheckpointing(1000, CheckpointingMode.EXACTLY_ONCE);
    env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1000, Time.seconds(10)));
    env.addDefaultKryoSerializer(Message.class, ProtobufSerializer.class); // make sure ProtobufSerializer is serialized/deserialized by protobuf.

    // make parameters available in the web interface
    env.getConfig().setGlobalJobParameters(params);

    String[] words = new String[100000];
    Random rnd = new Random();
    for (int i = 0; i < words.length; ++i) {
      words[i] = String.valueOf(rnd.nextInt(10));
    }
    DataStreamSource<ProtobufGeneratedClass> stream1 = env.addSource(new MySourceFunction(""randomProtobufGeneratedClass1"")).setParallelism(4);
    BroadcastStream<ProtobufGeneratedClass> stream2 = env.addSource(new MySourceFunction(""randomProtobufGeneratedClass2"")).setParallelism(3)
        .broadcast(new MapStateDescriptor[0]);
    SingleOutputStreamOperator<ProtobufGeneratedClass> output = stream1.shuffle()
        .map(new MapFunction<ProtobufGeneratedClass, ProtobufGeneratedClass>() {

          @Override
          public ProtobufGeneratedClass map(ProtobufGeneratedClass value) throws Exception {
            return value;
          }
        }).setParallelism(2).disableChaining()
        .keyBy(x -> x.hashCode() % 10)
        .connect(stream2)
        .process(new MyProcessFunction()).disableChaining();

    output.getSideOutput(OUTPUT_TAG_1).rescale().
        addSink(new SinkFunction<ProtobufGeneratedClass>() {
          @Override
          public void invoke(ProtobufGeneratedClass value) throws Exception {
            log.info(""blah 1"");
          }
        }).setParallelism(1);

    output.getSideOutput(OUTPUT_TAG_2).rescale().
        addSink(new SinkFunction<ProtobufGeneratedClass>() {
          @Override
          public void invoke(ProtobufGeneratedClass value) throws Exception {
            log.info(""blah 2"");
          }
        }).setParallelism(2);

    output.getSideOutput(OUTPUT_TAG_3).rescale().
        addSink(new SinkFunction<ProtobufGeneratedClass>() {
          @Override
          public void invoke(ProtobufGeneratedClass value) throws Exception {
            log.info(""blah 3"");
          }
        }).setParallelism(3);

    output.map(new MapFunction<ProtobufGeneratedClass, String>() {
      @Override
      public String map(ProtobufGeneratedClass value) throws Exception {
        return """" + value.toString().length();
      }
    }).print();
    env.execute(""reproduce-the-problem"");
  }

  public static class MyProcessFunction extends
      KeyedBroadcastProcessFunction<ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass> {

    @Override
    public void processElement(ProtobufGeneratedClass ProtobufGeneratedClass,
        KeyedBroadcastProcessFunction<ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass>.ReadOnlyContext readOnlyContext,
        Collector<ProtobufGeneratedClass> collector) throws Exception {
      collector.collect(ProtobufGeneratedClass);
    }

    @Override
    public void processBroadcastElement(ProtobufGeneratedClass s,
        KeyedBroadcastProcessFunction<ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass>.Context context,
        Collector<ProtobufGeneratedClass> collector) throws Exception {
      context.output(OUTPUT_TAG_1, s);
      context.output(OUTPUT_TAG_2, s);
      context.output(OUTPUT_TAG_3, s);
    }
  }
}

{code}",shenjiaqi,shenjiaqi,Major,Closed,Fixed,03/Apr/23 11:39,20/Apr/23 07:05
Bug,FLINK-31711,13531280,OpenAPI spec omits complete-statement request body,"The OpenAPI generator omits request bodies for get requests because it is usually a bad idea.

Still, the generator shouldn't omit this on it's own.",chesnay,chesnay,Major,Closed,Fixed,03/Apr/23 13:03,04/Apr/23 08:36
Bug,FLINK-31716,13531330,Event UID field is missing the first time that an event is consumed,"on `EventUtils.createOrUpdateEvent` we use a `Consumer<Event>` instance to `accept` the underlying event that is being created or updated.

The first time an event is created, we are calling `client.resource(event).createOrReplace()` but we are discarding the return value of such method, and we are returning the `event` that we just created, which has an empty UID field.",rodmeneses,rodmeneses,Major,Closed,Fixed,03/Apr/23 18:08,19/Apr/23 08:05
Bug,FLINK-31717,13531340,Unit tests running with local kube config,"Some unit tests are using local kube environment. This can be dangerous when pointing to sensitive clusters e.g. in prod.

{quote}2023-04-03 12:32:53,956 i.f.k.c.Config                 [DEBUG] Found for Kubernetes config at: [/Users/<redacted>/.kube/config].
{quote}

A misconfigured kube config environment revealed the issue:

{quote}[ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 0.012 s <<< FAILURE! - in org.apache.flink.kubernetes.operator.FlinkOperatorTest
[ERROR] org.apache.flink.kubernetes.operator.FlinkOperatorTest.testConfigurationPassedToJOSDK  Time elapsed: 0.008 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.kubernetes.operator.FlinkOperatorTest.testConfigurationPassedToJOSDK(FlinkOperatorTest.java:63)

[ERROR] org.apache.flink.kubernetes.operator.FlinkOperatorTest.testLeaderElectionConfig  Time elapsed: 0.004 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.kubernetes.operator.FlinkOperatorTest.testLeaderElectionConfig(FlinkOperatorTest.java:108){quote}

",mateczagany,morhidi,Critical,Closed,Fixed,03/Apr/23 20:28,12/May/23 11:52
Bug,FLINK-31718,13531379,pulsar connector v3.0 branch's CI is not working properly,"After FLINK-30963, we no longer manually set {{flink_url}}, but it is required in pulsar connector's own {{ci.yml}}, which causes CI to fail to run normally. The root of the problem is that the v3.0 branch does not use the {{ci.yml}} in {{flink-connector-shared-utils}}.",Weijie Guo,Weijie Guo,Major,Closed,Fixed,04/Apr/23 04:43,04/Apr/23 06:38
Bug,FLINK-31720,13531399,Pulsar connector v3.0 branch's configuration html files are empty,"Currently, the generated configuration html files of Pulsar connector v3.0 branch are empty. We should add it.",tanyuxin,tanyuxin,Major,Closed,Fixed,04/Apr/23 07:25,04/Apr/23 09:43
Bug,FLINK-31723,13531409,DispatcherTest#testCancellationDuringInitialization is unstable,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47889&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=6761

{noformat}
Apr 04 02:26:26 [ERROR] org.apache.flink.runtime.dispatcher.DispatcherTest.testCancellationDuringInitialization  Time elapsed: 0.033 s  <<< FAILURE!
Apr 04 02:26:26 java.lang.AssertionError: 
Apr 04 02:26:26 
Apr 04 02:26:26 Expected: is <CANCELLING>
Apr 04 02:26:26      but: was <CANCELED>
Apr 04 02:26:26 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Apr 04 02:26:26 	at org.junit.Assert.assertThat(Assert.java:964)
Apr 04 02:26:26 	at org.junit.Assert.assertThat(Assert.java:930)
Apr 04 02:26:26 	at org.apache.flink.runtime.dispatcher.DispatcherTest.testCancellationDuringInitialization(DispatcherTest.java:389)
[...]
{noformat}",dmvk,Sergey Nuyanzin,Critical,Resolved,Fixed,04/Apr/23 08:46,21/Apr/23 13:12
Bug,FLINK-31733,13531564,Model name clashes in OpenAPI spec,"The OpenAPi spec uses simple class names for naming models. There are however several models, usually inner classes, that share simple names, like ""Summary"".

This goes undetected and breaks the model for some API calls.",chesnay,chesnay,Major,Closed,Fixed,05/Apr/23 10:32,06/Apr/23 14:59
Bug,FLINK-31735,13531583,JobDetailsInfo plan incorrectly documented as string,"The {{plan}} field in the JobDefaultsInfo contains an object, not a string. Internally we handle it as a string, but write it out as an object.
The docs generators aren't aware of this.",chesnay,chesnay,Major,Closed,Fixed,05/Apr/23 13:19,06/Apr/23 15:22
Bug,FLINK-31738,13531610,FlameGraphTypeQueryParameter#Type clashes with java.reflect.Type in generated clients,"Generating a client with the openapi generators causes compile errors because the generated file imports java.reflect.Type, but also the generated ""Type"" model.

For convenience it would be neat to give this enum a slightly different name, because working around this issue is surprisingly annoying.",chesnay,chesnay,Major,Resolved,Fixed,05/Apr/23 14:38,11/Apr/23 10:30
Bug,FLINK-31739,13531631,ElasticSearch and Cassandra connector v3.0 branch's CI is not working properly,"After FLINK-30963, we no longer manually set {{flink_url}}, but it is required in some connector's own {{ci.yml}}, which causes CI to fail to run like [this|https://github.com/apache/flink-connector-elasticsearch/actions/runs/4620241065]). The root of this problem is that these branch does not use the {{ci.yml}} in {{flink-connector-shared-utils}}.",Weijie Guo,Weijie Guo,Major,Closed,Fixed,05/Apr/23 15:54,06/Apr/23 14:01
Bug,FLINK-31743,13531718,Avoid relocating the RocksDB's log failure when filename exceeds 255 characters,"Since FLINK-24785 , the file name of the rocksdb LOG is generated by parsing the db path, when the db path is long and the filename exceeds 255 characters, the creation of the file will fail, so the relevant rocksdb LOG cannot be seen in the flink log dir.",Feifan Wang,assassinj,Major,Resolved,Fixed,06/Apr/23 08:54,19/May/23 15:41
Bug,FLINK-31752,13531831,SourceOperatorStreamTask increments numRecordsOut twice,"The counter of numRecordsOut was introduce to ChainingOutput to reduce the function call stack depth in 
https://issues.apache.org/jira/browse/FLINK-30536

But SourceOperatorStreamTask.AsyncDataOutputToOutput increments the counter of numRecordsOut too. This results in the source operator's numRecordsOut are doubled.

We should delete the numRecordsOut.inc in SourceOperatorStreamTask.AsyncDataOutputToOutput.

[~xtsong][~lindong] Could you please take a look at this.
",yunfengzhou,huwh,Major,Closed,Fixed,07/Apr/23 08:00,24/Apr/23 05:07
Bug,FLINK-31758,13532057,Some external connectors sql client jar has a wrong download url in document,"After FLINK-30378, we can load sql connector data from external connector's own data file. However, we did not replace \{{$full_version}}, resulting in an incorrect URL in the download link. for example: {{https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-mongodb/$full_version/flink-sql-connector-mongodb-$full_version.jar.}}",Weijie Guo,Weijie Guo,Major,Closed,Fixed,10/Apr/23 14:59,12/Apr/23 10:11
Bug,FLINK-31763,13532124,Convert requested buffers to overdraft  buffers when pool size is decreased,"As we discussed in FLINK-31610, new buffers can be requested only when ""{_}numOfRequestedMemorySegments + numberOfRequestedOverdraftMemorySegments < poolSize + maxOverdraftBuffersPerGate""{_}.

Consider such a scenario, the {{{}CurrentPoolSize = 5{}}}, {{{}numOfRequestedMemorySegments = 7{}}}, {{{}maxOverdraftBuffersPerGate = 2{}}}. If {{{}numberOfRequestedOverdraftMemorySegments = 0{}}}, then 2 buffers can be requested now. 

We should convert {{numberOfRequestedMemorySegments}} to {{numberOfRequestedOverdraftMemorySegments}} when poolSize is decreased. Further more, we can changes the definition of overdraft buffer from static to dynamic: 
 * When _numberOfRequestedMemorySegments <= poolSize,_ all buffers are ordinary buffer
 * When _numberOfRequestedMemorySegments > poolSize,_ the `{_}ordinary buffer size = poolSize`{_}, and `{_}the overdraft buffer size = numberOfRequestedMemorySegments - poolSize`{_}

This allows us to remove {{{}numberOfRequestedOverdraftMemorySegments{}}}, which helps us simplify logic and maintain consistency.

 ",Weijie Guo,Weijie Guo,Major,Closed,Fixed,11/Apr/23 05:38,17/Apr/23 09:29
Bug,FLINK-31765,13532133,Disable changelog backend for the StatefulJobSavepointMigrationITCase and StatefulJobWBroadcastStateMigrationITCase with RocksDB and checkpoints,"In FLINK-31593 we discovered an instability when generating the migration test data for 1.17 for {{StatefulJobSavepointMigrationITCase}} and {{StatefulJobWBroadcastStateMigrationITCase}}. According to the discussion in FLINK-31593, we concluded that it's caused by a non-determinism that's happening in the changelog backend code. As a workaround, we're going to disable the changelog backend in these tests for now.

We're not touching 1.16 because it didn't appear in that branch. The non-determinism seems to kick in only when generating the checkpoint files. 

For 1.17, we're gonna create a backport for consistency reasons because we have to enable it for the data generation.",mapohl,mapohl,Critical,Resolved,Fixed,11/Apr/23 06:38,13/Apr/23 06:03
Bug,FLINK-31770,13532142,OracleExactlyOnceSinkE2eTest.testInsert fails for JDBC connector,"{code:java}
Caused by: org.apache.flink.util.FlinkRuntimeException: unable to start XA transaction, xid: 201:cea0dbd44c6403283f4050f627bed37c020000000000000000000000:e0070697, error -3: resource manager error has occurred. [XAErr (-3): A resource manager error has occured in the transaction branch. ORA-2045 SQLErr (0)]
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.wrapException(XaFacadeImpl.java:369)
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.access$800(XaFacadeImpl.java:67)
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl$Command.lambda$fromRunnable$0(XaFacadeImpl.java:301)
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl$Command.lambda$fromRunnable$4(XaFacadeImpl.java:340)
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.execute(XaFacadeImpl.java:280)
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.start(XaFacadeImpl.java:170)
	at org.apache.flink.connector.jdbc.xa.XaFacadePoolingImpl.start(XaFacadePoolingImpl.java:84)
	at org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.beginTx(JdbcXaSinkFunction.java:316)
	at org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.open(JdbcXaSinkFunction.java:241)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:100)
	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:731)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.call(StreamTaskActionExecutor.java:100)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:706)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:672)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
	at java.lang.Thread.run(Thread.java:750)
{code}

https://github.com/apache/flink-connector-jdbc/actions/runs/4647776511/jobs/8224977183#step:13:325",martijnvisser,martijnvisser,Blocker,Closed,Fixed,11/Apr/23 08:10,13/Jun/23 09:38
Bug,FLINK-31772,13532179,KinesisStreamsSink Performance regression due to AIMD rate limiting strategy,"h1. Issue

While benchmarking the {{KinesisStreamSink}} for 1.15 against the legacy {{FlinkKinesisProduced}} , it is observed that the new sink has a performance regression against the deprecated sink for same environment setting.

Further investigation identified that the AIMD Ratelimiting strategy is the bottleneck for the regression. 

Attached results for {{KinesisStreamSink}}  against {FlinkKinesisProducer} and {KinesisStreamSink} after disabling {{AIMDRatelimitingStrategy}}



h2. Environment Settings
- Benchmarking was performed on AWS KDA.
- Application logic is just sending records downstream
- Application parallelism was tested to be 1.
- Kinesis stream number of shards was tested with 8 and 12.
- payload size was 1Kb and 100Kb.

 ",chalixar,chalixar,Major,Resolved,Fixed,11/Apr/23 12:07,16/May/23 13:43
Bug,FLINK-31792,13532442,Errors are not reported in the Web UI,"After FLINK-29747, NzNotificationService can no longer be resolved by injector, and because we're using the injector directly, this is silently ignored.",dmvk,dmvk,Critical,Resolved,Fixed,13/Apr/23 05:59,28/Apr/23 16:58
Bug,FLINK-31801,13532588,Missing elasticsearch connector on maven central repository ,There are no versions 3.0.0-1.17 of flink-connector-elasticsearch6 and flink-connector-elasticsearch7 on maven central repository in document https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/datastream/elasticsearch/,martijnvisser,zjureel,Major,Closed,Fixed,14/Apr/23 00:59,19/Jun/23 12:26
Bug,FLINK-31803,13532617,UpdateJobResourceRequirementsRecoveryITCase.testRescaledJobGraphsWillBeRecoveredCorrectly(Path) is unstable on azure,"{noformat}
Apr 07 01:28:23 java.util.concurrent.CompletionException: 
Apr 07 01:28:23 org.apache.flink.runtime.rest.util.RestClientException: [org.apache.flink.runtime.rest.NotFoundException: Job d3538259fba86dfc0bd9bd5680076836 not found
Apr 07 01:28:23 	at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$1(AbstractExecutionGraphHandler.java:99)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
Apr 07 01:28:23 	at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.lambda$getExecutionGraphInternal$0(DefaultExecutionGraphCache.java:109)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
Apr 07 01:28:23 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:260)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
Apr 07 01:28:23 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1275)
Apr 07 01:28:23 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)

{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47996&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=7713",dmvk,Sergey Nuyanzin,Critical,Resolved,Fixed,14/Apr/23 07:58,21/Apr/23 10:39
Bug,FLINK-31805,13532620,Cassandra Source shouldn't use IOUtils,IOUtils is not part of the public API and shouldn't be used.,chesnay,chesnay,Major,Closed,Fixed,14/Apr/23 08:11,14/Apr/23 10:28
Bug,FLINK-31808,13532639,wrong examples of how to set operator name  in documents,"[https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/overview/#name-and-description]

 
{code:java}
.setName(""filter""){code}
 should be
{code:java}
.name(""filter""){code}",huwh,huwh,Major,Closed,Fixed,14/Apr/23 09:26,15/Apr/23 05:14
Bug,FLINK-31812,13532664,SavePoint from /jars/:jarid:/run api on body is not anymore set to null if empty,"Since https://issues.apache.org/jira/browse/FLINK-29543 the 
savepointPath from the body is not anymore transform to null if empty: [https://github.com/apache/flink/pull/21012/files#diff-c6d9a43d970eb07642a87e4bf9ec6a9dc7d363b1b5b557ed76f73d8de1cc5a54R145]
 
This leads to issue running a flink job in release 1.17 with lyft operator which set savePoint in body to empty string: [https://github.com/lyft/flinkk8soperator/blob/master/pkg/controller/flinkapplication/flink_state_machine.go#L721]
 
Issue faced by the job as the savepointPath is setto empty string:
{code:java}
org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
3	at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97)
4	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
5	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
6	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
7	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1705)
8	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
9	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
10	at java.base/java.lang.Thread.run(Thread.java:829)
11Caused by: java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: empty checkpoint pointer
12	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314)
13	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:319)
14	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1702)
15	... 3 more
16Caused by: java.lang.IllegalArgumentException: empty checkpoint pointer
17	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
18	at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorageAccess.resolveCheckpointPointer(AbstractFsCheckpointStorageAccess.java:240)
19	at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorageAccess.resolveCheckpoint(AbstractFsCheckpointStorageAccess.java:136)
20	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1824)
21	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint(DefaultExecutionGraphFactory.java:223)
22	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:198)
23	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:365)
24	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:210)
25	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:136)
26	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:152)
27	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:119)
28	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:371)
29	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:348)
30	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:123)
31	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95)
32	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
33	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
34	... 3 more
35 {code}",nfraison.datadog,nfraison.datadog,Minor,Closed,Fixed,14/Apr/23 13:26,07/Jun/23 03:02
Bug,FLINK-31818,13532827,parsing error of 'security.kerberos.access.hadoopFileSystems' in flink-conf.yaml,"There is a parsing error when I gave two or more hdfs namenodes URI separated by commas as the value of key attribute 'security.kerberos.access.hadoopFileSystems'.

 

For example, I set this key attribute and value like below in flink-conf.yaml,
{code:java}
security.kerberos.access.hadoopFileSystems: hdfs://hadoop-nn1.testurl.com:8020,hdfs://hadoop-nn2.testurl.com:8020 {code}
 

then, the slash ""/"" is missing in second URI in parsed value
{code:java}
hdfs://hadoop-nn1.testurl.com:8020,hdfs:/hadoop-nn2.testurl.com:8020{code}
 

 

Received error message is here.
{code:java}
Caused by: org.apache.flink.util.FlinkRuntimeException: java.io.IOException: Incomplete HDFS URI, no host: hdfs://hadoop-nn1.testurl.com:8020,hdfs:/hadoop-nn2.testurl.com:8020
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.lambda$getFileSystemsToAccess$2(HadoopFSDelegationTokenProvider.java:168) ~[flink-dist-1.17.0.jar:1.17.0]
   at java.util.ArrayList.forEach(ArrayList.java:1259) ~[?:1.8.0_362]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.getFileSystemsToAccess(HadoopFSDelegationTokenProvider.java:157) ~[flink-dist-1.17.0.jar:1.17.0]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.lambda$obtainDelegationTokens$1(HadoopFSDelegationTokenProvider.java:113) ~[flink-dist-1.17.0.jar:1.17.0]
   at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_362]
   at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_362]
   at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1966) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.obtainDelegationTokens(HadoopFSDelegationTokenProvider.java:108) ~[flink-dist-1.17.0.jar:1.17.0]
   at org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$obtainDelegationTokensAndGetNextRenewal$1(DefaultDelegationTokenManager.java:228) ~[flink-dist-1.17.0.jar:1.17.0]
   ... 13 more
Caused by: java.io.IOException: Incomplete HDFS URI, no host: hdfs://hadoop-nn1.testurl.com:8020,hdfs:/hadoop-nn2.testurl.com:8020
   at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:156) ~[hadoop-hdfs-client-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3241) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:122) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3290) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3258) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:471) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.lambda$getFileSystemsToAccess$2(HadoopFSDelegationTokenProvider.java:163) ~[flink-dist-1.17.0.jar:1.17.0]
   at java.util.ArrayList.forEach(ArrayList.java:1259) ~[?:1.8.0_362]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.getFileSystemsToAccess(HadoopFSDelegationTokenProvider.java:157) ~[flink-dist-1.17.0.jar:1.17.0]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.lambda$obtainDelegationTokens$1(HadoopFSDelegationTokenProvider.java:113) ~[flink-dist-1.17.0.jar:1.17.0]
   at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_362]
   at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_362]
   at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1966) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.obtainDelegationTokens(HadoopFSDelegationTokenProvider.java:108) ~[flink-dist-1.17.0.jar:1.17.0]
   at org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$obtainDelegationTokensAndGetNextRenewal$1(DefaultDelegationTokenManager.java:228) ~[flink-dist-1.17.0.jar:1.17.0]
   ... 13 more {code}
 ",seung-min,seung-min,Major,Closed,Fixed,17/Apr/23 05:57,17/Apr/23 15:17
Bug,FLINK-31823,13532865,RestHandlerConfigurationTest.testWebRescaleFeatureFlagWithReactiveMode is unstable,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48177&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8509]

{noformat}
Apr 16 01:15:08 [ERROR] Failures: 
Apr 16 01:15:08 [ERROR]   RestHandlerConfigurationTest.testWebRescaleFeatureFlagWithReactiveMode:84 
Apr 16 01:15:08 expected: false
Apr 16 01:15:08  but was: true

{noformat}",dmvk,Sergey Nuyanzin,Critical,Resolved,Fixed,17/Apr/23 11:22,18/Apr/23 13:30
Bug,FLINK-31831,13532973,TaskManagerDisconnectOnShutdownITCase.testTaskManagerProcessFailure is unstable,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48212&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8399

{noformat}
Apr 18 04:17:09 [ERROR] org.apache.flink.test.recovery.TaskManagerDisconnectOnShutdownITCase.testTaskManagerProcessFailure  Time elapsed: 2.844 s  <<< FAILURE!
Apr 18 04:17:09 java.lang.AssertionError: Failed to initialize the cluster entrypoint .
Apr 18 04:17:09 	at org.junit.Assert.fail(Assert.java:89)
Apr 18 04:17:09 	at org.apache.flink.test.recovery.TaskManagerDisconnectOnShutdownITCase.testTaskManagerProcessFailure(TaskManagerDisconnectOnShutdownITCase.java:136)
Apr 18 04:17:09 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 18 04:17:09 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 18 04:17:09 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)


{noformat}",wanglijie,Sergey Nuyanzin,Critical,Closed,Fixed,18/Apr/23 05:40,26/Apr/23 06:32
Bug,FLINK-31834,13533001,Azure Warning: no space left on device,"In this CI run: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48213&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=841082b6-1a93-5908-4d37-a071f4387a5f&l=21

There was this warning:
{code}
Loaded image: confluentinc/cp-kafka:6.2.2
Loaded image: testcontainers/ryuk:0.3.3
ApplyLayer exit status 1 stdout:  stderr: write /opt/jdk-15.0.1+9/lib/modules: no space left on device
##[error]Bash exited with code '1'.
Finishing: Restore docker images
{code}",rmetzger,rmetzger,Major,Closed,Fixed,18/Apr/23 08:17,05/May/23 06:47
Bug,FLINK-31835,13533004,DataTypeHint don't support Row<i Array<int>>,"Using DataTypeHint(""Row<t ARRAY<INT>>"") in a UDF gives the following error:

 
{code:java}
Caused by: java.lang.ClassCastException: class [I cannot be cast to class [Ljava.lang.Object; ([I and [Ljava.lang.Object; are in module java.base of loader 'bootstrap')
org.apache.flink.table.data.conversion.ArrayObjectArrayConverter.toInternal(ArrayObjectArrayConverter.java:40)
org.apache.flink.table.data.conversion.DataStructureConverter.toInternalOrNull(DataStructureConverter.java:61)
org.apache.flink.table.data.conversion.RowRowConverter.toInternal(RowRowConverter.java:75)
org.apache.flink.table.data.conversion.RowRowConverter.toInternal(RowRowConverter.java:37)
org.apache.flink.table.data.conversion.DataStructureConverter.toInternalOrNull(DataStructureConverter.java:61)
StreamExecCalc$251.processElement_split9(Unknown Source)
StreamExecCalc$251.processElement(Unknown Source)
org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) {code}
 

The function is as follows:
{code:java}
@DataTypeHint(""Row<t ARRAY<INT>>"")
public Row eval() {
int[] i = new int[3];
return Row.of(i);
} {code}
 

This error is not reported when testing other simple types, so it is not an environmental problem.",aitozi,jeff-zou,Major,Closed,Fixed,18/Apr/23 09:10,12/Jun/23 09:53
Bug,FLINK-31839,13533035,Token delegation fails when both flink-s3-fs-hadoop and flink-s3-fs-presto plugins are used,"{code:java}
2023-04-07 09:18:32,814 [main] ERROR
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] -
Failed to initialize delegation token provider s3
java.lang.IllegalStateException: Delegation token provider with service
name {} has multiple implementations [s3]
at org.apache.flink.util.Preconditions.checkState(Preconditions.java:215)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$loadProviders$0(DefaultDelegationTokenManager.java:133)
~[flink-dist-1.17.0.jar:1.17.0]
at java.util.Iterator.forEachRemaining(Unknown Source) ~[?:?]
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.loadProviders(DefaultDelegationTokenManager.java:156)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.<init>(DefaultDelegationTokenManager.java:111)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManagerFactory.create(DefaultDelegationTokenManagerFactory.java:50)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:392)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:282)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:232)
~[flink-dist-1.17.0.jar:1.17.0]
at java.security.AccessController.doPrivileged(Native Method) ~[?:?]
at javax.security.auth.Subject.doAs(Unknown Source) [?:?]
at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
[hadoop-common-2.8.5.jar:?]
at
org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:229)
[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:729)
[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.container.entrypoint.StandaloneApplicationClusterEntryPoint.main(StandaloneApplicationClusterEntryPoint.java:82)
[flink-dist-1.17.0.jar:1.17.0]
2023-04-07 09:18:32,824 [main] INFO
org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Shutting
StandaloneApplicationClusterEntryPoint down with application status FAILED.
Diagnostics org.apache.flink.util.FlinkRuntimeException:
java.lang.IllegalStateException: Delegation token provider with service
name {} has multiple implementations [s3]
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$loadProviders$0(DefaultDelegationTokenManager.java:151)
at java.base/java.util.Iterator.forEachRemaining(Unknown Source)
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.loadProviders(DefaultDelegationTokenManager.java:156)
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.<init>(DefaultDelegationTokenManager.java:111)
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManagerFactory.create(DefaultDelegationTokenManagerFactory.java:50)
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:392)
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:282)
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:232)
at java.base/java.security.AccessController.doPrivileged(Native Method)
at java.base/javax.security.auth.Subject.doAs(Unknown Source)
at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
at
org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:229)
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:729)
at
org.apache.flink.container.entrypoint.StandaloneApplicationClusterEntryPoint.main(StandaloneApplicationClusterEntryPoint.java:82)
Caused by: java.lang.IllegalStateException: Delegation token provider with
service name {} has multiple implementations [s3]
at org.apache.flink.util.Preconditions.checkState(Preconditions.java:215)
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$loadProviders$0(DefaultDelegationTokenManager.java:133)
... 14 more
{code}
",gaborgsomogyi,gaborgsomogyi,Critical,Closed,Fixed,18/Apr/23 11:52,20/Apr/23 13:38
Bug,FLINK-31840,13533036,NullPointerException in operators.window.slicing.SliceAssigners$AbstractSliceAssigner.assignSliceEnd,"While running a Flink SQL Query (with a hop window), I got this error.
{code}
Caused by: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:99)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at StreamExecCalc$11.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	... 23 more
Caused by: java.lang.NullPointerException
	at org.apache.flink.table.runtime.operators.window.slicing.SliceAssigners$AbstractSliceAssigner.assignSliceEnd(SliceAssigners.java:558)
	at org.apache.flink.table.runtime.operators.aggregate.window.LocalSlicingWindowAggOperator.processElement(LocalSlicingWindowAggOperator.java:114)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	... 29 more
{code}

It was caused by a timestamp field containing NULL values.",lincoln.86xy,rmetzger,Major,Closed,Fixed,18/Apr/23 11:55,18/Apr/23 14:24
Bug,FLINK-31866,13533384,Autoscaler metric trimming reduces the number of metric observations on recovery,"The autoscaler uses a ConfigMap to store past metric observations which is used to re-initialize the autoscaler state in case of failures or upgrades.

Whenever trimming of the ConfigMap occurs, we need to make sure we also update the timestamp for the start of the metric collection, so any removed observations can be compensated with by collecting new ones. If we don't do this, the metric window will effectively shrink due to removing observations.

This can lead to triggering scaling decisions when the operator gets redeployed due to the removed items.",mxm,mxm,Major,Resolved,Fixed,20/Apr/23 11:25,27/Apr/23 15:15
Bug,FLINK-31868,13533388,Fix DefaultInputSplitAssigner javadoc for class,"Based on the discussion[1] on the mailing list {{there
is no requirement of the order of splits by Flink itself}}, we should fix the discrepancy between the code and the comment by updating the comment.

 

[[1] https://lists.apache.org/thread/74m7z2kzgpzylhrp1oq4lz37pnqjmbkh|https://lists.apache.org/thread/74m7z2kzgpzylhrp1oq4lz37pnqjmbkh]

 ",pvary,pvary,Minor,Closed,Fixed,20/Apr/23 11:40,21/Apr/23 09:31
Bug,FLINK-31869,13533409,test_multi_sessionjob.sh gets stuck very frequently,The test_multi_sessionjob.sh gets stuck almost all the time on recent builds.,gyfora,gyfora,Blocker,Closed,Fixed,20/Apr/23 14:25,21/Apr/23 12:54
Bug,FLINK-31873,13533458,Add setMaxParallelism to the DataStreamSink Class,"When turning on Flink reactive mode, it is suggested to convert all {{setParallelism}} calls to {{setMaxParallelism}} from [elastic scaling docs|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/elastic_scaling/#configuration].

With the current implementation of the {{DataStreamSink}} class, only the {{[setParallelism|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/DataStreamSink.java#L172-L181]}} function of the {{[Transformation|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/dag/Transformation.java#L248-L285]}} class is exposed - {{Transformation}} also has the {{[setMaxParallelism|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/dag/Transformation.java#L277-L285]}} function which is not exposed.

 

This means for any sink in the Flink pipeline, we cannot set a max parallelism.",eric.xiao,eric.xiao,Major,Closed,Fixed,20/Apr/23 20:24,28/Apr/23 14:35
Bug,FLINK-31878,13533513,Fix the wrong name of PauseOrResumeSplitsTask#toString in connector fetcher ,The class name PauseOrResumeSplitsTask#toString is not right. Users will be very confused when calling the toString method of the class. So we should fix it.,tanyuxin,tanyuxin,Minor,Closed,Fixed,21/Apr/23 07:51,14/Jun/23 11:23
Bug,FLINK-31882,13533666,SqlGateway will throw exception when executing DeleteFromFilterOperation,"Reproduce step:

Our sink implements `SupportsDeletePushDown`, so when we test a DELETE statement, the `TableEnvironmentImpl` will call the 
`TableResultInternal executeInternal(DeleteFromFilterOperation deleteFromFilterOperation)` at line 895. This method won't return the JobClient, but the SqlGateway requires one, thus a exception occurs.
Stack:
Caused by: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Can't get job client for the operation d4ba1029-664c-44c0-922b-021eb9e1c527.
at org.apache.flink.table.gateway.service.operation.OperationExecutor.lambda$callModifyOperations$6(OperationExecutor.java:521)
at java.base/java.util.Optional.orElseThrow(Optional.java:408)
at org.apache.flink.table.gateway.service.operation.OperationExecutor.callModifyOperations(OperationExecutor.java:518)
at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:431)
at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:200)
at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212)
at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119)
at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258)",luoyuxia,yzl,Major,Resolved,Fixed,23/Apr/23 02:30,17/May/23 01:42
Bug,FLINK-31884,13533673,Upgrade ExecNode to new version causes the old serialized plan failed to pass Json SerDe round trip,"h4. How to Reproduce

Firstly, add a test to dump the compiled plan JSON.
{code:java}
@Test
public void debug() {
    tableEnv.executeSql(""create table foo (f0 int, f1 string) with ('connector' = 'datagen')"");
    tableEnv.executeSql(""create table bar (f0 int, f1 string) with ('connector' = 'print')"");
    tableEnv.compilePlanSql(""insert into bar select * from foo"")
            .writeToFile(new File(""/path/to/debug.json""));
}
{code}
The JSON context is as follows
{code:json}
{
  ""flinkVersion"" : ""1.18"",
  ""nodes"" : [ {
    ""id"" : 1,
    ""type"" : ""stream-exec-table-source-scan_1"",
    ""scanTableSource"" : {
      ""table"" : {
        ""identifier"" : ""`default_catalog`.`default_database`.`foo`"",
        ""resolvedTable"" : {
          ""schema"" : {
            ""columns"" : [ {
              ""name"" : ""f0"",
              ""dataType"" : ""INT""
            }, {
              ""name"" : ""f1"",
              ""dataType"" : ""VARCHAR(2147483647)""
            } ],
            ""watermarkSpecs"" : [ ]
          },
          ""partitionKeys"" : [ ],
          ""options"" : {
            ""connector"" : ""datagen""
          }
        }
      }
    },
    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
    ""description"" : ""TableSourceScan(table=[[default_catalog, default_database, foo]], fields=[f0, f1])"",
    ""inputProperties"" : [ ]
  }, {
    ""id"" : 2,
    ""type"" : ""stream-exec-sink_1"",
    ""configuration"" : {
      ""table.exec.sink.keyed-shuffle"" : ""AUTO"",
      ""table.exec.sink.not-null-enforcer"" : ""ERROR"",
      ""table.exec.sink.type-length-enforcer"" : ""IGNORE"",
      ""table.exec.sink.upsert-materialize"" : ""AUTO""
    },
    ""dynamicTableSink"" : {
      ""table"" : {
        ""identifier"" : ""`default_catalog`.`default_database`.`bar`"",
        ""resolvedTable"" : {
          ""schema"" : {
            ""columns"" : [ {
              ""name"" : ""f0"",
              ""dataType"" : ""INT""
            }, {
              ""name"" : ""f1"",
              ""dataType"" : ""VARCHAR(2147483647)""
            } ],
            ""watermarkSpecs"" : [ ]
          },
          ""partitionKeys"" : [ ],
          ""options"" : {
            ""connector"" : ""print""
          }
        }
      }
    },
    ""inputChangelogMode"" : [ ""INSERT"" ],
    ""inputProperties"" : [ {
      ""requiredDistribution"" : {
        ""type"" : ""UNKNOWN""
      },
      ""damBehavior"" : ""PIPELINED"",
      ""priority"" : 0
    } ],
    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
    ""description"" : ""Sink(table=[default_catalog.default_database.bar], fields=[f0, f1])""
  } ],
  ""edges"" : [ {
    ""source"" : 1,
    ""target"" : 2,
    ""shuffle"" : {
      ""type"" : ""FORWARD""
    },
    ""shuffleMode"" : ""PIPELINED""
  } ]
}
{code}
Then upgrade the StreamExecSink to a new version
{code:java}
@ExecNodeMetadata(
        name = ""stream-exec-sink"",
        version = 1,
        consumedOptions = {
            ""table.exec.sink.not-null-enforcer"",
            ""table.exec.sink.type-length-enforcer"",
            ""table.exec.sink.upsert-materialize"",
            ""table.exec.sink.keyed-shuffle""
        },
        producedTransformations = {
            CommonExecSink.CONSTRAINT_VALIDATOR_TRANSFORMATION,
            CommonExecSink.PARTITIONER_TRANSFORMATION,
            CommonExecSink.UPSERT_MATERIALIZE_TRANSFORMATION,
            CommonExecSink.TIMESTAMP_INSERTER_TRANSFORMATION,
            CommonExecSink.SINK_TRANSFORMATION
        },
        minPlanVersion = FlinkVersion.v1_15,
        minStateVersion = FlinkVersion.v1_15)
@ExecNodeMetadata(
        name = ""stream-exec-sink"",
        version = 2,
        consumedOptions = {
            ""table.exec.sink.not-null-enforcer"",
            ""table.exec.sink.type-length-enforcer"",
            ""table.exec.sink.upsert-materialize"",
            ""table.exec.sink.keyed-shuffle""
        },
        producedTransformations = {
            CommonExecSink.CONSTRAINT_VALIDATOR_TRANSFORMATION,
            CommonExecSink.PARTITIONER_TRANSFORMATION,
            CommonExecSink.UPSERT_MATERIALIZE_TRANSFORMATION,
            CommonExecSink.TIMESTAMP_INSERTER_TRANSFORMATION,
            CommonExecSink.SINK_TRANSFORMATION
        },
        minPlanVersion = FlinkVersion.v1_18,
        minStateVersion = FlinkVersion.v1_15)
public class StreamExecSink extends CommonExecSink implements StreamExecNode<Object> {
}
{code}
And then load the previous plan and print it as JSON text
{code:java}
tableEnv.loadPlan(PlanReference.fromFile(""/path/to/debug.json"")).printJsonString();
{code}
The SerDe lost idempotence since the version for StreamExecSink became version 2.
{code:json}
{
  ""flinkVersion"" : ""1.18"",
  ""nodes"" : [ {
    ""id"" : 1,
    ""type"" : ""stream-exec-table-source-scan_1"",
    ""scanTableSource"" : {
      ""table"" : {
        ""identifier"" : ""`default_catalog`.`default_database`.`foo`""
      }
    },
    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
    ""description"" : ""TableSourceScan(table=[[default_catalog, default_database, foo]], fields=[f0, f1])"",
    ""inputProperties"" : [ ]
  }, {
    ""id"" : 2,
    ""type"" : ""stream-exec-sink_2"",
    ""configuration"" : {
      ""table.exec.sink.keyed-shuffle"" : ""AUTO"",
      ""table.exec.sink.not-null-enforcer"" : ""ERROR"",
      ""table.exec.sink.type-length-enforcer"" : ""IGNORE"",
      ""table.exec.sink.upsert-materialize"" : ""AUTO""
    },
    ""dynamicTableSink"" : {
      ""table"" : {
        ""identifier"" : ""`default_catalog`.`default_database`.`bar`""
      }
    },
    ""inputChangelogMode"" : [ ""INSERT"" ],
    ""inputProperties"" : [ {
      ""requiredDistribution"" : {
        ""type"" : ""UNKNOWN""
      },
      ""damBehavior"" : ""PIPELINED"",
      ""priority"" : 0
    } ],
    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
    ""description"" : ""Sink(table=[default_catalog.default_database.bar], fields=[f0, f1])""
  } ],
  ""edges"" : [ {
    ""source"" : 1,
    ""target"" : 2,
    ""shuffle"" : {
      ""type"" : ""FORWARD""
    },
    ""shuffleMode"" : ""PIPELINED""
  } ]
}
{code}
h4. Root Cause

ExecNodeBase#getContextFromAnnotation always uses the newest ExecNode version for SerDe. As a result, although the deserialized CompilePlan object is correct, #printAsJson will create a new context with the newest version.
 
h4. Suggested Fix

If the member variable `isCompiled` is true, then #getContextFromAnnotation should return the context which reads from the JSON plan instead of instantiating a new one.
 ",qingyue,qingyue,Major,Closed,Fixed,23/Apr/23 06:39,16/May/23 16:29
Bug,FLINK-31900,13533744,"Fix some typo in java doc, comments and assertion message",As the title.,Feifan Wang,Feifan Wang,Minor,Closed,Fixed,24/Apr/23 03:11,28/Apr/23 11:49
Bug,FLINK-31914,13533791,Failing to close FlinkKafkaInternalProducer created in KafkaWriter with exactly-once semantic results in memory leak,"Hi [~arvid] , If Exactly-Once writing is enabled, Kafka's transactional writing will be used. KafkaWriter will create FlinkKafkaInternalProducer in the initialization and snapshotState methods, but there is no place to close it. As Checkpoints increase, Producers will continue to accumulate. Each Producer maintains a Buffer, which will cause memory leaks and Job OOM.
By dumping an in-memory instance of Task Manager, you can see that there are a lot of Producers:

!image-2023-04-25-13-47-25-703.png!",,datariver,Major,Closed,Fixed,24/Apr/23 08:24,25/Apr/23 06:19
Bug,FLINK-31917,13533800,Loss of Idempotence in JsonSerDe Round Trip for AggregateCall and RexNode,"JsonSerDeTestUtil#testJsonRoundTrip only checks the equality between spec and deserialized object. Some corner cases are detected when serializing the deserialized object again.
{code:java}
static <T> T testJsonRoundTrip(SerdeContext serdeContext, T spec, Class<T> clazz)
            throws IOException {
        String actualJson = toJson(serdeContext, spec);
        T actual = toObject(serdeContext, actualJson, clazz);

        assertThat(actual).isEqualTo(spec);
        assertThat(actualJson).isEqualTo(toJson(serdeContext, actual)); // this will eval some corner cases
        return actual;
    }
{code}
The discovered corner cases are listed as follows.
h5. 1. SerDe for AggregateCall

When deserializing the aggregate call, we should check the JsonNodeType to avoid converting null to ""null"" string.
[https://github.com/apache/flink/blob/f9b3e0b7bc0432001b4a197539a0712b16e0b33b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/serde/AggregateCallJsonDeserializer.java#L64]
h5. Suggested Fix
{code:java}
JsonNode nameNode = jsonNode.required(FIELD_NAME_NAME);
final String name = JsonNodeType.NULL ? null : nameNode.asText();
{code}
h5. 2. SerDe for RexNode

RexNodeJsonSerdeTest#testSystemFunction should create the temporary system function instead of the temporary catalog function.
[https://github.com/apache/flink/blob/f9b3e0b7bc0432001b4a197539a0712b16e0b33b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/serde/RexNodeJsonSerdeTest.java#L209]
h5. Suggested Fix

Use functionCatalog#registerTemporarySystemFunction to test.
h5. 3. About RexLiteral type

RexNodeJsonSerdeTest#testRexNodeSerde has a test spec as follows
{code:java}
//This will create the literal with DOUBLE as the literal type, and DECIMAL as the broad type of this literal. You can refer to Calcite for more details
rexBuilder.makeExactLiteral(BigDecimal.valueOf(Double.MAX_VALUE), FACTORY.createSqlType(SqlTypeName.DOUBLE))
{code}
The RexNodeJsonSerializer uses `typeName`(which is DECIMAL) as the literal's type, as a result, the rel data type is serialized as double, but the value is serialized as a string (in case lost the precision)
[https://github.com/apache/flink/blob/f9b3e0b7bc0432001b4a197539a0712b16e0b33b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/serde/RexNodeJsonSerializer.java#L197]

And then, during the deserialization, according to the JSON, the deserialized literal will assign DOUBLE as the literal type and the broad type of the literal.
This will cause the comparison failure
{code:java}
expected: {""kind"": ""LITERAL"", ""value"": ""1.7976931348623157E+308""}
actual: {""kind"": ""LITERAL"", ""value"": 1.7976931348623157E+308}
{code}
h5. Suggested Fix

SARG is a special case and can be coped first, and for the rest type, we can use literal.getType().getSqlTypeName() instead of literal.getTypeName().
{code:java}
// first cope with SARG type
if (literal.getTypeName() == SARG) {
    serializeSargValue(
        (Sarg<?>) value, literal.getType().getSqlTypeName(), gen, serializerProvider);
} else {
    serializeLiteralValue(
        value,
        literal.getType().getSqlTypeName(),
        gen,
        serializerProvider);
}
{code}",qingyue,qingyue,Major,Closed,Fixed,24/Apr/23 08:37,16/May/23 16:31
Bug,FLINK-31923,13533836,Connector weekly runs are only testing main branches instead of all supported branches,"We have a weekly scheduled build for connectors. That's only triggered for the {{main}} branches, because that's how the Github Actions {{schedule}} works, per https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#schedule

We can resolve that by having the Github Action flow checkout multiple branches as a matrix to run these weekly tests.",martijnvisser,martijnvisser,Major,Resolved,Fixed,24/Apr/23 11:47,27/Jun/23 18:06
Bug,FLINK-31927,13533886,Cassandra source raises an exception on Flink 1.16.0 if the user enables the metrics in the cassandra driver,CassandraSplitEnumerator#prepareSplits() raises  java.lang.NoClassDefFoundError: com/codahale/metrics/Gauge when calling cluster.getMetadata() leading to NPE in CassandraSplitEnumerator#start() async callback. ,echauchot,echauchot,Blocker,Resolved,Fixed,24/Apr/23 18:26,05/May/23 09:23
Bug,FLINK-31935,13533975,The new resource requirements REST API is only available for session clusters,"We need to register both `JobResourceRequirementsHandler` and `
JobResourceRequirementsUpdateHandler` for application / per-job clusters as well.
 
These handlers have been introduced as part of FLINK-31316.",dmvk,dmvk,Major,Closed,Fixed,25/Apr/23 09:45,26/Apr/23 07:43
Bug,FLINK-31959,13534303,Correct the unaligned checkpoint type at checkpoint level,"FLINK-18851 added the checkpoint type in web UI to distinguish aligned checkpoint, unaligned checkpoint, savepoint and savepoint on cancel in {*}Flink 1.12{*}.

It distinguishes between UC and AC based on whether UC is enabled or disabled.[1]

However, FLINK-19680, FLINK-19681 and FLINK-19682 introduced the alignment-timeout in {*}Flink 1.13{*}, and it has been changed to {{aligned-checkpoint-timeout.}}
{code:java}
When activated, each checkpoint will still begin as an aligned checkpoint, but when the global checkpoint duration exceeds the aligned-checkpoint-timeout, if the aligned checkpoint has not completed, then the checkpoint will proceed as an unaligned checkpoint.{code}
If UC and AC-timeout is enabled and the checkpoint is completed as aligned checkpoint. It should show the aligned checkpoint instead of unaligned checkpoint.

 

[1] [https://github.com/apache/flink/blob/a3368635e3d06f764d144f8c8e2e06e499e79665/flink-runtime-web/web-dashboard/src/app/pages/job/checkpoints/detail/job-checkpoints-detail.component.ts#L118]

 ",fanrui,fanrui,Major,Closed,Fixed,27/Apr/23 10:43,10/May/23 08:42
Bug,FLINK-31962,13534357,libssl not found when running CI,"{code:java}
Installed Maven 3.2.5 to /home/vsts/maven_cache/apache-maven-3.2.5
Installing required software
Reading package lists...
Building dependency tree...
Reading state information...
bc is already the newest version (1.07.1-2build1).
bc set to manually installed.
libapr1 is already the newest version (1.6.5-1ubuntu1).
libapr1 set to manually installed.
0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.
--2023-04-27 11:42:53--  http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.11_amd64.deb
Resolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...
Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2023-04-27 11:42:53 ERROR 404: Not Found.
{code}

",martijnvisser,martijnvisser,Blocker,Closed,Fixed,27/Apr/23 19:57,04/May/23 11:44
Bug,FLINK-31963,13534378,java.lang.ArrayIndexOutOfBoundsException when scaling down with unaligned checkpoints,"I'm testing Autoscaler through Kubernetes Operator and I'm facing the following issue.

As you know, when a job is scaled down through the autoscaler, the job manager and task manager go down and then back up again.

When this happens, an index out of bounds exception is thrown and the state is not restored from a checkpoint.

[~gyfora] told me via the Flink Slack troubleshooting channel that this is likely an issue with Unaligned Checkpoint and not an issue with the autoscaler, but I'm opening a ticket with Gyula for more clarification.

Please see the attached JM and TM error logs.
Thank you.",srichter,tanee.kim,Critical,Resolved,Fixed,28/Apr/23 02:01,19/May/23 04:55
Bug,FLINK-31965,13534398,Fix ClassNotFoundException in benchmarks,"The benchmarks rely on the test jar of `flink-streaming-java`. However, the jar is set to test scope, thus not included in the packaged jar. Therefore ClassNotFoundException occurs while running the benchmarks with `java --jar xxx` command.

```
java.lang.NoClassDefFoundError: org/apache/flink/streaming/util/KeyedOneInputStreamOperatorTestHarness
    at org.apache.flink.contrib.streaming.state.benchmark.RescalingBenchmark.prepareState(RescalingBenchmark.java:111)
    at org.apache.flink.contrib.streaming.state.benchmark.RescalingBenchmark.setUp(RescalingBenchmark.java:78)
    at org.apache.flink.state.benchmark.RocksdbStateBackendRescalingBenchmarkExecutor.setUp(RocksdbStateBackendRescalingBenchmarkExecutor.java:66)
    at org.apache.flink.state.benchmark.generated.RocksdbStateBackendRescalingBenchmarkExecutor_rescaleRocksDB_jmhTest._jmh_tryInit_f_rocksdbstatebackendrescalingbenchmarkexecutor0_0(RocksdbStateBackendRescalingBenchmarkExecutor_rescaleRocksDB_jmhTest.java:370)
    at org.apache.flink.state.benchmark.generated.RocksdbStateBackendRescalingBenchmarkExecutor_rescaleRocksDB_jmhTest.rescaleRocksDB_AverageTime(RocksdbStateBackendRescalingBenchmarkExecutor_rescaleRocksDB_jmhTest.java:147)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:453)
    at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:437)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 17 more
```",Paul Lin,Paul Lin,Minor,Closed,Fixed,28/Apr/23 06:29,09/May/23 06:52
Bug,FLINK-31967,13534418,SQL with LAG function NullPointerException,"I want to make a query with the LAG function. And got Job Exception without any explanations.

 

*Code:*
{code:java}
private static void t1_LeadLag(DataStream<UserModel> ds, StreamExecutionEnvironment env) {
    StreamTableEnvironment te = StreamTableEnvironment.create(env);
    Table t = te.fromDataStream(ds, Schema.newBuilder().columnByExpression(""proctime"", ""proctime()"").build());

    te.createTemporaryView(""users"", t);

    Table res = te.sqlQuery(""SELECT userId, `count`,\n"" +
            "" LAG(`count`) OVER (PARTITION BY userId ORDER BY proctime) AS prev_quantity\n"" +
            "" FROM users"");

    te.toChangelogStream(res).print();
}{code}
 

*Input:*

{""userId"":3,""count"":0,""dt"":""2023-04-28T07:44:21.551Z""}

 

*Exception:* I remove part about basic JobExecutionException and kept the important(i think)
{code:java}
Caused by: java.lang.NullPointerException
    at org.apache.flink.table.data.GenericRowData.getInt(GenericRowData.java:149)
    at org.apache.flink.table.data.RowData.lambda$createFieldGetter$245ca7d1$6(RowData.java:245)
    at org$apache$flink$table$runtime$functions$aggregate$LagAggFunction$LagAcc$2$Converter.toExternal(Unknown Source)
    at org.apache.flink.table.data.conversion.StructuredObjectConverter.toExternal(StructuredObjectConverter.java:101)
    at UnboundedOverAggregateHelper$15.setAccumulators(Unknown Source)
    at org.apache.flink.table.runtime.operators.over.ProcTimeUnboundedPrecedingFunction.processElement(ProcTimeUnboundedPrecedingFunction.java:92)
    at org.apache.flink.table.runtime.operators.over.ProcTimeUnboundedPrecedingFunction.processElement(ProcTimeUnboundedPrecedingFunction.java:42)
    at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83)
    at org.apache.flink.streaming.runtime.io.RecordProcessorUtils.lambda$getRecordProcessor$0(RecordProcessorUtils.java:60)
    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:237)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:146)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.base/java.lang.Thread.run(Thread.java:829){code}",pyro,padavan,Major,Closed,Fixed,28/Apr/23 08:06,31/May/23 03:07
Bug,FLINK-31974,13534509,JobManager crashes after KubernetesClientException exception with FatalExitExceptionHandler,"When resource quota limit is reached JobManager will throw

 

org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-2"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.

 

In {*}1.16.1 , this is handled gracefully{*}:
{code}
2023-04-28 22:07:24,631 WARN  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Failed requesting worker with resource spec WorkerResourceSpec \{cpuCores=1.0, taskHeapSize=25.600mb (26843542 bytes), taskOffHeapSize=0 bytes, networkMemSize=64.000mb (67108864 bytes), managedMemSize=230.400mb (241591914 bytes), numSlots=4}, current pending count: 0
java.util.concurrent.CompletionException: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-138"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.
        at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]
        at java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source) ~[?:?]
        at java.util.concurrent.CompletableFuture$AsyncRun.run(Unknown Source) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
        at java.lang.Thread.run(Unknown Source) ~[?:?]
Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-138"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:684) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:664) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:613) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:558) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:308) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:644) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:83) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:61) ~[flink-dist-1.16.1.jar:1.16.1]
        at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$createTaskManagerPod$1(Fabric8FlinkKubeClient.java:163) ~[flink-dist-1.16.1.jar:1.16.1]
        ... 4 more
{code}

But , {*}in Flink 1.17.0 , Job Manager crashes{*}:
{code}
2023-04-28 20:50:50,534 ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-15' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-2"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.
        at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]
        at java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source) ~[?:?]
        at java.util.concurrent.CompletableFuture$AsyncRun.run(Unknown Source) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
        at java.lang.Thread.run(Unknown Source) ~[?:?]
Caused by: org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-2"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:684) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:664) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:613) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:558) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:308) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:644) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:83) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:61) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$createTaskManagerPod$1(Fabric8FlinkKubeClient.java:163) ~[flink-dist-1.17.0.jar:1.17.0]
        ... 4 more
{code}",gyfora,sergiosp,Major,Closed,Fixed,28/Apr/23 22:18,01/Jun/23 18:12
Bug,FLINK-31994,13534906,Update copyright in NOTICE files of flink-shaded ,"The copyright of all flink-shaded dependency NOTICE files are still 2021, we can use tools/update_notice_year.sh to update it.",martijnvisser,leonard,Major,Closed,Fixed,04/May/23 04:34,04/May/23 06:45
Bug,FLINK-31995,13534922,DirectExecutorService doesn't follow the ExecutorService contract throwing a RejectedExecutionException in case it's already shut down,"We experienced an issue where we tested behavior using the {{DirectExecutorService}} with the {{ExecutorService}} being shutdown already. The tests succeeded. The production code failed, though, because we used a singleThreadExecutor for which the task execution failed after the executor was stopped. We should add this behavior to the {{DirectExecutorService}} as well to make the unit tests be closer to the production code.",mapohl,mapohl,Major,Resolved,Fixed,04/May/23 07:45,30/May/23 13:31
Bug,FLINK-31996,13534928,Chaining operators with different max parallelism prevents rescaling,"We might chain operators with different max parallelism together if they are set to have the same parallelism initially.

When we decide to rescale the JobGraph vertices (using AdaptiveScheduler), we're gapped by the lowest maxParallelism of the operator chain. This is especially visible with things like CollectSink, TwoPhaseCommitSink, CDC, and a GlobalCommiter with maxParallelism set to 1.

 

An obvious solution would be to prevent the chaining of operators with different maxParallelism, but we need to double-check this doesn't introduce a breaking change.",dmvk,dmvk,Major,Closed,Fixed,04/May/23 08:22,06/Jul/23 13:43
Bug,FLINK-32003,13535025,Release 3.0.0-1.16 and 1.16.1 doesn't work with OAuth2,"The release for 3.0.0-1.16 and 1.16.1 depends on Pulsar client version 2.10.1.

There is an issue using OAuth2 with this client version which results in the following error.


{code:java}
Exception in thread ""main"" java.lang.RuntimeException: org.apache.pulsar.client.admin.PulsarAdminException: java.util.concurrent.CompletionException: org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector$RetryException: Could not complete the operation. Number of retries has been exhausted. Failed reason: https://func-test-31a67160-533f-4a5f-81a8-30b6221f34a9.gcp-shared-gcp-usce1-martin.streamnative.g.snio.cloud:443
	at me.nlu.pulsar.PulsarAdminTester.main(PulsarAdminTester.java:56)
Caused by: org.apache.pulsar.client.admin.PulsarAdminException: java.util.concurrent.CompletionException: org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector$RetryException: Could not complete the operation. Number of retries has been exhausted. Failed reason: https://func-test-31a67160-533f-4a5f-81a8-30b6221f34a9.gcp-shared-gcp-usce1-martin.streamnative.g.snio.cloud:443
	at org.apache.pulsar.client.admin.internal.BaseResource.getApiException(BaseResource.java:251)
	at org.apache.pulsar.client.admin.internal.TopicsImpl$1.failed(TopicsImpl.java:187)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.JerseyInvocation$1.failed(JerseyInvocation.java:882)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime.processFailure(ClientRuntime.java:247)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime.processFailure(ClientRuntime.java:242)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime.access$100(ClientRuntime.java:62)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime$2.lambda$failure$1(ClientRuntime.java:178)
	at org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.apache.pulsar.shade.org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:288)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime$2.failure(ClientRuntime.java:178)
	at org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector.lambda$apply$1(AsyncHttpConnector.java:218)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector.lambda$retryOperation$4(AsyncHttpConnector.java:277)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.pulsar.shade.org.asynchttpclient.netty.NettyResponseFuture.abort(NettyResponseFuture.java:273)
	at org.apache.pulsar.shade.org.asynchttpclient.netty.channel.NettyConnectListener.onFailure(NettyConnectListener.java:181)
	at org.apache.pulsar.shade.org.asynchttpclient.netty.channel.NettyConnectListener$1.onFailure(NettyConnectListener.java:151)
	at org.apache.pulsar.shade.org.asynchttpclient.netty.SimpleFutureListener.operationComplete(SimpleFutureListener.java:26)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:571)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:550)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
	at org.apache.pulsar.shade.io.netty.handler.ssl.SslHandler.setHandshakeFailure(SslHandler.java:1882)
	at org.apache.pulsar.shade.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1067)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
	at org.apache.pulsar.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
	at org.apache.pulsar.shade.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:813)
	at org.apache.pulsar.shade.io.netty.util.concurrent.AbstractEventExecutor.runTask$$$capture(AbstractEventExecutor.java:174)
	at org.apache.pulsar.shade.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java)
	at org.apache.pulsar.shade.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)
	at org.apache.pulsar.shade.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at org.apache.pulsar.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
	at org.apache.pulsar.shade.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
	at org.apache.pulsar.shade.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.apache.pulsar.shade.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.util.concurrent.CompletionException: org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector$RetryException: Could not complete the operation. Number of retries has been exhausted. Failed reason: https://func-test-31a67160-533f-4a5f-81a8-30b6221f34a9.gcp-shared-gcp-usce1-martin.streamnative.g.snio.cloud:443
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$OrApply.tryFire(CompletableFuture.java:1503)
	... 37 more
Caused by: org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector$RetryException: Could not complete the operation. Number of retries has been exhausted. Failed reason: https://func-test-31a67160-533f-4a5f-81a8-30b6221f34a9.gcp-shared-gcp-usce1-martin.streamnative.g.snio.cloud:443
	at org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector.lambda$retryOperation$4(AsyncHttpConnector.java:279)
	... 34 more
Caused by: java.net.ConnectException: https://func-test-31a67160-533f-4a5f-81a8-30b6221f34a9.gcp-shared-gcp-usce1-martin.streamnative.g.snio.cloud:443
	at org.apache.pulsar.shade.org.asynchttpclient.netty.channel.NettyConnectListener.onFailure(NettyConnectListener.java:179)
	... 28 more
Caused by: java.nio.channels.ClosedChannelException
	at org.apache.pulsar.shade.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1064)
	... 17 more{code}

We need to upgrade the pulsar-client-all version to at least 2.10.2 (for which I have verified the client auth issue is fixed) and publish new releases for 1.16 and 3.0.0-1.16.",tison,nlu90,Major,Resolved,Fixed,04/May/23 16:25,24/May/23 08:06
Bug,FLINK-32008,13535096,Protobuf format cannot work with FileSystem Connector,"The protobuf format throws exception when working with Map data type. I uploaded a example project to reproduce the problem.

 
{code:java}
Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:261)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:131)
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:417)
    at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:165)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:114)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
Caused by: java.io.IOException: Failed to deserialize PB object.
    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.deserialize(PbRowDataDeserializationSchema.java:75)
    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.deserialize(PbRowDataDeserializationSchema.java:42)
    at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82)
    at org.apache.flink.connector.file.table.DeserializationSchemaAdapter$LineBytesInputFormat.readRecord(DeserializationSchemaAdapter.java:197)
    at org.apache.flink.connector.file.table.DeserializationSchemaAdapter$LineBytesInputFormat.nextRecord(DeserializationSchemaAdapter.java:210)
    at org.apache.flink.connector.file.table.DeserializationSchemaAdapter$Reader.readBatch(DeserializationSchemaAdapter.java:124)
    at org.apache.flink.connector.file.src.util.RecordMapperWrapperRecordIterator$1.readBatch(RecordMapperWrapperRecordIterator.java:82)
    at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:67)
    at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:162)
    ... 6 more
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.formats.protobuf.deserialize.ProtoToRowConverter.convertProtoBinaryToRow(ProtoToRowConverter.java:129)
    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.deserialize(PbRowDataDeserializationSchema.java:70)
    ... 15 more
Caused by: com.google.protobuf.InvalidProtocolBufferException: While parsing a protocol message, the input ended unexpectedly in the middle of a field.  This could mean either that the input has been truncated or that an embedded message misreported its own length.
    at com.google.protobuf.InvalidProtocolBufferException.truncatedMessage(InvalidProtocolBufferException.java:115)
    at com.google.protobuf.CodedInputStream$ArrayDecoder.pushLimit(CodedInputStream.java:1196)
    at com.google.protobuf.CodedInputStream$ArrayDecoder.readMessage(CodedInputStream.java:887)
    at com.example.proto.MapMessage.<init>(MapMessage.java:64)
    at com.example.proto.MapMessage.<init>(MapMessage.java:9)
    at com.example.proto.MapMessage$1.parsePartialFrom(MapMessage.java:756)
    at com.example.proto.MapMessage$1.parsePartialFrom(MapMessage.java:750)
    at com.google.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:158)
    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:191)
    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:203)
    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:208)
    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:48)
    at com.example.proto.MapMessage.parseFrom(MapMessage.java:320)
    ... 21 more {code}",rskraba,xuannan,Major,Closed,Fixed,05/May/23 06:22,17/Jun/23 14:55
Bug,FLINK-32010,13535133,KubernetesLeaderRetrievalDriver always waits for lease update to resolve leadership,"The k8s-based leader retrieval is based on ConfigMap watching. The config map lifecycle (from the consumer point of view) is handled as a series of events with the following types:
 * ADDED -> the first time the consumer has seen the CM
 * UPDATED -> any further changes to the CM
 * DELETED -> ... you get the idea

The implementation assumes that ElectionDriver (the one that creates the CM) and ElectionRetriver are started simultaneously and therefore ignore the ADDED events because the CM is always created as empty and is updated with the leadership information later on.

This assumption is incorrect in the following cases (I might be missing some, but that's not important, the goal is to illustrate the problem):
 * TM joining the cluster later when the leaders are established to discover RM / JM
 * RM tries to discover JM when 
MultipleComponentLeaderElectionDriver is used

This, for example, leads to higher job submission latencies that could be unnecessarily held back for up to the lease retry period [1].

[1] Configured by _high-availability.kubernetes.leader-election.retry-period_",dmvk,dmvk,Major,Resolved,Fixed,05/May/23 09:50,06/May/23 06:20
Bug,FLINK-32012,13535161,Operator failed to rollback due to missing HA metadata,"The operator has well detected that the job was failing and initiate the rollback but this rollback has failed due to `Rollback is not possible due to missing HA metadata`

We are relying on saevpoint upgrade mode and zookeeper HA.

The operator is performing a set of action to also delete this HA data in savepoint upgrade mode:
 * [flink-kubernetes-operator/AbstractFlinkService.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java#L346] : Suspend job with savepoint and deleteClusterDeployment

 * [flink-kubernetes-operator/StandaloneFlinkService.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/StandaloneFlinkService.java#L158] : Remove JM + TM deployment and delete HA data

 * [flink-kubernetes-operator/AbstractFlinkService.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java#L1008] : Wait cluster shutdown and delete zookeeper HA data

 * [flink-kubernetes-operator/FlinkUtils.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkUtils.java#L155] : Remove all child znode

Then when running rollback the operator is looking for HA data even if we rely on sevepoint upgrade mode:
 * [flink-kubernetes-operator/AbstractFlinkResourceReconciler.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java#L164] Perform reconcile of rollback if it should rollback

 * [flink-kubernetes-operator/AbstractFlinkResourceReconciler.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java#L387] Rollback failed as HA data is not available

 * [flink-kubernetes-operator/FlinkUtils.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkUtils.java#L220] Check if some child znodes are available

For both step the pattern looks to be the same for kubernetes HA so it doesn't looks to be linked to a bug with zookeeper.

 

From https://issues.apache.org/jira/browse/FLINK-30305 it looks to be expected that the HA data has been deleted (as it is also performed by flink when relying on savepoint upgrade mode).

Still the use case seems to differ from https://issues.apache.org/jira/browse/FLINK-30305 as the operator is aware of the failure and treat a specific rollback event.

So I'm wondering why we enforce such a check when performing rollback if we rely on savepoint upgrade mode. Would it be fine to not rely on the HA data and rollback from the last savepoint (the one we used in the deployment step)?",,nfraison.datadog,Major,Closed,Fixed,05/May/23 12:07,27/Jun/23 15:39
Bug,FLINK-32018,13535235,Many builds of benchmark have been interrupted since 20230428,"Since 2023.04.28, flink-statebackend-benchmarks-java8, flink-statebackend-benchmarks-java11, flink-master-benchmarks-java8 and  flink-master-benchmarks-java11 all failed.

 

[http://codespeed.dak8s.net:8080/job/flink-statebackend-benchmarks-java8/]

[http://codespeed.dak8s.net:8080/job/flink-statebackend-benchmarks-java11/]

[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-java8/] 

[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-java11/] ",Yanfei Lei,Yanfei Lei,Critical,Resolved,Fixed,06/May/23 02:13,16/May/23 08:40
Bug,FLINK-32023,13535251,execution.buffer-timeout cannot be set to -1 ms,"The desc for execution.buffer-timeout is as following:
{code:java}
public static final ConfigOption<Duration> BUFFER_TIMEOUT =
        ConfigOptions.key(""execution.buffer-timeout"")
                .durationType()
                .defaultValue(Duration.ofMillis(100))
                .withDescription(
                        Description.builder()
                                .text(
                                        ""The maximum time frequency (milliseconds) for the flushing of the output buffers. By default ""
                                                + ""the output buffers flush frequently to provide low latency and to aid smooth developer ""
                                                + ""experience. Setting the parameter can result in three logical modes:"")
                                .list(
                                        text(
                                                ""A positive value triggers flushing periodically by that interval""),
                                        text(
                                                FLUSH_AFTER_EVERY_RECORD
                                                        + "" triggers flushing after every record thus minimizing latency""),
                                        text(
                                                DISABLED_NETWORK_BUFFER_TIMEOUT
                                                        + "" ms triggers flushing only when the output buffer is full thus maximizing ""
                                                        + ""throughput""))
                                .build()); {code}
When we set execution.buffer-timeout to -1 ms, the following error is reported:
{code:java}
Caused by: java.lang.IllegalArgumentException: Could not parse value '-1 ms' for key 'execution.buffer-timeout'.
    at org.apache.flink.configuration.Configuration.getOptional(Configuration.java:856)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.configure(StreamExecutionEnvironment.java:822)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.<init>(StreamExecutionEnvironment.java:224)
    at org.apache.flink.streaming.api.environment.StreamContextEnvironment.<init>(StreamContextEnvironment.java:51)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createStreamExecutionEnvironment(StreamExecutionEnvironment.java:1996)
    at java.util.Optional.orElseGet(Optional.java:267)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getExecutionEnvironment(StreamExecutionEnvironment.java:1986)
    at com.kuaishou.flink.examples.api.WordCount.main(WordCount.java:27)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:327)
    ... 11 more
Caused by: java.lang.NumberFormatException: text does not start with a number
    at org.apache.flink.util.TimeUtils.parseDuration(TimeUtils.java:78)
    at org.apache.flink.configuration.Configuration.convertToDuration(Configuration.java:1058)
    at org.apache.flink.configuration.Configuration.convertValue(Configuration.java:996)
    at org.apache.flink.configuration.Configuration.lambda$getOptional$2(Configuration.java:853)
    at java.util.Optional.map(Optional.java:215)
    at org.apache.flink.configuration.Configuration.getOptional(Configuration.java:853)
    ... 23 more {code}
The reason is that the value for Duration can not be negative. We should change the behavior or support to trigger flushing only when the output buffer is full.",Jiangang,Jiangang,Major,Closed,Fixed,06/May/23 07:13,06/Jun/23 02:20
Bug,FLINK-32027,13535320,Batch jobs could hang at shuffle phase when max parallelism is really large,"In batch stream mode with adaptive batch schedule mode, If we set the max parallelism large as 32768 (pipeline.max-parallelism), the job could hang at the shuffle phase:

It would hang for a long time and show ""No bytes sent"":
 !image-2023-05-08-11-12-58-361.png! 

After some time to debug, we can see the downstream operator did not receive the end-of-partition event.
",Weijie Guo,yunta,Blocker,Closed,Fixed,08/May/23 03:17,10/May/23 03:00
Bug,FLINK-32029,13535355,FutureUtils.handleUncaughtException swallows exceptions that are caused by the exception handler code,I ran into an issue in FLINK-31773 where the passed exception handler relies on the {{leaderContender}} field of the {{DefaultLeaderElectionService}}. This field can become {{null}} with the changes of FLINK-31773. But the {{null}} check was missed in the error handling code. This bug wasn't exposed because {{FutureUtils.handleUncaughtException(..)}} expects the passed error handler callback to be bug-free.,mapohl,mapohl,Major,Resolved,Fixed,08/May/23 09:18,10/May/23 16:15
Bug,FLINK-32031,13535362,Flink GCP Connector having issues with Conscrypt library,"When using the current pubsub connector it is not using the latest libraries bom due to which when the connector is used in Cloud Dataproc it is failing with conscrypt related issues.

 
{code:java}
Caused by: repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannel$AnnotatedSocketException: Network is unreachable: pubsub.googleapis.com/2607:f8b0:4001:c23:0:0:0:5f:443
Caused by: java.net.SocketException: Network is unreachable
    at java.base/sun.nio.ch.Net.connect0(Native Method)
    at java.base/sun.nio.ch.Net.connect(Net.java:483)
    at java.base/sun.nio.ch.Net.connect(Net.java:472)
    at java.base/sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:692)
    at repackaged.io.grpc.netty.shaded.io.netty.util.internal.SocketUtils$3.run(SocketUtils.java:91)
    at repackaged.io.grpc.netty.shaded.io.netty.util.internal.SocketUtils$3.run(SocketUtils.java:88)
    at java.base/java.security.AccessController.doPrivileged(Native Method)
    at repackaged.io.grpc.netty.shaded.io.netty.util.internal.SocketUtils.connect(SocketUtils.java:88)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:315)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:248)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1342)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:548)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:533)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.ChannelDuplexHandler.connect(ChannelDuplexHandler.java:54)
    at repackaged.io.grpc.netty.shaded.io.grpc.netty.WriteBufferingAndExceptionHandler.connect(WriteBufferingAndExceptionHandler.java:150)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:548)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.access$1000(AbstractChannelHandlerContext.java:61)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext$9.run(AbstractChannelHandlerContext.java:538)
    at repackaged.io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    at repackaged.io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    at repackaged.io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    at repackaged.io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at repackaged.io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:829){code}

On disabling ipv6 getting the following error:-
{code:java}
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
    at io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:244)
    at io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:225)
    at io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:142)
    at com.google.pubsub.v1.SubscriberGrpc$SubscriberBlockingStub.pull(SubscriberGrpc.java:1641)
    at org.apache.flink.streaming.connectors.gcp.pubsub.BlockingGrpcPubSubSubscriber.pull(BlockingGrpcPubSubSubscriber.java:73)
    at org.apache.flink.streaming.connectors.gcp.pubsub.BlockingGrpcPubSubSubscriber.pull(BlockingGrpcPubSubSubscriber.java:77)
    at org.apache.flink.streaming.connectors.gcp.pubsub.BlockingGrpcPubSubSubscriber.pull(BlockingGrpcPubSubSubscriber.java:77)
    at org.apache.flink.streaming.connectors.gcp.pubsub.BlockingGrpcPubSubSubscriber.pull(BlockingGrpcPubSubSubscriber.java:77)
    at org.apache.flink.streaming.connectors.gcp.pubsub.BlockingGrpcPubSubSubscriber.pull(BlockingGrpcPubSubSubscriber.java:67)
    at org.apache.flink.streaming.connectors.gcp.pubsub.PubSubSource.run(PubSubSource.java:128)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:332)
Caused by: javax.net.ssl.SSLHandshakeException: No subjectAltNames on the certificate match
    at org.conscrypt.SSLUtils.toSSLHandshakeException(SSLUtils.java:361)
    at org.conscrypt.ConscryptEngine.convertException(ConscryptEngine.java:1135)
    at org.conscrypt.ConscryptEngine.readPlaintextData(ConscryptEngine.java:1090)
    at org.conscrypt.ConscryptEngine.unwrap(ConscryptEngine.java:867)
    at org.conscrypt.ConscryptEngine.unwrap(ConscryptEngine.java:738)
    at org.conscrypt.Java8EngineWrapper.unwrap(Java8EngineWrapper.java:252)
    at org.conscrypt.Conscrypt.unwrap(Conscrypt.java:605)    {code}
 ",jjayadeep,jjayadeep,Major,Closed,Fixed,08/May/23 10:31,04/Jul/23 09:08
Bug,FLINK-32034,13535386,Python's DistUtils is deprecated as of 3.10,"I have recent just went through an upgrade from 1.13 to 1.17, along with that I upgraded the python version on our Flink Session server. Most everything that is part of our workflow works, except for Python Dependency Management.

After doing some digging, I found the reason is due to the DeprecationWarning that is printed when trying to get the site packages path. The script is 

GET_SITE_PACKAGES_PATH_SCRIPT and it is executed in the getSitePackagesPath method in the PythonEnvironmentManagerUtils class. The issue is that the DeprecationWarning is included into the PYTHONPATH environment variable which is passed to the beam runner. The deprecation warning breaks Python's ability to find the site packages due to characters that are not allowed in filesystem paths.

 

Example of the PYTHONPATH environment variable:
PYTHONPATH == <string>:1: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives:/tmp/python-dist-c63e1464-925c-4289-bb71-c6f50e83186f/python-requirements/lib/python3.10/site-packages
HADOOP_CONF_DIR == /opt/flink/conf",coltenp,coltenp,Minor,Closed,Fixed,08/May/23 14:23,15/Jun/23 12:16
Bug,FLINK-32041,13535530,flink-kubernetes-operator RoleBinding for Leases not created in correct namespace when using watchNamespaces,"When enabling [HA for flink-kubernetes-operator|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/configuration/#leader-election-and-high-availability] RBAC rules must be created to allow the flink-operator to manage k8s Lease resources.  When not using {{{}watchNamespaces{}}}, the RBAC rules are created at the k8s cluster level scope, giving the flink-operator ServiceAccount the ability to manage all needed k8s resources for all namespaces.

However, when using {{{}watchNamespaces{}}}, RBAC rules are only created in the {{{}watchNamepaces{}}}.  For most rules, this is correct, as the operator needs to manage resources like Flink pods and deployments in the {{{}watchNamespaces{}}}.  

However, For flink-kubernetes-operator HA, the Lease resource is managed in the same namespace in which the operator is deployed.  

The Helm chart should be fixed so that the proper RBAC rules for Leases are created to allow the operator's ServiceAccount in the operator's namespace.

Mailing list discussion [here.|https://lists.apache.org/thread/yq89jm0szkcodfocm5x7vqnqdmh0h1l0]",tchin,ottomata,Major,Closed,Fixed,09/May/23 14:27,02/Jun/23 12:16
Bug,FLINK-32043,13535608,SqlClient session unrecoverable once one wrong setting occurred,"In sql client, it can not work normally once one wrong setting occurred
{code:java}
// wrong setting here

Flink SQL> SET table.sql-dialect = flink;
[INFO] Execute statement succeed.

Flink SQL> select '' AS f1, a from t1;
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINK

Flink SQL> SET table.sql-dialect = default;
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINK

Flink SQL> RESET table.sql-dialect;
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINK

Flink SQL> RESET;
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINK 
{code}",fsk119,lincoln.86xy,Critical,Closed,Fixed,10/May/23 08:33,25/May/23 11:51
Bug,FLINK-32048,13535652,"DecimalITCase.testAggMinGroupBy fails with ""Insufficient number of network buffers""","[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48855&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4]
{code:java}
May 10 09:37:41 Caused by: java.io.IOException: Insufficient number of network buffers: required 1, but only 0 available. The total number of network buffers is currently set to 2048 of 32768 bytes each. You can increase this number by setting the configuration keys 'taskmanager.memory.network.fraction', 'taskmanager.memory.network.min', and 'taskmanager.memory.network.max'.
May 10 09:37:41 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.internalCreateBufferPool(NetworkBufferPool.java:495)
May 10 09:37:41 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.createBufferPool(NetworkBufferPool.java:456)
May 10 09:37:41 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.lambda$createBufferPoolFactory$3(SingleInputGateFactory.java:330)
May 10 09:37:41 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setup(SingleInputGate.java:274)
May 10 09:37:41 	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.setup(InputGateWithMetrics.java:105)
May 10 09:37:41 	at org.apache.flink.runtime.taskmanager.Task.setupPartitionsAndGates(Task.java:969)
May 10 09:37:41 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:654)
May 10 09:37:41 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
May 10 09:37:41 	at java.lang.Thread.run(Thread.java:748)
{code}",tanyuxin,wanglijie,Critical,Resolved,Fixed,10/May/23 14:22,11/May/23 11:57
Bug,FLINK-32056,13535745,Update the used Pulsar connector in flink-python to 4.0.0,"flink-python still references and tests flink-connector-pulsar:3.0.0, while it should be using flink-connector-pulsar:4.0.0. That's because the newer version is the only version compatible with Flink 1.17 and it doesn't rely on flink-shaded. ",martijnvisser,martijnvisser,Critical,Closed,Fixed,11/May/23 07:44,23/May/23 12:10
Bug,FLINK-32061,13535773,Resource metric groups are not cleaned up on removal,Not cleaning up leaks memory.,mxm,mxm,Major,Resolved,Fixed,11/May/23 10:56,11/May/23 11:13
Bug,FLINK-32066,13535927,Flink CI service on Azure stops responding to pull requests,"As of the time when this issue was created, Flink's CI service on Azure could no longer be triggered by new pull requests.
!20230512152023.jpg!",jingge,Wencong Liu,Blocker,Resolved,Fixed,12/May/23 07:22,12/May/23 16:52
Bug,FLINK-32067,13535943,"When no pod template configured, an invalid null pod template is configured ","https://issues.apache.org/jira/browse/FLINK-30609 introduced a bug in the podtemplate logic that breaks deployments when no podtemplates are configured.

The basic example doesnt work anymore for example. The reason is that an invalid null object is set as podtemplate when nothing is configured.",gyfora,gyfora,Blocker,Closed,Fixed,12/May/23 09:37,12/May/23 12:48
Bug,FLINK-32094,13536145,startScheduling.BATCH performance regression since May 11th,http://codespeed.dak8s.net:8000/timeline/#/?exe=5&ben=startScheduling.BATCH&extr=on&quarts=on&equid=off&env=2&revs=200,tanyuxin,martijnvisser,Blocker,Resolved,Fixed,14/May/23 21:57,15/May/23 13:35
Bug,FLINK-32096,13536175,Flink 1.17 doc points to non-existant branch in flink-training repo,"There is one broken link in Flink documentation which i think needs commiter or PMC privileges.
In [Hands-on|https://nightlies.apache.org/flink/flink-docs-stable/docs/learn-flink/etl/#hands-on] section

>  The hands-on exercise that goes with this section is the Rides and Fares .

[Rides and Fares|https://github.com/apache/flink-training/blob/release-1.17//rides-and-fares] points to non-existent branch in flink-training repo and leads to 404 (Not Found)  [hyper-link|https://github.com/apache/flink-training/blob/release-1.17//rides-and-fares]. This is due to missing `release-1.17` branch in [flink-training|https://github.com/apache/flink-training/] repo.
 ",yunta,samrat007,Major,Resolved,Fixed,15/May/23 07:08,16/May/23 07:03
Bug,FLINK-32100,13536214,Max parallelism is incorrectly calculated with multiple topics,"So far, we've taken the max number partitions we can find. However, the correct way to calculate the
max source parallelism would be to sum the number of partitions of all topis.
",mxm,mxm,Major,Resolved,Fixed,15/May/23 10:57,16/May/23 17:23
Bug,FLINK-32103,13536319,RBAC flinkdeployments/finalizers missing for OpenShift Deployment,"In OpenShift 4.10 and above, I'm noticing with the Flink 1.5.0 RC release that there's an issue with flinkdeployments on OpenShift.  Flinkdeployments are stuck in upgrading:
{quote}oc get flinkdep

NAME                                    JOB STATUS   LIFECYCLE STATE

basic-example                                        UPGRADING
{quote}
 

The error message looks like:
{quote}oc describe flinkdep basic-example

....

Error:                          {""type"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException"",""message"":""org.apache.flink.client.deployment.ClusterDeploymentException: Could not create Kubernetes cluster \""basic-example\""."",""throwableList"":[\{""type"":""org.apache.flink.client.deployment.ClusterDeploymentException"",""message"":""Could not create Kubernetes cluster \""basic-example\"".""},\{""type"":""org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException"",""message"":""Failure executing: POST at: https://172.30.0.1/apis/apps/v1/namespaces/default/deployments. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. deployments.apps \""basic-example\"" is forbidden: cannot set blockOwnerDeletion if an ownerReference refers to a resource you can't set finalizers on: , <nil>.""}]}

 

 Job Manager Deployment Status:  MISSING
{quote}
 

The solution is to fix it in the rbac.yaml of the helm template, adding a ""  - flinkdeployments/finalizers"" line to the flink.apache.org apiGroup.

 

If the Operator is already running and flinkdeployments are having trouble on OpenShift, then someone can manually edit the flink-kubernetes-operator.v1.5.0 clusterrole and add the

""  - flinkdeployments/finalizers"" in the flink.apache.org apiGroup.

 

I'll create a PR that addresses this.",jbusche,jbusche,Major,Closed,Fixed,15/May/23 21:16,19/May/23 06:33
Bug,FLINK-32106,13536366,Unstable connection to archive.apache.org on AZP,"Azure pipelines fail with 
{noformat}
Using Google mirror

Resolving archive.apache.org (archive.apache.org)... 138.201.131.134, 2a01:4f8:172:2ec5::2
Connecting to archive.apache.org (archive.apache.org)|138.201.131.134|:443... failed: Connection timed out.
Connecting to archive.apache.org (archive.apache.org)|2a01:4f8:172:2ec5::2|:443... failed: Network is unreachable.
##[error]Bash exited with code '4'.

{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49020&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=9baa6deb-e632-5387-d76c-cf2ba9138f2e&l=16",Sergey Nuyanzin,Sergey Nuyanzin,Critical,Closed,Fixed,16/May/23 07:05,25/May/23 10:46
Bug,FLINK-32110,13536413,TM native memory leak when using time window in Pyflink ThreadMode,"If job use time window in Pyflink thread mode, TM native memory will grow slowly during the job running until TM can't allocate memory from operate system.

The leak rate is likely proportional to the number of key.",yunjunluo,yunjunluo,Major,Closed,Fixed,16/May/23 14:06,25/May/23 09:50
Bug,FLINK-32111,13536420,Replacing cluster in failed state with a new one failed,"I deployed a problematic cluster(HA enabled with 3 JMs) to check cluster updates process. The cluster was in restart loops. Then I provided a newer CRD (Updated several configurations) and expected the cluster to get re-deployed. however the following exception happened

 

Caused by: java.lang.NullPointerException
        at org.apache.flink.kubernetes.operator.service.CheckpointHistoryWrapper.getInProgressCheckpoint(CheckpointHistoryWrapper.java:60) 
        at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.getCheckpointInfo(AbstractFlinkService.java:564) 
        at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.getLastCheckpoint(AbstractFlinkService.java:520) 
        at org.apache.flink.kubernetes.operator.observer.SavepointObserver.observeLatestSavepoint(SavepointObserver.java:209) 
        at org.apache.flink.kubernetes.operator.observer.SavepointObserver.observeSavepointStatus(SavepointObserver.java:73) 
        at org.apache.flink.kubernetes.operator.observer.deployment.ApplicationObserver.observeFlinkCluster(ApplicationObserver.java:61) 
        at org.apache.flink.kubernetes.operator.observer.deployment.AbstractFlinkDeploymentObserver.observeInternal(AbstractFlinkDeploymentObserver.java:73) 
        at org.apache.flink.kubernetes.operator.observer.AbstractFlinkResourceObserver.observe(AbstractFlinkResourceObserver.java:53) 
        at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:134) 

 

upgradeMode was first `last-state` and then I changed it to `stateless` but it still did not deploy the new cluster.",tamirsagi,tamirsagi,Major,Closed,Fixed,16/May/23 15:07,15/Jun/23 11:40
Bug,FLINK-32123,13536612,Avro Confluent Schema Registry nightly end-to-end test failed due to timeout,"For the past few hours, E2E tests fail with: 'Avro Confluent Schema Registry nightly end-to-end test' failed after 9 minutes and 53 seconds! Test exited with exit code 1

Looks like [https://archive.apache.org/dist/kafka/]  mirror is overloaded – download locally took more than 30min

Lets switch to  [https://downloads.apache.org|https://downloads.apache.org/] mirror
 ",pgaref,pgaref,Blocker,Closed,Fixed,17/May/23 19:25,20/May/23 17:38
Bug,FLINK-32129,13536800,Filesystem connector is not compatible with option 'pipeline.generic-types',"Filesystem connector always output 'PartitionCommitInfo' message even when there is no partition in the sink table, which will cause exception `java.lang.UnsupportedOperationException: Generic types have been disabled in the ExecutionConfig and type java.util.List is treated as a generic type.` when `pipeline.generic-types` is false",zjureel,zjureel,Major,Closed,Fixed,19/May/23 02:27,22/May/23 11:47
Bug,FLINK-32134,13536868,Autoscaler min/max parallelism configs should respect the current job parallelism,The autoscaler should never scale the job purely due to max/min parallelism configs. We should adjust these limits to the current parallelism,gyfora,gyfora,Major,Closed,Fixed,19/May/23 13:19,22/May/23 13:42
Bug,FLINK-32136,13536902,Pyflink gateway server launch fails when purelib != platlib,"On distros where python's {{purelib}} is different than {{platlib}} (e.g. Amazon Linux 2, but from my research it's all of the Redhat-based ones), you wind up with components of packages being installed across two different locations (e.g. {{/usr/local/lib/python3.7/site-packages/pyflink}} and {{{}/usr/local/lib64/python3.7/site-packages/pyflink{}}}).

{{_find_flink_home}} [handles|https://github.com/apache/flink/blob/06688f345f6793a8964ec00002175f44cda13c33/flink-python/pyflink/find_flink_home.py#L58C63-L60] this, and in flink releases <= 1.13.2 its setting of the {{FLINK_LIB_DIR}} environment variable was the one being used. However, from 1.13.3, a refactoring of {{launch_gateway_server_process}} ([1.13.2,|https://github.com/apache/flink/blob/release-1.13.2/flink-python/pyflink/pyflink_gateway_server.py#L200] [1.13.3|https://github.com/apache/flink/blob/release-1.13.3/flink-python/pyflink/pyflink_gateway_server.py#L280]) re-ordered some method calls. {{{}prepare_environment_variable{}}}'s [non-awareness|https://github.com/apache/flink/blob/release-1.13.3/flink-python/pyflink/pyflink_gateway_server.py#L94C67-L95] of multiple homes and setting of {{FLINK_LIB_DIR}} now is the one that matters, and it is the incorrect location.

I've confirmed this problem on Amazon Linux 2 and 2023. The problem does not exist on, for example, Ubuntu 20 and 22 (for which {{platlib}} == {{{}purelib{}}}).

Repro steps on Amazon Linux 2
{quote}{{yum -y install python3 java-11}}
{{pip3 install apache-flink==1.13.3}}
{{python3 -c 'from pyflink.table import EnvironmentSettings ; EnvironmentSettings.new_instance()'}}
{quote}
The resulting error is
{quote}{{The flink-python jar is not found in the opt folder of the FLINK_HOME: /usr/local/lib64/python3.7/site-packages/pyflink}}
{{Error: Could not find or load main class org.apache.flink.client.python.PythonGatewayServer}}
{{Caused by: java.lang.ClassNotFoundException: org.apache.flink.client.python.PythonGatewayServer}}
{{Traceback (most recent call last):}}
{{  File ""<string>"", line 1, in <module>}}
{{  File ""/usr/local/lib64/python3.7/site-packages/pyflink/table/environment_settings.py"", line 214, in new_instance}}
{{    return EnvironmentSettings.Builder()}}
{{  File ""/usr/local/lib64/python3.7/site-packages/pyflink/table/environment_settings.py"", line 48, in {_}{{_}}init{{_}}{_}}}
{{    gateway = get_gateway()}}
{{  File ""/usr/local/lib64/python3.7/site-packages/pyflink/java_gateway.py"", line 62, in get_gateway}}
{{    _gateway = launch_gateway()}}
{{  File ""/usr/local/lib64/python3.7/site-packages/pyflink/java_gateway.py"", line 112, in launch_gateway}}
{{    raise Exception(""Java gateway process exited before sending its port number"")}}
{{Exception: Java gateway process exited before sending its port number}}
{quote}
The flink home under /lib64/ does not contain the jar, but it is in the /lib/ location
{quote}{{bash-4.2# find /usr/local/lib64/python3.7/site-packages/pyflink -name ""flink-python*.jar""}}
{{bash-4.2# find /usr/local/lib/python3.7/site-packages/pyflink -name ""flink-python*.jar""}}
{{/usr/local/lib/python3.7/site-packages/pyflink/opt/flink-python_2.11-1.13.3.jar}}
{quote}
 ",dianfu,wash,Major,Closed,Fixed,19/May/23 17:34,17/Jun/23 10:03
Bug,FLINK-32139,13536951,Data accidentally deleted and not deleted when upsert sink to hbase,"h4. *Problem background*

We meet data accidental deletion and non deletion issues when synchronizing MySQL cdc data to HBase using HBase connectors.
h3. Reproduction steps

1、The Flink job with 1 parallelism synchronize a MySQL table into HBase. SinkUpsertMaterializer is tunned off by setting {{{}table.exec.sink.upsert-materialize = 'NONE'{}}}。

MySQL table schema is as follows。
{code:java}
CREATE TABLE `source_sample_1001` (
`id` int(11) NOT NULL AUTO_INCREMENT,
`name` varchar(200) DEFAULT NULL,
`age` int(11) DEFAULT NULL,
`weight` float DEFAULT NULL,
PRIMARY KEY (`id`)
);{code}
The source table definition in Flink is as follows.
{code:java}
CREATE TABLE `source_sample_1001` (
    `id` bigint,
    `name` String,
    `age` bigint,
    `weight` float,
  PRIMARY KEY (`id`) NOT ENFORCED
) WITH (
'connector' = 'mysql-cdc' ,
'hostname' = '${ip}',
'port' = '3306',
'username' = '${user}',
'password' = '${password}',
'database-name' = 'testdb_0010',
'table-name' = 'source_sample_1001'
);{code}
HBase sink table are created in {{testdb_0011}} namespace.
{code:java}
CREATE 'testdb_0011:source_sample_1001', 'data'
​
describe 'testdb_0011:source_sample_1001'
 
# describe output
Table testdb_0011:source_sample_1001 is ENABLED                                                                                                                                                         
testdb_0011:source_sample_1001                                                                                                                                                                          
COLUMN FAMILIES DESCRIPTION                                                                                                                                                                             {NAME => 'data', BLOOMFILTER => 'ROW', IN_MEMORY => 'false', VERSIONS => '1', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', COMPRESSION => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0' , BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}
{code}
 

                                                                                                                                
The sink table definition in Flink.
{code:java}
CREATE TABLE `hbase_sink1` (
    `id` STRING COMMENT 'unique id',
    `data` ROW<
        `name` string,
        `age` string,
        `weight` string
    >,
    primary key(`id`) not enforced
) WITH (
  'connector' = 'hbase-2.2',
  'table-name' = 'testdb_0011:source_sample_1001',
  'zookeeper.quorum' = '${hbase.zookeeper.quorum}'
);{code}
DML in flink to synchronize data.
{code:java}
INSERT INTO `hbase_sink1` SELECT
    `id`, row(`name`, `age`, `weight`)
FROM (
    SELECT
        REVERSE(CONCAT_WS('', CAST(id AS VARCHAR ))) as id,
        `name`, cast(`age` as varchar) as `age`, cast(`weight` as varchar) as `weight`
    FROM `source_sample_1001`
) t;{code}
2、Another flink job sinks datagen data to the MySQL table {{source_sample_1001}} 。id range from 1 to 10_000， that means source_sample_1001 will have at most 10_000 records。
{code:java}
CREATE TABLE datagen_source (
    `id` int,
    `name` String,
    `age` int,
    `weight` int
) WITH (
   'connector' = 'datagen',
  'fields.id.kind' = 'random',
  'fields.id.min' = '1',
  'fields.id.max' = '10000',
  'fields.name.length' = '20',
  'fields.age.min' = '1',
  'fields.age.max' = '150',
  'fields.weight.min' = '5',
  'fields.weight.max' = '300',
  'rows-per-second' = '5000'
);
​
CREATE TABLE `source_sample_1001` (
    `id` bigint,
    `name` String,
    `age` bigint,
    `weight` float,
  PRIMARY KEY (`id`) NOT ENFORCED
) WITH (
  'connector' = 'jdbc',
  'url' = 'jdbc:mysql://${ip}:3306/testdb_0010?rewriteBatchedStatements=true&serverTimezone=Asia/Shanghai',
  'table-name' = 'source_sample_1001',
  'username' = '${user}',
  'password' = '${password}',
  'sink.buffer-flush.max-rows' = '500',
  'sink.buffer-flush.interval' = '1s'
);
​
-- dml
INSERT INTO `source_sample_1001` SELECT `id`, `name`, `age`, cast(`weight` as float) FROM `datagen_source`;{code}
3、A bash script deletes the MySQL table {{source_sample_1001}} with batch 10.
{code:java}
#!/bin/bash
​
mysql1=""mysql -h${ip} -u${user} -p${password}""
batch=10
​
for ((i=1; ;i++)); do
echo ""iteration $i start""
for ((j=1; j<=10000; j+=10)); do
  $mysql1 -e ""delete from testdb_0010.source_sample_1001 where id >= $j and id < $((j+10))""
done
echo ""iteration $i end""
sleep 10
done{code}
4、Start the above two flink jobs and the bash script. Wait for several minutes, usually 5 minutes is enough. Please note that deleting data bash script is necessary for reproduce the problem.

5、Stop the bash script, and waiting for MySQL table to fill up with 10_000 data by the datagen flink job。And then stop datagen flink job. Waiting for the sink hbase job to read all the binlog of MySQL table {{{}source_sample_1001{}}}.

6、Check the hbase table and reproduce the issue of data loss. As shown below, 67 records were lost in a test.
{code:java}
hbase(main):006:0> count 'testdb_0011:source_sample_1001'                                                   
9933 row(s)
Took 0.8724 seconds                                                                                                                                                                                     
=> 9933{code}
Find out a missing record and check the raw data in HBase.
{code:java}
hbase(main):008:0> get 'testdb_0011:source_sample_1001', '24'
COLUMN                                             CELL                                                                                                                                                
0 row(s)
Took 0.0029 seconds                                                                                                                                                                                     
hbase(main):009:0> scan 'testdb_0011:source_sample_1001', {RAW => true, VERSIONS => 1000, STARTROW => '24', STOPROW => '24'}
ROW                                                 COLUMN+CELL                                                                                                                                         
24                                                 column=data:name, timestamp=2023-05-20T21:17:44.884, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:44.884, value=3a8f571c25a9d9040ef3                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:43.769, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:43.769, value=5aada98281ee0a961841                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:42.902, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:42.902, value=599790a9a641e6121ab3                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:41.614, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:41.614, value=4ece6410d32959457f80                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:40.885, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:40.885, value=9edcfcf1c958a7e4ae2a                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:40.841, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:40.841, value=3d82dcf982d5bcd5b6b7                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:39.788, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:39.788, value=2888a338b65caaf15b30                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.799, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.799, value=a8d7549e18ef0c0e8674                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.688, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.688, value=ada7237e52d030dcef7a                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.650, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.650, value=482feed26918dcdc911e                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:34.885, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:34.885, value=36d6bdd585dbb65dedb7                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.905, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.905, value=6e15c4462f8435040700                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.803, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.803, value=d122df5afd4eac32da72                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.693, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.693, value=ed603d47fedb3852b520                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:31.784, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:31.784, value=1ebdd5fe6310850b8098                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:30.684, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:30.684, value=cc628ba45d1ad07fce2f                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:29.812, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:29.812, value=c1d4df6e987bdb3cd0a3                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:29.590, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:29.590, value=535557700ca01c6b6b1e                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:28.876, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:28.876, value=a63c2ebfefc82eab4bcf                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:28.565, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:28.565, value=dd2b24ff0dfa672c49ba                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:27.879, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:27.879, value=69dbe1287c2bc54781ab                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:27.699, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:27.699, value=775d06dcbf1148e665ee                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:24.209, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:24.209, value=e23c010ab06125c88870                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:22.480, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:20.716, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:18.678, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:17.720, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:16.858, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:16.682, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:15.753, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:14.571, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:11.572, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:09.681, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:08.792, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:05.888, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:05.754, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:03.626, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:02.652, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:01.790, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:00.986, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:59.797, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:58.982, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:58.781, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:58.626, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:58.149, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:56.610, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:51.655, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:51.458, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:44.860, type=Delete                                                                                    
1 row(s)
Took 0.1466 seconds                                                                                  {code}
7、Start the bash script to delete all data of the MySQL table. Waiting for the sink hbase job to read all the binlog of MySQL table {{{}source_sample_1001{}}}.

6、Check the hbase table and reproduce the issue of data no deletion. As shown below, 6 records were not deleted in the test.
{code:java}
hbase(main):012:0> count 'testdb_0011:source_sample_1001'
6 row(s)
Took 0.5121 seconds                                                                                                                                                                                     
=> 6{code}
Check the raw data of a record in HBase.
{code:java}
hbase(main):013:0> get 'testdb_0011:source_sample_1001', '3668'
COLUMN                                             CELL                                                                                                                                                
data:name                                         timestamp=2023-05-20T21:17:26.714, value=ebb15f905622340d0351                                                                                       
1 row(s)
Took 0.0037 seconds                                                                                                                                                                                     
hbase(main):014:0> scan 'testdb_0011:source_sample_1001', {RAW => true, VERSIONS => 1000, STARTROW => '3668', STOPROW => '3668'}
ROW                                                 COLUMN+CELL                                                                                                                                         
3668                                               column=data:name, timestamp=2023-05-20T21:17:45.728, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:45.728, value=c675a12c7cbed27599c3                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:44.693, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:44.693, value=413921aa1ac44f545954                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:43.854, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:43.854, value=7d44b0efc0923e4035b7                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:41.721, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:41.721, value=60bfaef81bf8efdf781a                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:40.763, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:40.763, value=2c371f9cd3909dd3b3f8                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:37.872, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:37.872, value=9e32087cb39065976e50                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:32.573, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:32.573, value=708364bf84dad4a04170                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:26.811, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:26.811, value=c0e8e11eed3f8410dea9                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:26.714, value=ebb15f905622340d0351                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:24.310, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:24.310, value=21681a161ed2ccbe884e                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:23.508, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:23.508, value=a1ef547a9efd57a7a0e2                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:22.788, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:22.788, value=34e688060e6c40f4f83b                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:21.746, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:17.761, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:12.610, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:11.909, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:07.846, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:06.901, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:06.758, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:06.569, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:02.689, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:00.344, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:59.961, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:59.415, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:58.916, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:58.781, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:58.718, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:58.339, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:56.340, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:55.883, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:55.683, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:55.056, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:46.845, type=Delete                                                                                    
1 row(s)
Took 0.0457 seconds                                                                                          {code}
h4. *Reason for the problem*

The [HBase connector|https://github.com/apache/flink/blob/06688f345f6793a8964ec00002175f44cda13c33/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java#L177] use the [Delete key type|https://github.com/apache/hbase/blob/c05ee564d3026688bcfdc456071059c7c8409694/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Delete.java#L380] [without timestamp|https://github.com/apache/flink/blob/06688f345f6793a8964ec00002175f44cda13c33/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java#L168] to {{{}delete the latest version of the specified column. This is an expensive call in that on the server-side, it first does a get to find the latest versions timestamp. Then it adds a delete using the fetched cells timestamp{}}}. Causing the following issues:

Problem 1: When writing update data, the timestamp of -U and +U added by the hbase server to the update message may be the same, and -U deleted the latest version of +U data, resulting in accidental deletion of the data. The problem was also reported by https://issues.apache.org/jira/browse/FLINK-28910

Problem 2: When there are multiple versions of HBase data, deleting the data will exposes earlier versions of the data, and resulting in the issue of data no deletion.
h4. *Solution proposal* 

Use the [DeleteColumn key type|https://github.com/apache/hbase/blob/c05ee564d3026688bcfdc456071059c7c8409694/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Delete.java#L322] and set strongly increasing timestamp for [put|https://github.com/lzshlzsh/flink/blob/a2341810a244b97a3af32951e17efbc49f570cdd/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java#L138] and [delete|https://github.com/lzshlzsh/flink/blob/a2341810a244b97a3af32951e17efbc49f570cdd/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java#L170] mutation. The delete mutation will delete all versions of the specified column with a timestamp less than or equal to the specified.

I have test the proposed solution for several days, and neither the data accidental deletion nor no deletion issues happen.",LiuZeshan,LiuZeshan,Major,Closed,Fixed,20/May/23 14:49,30/Jun/23 12:41
Bug,FLINK-32141,13536955,SharedStateRegistry print too much info log,"FLINK-29095 added some log to SharedStateRegistry for trouble shooting. Among them, a info log be added when newHandle is equal to the registered one:

[https://github.com/apache/flink/blob/release-1.17.0/flink-runtime/src/main/java/org/apache/flink/runtime/state/SharedStateRegistryImpl.java#L117]

!image-2023-05-21-00-26-20-026.png|width=775,height=126!

But this case cannot be considered as a potential bug, because FsStateChangelogStorage will directly use the FileStateHandle of the previous checkpoint instead of PlaceholderStreamStateHandle.

In our tests, JobManager printed so much of this log that useful information was overwhelmed.

So I suggest change this log level to trace, WDYT [~Yanfei Lei], [~klion26] ?",Feifan Wang,Feifan Wang,Major,Resolved,Fixed,20/May/23 16:35,22/May/23 15:22
Bug,FLINK-32145,13537064,Fix incorrect docker image links of specific versions in flink-web site,"Current links of docker images in many release announcements point to an incorrect place, we should fix them to the correct locations.",yunta,yunta,Major,Resolved,Fixed,22/May/23 07:53,22/May/23 09:19
Bug,FLINK-32162,13537321,Misleading log message due to missing null check,"Updating the job requirements always logs ""Failed to update requirements for job {}."" because we don't check whether the error is not null.",chesnay,chesnay,Minor,Closed,Fixed,23/May/23 11:27,23/May/23 14:55
Bug,FLINK-32172,13537394,KafkaExample can not run with args,"i fork and clone flink-connector-kafka repo. after build and package, i run org/apache/flink/streaming/kafka/test/KafkaExample.java main() but failed,

comment say:
Example usage: --input-topic test-input --output-topic test-output --bootstrap.servers
* localhost:9092 --group.id myconsumer
 
but console print: Missing parameters!  from KafkaExampleUtil where need 5 paramters but we have 4
 
thank you for your attention to this matter",Weijie Guo,xulongfeng2018,Not a Priority,Closed,Fixed,23/May/23 23:16,25/May/23 13:05
Bug,FLINK-32189,13537639,Integration tests fail due to Process Exit Code: 239 and NoClassDefFound in logs ,Since there are multiple similar cases with different classes mentioned in {{NoClassDefFound}} this is an umbrella for such cases,,Sergey Nuyanzin,Critical,Closed,Fixed,25/May/23 10:55,13/Jun/23 21:36
Bug,FLINK-32190,13537651,Bad link in Flink page,"on the page: [https://flink.apache.org/use-cases/]
in the section: What are typical data analytics applications?

The first link: [Quality monitoring of Telco networks|http://2016.flink-forward.org/kb_sessions/a-brief-history-of-time-with-apache-flink-real-time-monitoring-and-analysis-with-flink-kafka-hb/]

returns a 404 error: 
h1. This site can’t be reached

Check if there is a typo in 2016.flink-forward.org.
DNS_PROBE_FINISHED_NXDOMAIN",JunRuiLi,claude,Minor,Closed,Fixed,25/May/23 11:50,01/Jun/23 06:09
Bug,FLINK-32199,13537747,MetricStore does not remove metrics of nonexistent parallelism in TaskMetricStore when scale down job parallelism,"After FLINK-29615, FLINK will update the subtask metrics store when scaling down parallelism. However, task metrics are added in the form of ""subtaskIndex + metric.name"" or ""subtaskIndex + operatorName + metric.name"". Users will be able to find many redundant metrics through JobVertexMetricsHandler, which will be very troublesome for users.",JunRuiLi,JunRuiLi,Major,Closed,Fixed,26/May/23 05:03,05/Jun/23 07:54
Bug,FLINK-32204,13537766,ZooKeeperLeaderElectionTest.testZooKeeperReelectionWithReplacement fails with The ExecutorService is shut down already. No Callables can be executed on AZP,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49386&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=7095] fails as
{noformat}

May 25 18:45:50 Caused by: java.util.concurrent.RejectedExecutionException: The ExecutorService is shut down already. No Callables can be executed.
May 25 18:45:50 	at org.apache.flink.util.concurrent.DirectExecutorService.throwRejectedExecutionExceptionIfShutdown(DirectExecutorService.java:237)
May 25 18:45:50 	at org.apache.flink.util.concurrent.DirectExecutorService.submit(DirectExecutorService.java:100)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache.publishEvent(TreeCache.java:902)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache.publishEvent(TreeCache.java:894)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache.access$1200(TreeCache.java:79)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache$TreeNode.processResult(TreeCache.java:489)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:926)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:683)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:272)
May 25 18:45:50 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:634)
May 25 18:45:50 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:553)
May 25 18:45:50
{noformat}",mapohl,Sergey Nuyanzin,Critical,Resolved,Fixed,26/May/23 08:03,13/Jun/23 07:16
Bug,FLINK-32217,13538092,Retain metric store can cause NPE ,"When metricsFetcher fetches metrics, it will update the metricsStore ([here|https://github.com/apache/flink/blob/d6c3d332340922c24d1af9dd8835d0bf790184b5/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricStore.java#LL91C44-L91C44]). But in this method, it can get null metricStore and cause NPE, which will lead to incorrect results of metrics retain, and we should also fix it from the perspective of stability.",JunRuiLi,JunRuiLi,Major,Closed,Fixed,30/May/23 09:23,05/Jun/23 08:20
Bug,FLINK-32219,13538176,SQL client hangs when executing EXECUTE PLAN,"I compiled a plan for an INSERT statement and executed the plan, but the SQL client became unresponsive when executing the EXECUTE PLAN statement. I confirmed that the Flink job is running normally by checking the Flink dashboard. The only issue is that the SQL client becomes stuck and cannot accept new commands. I printed the stack trace of the SQL client process, and here is a part of it for reference.
{code:java}
""pool-2-thread-1"" #30 prio=5 os_prio=31 tid=0x00000001172e5000 nid=0x6d03 waiting on condition [0x0000000173e01000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000076e72af20> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:83)
	at org.apache.flink.table.api.internal.InsertResultProvider.access$200(InsertResultProvider.java:37)
	at org.apache.flink.table.api.internal.InsertResultProvider$Iterator.hasNext(InsertResultProvider.java:106)
	at java.util.Iterator.forEachRemaining(Iterator.java:115)
	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:115)
	at org.apache.flink.table.gateway.service.result.ResultFetcher.fromTableResult(ResultFetcher.java:163)
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.callOperation(OperationExecutor.java:542)
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:440)
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:195)
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212)
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl$$Lambda$389/1391083077.apply(Unknown Source)
	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119)
	at org.apache.flink.table.gateway.service.operation.OperationManager$$Lambda$390/208625838.call(Unknown Source)
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258)
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation$$Lambda$392/670621032.run(Unknown Source)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
{code}",qingyue,xu_shuai_,Major,Closed,Fixed,31/May/23 03:01,13/Jun/23 07:26
Bug,FLINK-32226,13538254,RestClusterClient leaks jobgraph file if submission fails,"{code:java}
        submissionFuture
                .thenCompose(ignored -> jobGraphFileFuture)
                .thenAccept(
                        jobGraphFile -> {
                            try {
                                Files.delete(jobGraphFile);
                            } catch (IOException e) {
                                LOG.warn(""Could not delete temporary file {}."", jobGraphFile, e);
                            }
                        });
{code}
",chesnay,chesnay,Minor,Closed,Fixed,31/May/23 11:32,01/Jun/23 09:06
Bug,FLINK-32244,13538495,flink-master-benchmarks-regression-check always fails since 2023.05.30,"Since 2023.05.30, the flink-master-benchmarks-regression-check always fail:

[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1631|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1631/]

[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1632|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1631/]

[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1633] ",Yanfei Lei,wanglijie,Major,Closed,Fixed,02/Jun/23 02:09,07/Jun/23 08:16
Bug,FLINK-32245,13538496,NonDeterministicTests #testTemporalFunctionsInBatchMode failure masked due to incorrect test initialization,"The test case NonDeterministicTests #testTemporalFunctionsInBatchMode has been consistently failing due to incorrect test initialization.

 

However, this failure has been masked because the test class name ends with ""Tests"", causing the CI to skip the test case, which has been further validated by searching through the historical logs of the CI.

This issue needs to be addressed, and the test case should be executed to ensure proper testing. ",qingyue,qingyue,Major,Closed,Fixed,02/Jun/23 02:13,02/Jun/23 13:05
Bug,FLINK-32249,13538706,A Java string should be used instead of a Calcite NlsString to construct the column comment of CatalogTable,"when Flink interacts with CatalogTable, it directly passes the Calcite's NlsString comment as a string to the comment attribute of the schema and column. In theory, a Java string should be passed here, otherwise the CatalogTable implementers may encounter special character encoding issues, e.g., an issue in apache paimon: [https://github.com/apache/incubator-paimon/issues/1262]

also tested in sql-client:

{code}

Flink SQL> CREATE TABLE s1 (
>     order_id    STRING comment '测试中文',
>     price       DECIMAL(32,2) comment _utf8'测试_utf8中文',
>     currency    STRING,
>     order_time  TIMESTAMP(3)
> ) comment '测试中文table comment' WITH ('connector'='dategen');
[INFO] Execute statement succeed.

Flink SQL> show tables;
+------------+
| table name |
+------------+
|         s1 |
+------------+
1 row in set

Flink SQL> desc s1;
+------------+----------------+------+-----+--------+-----------+-------------------------+
|       name |           type | null | key | extras | watermark |                 comment |
+------------+----------------+------+-----+--------+-----------+-------------------------+
|   order_id |         STRING | TRUE |     |        |           | u&'\6d4b\8bd5\4e2d\6587 |
|      price | DECIMAL(32, 2) | TRUE |     |        |           |     _UTF8'测试_utf8中文 |
|   currency |         STRING | TRUE |     |        |           |                         |
| order_time |   TIMESTAMP(3) | TRUE |     |        |           |                         |
+------------+----------------+------+-----+--------+-----------+-------------------------+
4 rows in set

Flink SQL> show create table s1;
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                                                                                                                                   result |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| CREATE TABLE `default_catalog`.`default_database`.`s1` (
  `order_id` VARCHAR(2147483647),
  `price` DECIMAL(32, 2),
  `currency` VARCHAR(2147483647),
  `order_time` TIMESTAMP(3)
) COMMENT '测试中文table comment'
WITH (
  'connector' = 'dategen'
)
 |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set

{code}",lincoln.86xy,lincoln.86xy,Major,Closed,Fixed,04/Jun/23 15:38,06/Jun/23 00:53
Bug,FLINK-32250,13538729,Revert the Field name of BUFFER_TIMEOUT to improve compatibility,"FLINK-32023 changed the `ExecutionOptions.BUFFER_TIMEOUT` to `ExecutionOptions.BUFFER_TIMEOUT_INTERVAL`, the filed name should be reverted.

Because the `ExecutionOptions` is a public evolving API, some flink users are using the `ExecutionOptions.BUFFER_TIMEOUT` in their code. If we update it, the code cannot upgrade to 1.18 directly.

 

BTW, the option name is changed from `execution.buffer-timeout` to `execution.buffer-timeout.interval`. However, we marked the `execution.buffer-timeout` as `DeprecatedKeys`. So 1.18 is compatible with the old option name.

 ",fanrui,fanrui,Major,Closed,Fixed,05/Jun/23 05:07,06/Jun/23 02:24
Bug,FLINK-32254,13538769,FineGrainedSlotManager may not allocate enough taskmangers if maxSlotNum is configured,"We ran a job with {{slotmanager.number-of-slots.max = 10}}, {{taskmanager.numberOfTaskSlots = 10}} and {{taskmanager.memory.process.size: 24000m}}. The resources of the cluster are sufficient, but no TaskManager can be allocated. It seems that there is a problem with the calculation logic of {{SlotManagerConfiguration#getMaxTotalMem}}. Due to the rounding down of division, the calculated {{MemorySize}} is too small.",Weijie Guo,Weijie Guo,Major,Closed,Fixed,05/Jun/23 13:04,07/Jun/23 09:01
Bug,FLINK-32289,13539226,The metadata column type is incorrect in Kafka table connector example,"The example[1] defined ts column with TIMESTAMP type

 
{code:java}
  `ts` TIMESTAMP(3) METADATA FROM 'timestamp'
{code}
the correct column type should be TIMESTAMP_LTZ type.

 
{code:java}
 `ts` TIMESTAMP_LTZ(3) METADATA FROM 'timestamp'  {code}
 

[1] https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/table/kafka/#how-to-create-a-kafka-table",xiqian_yu,leonard,Major,Resolved,Fixed,08/Jun/23 09:10,13/Jun/23 14:11
Bug,FLINK-32292,13539349,TableUtils.getRowTypeInfo fails to get type information of Tuple,,zhangzp,zhangzp,Major,Closed,Fixed,09/Jun/23 01:28,09/Jun/23 02:17
Bug,FLINK-32294,13539354,The CI fails due to HiveITCase,"2 ITCases fail:
 * HiveITCase.testHiveDialect
 * HiveITCase.testReadWriteHive

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49766&view=logs&j=ef799394-2d67-5ff4-b2e5-410b80c9c0af&t=9e5768bc-daae-5f5f-1861-e58617922c7a&l=14346]

 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49766&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&s=ae4f8708-9994-57d3-c2d7-b892156e7812&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=14652]

 

 ",yuxia,fanrui,Major,Closed,Fixed,09/Jun/23 02:28,09/Jun/23 03:14
Bug,FLINK-32311,13539651,ZooKeeperLeaderElectionTest.testZooKeeperReelectionWithReplacement and DefaultLeaderElectionService.onGrantLeadership fell into dead lock,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49750&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8]

 

there are 2 threads one locked {{0x00000000e3a8a1e8}} and waiting for {{0x00000000e3a89c18}}

{noformat}

2023-06-08T01:18:54.5609123Z Jun 08 01:18:54 ""ForkJoinPool-50-worker-25-EventThread"" #956 daemon prio=5 os_prio=0 tid=0x00007f9374253800 nid=0x6a4e waiting for monitor entry [0x00007f94b63e1000]
2023-06-08T01:18:54.5609820Z Jun 08 01:18:54    java.lang.Thread.State: BLOCKED (on object monitor)
2023-06-08T01:18:54.5610557Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.runInLeaderEventThread(DefaultLeaderElectionService.java:425)
2023-06-08T01:18:54.5611459Z Jun 08 01:18:54 	- waiting to lock <0x00000000e3a89c18> (a java.lang.Object)
2023-06-08T01:18:54.5612198Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.onGrantLeadership(DefaultLeaderElectionService.java:300)
2023-06-08T01:18:54.5613110Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.isLeader(ZooKeeperLeaderElectionDriver.java:153)
2023-06-08T01:18:54.5614070Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch$$Lambda$1649/586959400.accept(Unknown Source)
2023-06-08T01:18:54.5615014Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager.lambda$forEach$0(MappingListenerManager.java:92)
2023-06-08T01:18:54.5616259Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager$$Lambda$1640/1393625763.run(Unknown Source)
2023-06-08T01:18:54.5617137Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager$$Lambda$1633/2012730699.execute(Unknown Source)
2023-06-08T01:18:54.5618047Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager.forEach(MappingListenerManager.java:89)
2023-06-08T01:18:54.5618994Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.StandardListenerManager.forEach(StandardListenerManager.java:89)
2023-06-08T01:18:54.5620071Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:711)
2023-06-08T01:18:54.5621198Z Jun 08 01:18:54 	- locked <0x00000000e3a8a1e8> (a org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch)
2023-06-08T01:18:54.5622072Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.checkLeadership(LeaderLatch.java:597)
2023-06-08T01:18:54.5622991Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.access$600(LeaderLatch.java:64)
2023-06-08T01:18:54.5623988Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch$7.processResult(LeaderLatch.java:648)
2023-06-08T01:18:54.5624965Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:926)
2023-06-08T01:18:54.5626218Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:683)
2023-06-08T01:18:54.5627369Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152)
2023-06-08T01:18:54.5628353Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:187)
2023-06-08T01:18:54.5629281Z Jun 08 01:18:54 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:666)
2023-06-08T01:18:54.5630124Z Jun 08 01:18:54 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:553)
{noformat}
and another locked {{0x00000000e3a89c18}} and waits for {{0x00000000e3a8a1e8}}
{noformat}
2023-06-08T01:18:54.5738286Z Jun 08 01:18:54 ""ForkJoinPool-50-worker-25"" #620 daemon prio=5 os_prio=0 tid=0x00007f953874f000 nid=0x682e waiting for monitor entry [0x00007f95461d4000]
2023-06-08T01:18:54.5738959Z Jun 08 01:18:54    java.lang.Thread.State: BLOCKED (on object monitor)
2023-06-08T01:18:54.5739645Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.close(LeaderLatch.java:203)
2023-06-08T01:18:54.5740731Z Jun 08 01:18:54 	- waiting to lock <0x00000000e3a8a1e8> (a org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch)
2023-06-08T01:18:54.5741591Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.close(LeaderLatch.java:190)
2023-06-08T01:18:54.5742609Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.close(ZooKeeperLeaderElectionDriver.java:135)
2023-06-08T01:18:54.5743491Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.close(DefaultLeaderElectionService.java:217)
2023-06-08T01:18:54.5744427Z Jun 08 01:18:54 	- locked <0x00000000e3a89c18> (a java.lang.Object)
2023-06-08T01:18:54.5745200Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.testZooKeeperReelectionWithReplacement(ZooKeeperLeaderElectionTest.java:346)
2023-06-08T01:18:54.5746206Z Jun 08 01:18:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-06-08T01:18:54.5746829Z Jun 08 01:18:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-06-08T01:18:54.5747552Z Jun 08 01:18:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-06-08T01:18:54.5748207Z Jun 08 01:18:54 	at java.lang.reflect.Method.invoke(Method.java:498)
...
{noformat}",mapohl,Sergey Nuyanzin,Critical,Resolved,Fixed,12/Jun/23 11:17,15/Jun/23 12:00
Bug,FLINK-32313,13539653,CrateDB relies on flink-shaded in flink-connector-jdbc,See https://github.com/apache/flink-connector-jdbc/blob/main/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/databases/cratedb/catalog/CrateDBCatalog.java#L27 - JDBC shouldn't rely on flink-shaded. ,snuyanzin,martijnvisser,Blocker,Closed,Fixed,12/Jun/23 11:47,12/Jun/23 18:50
Bug,FLINK-32322,13539788,Flink CI mirror service on Azure stops responding to commits to master,Last time it ran it was on Saturday(11.06.2023),jingge,Sergey Nuyanzin,Blocker,Resolved,Fixed,13/Jun/23 06:52,13/Jun/23 12:33
Bug,FLINK-32325,13539817,SqlServerDynamicTableSourceITCase is flaky,"{code:java}
[INFO] Running org.apache.flink.connector.jdbc.databases.sqlserver.table.SqlServerDynamicTableSourceITCase
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.533 s - in org.apache.flink.connector.jdbc.JdbcITCase
[INFO] Running org.apache.flink.connector.jdbc.databases.sqlserver.table.SqlServerTableSourceITCase
Jun 13, 2023 8:49:50 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 39249b3b-40c2-4f71-9598-20abe5e93d2d Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:50 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 2c0e4870-7284-4022-b97d-7f441fc834dd Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:50 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 34e9eff4-445c-477e-8975-d23180897ff8 Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:50 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 0d4fe549-66e7-4354-b7c5-ed7ee66527d2 Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:51 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 4e98d176-2f1f-4dec-af3e-798ecb536c39 Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:52 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 32ba6716-772b-42f3-b27c-9ec1593adcd7 Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:32ba6716-772b-42f3-b27c-9ec1593adcd7
Jun 13, 2023 8:49:53 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: fe5e363f-fade-48b8-beb1-9f2e3a524282 Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:54 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: b454a476-5f05-4cd7-bb43-f62e1f7e030e Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:55 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 50282ce3-1fdc-4fa5-8467-4cd4867a8395 Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:56 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 837d01bc-7b0c-4532-88d2-1d91671d74f3 Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:837d01bc-7b0c-4532-88d2-1d91671d74f3
Jun 13, 2023 8:49:57 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 43ed7181-5b5d-46e3-b7d2-f3cd2decb043 Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:43ed7181-5b5d-46e3-b7d2-f3cd2decb043
Jun 13, 2023 8:49:58 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: f5a54844-ef86-4675-9b39-ede75733686b Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:f5a54844-ef86-4675-9b39-ede75733686b
Jun 13, 2023 8:49:59 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 82da197b-0c48-4cb1-9a0b-e5dbfa27c616 Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:82da197b-0c48-4cb1-9a0b-e5dbfa27c616
Jun 13, 2023 8:50:00 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: df9a8372-08cb-4686-85de-2a5ca5aabe52 Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:df9a8372-08cb-4686-85de-2a5ca5aabe52
{code}

https://github.com/apache/flink-connector-jdbc/actions/runs/5253247136/jobs/9490321045#step:13:322",martijnvisser,martijnvisser,Critical,Closed,Fixed,13/Jun/23 09:40,15/Jun/23 11:37
Bug,FLINK-32334,13540002,Operator failed to create taskmanager deployment because it already exist,"During a job upgrade the operator has failed to start the new job because it has failed to create the taskmanager deployment:

 
{code:java}
Jun 12 19:45:28.115 >>> Status | Error | UPGRADING | {""type"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException"",""message"":""org.apache.flink.client.deployment.ClusterDeploymentException: Could not create Kubernetes cluster \""flink-metering\""."",""throwableList"":[{""type"":""org.apache.flink.client.deployment.ClusterDeploymentException"",""message"":""Could not create Kubernetes cluster \""flink-metering\"".""},{""type"":""org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException"",""message"":""Failure executing: POST at: https://10.129.144.1/apis/apps/v1/namespaces/metering/deployments. Message: object is being deleted: deployments.apps \""flink-metering-taskmanager\"" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=apps, kind=deployments, name=flink-metering-taskmanager, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=object is being deleted: deployments.apps \""flink-metering-taskmanager\"" already exists, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).""}]} {code}
As indicated in the error log this is due to taskmanger deployment still existing while it is under deletion.

Looking at the [source code|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/StandaloneFlinkService.java#L150] we are well relying on FOREGROUND policy by default.

Still it seems that the REST API call to delete only wait until the resource has been modified and the {{deletionTimestamp}} has been added to the metadata: [ensure delete returns only when the delete operation is fully finished -  Issue #3246 -  fabric8io/kubernetes-client|https://github.com/fabric8io/kubernetes-client/issues/3246#issuecomment-874019899]

So we could face this issue if the k8s cluster is slow to ""really"" delete the deployment

 ",nfraison.datadog,nfraison.datadog,Major,Closed,Fixed,14/Jun/23 07:57,19/Jun/23 09:58
Bug,FLINK-32337,13540035,SQL array_union could return wrong result,"This is was mentioned at [https://github.com/apache/flink/pull/22717#issuecomment-1587333488]

 how to reproduce
{code:sql}
SELECT array_union(ARRAY[CAST(NULL AS INT)], ARRAY[1]); -- returns [NULL, 1], this is OK
SELECT array_union(ARRAY[1], ARRAY[CAST(NULL AS INT)]); -- returns [1, 0], this is NOT OK
{code}",Sergey Nuyanzin,Sergey Nuyanzin,Major,Closed,Fixed,14/Jun/23 11:11,15/Jun/23 22:43
Bug,FLINK-32347,13540172,Exceptions from the CompletedCheckpointStore are not registered by the CheckpointFailureManager ,"Currently if an error occurs while saving a completed checkpoint in the {_}CompletedCheckpointStore{_}, _CheckpointCoordinator_ doesn't call _CheckpointFailureManager_ to handle the error. Such behavior leads to the fact, that errors from _CompletedCheckpointStore_ don't increase the failed checkpoints count and _'execution.checkpointing.tolerable-failed-checkpoints'_ option does not limit the number of errors of this kind in any way.

Possible solution may be to move the notification of _CheckpointFailureManager_ about successful checkpoint after storing completed checkpoint in the _CompletedCheckpointStore_ and providing the exception to the _CheckpointFailureManager_ in the {_}CheckpointCoordinator#{_}{_}[addCompletedCheckpointToStoreAndSubsumeOldest()|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L1440]{_} method.",srichter,vivacell,Major,Closed,Fixed,15/Jun/23 09:05,27/Jun/23 13:12
Bug,FLINK-32348,13540181,MongoDB tests are flaky and time out,https://github.com/apache/flink-connector-mongodb/actions/runs/5232649632/jobs/9447519651#step:13:39307,jiabao.sun,martijnvisser,Critical,Resolved,Fixed,15/Jun/23 09:47,12/Jul/23 09:13
Bug,FLINK-32357,13540212,Elasticsearch v3.0 won't compile when testing against Flink 1.17.1,"{code:java}
[INFO] ------------------------------------------------------------------------
Error:  Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-connector-elasticsearch-base: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test failed: org.junit.platform.commons.JUnitException: TestEngine with ID 'archunit' failed to discover tests: com.tngtech.archunit.lang.syntax.elements.MethodsThat.areAnnotatedWith(Ljava/lang/Class;)Ljava/lang/Object; -> [Help 1]
{code}

https://github.com/apache/flink-connector-elasticsearch/actions/runs/5277721611/jobs/9546112876#step:13:159

",reta,martijnvisser,Major,Closed,Fixed,15/Jun/23 11:48,19/Jun/23 11:58
Bug,FLINK-32368,13540393,KubernetesTestFixture doesn't implement the checkAndUpdateConfigMapFunction properly,"[FlinkKubeClient.checkAndUpdateConfigMap|https://github.com/apache/flink/blob/ab3eb40d920fa609f49164a0bbb5fcbb3004a808/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/FlinkKubeClient.java#L163] expects an error to be forwarded through the {{CompletableFuture}} instead of throwing a {{RuntimeException}}.

The actual implementation implements it accordingly in [Fabric8FlinkKubeClient:313|https://github.com/apache/flink/blob/025a95b627faf8ec8b725a7784d1279b41e10ba7/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/Fabric8FlinkKubeClient.java#L313] where a {{CompletionException}} is thrown within the {{CompletableFuture}}'s {{supplyAsync}} call resulting the future to fail.

{{KubernetesTestFixture}} doesn't make the returned future complete exceptionally but throws a {{CompletionException}} (see [KubernetesTestFixture:172|https://github.com/apache/flink/blob/6fc5f789869433688eb5f62494f1a4404e0dd11b/flink-kubernetes/src/test/java/org/apache/flink/kubernetes/highavailability/KubernetesTestFixture.java#L172]).

This results in inconsistent test behavior.",mapohl,mapohl,Major,Resolved,Fixed,16/Jun/23 12:31,19/Jun/23 06:46
Bug,FLINK-32374,13540540,ExecNodeGraphInternalPlan#writeToFile should support TRUNCATE_EXISTING for overwriting,"If the existing JSON plan is not truncated when overwriting, and the newly generated JSON plan contents are shorter than the previous JSON plan content, the plan be an invalid JSON.
h4. How to reproduce
{code:sql}
Flink SQL> create table debug_sink (f0 int, f1 string) with ('connector' = 'blackhole');
[INFO] Execute statement succeed.

Flink SQL> create table dummy_source (f0 int, f1 int, f2 string, f3 string) with ('connector' = 'datagen');
[INFO] Execute statement succeed.

Flink SQL> compile plan '/foo/bar/debug.json' for insert into debug_sink select if(f0 > f1, f0, f1) as f0, concat(f2, f3) as f1 from dummy_source;
[INFO] Execute statement succeed.

Flink SQL> set 'table.plan.force-recompile' = 'true';
[INFO] Execute statement succeed.

Flink SQL> compile plan '/foo/bar/debug.json' for insert into debug_sink select * from (values (2, 'bye')) T (id, message);
[INFO] Execute statement succeed.
{code}
cat -n debug.json, and check L#67
{code:json}
     1	{
     2	  ""flinkVersion"" : ""1.17"",
     3	  ""nodes"" : [ {
     4	    ""id"" : 15,
     5	    ""type"" : ""stream-exec-values_1"",
     6	    ""tuples"" : [ [ {
     7	      ""kind"" : ""LITERAL"",
     8	      ""value"" : ""2"",
     9	      ""type"" : ""INT NOT NULL""
    10	    }, {
    11	      ""kind"" : ""LITERAL"",
    12	      ""value"" : ""bye"",
    13	      ""type"" : ""CHAR(3) NOT NULL""
    14	    } ] ],
    15	    ""outputType"" : ""ROW<`id` INT NOT NULL, `message` CHAR(3) NOT NULL>"",
    16	    ""description"" : ""Values(tuples=[[{ 2, _UTF-16LE'bye' }]])"",
    17	    ""inputProperties"" : [ ]
    18	  }, {
    19	    ""id"" : 16,
    20	    ""type"" : ""stream-exec-sink_1"",
    21	    ""configuration"" : {
    22	      ""table.exec.sink.keyed-shuffle"" : ""AUTO"",
    23	      ""table.exec.sink.not-null-enforcer"" : ""ERROR"",
    24	      ""table.exec.sink.type-length-enforcer"" : ""IGNORE"",
    25	      ""table.exec.sink.upsert-materialize"" : ""AUTO""
    26	    },
    27	    ""dynamicTableSink"" : {
    28	      ""table"" : {
    29	        ""identifier"" : ""`default_catalog`.`default_database`.`debug_sink`"",
    30	        ""resolvedTable"" : {
    31	          ""schema"" : {
    32	            ""columns"" : [ {
    33	              ""name"" : ""f0"",
    34	              ""dataType"" : ""INT""
    35	            }, {
    36	              ""name"" : ""f1"",
    37	              ""dataType"" : ""VARCHAR(2147483647)""
    38	            } ],
    39	            ""watermarkSpecs"" : [ ]
    40	          },
    41	          ""partitionKeys"" : [ ],
    42	          ""options"" : {
    43	            ""connector"" : ""blackhole""
    44	          }
    45	        }
    46	      }
    47	    },
    48	    ""inputChangelogMode"" : [ ""INSERT"" ],
    49	    ""inputProperties"" : [ {
    50	      ""requiredDistribution"" : {
    51	        ""type"" : ""UNKNOWN""
    52	      },
    53	      ""damBehavior"" : ""PIPELINED"",
    54	      ""priority"" : 0
    55	    } ],
    56	    ""outputType"" : ""ROW<`id` INT NOT NULL, `message` CHAR(3) NOT NULL>"",
    57	    ""description"" : ""Sink(table=[default_catalog.default_database.debug_sink], fields=[id, message])""
    58	  } ],
    59	  ""edges"" : [ {
    60	    ""source"" : 15,
    61	    ""target"" : 16,
    62	    ""shuffle"" : {
    63	      ""type"" : ""FORWARD""
    64	    },
    65	    ""shuffleMode"" : ""PIPELINED""
    66	  } ]
    67	} ""$CONCAT$1"",
    68	      ""operands"" : [ {
    69	        ""kind"" : ""INPUT_REF"",
    70	        ""inputIndex"" : 2,
    71	        ""type"" : ""VARCHAR(2147483647)""
    72	      }, {
    73	        ""kind"" : ""INPUT_REF"",
    74	        ""inputIndex"" : 3,
    75	        ""type"" : ""VARCHAR(2147483647)""
    76	      } ],
    77	      ""type"" : ""VARCHAR(2147483647)""
    78	    } ],
    79	    ""condition"" : null,
    80	    ""inputProperties"" : [ {
    81	      ""requiredDistribution"" : {
    82	        ""type"" : ""UNKNOWN""
    83	      },
    84	      ""damBehavior"" : ""PIPELINED"",
    85	      ""priority"" : 0
    86	    } ],
    87	    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
    88	    ""description"" : ""Calc(select=[IF((f0 > f1), f0, f1) AS f0, CONCAT(f2, f3) AS f1])""
    89	  }, {
    90	    ""id"" : 14,
    91	    ""type"" : ""stream-exec-sink_1"",
    92	    ""configuration"" : {
    93	      ""table.exec.sink.keyed-shuffle"" : ""AUTO"",
    94	      ""table.exec.sink.not-null-enforcer"" : ""ERROR"",
    95	      ""table.exec.sink.type-length-enforcer"" : ""IGNORE"",
    96	      ""table.exec.sink.upsert-materialize"" : ""AUTO""
    97	    },
    98	    ""dynamicTableSink"" : {
    99	      ""table"" : {
   100	        ""identifier"" : ""`default_catalog`.`default_database`.`debug_sink`"",
   101	        ""resolvedTable"" : {
   102	          ""schema"" : {
   103	            ""columns"" : [ {
   104	              ""name"" : ""f0"",
   105	              ""dataType"" : ""INT""
   106	            }, {
   107	              ""name"" : ""f1"",
   108	              ""dataType"" : ""VARCHAR(2147483647)""
   109	            } ],
   110	            ""watermarkSpecs"" : [ ]
   111	          },
   112	          ""partitionKeys"" : [ ],
   113	          ""options"" : {
   114	            ""connector"" : ""blackhole""
   115	          }
   116	        }
   117	      }
   118	    },
   119	    ""inputChangelogMode"" : [ ""INSERT"" ],
   120	    ""inputProperties"" : [ {
   121	      ""requiredDistribution"" : {
   122	        ""type"" : ""UNKNOWN""
   123	      },
   124	      ""damBehavior"" : ""PIPELINED"",
   125	      ""priority"" : 0
   126	    } ],
   127	    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
   128	    ""description"" : ""Sink(table=[default_catalog.default_database.debug_sink], fields=[f0, f1])""
   129	  } ],
   130	  ""edges"" : [ {
   131	    ""source"" : 12,
   132	    ""target"" : 13,
   133	    ""shuffle"" : {
   134	      ""type"" : ""FORWARD""
   135	    },
   136	    ""shuffleMode"" : ""PIPELINED""
   137	  }, {
   138	    ""source"" : 13,
   139	    ""target"" : 14,
   140	    ""shuffle"" : {
   141	      ""type"" : ""FORWARD""
   142	    },
   143	    ""shuffleMode"" : ""PIPELINED""
   144	  } ]
   145	}
{code}",qingyue,qingyue,Major,Closed,Fixed,19/Jun/23 01:25,20/Jun/23 01:49
Bug,FLINK-32392,13540732,Several jobs failed on AZP with No space left on device,"This Build failed with no space left https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50162&view=logs&j=585d8b77-fa33-51bc-8163-03e54ba9ce5b
{noformat}
##[error]Unhandled exception. System.IO.IOException: No space left on device : '/home/vsts/agents/3.220.5/_diag/Worker_20230619-021757-utc.log'
   at System.IO.RandomAccess.WriteAtOffset(SafeFileHandle handle, ReadOnlySpan`1 buffer, Int64 fileOffset)
   at System.IO.Strategies.BufferedFileStreamStrategy.FlushWrite()
   at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder)
   at System.Diagnostics.TextWriterTraceListener.Flush()
   at Microsoft.VisualStudio.Services.Agent.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id) in /home/vsts/work/1/s/src/Microsoft.VisualStudio.Services.Agent/HostTraceListener.cs:line 151
   at Microsoft.VisualStudio.Services.Agent.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message) in /home/vsts/work/1/s/src/Microsoft.VisualStudio.Services.Agent/HostTraceListener.cs:line 81
   at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message)
   at Microsoft.VisualStudio.Services.Agent.Util.ProcessInvoker.ProcessExitedHandler(Object sender, EventArgs e) in /home/vsts/work/1/s/src/Agent.Sdk/ProcessInvoker.cs:line 496
   at System.Diagnostics.Process.OnExited()
   at System.Diagnostics.Process.RaiseOnExited()
   at System.Diagnostics.Process.CompletionCallback(Object waitHandleContext, Boolean wasSignaled)
   at System.Threading._ThreadPoolWaitOrTimerCallback.WaitOrTimerCallback_Context_f(Object state)
   at System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state)
--- End of stack trace from previous location ---
   at System.Threading._ThreadPoolWaitOrTimerCallback.PerformWaitOrTimerCallback(_ThreadPoolWaitOrTimerCallback helper, Boolean timedOut)
   at System.Threading.PortableThreadPool.CompleteWait(RegisteredWaitHandle handle, Boolean timedOut)
   at System.Threading.ThreadPoolWorkQueue.Dispatch()
   at System.Threading.PortableThreadPool.WorkerThread.WorkerThreadStart()
,##[error]The hosted runner encountered an error while running your job. (Error Type: Failure).
{noformat}
for 1.16, 1.17 it happens while   'Upload artifacts to S3'
for 1.18 while 'Deploy maven snapshot'",renqs,Sergey Nuyanzin,Critical,Resolved,Fixed,20/Jun/23 10:00,28/Jun/23 06:10
Bug,FLINK-32408,13540894,JobManager HA configuration update needed in Flink k8s Operator ,"In flink 1.17 documentation it says, to configure job manger ha we have to configure *high-availability.type* key not *high-availability* key{*}.{*} (It seems to be changed from 1.17)

And currently kubernetes-operator-1.5.0 says it supports flink 1.17 version. 
So I expected that configuring job manager ha with *high-availability.type* should work but it didn't, only *high-availability* works

*ref*
[https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/config/#high-availability] 
[https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-release-1.5/docs/concepts/overview/#core] ",,dongwoo.kim,Minor,Closed,Fixed,21/Jun/23 13:09,21/Jun/23 14:54
Bug,FLINK-32412,13541002,JobID collisions in FlinkSessionJob,"From time to time we see {{JobId}} collisions in our deployments due to the low entropy of the generated {{{}JobId{}}}. The problem is that, although the {{uid}} from the k8s-resource (which is a UUID V4), only the {{hashCode}} of it will be used for the {{{}JobId{}}}. The {{hashCode}} is an integer, thus 32 bits. If we look at the birthday problem theorem we can expect a collision with a 50% chance with only 77000 random integers. 

In reality we seem to see the problem more often, but this could be because the {{uid}} might not be completely random, therefore increasing the chances if we just use parts of it.

We propose to at least use the complete 64 bits of the upper part of the {{{}JobId{}}}, where 5.1×10{^}9{^} IDs are needed for a collision chance of 50%. We could even argue that most probably 64 bit for the generation number is not needed and another 32 bit could be spent on the uid to increase the entropy of the {{JobId}} even further (This would mean the max generation would be 4,294,967,295).

Our suggestion for using 64 bits would be:
{code:java}
new JobID(
    UUID.fromString(Preconditions.checkNotNull(uid)).getMostSignificantBits(), 
    Preconditions.checkNotNull(generation)
);
{code}
Any thoughts on this? I would create a PR once we know how to proceed.",fabiowanner,fabiowanner,Major,Closed,Fixed,22/Jun/23 09:30,26/Jun/23 14:17
Bug,FLINK-32421,13541194,EmbeddedLeaderServiceTest.testConcurrentRevokeLeadershipAndShutdown is not properly implemented,"The purpose of {{EmbeddedLeaderServiceTest.testConcurrentRevokeLeadershipAndShutdown}} is to check that there is no {{NullPointerException}} happening if the event processing happens after the shutdown of the {{EmbeddedExecutorService}} (see FLINK-11855).

But the concurrent execution is not handled properly. The test also succeeds if the close call happened before the shutdown (due to the multi-threaded nature of the test) which leaves us without the actual test scenario being tested.",mapohl,mapohl,Major,Resolved,Fixed,23/Jun/23 15:51,05/Jul/23 15:52
Bug,FLINK-32425,13541294,Fix Opensearch Connector wrong empty doc link,"There is an empty link(""see here for further information"") in https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/datastream/opensearch/. And we should fix this.

The link should be like this (""See how to link with it for cluster execution here."") in https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/opensearch/
",tanyuxin,tanyuxin,Minor,Closed,Fixed,25/Jun/23 09:21,26/Jun/23 09:21
Bug,FLINK-32426,13541296,Fix adaptive local hash agg can't work when auxGrouping exist,"For the following case, the field `a` is primary key,  we select from `AuxGroupingTable` and group by a, b. Since a is primary key, it also guarantee the unique, so planner will extract b as auxGrouping field.
{code:java}
registerCollection(
  ""AuxGroupingTable"",
  data2,
  type2,
  ""a, b, c, d, e"",
  nullablesOfData2,
  FlinkStatistic.builder().uniqueKeys(Set(Set(""a"").asJava).asJava).build())

checkResult(
  ""SELECT a, b, COUNT(c) FROM AuxGroupingTable GROUP BY a, b"",
  Seq(
    row(1, 1, 1),
    row(2, 3, 2),
    row(3, 4, 3),
    row(4, 10, 4),
    row(5, 11, 5)
  )
) {code}
 

Due to the generated code doesn't get auxGrouping fields from input RowData and then setting it to aggBuffer, the aggBuffer RowData loses some fields, and it will throw an index Exception when get the field from it. As following:
{code:java}
Caused by: java.lang.AssertionError: index (1) should < 1
    at org.apache.flink.table.data.binary.BinaryRowData.assertIndexIsValid(BinaryRowData.java:127)
    at org.apache.flink.table.data.binary.BinaryRowData.isNullAt(BinaryRowData.java:156)
    at org.apache.flink.table.data.utils.JoinedRowData.isNullAt(JoinedRowData.java:113)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.toBinaryRow(RowDataSerializer.java:201)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:103)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:48)
    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:165)
    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:43)
    at org.apache.flink.runtime.plugable.SerializationDelegate.write(SerializationDelegate.java:54)
    at org.apache.flink.runtime.io.network.api.writer.RecordWriter.serializeRecord(RecordWriter.java:141)
    at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:107)
    at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:55)
    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:134)
    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collectAndCheckIfChained(RecordWriterOutput.java:114)
    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:95)
    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:48)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:59)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:31)
    at LocalHashAggregateWithKeys$39.processElement_split2(Unknown Source)
    at LocalHashAggregateWithKeys$39.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
    at BatchExecCalc$10.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
    at SourceConversion$6.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
    at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333) {code}",lsy,lsy,Critical,Closed,Fixed,25/Jun/23 09:59,29/Jun/23 12:27
Bug,FLINK-32441,13541480,DefaultSchedulerTest#testTriggerCheckpointAndCompletedAfterStore fails with timeout on AZP,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50461&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9274

fails with timeout on {{DefaultSchedulerTest#testTriggerCheckpointAndCompletedAfterStore}}

",srichter,Sergey Nuyanzin,Critical,Closed,Fixed,26/Jun/23 22:21,27/Jun/23 13:08
Bug,FLINK-32446,13541560,MongoWriter should regularly check whether the last write time is greater than the specified time.,"Mongo sink waits for new record to write previous records. I have a upsert-kafka topic filled that has already some events. I start a new upsert-kafka to mongo db sink job. I expect all the data from the topic to be loaded to mongodb right away. But instead, only the first record is written to mongo db. The rest of the records don’t arrive in mongodb until a new event is written to kafka topic. The new event that was written is delayed until the next event arrives. 

To prevent this problem, the MongoWriter should regularly check whether the last write time is greater than the specified time.",jiabao.sun,jiabao.sun,Major,Resolved,Fixed,27/Jun/23 12:09,28/Jun/23 06:08
Bug,FLINK-32447,13541582,table hints lost when they inside a view referenced by an external query,"Table hints will lost when they inside a view referenced by an external query, this is due to the upgrading of calcite-1.28 (affected by CALCITE-4640 which changed the default implementation of SqlDialect suppresses all table hints).
This can be reproduced by adding a new case to current {code}OptionsHintTest{code}:
{code}
+
+  @Test
+  def testOptionsHintInsideView(): Unit = {
+    util.tableEnv.executeSql(
+      ""create view v1 as select * from t1 /*+ OPTIONS(k1='#v111', k4='#v444')*/"")
+    util.verifyExecPlan(s""""""
+                           |select * from t2 join v1 on v1.a = t2.d
+                           |"""""".stripMargin)
+  }
{code}
wrong plan which lost table hints(dynamic options):
{code}
Join(joinType=[InnerJoin], where=[(a = d)], select=[d, e, f, a, b, c], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
:- Exchange(distribution=[hash[d]])
:  +- LegacyTableSourceScan(table=[[default_catalog, default_database, t2, source: [OptionsTableSource(props={k3=v3, k4=v4})]]], fields=[d, e, f])
+- Exchange(distribution=[hash[a]])
   +- Calc(select=[a, b, (a + 1) AS c])
      +- LegacyTableSourceScan(table=[[default_catalog, default_database, t1, source: [OptionsTableSource(props={k1=v1, k2=v2})]]], fields=[a, b])
{code}

We should use {code}AnsiSqlDialect{code} instead to reserve table hints.",lincoln.86xy,lincoln.86xy,Major,Closed,Fixed,27/Jun/23 14:23,29/Jun/23 04:26
Bug,FLINK-32448,13541612,Connector Shared Utils checks out wrong branch when running CI for PRs,"Since FLINK-31923, when a branch is not specified, all CI runs use {{main}} as the default branch when none is specified. This doesn't work when submitting a PR, since it shouldn't use {{main}} but it should use the specific ref that triggered that workflow. ",martijnvisser,martijnvisser,Blocker,Closed,Fixed,27/Jun/23 17:58,27/Jun/23 18:53
Bug,FLINK-32453,13541653,flink-connector-kafka does not build against Flink 1.18-SNAPSHOT,"There are a few breaking changes in test utility code that prevents {{apache/flink-connector-kafka}} from building against Flink 1.18-SNAPSHOT. This umbrella ticket captures all breaking changes, and should only be closed once we make things build again.",tzulitai,tzulitai,Blocker,Closed,Fixed,28/Jun/23 02:25,12/Jul/23 12:51
Bug,FLINK-32461,13541685,manage  union operator state increase very large in Jobmanager ,"This issue doesn't usually occur, but it happens during busy nights when the machines are more active. The ""manage operator state"" will increase significantly, and I found the number of  operator union state object is 128 ,same with the parallelism .Whether the union state only needs to be loaded once?

 !screenshot-1.png! 
 !image-2023-06-28-16-24-11-538.png! 
",,1026688210,Major,Closed,Fixed,28/Jun/23 08:06,02/Jul/23 02:51
Bug,FLINK-32465,13541745,KerberosLoginProvider.isLoginPossible does accidental login with keytab,"In KerberosLoginProvider.isLoginPossible there is a call to UserGroupInformation.getCurrentUser() before principal check (keytab usage). This triggers an accidental login with either kerberos credentials if available, or as the local OS user, based on security settings. This is not problematic most of the time since KerberosLoginProvider.doLogin overwrites the credentials with keytab. The problem hurts however when login fails for whatever reason. Such case the workload is just not starting.",gaborgsomogyi,gaborgsomogyi,Major,Closed,Fixed,28/Jun/23 09:30,28/Jun/23 15:51
Bug,FLINK-32466,13541748,Invalid input strategy for many functions which allows BINARY strings,"""string"" in SQL terms covers both character strings and binary strings. The author of CONCAT might not have known this. In any case, the code gen instead of the validator fails when executing:

{code}
TableEnvironment t = TableEnvironment.create(EnvironmentSettings.inStreamingMode());
t.createTemporaryView(""t"", t.fromValues(lit(new byte[] {97})));
t.executeSql(""SELECT CONCAT(f0, '-magic') FROM t"").print();
{code}

As future work, we should also allow binary strings.",twalthr,twalthr,Major,Closed,Fixed,28/Jun/23 09:35,10/Jul/23 17:12
Bug,FLINK-32478,13541888,SourceCoordinatorAlignmentTest.testAnnounceCombinedWatermarkWithoutStart fails,"SourceCoordinatorAlignmentTest.testAnnounceCombinedWatermarkWithoutStart fails

 

Root cause: multiple sources share the same thread pool, and the second source cannot start due to the first source closes the shared thread pool.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50611&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8613",fanrui,fanrui,Blocker,Closed,Fixed,29/Jun/23 06:14,30/Jun/23 12:16
Bug,FLINK-32490,13541970,ArrayElementOutputTypeStrategyTest>TypeStrategiesTestBase.testTypeStrategy constantly fails,"ArrayElementOutputTypeStrategyTest fails with
{noformat}
[ERROR] Failures: 
[ERROR]   ArrayElementOutputTypeStrategyTest>TypeStrategiesTestBase.testTypeStrategy:58 
expected: ""INT NOT NULL (AtomicDataType@26e4eacd)""
 but was: ""INT NOT NULL (AtomicDataType@1716b369)""
[ERROR]   ArrayElementOutputTypeStrategyTest>TypeStrategiesTestBase.testTypeStrategy:58 
expected: ""INT (AtomicDataType@18ab74e7)""
 but was: ""INT (AtomicDataType@f0704a2)""
{noformat}
 

also could be reproduced locally",,Sergey Nuyanzin,Blocker,Resolved,Fixed,29/Jun/23 16:04,29/Jun/23 16:11
Bug,CASSANDRA-18131,13516372,LongBTreeTest times out after btree improvements from CASSANDRA-15510,"Happening in both ci-cassandra.a.o and circleci.

LongBTreeTest is timing out on 4.0, 4.1, trunk branches.

Started back in mid April (https://github.com/apache/cassandra/commit/018c8e0d5e and https://github.com/apache/cassandra/commit/596daeb7f08). 

Nightlies shows when the failures started, evident by the 'jdk=jdk_1.8_latest,label=cassandra,split=7/' subfolder missing in the following… 
- https://nightlies.apache.org/cassandra/trunk/Cassandra-trunk-test-burn/1254/Cassandra-trunk-test-burn/
- https://nightlies.apache.org/cassandra/cassandra-4.0/Cassandra-4.0-test-burn/343/",mmuzaf,mck,Normal,Resolved,Fixed,03/Jan/23 19:45,03/Apr/23 13:35
Bug,CASSANDRA-18136,13516715,Upgrade maven-shade-plugin to fix shaded dtest JAR build,"Could not build shaded dtest JAR with ./build-shaded-dtest-jar.sh due to:
{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.2.1:shade (default) on project cassandra-dtest-shaded: Error creating shaded jar: Problem shading JAR ~/Repos/apache/cassandra/target/cassandra-dtest-shaded-4.0.1-SNAPSHOT.jar entry net/openhft/chronicle/wire/YamlWire$TextValueIn.class: org.apache.maven.plugin.MojoExecutionException: Error in ASM processing class net/openhft/chronicle/wire/YamlWire$TextValueIn.class: 65536 -> [Help 1] {code}
 

Tried on both Java 8 and Java 11, included ant clean / realclean / unlinking the entire ~/.m2/repository.

 

Fixed by upgrading maven-shade-plugin in relocate-dependencies.pom:
{code:java}
                 <groupId>org.apache.maven.plugins</groupId>
                 <artifactId>maven-shade-plugin</artifactId>
-                <version>3.2.1</version>
+                <version>3.4.1</version>{code}",,aratnofsky,Normal,Resolved,Fixed,05/Jan/23 19:29,14/Mar/23 21:00
Bug,CASSANDRA-18140,13517286,getsstables --show-levels JMX serialization error,While the interface is compliant and tested by JMXStandardsTest the implementation is not actually serializable: {{java.io.NotSerializableException: com.google.common.collect.AbstractMapBasedMultimap$AsMap}},jwest,jwest,Normal,Resolved,Fixed,09/Jan/23 19:38,11/Jan/23 23:44
Bug,CASSANDRA-18141,13517448,Cqlsh incorrectly formats duration,"It looks like something broke between C* 3.11 and C* 4.X when it comes to duration types.

Example:

CREATE KEYSPACE users WITH replication = \{'class': 'NetworkTopologyStrategy', 'datacenter1': '1'} AND durable_writes = true;
CREATE TABLE users.user_credentials_by_email (email text,la_duration duration,PRIMARY KEY(email));
INSERT INTO users.user_credentials_by_email (email, la_duration ) VALUES ( 'test0@test.com', 12h);
INSERT INTO users.user_credentials_by_email (email, la_duration ) VALUES ( 'test1@test.com', 12h30m);
INSERT INTO users.user_credentials_by_email (email, la_duration ) VALUES ( 'test2@test.com', 12h30m30s);
INSERT INTO users.user_credentials_by_email (email, la_duration ) VALUES ( 'test3@test.com', 12h30m30s250ms);
INSERT INTO users.user_credentials_by_email (email, la_duration ) VALUES ( 'test4@test.com', PT12H30M);

3.11:

cassandra@cqlsh> SHOW VERSION ;
[cqlsh 5.0.1 | Cassandra 3.11.15-SNAPSHOT | CQL spec 3.4.4 | Native protocol v4]
cassandra@cqlsh> SELECT * FROM users.user_credentials_by_email ;

 email          | la_duration
----------------+----------------
 test0@test.com |            12h
 test3@test.com | 12h30m30s250ms
 test4@test.com |         12h30m
 test1@test.com |         12h30m
 test2@test.com |      12h30m30s

(5 rows)

4.X:

cassandra@cqlsh> SHOW VERSION ;
[cqlsh 6.0.0 | Cassandra 4.0.8-SNAPSHOT | CQL spec 3.4.5 | Native protocol v5]
cassandra@cqlsh> SELECT * FROM users.user_credentials_by_email ;

 email          | la_duration
----------------+-----------------------------------------------------
 test0@test.com |                                               12.0h
 test3@test.com | 12.508402777777778h30.504166666666666m30.25s250.0ms
 test4@test.com |                                          12.5h30.0m
 test1@test.com |                                          12.5h30.0m
 test2@test.com |                       12.508333333333333h30.5m30.0s

(5 rows)",masokol,masokol,Normal,Resolved,Fixed,10/Jan/23 12:34,30/Jan/23 12:47
Bug,CASSANDRA-18143,13517582,upgradesstables does not always upgrade tables in proper order.,"The SSTableUpgrader accepts tools in the hash order provided by Directories.SSTableLister rather than ordering them to ensure that they are upgraded in the proper order.

They should be ordered by their id. The comparator for SSTableId is available in SSTableIdFactory.COMPARATOR. 
 
Dev discussion thread: https://lists.apache.org/thread/w6pm5hbdxt295mtvlckv0joyk8x4o8nf",claude,claude,Normal,Resolved,Fixed,11/Jan/23 09:54,14/Mar/23 14:33
Bug,CASSANDRA-18144,13517670,org.apache.cassandra.db.compaction.CompactionStrategyManagerBoundaryReloadTest.testReload fails when running with TrieMemtables,"https://app.circleci.com/pipelines/github/dcapwell/cassandra/1771/workflows/1bd920c8-8568-44b3-9e8b-b152a73cf4fc/jobs/15393

{code}
java.lang.RuntimeException: Error setting schema for test (query was: alter table cql_test_keyspace.table_01 with compaction = {'class': 'SizeTieredCompactionStrategy', 'enabled': false})
	at org.apache.cassandra.cql3.CQLTester.schemaChange(CQLTester.java:1222)
	at org.apache.cassandra.cql3.CQLTester.alterTable(CQLTester.java:1009)
	at org.apache.cassandra.db.compaction.CompactionStrategyManagerBoundaryReloadTest.testReload(CompactionStrategyManagerBoundaryReloadTest.java:82)
Caused by: java.lang.ClassCastException: org.apache.cassandra.dht.ByteOrderedPartitioner$BytesToken cannot be cast to org.apache.cassandra.dht.Murmur3Partitioner$LongToken
	at org.apache.cassandra.dht.Murmur3Partitioner$1.valueForToken(Murmur3Partitioner.java:68)
	at org.apache.cassandra.dht.Splitter$WeightedRange.totalTokens(Splitter.java:278)
	at org.apache.cassandra.dht.Splitter.splitOwnedRanges(Splitter.java:129)
	at org.apache.cassandra.db.ColumnFamilyStore.localRangeSplits(ColumnFamilyStore.java:1504)
	at org.apache.cassandra.db.memtable.AbstractShardedMemtable.<init>(AbstractShardedMemtable.java:65)
	at org.apache.cassandra.db.memtable.TrieMemtable.<init>(TrieMemtable.java:142)
	at org.apache.cassandra.db.memtable.TrieMemtable$Factory.create(TrieMemtable.java:688)
	at org.apache.cassandra.db.ColumnFamilyStore.createMemtable(ColumnFamilyStore.java:1375)
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.<init>(ColumnFamilyStore.java:1173)
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.<init>(ColumnFamilyStore.java:1137)
	at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:1000)
	at org.apache.cassandra.db.ColumnFamilyStore.switchMemtableIfCurrent(ColumnFamilyStore.java:981)
	at org.apache.cassandra.db.ColumnFamilyStore.switchMemtableOrNotify(ColumnFamilyStore.java:966)
	at org.apache.cassandra.db.ColumnFamilyStore.reload(ColumnFamilyStore.java:393)
{code}

First reported to slack: https://the-asf.slack.com/archives/CK23JSY2K/p1673382016638189",kamalesh0420,dcapwell,Normal,Resolved,Fixed,11/Jan/23 19:13,06/Mar/23 21:04
Bug,CASSANDRA-18146,13517682,commons-cli vulnerability: CVE-2021-37533,"This CVE is being reported by the OWASP scan for:

commons-cli-1.1.jar: CVE-2021-37533
commons-codec-1.9.jar: CVE-2021-37533
commons-math3-3.2.jar: CVE-2021-37533

additionally commons-lang3-3.1.jar is also reported on 3.x.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,11/Jan/23 20:47,17/Jan/23 16:02
Bug,CASSANDRA-18147,13517683,netty-all vulnerability: CVE-2022-41915,This is being reported by the OWASP scan.,brandon.williams,brandon.williams,Normal,Resolved,Fixed,11/Jan/23 20:55,30/Jan/23 13:32
Bug,CASSANDRA-18148,13517684,netty-all vulnerability: CVE-2022-41881,This is showing in the OWASP scan.,brandon.williams,brandon.williams,Normal,Resolved,Fixed,11/Jan/23 21:02,23/Jan/23 12:59
Bug,CASSANDRA-18149,13517685,"snakeyaml vulnerabilities: CVE-2021-4235, CVE-2022-1471, CVE-2022-3064","The OWASP scan is reporting these for both snakeyaml-1.11 and snakeyaml-1.26.

These are similar to CASSANDRA-17907 in that they require access to the yaml to have any effect.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,11/Jan/23 21:29,24/Jan/23 18:30
Bug,CASSANDRA-18152,13517860,mockito-inline causes tests to fail beacause o.a.c.distributed.mock.nodetool.InternalNodeProbe spies on StorageServiceMBean,"While working on CASSANDRA-14361, when we included mockito-inline into the build to test the new functionality, unrelated tests in CI started to fail. (1)

This is happening because mockito, together with stuff which enables static mocking, just does not play together with our way of doing things in dtest framework.

The workaround is consisting of removing Mockito from InternalNodeProbe, it tries to spy on StorageService to not send any notifications back. This might be workarounded so we do not need Mockito hence tests are fixed and mocking of static methods is possible without any other tests failing.

(1) https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2168/#showFailuresLink

see also: [https://github.com/mockito/mockito/issues/2203]",smiklosovic,smiklosovic,Normal,Resolved,Fixed,12/Jan/23 13:54,17/Jan/23 09:56
Bug,CASSANDRA-18153,13517919,"Memtable being flushed without hostId in version ""me"" and newer during CommitLogReplay","On ticket CASSANDRA-16619 some files were changed to allow Cassandra to store HostID in the new ""me"" SSTable version.

But SSTables flushed during CommitLogReplay miss this HostID info.

 

In the next Cassandra startup, if these SSTables were still present, system.log will show:


{{WARN Origin of 3 sstables is unknown or doesn't match the local node; commitLogIntervals for them were ignored}}

{{WARN }}{{{}Origin of 3 sstables is unknown or doesn't match the local node; commitLogIntervals for them were ignored{}}}{{{}{}}}{{ }}

 

And debug.log will show a list of SSTables, witch can include ""md"" and ""me"" version (before upgradesstables):

 

{{Ignored commitLogIntervals from the following sstables: [/var/lib/cassandra/data/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/me-3-big-Data.db, /var/lib/cassandra/data/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/md-1-big-Data.db, /var/lib/cassandra/data/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/md-2-big-Data.db]}}

 

https://issues.apache.org/jira/browse/CASSANDRA-16619",abonacin,abonacin,Normal,Resolved,Fixed,12/Jan/23 19:43,12/Apr/23 16:10
Bug,CASSANDRA-18156,13519645,Test Failure: repair_tests.deprecated_repair_test.TestDeprecatedRepairNotifications.test_deprecated_repair_error_notification,"Failing since https://ci-cassandra.apache.org/job/Cassandra-3.0/313/testReport/dtest.repair_tests.deprecated_repair_test/TestDeprecatedRepairNotifications which is https://github.com/apache/cassandra/commit/13d495aa7d5b7a7c121fcc9e105f79107c5c2a1c from CASSANDRA-17254 

but can be reproduced earlier in circleci, e.g. https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/64/workflows/f99abc05-aa9b-42de-a424-44ab7c247e76/jobs/3175 

Only two commits are in this time-range (10th to 30th November) in cassandra-dtest
 - https://github.com/apache/cassandra-dtest/commit/7f2b8eda5c52fb6f637aa7166e2d48cd34a64eec (CASSANDRA-17679)
 - https://github.com/apache/cassandra-dtest/commit/e0d3cc90558a17b63634d15ee6df339ceb87b225 (CASSANDRA-15402)",brandon.williams,mck,Normal,Resolved,Fixed,15/Jan/23 09:05,22/Mar/23 10:51
Bug,CASSANDRA-18164,13519659,"Test Failure: CASTest Message serializedSize(12) does not match what was written with serialize(out, 12) for verb PAXOS2_COMMIT_AND_PREPARE_RSP","Flaky. Teardown doesn't look to be clean.

{noformat}
org.apache.cassandra.distributed.shared.ShutdownException: Uncaught exceptions were thrown during test
	at org.apache.cassandra.distributed.impl.AbstractCluster.checkAndResetUncaughtExceptions(AbstractCluster.java:1057)
	at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1043)
	at org.apache.cassandra.distributed.test.CASTest.afterClass(CASTest.java:97)
	Suppressed: java.lang.RuntimeException: java.lang.AssertionError: Message serializedSize(12) does not match what was written with serialize(out, 12) for verb PAXOS2_COMMIT_AND_PREPARE_RSP and serializer class org.apache.cassandra.net.Message$Serializer; expected 1077, actual 1079
		at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:108)
		at org.apache.cassandra.distributed.impl.Instance.lambda$null$6(Instance.java:512)
		at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)
		at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:142)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.lang.Thread.run(Thread.java:750)
	Caused by: java.lang.AssertionError: Message serializedSize(12) does not match what was written with serialize(out, 12) for verb PAXOS2_COMMIT_AND_PREPARE_RSP and serializer class org.apache.cassandra.net.Message$Serializer; expected 1077, actual 1079
		at org.apache.cassandra.distributed.impl.Instance.serializeMessage(Instance.java:422)
		at org.apache.cassandra.distributed.impl.Instance.lambda$registerOutboundFilter$5(Instance.java:362)
		at org.apache.cassandra.net.OutboundSink$Filtered.accept(OutboundSink.java:54)
		at org.apache.cassandra.net.OutboundSink.accept(OutboundSink.java:70)
		at org.apache.cassandra.net.MessagingService.send(MessagingService.java:425)
		at org.apache.cassandra.net.MessagingService.send(MessagingService.java:395)
		at org.apache.cassandra.net.MessagingService.respond(MessagingService.java:407)
		at org.apache.cassandra.service.paxos.PaxosCommitAndPrepare$RequestHandler.doVerb(PaxosCommitAndPrepare.java:128)
		at org.apache.cassandra.net.InboundSink.lambda$new$0(InboundSink.java:78)
		at org.apache.cassandra.net.InboundSink$Filtered.accept(InboundSink.java:64)
		at org.apache.cassandra.net.InboundSink$Filtered.accept(InboundSink.java:50)
		at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:97)
{noformat}",benedict,mck,Normal,Resolved,Fixed,15/Jan/23 11:55,20/Jan/23 10:36
Bug,CASSANDRA-18181,13520255,"Fix tests post JDK-8210522 (rewrite reflection of ""modifiers"" field)"," 

From JDK-8210522:
{code:java}
Core reflection has a filtering mechanism to hide security and integrity sensitive fields and methods from Class getXXXField(s) and getXXXMethod(s). The filtering mechanism has been used for several releases to hide security sensitive fields such as System.security and Class.classLoader.
This CSR proposes to extend the filters to hide fields from a number of highly security sensitive classes in java.lang.reflect and java.lang.invoke.
{code}
We are using at a few places in our tests 
{code:java}
Field.class.getDeclaredField(""modifiers"");{code}
This breaks as expected when tests are run with JDK17, example:
 
{code:java}
java.lang.RuntimeException: java.lang.NoSuchFieldException: modifiers
 at org.apache.cassandra.transport.MessagePayloadTest.makeCqlQueryHandlerAccessible(MessagePayloadTest.java:79)
 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.base/java.lang.reflect.Method.invoke(Method.java:568)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
 at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
 at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
 at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:221)
 at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54) 
Caused by: java.lang.NoSuchFieldException: modifiers at java.base/java.lang.Class.getDeclaredField(Class.java:2610) 
at org.apache.cassandra.transport.MessagePayloadTest.makeCqlQueryHandlerAccessible(MessagePayloadTest.java:70) 
... 15 more{code}
 ",e.dimitrova,e.dimitrova,Normal,Resolved,Fixed,19/Jan/23 23:58,30/Jan/23 22:25
Bug,CASSANDRA-18183,13520578,rat targets do not adhere to build.dir property,"I detected this when I was trying to put my build dir to ramdisk. I have plenty of RAM available on my workstation (64GB) and I was thinking about moving ""build"" dir to ramdisk so I could make it faster a little bit and also spare some write cycles to ssd. It can look like irrelevant improvement but I think that if devs are building the project repeatedly times and times again, this can easily add up.
{code:java}
mkdir /tmp/cassandra
# in /etc/fstab
tmpfs /tmp/cassandra tmpfs defaults,noatime,size=2048M,x-gvfs-show,mode=1777 0 0
# then sudo mount -a
# I have worktree setup so the build for each branch will end up in different dir:
# mkdir -p /tmp/cassandra/{trunk,cassandra-4.1,cassandra-4.0,cassandra-3.11,cassandra-3.0}
{code}
Then in build.properties for each respective branch:
{code:java}
ant.gen-doc.skip: true
build.dir: /tmp/cassandra/trunk/build
build.dir.lib: /tmp/cassandra/trunk/build/lib
{code}
The problem with this is that it fails on rat, because there is not ""build.dir"" property used, it is hardcoded to ""build"" but there is not anything to rat on so it will hang.

To have the very same experience, I am also creating a symlink 
 
{code}
ln -s /tmp/cassandra/trunk/build build
{code}

so ""cd build / ls build"" in the root of the repository will take me to ramdisk. The problem with this is that there is ""build/"" in .gitignore but not ""build"" (as file) so the repository is in dirty state. I suggest to add ""build"" to .gitignore as part of this PR as that is just an opportunistic fix really.",smiklosovic,smiklosovic,Normal,Resolved,Fixed,20/Jan/23 12:12,30/Jan/23 10:38
Bug,CASSANDRA-18184,13520855,'Maximum memory usage reached' chunk cache log message doesn't specify which cache is exhausted,"With Cassandra 4.0.x, we are seeing this cassandra.log message very frequently on the majority of our Cassandra 4.0 clusters:

    _[INFO ] [epollEventLoopGroup-5-3] cluster_id=99 ip_address=127.0.0.50  NoSpamLogger.java:92 - Maximum memory usage reached (128.000MiB), cannot allocate chunk of 8.000MiB_

It took me several weeks to track down what it means, until I saw this start-up message

    _BufferPools.java:49 - Global buffer pool limit is 2.000GiB for chunk-cache and 128.000MiB for networking_

This maximum memory usage warning would benefit from clarifying that its the {*}network cache{*}, not the *off-heap chunk* *cache* which is exhausted.  With 'chunk cache' in both warning messages, they're too easily confused.

 ",yongjiang,bschoeni,Normal,Resolved,Fixed,20/Jan/23 19:28,07/Jul/23 03:36
Bug,CASSANDRA-18188,13521037,Test failure in upgrade_tests.cql_tests.cls.test_limit_ranges,"https://ci-cassandra.apache.org/job/Cassandra-trunk/1434/testReport/dtest-upgrade.upgrade_tests.cql_tests/cls/test_limit_ranges/

{noformat}
self = <abc.TestCQLNodes2RF1_Upgrade_indev_4_1_x_To_indev_trunk object at 0x7f27f9268e10>

    def test_limit_ranges(self):
        """""" Validate LIMIT option for 'range queries' in SELECT statements """"""
        cursor = self.prepare(ordered=True)
    
        cursor.execute(""""""
            CREATE TABLE clicks (
                userid int,
                url text,
                time bigint,
                PRIMARY KEY (userid, url)
            ) WITH COMPACT STORAGE;
        """""")
    
>       for is_upgraded, cursor in self.do_upgrade(cursor):

upgrade_tests/cql_tests.py:318: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
upgrade_tests/upgrade_base.py:197: in do_upgrade
    node1.start(wait_for_binary_proto=True)
../venv/lib/python3.11/site-packages/ccmlib/node.py:896: in start
    node.watch_log_for_alive(self, from_mark=mark)
../venv/lib/python3.11/site-packages/ccmlib/node.py:665: in watch_log_for_alive
    self.watch_log_for(tofind, from_mark=from_mark, timeout=timeout, filename=filename)
../venv/lib/python3.11/site-packages/ccmlib/node.py:584: in watch_log_for
    time.sleep(1)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

title = 'Timeout'
stream = <_io.TextIOWrapper name='<stderr>' mode='w' encoding='utf-8'>
sep = '+'

    def write_title(title, stream=None, sep=""~""):
        """"""Write a section title.
    
        If *stream* is None sys.stderr will be used, *sep* is used to
        draw the line.
        """"""
        if stream is None:
            stream = sys.stderr
>       width = py.io.get_terminal_width()
E       AttributeError: module 'py' has no attribute 'io'

../venv/lib/python3.11/site-packages/pytest_timeout.py:444: AttributeError
{noformat}",brandon.williams,maedhroz,Normal,Resolved,Fixed,23/Jan/23 21:37,24/Jan/23 20:53
Bug,CASSANDRA-18191,13521100,Native Transport SSL tests failing," 
Cassandra 3.0
 

TestNativeTransportSSL.test_connect_to_ssl and TestNativeTransportSSL.test_connect_to_ssl (novnode)

[https://butler.cassandra.apache.org/#/ci/upstream/workflow/Cassandra-3.0/failure/native_transport_ssl_test/TestNativeTransportSSL/test_connect_to_ssl]

As well as 
TestNativeTransportSSL.test_connect_to_ssl_optional and TestNativeTransportSSL.test_connect_to_ssl_optional (nvnode)

[https://butler.cassandra.apache.org/#/ci/upstream/workflow/Cassandra-3.0/failure/native_transport_ssl_test/TestNativeTransportSSL/test_use_custom_ssl_port]


are failing after 2 merges that don't seem like they would impact SSL or server availaibility.

Most recent error message is:

cassandra.cluster.NoHostAvailable: ('Unable to connect to any servers',

{'127.0.0.1:9042': PermissionError(1, ""Tried connecting to [('127.0.0.1', 9042)]. Last error: [SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:992)"")}

)

 

Most recent changes made by 
[@smiklosovic|https://github.com/apache/cassandra/commits?author=smiklosovic] and [@driftx|https://github.com/apache/cassandra/commits?author=driftx] .",brandon.williams,claude,Normal,Resolved,Fixed,24/Jan/23 10:19,24/Jan/23 20:53
Bug,CASSANDRA-18194,13521166,PaxosPrepare may add instances to the Electorate that are not in gossip,"org.apache.cassandra.service.paxos.PaxosPrepare.RequestHandler#execute is given a list of electorate from the peer and attempts to compute its own, then replies back with this set.

On the peer side, we then have the set we sent and the set from the other instance... we then fetch the EndpointState from Gossiper and store into a Map, a map we later attempt to inject into Gossip.

It is possible that Gossiper does not know about the instance yet, so returns a null; causing a NullPointerException in downstream code.",dcapwell,dcapwell,Normal,Resolved,Fixed,24/Jan/23 20:20,08/Feb/23 11:52
Bug,CASSANDRA-18197,13521503,Builds being often aborted because of python3 ./scripts/gen-nodetool-docs.py being slow,"As discussed on CASSANDRA-18181, recently we see builds aborted as python3 ./scripts/gen-nodetool-docs.py is too slow. 

According to [~brandon.williams] most of the time in the script is spent in create_adoc, which is serially forking nodetool help for every command to gather it.

CC [~Anthony Grasso]",brandon.williams,e.dimitrova,Normal,Resolved,Fixed,25/Jan/23 15:04,26/Jan/23 02:05
Bug,CASSANDRA-18198,13521528,"""AttributeError: module 'py' has no attribute 'io'"" reported in multiple tests","{{title = 'Timeout'}}
{{stream = <_io.TextIOWrapper name='<stderr>' mode='w' encoding='utf-8'>}}
{{{}sep = '+'{}}}{{{}def write_title(title, stream=None, sep=""~""):{}}}
{{{}""""""Write a section title.{}}}{{{}If *stream* is None sys.stderr will be used, *sep* is used to{}}}
{{draw the line.}}
{{""""""}}
{{if stream is None:}}
{{stream = sys.stderr}}
{{> width = py.io.get_terminal_width()}}
{{E AttributeError: module 'py' has no attribute 'io}}

 

is reported in multiple tests as noted below.

possibly a class loader issue associated with CASSANDRA-18150

4.1
[https://ci-cassandra.apache.org/job/Cassandra-4.1/256/testReport/dtest-offheap.repair_tests.incremental_repair_test/TestIncRepair/test_multiple_full_repairs_lcs]

3.11
[https://ci-cassandra.apache.org/job/Cassandra-3.11/424/testReport/dtest.bootstrap_test/TestBootstrap/test_simultaneous_bootstrap/]

3.0
[https://ci-cassandra.apache.org/job/Cassandra-3.0/328/testReport/dtest-upgrade.upgrade_tests.upgrade_supercolumns_test/TestSCUpgrade/test_upgrade_super_columns_through_all_versions/]",brandon.williams,claude,Normal,Resolved,Fixed,25/Jan/23 15:52,20/Feb/23 02:09
Bug,CASSANDRA-18200,13521587,Cassandra messaging to self changed behavior,"During testing of Cassandra on AWS, we noticed some behavior changes between Cassandra 3.11 and Cassandra 4.0 when it comes to messaging.
When performing a range query with consistency local_quorum, Cassandra sents a request to itself and some peers.
In case of Cassandra 4.0, it's trying to connect to itself using the broadcast_address while in Cassandra 3.11 it's connecting using the local address (see [https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/net/OutboundTcpConnectionPool.java#L152].

This translation seems to be missing in Cassandra 4.X. I think the best place to fix it would be here (see attached file): [https://github.com/apache/cassandra/blob/cassandra-4.0/src/java/org/apache/cassandra/net/OutboundConnectionSettings.java#L451]",masokol,masokol,Normal,Resolved,Fixed,26/Jan/23 06:59,30/Jan/23 13:08
Bug,CASSANDRA-18207,13521952,Nodetool documentation not working,"Clicking on the nodetool in the Tool branch of the doco tree doesn't do anything.

Also, links found in Google search (eg [https://cassandra.apache.org/doc/latest/cassandra/tools/nodetool/tablehistograms.html)] result in ""Not Found"" error.",brandon.williams,slater_ben,Normal,Resolved,Fixed,29/Jan/23 23:39,30/Jan/23 21:51
Bug,CASSANDRA-18210,13522294,cassandra.yaml links don't work,"On [https://cassandra.apache.org/doc/4.0/] the cassandra.yaml links in Configuring Cassandra don't work.
 # Go to [https://cassandra.apache.org/doc/4.0/cassandra/configuration/index.html]
 # Click [cassandra.yaml|https://cassandra.apache.org/doc/4.0/cassandra/configuration/index.html#configuration/cass_yaml_file.adoc]
 # You remain on the Configuring Cassandra page

The same appears to be true for trunk, 4.1, and 3.11 also.",brandon.williams,danyoung,Normal,Resolved,Fixed,31/Jan/23 19:47,31/Jan/23 20:39
Bug,CASSANDRA-18211,13522297, NoSuchFileException when removing a snapshot,"Automatic deletion of expired snapshots maintained by {{SnapshotManager}} can race with manual snapshot removal in a way an exception like {{NoSuchFileException}} is thrown. 

It is because the snapshot directory existence is checked and deleted if it exists as a non-atomic operation. Since we can potentially have two threads attempting to do that at the same time (automatic and manual snapshot removal) it may lead to a race in rare situations.
",smiklosovic,jlewandowski,Normal,Resolved,Fixed,31/Jan/23 20:24,17/Feb/23 16:34
Bug,CASSANDRA-18215,13522530,Fix the output of FQL dump tool to properly separate entries,"This was reported in (1)

(1) https://github.com/apache/cassandra/pull/2050

If I create a table something like this:

{code}
CREATE TABLE t1 ( id int PRIMARY KEY , v1 int, v2 int, v3 int) ;
{code}

and inserted a row using:

{code}
try (CqlSession cqlSession = CqlSession.builder().build()) {
    PreparedStatement preparedStatement = cqlSession.prepare(""INSERT INTO ks1.t1 (id, v1, v2, v3) VALUES (?, ?, ?, ?)"");
    cqlSession.execute(preparedStatement.bind(1, 1, 1, 1));
}
{code}

The output of fqltool looks something like this:

{code}
Type: single-query
Query start time: 1673373829119
Protocol version: 5
Generated timestamp:-9223372036854775808
Generated nowInSeconds:1673373829
Query: INSERT INTO ks1.t1 (id, v1, v2, v3) VALUES (?, ?, ?, ?)
Values: 
00000000 00 00 00 01                                      ····             
00000000 00 00 00 01                                      ····             
-----
00000000 00 00 00 01                                      ····             
-----
00000000 00 00 00 01                                      ····             
-----
{code}


----- is not printed between the first and second value

{code}
Values: 
00000000 00 00 00 01                                      ····             
00000000 00 00 00 01                                      ····   
{code}

We are normally very cautious about changing the output of the tooling but in this case I think this is a legit bug which should be fixed.",n.v.harikrishna,smiklosovic,Normal,Resolved,Fixed,01/Feb/23 15:01,07/Feb/23 13:51
Bug,CASSANDRA-18218,13522592,OOM while running ShortAccordSimulationTest,"{{ShortAccordSimulationTest}} seems to consistently run out of heap when run locally. This is true even when we scale the number of threads and duration down...

{noformat}
AccordSimulationRunner.main(new String[] { ""run"", ""-n"", ""3"", ""-t"", ""10"", ""--cluster-action-limit"", ""-1"", ""-c"", ""2"", ""-s"", ""10""});
{noformat}

{noformat}
ERROR [CommandStore[0]:1] node1 CS:[0] OP:0x8fb4f8f1 2023-01-31 17:16:27,143 Operation AsyncOperation{RUNNING}-0x8fb4f8f1 failed
java.lang.OutOfMemoryError: Java heap space
   at java.util.Arrays.copyOf(Arrays.java:3181)
   at accord.local.Command$NotifyWaitingOn.push(Command.java:794)
   at accord.local.Command$NotifyWaitingOn.accept(Command.java:757)
   at accord.local.Command.maybeExecute(Command.java:600)
   at accord.local.Command.onChange(Command.java:546)
   at org.apache.cassandra.service.accord.ListenerProxy$CommandListenerProxy.lambda$onChange$0(ListenerProxy.java:148)
   at org.apache.cassandra.service.accord.ListenerProxy$CommandListenerProxy$$Lambda$4606/1457728285.accept(Unknown Source)
   at org.apache.cassandra.service.accord.async.AsyncOperation$ForConsumer.apply(AsyncOperation.java:261)
   at org.apache.cassandra.service.accord.async.AsyncOperation$ForConsumer.apply(AsyncOperation.java:248)
   at org.apache.cassandra.service.accord.async.AsyncOperation.runInternal(AsyncOperation.java:154)
   at org.apache.cassandra.service.accord.async.AsyncOperation.run(AsyncOperation.java:194)
   at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
   at org.apache.cassandra.concurrent.SyncFutureTask.run(SyncFutureTask.java:68)
   at org.apache.cassandra.simulator.systems.InterceptingExecutor$AbstractSingleThreadedExecutorPlus.lambda$new$0(InterceptingExecutor.java:584)
   at org.apache.cassandra.simulator.systems.InterceptingExecutor$AbstractSingleThreadedExecutorPlus$$Lambda$768/827906088.run(Unknown Source)
   at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
   at java.lang.Thread.run(Thread.java:748)
{noformat}

JVM args make it seem like we’re passing both a 1 GiB and 8 GiB heap size, although that doesn’t seem to have any bearing on the result. Setting only 8 GiB just takes longer to hit the same problem.

{noformat}
INFO  [isolatedExecutor:2] node1 2023-01-31 17:15:56,940 JVM Arguments: [-Dstorage-config=/Users/maedhroz/Forks/cassandra/test/conf, -Djava.awt.headless=true, -javaagent:/Users/maedhroz/Forks/cassandra/lib/jamm-0.3.2.jar, -ea, -Djava.io.tmpdir=/var/folders/4d/zfjs7m7s6x5_l93k33r5k6680000gn/T/, -Dcassandra.debugrefcount=true, -Xss384k, -XX:SoftRefLRUPolicyMSPerMB=0, -XX:HeapDumpPath=build/test, -Dcassandra.test.driver.connection_timeout_ms=10000, -Dcassandra.test.driver.read_timeout_ms=24000, -Dcassandra.memtable_row_overhead_computation_step=100, -Dcassandra.test.use_prepared=true, -Dcassandra.test.sstableformatdevelopment=true, -Djava.security.egd=file:/dev/urandom, -Dcassandra.testtag=, -Dcassandra.keepBriefBrief=${cassandra.keepBriefBrief}, -Dcassandra.strict.runtime.checks=true, -Dcassandra.reads.thresholds.coordinator.defensive_checks_enabled=true, -DQT_SHRINKS=0, -Dlogback.configurationFile=test/conf/logback-simulator.xml, -Dcassandra.ring_delay_ms=10000, -Dcassandra.tolerate_sstable_size=true, -Dcassandra.skip_sync=true, -Dcassandra.debugrefcount=false, -Dcassandra.test.simulator.determinismcheck=strict, -Dcassandra.test.simulator.print_asm=none, -javaagent:/Users/maedhroz/Forks/cassandra/build/test/lib/jars/simulator-asm.jar, -Xbootclasspath/a:/Users/maedhroz/Forks/cassandra/build/test/lib/jars/simulator-bootstrap.jar, -XX:ActiveProcessorCount=4, -XX:-TieredCompilation, -XX:-BackgroundCompilation, -XX:CICompilerCount=1, -XX:Tier4CompileThreshold=1000, -XX:ReservedCodeCacheSize=256M, -Xmx8G, -Xmx1024m]
{noformat}",maedhroz,maedhroz,Normal,Resolved,Fixed,01/Feb/23 20:43,02/Feb/23 21:18
Bug,CASSANDRA-18223,13522817,Byteman rule in stop_data_reads.btm cannot compile against accord.messages.ReplyContext,"The Python {{read_repair_test}} relies on a Byteman rule on the {{doVerb()}} method in {{ReadCommandVerbHandler}}, but {{accord.messages.ReplyContext}} isn’t on the classpath. This is probably because we don't include it in the list of jars created in {{byteman_validate}}.

{noformat}
AssertionError: byteman script didn't compile
  Checking rule disable data reads against class org.apache.cassandra.db.ReadCommandVerbHandler
  Parsed rule ""disable data reads"" for class org.apache.cassandra.db.ReadCommandVerbHandler
  ERROR : Failed to check rule ""disable data reads"" loaded from /home/cassandra/cassandra-dtest/byteman/read_repair/stop_data_reads.btm line 8 against method doVerb(org.apache.cassandra.net.Message) void
  java.lang.NoClassDefFoundError: accord/messages/ReplyContext
{noformat}

ex. https://app.circleci.com/pipelines/github/maedhroz/cassandra/686/workflows/ffd1e528-b8ec-4534-a333-ab450e110e89/jobs/6481/tests#failed-test-0

It might make sense to fix this after CASSANDRA-18204 wraps up, so we know exactly how the Accord library is pulled into C*. Then, once we do fix it, we should fix in a way that still works w/ 4.0 and 4.1, etc. (i.e. Don't assume the Accord library must be present.)",shiraishi,maedhroz,Normal,Resolved,Fixed,02/Feb/23 16:50,20/Feb/23 20:47
Bug,CASSANDRA-18224,13522835,CEP-15 (C*) No commandsForKey in context for key PartitionKey,"See https://app.circleci.com/pipelines/github/maedhroz/cassandra/688/workflows/84cb0f28-d010-43c6-97cb-55af4a31a3ce/jobs/6554/tests#failed-test-0

{code}
ERROR [CommandStore[25]:1] node1 CS:[25] OP:0xe17e624f 2023-02-02 19:46:44,764 Operation AsyncOperation{RUNNING}-0xe17e624f failed
java.lang.IllegalArgumentException: No commandsForKey in context for key PartitionKey{tableId=dbb35709-2b11-38b3-9fcb-9a197512080e, key=DecoratedKey(5414964737888519087, 242cad05)}
	at org.apache.cassandra.service.accord.AccordCommandStore.getCommandsForKeyInternal(AccordCommandStore.java:442)
	at org.apache.cassandra.service.accord.AccordCommandStore$SafeAccordCommandStore.commandsForKey(AccordCommandStore.java:204)
	at org.apache.cassandra.service.accord.AccordCommandStore$SafeAccordCommandStore.lambda$register$1(AccordCommandStore.java:190)
	at accord.primitives.Routables$Helper.foldl(Routables.java:282)
	at accord.primitives.Routables.foldl(Routables.java:134)
	at org.apache.cassandra.service.accord.AccordCommandStore$SafeAccordCommandStore.register(AccordCommandStore.java:190)
	at accord.local.Command.set(Command.java:1015)
	at accord.local.Command.commit(Command.java:377)
	at accord.messages.Commit.apply$unsync(Commit.java:152)
	at accord.messages.Commit.apply(Commit.java)
	at accord.messages.Commit.apply(Commit.java:40)
	at org.apache.cassandra.service.accord.async.AsyncOperation$ForFunction.apply(AsyncOperation.java:239)
	at org.apache.cassandra.service.accord.async.AsyncOperation$ForFunction.apply(AsyncOperation.java:226)
	at org.apache.cassandra.service.accord.async.AsyncOperation.runInternal(AsyncOperation.java:154)
	at org.apache.cassandra.service.accord.async.AsyncOperation.run(AsyncOperation.java:194)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.SyncFutureTask.run(SyncFutureTask.java:68)
	at org.apache.cassandra.simulator.systems.InterceptingExecutor$AbstractSingleThreadedExecutorPlus.lambda$new$0(InterceptingExecutor.java:584)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
{code}",,dcapwell,Normal,Resolved,Fixed,02/Feb/23 22:12,10/Mar/23 21:51
Bug,CASSANDRA-18225,13522836,"CEP-15: (C*) Accord adds a accord.messages.Defer which isn't supported for persisting, causing the operation to fail","See https://app.circleci.com/pipelines/github/maedhroz/cassandra/688/workflows/84cb0f28-d010-43c6-97cb-55af4a31a3ce/jobs/6554/tests#failed-test-0

{code}
ERROR [CommandStore[25]:1] node1 CS:[25] OP:0xf01a5a4b 2023-02-02 19:46:44,701 Operation AsyncOperation{RUNNING}-0xf01a5a4b failed
java.lang.RuntimeException: Unhandled non-transient listener: accord.messages.Defer@3d36718e
	at org.apache.cassandra.service.accord.AccordCommand.maybeWrapListener(AccordCommand.java:711)
	at org.apache.cassandra.service.accord.AccordCommand.addListener(AccordCommand.java:717)
	at accord.messages.Defer.add(Defer.java:67)
	at accord.messages.Commit.apply$unsync(Commit.java:163)
	at accord.messages.Commit.apply(Commit.java)
	at accord.messages.Commit.apply(Commit.java:40)
	at org.apache.cassandra.service.accord.async.AsyncOperation$ForFunction.apply(AsyncOperation.java:239)
	at org.apache.cassandra.service.accord.async.AsyncOperation$ForFunction.apply(AsyncOperation.java:226)
	at org.apache.cassandra.service.accord.async.AsyncOperation.runInternal(AsyncOperation.java:154)
	at org.apache.cassandra.service.accord.async.AsyncOperation.run(AsyncOperation.java:194)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.SyncFutureTask.run(SyncFutureTask.java:68)
	at org.apache.cassandra.simulator.systems.InterceptingExecutor$AbstractSingleThreadedExecutorPlus.lambda$new$0(InterceptingExecutor.java:584)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
{code}",,dcapwell,Normal,Resolved,Fixed,02/Feb/23 22:13,10/Mar/23 21:51
Bug,CASSANDRA-18240,13523522,Using SELECT COUNT(*) FROM... LIMIT 1 in the returning section results in ClassCastException,"{noformat}
cqlsh> 
BEGIN TRANSACTION 
  LET row1 = (SELECT * FROM ks.tbl1 WHERE k = 5); 
  SELECT COUNT(*) FROM ks.tbl1 LIMIT 1; 
  IF row1 IS NULL THEN 
    INSERT INTO ks.tbl1 (k, v) VALUES (5, 10);
  END IF
COMMIT TRANSACTION;

NoHostAvailable: ('Unable to complete the operation against any hosts', {<Host: 127.0.0.1:9042 datacenter1>: <Error from server: code=0000 [Server error] message=""java.lang.ClassCastException: org.apache.cassandra.db.PartitionRangeReadCommand cannot be cast to org.apache.cassandra.db.SinglePartitionReadQuery$Group"">})
{noformat}",jlewandowski,jlewandowski,Normal,Resolved,Fixed,07/Feb/23 14:45,27/Feb/23 11:58
Bug,CASSANDRA-18241,13523523,Invalid result when a non existent row is assigned to a variable,"{noformat}
cqlsh:ks> CREATE TABLE tbl3 (k INT, c INT, v INT, PRIMARY KEY (k, c));
cqlsh:ks> 
BEGIN TRANSACTION 
  LET row = (SELECT * FROM tbl3 WHERE k=1 LIMIT 1); 
  SELECT row.k, row.c, row.v; 
  IF row.c > 10 THEN 
    INSERT INTO ks.tbl3 (k, c, v) VALUES (1, 10, 110); 
  END IF
COMMIT TRANSACTION;

 row.k | row.c | row.v
-------+-------+-------
     1 |  null |  null

cqlsh:ks> SELECT * FROM tbl3;

 k | c | v
---+---+---

(0 rows)
{noformat}

as discussed on Slack, the transaction should return:
{noformat}
 row.k | row.c | row.v
-------+-------+-------
  null |  null |  null
{noformat}
",jlewandowski,jlewandowski,Normal,Resolved,Fixed,07/Feb/23 14:50,20/Mar/23 22:25
Bug,CASSANDRA-18256,13524256,Backport CASSANDRA-17205 (remove strong self-ref in tidier) to all supported lines,CASSANDRA-17205 should probably have been backported to all supported branches. It's potentially masking some issues seen w/refs in other locations so we should pull it back to all supported lines.,jmckenzie,jmckenzie,Normal,Resolved,Fixed,10/Feb/23 16:35,05/Apr/23 00:22
Bug,CASSANDRA-18264,13524915,CustomClassLoader does not load jars rendering triggers from JARs broken,"A user had to downgrade to 4.0.7 from 4.1.0 because they hit a problem with CustomClassLoader for triggers. 

User says that in Apache Cassandra 4.1.0 the trigger mechanism does not work, not their trigger, but the possibility of loading any trigger in Cassandra.

In the Cassandra 4.1.0 version of CustomClassLoader (https://github.com/apache/cassandra/blob/cassandra-4.1/src/java/org/apache/cassandra/triggers/CustomClassLoader.java) the code is changed in such a way that when copying the JAR Cassandra uses java.nio.file.Files, while earlier versions (cassandra 4.0.X or 3.X) used Guava com.google.common.io.Files to copy the JAR file.

The difference between one and the other is that Guava by default overwrites the file if it already exists and user has permissions to do so, and in Java by default it does not overwrite.

Copying is done here (1) from inputJar to out. However, the problem is that we are getting temporary file from here (2) and the implementation loops unless it succeeds to create an empty file. (3) - But that fails to copy the file to out because copying does not work when the target file already exists.

(1) https://github.com/apache/cassandra/blob/cassandra-4.1/src/java/org/apache/cassandra/triggers/CustomClassLoader.java#L86

(2) https://github.com/apache/cassandra/blob/cassandra-4.1/src/java/org/apache/cassandra/triggers/CustomClassLoader.java#L81

(3) https://github.com/apache/cassandra/blob/cassandra-4.1/src/java/org/apache/cassandra/io/util/FileUtils.java#L152",smiklosovic,smiklosovic,Normal,Resolved,Fixed,15/Feb/23 21:22,17/Feb/23 16:14
Bug,CASSANDRA-18267,13525003,keepbrief is not called,"Discovered in CASSANDRA-18179 

The keepbrief macro is never called in build.xml

And attempts to use it result in
{noformat}
Buildfile: /Users/mick/src/apache/cassandra/build.xml
    [javac] Compiling 5 source files to /Users/mick/src/apache/cassandra/build/test/classes
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/EchoEclipseProjectLibs.java:21: error: package org.apache.cassandra.io.util does not exist
    [javac] import org.apache.cassandra.io.util.File;
    [javac]                                    ^
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/EchoEclipseProjectLibs.java:23: error: package org.apache.commons.io does not exist
    [javac] import org.apache.commons.io.FilenameUtils;
    [javac]                             ^
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/TestNameCheckTask.java:26: error: package org.junit does not exist
    [javac] import org.junit.Test;
    [javac]                 ^
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/TestNameCheckTask.java:30: error: package org.reflections does not exist
    [javac] import org.reflections.Reflections;
    [javac]                       ^
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/TestNameCheckTask.java:31: error: package org.reflections.scanners does not exist
    [javac] import org.reflections.scanners.Scanners;
    [javac]                                ^
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/TestNameCheckTask.java:32: error: package org.reflections.util does not exist
    [javac] import org.reflections.util.ConfigurationBuilder;
    [javac]                            ^
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/TestNameCheckTask.java:38: error: cannot find symbol
    [javac]     private static final Reflections reflections = new Reflections(new ConfigurationBuilder()
    [javac]                          ^
    [javac]   symbol:   class Reflections
    [javac]   location: class TestNameCheckTask
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/EchoEclipseProjectLibs.java:38: error: cannot find symbol
    [javac]         echo.setFile(new File("".classpath"").toJavaIOFile());
    [javac]                          ^
    [javac]   symbol:   class File
    [javac]   location: class EchoEclipseProjectLibs
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/EchoEclipseProjectLibs.java:52: error: cannot find symbol
    [javac]             String srcJar = FilenameUtils.getBaseName(jars[i]) + ""-sources.jar"";
    [javac]                             ^
    [javac]   symbol:   variable FilenameUtils
    [javac]   location: class EchoEclipseProjectLibs
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/EchoEclipseProjectLibs.java:53: error: cannot find symbol
    [javac]             String srcDir = FilenameUtils.concat(project.getProperty(""build.test.dir""), ""sources"");
    [javac]                             ^
    [javac]   symbol:   variable FilenameUtils
    [javac]   location: class EchoEclipseProjectLibs
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/EchoEclipseProjectLibs.java:54: error: cannot find symbol
    [javac]             File srcFile = new File(FilenameUtils.concat(srcDir, srcJar));
    [javac]             ^
    [javac]   symbol:   class File
    [javac]   location: class EchoEclipseProjectLibs
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/EchoEclipseProjectLibs.java:54: error: cannot find symbol
    [javac]             File srcFile = new File(FilenameUtils.concat(srcDir, srcJar));
    [javac]                                ^
    [javac]   symbol:   class File
    [javac]   location: class EchoEclipseProjectLibs
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/EchoEclipseProjectLibs.java:54: error: cannot find symbol
    [javac]             File srcFile = new File(FilenameUtils.concat(srcDir, srcJar));
    [javac]                                     ^
    [javac]   symbol:   variable FilenameUtils
    [javac]   location: class EchoEclipseProjectLibs
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/TestNameCheckTask.java:38: error: cannot find symbol
    [javac]     private static final Reflections reflections = new Reflections(new ConfigurationBuilder()
    [javac]                                                        ^
    [javac]   symbol:   class Reflections
    [javac]   location: class TestNameCheckTask
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/TestNameCheckTask.java:40: error: cannot find symbol
    [javac]                                                                    .setScanners(Scanners.MethodsAnnotated, Scanners.SubTypes)
    [javac]                                                                                 ^
    [javac]   symbol:   variable Scanners
    [javac]   location: class TestNameCheckTask
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/TestNameCheckTask.java:40: error: cannot find symbol
    [javac]                                                                    .setScanners(Scanners.MethodsAnnotated, Scanners.SubTypes)
    [javac]                                                                                                            ^
    [javac]   symbol:   variable Scanners
    [javac]   location: class TestNameCheckTask
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/TestNameCheckTask.java:38: error: cannot find symbol
    [javac]     private static final Reflections reflections = new Reflections(new ConfigurationBuilder()
    [javac]                                                                        ^
    [javac]   symbol:   class ConfigurationBuilder
    [javac]   location: class TestNameCheckTask
    [javac] /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/TestNameCheckTask.java:51: error: cannot find symbol
    [javac]         Set<Method> methodsAnnotatedWith = reflections.getMethodsAnnotatedWith(Test.class);
    [javac]                                                                                ^
    [javac]   symbol:   class Test
    [javac]   location: class TestNameCheckTask
    [javac] Note: /Users/mick/src/apache/cassandra/test/anttasks/org/apache/cassandra/anttasks/TestNameCheckTask.java uses unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 18 errors

BUILD FAILED
{noformat}",e.dimitrova,mck,Normal,Resolved,Fixed,16/Feb/23 10:24,13/Apr/23 14:37
Bug,CASSANDRA-18270,13525224,ssl-factory demo in examples is broken,"this fails, it is not happening in cassandra-4.1

{code}
cd examples/ssl-factory
ant build && ant test
{code}

My suspicion is that SSL factory related stuff was recently changed, in trunk, by (1) and this broke related ssl test.

[~maulin.vasavada] do you have some time to look into that as you are the author of the tests? I think I fixed the most of it here (2) but one test is still failing and I can not wrap my head around that one. It gives:

{code}
    [junit] Testcase: buildKeyManagerFactoryHappyPathForUnencryptedKey(org.apache.cassandra.security.KubernetesSecretsPEMSslContextFactoryTest):        Caused an ERROR
    [junit] Failed to build key manager store for secure connections
    [junit] javax.net.ssl.SSLException: Failed to build key manager store for secure connections
    [junit]     at org.apache.cassandra.security.PEMBasedSslContextFactory.buildKeyManagerFactory(PEMBasedSslContextFactory.java:267)
    [junit]     at org.apache.cassandra.security.PEMBasedSslContextFactory.buildKeyManagerFactory(PEMBasedSslContextFactory.java:229)
    [junit]     at org.apache.cassandra.security.KubernetesSecretsPEMSslContextFactory.buildKeyManagerFactory(KubernetesSecretsPEMSslContextFactory.java:169)
    [junit]     at org.apache.cassandra.security.KubernetesSecretsPEMSslContextFactoryTest.buildKeyManagerFactoryHappyPathForUnencryptedKey(KubernetesSecretsPEMSslContextFactoryTest.java:244)
    [junit] Caused by: java.io.IOException: overrun, bytes = 1195
    [junit]     at javax.crypto.EncryptedPrivateKeyInfo.<init>(EncryptedPrivateKeyInfo.java:95)
    [junit]     at org.apache.cassandra.security.PEMReader.extractPrivateKey(PEMReader.java:108)
    [junit]     at org.apache.cassandra.security.PEMBasedSslContextFactory.buildKeyStore(PEMBasedSslContextFactory.java:319)
    [junit]     at org.apache.cassandra.security.PEMBasedSslContextFactory.buildKeyManagerFactory(PEMBasedSslContextFactory.java:251)
{code}

(1) https://github.com/apache/cassandra/commit/ed3901823a5fe9f8838d8b592a1b7703b12e810b
(2) https://github.com/instaclustr/cassandra/tree/CASSANDRA-18264-trunk-followup

cc [~Jyothsnakonisa]",maulin.vasavada,smiklosovic,Normal,Resolved,Fixed,17/Feb/23 16:10,08/May/23 20:32
Bug,CASSANDRA-18291,13526516,Too early schema version change in system local table,"Schema version in the system local table is updated after the schema changes is saved but before it is applied. 

Found by [~maxtomassi]
",jlewandowski,jlewandowski,Normal,Resolved,Fixed,28/Feb/23 12:05,09/Mar/23 07:22
Bug,CASSANDRA-18292,13526578,Gossip stateMapOrdering does not have correct ordering when both EndpointState are in the bootstrapping set,"There is a bug when stateMapOrdering sees two or more EndpointState that are both in the bootstrapping set, this may cause the ordering to change causing Collections.sort to fail
",dcapwell,dcapwell,Normal,Resolved,Fixed,28/Feb/23 21:03,02/Mar/23 17:47
Bug,CASSANDRA-18294,13526756,die disk failure policy will not kill jvm as documented,"After Cassandra has successfully starts up with disk_failure_policy die, when encounter a file system error, Cassandra server will only throw exception instead of shut down gossip and client transport and kill JVM. Document: [https://cassandra.apache.org/doc/latest/cassandra/configuration/cass_yaml_file.html#disk_failure_policy]

 

The reason for this is the default FS error handler is not handing policy die correctly. Instead of shutting down gossip and native transport, it throws an error.

 

The JVMStabilityInspectorTest is passing because the error handler is not set so no exception is thrown.",curlylrt,curlylrt,Normal,Resolved,Fixed,01/Mar/23 23:02,15/Mar/23 21:31
Bug,CASSANDRA-18303,13527287,Feature documentation lost / moved out of focus,"Documentation added with CASSANDRA-17344 wasn't moved with CASSANDRA-17976 and now the documentation on the website page ""Cassandra/Operating/Virtual tables"" shows an outdated version for [4.1|https://cassandra.apache.org/doc/4.1/cassandra/operating/virtualtables.html] and [latest|https://cassandra.apache.org/doc/latest/cassandra/operating/virtualtables.html].",rtib,rtib,Normal,Resolved,Fixed,06/Mar/23 14:08,15/Mar/23 10:43
Bug,CASSANDRA-18304,13527352,hinted_handoff_enabled=false is not honored,"I've had some dtests with disabled hints failing.

After investigation it seems that CASSANDRA-17164 moved hint submission on timeout from [RequestCallbacks.onExpired|https://github.com/apache/cassandra/commit/d2923275e360a1ee9db498e748c269f701bb3a8b#diff-b73c13ea8cae91a215495917fe5e90d55c9f4a283f9e053110992bc9a60004b8L176] to [AbstractWriteResponseHandler.onFailure|https://github.com/apache/cassandra/commit/d2923275e360a1ee9db498e748c269f701bb3a8b#diff-3b202de0d077638bede7bf4076a15eb4d483b717f955f11e743efb8d27c6eb1dR285], but it no longer checks if {{CallbackInfo.shouldHint}} which checks for {{StorageProxy.shouldHint}} which ultimately checks if {{{}hinted_handoff_enabled=true{}}}.

This could cause some tests which expect hints to be disabled to fail intermittently.",paulo,paulo,Normal,Resolved,Fixed,06/Mar/23 23:14,24/Mar/23 16:19
Bug,CASSANDRA-18307,13527610,Release 4.0.8 not available on jfrog package repositories,Release 4.0.8 was published to dist/downloads.a.o only and seems not available at apache.jfrog.io neither for Debian nor RPM.,brandon.williams,rtib,Normal,Resolved,Fixed,08/Mar/23 09:19,09/Mar/23 12:17
Bug,CASSANDRA-18308,13527640,Fix broken sstableverify dtest,"I accidentally broken sstableverify dtest in CASSANDRA-17056 for versions <= 4.1
",jlewandowski,jlewandowski,Normal,Resolved,Fixed,08/Mar/23 13:19,08/Mar/23 16:37
Bug,CASSANDRA-18309,13527717,Remove git hook for pre-push as it is redundant and causes issues when merging to mainline,"{code}
[cep-15-accord][~/repos/apache-cassandra]$ git push origin cep-15-accord
Warning: Permanently added 'github.com' (ED25519) to the list of known hosts.
Entering 'modules/accord'
Username for 'https://github.com':
{code}

This is caused by .build/git/git-hooks/pre-push/100-push-submodules.sh logic

{code}
  local -r cmd='
branch=""$(git rev-parse --abbrev-ref HEAD)""
[[ ""$branch"" == ""HEAD"" ]] && exit 0

default_remote=""$(git config --local --get branch.""${branch}"".remote || true)""
remote=""${default_remote:-origin}""

git push --atomic ""$remote"" ""$branch""
'
  git submodule foreach --recursive ""$cmd""
{code}

This logic was to make sure that the submodule is pushed before you push your changes, but this is slightly redundant as .build/git/git-hooks/pre-commit/100-verify-submodules-pushed.sh will not allow you to commit the submodule SHA until it can confirm its on GitHub.",dcapwell,dcapwell,Normal,Resolved,Fixed,09/Mar/23 00:00,11/Apr/23 20:27
Bug,CASSANDRA-18311,13527797,BufferPool incorrectly counts memoryInUse when putUnusedPortion is used,"The counter is incorrectly decremented by the size of the unused portion of the provided buffer.
It should be decremented by the number of bytes actually returned to the pool (that may be different than ""size""). The number should be calculated as a difference between original and resulting buffer capacity.",jtgrabowski,jtgrabowski,Normal,Resolved,Fixed,09/Mar/23 11:33,13/Mar/23 09:09
Bug,CASSANDRA-18320,13528031,Incompatible file system thrown while running Simulator,"{code}
java.io.UncheckedIOException
	at org.apache.cassandra.io.util.PathUtils.propagateUnchecked(PathUtils.java:831)
	at org.apache.cassandra.io.util.PathUtils.propagateUnchecked(PathUtils.java:816)
	at org.apache.cassandra.io.util.PathUtils.delete(PathUtils.java:257)
	at org.apache.cassandra.io.util.PathUtils.deleteRecursive(PathUtils.java:381)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at java.util.stream.SortedOps$RefSortingSink.end(SortedOps.java:395)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:483)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.apache.cassandra.io.util.PathUtils.forEach(PathUtils.java:155)
	at org.apache.cassandra.io.util.PathUtils.deleteRecursive(PathUtils.java:378)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at java.util.stream.SortedOps$RefSortingSink.end(SortedOps.java:395)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:483)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.apache.cassandra.io.util.PathUtils.forEach(PathUtils.java:155)
	at org.apache.cassandra.io.util.PathUtils.deleteRecursive(PathUtils.java:378)
	at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1047)
	at org.apache.cassandra.simulator.ClusterSimulation.close(ClusterSimulation.java:816)
	at org.apache.cassandra.simulator.SimulationRunner$Run.run(SimulationRunner.java:370)
	at org.apache.cassandra.simulator.SimulationRunner$BasicCommand.run(SimulationRunner.java:345)
	at org.apache.cassandra.simulator.paxos.PaxosSimulationRunner.main(PaxosSimulationRunner.java:148)
	at org.apache.cassandra.simulator.test.ShortPaxosSimulationTest.simulationTest(ShortPaxosSimulationTest.java:33)
Caused by: java.nio.file.DirectoryNotEmptyException: /cassandra/node1/commitlog
	at com.google.common.jimfs.FileSystemView.checkEmpty(FileSystemView.java:535)
	at com.google.common.jimfs.FileSystemView.checkDeletable(FileSystemView.java:517)
	at com.google.common.jimfs.FileSystemView.delete(FileSystemView.java:479)
	at com.google.common.jimfs.FileSystemView.deleteFile(FileSystemView.java:465)
	at com.google.common.jimfs.JimfsFileSystemProvider.delete(JimfsFileSystemProvider.java:261)
	at java.nio.file.Files.delete(Files.java:1126)
	at org.apache.cassandra.io.util.PathUtils.delete(PathUtils.java:252)
{code}",dcapwell,e.dimitrova,Normal,Resolved,Fixed,10/Mar/23 19:26,11/Apr/23 20:27
Bug,CASSANDRA-18326,13528378,Debian package repository misconfiguration,"Debian apt is failing on current jfrog repository access for 40x releases with:

{code}
W: Conflicting distribution: https://debian.cassandra.apache.org 40x InRelease (expected 40x but got 40)

E: Repository 'https://debian.cassandra.apache.org 40x InRelease' changed its 'Codename' value from '40x' to '40'

N: This must be accepted explicitly before updates for this repository can be applied. See apt-secure(8) manpage for details.
{code}

This is caused by the typo in [dists/40x/Release|https://debian.cassandra.apache.org/dists/40x/Release] containing
{code}
Codename: 40
{code}
but it is expected to be
{code}
Codename: 40x
{code}
",brandon.williams,rtib,Normal,Resolved,Fixed,14/Mar/23 08:54,14/Mar/23 12:38
Bug,CASSANDRA-18332,13528502,Backport CASSANDRA-17205 to 4.0 branch (strong ref leak),"See description in CASSANDRA-17205; this should have been applied on 4.0 and merged up but was overlooked.

 

Also double-check that strong leaks are logged at ERROR instead of WARN on both 4.0, 4.1, and trunk (see [comment|https://issues.apache.org/jira/browse/CASSANDRA-18176?focusedCommentId=17687184&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17687184])",jmckenzie,jmckenzie,Normal,Resolved,Fixed,14/Mar/23 20:39,05/Apr/23 00:22
Bug,CASSANDRA-18336,13528705,Do not remove SSTables when cause of FSReadError is OutOfMemoryError while using best_effort disk failure policy,"1.When this exception occurs in the system
{code:java}
// 
ERROR [CompactionExecutor:351627] 2023-02-21 17:59:20,721 CassandraDaemon.java:581 - Exception in thread Thread[CompactionExecutor:351627,1,main]
org.apache.cassandra.io.FSReadError: java.io.IOException: Map failed
    at org.apache.cassandra.io.util.ChannelProxy.map(ChannelProxy.java:167)
    at org.apache.cassandra.io.util.MmappedRegions$State.add(MmappedRegions.java:310)
    at org.apache.cassandra.io.util.MmappedRegions$State.access$400(MmappedRegions.java:246)
    at org.apache.cassandra.io.util.MmappedRegions.updateState(MmappedRegions.java:170)
    at org.apache.cassandra.io.util.MmappedRegions.<init>(MmappedRegions.java:73)
    at org.apache.cassandra.io.util.MmappedRegions.<init>(MmappedRegions.java:61)
    at org.apache.cassandra.io.util.MmappedRegions.map(MmappedRegions.java:104)
    at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:365)
    at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openEarly(BigTableWriter.java:337)
    at org.apache.cassandra.io.sstable.SSTableRewriter.maybeReopenEarly(SSTableRewriter.java:172)
    at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:124)
    at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.realAppend(DefaultCompactionWriter.java:64)
    at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:137)
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:193)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:77)
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:100)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:298)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.IOException: Map failed
    at java.base/sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:1016)
    at org.apache.cassandra.io.util.ChannelProxy.map(ChannelProxy.java:163)
    ... 23 common frames omitted
Caused by: java.lang.OutOfMemoryError: Map failed
    at java.base/sun.nio.ch.FileChannelImpl.map0(Native Method)
    at java.base/sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:1013)


{code}
2.Restart the node, Verifying logfile transaction ,All sstables are deleted
{code:java}
// code placeholder
INFO  [main] 2023-02-21 18:00:23,350 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8819408-big-Index.db 
INFO  [main] 2023-02-21 18:00:23,615 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8819408-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,504 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_c923b230-b077-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:46,510 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_461935b0-b1ce-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:46,517 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,517 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,518 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,520 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,520 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,520 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,521 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,521 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,521 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,526 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,526 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,537 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,537 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,537 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,539 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,539 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,540 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,543 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,545 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,545 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,545 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,545 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,546 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,549 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_461935b0-b1ce-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:46,550 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_69071e60-b18e-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:46,577 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,577 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,579 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,579 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,584 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,584 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,585 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,585 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,585 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,585 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,586 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,590 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,592 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,592 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,602 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,602 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,602 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,602 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,606 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_69071e60-b18e-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:46,610 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_8b8205e0-b18e-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:46,641 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,644 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,644 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,644 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,684 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,684 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,684 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,684 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,685 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,687 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,688 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,727 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,731 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,732 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,732 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,771 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,771 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,774 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_8b8205e0-b18e-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:46,775 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_008f3d00-b1ce-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:46,779 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830650-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,787 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830650-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,020 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_008f3d00-b1ce-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:47,022 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_6f265950-b18e-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:47,050 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,055 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,055 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,072 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,072 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,072 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,072 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,074 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,074 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,077 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,078 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,092 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,097 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,098 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,117 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_6f265950-b18e-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:47,118 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_fb014430-b18e-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,133 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,134 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,134 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,246 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,246 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,246 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,247 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,247 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,255 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,255 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,255 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,368 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,369 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,369 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,369 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,369 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,374 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,374 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,374 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,484 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,485 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,485 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,485 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,490 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_fb014430-b18e-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:47,492 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_unknowncompactiontype_695c4f33-b1ce-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:47,502 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Index.db 
INFO  [main] 2023-02-21 18:00:48,045 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Filter.db 
INFO  [main] 2023-02-21 18:00:48,053 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Summary.db 
INFO  [main] 2023-02-21 18:00:48,053 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Data.db 
INFO  [main] 2023-02-21 18:01:21,166 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Digest.crc32 
INFO  [main] 2023-02-21 18:01:21,202 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:01:21,272 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Statistics.db 
INFO  [main] 2023-02-21 18:01:21,272 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-TOC.txt 
INFO  [main] 2023-02-21 18:01:21,272 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Index.db 
INFO  [main] 2023-02-21 18:01:21,276 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Filter.db 
INFO  [main] 2023-02-21 18:01:21,276 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Data.db 
INFO  [main] 2023-02-21 18:01:21,500 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Summary.db 
INFO  [main] 2023-02-21 18:01:21,500 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Digest.crc32 
INFO  [main] 2023-02-21 18:01:21,500 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:01:21,501 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Statistics.db 
INFO  [main] 2023-02-21 18:01:21,501 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-TOC.txt 
INFO  [main] 2023-02-21 18:01:21,501 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Index.db 
INFO  [main] 2023-02-21 18:01:21,841 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Filter.db 
INFO  [main] 2023-02-21 18:01:21,842 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Summary.db 
INFO  [main] 2023-02-21 18:01:21,842 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Data.db 
INFO  [main] 2023-02-21 18:01:22,779 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Digest.crc32 
INFO  [main] 2023-02-21 18:01:22,779 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:01:22,780 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Statistics.db 
INFO  [main] 2023-02-21 18:01:22,780 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-TOC.txt 
INFO  [main] 2023-02-21 18:01:22,780 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Filter.db 
INFO  [main] 2023-02-21 18:01:22,825 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Index.db 
INFO  [main] 2023-02-21 18:01:24,891 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Summary.db 
INFO  [main] 2023-02-21 18:01:24,892 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Data.db 
INFO  [main] 2023-02-21 18:02:02,190 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Digest.crc32 
INFO  [main] 2023-02-21 18:02:02,352 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:02:02,461 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Statistics.db 
INFO  [main] 2023-02-21 18:02:02,461 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-TOC.txt 
INFO  [main] 2023-02-21 18:02:02,462 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Index.db 
INFO  [main] 2023-02-21 18:02:02,466 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Filter.db 
INFO  [main] 2023-02-21 18:02:02,467 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Data.db 
INFO  [main] 2023-02-21 18:02:02,763 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Summary.db 
INFO  [main] 2023-02-21 18:02:02,764 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Digest.crc32 
INFO  [main] 2023-02-21 18:02:02,764 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:02:02,764 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Statistics.db 
INFO  [main] 2023-02-21 18:02:02,764 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-TOC.txt 
INFO  [main] 2023-02-21 18:02:02,765 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Index.db 
INFO  [main] 2023-02-21 18:02:05,377 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Filter.db 
INFO  [main] 2023-02-21 18:02:05,388 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Summary.db 
INFO  [main] 2023-02-21 18:02:05,388 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Data.db 
INFO  [main] 2023-02-21 18:02:41,367 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Digest.crc32 
INFO  [main] 2023-02-21 18:02:41,368 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:02:41,397 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Statistics.db 
INFO  [main] 2023-02-21 18:02:41,397 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-TOC.txt 
INFO  [main] 2023-02-21 18:02:41,398 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Index.db 
INFO  [main] 2023-02-21 18:02:42,034 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Filter.db 
INFO  [main] 2023-02-21 18:02:42,049 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Data.db 
INFO  [main] 2023-02-21 18:04:02,731 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Summary.db 
INFO  [main] 2023-02-21 18:04:02,732 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Digest.crc32 
INFO  [main] 2023-02-21 18:04:02,732 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:04:02,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Statistics.db 
INFO  [main] 2023-02-21 18:04:02,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-TOC.txt 
INFO  [main] 2023-02-21 18:04:02,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Index.db 
INFO  [main] 2023-02-21 18:04:02,784 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Filter.db 
INFO  [main] 2023-02-21 18:04:02,785 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Data.db 
INFO  [main] 2023-02-21 18:04:02,889 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Summary.db 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Digest.crc32 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Statistics.db 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-TOC.txt 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Index.db 
INFO  [main] 2023-02-21 18:04:03,384 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Filter.db 
INFO  [main] 2023-02-21 18:04:03,418 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Data.db 
INFO  [main] 2023-02-21 18:04:38,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Summary.db 
INFO  [main] 2023-02-21 18:04:38,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Digest.crc32 
INFO  [main] 2023-02-21 18:04:38,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:04:38,245 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Statistics.db 
INFO  [main] 2023-02-21 18:04:38,245 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-TOC.txt 
INFO  [main] 2023-02-21 18:04:38,246 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Filter.db 
INFO  [main] 2023-02-21 18:04:38,293 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Index.db 
INFO  [main] 2023-02-21 18:04:39,438 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Summary.db 
INFO  [main] 2023-02-21 18:04:39,438 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Data.db 
INFO  [main] 2023-02-21 18:05:28,014 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Digest.crc32 
INFO  [main] 2023-02-21 18:05:28,015 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:05:28,041 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Statistics.db 
INFO  [main] 2023-02-21 18:05:28,041 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-TOC.txt 
INFO  [main] 2023-02-21 18:05:28,041 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Filter.db 
INFO  [main] 2023-02-21 18:05:28,042 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Index.db 
INFO  [main] 2023-02-21 18:05:28,277 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Data.db 
INFO  [main] 2023-02-21 18:06:17,552 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Summary.db 
INFO  [main] 2023-02-21 18:06:17,553 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Digest.crc32 
INFO  [main] 2023-02-21 18:06:17,554 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:06:17,565 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Statistics.db 
INFO  [main] 2023-02-21 18:06:17,565 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-TOC.txt 
INFO  [main] 2023-02-21 18:06:17,566 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Index.db 
INFO  [main] 2023-02-21 18:06:17,567 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Filter.db 
INFO  [main] 2023-02-21 18:06:17,568 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Summary.db 
INFO  [main] 2023-02-21 18:06:17,568 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Data.db 
INFO  [main] 2023-02-21 18:06:24,899 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Digest.crc32 
INFO  [main] 2023-02-21 18:06:24,900 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:06:24,932 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Statistics.db 
INFO  [main] 2023-02-21 18:06:24,933 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-TOC.txt 
INFO  [main] 2023-02-21 18:06:24,933 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Filter.db 
INFO  [main] 2023-02-21 18:06:24,949 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Index.db 
INFO  [main] 2023-02-21 18:06:29,880 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Data.db 
INFO  [main] 2023-02-21 18:08:11,665 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Summary.db 
INFO  [main] 2023-02-21 18:08:11,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Digest.crc32 
INFO  [main] 2023-02-21 18:08:11,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:08:11,667 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Statistics.db 
INFO  [main] 2023-02-21 18:08:11,667 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-TOC.txt 
INFO  [main] 2023-02-21 18:08:11,667 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Index.db 
INFO  [main] 2023-02-21 18:08:11,717 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Filter.db 
INFO  [main] 2023-02-21 18:08:11,717 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Summary.db 
INFO  [main] 2023-02-21 18:08:11,718 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Data.db 
INFO  [main] 2023-02-21 18:08:22,177 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Digest.crc32 
INFO  [main] 2023-02-21 18:08:22,178 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:08:22,178 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Statistics.db 
INFO  [main] 2023-02-21 18:08:22,178 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-TOC.txt 
INFO  [main] 2023-02-21 18:08:22,178 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Filter.db 
INFO  [main] 2023-02-21 18:08:22,212 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Index.db 
INFO  [main] 2023-02-21 18:08:22,641 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Summary.db 
INFO  [main] 2023-02-21 18:08:22,642 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Data.db 
INFO  [main] 2023-02-21 18:09:16,035 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Digest.crc32 
INFO  [main] 2023-02-21 18:09:16,036 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:09:16,162 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Statistics.db 
INFO  [main] 2023-02-21 18:09:16,162 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-TOC.txt 
INFO  [main] 2023-02-21 18:09:16,163 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Index.db 
INFO  [main] 2023-02-21 18:09:16,302 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Filter.db 
INFO  [main] 2023-02-21 18:09:16,303 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Summary.db 
INFO  [main] 2023-02-21 18:09:16,303 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Data.db 
INFO  [main] 2023-02-21 18:09:30,352 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Digest.crc32 
INFO  [main] 2023-02-21 18:09:30,353 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:09:30,353 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Statistics.db 
INFO  [main] 2023-02-21 18:09:30,354 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-TOC.txt 
INFO  [main] 2023-02-21 18:09:30,354 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Filter.db 
INFO  [main] 2023-02-21 18:09:30,377 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Index.db 
INFO  [main] 2023-02-21 18:09:32,789 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Summary.db 
INFO  [main] 2023-02-21 18:09:32,789 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Data.db 
INFO  [main] 2023-02-21 18:10:17,487 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Digest.crc32 
INFO  [main] 2023-02-21 18:10:17,692 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:10:17,741 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Statistics.db 
INFO  [main] 2023-02-21 18:10:17,742 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-TOC.txt 
INFO  [main] 2023-02-21 18:10:17,743 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Filter.db 
INFO  [main] 2023-02-21 18:10:17,743 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Index.db 
INFO  [main] 2023-02-21 18:10:17,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Summary.db 
INFO  [main] 2023-02-21 18:10:17,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Data.db 
INFO  [main] 2023-02-21 18:10:17,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Digest.crc32 
INFO  [main] 2023-02-21 18:10:17,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:10:17,759 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Statistics.db 
INFO  [main] 2023-02-21 18:10:17,759 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-TOC.txt 
INFO  [main] 2023-02-21 18:10:17,760 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Index.db 
INFO  [main] 2023-02-21 18:10:18,081 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Filter.db 
INFO  [main] 2023-02-21 18:10:18,117 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Data.db 
INFO  [main] 2023-02-21 18:11:06,042 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Summary.db 
INFO  [main] 2023-02-21 18:11:06,043 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Digest.crc32 
INFO  [main] 2023-02-21 18:11:06,043 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:11:06,079 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Statistics.db 
INFO  [main] 2023-02-21 18:11:06,079 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-TOC.txt 
INFO  [main] 2023-02-21 18:11:06,080 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Index.db 
INFO  [main] 2023-02-21 18:11:06,159 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Filter.db 
INFO  [main] 2023-02-21 18:11:06,159 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Data.db 
INFO  [main] 2023-02-21 18:11:16,709 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Summary.db 
INFO  [main] 2023-02-21 18:11:16,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Digest.crc32 
INFO  [main] 2023-02-21 18:11:16,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:11:16,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Statistics.db 
INFO  [main] 2023-02-21 18:11:16,902 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-TOC.txt 
INFO  [main] 2023-02-21 18:11:16,903 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Index.db 
INFO  [main] 2023-02-21 18:11:17,170 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Filter.db 
INFO  [main] 2023-02-21 18:11:17,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Summary.db 
INFO  [main] 2023-02-21 18:11:17,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Data.db 
INFO  [main] 2023-02-21 18:11:59,054 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Digest.crc32 
INFO  [main] 2023-02-21 18:11:59,055 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:11:59,076 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Statistics.db 
INFO  [main] 2023-02-21 18:11:59,076 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-TOC.txt 
INFO  [main] 2023-02-21 18:11:59,076 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Index.db 
INFO  [main] 2023-02-21 18:11:59,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Filter.db 
INFO  [main] 2023-02-21 18:11:59,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Summary.db 
INFO  [main] 2023-02-21 18:11:59,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Data.db 
INFO  [main] 2023-02-21 18:12:28,397 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Digest.crc32 
INFO  [main] 2023-02-21 18:12:28,397 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:12:28,398 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Statistics.db 
INFO  [main] 2023-02-21 18:12:28,398 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-TOC.txt 
INFO  [main] 2023-02-21 18:12:28,398 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Index.db 
INFO  [main] 2023-02-21 18:12:28,400 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Filter.db 
INFO  [main] 2023-02-21 18:12:28,400 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Data.db 
INFO  [main] 2023-02-21 18:12:42,749 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Summary.db 
INFO  [main] 2023-02-21 18:12:42,750 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Digest.crc32 
INFO  [main] 2023-02-21 18:12:42,750 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:12:42,774 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Statistics.db 
INFO  [main] 2023-02-21 18:12:42,774 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-TOC.txt 
INFO  [main] 2023-02-21 18:12:42,775 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Filter.db 
INFO  [main] 2023-02-21 18:12:42,775 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Index.db 
INFO  [main] 2023-02-21 18:12:42,820 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Summary.db 
INFO  [main] 2023-02-21 18:12:42,821 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Data.db 
INFO  [main] 2023-02-21 18:12:55,614 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Digest.crc32 
INFO  [main] 2023-02-21 18:12:55,618 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:12:55,630 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Statistics.db 
INFO  [main] 2023-02-21 18:12:55,630 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-TOC.txt 
INFO  [main] 2023-02-21 18:12:55,630 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Filter.db 
INFO  [main] 2023-02-21 18:12:55,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Index.db 
INFO  [main] 2023-02-21 18:12:57,535 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Data.db 
INFO  [main] 2023-02-21 18:15:07,614 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Summary.db 
INFO  [main] 2023-02-21 18:15:07,615 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Digest.crc32 
INFO  [main] 2023-02-21 18:15:07,615 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:15:07,640 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Statistics.db 
INFO  [main] 2023-02-21 18:15:07,640 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-TOC.txt 
INFO  [main] 2023-02-21 18:15:07,641 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Index.db 
INFO  [main] 2023-02-21 18:15:07,909 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Filter.db 
INFO  [main] 2023-02-21 18:15:07,950 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Data.db 
INFO  [main] 2023-02-21 18:15:58,262 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Summary.db 
INFO  [main] 2023-02-21 18:15:58,263 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Digest.crc32 
INFO  [main] 2023-02-21 18:15:58,263 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:15:58,325 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Statistics.db 
INFO  [main] 2023-02-21 18:15:58,325 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-TOC.txt 
INFO  [main] 2023-02-21 18:15:58,326 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Index.db 
INFO  [main] 2023-02-21 18:15:58,326 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Filter.db 
INFO  [main] 2023-02-21 18:15:58,326 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Summary.db 
INFO  [main] 2023-02-21 18:15:58,327 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Data.db 
INFO  [main] 2023-02-21 18:15:58,535 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Digest.crc32 
INFO  [main] 2023-02-21 18:15:58,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:15:58,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Statistics.db 
INFO  [main] 2023-02-21 18:15:58,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-TOC.txt 
INFO  [main] 2023-02-21 18:15:58,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Index.db 
INFO  [main] 2023-02-21 18:15:58,550 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Filter.db 
INFO  [main] 2023-02-21 18:15:58,551 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Summary.db 
INFO  [main] 2023-02-21 18:15:58,551 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Data.db 
INFO  [main] 2023-02-21 18:15:58,856 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Digest.crc32 
INFO  [main] 2023-02-21 18:15:58,857 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:15:58,858 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Statistics.db 
INFO  [main] 2023-02-21 18:15:58,858 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-TOC.txt 
INFO  [main] 2023-02-21 18:15:58,858 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Index.db 
INFO  [main] 2023-02-21 18:15:58,860 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Filter.db 
INFO  [main] 2023-02-21 18:15:58,860 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Data.db 
INFO  [main] 2023-02-21 18:16:19,652 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Summary.db 
INFO  [main] 2023-02-21 18:16:19,653 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Digest.crc32 
INFO  [main] 2023-02-21 18:16:19,654 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:16:19,664 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Statistics.db 
INFO  [main] 2023-02-21 18:16:19,664 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-TOC.txt 
INFO  [main] 2023-02-21 18:16:19,665 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:16:19,772 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Data.db 
INFO  [main] 2023-02-21 18:17:13,519 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Filter.db 
INFO  [main] 2023-02-21 18:17:13,519 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Index.db 
INFO  [main] 2023-02-21 18:17:13,537 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Statistics.db 
INFO  [main] 2023-02-21 18:17:13,538 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Summary.db 
INFO  [main] 2023-02-21 18:17:13,538 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Index.db 
INFO  [main] 2023-02-21 18:17:13,549 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Filter.db 
INFO  [main] 2023-02-21 18:17:13,549 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Data.db 
INFO  [main] 2023-02-21 18:17:14,232 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Summary.db 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Digest.crc32 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Statistics.db 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-TOC.txt 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Index.db 
INFO  [main] 2023-02-21 18:17:14,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Filter.db 
INFO  [main] 2023-02-21 18:17:14,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Summary.db 
INFO  [main] 2023-02-21 18:17:14,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Data.db 
INFO  [main] 2023-02-21 18:17:14,339 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Digest.crc32 
INFO  [main] 2023-02-21 18:17:14,339 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:17:14,339 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Statistics.db 
INFO  [main] 2023-02-21 18:17:14,340 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-TOC.txt 
INFO  [main] 2023-02-21 18:17:14,340 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Index.db 
INFO  [main] 2023-02-21 18:17:14,354 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Filter.db 
INFO  [main] 2023-02-21 18:17:14,354 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Data.db 
INFO  [main] 2023-02-21 18:17:17,432 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Summary.db 
INFO  [main] 2023-02-21 18:17:17,433 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Digest.crc32 
INFO  [main] 2023-02-21 18:17:17,433 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:17:17,449 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Statistics.db 
INFO  [main] 2023-02-21 18:17:17,450 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-TOC.txt 
INFO  [main] 2023-02-21 18:17:17,450 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Index.db 
INFO  [main] 2023-02-21 18:17:17,969 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Filter.db 
INFO  [main] 2023-02-21 18:17:17,970 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Summary.db 
INFO  [main] 2023-02-21 18:17:17,970 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Data.db 
INFO  [main] 2023-02-21 18:18:43,709 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:43,710 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:43,710 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:43,710 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:43,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Index.db 
INFO  [main] 2023-02-21 18:18:43,739 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Filter.db 
INFO  [main] 2023-02-21 18:18:43,739 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Summary.db 
INFO  [main] 2023-02-21 18:18:43,739 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Data.db 
INFO  [main] 2023-02-21 18:18:43,885 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:43,885 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:43,885 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:43,886 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:43,886 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Index.db 
INFO  [main] 2023-02-21 18:18:44,106 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Filter.db 
INFO  [main] 2023-02-21 18:18:44,106 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Data.db 
INFO  [main] 2023-02-21 18:18:47,122 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Summary.db 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Index.db 
INFO  [main] 2023-02-21 18:18:47,199 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Filter.db 
INFO  [main] 2023-02-21 18:18:47,199 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Summary.db 
INFO  [main] 2023-02-21 18:18:47,199 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Data.db 
INFO  [main] 2023-02-21 18:18:49,411 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:49,412 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:49,412 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:49,412 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:49,413 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Index.db 
INFO  [main] 2023-02-21 18:18:49,414 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Filter.db 
INFO  [main] 2023-02-21 18:18:49,414 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Data.db 
INFO  [main] 2023-02-21 18:18:49,634 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Summary.db 
INFO  [main] 2023-02-21 18:18:49,635 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:49,635 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:49,635 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:49,635 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:49,636 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Index.db 
INFO  [main] 2023-02-21 18:18:49,699 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Filter.db 
INFO  [main] 2023-02-21 18:18:49,699 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Summary.db 
INFO  [main] 2023-02-21 18:18:49,699 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Data.db 
INFO  [main] 2023-02-21 18:19:17,136 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Digest.crc32 
INFO  [main] 2023-02-21 18:19:17,137 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:19:17,253 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Statistics.db 
INFO  [main] 2023-02-21 18:19:17,253 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-TOC.txt 
INFO  [main] 2023-02-21 18:19:17,253 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Index.db 
INFO  [main] 2023-02-21 18:19:17,310 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Filter.db 
INFO  [main] 2023-02-21 18:19:17,310 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Data.db 
INFO  [main] 2023-02-21 18:19:36,881 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Summary.db 
INFO  [main] 2023-02-21 18:19:36,882 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Digest.crc32 
INFO  [main] 2023-02-21 18:19:36,882 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:19:36,883 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Statistics.db 
INFO  [main] 2023-02-21 18:19:36,883 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-TOC.txt 
INFO  [main] 2023-02-21 18:19:36,883 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Filter.db 
INFO  [main] 2023-02-21 18:19:36,884 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Index.db 
INFO  [main] 2023-02-21 18:19:36,917 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Summary.db 
INFO  [main] 2023-02-21 18:19:36,917 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Data.db 
INFO  [main] 2023-02-21 18:19:45,481 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Digest.crc32 
INFO  [main] 2023-02-21 18:19:45,482 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:19:45,483 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Statistics.db 
INFO  [main] 2023-02-21 18:19:45,483 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-TOC.txt 
INFO  [main] 2023-02-21 18:19:45,483 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Filter.db 
INFO  [main] 2023-02-21 18:19:45,570 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Index.db 
INFO  [main] 2023-02-21 18:19:45,639 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Summary.db 
INFO  [main] 2023-02-21 18:19:45,640 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Data.db 
INFO  [main] 2023-02-21 18:20:48,586 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Digest.crc32 
INFO  [main] 2023-02-21 18:20:48,587 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:20:48,618 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Statistics.db 
INFO  [main] 2023-02-21 18:20:48,618 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-TOC.txt 
INFO  [main] 2023-02-21 18:20:48,619 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Index.db 
INFO  [main] 2023-02-21 18:20:48,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Filter.db 
INFO  [main] 2023-02-21 18:20:48,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Summary.db 
INFO  [main] 2023-02-21 18:20:48,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Data.db 
INFO  [main] 2023-02-21 18:20:51,050 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Digest.crc32 
INFO  [main] 2023-02-21 18:20:51,051 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:20:51,051 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Statistics.db 
INFO  [main] 2023-02-21 18:20:51,051 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-TOC.txt 
INFO  [main] 2023-02-21 18:20:51,052 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Filter.db 
INFO  [main] 2023-02-21 18:20:51,061 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Index.db 
INFO  [main] 2023-02-21 18:20:51,121 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Summary.db 
INFO  [main] 2023-02-21 18:20:51,121 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Data.db 
INFO  [main] 2023-02-21 18:21:19,118 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Digest.crc32 
INFO  [main] 2023-02-21 18:21:19,118 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:21:19,140 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Statistics.db 
INFO  [main] 2023-02-21 18:21:19,140 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-TOC.txt 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Filter.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Index.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Summary.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Data.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Digest.crc32 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Statistics.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-TOC.txt 
INFO  [main] 2023-02-21 18:21:19,142 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Index.db 
INFO  [main] 2023-02-21 18:21:19,160 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Filter.db 
INFO  [main] 2023-02-21 18:21:19,160 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Data.db 
INFO  [main] 2023-02-21 18:21:22,503 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Summary.db 
INFO  [main] 2023-02-21 18:21:22,504 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Digest.crc32 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Statistics.db 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-TOC.txt 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Filter.db 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Index.db 
INFO  [main] 2023-02-21 18:21:22,660 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Data.db 
INFO  [main] 2023-02-21 18:22:11,241 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Summary.db 
INFO  [main] 2023-02-21 18:22:11,242 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:11,242 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:11,244 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:11,244 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:11,244 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Index.db 
INFO  [main] 2023-02-21 18:22:11,328 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Filter.db 
INFO  [main] 2023-02-21 18:22:11,335 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Summary.db 
INFO  [main] 2023-02-21 18:22:11,335 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Data.db 
INFO  [main] 2023-02-21 18:22:42,109 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:42,109 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:42,109 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:42,110 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:42,110 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Index.db 
INFO  [main] 2023-02-21 18:22:42,481 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Filter.db 
INFO  [main] 2023-02-21 18:22:42,481 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Summary.db 
INFO  [main] 2023-02-21 18:22:42,481 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Data.db 
INFO  [main] 2023-02-21 18:22:50,519 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:50,520 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:50,539 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:50,539 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:50,540 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Index.db 
INFO  [main] 2023-02-21 18:22:50,570 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Filter.db 
INFO  [main] 2023-02-21 18:22:50,570 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Data.db 
INFO  [main] 2023-02-21 18:22:50,729 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Summary.db 
INFO  [main] 2023-02-21 18:22:50,729 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:50,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:50,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:50,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:50,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Index.db 
INFO  [main] 2023-02-21 18:22:50,736 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Filter.db 
INFO  [main] 2023-02-21 18:22:50,736 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Summary.db 
INFO  [main] 2023-02-21 18:22:50,736 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Data.db 
INFO  [main] 2023-02-21 18:22:51,729 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:51,729 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:51,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:51,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:51,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Index.db 
INFO  [main] 2023-02-21 18:22:51,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Filter.db 
INFO  [main] 2023-02-21 18:22:51,759 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Summary.db 
INFO  [main] 2023-02-21 18:22:51,759 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Data.db 
INFO  [main] 2023-02-21 18:22:54,456 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:54,457 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:54,457 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:54,457 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:54,646 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_unknowncompactiontype_695c4f33-b1ce-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:22:54,648 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_60e393e0-b1ce-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:22:54,650 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830665-big-Index.db 
INFO  [main] 2023-02-21 18:22:54,656 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830665-big-Data.db 
INFO  [main] 2023-02-21 18:22:54,673 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_60e393e0-b1ce-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:22:54,694 Keyspace.java:386 - Creating replication strategy kairosdb params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=2}}
INFO  [main] 2023-02-21 18:22:54,715 ColumnFamilyStore.java:385 - Initializing kairosdb.data_points
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:54,720 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830647-big (179.084MiB)
INFO  [SSTableBatchOpen:5] 2023-02-21 18:22:54,721 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830662-big (4.039MiB)
INFO  [SSTableBatchOpen:7] 2023-02-21 18:22:54,721 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830663-big (3.589MiB)
INFO  [SSTableBatchOpen:6] 2023-02-21 18:22:54,721 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830661-big (39.789MiB)
INFO  [SSTableBatchOpen:8] 2023-02-21 18:22:54,721 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830664-big (6.007MiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:54,739 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830583-big (190.543MiB)
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:54,739 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830440-big (191.089MiB)
INFO  [SSTableBatchOpen:4] 2023-02-21 18:22:54,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830513-big (194.560MiB)
INFO  [main] 2023-02-21 18:22:54,947 ColumnFamilyStore.java:385 - Initializing kairosdb.row_key_index
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:54,977 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_key_index-8742543087ba11eba3799bdca9e7ad04/mc-1-big (7.580MiB)
INFO  [main] 2023-02-21 18:22:55,023 ColumnFamilyStore.java:385 - Initializing kairosdb.row_key_time_index
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,054 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_key_time_index-87a4234087ba11eba3799bdca9e7ad04/nb-26770-big (0.075KiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:55,070 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_key_time_index-87a4234087ba11eba3799bdca9e7ad04/nb-26769-big (0.052KiB)
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:55,077 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_key_time_index-87a4234087ba11eba3799bdca9e7ad04/nb-26768-big (2.671MiB)
INFO  [main] 2023-02-21 18:22:55,131 ColumnFamilyStore.java:385 - Initializing kairosdb.row_keys
INFO  [SSTableBatchOpen:5] 2023-02-21 18:22:55,135 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/nb-796510-big (7.682MiB)
INFO  [SSTableBatchOpen:4] 2023-02-21 18:22:55,190 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/nb-769597-big (50.002MiB)
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:55,203 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/mc-75-big (87.496MiB)
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,209 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/mc-256221-big (51.492MiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:55,211 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/nb-550752-big (50.323MiB)
INFO  [main] 2023-02-21 18:22:55,357 ColumnFamilyStore.java:385 - Initializing kairosdb.service_index
INFO  [main] 2023-02-21 18:22:55,381 ColumnFamilyStore.java:385 - Initializing kairosdb.spec
INFO  [main] 2023-02-21 18:22:55,393 ColumnFamilyStore.java:385 - Initializing kairosdb.string_index
INFO  [main] 2023-02-21 18:22:55,409 ColumnFamilyStore.java:385 - Initializing kairosdb.tag_indexed_row_keys
INFO  [main] 2023-02-21 18:22:55,419 Keyspace.java:386 - Creating replication strategy system_auth params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}
INFO  [main] 2023-02-21 18:22:55,425 ColumnFamilyStore.java:385 - Initializing system_auth.network_permissions
INFO  [main] 2023-02-21 18:22:55,440 ColumnFamilyStore.java:385 - Initializing system_auth.resource_role_permissons_index
INFO  [main] 2023-02-21 18:22:55,457 ColumnFamilyStore.java:385 - Initializing system_auth.role_members
INFO  [main] 2023-02-21 18:22:55,473 ColumnFamilyStore.java:385 - Initializing system_auth.role_permissions
INFO  [main] 2023-02-21 18:22:55,485 ColumnFamilyStore.java:385 - Initializing system_auth.roles
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,518 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_auth/roles-5bc52802de2535edaeab188eecebb090/mc-1-big (0.100KiB)
INFO  [main] 2023-02-21 18:22:55,543 Keyspace.java:386 - Creating replication strategy system_distributed params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=3}}
INFO  [main] 2023-02-21 18:22:55,558 ColumnFamilyStore.java:385 - Initializing system_distributed.parent_repair_history
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:55,577 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-49-big (1.398KiB)
INFO  [SSTableBatchOpen:24] 2023-02-21 18:22:55,585 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-44-big (1.376KiB)
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,591 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-24-big (0.863KiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:55,593 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-5-big (0.644KiB)
INFO  [SSTableBatchOpen:8] 2023-02-21 18:22:55,594 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-12-big (1.130KiB)
INFO  [SSTableBatchOpen:9] 2023-02-21 18:22:55,595 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-16-big (0.990KiB)
INFO  [SSTableBatchOpen:18] 2023-02-21 18:22:55,598 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-39-big (1.646KiB)
INFO  [SSTableBatchOpen:14] 2023-02-21 18:22:55,598 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-57-big (2.019KiB)
INFO  [SSTableBatchOpen:4] 2023-02-21 18:22:55,605 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-68-big (0.920KiB)
INFO  [SSTableBatchOpen:16] 2023-02-21 18:22:55,606 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-107-big (0.728KiB)
INFO  [SSTableBatchOpen:23] 2023-02-21 18:22:55,607 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-43-big (0.592KiB)
INFO  [SSTableBatchOpen:15] 2023-02-21 18:22:55,608 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-31-big (1.451KiB)
INFO  [SSTableBatchOpen:11] 2023-02-21 18:22:55,611 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-80-big (0.944KiB)
INFO  [SSTableBatchOpen:20] 2023-02-21 18:22:55,611 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-106-big (0.589KiB)
INFO  [SSTableBatchOpen:5] 2023-02-21 18:22:55,622 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-88-big (0.935KiB)
INFO  [SSTableBatchOpen:6] 2023-02-21 18:22:55,623 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-20-big (1.151KiB)
INFO  [SSTableBatchOpen:10] 2023-02-21 18:22:55,623 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-76-big (1.481KiB)
INFO  [SSTableBatchOpen:12] 2023-02-21 18:22:55,624 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/mc-1-big (16.284KiB)
INFO  [SSTableBatchOpen:17] 2023-02-21 18:22:55,626 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-95-big (3.737KiB)
INFO  [SSTableBatchOpen:13] 2023-02-21 18:22:55,626 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-84-big (1.031KiB)
INFO  [SSTableBatchOpen:7] 2023-02-21 18:22:55,633 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-64-big (1.132KiB)
INFO  [SSTableBatchOpen:19] 2023-02-21 18:22:55,636 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-100-big (0.561KiB)
INFO  [SSTableBatchOpen:21] 2023-02-21 18:22:55,656 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-99-big (1.159KiB)
INFO  [SSTableBatchOpen:22] 2023-02-21 18:22:55,683 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-104-big (0.720KiB)
INFO  [main] 2023-02-21 18:22:55,733 ColumnFamilyStore.java:385 - Initializing system_distributed.repair_history
INFO  [SSTableBatchOpen:14] 2023-02-21 18:22:55,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-45-big (2.741KiB)
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:55,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-25-big (1.789KiB)
INFO  [SSTableBatchOpen:7] 2023-02-21 18:22:55,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-89-big (1.814KiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:55,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-112-big (1.327KiB)
INFO  [SSTableBatchOpen:19] 2023-02-21 18:22:55,749 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-32-big (3.927KiB)
INFO  [SSTableBatchOpen:4] 2023-02-21 18:22:55,756 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-67-big (3.370KiB)
INFO  [SSTableBatchOpen:18] 2023-02-21 18:22:55,757 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-81-big (3.249KiB)
INFO  [SSTableBatchOpen:10] 2023-02-21 18:22:55,758 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-110-big (1.354KiB)
INFO  [SSTableBatchOpen:11] 2023-02-21 18:22:55,761 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-44-big (1.250KiB)
INFO  [SSTableBatchOpen:12] 2023-02-21 18:22:55,761 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-50-big (3.340KiB)
INFO  [SSTableBatchOpen:6] 2023-02-21 18:22:55,765 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-93-big (2.319KiB)
INFO  [SSTableBatchOpen:21] 2023-02-21 18:22:55,765 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-85-big (1.789KiB)
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,770 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-113-big (1.334KiB)
INFO  [SSTableBatchOpen:15] 2023-02-21 18:22:55,773 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-101-big (3.239KiB)
INFO  [SSTableBatchOpen:22] 2023-02-21 18:22:55,774 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-21-big (2.331KiB)
INFO  [SSTableBatchOpen:8] 2023-02-21 18:22:55,785 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-40-big (4.339KiB)
INFO  [SSTableBatchOpen:23] 2023-02-21 18:22:55,789 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-17-big (2.308KiB)
INFO  [SSTableBatchOpen:13] 2023-02-21 18:22:55,794 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-106-big (0.766KiB)
INFO  [SSTableBatchOpen:16] 2023-02-21 18:22:55,797 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-13-big (2.822KiB)
INFO  [SSTableBatchOpen:9] 2023-02-21 18:22:55,802 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-105-big (2.460KiB)
INFO  [SSTableBatchOpen:5] 2023-02-21 18:22:55,802 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-6-big (1.802KiB)
INFO  [SSTableBatchOpen:24] 2023-02-21 18:22:55,805 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-74-big (2.896KiB)
INFO  [SSTableBatchOpen:17] 2023-02-21 18:22:55,808 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/mc-1-big (0.812KiB)
INFO  [SSTableBatchOpen:20] 2023-02-21 18:22:55,811 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-59-big (5.973KiB) {code}
3.Bugs can be reproduced.Just set  vm.max_ map_ count as a small value, and then trigger OOM, and restart the node.",smiklosovic,naizhen,Normal,Resolved,Fixed,16/Mar/23 01:02,27/Apr/23 09:32
Bug,CASSANDRA-18337,13528706,Operations.migrateReadRequiredOperations fails due to concurrent access when TransactionStatement is prepared,"{code}
java.util.NoSuchElementException
	at java.base/java.util.ArrayList$Itr.next(ArrayList.java:1000)
	at org.apache.cassandra.cql3.Operations.migrateReadRequiredOperations(Operations.java:71)
	at org.apache.cassandra.cql3.Operations.migrateReadRequiredOperations(Operations.java:63)
	at org.apache.cassandra.cql3.statements.ModificationStatement.getTxnWriteFragment(ModificationStatement.java:828)
	at org.apache.cassandra.cql3.statements.TransactionStatement.createWriteFragments(TransactionStatement.java:290)
	at org.apache.cassandra.cql3.statements.TransactionStatement.createUpdate(TransactionStatement.java:309)
	at org.apache.cassandra.cql3.statements.TransactionStatement.createTxn(TransactionStatement.java:334)
	at org.apache.cassandra.cql3.statements.TransactionStatement.execute(TransactionStatement.java:375)
{code}

this was caused by having shared mutable state!  when we start creating the txn objects we would also mutate the mutations that had operations that need to be run in the txn, this has an issue when the txn is run from prepared statements as the object is shared by multiple threads, causing the array to be mutated while iterating.",dcapwell,dcapwell,Normal,Resolved,Fixed,16/Mar/23 01:08,11/Apr/23 20:27
Bug,CASSANDRA-18343,13529140,JDK17 - fix nodetool_test.TestNodetool.test_sjk," 
[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2321/workflows/de1f521d-c5cb-4ddd-bc45-9ec71b577bf3/jobs/19923/tests]
 
{code:java}
AssertionError: Expected 'SJK hh' output assert False == True self = <nodetool_test.TestNodetool object at 0x7f9ab6d95908> @since('4.0') def test_sjk(self): """""" Verify that SJK generally works. """""" cluster = self.cluster cluster.populate([1]).start() node = cluster.nodelist()[0] out, err, _ = node.nodetool('sjk --help') logger.debug(out) hasPattern = False for line in out.split(os.linesep): if "" ttop [Thread Top] Displays threads from JVM process"" == line: hasPattern = True assert hasPattern == True, ""Expected help about SJK ttop"" out, err, _ = node.nodetool('sjk') logger.debug(out) hasPattern = False for line in out.split(os.linesep): if "" ttop [Thread Top] Displays threads from JVM process"" == line: hasPattern = True assert hasPattern == True, ""Expected help about SJK ttop"" out, err, _ = node.nodetool('sjk hh -n 10 --live') logger.debug(out) hasPattern = False for line in out.split(os.linesep): if re.match('.*Instances.*Bytes.*Type.*', line): hasPattern = True > assert hasPattern == True, ""Expected 'SJK hh' output"" E AssertionError: Expected 'SJK hh' output E assert False == True nodetool_test.py:482: AssertionError{code}",e.dimitrova,e.dimitrova,Normal,Resolved,Fixed,19/Mar/23 19:28,23/Mar/23 19:11
Bug,CASSANDRA-18346,13529291,Error Unknown column during deserialization missing keyspace and table name,"The ERROR message generated in ColumnSubselection.java when a column name is not found only prints the column name, not the keyspace and table.  It can be difficult to track down the source when more than one table uses the same name.  E.g., 'id'.
{quote}{{if (column == null)}}
{
{{        column = metadata.getDroppedColumn(name);}}
{{        if (column == null)}}
{{                throw new UnknownColumnException(""Unknown column "" + UTF8Type.instance.getString(name) + "" during deserialization"");}}
{{}}}
{quote}
Example:

[ERROR] cluster_id=15 ip_address=192.168.65.10  java.lang.RuntimeException: Unknown column id during deserialization

Proposed:

[ERROR] cluster_id=15 ip_address=192.168.65.10  java.lang.RuntimeException: Unknown column id in table cycling.route during deserialization",manish.c.ghildiyal@gmail.com,bschoeni,Low,Resolved,Fixed,20/Mar/23 18:16,07/Jul/23 03:34
Bug,CASSANDRA-18353,13529504,"Cqlsh command ""COPY … TO STDOUT"" fails with ""… object is not callable""","Since 4.1.0, cqlsh fails on COPY commands with standard output as target. 

Steps to reproduce:
{noformat}
$> docker run -d --name cassandra --rm cassandra:4.1.1
$> docker exec cassandra cqlsh -e 'COPY system.local(cluster_name) TO STDOUT'
  <stdin>:1:'NoneType' object is not callable
{noformat}

Possibly a regression introduced by this commit [6341319|https://github.com/apache/cassandra/commit/634131961af9c1d88b34797c1c45000f71a76dae#diff-584645e6e932edd7a17f03c79ae87b1f1f8ed54919a31ce8785af156b89c0b76L260].

Also, it looks like it has happened before: CASSANDRA-12497",brandon.williams,gustavb,Normal,Resolved,Fixed,21/Mar/23 20:21,22/Mar/23 18:31
Bug,CASSANDRA-18354,13529517,Remove obsolete 'six' package reintroduced by a merge,"The 4.1.x and trunk commits for CASSANDRA-18088 inadvertently re-introduced 'six' and obsolete python 2.7 conditional checks and should be removed.

I.e., SaferScanner = Py36SaferScanner if six.PY3 else Py2SaferScanner – but there is no Py2SaferScanner anymore.",brandon.williams,bschoeni,Normal,Resolved,Fixed,21/Mar/23 23:15,22/Mar/23 15:24
Bug,CASSANDRA-18359,13529865,NullPointerException on SnapshotLoader.loadSnapshots,"Node startup fail with on 4.1.1:

{noformat}
INFO [main] 2023-03-23 18:13:13,585 MigrationCoordinator.java:257 - Starting migration coordinator and scheduling pulling schema versions every PT1M
ERROR [main] 2023-03-23 18:13:13,592 CassandraDaemon.java:898 - Exception encountered during startup
java.lang.NullPointerException: null
	at org.apache.cassandra.service.snapshot.SnapshotLoader$Visitor.preVisitDirectory(SnapshotLoader.java:106)
	at org.apache.cassandra.service.snapshot.SnapshotLoader$Visitor.preVisitDirectory(SnapshotLoader.java:77)
	at java.base/java.nio.file.Files.walkFileTree(Files.java:2732)
	at org.apache.cassandra.service.snapshot.SnapshotLoader.loadSnapshots(SnapshotLoader.java:162)
	at org.apache.cassandra.service.snapshot.SnapshotManager.loadSnapshots(SnapshotManager.java:114)
	at org.apache.cassandra.service.snapshot.SnapshotManager.start(SnapshotManager.java:88)
	at org.apache.cassandra.service.StorageService.startSnapshotManager(StorageService.java:1050)
	at org.apache.cassandra.service.StorageService.prepareToJoin(StorageService.java:1043)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:842)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:775)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:425)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:752)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:876)
{noformat}",smiklosovic,paulo,Normal,Resolved,Fixed,23/Mar/23 21:08,27/Mar/23 06:54
Bug,CASSANDRA-18363,13529991,Test failure: cqlsh_tests.test_cqlsh.TestCqlsh.test_list_queries,Looks like this needs to be updated for DDM: https://app.circleci.com/pipelines/github/driftx/cassandra/930/workflows/0dc6fa5b-65eb-4ea8-9238-2d78b889d9bc/jobs/16238/tests,,brandon.williams,Normal,Resolved,Fixed,24/Mar/23 15:14,24/Mar/23 15:24
Bug,CASSANDRA-18368,13530240,Redhat 40x repo signature is not valid,"Looks like part of the packaging upload troubles we had with the 4.0.8 release:

{noformat}
gpg --verify repomd.xml.asc repomd.xml
gpg: Signature made Wed 19 Oct 2022 20:19:43 AEDT
gpg:                using RSA key A4C465FEA0C552561A392A61E91335D77E3E87CB
gpg: BAD signature from ""Michael Semb Wever <mick@thelastpickle.com>"" [unknown]
{noformat}",brandon.williams,brandon.williams,Normal,Resolved,Fixed,27/Mar/23 12:14,27/Mar/23 15:05
Bug,CASSANDRA-18370,13530347,BulkLoader tool initializes schema unnecessarily via streaming - 4.0,"Similar to CASSANDRA-17740, {{BulkLoader}} initializes the schema/system keyspace, which caused the issue in bulk loader tool. See more details in CASSANDRA-17740. Please help fix the issue in 4.0 branch as well. Thanks.",maedhroz,yijunm_ts,Normal,Resolved,Fixed,28/Mar/23 06:13,31/Mar/23 17:03
Bug,CASSANDRA-18371,13530417,Snapshots with dots in their name are not returned in listsnapshots,,smiklosovic,smiklosovic,Normal,Resolved,Fixed,28/Mar/23 11:53,29/Mar/23 06:57
Bug,CASSANDRA-18375,13530496,CEP-15 (Accord) Expected reply message with verb ACCORD_INFORM_OF_TXNID_RSP but got ACCORD_SIMPLE_RSP,"{code}
java.lang.IllegalArgumentException: Expected reply message with verb ACCORD_INFORM_OF_TXNID_RSP but got ACCORD_SIMPLE_RSP
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:440)
	at org.apache.cassandra.service.accord.AccordMessageSink.reply(AccordMessageSink.java:125)
	at accord.local.Node.reply(Node.java:361)
	at accord.messages.AbstractEpochRequest.accept(AbstractEpochRequest.java:63)
	at accord.messages.InformOfTxnId.accept(InformOfTxnId.java:66)
	at accord.messages.InformOfTxnId.accept(InformOfTxnId.java:31)
	at org.apache.cassandra.service.accord.async.AsyncOperation.finish(AsyncOperation.java:165)
	at org.apache.cassandra.service.accord.async.AsyncOperation.finish(AsyncOperation.java:176)
	at org.apache.cassandra.service.accord.async.AsyncOperation.runInternal(AsyncOperation.java:247)
	at org.apache.cassandra.service.accord.async.AsyncOperation.run(AsyncOperation.java:268)
	at org.apache.cassandra.service.accord.async.AsyncOperation.callback(AsyncOperation.java:157)
	at accord.utils.async.AsyncCallbacks.lambda$null$0(AsyncCallbacks.java:31)
{code}",dcapwell,dcapwell,Normal,Resolved,Fixed,28/Mar/23 20:51,11/Apr/23 20:26
Bug,CASSANDRA-18377,13530498,CEP-15 (Accord) AsyncOperation can not fail as it has already reached FINISHED,"{code}
java.lang.IllegalArgumentException: Unexpected state FINISHED
	at accord.utils.Invariants.illegalArgument(Invariants.java:54)
	at accord.utils.Invariants.checkArgument(Invariants.java:202)
	at org.apache.cassandra.service.accord.async.AsyncOperation.fail(AsyncOperation.java:182)
	at org.apache.cassandra.service.accord.async.AsyncOperation.run(AsyncOperation.java:273)
{code}

And

{code}
java.lang.IllegalArgumentException: Unexpected state FINISHED
	at accord.utils.Invariants.illegalArgument(Invariants.java:54)
	at accord.utils.Invariants.checkArgument(Invariants.java:202)
	at org.apache.cassandra.service.accord.async.AsyncOperation.fail(AsyncOperation.java:182)
	at org.apache.cassandra.service.accord.async.AsyncOperation.run(AsyncOperation.java:273)
	at org.apache.cassandra.service.accord.async.AsyncOperation.callback(AsyncOperation.java:157)
	at accord.utils.async.AsyncCallbacks.lambda$null$0(AsyncCallbacks.java:31)
{code}",,dcapwell,Normal,Resolved,Fixed,28/Mar/23 20:55,05/Apr/23 16:36
Bug,CASSANDRA-18378,13530499,CEP-15 (Accord) accord.messages.Defer rejects Recurrent retry of Commit,"{code}
java.lang.IllegalStateException: Recurrent retry of Commit{…}
	at accord.messages.Defer.add(Defer.java:63)
	at accord.messages.Commit.apply(Commit.java:167)
	at accord.messages.Commit.apply(Commit.java:42)
	at org.apache.cassandra.service.accord.async.AsyncOperation$ForFunction.apply(AsyncOperation.java:321)
	at org.apache.cassandra.service.accord.async.AsyncOperation$ForFunction.apply(AsyncOperation.java:308)
	at org.apache.cassandra.service.accord.async.AsyncOperation.runInternal(AsyncOperation.java:226)
	at org.apache.cassandra.service.accord.async.AsyncOperation.run(AsyncOperation.java:268)
{code}",dcapwell,dcapwell,Normal,Resolved,Fixed,28/Mar/23 20:56,07/Apr/23 21:00
Bug,CASSANDRA-18389,13530651,jackson-core-2.13.2.jar vulnerability: CVE-2022-45688,This is currently failing in the OWASP scan.,brandon.williams,brandon.williams,Normal,Resolved,Fixed,29/Mar/23 16:03,03/Apr/23 11:11
Bug,CASSANDRA-18391,13530860,consistent timeout: dtest-upgrade.upgrade_tests.cql_tests.cls.test_cql3_non_compound_range_tombstones on trunk,"Failed 30 times in the last 30 runs. Flakiness: 0%, Stability: 0%

link: https://ci-cassandra.apache.org/job/Cassandra-trunk/1511/testReport/dtest-upgrade.upgrade_tests.cql_tests/cls/test_cql3_non_compound_range_tombstones/
Error message: failed on setup with ""Failed: Timeout >900.0s""",brandon.williams,jmckenzie,Normal,Resolved,Fixed,30/Mar/23 19:50,18/Apr/23 14:51
Bug,CASSANDRA-18395,13530942,Rename internal state() method in AbstractFuture to not conflict with Java 19 changes,"From Java 19 we have new method Future.state and it conflicts with our private method. Fix bellow. 

[https://download.java.net/java/early_access/jdk20/docs/api/java.base/java/util/concurrent/Future.html#state()]

 
{code:java}
+++ b/src/java/org/apache/cassandra/utils/concurrent/AbstractFuture.java
@@ -494,11 +494,11 @@ public abstract class AbstractFuture<V> implements Future<V>
     public String toString()
     {
         String description = description();
-        String state = state();
+        String state = stateInfo();
         return description == null ? state : (state + ' ' + description);
     }
 
-    private String state()
+    private String stateInfo()
     {
         Object result = this.result;
         if (isSuccess(result)){code}",slachiewicz,slachiewicz,Normal,Resolved,Fixed,31/Mar/23 09:27,10/Apr/23 18:04
Bug,CASSANDRA-18403,13531191,CEP-21 Always populate local gossip state at startup,"Properly initialise the gossip state for the local node during startup
 ",marcuse,samt,Normal,Resolved,Fixed,03/Apr/23 08:31,04/Apr/23 07:59
Bug,CASSANDRA-18422,13531343,"CEP-15 (Accord) Original and recover coordinators may hit a race condition with PreApply where reads and writes are interleaved, causing one of the coordinators to see the writes from the other","While verifying CASSANDRA-18364 I saw the following history violation in simulator

{code}
[junit-timeout] Testcase: simulationTest(org.apache.cassandra.simulator.test.ShortAccordSimulationTest)-.jdk1.8:        Caused an ERROR
[junit-timeout] Failed on seed 0xadaca81151490353
[junit-timeout] org.apache.cassandra.simulator.SimulationException: Failed on seed 0xadaca81151490353
[junit-timeout] Caused by: java.lang.AssertionError: History violations detected
[junit-timeout]         at org.apache.cassandra.simulator.paxos.PaxosSimulation.logAndThrow(PaxosSimulation.java:315)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.PaxosSimulation.isDone(PaxosSimulation.java:278)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.PaxosSimulation$2.hasNext(PaxosSimulation.java:249)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.PaxosSimulation.run(PaxosSimulation.java:224)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.AbstractPairOfSequencesPaxosSimulation.run(AbstractPairOfSequencesPaxosSimulation.java:297)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.PairOfSequencesAccordSimulation.run(PairOfSequencesAccordSimulation.java:62)
[junit-timeout]         at org.apache.cassandra.simulator.SimulationRunner$Run.run(SimulationRunner.java:374)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.AccordSimulationRunner$Run.run(AccordSimulationRunner.java:39)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.AccordSimulationRunner$Run.run(AccordSimulationRunner.java:30)
[junit-timeout]         at org.apache.cassandra.simulator.SimulationRunner$BasicCommand.run(SimulationRunner.java:355)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.AccordSimulationRunner.main(AccordSimulationRunner.java:76)
[junit-timeout]         at org.apache.cassandra.simulator.test.ShortAccordSimulationTest.simulationTest(ShortAccordSimulationTest.java:32)
[junit-timeout]         Suppressed: org.apache.cassandra.simulator.paxos.HistoryViolation: Inconsistent sequences on 1: [2, 0, 1, 6, 8, 9, 13, 14, 16, 19, 20, 22, 23, 25, 26, 28, 29, 31, 32, 34, 35, 37, 40, 43, 47, 48, 49, 54, 56, 57, 58, 60, 64, 68, 70, 71, 74, 76, 79, 80, 83, 85, 87, 87] vs [2, 0, 1, 6, 8, 9, 13, 14, 16, 19, 20, 22, 23, 25, 26, 28, 29, 31, 32, 34, 35, 37, 40, 43, 47, 48, 49, 54, 56, 57, 58, 60, 64, 68, 70, 71, 74, 76, 79, 80, 83, 85, 87]+90
[junit-timeout]                 at accord.verify.StrictSerializabilityVerifier$Register.updateSequence(StrictSerializabilityVerifier.java:607)
[junit-timeout]                 at accord.verify.StrictSerializabilityVerifier$Register.access$100(StrictSerializabilityVerifier.java:576)
[junit-timeout]                 at accord.verify.StrictSerializabilityVerifier.apply(StrictSerializabilityVerifier.java:825)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.StrictSerializabilityValidator$1.lambda$close$0(StrictSerializabilityValidator.java:66)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.StrictSerializabilityValidator.convertHistoryViolation(StrictSerializabilityValidator.java:89)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.StrictSerializabilityValidator.access$200(StrictSerializabilityValidator.java:27)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.StrictSerializabilityValidator$1.close(StrictSerializabilityValidator.java:66)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.LoggingHistoryValidator$1.close(LoggingHistoryValidator.java:63)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.PairOfSequencesAccordSimulation$ReadWriteOperation.verify(PairOfSequencesAccordSimulation.java:218)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.PaxosSimulation$Operation.accept(PaxosSimulation.java:135)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.PairOfSequencesAccordSimulation$ReadWriteOperation.accept(PairOfSequencesAccordSimulation.java:171)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.PaxosSimulation$Operation.accept(PaxosSimulation.java:83)
[junit-timeout]                 at org.apache.cassandra.simulator.systems.SimulatedActionCallable$1.run(SimulatedActionCallable.java:47)
[junit-timeout]                 at org.apache.cassandra.simulator.systems.InterceptingExecutor$InterceptingPooledExecutor$WaitingThread.lambda$new$1(InterceptingExecutor.java:317)
[junit-timeout]                 at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[junit-timeout]                 at java.lang.Thread.run(Thread.java:750)
{code}

Adding logging to track message passing, reads, and writes, I have the following ordering

{code}
[isolatedExecutor:3]  node3 2023-04-03 12:54:30,200 send(/127.0.0.1:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ))
[isolatedExecutor:3]  node3 2023-04-03 12:54:30,200 send(/127.0.0.2:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ))
[isolatedExecutor:3]  node3 2023-04-03 12:54:30,200 send(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ))
[CommandStore[2]:1]   node1 2023-04-03 12:54:30,208 CS:[2] OP:0xea64a268 reply(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ), (from:/127.0.0.1:7012, type:REQUEST_RESPONSE verb:ACCORD_PREACCEPT_RSP))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,209 CS:[2] OP:0x9761fb36 reply(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_PREACCEPT_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,210 CS:[2] OP:0xe62230f2 reply(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ), (from:/127.0.0.2:7012, type:REQUEST_RESPONSE verb:ACCORD_PREACCEPT_RSP))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,211 CS:[2] OP:0xc5563e5d send(/127.0.0.1:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,212 CS:[2] OP:0xc5563e5d send(/127.0.0.2:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,212 CS:[2] OP:0xc5563e5d send(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,329 CS:[2] OP:0xa3e62850 send(/127.0.0.1:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,329 CS:[2] OP:0xa3e62850 send(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,329 CS:[2] OP:0xa3e62850 send(/127.0.0.3:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,334 CS:[2] OP:0xf8562cfb reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_RECOVER_RSP))
[CommandStore[2]:1]   node1 2023-04-03 12:54:30,338 CS:[2] OP:0xcfd2540f reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ), (from:/127.0.0.1:7012, type:REQUEST_RESPONSE verb:ACCORD_RECOVER_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,338 CS:[2] OP:0xc4cf5af8 reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ), (from:/127.0.0.2:7012, type:REQUEST_RESPONSE verb:ACCORD_RECOVER_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,340 CS:[2] OP:0x9e4f00f0 send(/127.0.0.1:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,340 CS:[2] OP:0x9e4f00f0 send(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,340 CS:[2] OP:0x9e4f00f0 send(/127.0.0.3:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,343 CS:[2] OP:0xb60153ab reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_GET_DEPS_RSP))
[CommandStore[2]:1]   node1 2023-04-03 12:54:30,344 CS:[2] OP:0xac20a1d6 reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ), (from:/127.0.0.1:7012, type:REQUEST_RESPONSE verb:ACCORD_GET_DEPS_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,345 CS:[2] OP:0xa73f7484 reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ), (from:/127.0.0.2:7012, type:REQUEST_RESPONSE verb:ACCORD_GET_DEPS_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,347 CS:[2] OP:0xfc37fb1a send(/127.0.0.1:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,347 CS:[2] OP:0xfc37fb1a send(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,347 CS:[2] OP:0xfc37fb1a send(/127.0.0.3:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,349 CS:[2] OP:0xff574276 Performing read
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,349 CS:[2] OP:0xff574276 Performing read
[ReadStage:1]         node3 2023-04-03 12:54:30,351 Performing read; post
[ReadStage:1]         node3 2023-04-03 12:54:30,351 Performing read; post
[ReadStage:1]         node3 2023-04-03 12:54:30,351 Performing read; pre
[ReadStage:1]         node3 2023-04-03 12:54:30,351 Performing read; pre
[ReadStage:1]         node3 2023-04-03 12:54:30,351 reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_READ_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,359 Performing coordinated write
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,359 send(/127.0.0.1:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,359 send(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,360 send(/127.0.0.3:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,363 CS:[2] OP:0x8bdb6795 Performing read
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,363 CS:[2] OP:0x8bdb6795 Performing read
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,364 CS:[2] OP:0x92e94460 Performing write
[MutationStage:4]     node3 2023-04-03 12:54:30,364 Performing write: pre
[MutationStage:4]     node3 2023-04-03 12:54:30,365 Performing write: post
[ReadStage:1]         node3 2023-04-03 12:54:30,365 Performing read; post
[ReadStage:1]         node3 2023-04-03 12:54:30,365 Performing read; pre
[ReadStage:1]         node3 2023-04-03 12:54:30,369 Performing read; post
[ReadStage:1]         node3 2023-04-03 12:54:30,369 Performing read; pre
[ReadStage:1]         node3 2023-04-03 12:54:30,369 reply(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_READ_RSP))
[CommandStore[2]:1]   node1 2023-04-03 12:54:30,370 CS:[2] OP:0xa59dc286 Performing write
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,374 CS:[2] OP:0xab0f3ca4 Performing write
[MutationStage:1]     node2 2023-04-03 12:54:30,374 Performing write: pre
[MutationStage:1]     node2 2023-04-03 12:54:30,375 Performing write: post
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,376 Performing coordinated write
[MutationStage:3]     node1 2023-04-03 12:54:30,376 Performing write: pre
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,377 send(/127.0.0.1:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,377 send(/127.0.0.2:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,377 send(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,382 reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_APPLY_RSP))
[MutationStage:3]     node1 2023-04-03 12:54:30,382 Performing write: post
{code}

(The transaction has a returning select and an auto-read, which is why there are double logs for reads)

Here we see the following timing

{code}
T00 node3 starts txn
T01 node3 sends COMMIT
T02 node2 starts recover
T03 all nodes ack to the pending recover
T04 node2 sends COMMIT
T05 node3 performs reads needed for txn
T06 node3 sends read results to node2
T07 node2 performs write locally and send APPLY
T08 node3 performs write
T09 node3 performs reads needed for txn
T10 node3 send reads to node3
T11 node3 performs write and sends APPLY
T12 node3 ACKs APPLY to node2
{code}

Given the fact the simulator got a response back and didn’t get a preempt, this implies that the original coordinator was able to complete the full transaction without issues and reply back, but the reads/writes were interleaved between node3 and node2 causing the second write to observe the first write",dcapwell,dcapwell,Normal,Resolved,Fixed,03/Apr/23 20:54,11/Apr/23 20:25
Bug,CASSANDRA-18431,13531653,Cassandra doesn't start on JDK17,"CASSANDRA-17199 added a new call to jamm measureDeep which hits some JDK internals and prevents us from starting Cassandra with JDK17. This will be solved with CASSANDRA-18329.

Until new version of jamm lands we can workaround the problem either by adding add-opens or by replacing temporarily

{code:java}
public static final long IPV6_SOCKET_ADDRESS_SIZE = ObjectSizes.measureDeep(new InetSocketAddress(getIpvAddress(16), 42));
{code}

with

{code:java}
public static final long IPV6_SOCKET_ADDRESS_SIZE = 168;
{code}

measured with JOL.

Then we can switch back to the current call [here|https://github.com/apache/cassandra/commit/4444721b6de555352bf0ac3ef7e36f94dc832f41#diff-1122d7d3efe9721af7244d373e66378f7e90cb05fd65859a52e8a3ea58a7c8f9R45] later.",e.dimitrova,e.dimitrova,Normal,Resolved,Fixed,05/Apr/23 19:44,06/Apr/23 19:08
Bug,CASSANDRA-18436,13531898,Unit tests in org.apache.cassandra.cql3.EmptyValuesTest class occasionally failing with JDK17," 

All of them failed with the below stack trace for the same assertion failing:
{code:java}
junit.framework.AssertionFailedError: at org.apache.cassandra.cql3.EmptyValuesTest.verify(EmptyValuesTest.java:90) at org.apache.cassandra.cql3.EmptyValuesTest.verifyJsonInsert(EmptyValuesTest.java:112) at org.apache.cassandra.cql3.EmptyValuesTest.testEmptyDecimal(EmptyValuesTest.java:192) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
 

Unfortunately I do not have a link to the CI run as this was seen last in private infra and not in CircleCI. Maybe we want to check with the multiplexer for flakiness. ",djatnieks,e.dimitrova,Normal,Resolved,Fixed,07/Apr/23 18:10,12/May/23 15:11
Bug,CASSANDRA-18437,13531906,Fix org.apache.cassandra.transport.MessagePayloadTest-.jdk17,"[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2182/workflows/e3dc630b-b7a4-4f5b-8f29-489bf43ad90f]
{code:java}
java.lang.RuntimeException: java.lang.NoSuchFieldException: modifiers at org.apache.cassandra.transport.MessagePayloadTest.resetCqlQueryHandlerField(MessagePayloadTest.java:98) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) Caused by: java.lang.NoSuchFieldException: modifiers at java.base/java.lang.Class.getDeclaredField(Class.java:2610) at org.apache.cassandra.transport.MessagePayloadTest.resetCqlQueryHandlerField(MessagePayloadTest.java:88){code}",e.dimitrova,e.dimitrova,Normal,Resolved,Fixed,07/Apr/23 20:36,10/Apr/23 15:27
Bug,CASSANDRA-18448,13532568,"Missing ""SSTable Count"" metric  when using nodetool with ""--format"" option","Hi, 

I'm using ""nodetool cfstats --format json"" to gather some metrics/infomation about our tables. 
I noticed that the ""SSTable Count"" is missing when using ""–format"" option. 

If I don't use ""–format""  option, I can set ""SSTable Count"" in the output. 

*Output of ""nodetool cfstats --format json | jq"":* 
{code:java}
{  ""total_number_of_tables"": 38,  ""stress_test"": {    ""write_latency_ms"": 0.8536725334338424,    ""tables"": {      ""res1"": {        ""average_tombstones_per_slice_last_five_minutes"": null,        ""bloom_filter_off_heap_memory_used"": ""159256"",        ""memtable_switch_count"": 754,        ""maximum_tombstones_per_slice_last_five_minutes"": 0,        ""memtable_cell_count"": 0,        ""memtable_data_size"": ""0"",        ""average_live_cells_per_slice_last_five_minutes"": null,        ""local_read_latency_ms"": ""NaN"",        ""local_write_latency_ms"": ""NaN"",        ""pending_flushes"": 0,        ""compacted_partition_minimum_bytes"": 785940,        ""local_read_count"": 0,        ""sstable_compression_ratio"": 0.6294161376582798,        ""dropped_mutations"": ""52751"",        ""bloom_filter_false_positives"": 0,        ""off_heap_memory_used_total"": ""58842196"",        ""memtable_off_heap_memory_used"": ""0"",        ""index_summary_off_heap_memory_used"": ""18972"",        ""bloom_filter_space_used"": ""159408"",        ""sstables_in_each_level"": [],        ""compacted_partition_maximum_bytes"": 4055269,        ""space_used_total"": ""302694398635"",        ""local_write_count"": 297111,        ""compression_metadata_off_heap_memory_used"": ""58663968"",        ""number_of_partitions_estimate"": 99614,        ""maximum_live_cells_per_slice_last_five_minutes"": 0,        ""space_used_live"": ""302694398635"",        ""compacted_partition_mean_bytes"": 3827283,        ""bloom_filter_false_ratio"": ""0.00000"",        ""percent_repaired"": 0,        ""space_used_by_snapshots_total"": ""0""      }    },    ""read_latency_ms"": null,    ""pending_flushes"": 0,    ""write_count"": 594308,    ""read_latency"": null,    ""read_count"": 0  }}
 {code}
*Output of ""nodetool cfstats"":* 
{code:java}
----------------
Keyspace : stress_test
        Read Count: 0
        Read Latency: NaN ms
        Write Count: 594308
        Write Latency: 0.8536725334338424 ms
        Pending Flushes: 0
                Table: res1
                SSTable count: 19                
                Space used (live): 302694398635
                Space used (total): 302694398635
                Space used by snapshots (total): 0
                Off heap memory used (total): 58842196
                SSTable Compression Ratio: 0.6294161376582798
                Number of partitions (estimate): 99614
                Memtable cell count: 0
                Memtable data size: 0
                Memtable off heap memory used: 0
                Memtable switch count: 754
                Local read count: 0
                Local read latency: NaN ms
                Local write count: 297111
                Local write latency: NaN ms
                Pending flushes: 0
                Percent repaired: 0.0
                Bloom filter false positives: 0
                Bloom filter false ratio: 0.00000
                Bloom filter space used: 159408
                Bloom filter off heap memory used: 159256
                Index summary off heap memory used: 18972
                Compression metadata off heap memory used: 58663968
                Compacted partition minimum bytes: 785940
                Compacted partition maximum bytes: 4055269
                Compacted partition mean bytes: 3827283
                Average live cells per slice (last five minutes): NaN
                Maximum live cells per slice (last five minutes): 0
                Average tombstones per slice (last five minutes): NaN
                Maximum tombstones per slice (last five minutes): 0
                Dropped Mutations: 52751*  
----------------
 {code}
 ",adelapena,Aburadeh,Normal,Resolved,Fixed,13/Apr/23 19:58,17/Apr/23 18:35
Bug,CASSANDRA-18471,13533467,CEP-15 Accord: NotWitnessed commands can receive an invalidate promise but would return Zero instead,"While working on CASSANDRA-18451 I hit the following failure

{code}
Failed on seed -5929214838499924343
accord.burn.SimulationException: Failed on seed -5929214838499924343
Caused by: java.lang.AssertionError: Unexpected exception encountered
	at accord.impl.basic.PropagatingPendingQueue.poll(PropagatingPendingQueue.java:73)
	at accord.impl.basic.Cluster.processPending(Cluster.java:179)
	at accord.impl.basic.Cluster.run(Cluster.java:296)
	at accord.burn.BurnTest.burn(BurnTest.java:309)
	at accord.burn.BurnTest.run(BurnTest.java:386)
	at accord.burn.BurnTest.testOne(BurnTest.java:372)
	Suppressed: java.lang.IllegalStateException: Received replies from a node that must have known the route, but that did not include it
		at accord.coordinate.Invalidate.invalidate(Invalidate.java:204)
		at accord.coordinate.Invalidate.handle(Invalidate.java:131)
		at accord.coordinate.Invalidate.onSuccess(Invalidate.java:105)
		at accord.coordinate.Invalidate.onSuccess(Invalidate.java:51)
		at accord.impl.basic.Cluster.lambda$processNext$1(Cluster.java:209)
		at accord.impl.basic.Cluster.now(Cluster.java:260)
		at accord.impl.basic.Cluster.processNext(Cluster.java:206)
		at accord.impl.basic.Cluster.processPending(Cluster.java:183)
{code}

In a debugger was able to figure out the state and create a unit test to hit the same situation

{code}
class InvalidateTest
{
    @Test
    void test() throws ExecutionException
    {
        try (MockCluster cluster = MockCluster.builder().replication(2).nodes(2).build())
        {
            Node n1 = cluster.get(1);
            Node n2 = cluster.get(2);

            RoutingKey n1RoutingKey = n1.topology().current().get(0).range.end();
            IntKey.Raw n1key = IntKey.key(((IntKey.Routing) n1RoutingKey).key);

            RoutingKey n2RoutingKey = n1.topology().current().get(1).range.end();
            IntKey.Raw n2key = IntKey.key(((IntKey.Routing) n2RoutingKey).key);

            Keys keys = Keys.of(n1key, n2key);


            Node coordinator = n1;
            TxnId txnId = coordinator.nextTxnId(Txn.Kind.Read, Routable.Domain.Key);
            Txn txn = readOnly(keys);

            AsyncChains.getUninterruptibly(n2.commandStores().unsafeForKey(n2key).execute(PreLoadContext.contextFor(txnId, keys), store -> {
                Ranges ranges = store.ranges().currentRanges();
                PartialTxn partial = txn.slice(ranges, true);
                FullKeyRoute route = keys.toRoute(n2RoutingKey);
//                RoutingKey progressKey = n2RoutingKey.toUnseekable(); // if this is non-null this passes
                RoutingKey progressKey = null;
                CheckedCommands.preaccept(store, txnId, partial, route, progressKey);
                CheckedCommands.accept(store, txnId, Ballot.ZERO, route.slice(ranges), partial.keys().slice(ranges), progressKey, txnId, PartialDeps.builder(ranges).build());
            }));
            AsyncChains.getUninterruptibly(new AsyncChains.Head<Outcome>() {
                @Override
                protected void start(BiConsumer<? super Outcome, Throwable> callback) {
                    Invalidate.invalidate(coordinator, txnId, keys.toUnseekables(), callback);
                }
            });
        }
    }

    private static Txn readOnly(Seekables<?, ?> keys)
    {
        Read read = MockStore.read(keys);
        Query query = Mockito.mock(Query.class);
        return new Txn.InMemory(keys, read, query);
    }
}
{code}",dcapwell,dcapwell,Normal,Resolved,Fixed,20/Apr/23 23:04,25/Apr/23 16:39
Bug,CASSANDRA-18472,13533567,Docker images can no longer be built due to virtualenv from pip,"{noformat}
 => [linux/amd64 35/56] WORKDIR /home/cassandra                                                                                                                                                              0.1s
 => [linux/amd64 36/56] RUN echo 'export ANT_HOME=/usr/share/ant' >> /home/cassandra/.bashrc &&     echo 'export JAVA8_HOME=/usr/lib/jvm/java-8-openjdk-$(dpkg --print-architecture)' >> /home/cassandra/.b  0.2s
 => ERROR [linux/amd64 37/56] RUN virtualenv --python=python2.7 env2.7                                                                                                                                       0.5s
------
 > [linux/amd64 37/56] RUN virtualenv --python=python2.7 env2.7:
#100 0.424 RuntimeError: failed to find interpreter for Builtin discover of python_spec='python2.7'
------
ubuntu2004_j11.docker:128
--------------------
 126 |     # included in the base image, the compiled objects are not updated by pip at run time, which can
 127 |     # cause errors if the tests rely on new driver functionality or bug fixes.
 128 | >>> RUN virtualenv --python=python2.7 env2.7
 129 |     RUN chmod +x env2.7/bin/activate
 130 |     RUN /bin/bash -c ""export CASS_DRIVER_NO_CYTHON=1 CASS_DRIVER_NO_EXTENSIONS=1 && source ~/env2.7/bin/activate && pip2 install --upgrade pip && pip2 install -r /opt/requirements.txt && pip2 freeze --user""
--------------------
error: failed to solve: rpc error: code = Unknown desc = process ""/bin/sh -c virtualenv --python=python2.7 env2.7"" did not complete successfully: exit code: 1
{noformat}",brandon.williams,brandon.williams,Normal,Resolved,Fixed,21/Apr/23 14:09,05/May/23 10:18
Bug,CASSANDRA-18477,13533829,Do not require allow filtering when all primary keys are specified in SELECT ,"This was discussed in

https://lists.apache.org/thread/loj6jgv54szdvyt3wmvbtwwrrg1dtlxq

Basically, when I have this table:

{code}
create table ks.tb (p1 int, c1 int, col1 int, col2 int, primary key (p1, c1));
{code}

and I do this

{code}
select * from ks.tb where p1 = 1 and c1 = 2 and col2 = 1;
{code}

this will fail and it will require to use ALLOW FILTERING just because we are also specifying ""col2"". This is clearly a bug - there is no reason to require it if we are going to fetch one row only as all partition and clustering keys were specified.",smiklosovic,smiklosovic,Normal,Resolved,Fixed,24/Apr/23 11:19,24/Apr/23 11:28
Bug,CASSANDRA-18497,13534999,snakeyaml vulnerability: CVE-2023-2251,"This is failing the OWASP scan.

https://nvd.nist.gov/vuln/detail/CVE-2023-2251",brandon.williams,brandon.williams,Normal,Resolved,Fixed,04/May/23 14:01,04/May/23 15:30
Bug,CASSANDRA-18505,13535395,NPE when deserializing malformed collections from client,"When deserializing collections sent from the client, if an element in the collection is incorrectly serialized, Collections.getValue can return null if the length of the element is negative.  Currently this isn't detected and serialization continues, calling validate and throwing an NPE in serializers that don't handle null value buffers.

Detect the malformed input and throw a better MarshalException so it will be converted to an InvalidRequestException for the client.
",jonmeredith,jonmeredith,Normal,Resolved,Fixed,08/May/23 16:32,10/May/23 00:13
Bug,CASSANDRA-18507,13535502,Partial compaction can resurrect deleted data,"If there isn't enough disk space available to compact all existing sstables, Cassandra will attempt to perform a partial compaction by removing sstables from the set of candidate sstables to be compacted, starting with the largest one. It is possible that the sstable removed from the set of sstables to compact contains data for which there are tombstones in another (more recent) sstable. Since the overlaps between sstables is computed when the {{CompactionController}} is created, and the {{CompactionController}} is created before the removal of any sstables from the set of sstables to be compacted this computed overlap will be outdated when checking which sstables are covered by certain tombstones. This leads to the faulty conclusion that the tombstones can be pruned during the compaction, causing the data to be resurrected.

The issue is present in Cassandra 4.0 and 4.1. Cassandra 3.11 creates the {{CompactionController}} after the set of sstables to compact has been reduced, and is thus not affected. {{trunk}} does not appear to support partial compactions at all, but instead refuses to compact when the disk is full.

This regression appears to have been introduced by CASSANDRA-13068.",toblin,toblin,Normal,Resolved,Fixed,09/May/23 10:19,18/May/23 17:44
Bug,CASSANDRA-18512,13535590,nodetool describecluster command is not showing correct Down count.  ,"There are some nodes down in the cluster of Cassandra Version 4.x

# nodetool describecluster command output shows these ips as unreachable.

UNREACHABLE: [<ip1>, <ip2>, <ip3>]

Stats for all nodes:
        Live: 3
        Joining: 0
        Moving: 0
        Leaving: 0
        Unreachable: 3

But under data center , count of down pod is always shown as 0.

Data Centers: 
    dc1 #Nodes: 3 #Down: 0
    dc2 #Nodes: 3 #Down: 0

 

Steps to reproduce:
 # Setup two Data centers dc1,dc2, each datacenter was having 3 nodes - dc1:3,dc2:3
 # mark down any 3 nodes of two data centers.
 # Run nodetool describecluster command from the live node and check the Unreachable count , which is 3 and Down Count is 0 , both are not matched.

 

Expected Output: Unreachable and Down count should have the same value.

Data Centers:
        dc1 #Nodes: 3 #Down: 1

        dc2 #Nodes: 3 #Down: 2

 

 ",ranju,ranju,Normal,Resolved,Fixed,10/May/23 05:27,05/Jun/23 14:07
Bug,CASSANDRA-18540,13536924,"negotiatedProtocolMustBeAcceptedProtocolTest tests fail with ""TLSv1.1 failed to negotiate"" on JDK17","Note: This depends on having a fix for CASSANDRA-18180, otherwise most/all tests in {{NativeTransportEncryptionOptionsTest}} and {{InternodeEncryptionOptionsTest}} are failing due to that issue.

Using the patch for CASSANDRA-18180, the {{negotiatedProtocolMustBeAcceptedProtocolTest}} test in both {{NativeTransportEncryptionOptionsTest}} and {{InternodeEncryptionOptionsTest}} fails with ""TLSv1.1 failed to negotiate"" on JDK17.

From what I can see, the {{negotiatedProtocolMustBeAcceptedProtocolTest}} is failing because in JDK11 and JDK17 the ""TLSv1.1"" protocol is disabled.

Since TLSv1.1 is disabled in JDK11 and 17, one possibility is to change the test to use TLSv1.2 instead of TLSv1.1. That should work directly with JDK11 and 17, since TLSv1.2 is one of the defaults, and it won't be an issue for JDK8 as that will be dropped.

Also, I think the point of the {{negotiatedProtocolMustBeAcceptedProtocolTest}} is to test that the {{accepted_protocols}} option is working correctly rather than the choice of _which_ protocol is used. Meaning, I don’t think the intent was to test TLSv1.1 specifically, rather that the mechanism of accepted protocols works and choosing TLSv1.1 was at the time convenient - but I could be wrong.

It also seems to me like bit of a coincidence that these tests are currently working on JDK11, at least on CI. Indeed, running locally with JDK11, these fail for me:

{noformat}
$ pwd
/Users/dan.jatnieks/apache/cassandra-4.0

$ java -version
openjdk version ""11.0.11"" 2021-04-20
OpenJDK Runtime Environment AdoptOpenJDK-11.0.11+9 (build 11.0.11+9)
OpenJDK 64-Bit Server VM AdoptOpenJDK-11.0.11+9 (build 11.0.11+9, mixed mode)

$ ant test-jvm-dtest-some -Dtest.name=org.apache.cassandra.distributed.test.NativeTransportEncryptionOptionsTest -Duse.jdk11=true

...

[junit-timeout] Testcase: negotiatedProtocolMustBeAcceptedProtocolTest(org.apache.cassandra.distributed.test.NativeTransportEncryptionOptionsTest):	FAILED
[junit-timeout] Should be possible to establish a TLSv1.1 connection expected:<NEGOTIATED> but was:<FAILED_TO_NEGOTIATE>
[junit-timeout] junit.framework.AssertionFailedError: Should be possible to establish a TLSv1.1 connection expected:<NEGOTIATED> but was:<FAILED_TO_NEGOTIATE>
[junit-timeout] 	at org.apache.cassandra.distributed.test.NativeTransportEncryptionOptionsTest.negotiatedProtocolMustBeAcceptedProtocolTest(NativeTransportEncryptionOptionsTest.java:160)
[junit-timeout] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit-timeout] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[junit-timeout] 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{noformat}

I believe these work on CI because of CASSANDRA-16848 - in that ticket, after 2021-Apr JDK8 dropped TLSv1.1 which led to a fix in [cassandra-build|https://github.com/apache/cassandra-builds/commit/d1a3a0c59b3c5c17697d6a6656cd5d4f3a1cdbe9] docker code to make sure TLSv1.1 is accepted. 

I say coincidence because this change also makes it work for JDK11 and JDK17, and I've been able to verify that making a change locally to the JDK {{java.security}} file. I’m not sure that at the time of CASSANDRA-16848 it was intended for any JDK versions.

The point of mentioning this is that if {{negotiatedProtocolMustBeAcceptedProtocolTest}} is changed to use TLSv1.2, and support for JDK8 is dropped, then the changes made in CASSANDRA-16848 could also be reverted.
",djatnieks,djatnieks,Normal,Resolved,Fixed,20/May/23 00:16,14/Jun/23 18:23
Bug,CASSANDRA-18541,13536935,AUTH requests use too much resources,"Hello. I see unexpected CPU usage in a rare situation that may be worth digging into.
We have C* 4.0.9 on Debian running on Java 11.0.18.
It's a small cluster of 3 nodes on commodity hardware (6 cores CPU, 32 Gb RAM, 2 x 512 Gb SSD NVME).
This ring has about 35 clients using Datastax Java Driver for Apache Cassandra.
In the driver connection settings, we use the following:
CONNECTION_POOL_LOCAL_SIZE = 400
CONNECTION_POOL_REMOTE_SIZE = 100
 
And for some reason, from time to time, it causes hundreds of AUTH requests per second that leads to an enormous CPU usage.
And yes, it's easy not to use these settings in the driver, leaving defaults that don't produce such an amount of AUTHs. But isn't it weird that ~150 AUTH rps consume ~1200% CPU?
Please see attached graphs.

I have the following in the settings:

authenticator: PasswordAuthenticator

authorizer: CassandraAuthorizer

roles_validity_in_ms: 600000

permissions_validity_in_ms: 600000

credentials_validity_in_ms: 600000

Please let me know if I can provide any other necessary information.
Thanks for your work. Cassandra is amazing :)",,yury.vidineev,Normal,Resolved,Fixed,20/May/23 07:17,27/Jun/23 23:20
Bug,CASSANDRA-18543,13537232,Waiting for gossip to settle does not wait for live endpoints,"When a node starts it will get endpoint states (via shadow round) but have all nodes marked as down. The problem is the wait to settle only checks the size of endpoint states is stable before starting Native transport. Once native transport starts it will receive queries and fail consistency levels such as LOCAL_QUORUM since it still thinks nodes are down.

This is problem for a number of large clusters for our customers. The cluster has quorum but due to this issue a node restart is causing a bunch of query errors.

My initial solution to this was to only check live endpoints size in addition to size of endpoint states. This worked but I noticed in testing this fix that there also a lot of duplication of checking the same node (via Echo messages) for liveness. So the patch also removes this duplication of checking node is UP in markAlive.

The final problem I found while testing is sometimes could still not see a change in live endpoints due to only 1 second polling, so the patch allows for overridding the settle parameters. I could not reliability reproduce this but think its worth providing a way to override these hardcoded values.",smiklosovic,cam1982,Normal,Resolved,Fixed,23/May/23 00:36,15/Jun/23 13:58
Bug,CASSANDRA-18552,13537640,Debian packaging source should exclude git subdirectory,This balloons the source up to 400+MB instead of the ~13MB necessary.,mck,brandon.williams,Normal,Resolved,Fixed,25/May/23 11:05,26/May/23 09:52
Bug,CASSANDRA-18553,13537749,Generate.sh -s param to skip autodetection of tests,When using generate.sh auto detection of modified tests always kicks in. That can be a problem during dev when you want to test a given set of tests without getting all the others in the way. Also when you want to run the script without having to checkout the extra branches auto detection needs.,bereng,bereng,Normal,Resolved,Fixed,26/May/23 05:47,12/Jun/23 08:07
Bug,CASSANDRA-18558,13538329,remove dh_python use from debian packaging,"It looks like dh_python2 has been removed from debian, but it also looks like we don't need it:

{noformat}
E: dh_python2 dh_python2:408: no package to act on (python-foo or one with ${python:Depends} in Depends)
{noformat}",brandon.williams,brandon.williams,Normal,Resolved,Fixed,31/May/23 22:04,01/Jun/23 20:31
Bug,CASSANDRA-18560,13538448,Incorrect IP used for gossip across DCs with prefer_local=true,"After installing a new node using 4.0.10 we experienced a situation where the new node attempted to connect to the private ip of a random number of nodes remote DCs which are only accessible via public ip for cross dc communications.

The only impact was new nodes outbound connections, inbound from pre-4.0.10 were not affected.  system.peers_v2 (below) showed that the preferred_ip and preferred_port as null, only those in 4.0.10 nodes dc have perferred_ip values as expected.

We believe the issue originated with https://issues.apache.org/jira/browse/CASSANDRA-16718 

Details on cluster:
 * All nodes have public IP configured as well as private IP
 * Listen/rpc addressrs are configured for private ip, broadcast is public IP
 * prefer_local=true is enabled for all nodes

The log that showed the connection failing:
{code:java}
INFO  [Messaging-EventLoop-3-8] 2023-06-01 00:14:21,565 NoSpamLogger.java:92 - /99.81.<redacted>:7000->/44.208.<redacted>:7000-URGENT_MESSAGES-[no-channel] failed to connectio.netty.channel.ConnectTimeoutException: connection timed out: /10.26.5.11:7000  at io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe$2.run(AbstractEpollChannel.java:576){code}
99 and 44 instances can only access each other using public ips.

gossipinfo output from 4.0.10 node
{code:java}
/44.208.<redacted>
  generation:1661113358
  heartbeat:25267691
  LOAD:25267683:1.7882044268E10
  SCHEMA:24692061:e98b918d-499f-3ccc-8dbe-5af31f685bda
  DC:13:us-east-1
  RACK:15:1a
  RELEASE_VERSION:6:4.0.5
  NET_VERSION:2:12
  HOST_ID:3:9a41e668-060d-4cfe-bb1e-013f5116422d
  RPC_READY:1407:true
  INTERNAL_ADDRESS_AND_PORT:9:10.26.5.11:7000
  NATIVE_ADDRESS_AND_PORT:4:44.208.<redacted>:9042
  STATUS_WITH_PORT:1393:NORMAL,-2262036356854762881
  SSTABLE_VERSIONS:7:big-nb
  TOKENS:1392:<hidden> {code}
Peers output from 4.0.10 node:
{code:java}
   peer           | peer_port | data_center         | host_id                              | native_address | native_port | preferred_ip | preferred_port | rack | release_version | schema_version                       | tokens----------------+-----------+---------------------+--------------------------------------+----------------+-------------+--------------+----------------+------+-----------------+--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  44.208.<redacted> |      7000 |      us-east-1 | 9a41e668-060d-4cfe-bb1e-013f5116422d |  44.208.<redacted> |        9042 |         null |           null |   1a |           4.0.5 | e98b918d-499f-3ccc-8dbe-5af31f685bda |    {'-2262036356854762881', '-4197710115038136897', '-7072386316096662315', '2085255826742630980', '249732489387853170', '4976300208126705818', '7187184456885833289', '8777189009399731927'} {code}
To solve temporarily we routed outbound traffic to the private ip to public using iptables which resulted in successful outbound connections.",brandon.williams,bvernon,Urgent,Resolved,Fixed,01/Jun/23 16:58,10/Jul/23 15:08
Bug,CASSANDRA-18562,13538565,guava vulnerability CVE-2023-2976,This is failing the OWASP check.,brandon.williams,brandon.williams,Normal,Resolved,Fixed,02/Jun/23 10:37,05/Jul/23 04:19
Bug,CASSANDRA-18569,13538889,Bti shouldn't be available in compatibility mode,When having a node in compatibility mode sstable tries shouldn't be an option.,bereng,bereng,Normal,Resolved,Fixed,06/Jun/23 08:54,09/Jun/23 07:37
Bug,CASSANDRA-18575,13539152,Backport Cassandra-10508 Remove hard-coded SSL cipher suites and protocols,Cassandra 3.0 has ciphers hard coded and thus not allow more recent and secure ciphers for storage connections complicating migrations to later versions.,xgerman42,xgerman42,Normal,Resolved,Fixed,07/Jun/23 22:13,21/Jun/23 11:28
Bug,CASSANDRA-18593,13539845,CircleCI: Add separate approval step for oa utests on separate workflows,"CASSANDRA-14227 added new {{j*_utests_oa}} jobs to run unit tests with the new, not-default ""oa"" sstable format. Those tests depend on the {{start_j*_unit_tests}} approval step on the separate workflows.

However, the separate workflow is meant to allow running jobs individually, and having a common approval step for both {{j*_unit_tests}} and {{j*_utests_oa}} prevents us from doing so. That's the case for example of the development of CASSANDRA-18504, where I want to run a single unit test job without caring about the specialization, but the common approval step forces me to run both jobs, duplicating the costs. An example run can be seen [here|https://app.circleci.com/pipelines/github/adelapena/cassandra/2948/workflows/67e5bfcd-c8df-4558-a889-c8828e8dd310].

I think that {{j*_utests_oa}} should have its own separate approval step, the same way that {{{}j*_utest_cdc{}}}, {{{}j*_utest_fqltool{}}}, {{j*_utest_compression}} or {{j*_utest_system_keyspace_directory}} have their own approval step.",adelapena,adelapena,Normal,Resolved,Fixed,13/Jun/23 12:15,16/Jun/23 12:10
Bug,CASSANDRA-18596,13539973,Assertion error when describing mv as table,"When describing materialized view as a table Cassandra gets an assertion error.

Steps to reproduce:
CREATE KEYSPACE test WITH replication = \{'class': 'NetworkTopologyStrategy', 'datacenter1': '3'} AND durable_writes = true;
CREATE TABLE test.table1 (key1 text,key2 int,value int,PRIMARY KEY (key1, key2));
CREATE MATERIALIZED VIEW test.table1_by_value AS SELECT key1, key2, value FROM test.table1 WHERE value IS NOT NULL AND key1 IS NOT NULL AND key2 IS NOT NULL PRIMARY KEY(value, key1, key2);
DESCRIBE MATERIALIZED VIEW test.table1;
DESCRIBE TABLE test.table1_by_value;
DESCRIBE TABLE test.non_existing;
 
From the above the ""DESCRIBE TABLE test.table1_by_value;"" throws an assertion error while ""DESCRIBE TABLE test.non_existing;"" returns a meaningful error msg.",masokol,masokol,Normal,Resolved,Fixed,14/Jun/23 06:08,19/Jun/23 15:41
Bug,CASSANDRA-18608,13540519,"snappy-java vulnerability: CVE-2023-34455, CVE-2023-34454, CVE-2023-34453","Failing owasp:

[https://nvd.nist.gov/vuln/detail/CVE-2023-34455]
{quote}Due to use of an unchecked chunk length, an unrecoverable fatal error can occur in versions prior to 1.1.10.1.
{quote}

[https://nvd.nist.gov/vuln/detail/CVE-2023-34454]
{quote}Due to unchecked multiplications, an integer overflow may occur in versions prior to 1.1.10.1, causing an unrecoverable fatal error. 
{quote}

[https://nvd.nist.gov/vuln/detail/CVE-2023-34453]
{quote}Due to unchecked multiplications, an integer overflow may occur in versions prior to 1.1.10.1, causing a fatal error.
{quote}
",brandon.williams,brandon.williams,Normal,Resolved,Fixed,18/Jun/23 13:37,26/Jun/23 11:15
Bug,CASSANDRA-18609,13540520,snappy-java vulnerability: CVE-2023-34453,"Failing owasp:

[https://nvd.nist.gov/vuln/detail/CVE-2023-34453]

bq. Due to unchecked multiplications, an integer overflow may occur in versions prior to 1.1.10.1, causing a fatal error. ",brandon.williams,brandon.williams,Normal,Resolved,Fixed,18/Jun/23 13:40,22/Jun/23 16:45
Bug,CASSANDRA-18626,13541441,compaction_tombstone_warning_threshold and compaction_large_partition_warning_threshold cause deprecation warnings,"If you start trunk without any changes you will see at startup:

{noformat}
WARN  [main] 2023-06-26 15:59:36,613 YamlConfigurationLoader.java:426 - [compaction_tombstone_warning_threshold, compaction_large_partition_warning_threshold] parameters have been deprecated. They have new names and/or value format; For more information, please refer to NEWS.txt
{noformat}

We should give these the same treatment as CASSANDRA-18617, removing them from the default yaml to stop the warnings, but still accepting them with converters and @Replaces tags.",adelapena,brandon.williams,Normal,Resolved,Fixed,26/Jun/23 16:09,29/Jun/23 16:14
Bug,CASSANDRA-18630,13541550,jackson-databind-2.13.2.2.jar vulnerability: CVE-2023-35116,"https://nvd.nist.gov/vuln/detail/CVE-2023-35116

{noformat}
 An issue was discovered jackson-databind thru 2.15.2 allows attackers to cause a denial of service or other unspecified impacts via crafted object that uses cyclic dependencies. NOTE: the vendor's perspective is that the product is not intended for use with untrusted input.
{noformat}",brandon.williams,brandon.williams,Normal,Resolved,Fixed,27/Jun/23 11:31,28/Jun/23 11:08
Bug,CAMEL-18854,13516292,camel-rabbitmq x-queue-type no longer working,"Using rabbitMQ consumer  with arg.queue.x-queue-type=quorum is no longer working with 

3.20 , it was working with 3.18.

Whatever value is set to x-queue-type, connection is done with x-queue-type set to null.

Regression has been introduced by 
[Added support for rabbitmq x-queue-type by eduanb · Pull Request #8725 · apache/camel (github.com)|https://github.com/apache/camel/pull/8725]",,hdumont34,Minor,Resolved,Fixed,03/Jan/23 09:49,30/Jan/23 08:59
Bug,CAMEL-18856,13516323,camel-main - Unable to declare java.util.List bean,"I have been unsuccessful in declaring a java.util.List bean. According to camel-main documentation, a List bean should be declared using numeric keys in square brackets:
{noformat}
camel.beans.myprojects[0] = Camel
camel.beans.myprojects[1] = Kafka
camel.beans.myprojects[2] = Quarkus
{noformat}
This does, however, not actually declare a java.util.List, but a java.util.LinkedHashMap, instead.

Again, reproduction is easy:
{code:java}
@Test
public void testBindBeansList() {
    Main main = new Main();
    main.configure().addRoutesBuilder(new MyRouteBuilder());

    // defining a list bean
    main.addProperty(""camel.beans.myprojects[0]"", ""Camel"");
    main.addProperty(""camel.beans.myprojects[1]"", ""Kafka"");
    main.addProperty(""camel.beans.myprojects[2]"", ""Quarkus"");

    main.start();

    CamelContext camelContext = main.getCamelContext();
    assertNotNull(camelContext);

    Object bean = camelContext.getRegistry().lookupByName(""myprojects"");
    assertNotNull(bean);
    assertInstanceOf(java.util.List.class, bean);

    java.util.List<?> list = (java.util.List<?>) bean;
    assertEquals(3, list.size());
    assertEquals(""Camel"", list.get(0));
    assertEquals(""Kafka"", list.get(1));
    assertEquals(""Quarkus"", list.get(2));

    main.stop();
} {code}
There is a workaround, which is to use java.util.List#of, which is also more concise, but only possible if using Java 9 or above:
{noformat}
camel.beans.myprojects = #class:java.util.List#of(""Camel"", ""Kafka"", ""Quarkus""){noformat}
 ",davsclaus,chrissh,Minor,Resolved,Fixed,03/Jan/23 13:04,30/Jan/23 08:59
Bug,CAMEL-18865,13517020,camel-main - Setters not invoked on bean that implements Map,"We are having trouble configuring camel-jms with IBM-MQ as runtime implementation, using camel-main.

Doing so requires the declaration of a connection factory, which, in our case, is [com.ibm.mq.jms.MQConnectionFactory|https://www.ibm.com/docs/api/v1/content/SSFKSJ_9.2.0/com.ibm.mq.javadoc.doc/WMQJMSClasses/com/ibm/mq/jms/MQConnectionFactory.html]. Unfortunately, this class implements both {{javax.jms.ConnectionFactory}} and {{{}java.util.Map<java.lang.String,java.lang.Object>{}}}. Also, the setters of that class have side effects, that are non-trivial to reproduce.

What was really troubling us was that, contrary to intuition, defining a bean like this:
{code:java}
camel.beans.mqConnectionFactory = #class:com.ibm.mq.jms.MQConnectionFactory
camel.beans.mqConnectionFactory.hostName = mqhost
{code}
does not actually invoke the setHostName method on the factory. Instead, value ""mqhost"" is put under key ""hostName"" into the map.

Reproduction can be achieved with a quick custom class, like this:
{code:java}
package org.apache.camel.main;

import java.util.HashMap;

public class MyFooFactory extends HashMap<String, Object> {
    private String hostName;

    public String getHostName() {
        return hostName;
    }

    public void setHostName(String hostName) {
        this.hostName = hostName;
    }
}
{code}
and a unit test, which checks for the hostName:
{code:java}
    @Test
    public void testBindBeansDottedHybridMap() {
        Main main = new Main();
        main.configure().addRoutesBuilder(new MyRouteBuilder());

        // defining a factory bean
        main.addProperty(""camel.beans.myfactory"", ""#class:org.apache.camel.main.MyFooFactory"");
        main.addProperty(""camel.beans.myfactory.hostName"", ""localhost"");
        main.start();

        CamelContext camelContext = main.getCamelContext();
        assertNotNull(camelContext);

        Object bean = camelContext.getRegistry().lookupByName(""myfactory"");
        assertNotNull(bean);
        assertInstanceOf(MyFooFactory.class, bean);

        MyFooFactory factory = (MyFooFactory) bean;
        assertEquals(""localhost"", factory.getHostName());

        main.stop();
    }
{code}
For a custom bean that is under our own control, I would consider implementing java.util.Map and having additional custom getters and setters with side-effects to be invalid, but the IBM-MQ JMS client is out of our control - and required for our use case.

If the dotted annotation is used, it may be preferable to check for a suitable setter first, and only if that is missing, to check if the bean implements java.util.Map.",davsclaus,chrissh,Minor,Resolved,Fixed,06/Jan/23 09:42,30/Jan/23 08:59
Bug,CAMEL-18868,13517250,Aws2-s3: CreateDownloadLink does not work with useDefaultCredentialsProvider,"If component uses default credential provider (`useDefaultCredentialsProvider`), operation `createDownloadLink` fails with:

{quote}Caused by: java.lang.NullPointerException: Access key ID cannot be blank.
	at software.amazon.awssdk.utils.Validate.notNull(Validate.java:119)
	at software.amazon.awssdk.auth.credentials.AwsBasicCredentials.<init>(AwsBasicCredentials.java:66)
	at software.amazon.awssdk.auth.credentials.AwsBasicCredentials.<init>(AwsBasicCredentials.java:58)
	at software.amazon.awssdk.auth.credentials.AwsBasicCredentials.create(AwsBasicCredentials.java:78)
	at org.apache.camel.component.aws2.s3.AWS2S3Producer.createDownloadLink(AWS2S3Producer.java:590)
	at org.apache.camel.component.aws2.s3.AWS2S3Producer.process(AWS2S3Producer.java:122){quote}
",jondruse,jondruse,Minor,Resolved,Fixed,09/Jan/23 14:25,30/Jan/23 08:59
Bug,CAMEL-18871,13517256,camel-netty - Application does not recover (threads are WAITING) when NettyProducer pool is exhausted,"When the Camel Application sends a LOT of requests to a TCP/IP Server (which is not available/down) using Netty component, after a bit, it hangs because the Producer Pool is exhausted (when *producerPoolMaxTotal* is not -1).

 
{code:java}
from(""direct:start"")
.to(""netty://tcp://localhost:18667?connectTimeout=500&decoders=#myDecoders&encoders=#myEncoders&producerPoolMaxTotal=10&producerPoolMinEvictableIdle=-1&requestTimeout=250&sync=true"");
{code}
 

 

In previous version 3.14.1 the behavior was to fail when Pool was exhausted ({*}GenericObjectPool.{*}{*}WHEN_EXHAUSTED_FAIL{*}), however since version 3.14.2 and the migration to `{*}org.apache.commons:commons-pool2{*}` (https://issues.apache.org/jira/browse/CAMEL-17461) the default has become to block.

 

The proposal is to adjust `[https://github.com/apache/camel/blob/camel-3.14.7/components/camel-netty/src/main/java/org/apache/camel/component/netty/NettyProducer.java] ` , method `doStart()` to add optionally set the *BlockWhenExhausted* flag (BaseGenericObjectPool.setBlockWhenExhausted) and the *{{borrowMaxWaitMillis}}* parameters.

 
{code:java}
""Camel Thread #74 - NettyClientTCPWorker"" #1166 prio=5 os_prio=0 tid=0x00007fd661a1d800 nid=0x18599 waiting on condition [0x00007fd5ffb12000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000005ce0e8338> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:1323)
        at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:306)
        at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:223)
        at org.apache.camel.component.netty.NettyProducer.processWithBody(NettyProducer.java:259)
        at org.apache.camel.component.netty.NettyProducer.process(NettyProducer.java:228)
        at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:172)
        at org.apache.camel.impl.engine.CamelInternalProcessor.process(CamelInternalProcessor.java:398){code}
 ",,vincenzo.galluccio,Major,Resolved,Fixed,09/Jan/23 15:35,30/Jan/23 08:59
Bug,CAMEL-18872,13517415,camel-core-model - Rest DSL param example not available in XML and YAML DSL,A model change in 3.20.1 mistakenly removed examples from rest-dsl for param. Adding this back.,davsclaus,davsclaus,Minor,Resolved,Fixed,10/Jan/23 08:13,30/Jan/23 08:59
Bug,CAMEL-18878,13517556,Autowiring on endpoint works even if is disabled on component,"Simi9lar to  https://issues.apache.org/jira/browse/CAMEL-16527

If autowireEnabled=false is set into the component, the DefaultEndpoint in method ""doInit"" ignores the fact, that autowiring might be disabled on component/",jondruse,jondruse,Major,Resolved,Fixed,11/Jan/23 07:45,30/Jan/23 08:59
Bug,CAMEL-18922,13519151,TemplatedRoute fails to load with XML RouteLoader,"I created the following ticket yesterday:

https://issues.apache.org/jira/browse/CAMEL-18914

This issue was not a bug as it was a problem with the namespace.

I am creating a new ticket as the route template still doesn't load in 3.20.0 (In 3.19.0 it loads successfully).

Here is the code I am using:
{code:java}
// 1. Start Camel
context = new DefaultCamelContext();
context.start();

//2. Load route template
context.addRoutes((RouteBuilder) new MyTemplate());

//3. Create resource from string
String content = ""<templatedRoutes id=\""camel\"" xmlns=\""http://camel.apache.org/schema/spring\"">\n"" +
        ""    <!-- create two routes from the template -->\n"" +
        ""    <templatedRoute routeTemplateRef=\""myTemplate\"">\n"" +
        ""        <parameter name=\""name\"" value=\""Manually Loaded Tempate\""/>\n"" +
        ""        <parameter name=\""greeting\"" value=\""Hello\""/>\n"" +
        ""    </templatedRoute>\n"" +
        ""</templatedRoutes>"";

Resource resource = ResourceHelper.fromString(""in-memory.xml"", content);

//4. Load resource
ExtendedCamelContext extendedCamelContext = context.adapt(ExtendedCamelContext.class);
RoutesLoader loader = extendedCamelContext.getRoutesLoader();
loader.loadRoutes(resource);

//5. check the size of all routes
LOG.info(""RoutesSize = "" + context.getRoutesSize()); {code}
I added an example to reproduce this issue. When Camel version 3.19.0 is used the example works. When version 3.20.0 is used it fails. The example is configured with 3.19.0 (as the bom version).",davsclaus,skin27,Major,Resolved,Fixed,14/Jan/23 14:18,30/Jan/23 08:59
Bug,CAMEL-18944,13519967,REST YAML does not work in Spring Boot,"The following YAML fails when run with Spring Boot
{code:java}
- rest:
    id: rest-dc6e
    post:
      - id: post-248c
        consumes: application/json
        produces: application/json
        to: direct:demo
- route:
    id: demo
    from:
      uri: direct:demo
      id: from-e15c
      steps:
        - log:
            message: ${body}
            id: log-a55d
{code}

command to call service
{code:java}
curl -X POST -H ""Content-Type: application/json"" --data '{""id"":""666"",""address"":""666 Sin Street, Holy City""}' http://0.0.0.0:8080
{code}

REST Configuration component does not change anything
",davsclaus,marat.gubaidullin@gmail.com,Major,Resolved,Fixed,18/Jan/23 02:39,22/Mar/23 07:56
Bug,CAMEL-18953,13520112,camel-jetty - Should throw 504 error if continuation timeout was hit,Jetty returns 500 instead of 504 which was expected by some unit tests,davsclaus,davsclaus,Minor,Resolved,Fixed,18/Jan/23 21:25,30/Jan/23 08:59
Bug,CAMEL-18954,13520148,camel-micrometer - NPE on spring boot,"Hello everyone,

Unfortunately when I’m testing a route containing a micrometer endpoint on Camel 3.20.1 using Spring Boot, then I’m getting a
{code:java}
java.lang.NullPointerException: null
        at org.apache.camel.component.micrometer.eventnotifier.MicrometerExchangeEventNotifierNamingStrategy.getTags(MicrometerExchangeEventNotifierNamingStrategy.java:53)
        at org.apache.camel.component.micrometer.eventnotifier.MicrometerExchangeEventNotifier.handleDoneEvent(MicrometerExchangeEventNotifier.java:105)
       at org.apache.camel.component.micrometer.eventnotifier.MicrometerExchangeEventNotifier.notify(MicrometerExchangeEventNotifier.java:75)
        […]
{code}
 

The exception only occurs in case I’m using:
{code:java}
producer.send(ExchangeBuilder.anExchange(context).withBody(""hello"").build());{code}
It’s working fine in case of
{code:java}
producer.sendBody(“hello”);{code}
For both statements, in 3.18.3 it’s working fine – in any case, meaning in 3.18.3 and 3.20.1, the ExchangeBuilder.anExchange(context).withBody(""hello"").build().getEndpointFrom() is null, however, only in 3.20.1 it looks like that getEndpointFrom() indirectly causes the NPE in [line 53|https://github.com/apache/camel/blob/1d7da244fadca57b4054cc0defb70a70923c06f4/components/camel-micrometer/src/main/java/org/apache/camel/component/micrometer/eventnotifier/MicrometerExchangeEventNotifierNamingStrategy.java#L53].

Pls. find a repo case attached.[^repro.zip]",davsclaus,andreas.klug2@de.bosch.com,Minor,Resolved,Fixed,19/Jan/23 07:14,30/Jan/23 12:42
Bug,CAMEL-18956,13520169,Camel-Jcache: It's using a bundle of javax-cache-api,"It's not part of Jakarta move, but I guess we should use the normal dependency and not the bundle.",acosentino,acosentino,Major,Resolved,Fixed,19/Jan/23 09:55,30/Jan/23 08:59
Bug,CAMEL-18959,13520209,camel-salesforce: deserialization failure for streaming query response gets swallowed,,jeremyross,jeremyross,Minor,Resolved,Fixed,19/Jan/23 14:45,30/Jan/23 08:59
Bug,CAMEL-18965,13520963,Camel-CXF: OnCompletion not working anymore,"Since following change, the onCompletionHandler is not called anymore in error case (soap fault)

https://issues.apache.org/jira/browse/CAMEL-16796
[https://github.com/apache/camel/commit/45e64cce8ad2e1d22a5f55be13a6dfcaa504f58f]

{*}Reproducer{*}: [https://github.com/mash-sap/CAMEL-16532/tree/Camel-18965]",ffang,mash-sap,Minor,Resolved,Fixed,23/Jan/23 09:36,07/Jun/23 21:27
Bug,CAMEL-18968,13520983,Camel-aws2-sqs - Queue url might stay empty for the delayed queue.,"During investigation of [https://github.com/apache/camel-quarkus/issues/4389] I found that aws2-sqs producer is not able to send messages to the queueu which was created as delayed.

 

Queue url should be initialized during the endpoint initialization phase.  If queue does not exist in that time, the url stays empty for the whole life of the endpoint. Therefore majority of actions would fail.",jondruse,jondruse,Major,Resolved,Fixed,23/Jan/23 12:28,30/Jan/23 08:59
Bug,CAMEL-18980,13521695,camel snmp - SNMP Ver1 trap does not work,"Although camel-snmp supports SNMP v1, v2, and v3 trap, v1 trap does not work as expected.

By checking the test code for camel-snmp, v2 trap is only tested.
https://github.com/apache/camel/blob/camel-3.x/components/camel-snmp/src/test/java/org/apache/camel/component/snmp/TrapTest.java

Based on the TrapTest.java, I implemented a TrapTestSnmpv1.java. Please use the test code for this issue.",Federico Mariani,hfuruichi,Minor,Resolved,Fixed,27/Jan/23 07:19,27/Mar/23 13:32
Bug,CAMEL-18985,13521955,"camel-kafka: messages are getting lost with ""breakOnFirstError""","*Description:*

Messages are getting lost with ""breakOnFirstError=true"" when processing of a particular message failed several times in a raw.

 

*Configuration:*
 * autoCommitEnable=false
 * allowManualCommit=true
 * breakOnFirstError=true
 * autoOffsetReset=earliest
 * maxPollRecords is greater than one (e.g. 5 in this test)

 

*Test Scenario:*
 # inbound-topic contains 10 messages: 0,1,2,3,4,5,6,7,8,9
 # camel-route polls 5 messages: 0,1,2,3,4
 # camel-route successfully processes message=0 and manually commits offset
 # camel-route successfully processes message=1 and manually commits offset
 # camel-route fails with processing message=2 first time
 # breakOnFirstError=true causes camel-route to reconnect and poll from the committed offset
 # camel-route polls 5 messages: 2,3,4,5,6
 # camel-route fails with processing message=2 second time
 # *ACTUAL:* camel-route reconnects and polls messages 7,8,9 - as result messages 3,4,5,6 are never processed

*EXPECTED:* camel-route should do the following on the step 9:
 * reconnect and poll the same 5 messages again: 2,3,4,5,6
 * process message=2 third time (the test is configured to succeed on the 3rd attempt)
 * continue with processing messages 3,4,5,6...

 

*GitHub project reproducing the issue:*

[https://github.com/opershai/camel-kafka-lost-messages-demo] 

Test-runs:
 * 3.18.5 (failing): [https://github.com/opershai/camel-kafka-lost-messages-demo/actions/runs/4039702564] 
 * 3.19.0 (failing): [https://github.com/opershai/camel-kafka-lost-messages-demo/actions/runs/4039815226] 
 * 3.20.0 (failing): [https://github.com/opershai/camel-kafka-lost-messages-demo/actions/runs/4039709753] 
 * 3.20.1 (failing): [https://github.com/opershai/camel-kafka-lost-messages-demo/actions/runs/4039673222] 
 * 2.25.4 (passing): [https://github.com/opershai/camel-kafka-lost-messages-demo/actions/runs/4057277373] 

 

*Impact:*

It seems ""breakOnFirstError"" is not fully reliable in LTS versions of Camel Kafka component at the moment:
 * 3.20.x - due to this bug
 * 3.18.x - due to this bug
 * 3.14.x - due to another bug https://issues.apache.org/jira/browse/CAMEL-17925 as fix was not back-ported to the 3.14.x",orpiske,opershai,Major,Resolved,Fixed,30/Jan/23 01:59,26/Apr/23 15:31
Bug,CAMEL-19002,13522801,camel-jbang - Log command should detect lines without timestamp,"If you log data that uses new-line, then the camel log command may cause NPE",davsclaus,davsclaus,Major,Resolved,Fixed,02/Feb/23 15:12,03/Feb/23 08:25
Bug,CAMEL-19004,13522937,XML IO DSL do not parse route configuration with XML namespace,"XML IO DSL do not load {{routeConfiguration}} if it uses XML namespace like this:

{code}
<routeConfiguration xmlns=""http://camel.apache.org/schema/spring"">
  <onException>
  <exception>java.lang.Exception</exception>
  <handled><constant>true</constant></handled>
  <log message=""XML WARN: ${exception.message}""/>
  </onException>
</routeConfiguration>
{code}

A possible workaround is to explicitly remove the namespace, for instance like below:
{code}
<routeConfiguration>
...
</routeConfiguration>
{code}",,tturek,Minor,Resolved,Fixed,03/Feb/23 11:01,22/Feb/23 12:35
Bug,CAMEL-19006,13522949,XML IO DSL do not load templatedRoutes without XML namespace,"XML IO DSL do not load templatedRoutes without XML namespace like this:
{code}
<templatedRoutes>
    <templatedRoute routeTemplateRef=""myTemplate"" routeId=""my-route"">
        <parameter name=""foo"" value=""fooVal""/>
        <parameter name=""bar"" value=""barVal""/>
    </templatedRoute>
</templatedRoutes>
{code}

A possible workaround is to explicitly set the namespace, for instance like below:
{code}
<templatedRoutes xmlns=""http://camel.apache.org/schema/spring"">
{code}",,tturek,Minor,Resolved,Fixed,03/Feb/23 12:47,22/Feb/23 12:35
Bug,CAMEL-19014,13523266,SimpleLanguage cache issue,"Hi guys, I'm attaching a test class that will reproduce the issue that I'm experiencing.
As summary: The cache inside the SimpleLanguage class looks like it's going to be affected by the evaluation of the simple expression, producing unexpected results.
When the method *resolveTemplate* is used the result is not the expected one.
To make it work I have to simulate a new entry in the cache making use of the method *resolveTemplateNoCache.*
Thanks in advance
{code:java}
public class SimpleLanguageTest extends CamelTestSupport {
    @EndpointInject(""mock:result"")
    private MockEndpoint mockResult;

    @EndpointInject(""mock:fail"")
    private MockEndpoint mockWithFailure;

    @Test
    public void whenSimpleLanguageNotUseCachedEntriesItWillNotFail() throws Exception {

        mockResult.await(20, TimeUnit.SECONDS);
        mockResult.expectedMessageCount(102);
        assertMockEndpointsSatisfied();
        List<Exchange> exchanges = mockResult.getExchanges();
        exchanges.stream()
                .forEach(exchange -> assertTrue(exchange.getMessage().getHeader(""#CustomHeader"", String.class).equals(""This is a test a with startLabel: `Document` endLabel: `Document` and label: `ALabel`"")));


    }

    @Test
    public void whenSimpleLanguageUseCachedEntriesItWillFail() throws Exception {
        mockWithFailure.expectedMessageCount(102);
        assertMockEndpointsSatisfied();
        List<Exchange> exchanges = mockWithFailure.getExchanges();
        exchanges.stream()
                .forEach(exchange -> assertTrue(exchange.getMessage().getHeader(""#CustomHeader"", String.class).equals(""This is a test a with startLabel: `Document` endLabel: `Document` and label: `ALabel`"")));


    }

    @Override
    protected RoutesBuilder createRouteBuilder() throws Exception {

        return new RouteBuilder() {
            Map body = new HashMap() {{
                put(""label"", ""ALabel"");
                put(""startLabel"", ""Document"");
                put(""endLabel"", ""Document"");
            }};


            String simpleTemplate = ""This is a test a with startLabel: `${body.get('startLabel')}` endLabel: `${body.get('endLabel')}` and label: `${body.get('label')}`"";


            @Override
            public void configure() throws Exception {
                from(""timer://test-timer?fixedRate=true&period=10&delay=1"")
                        .process(new Processor() {
                            @Override
                            public void process(Exchange exchange) throws Exception {
                                exchange.getMessage().setBody(body);
                                exchange.getMessage().setHeader(""#CustomHeader"", resolveTemplateNoCache(simpleTemplate, exchange));
                            }
                        })
                        .to(""mock:result"");

                from(""timer://test-timer1?fixedRate=true&period=10&delay=1"")

                        .process(new Processor() {
                            @Override
                            public void process(Exchange exchange) throws Exception {
                                exchange.getMessage().setBody(body);
                                exchange.getMessage().setHeader(""#CustomHeader"", resolveTemplateNoCache(simpleTemplate, exchange));
                            }
                        })
                        .to(""mock:result"");

                from(""timer://test-timer2?fixedRate=true&period=10&delay=1"")
                        .process(new Processor() {
                            @Override
                            public void process(Exchange exchange) throws Exception {
                                exchange.getMessage().setBody(body);
                                exchange.getMessage().setHeader(""#CustomHeader"", resolveTemplateNoCache(simpleTemplate, exchange));
                            }
                        })
                        .to(""mock:result"");

                from(""timer://test-timer3?fixedRate=true&period=10&delay=1"")
                        .process(new Processor() {
                            @Override
                            public void process(Exchange exchange) throws Exception {
                                exchange.getMessage().setBody(body);
                                exchange.getMessage().setHeader(""#CustomHeader"", resolveTemplate(simpleTemplate, exchange));
                            }
                        })
                        .to(""mock:fail"");

                from(""timer://test-timer4?fixedRate=true&period=10&delay=1"")

                        .process(new Processor() {
                            @Override
                            public void process(Exchange exchange) throws Exception {
                                exchange.getMessage().setBody(body);
                                exchange.getMessage().setHeader(""#CustomHeader"", resolveTemplate(simpleTemplate, exchange));
                            }
                        })
                        .to(""mock:fail"");

                from(""timer://test-timer5?fixedRate=true&period=10&delay=1"")
                        .process(new Processor() {
                            @Override
                            public void process(Exchange exchange) throws Exception {
                                exchange.getMessage().setBody(body);
                                exchange.getMessage().setHeader(""#CustomHeader"", resolveTemplate(simpleTemplate, exchange));
                            }
                        })
                        .to(""mock:fail"");
            }

        };
    }

    public String resolveTemplate(String template, Exchange exchange) {
        var simpleExpression = new SimpleExpression();
        simpleExpression.setExpression(template);
        return simpleExpression.evaluate(exchange, String.class);

    }

    public String resolveTemplateNoCache(String template, Exchange exchange) {
        var simpleExpression = new SimpleExpression();
        //This will force to create a new entry in the cache in the SimpleLanguage class
        String nocache = String.join(""-"", ""-nocache"", UUID.randomUUID().toString());
        simpleExpression.setExpression(""%s%s"".formatted(template, nocache));
        return simpleExpression.evaluate(exchange, String.class).replace(nocache, """");

    }
}
 {code}",rhuanrocha,andrea81,Major,Resolved,Fixed,06/Feb/23 16:26,22/Mar/23 11:03
Bug,CAMEL-19018,13523475,camel-vertx-http: Headers may get erroneously duplicated ,"Given routes configured like this:

{code}
rest()
        .post(""/greeting"")
        .to(""direct:greet"")

        .post(""/hello"")
        .to(""direct:hello"");

from(""direct:greet"")
        .removeHeaders(""CamelHttp*"")
        .to(""vertx-http:http://localhost:8080/hello?httpMethod=POST"");

from(""direct:hello"")
        .setBody().simple(""Hello ${body}"");
{code}

When using cURL to POST to /greeting, the response reveals some of the headers get duplicated:

{code}
< HTTP/1.1 200 OK
< Accept: */*
< Accept: */*
< User-Agent: curl/7.81.0
< User-Agent: Vert.x-WebClient/4.3.7
< User-Agent: curl/7.81.0
< Content-Length: 17
< Content-Type: [application/json, application/json]
{code}
",jamesnetherton,jamesnetherton,Major,Resolved,Fixed,07/Feb/23 10:33,07/Feb/23 13:27
Bug,CAMEL-19026,13523828,camel-jbang - camel.main.backlogTracing=true,The backlog is in context but is disabled. It should be enabled when set this option.,davsclaus,davsclaus,Minor,Resolved,Fixed,08/Feb/23 19:49,07/Jul/23 08:55
Bug,CAMEL-19031,13524136,"When camel saga do compensated, the saga route don't stop it still run the next task.","The problem is : I find that when actionA is timeout and it executed compensated, In theory the saga route should be stopped, not continue to call the next actionB.  In my test show that when {color:#ff0000}the first task can't complete before its timeout, it will run compensation but the saga route don't stop, it still run second task and third task{color}. In my opinion, I think when the first task can't complete and run compensation, the second and third task shouldn't run.  [Test Code|[https://github.com/chen19980/SAGA_timeout_test]]",zhfeng,chen19980,Major,Resolved,Fixed,10/Feb/23 02:38,17/Feb/23 07:56
Bug,CAMEL-19034,13524213,Camel-AWS2-S3: GetObject should preserve the metadata ,"See 

https://camel.zulipchat.com/#narrow/stream/257298-camel/topic/aws2-s3.20getObject.20no.20metadata

GetObject doens't copy the metadata after setting the body.",acosentino,acosentino,Major,Resolved,Fixed,10/Feb/23 11:41,10/Feb/23 13:25
Bug,CAMEL-19047,13524634,CamelTestSupport (junit5) - quarkus and springboot checks are not executed with ContextPerClass,"There is a code in `CamelTestSupport` which verifies whether the right test parent is used in Quarkus or Spring-boot ([code|https://github.com/apache/camel/blob/main/components/camel-test/camel-test-junit5/src/main/java/org/apache/camel/test/junit5/CamelTestSupport.java#L373-L374]). 
These checks should be move above the `if` condition, to be called even if `Lifecycle.PER_CLASS` is used.",jondruse,jondruse,Major,Resolved,Fixed,14/Feb/23 09:09,15/Feb/23 08:47
Bug,CAMEL-19051,13524796,Camel-opentelemetry: Avoid using the GlobalOpenTelemetry.get() and allow for injection of ContextPropagators,"A problem occures when using opentelemetry 4.0.0-M1 with spring-boot auto configuration.

The autoconfiguration works as expected and the resulting openTelemetry bean is as expected.
But when the first request comes in, the autoconfigured bean is not used and another openTelemetry configured with OpenTelemetry.noop() is used.

org.apache.camel.opentelemetry.OpenTelemetryTracer#startExchangeBeginSpan uses GlobalOpenTelemetry.get(), which in turn looks into it's io.opentelemetry.api.GlobalOpenTelemetry#globalOpenTelemetry which was not set and so it creates the noop implementation.

First step to fix without side effects is to allow the explicit setting of the ContextPropagators which then can be used by org.apache.camel.opentelemetry.starter.OpenTelemetryAutoConfiguration#openTelemetryEventNotifier to use the proper autoconfigured instances.

https://camel.zulipchat.com/#narrow/stream/257301-camel-spring-boot/topic/opentelemetry/near/327478493",,acosentino,Major,Resolved,Fixed,15/Feb/23 08:47,15/Feb/23 10:53
Bug,CAMEL-19066,13525085,Multicast EIP sets correlationId on original Exchange,"When copying the result onto the original Exchange the Multicast EIP and related EIPs like Split and RecipientList copy the result back to the original Exchange, this copy includes the correlationId of the subexchange.

This can be somewhat confusing, especially with FlexibleAggregationStrategy which aggregates the result in an Exchange which is correlated to a different Exchange than the original Exchange.

Example:
{code:java}
from(""direct:foo"")
    .log(""Before split: exchangeId:'${exchangeId}' CorrelationId:'${exchangeProperty.CamelCorrelationId}'"")
    .split(body(), AggregationStrategies.flexible().pick(body()).accumulateInCollection(ArrayList.class))
    .log(""In split: exchangeId:'${exchangeId}' CorrelationId:'${exchangeProperty.CamelCorrelationId}'"")
    .end()
    .log(""After split: exchangeId:'${exchangeId}' CorrelationId:'${exchangeProperty.CamelCorrelationId}'""); {code}
Which produces the following Logging output for a two-Element List:
{code:java}
Before split: exchangeId:'742F0530A58A761-0000000000000000' CorrelationId:''
In split: exchangeId:'742F0530A58A761-0000000000000001' CorrelationId:'742F0530A58A761-0000000000000000'
In split: exchangeId:'742F0530A58A761-0000000000000002' CorrelationId:'742F0530A58A761-0000000000000000'
In split: exchangeId:'742F0530A58A761-0000000000000003' CorrelationId:'742F0530A58A761-0000000000000000'
After split: exchangeId:'742F0530A58A761-0000000000000000' CorrelationId:'742F0530A58A761-0000000000000001' {code}",davsclaus,hgarus,Minor,Resolved,Fixed,16/Feb/23 20:37,22/Feb/23 12:43
Bug,CAMEL-19067,13525089,Camel-JBang | camel init creates file but errors out on Windows,"Error while running camel init
 I am noticing that file is getting created - but then it  is erroring out

C:\work\jb>jbang --version
0.101.0
C:\work\jb>camel -V
3.20.2

It seems to not resolve sys:pid variable?

Here is complete stacktrace:
{code:java}
C:\work\jb>camel init b.xml

2023-02-16 12:53:30,548 main ERROR FileManager
(C:\Users\myuser/.camel/${sys:pid}.log) java.io.IOException: The
filename, directory name, or volume label syntax is incorrect
java.io.IOException: The filename, directory name, or volume label
syntax is incorrect
        at java.base/java.io.WinNTFileSystem.canonicalize0(Native Method)
        at java.base/java.io.WinNTFileSystem.canonicalize(WinNTFileSystem.java:465)
        at java.base/java.io.File.getCanonicalPath(File.java:626)
        at java.base/java.io.File.getCanonicalFile(File.java:651)
        at org.apache.logging.log4j.core.util.FileUtils.makeParentDirs(FileUtils.java:139)
        at org.apache.logging.log4j.core.appender.FileManager$FileManagerFactory.createManager(FileManager.java:436)
        at org.apache.logging.log4j.core.appender.FileManager$FileManagerFactory.createManager(FileManager.java:423)
        at org.apache.logging.log4j.core.appender.AbstractManager.getManager(AbstractManager.java:144)
        at org.apache.logging.log4j.core.appender.OutputStreamManager.getManager(OutputStreamManager.java:100)
        at org.apache.logging.log4j.core.appender.FileManager.getFileManager(FileManager.java:183)
        at org.apache.logging.log4j.core.appender.FileAppender$Builder.build(FileAppender.java:99)
        at org.apache.logging.log4j.core.appender.FileAppender$Builder.build(FileAppender.java:52)
        at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:124)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1133)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1058)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1050)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:659)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:257)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:303)
        at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:621)
        at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:694)
        at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:711)
        at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
        at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155)
        at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47)
        at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196)
        at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:137)
        at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:55)
        at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:47)
        at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:33)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:363)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:388)
        at org.apache.camel.util.FileUtil.<clinit>(FileUtil.java:39)
        at org.apache.camel.dsl.jbang.core.commands.Init.call(Init.java:85)
        at org.apache.camel.dsl.jbang.core.commands.Init.call(Init.java:44)
        at picocli.CommandLine.executeUserObject(CommandLine.java:2041)
        at picocli.CommandLine.access$1500(CommandLine.java:148)
        at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2461)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:2453)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:2415)
        at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2273)
        at picocli.CommandLine$RunLast.execute(CommandLine.java:2417)
        at picocli.CommandLine.execute(CommandLine.java:2170)
        at org.apache.camel.dsl.jbang.core.commands.CamelJBangMain.run(CamelJBangMain.java:134)
        at main.CamelJBang.main(CamelJBang.java:36)

2023-02-16 12:53:30,556 main ERROR Could not create plugin of type
class org.apache.logging.log4j.core.appender.FileAppender for element
File: java.lang.IllegalStateException: ManagerFactory
[org.apache.logging.log4j.core.appender.FileManager$FileManagerFactory@f99f5e0]
unable to create manager for [C:\Users\myuser/.camel/${sys:pid}.log]
with data [org.apache.logging.log4j.core.appender.FileManager$FactoryData@6aa61224]
java.lang.IllegalStateException: ManagerFactory
[org.apache.logging.log4j.core.appender.FileManager$FileManagerFactory@f99f5e0]
unable to create manager for [C:\Users\myuser/.camel/${sys:pid}.log]
with data [org.apache.logging.log4j.core.appender.FileManager$FactoryData@6aa61224]
        at org.apache.logging.log4j.core.appender.AbstractManager.getManager(AbstractManager.java:146)
        at org.apache.logging.log4j.core.appender.OutputStreamManager.getManager(OutputStreamManager.java:100)
        at org.apache.logging.log4j.core.appender.FileManager.getFileManager(FileManager.java:183)
        at org.apache.logging.log4j.core.appender.FileAppender$Builder.build(FileAppender.java:99)
        at org.apache.logging.log4j.core.appender.FileAppender$Builder.build(FileAppender.java:52)
        at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:124)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1133)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1058)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1050)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:659)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:257)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:303)
        at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:621)
        at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:694)
        at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:711)
        at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
        at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155)
        at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47)
        at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196)
        at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:137)
        at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:55)
        at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:47)
        at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:33)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:363)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:388)
        at org.apache.camel.util.FileUtil.<clinit>(FileUtil.java:39)
        at org.apache.camel.dsl.jbang.core.commands.Init.call(Init.java:85)
        at org.apache.camel.dsl.jbang.core.commands.Init.call(Init.java:44)
        at picocli.CommandLine.executeUserObject(CommandLine.java:2041)
        at picocli.CommandLine.access$1500(CommandLine.java:148)
        at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2461)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:2453)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:2415)
        at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2273)
        at picocli.CommandLine$RunLast.execute(CommandLine.java:2417)
        at picocli.CommandLine.execute(CommandLine.java:2170)
        at org.apache.camel.dsl.jbang.core.commands.CamelJBangMain.run(CamelJBangMain.java:134)
        at main.CamelJBang.main(CamelJBang.java:36)

2023-02-16 12:53:30,564 main ERROR Unable to invoke factory method in
class org.apache.logging.log4j.core.appender.FileAppender for element
File: java.lang.IllegalStateException: No factory method found for
class org.apache.logging.log4j.core.appender.FileAppender
java.lang.IllegalStateException: No factory method found for class
org.apache.logging.log4j.core.appender.FileAppender
        at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.findFactoryMethod(PluginBuilder.java:260)
        at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:136)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1133)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1058)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:1050)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:659)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:257)
        at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:303)
        at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:621)
        at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:694)
        at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:711)
        at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
        at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155)
        at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47)
        at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196)
        at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:137)
        at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:55)
        at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:47)
        at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:33)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:363)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:388)
        at org.apache.camel.util.FileUtil.<clinit>(FileUtil.java:39)
        at org.apache.camel.dsl.jbang.core.commands.Init.call(Init.java:85)
        at org.apache.camel.dsl.jbang.core.commands.Init.call(Init.java:44)
        at picocli.CommandLine.executeUserObject(CommandLine.java:2041)
        at picocli.CommandLine.access$1500(CommandLine.java:148)
        at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2461)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:2453)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:2415)
        at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2273)
        at picocli.CommandLine$RunLast.execute(CommandLine.java:2417)
        at picocli.CommandLine.execute(CommandLine.java:2170)
        at org.apache.camel.dsl.jbang.core.commands.CamelJBangMain.run(CamelJBangMain.java:134)
        at main.CamelJBang.main(CamelJBang.java:36)

2023-02-16 12:53:30,574 main ERROR Null object returned for File in Appenders.
2023-02-16 12:53:30,575 main ERROR Unable to locate appender ""file""
for logger config ""root""
{code}
",davsclaus,chirag884,Minor,Resolved,Fixed,16/Feb/23 21:28,17/Feb/23 08:36
Bug,CAMEL-19075,13525283,"camel-bean - Incorrect choice of overloaded method with several arguments, if one of them has brackets.","camel-bean incorectly choice method if class has overloaded methods and one of the paramets has bracket close symbol "")"".

 

Here is a test case:

 
{code:java}
@ExtendWith(MockitoExtension.class)
public class BeanProcessorOverloadedMethodsWithBracketsTest extends CamelTestSupport {

    private final String strArgWithBrackets = "")(string_with_brackets()))())"";

    @Test
    public void testOverloadedMethodWithBracketsParams() throws InterruptedException {
        template.sendBody(""direct:start"", null);
        MockEndpoint mock = getMockEndpoint(""mock:result"");
        String receivedExchangeBody = mock.getExchanges().get(0).getMessage().getBody(String.class);
        assertEquals(new MyOverloadedClass().myMethod(strArgWithBrackets, strArgWithBrackets), receivedExchangeBody);
    }

    @Override
    protected RouteBuilder createRouteBuilder() {
        return new RouteBuilder() {
            public void configure() {
                from(""direct:start"")
                        .bean(MyOverloadedClass.class, ""myMethod('"" + strArgWithBrackets + ""', '"" + strArgWithBrackets + ""')"")
                        .to(""mock:result"");
            }
        };
    }

    public static class MyOverloadedClass {
        public String myMethod() {
            return """";
        }

        public String myMethod(String str) {
            return str;
        }

        public String myMethod(String str1, String str2) {
            return str1 + str2;
        }
    }
} {code}
In test example above I am trying to call myMethod with two arguments, but instead of it camel-bean chooses method with one argument.

Test returns the assertion error:
{code:java}
org.opentest4j.AssertionFailedError: 
Expected :)(string_with_brackets()))()))(string_with_brackets()))())
Actual   :)(string_with_brackets()))()){code}
 ",,ArtemSt,Minor,Resolved,Fixed,18/Feb/23 11:30,23/Feb/23 17:18
Bug,CAMEL-19079,13525536,NullPointerException thrown when using the language:xquery endpoint,"I get a {{NullPointerException}} when running a basic route that sends data to the {{language:xquery}} endpoint. It looks like the exception is caused the the Saxon Configuration object not being initialized. 

Here is an example that reproduces the error:
{code:java}
public class CamelXQueryLanguageTest {
    public static void main(String[] args) throws Exception {
        CamelContext context = new DefaultCamelContext();
        context.start();
        
        ProducerTemplate producer = context.createProducerTemplate();
        String result = producer.requestBody(
            ""language:xquery:upper-case(/message/text())"", 
            ""<message>Hello from XQuery</message>"", 
            String.class);
        System.out.println(""Result: "" + result);
        
        context.stop();
    }
}
{code}
Here is the stack trace from this code:
{code:java}
Exception in thread ""main"" org.apache.camel.CamelExecutionException: Exception occurred during execution on the exchange: Exchange[]
	at org.apache.camel.CamelExecutionException.wrapCamelExecutionException(CamelExecutionException.java:45)
	at org.apache.camel.support.ExchangeHelper.extractResultBody(ExchangeHelper.java:660)
	at org.apache.camel.impl.engine.DefaultProducerTemplate.extractResultBody(DefaultProducerTemplate.java:591)
	at org.apache.camel.impl.engine.DefaultProducerTemplate.extractResultBody(DefaultProducerTemplate.java:587)
	at org.apache.camel.impl.engine.DefaultProducerTemplate.requestBody(DefaultProducerTemplate.java:414)
	at org.acme.CamelXQueryLanguageTest.main(CamelXQueryLanguageTest.java:14)
Caused by: org.apache.camel.RuntimeExpressionException: java.lang.NullPointerException: Cannot invoke ""net.sf.saxon.Configuration.makeErrorReporter()"" because ""config"" is null
	at org.apache.camel.component.xquery.XQueryBuilder.evaluate(XQueryBuilder.java:202)
	at org.apache.camel.component.xquery.XQueryBuilder.evaluate(XQueryBuilder.java:167)
	at org.apache.camel.component.language.LanguageProducer.process(LanguageProducer.java:110)
	at org.apache.camel.support.AsyncProcessorConverterHelper$ProcessorToAsyncProcessorBridge.process(AsyncProcessorConverterHelper.java:66)
	at org.apache.camel.impl.engine.SharedCamelInternalProcessor.process(SharedCamelInternalProcessor.java:214)
	at org.apache.camel.impl.engine.SharedCamelInternalProcessor$1.process(SharedCamelInternalProcessor.java:111)
	at org.apache.camel.impl.engine.DefaultAsyncProcessorAwaitManager.process(DefaultAsyncProcessorAwaitManager.java:83)
	at org.apache.camel.impl.engine.SharedCamelInternalProcessor.process(SharedCamelInternalProcessor.java:108)
	at org.apache.camel.support.cache.DefaultProducerCache.send(DefaultProducerCache.java:199)
	at org.apache.camel.impl.engine.DefaultProducerTemplate.send(DefaultProducerTemplate.java:176)
	at org.apache.camel.impl.engine.DefaultProducerTemplate.send(DefaultProducerTemplate.java:172)
	at org.apache.camel.impl.engine.DefaultProducerTemplate.requestBody(DefaultProducerTemplate.java:413)
	... 1 more
Caused by: java.lang.NullPointerException: Cannot invoke ""net.sf.saxon.Configuration.makeErrorReporter()"" because ""config"" is null
	at net.sf.saxon.query.DynamicQueryContext.<init>(DynamicQueryContext.java:58)
	at org.apache.camel.component.xquery.XQueryBuilder.createDynamicContext(XQueryBuilder.java:569)
	at org.apache.camel.component.xquery.XQueryBuilder.evaluateAsDOM(XQueryBuilder.java:230)
	at org.apache.camel.component.xquery.XQueryBuilder.evaluate(XQueryBuilder.java:192)
	... 12 more
{code}
If I create an instance of the Configuration object explicitly and set it in the XQueryLanguage, that NPE disappears only for another one to be thrown when evaluating the expression.

For example, if I add the following 2 lines before starting the context:
{code:java}
XQueryLanguage language = (XQueryLanguage) context.resolveLanguage(""xquery"");
language.setConfiguration(new Configuration());
					
context.start();
{code}
An exception now appears via:
{code}
Caused by: org.apache.camel.RuntimeExpressionException: java.lang.NullPointerException: Cannot invoke ""net.sf.saxon.query.XQueryExpression.run(net.sf.saxon.query.DynamicQueryContext, javax.xml.transform.Result, java.util.Properties)"" because ""expression"" is null
	at org.apache.camel.component.xquery.XQueryBuilder.evaluate(XQueryBuilder.java:202)
	at org.apache.camel.component.xquery.XQueryBuilder.evaluate(XQueryBuilder.java:167)
	at org.apache.camel.component.language.LanguageProducer.process(LanguageProducer.java:110)
	at org.apache.camel.support.AsyncProcessorConverterHelper$ProcessorToAsyncProcessorBridge.process(AsyncProcessorConverterHelper.java:66)
	at org.apache.camel.impl.engine.SharedCamelInternalProcessor.process(SharedCamelInternalProcessor.java:214)
	at org.apache.camel.impl.engine.SharedCamelInternalProcessor$1.process(SharedCamelInternalProcessor.java:111)
	at org.apache.camel.impl.engine.DefaultAsyncProcessorAwaitManager.process(DefaultAsyncProcessorAwaitManager.java:83)
	at org.apache.camel.impl.engine.SharedCamelInternalProcessor.process(SharedCamelInternalProcessor.java:108)
	at org.apache.camel.support.cache.DefaultProducerCache.send(DefaultProducerCache.java:199)
	at org.apache.camel.impl.engine.DefaultProducerTemplate.send(DefaultProducerTemplate.java:176)
	at org.apache.camel.impl.engine.DefaultProducerTemplate.send(DefaultProducerTemplate.java:172)
	at org.apache.camel.impl.engine.DefaultProducerTemplate.requestBody(DefaultProducerTemplate.java:413)
	... 1 more
Caused by: java.lang.NullPointerException: Cannot invoke ""net.sf.saxon.query.XQueryExpression.run(net.sf.saxon.query.DynamicQueryContext, javax.xml.transform.Result, java.util.Properties)"" because ""expression"" is null
	at org.apache.camel.component.xquery.XQueryBuilder.evaluateAsDOM(XQueryBuilder.java:232)
	at org.apache.camel.component.xquery.XQueryBuilder.evaluate(XQueryBuilder.java:192)
	... 12 more
{code}

 ",davsclaus,djcoleman,Minor,Resolved,Fixed,21/Feb/23 10:10,25/Feb/23 17:27
Bug,CAMEL-19081,13525625,Start a route with aggregation fails due to NPE in AggregateProcessor,"In my application I have a route with an aggregation using an aggregation repository ClusteredJdbcAggregationRepository and completionTimeout. This route is part of a micro service that can be scaled. From time to time, the micro service does not start due to a NPE:
{code:java}
Caused by: java.lang.NullPointerException: Cannot invoke ""org.apache.camel.Exchange.getProperty(org.apache.camel.ExchangePropertyKey, Object, java.lang.Class)"" because ""exchange"" is null         at org.apache.camel.processor.aggregate.AggregateProcessor.restoreTimeoutMapFromAggregationRepository(AggregateProcessor.java:920) ~[camel-core-processor-3.20.1.jar:3.20.1]  {code}
In my case the exchange is null because it has already been processed by one of the micro service instance.
 The relevant code is in [https://github.com/apache/camel/blob/camel-3.20.1/core/camel-core-processor/src/main/java/org/apache/camel/processor/aggregate/AggregateProcessor.java#L920]
{code:java}
for (String key : keys) {     
    Exchange exchange = aggregationRepository.get(camelContext, key);     // grab the timeout value     
    long timeout = exchange.getProperty(ExchangePropertyKey.AGGREGATED_TIMEOUT, 0L, long.class);{code}
 
The null check is missing. It's not a problem when aggregation repository is not shared among instance like LevelDB, but it is when it's shared like JDBC.",rnetuka,shenrard,Major,Resolved,Fixed,21/Feb/23 18:01,17/Mar/23 15:57
Bug,CAMEL-19095,13526360,Camel Karaf using buggy Saxon bundle with wrong imports,"Will provide PR soon as I finish testing fix.

Current problems is with:
{code:java}
<bundle dependency='true'>mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.xmlresolver/1.2_5</bundle>
<bundle dependency='true'>mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.saxon/11.4_1</bundle> {code}
Saxon 11.4 requires to import org.xmlresolver (v4.4.3) and not org.apache.xml.resolver (v1.2).

Temporary workaround is to define bundle replacements like below:
{code:java}
<bundle mode=""maven"" originalUri=""mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.xmlresolver/1.2_5"" replacement=""wrap:mvn:org.xmlresolver/xmlresolver/4.4.3""/>
<bundle mode=""maven"" originalUri=""mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.saxon/11.4_1"" replacement=""wrap:mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.saxon/11.4_2$overwrite=merge&amp;Import-Package=javax.xml.datatype,javax.xml.namespace,javax.xml.parsers,javax.xml.stream,javax.xml.stream.events,javax.xml.transform,javax.xml.transform.dom,javax.xml.transform.sax,javax.xml.transform.stream,javax.xml.transform.stax,javax.xml.xpath,org.xmlresolver,org.w3c.dom,org.xml.sax,org.xml.sax.ext,org.xml.sax.helpers,javax.xml,javax.xml.xquery;resolution:=optional,com.saxonica.functions.map;resolution:=optional,com.saxonica.stream;resolution:=optional""/> {code}",davsclaus,nannou9,Minor,Resolved,Fixed,27/Feb/23 16:11,02/Jun/23 06:22
Bug,CAMEL-19098,13526405,Possible performance issue invoking a bean method with a string parameter,"Hi,

 

As discussed in the Camel users email thread - [https://lists.apache.org/thread/3whp1726zttcckh20tdn0j9wj9vt6tgd]

 

I have noticed that when invoking a bean method with a string parameter, it works but a very special handling is needed to avoid a performance hit. Example code below.

When the parameter is not single quoted or double quoted - Camel tries to resolve the value and in the process tries to load a class by that name which is a very expensive process. This behavior is expected IMO.
When the parameter is single quoted or double quoted - Camel still tries to resolve the value as a class name. IMO this is not behaving properly.

This issue happens because ""StringQuoteHelper.splitSafeQuote(methodParameters, ',', true);"" invoked in ""MethodInfo.ParameterExpression#evaluate"" removes the single/double quotes.
Inside ""MethodInfo.evaluateParameterValue"" the ""BeanHelper.isValidParameterValue(exp)"" is invoked and returns false and therefor ""BeanHelper.isAssignableToExpectedType"" > ... > ""DefaultClassResolver.loadClass(String name, ClassLoader loader)"" is invoked every time the bean method is invoked.

The current workaround I found is to add both types of quotes. With this workaround, ""MethodInfo.ParameterExpression#evaluate"" removes the outer set of quotes but keeps the inner one and ""BeanHelper.isValidParameterValue(exp)"" returns true.

 

{{public class MyRouteBuilder extends RouteBuilder {}}
{{    @Override}}
{{    public void configure() throws Exception {}}
{{        from(""timer:foo?period=2000"")}}
{{            .to(""bean:myBean?method=myMethod(slow)"")}}
{{            .to(""bean:myBean?method=myMethod('alsoSlow1')"")}}
{{            .to(""bean:myBean?method=myMethod(\""alsoSlow2\"")"")}}
{{            .to(""bean:myBean?method=myMethod(\""'fast'\"")"");}}
{{    }}}
{{}}}



{{public class MyBean {}}
{{    public void myMethod(String str) {}}
{{        System.out.println(""str = "" + str);}}
{{    }}}
{{}}}

 ",davsclaus,elishamoshe,Major,Resolved,Fixed,27/Feb/23 22:18,01/Mar/23 14:05
Bug,CAMEL-19100,13526523,Milo component does not use dataChangeFilterTrigger value from route,"The dataChangeFilterTrigger configured is not applied when the monitoring is set up.

 
 
{code:java}
return new DataChangeFilter(
   DataChangeTrigger.StatusValueTimestamp,
   this.getDataChangeFilterDeadbandType(),
   this.getDataChangeFilterDeadbandValue());
}{code}

I think the code above has to be changed to something like this:


  
{code:java}
return new DataChangeFilter(
this.dataChangeFilterTrigger != null ? this.dataChangeFilterTrigger : DataChangeTrigger.StatusValueTimestamp,
this.getDataChangeFilterDeadbandType(),
this.getDataChangeFilterDeadbandValue());
}{code}",,rangoy,Major,Resolved,Fixed,28/Feb/23 12:50,01/Mar/23 10:35
Bug,CAMEL-19103,13526672,camel-jbang - can't run in background due to No Camel integration files to run,"Integration test like https://github.com/Croway/camel/blob/jbang-test/dsl/camel-jbang/camel-jbang-core/src/test/java/org/apache/camel/dsl/jbang/BackgroundTest.java fails when a command that uses _ProcessHandle.current().info().commandLine().orElse(null);_ is executed.

In my e2e camel-jbang test suite I'm using a docker image with camel-jbang installed, but when _ProcessHandle.current().info().commandLine().orElse(null);_ I get the same behavior as the integration tests.",Federico Mariani,Federico Mariani,Major,Resolved,Fixed,01/Mar/23 10:58,14/Mar/23 11:12
Bug,CAMEL-19111,13526879,Yaml DSL does not seem to work with split/xtokenize,"This scenario works with XML DSL

Here is XML DSL
{code:xml}
<?xml version=""1.0"" encoding=""UTF-8""?>
<!-- camel-k: language=xml --><routes xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
        xmlns=""http://camel.apache.org/schema/spring""
        xsi:schemaLocation=""
            http://camel.apache.org/schema/spring
            https://camel.apache.org/schema/spring/camel-spring.xsd"">    <!-- Write your routes here, for example: -->
    
    <route>
        <from uri=""file:///c://work//batchsplit//in""/>
        <split streaming=""true"">
            <xtokenize mode=""w"">//BatchMaster</xtokenize>
            <to uri=""file:///c://work//batchsplit//out?fileName=${exchangeId}.xml""/>
        </split>
    </route></routes>
 {code}
Here is Yaml version that I tried:
{code:yaml}
- route:
    id: route-a6b4
    from:
      uri: file:/work/batchsplit/in
      id: from-d1ca
      steps:
        - split:
            id: split-6d45
            expression: //Batchmaster
            xtokenize:
                id: xtokenize-460a
                mode: w
            streaming: true
            steps:
              - to:
                  uri: file:/work/batchsplit/out?fileName=${exchangeId}.xml
                  id: to-1a10
{code}

I have tried to run this thru camel-jbang.",davsclaus,chirag884,Major,Resolved,Fixed,02/Mar/23 15:27,03/Mar/23 07:28
Bug,CAMEL-19112,13526965,Unable to init camel file with JBang for multi dot file name suffix - eg 'foo.camel.yaml',"{code:java}
jbang run -Dcamel.jbang.version=3.20.2 camel@apache/camel init foo.camel.yaml

> Error: Unsupported file type: camel.yaml{code}
Tried also with latest camel version -Dcamel.jbang.version=4.0.0-M1",davsclaus,djelinek,Minor,Resolved,Fixed,03/Mar/23 09:58,25/Apr/23 14:51
Bug,CAMEL-19113,13526991,Platform-http-vertx: consume with comma separated does not work,"When rest DSL defines following endpoint
{code:java}
rest().get(""/test"")
        .consumes(""application/json,application/xml"")
        .produces(""application/json,application/xml"") {code}

requests with `content-type` header `application/json` or `application/xml` should be accepted. (works on camel-platform-htpp)",jondruse,jondruse,Major,Resolved,Fixed,03/Mar/23 13:52,07/Mar/23 12:52
Bug,CAMEL-19124,13527476,Tracer doesn't continue spans for AbstractInternalSpanDecorator,"As discussed in the [PR|https://github.com/apache/camel/pull/9389] it seems that the `Tracer`component from `camel-tracing` is not continuing spans if a `AbstractInternalSpanDecorator` is being created. So for the following route

{code:java}
from(""direct://foo"")
				.log(""hello"")
				.to(""direct://bar"")
				.to(""http://example.org"");
		from(""direct://bar"")
				.log(""hello"")
				.to(""direct://micrometer"")
				.to(""http://example.org"");
		from(""direct://micrometer"")
				.setHeader(MicrometerConstants.HEADER_METRIC_NAME, constant(""new.name""))
				.setHeader(MicrometerConstants.HEADER_METRIC_TAGS, constant(Tags.of(""dynamic-key"", ""dynamic-value"")))
				.to(""micrometer:counter:name.not.used?tags=key=value"")
				.to(""direct://baz"");
		from(""direct://baz"")
				.log(""hello"")
				.to(""bean:com.example.cameldemo.MyBean"")
				.to(""exec:wc?args=--words /usr/share/dict/words"")
				.process(exchange -> {
					// Use the Camel Exec String type converter to convert the ExecResult to String
					// In this case, the stdout is considered as output
					String wordCountOutput = exchange.getIn().getBody(String.class);
					// do something with the word count
					System.out.println(wordCountOutput);
				})
				.to(""file:///tmp/camel-outputdir?flatten=true"")
				.to(""http://example.org"");
{code}

you will get multiple traces since the `direct` components are not linked together (they are treated as a start of a new trace).

 !trace_view_without_fix.png! 

I think we agreed that for this route example, multiple routes are linked together but all in all they form a single trace. So if `org.apache.camel.tracing.Tracer.TracingEventNotifier#shouldExclude` would be changed from 

{code:java}
private boolean shouldExclude(SpanDecorator sd, Exchange exchange, Endpoint endpoint) {
            return sd instanceof AbstractInternalSpanDecorator || !sd.newSpan()
                    || isExcluded(exchange, endpoint);
        }
{code}

to

{code:java}
private boolean shouldExclude(SpanDecorator sd, Exchange exchange, Endpoint endpoint) {
            return !sd.newSpan()
                    || isExcluded(exchange, endpoint);
        }
{code}

then the span parent-child relationship gets maintained the way it should

 !trace_view_with_fix.png! 











",,marcingrzejszczak,Major,Resolved,Fixed,07/Mar/23 15:50,27/Mar/23 10:03
Bug,CAMEL-19133,13528050,camel-zookeeper - Zookeeper's service registration and discovery is not working with serialized,"{{{*}my code{*}:}}

{{@Component}}
{{public class MyRouter extends RouteBuilder {}}
{{@Value(""${server.port}"")}}
{{String port;}}

{{@Override}}
{{public void configure() throws Exception {}}
{{getCamelContext().addRoutePolicyFactory(new ServiceRegistrationRoutePolicyFactory());}}
{{from(""direct:1"")}}
{{.routeProperty(ServiceDefinition.SERVICE_META_NAME, ""addHello"")}}
{{.routeProperty(ServiceDefinition.SERVICE_META_ID, ""my-id"" + port)}}
{{.routeProperty(ServiceDefinition.SERVICE_META_PORT, port)}}
{{.process(new Processor() {}}
{{@Override}}
{{public void process(Exchange exchange) throws Exception {}}
{{exchange.getMessage().setBody(exchange.getMessage().getBody() + ""hello"");}}
{{}}}
{{})}}
{{.log(""${body}"");}}
{{from(""timer://foo?fixedRate=true&period=1000"")}}
{{.serviceCall()}}
{{.name(""addHello"")}}
{{.zookeeperServiceDiscovery()}}
{{.nodes(""127.0.0.1"")}}
{{.basePath(""/config"")}}
{{.end()}}
{{.log(""${body}"");}}
{{}}}
{{}}}

{{issue:}}

{{The service registration function of zookeeper will carry type information during serialization}}
{{!image-2023-03-11-09-09-06-741.png! }}
{{and an error will be reported due to different MetaData during deserialization}}
{{!image-2023-03-11-09-11-24-022.png! }}
{{solution:}}
{{The internal class MetaData in the ZooKeeperServiceDiscovery and ZooKeeperServiceRegistry needs to be extracted}}
{{This is my modified patch in the camel-3.20.2 revision of the camel-zookeeper project}}",davsclaus,ggboy,Minor,Resolved,Fixed,11/Mar/23 01:28,11/Mar/23 07:12
Bug,CAMEL-19136,13528136,camel-micrometer - Too many tags created by micrometer WebMvcTagsProvider,"From discussion  in zulip [https://camel.zulipchat.com/#narrow/stream/257298-camel/topic/springboot.20micrometer.20http.20metrics.20issue/near/340948367]

in https://issues.apache.org/jira/browse/CAMEL-18754 https://github.com/apache/camel-spring-boot/commit/ac318f0418e55c56d0fb426607c67a5ceb1f6742 new configuration has been introduced {{MicrometerTagsAutoConfiguration}}.

Request {{uri}} with placeholders in the metrics tag is replaced with actual value e.g.
{code}
 @RequestMapping(value = ""/users/{id}"" method = RequestMethod.GET)
{code}
instead of 
{code}
http_server_requests_seconds_bucket{application=""my-app"",exception=""None"",method=""GET"",outcome=""SUCCESS"",status=""200"",uri=""/users/{id}"",le=""0.894784851"",} 9999.0
{code}

is reported as 

{code}
http_server_requests_seconds_bucket{application=""my-app"",exception=""None"",method=""GET"",outcome=""SUCCESS"",status=""200"",uri=""/users/1"",le=""0.894784851"",} 1.0
http_server_requests_seconds_bucket{application=""my-app"",exception=""None"",method=""GET"",outcome=""SUCCESS"",status=""200"",uri=""/users/2"",le=""0.894784851"",} 1.0
http_server_requests_seconds_bucket{application=""my-app"",exception=""None"",method=""GET"",outcome=""SUCCESS"",status=""200"",uri=""/users/3"",le=""0.894784851"",} 1.0
http_server_requests_seconds_bucket{application=""my-app"",exception=""None"",method=""GET"",outcome=""SUCCESS"",status=""200"",uri=""/users/999999"",le=""0.894784851"",} 1.0
{code}
and of course will end up a lot of tags which is bad for prometheus.

and spring reports warning
{code}
logger_name     org.springframework.boot.actuate.autoconfigure.metrics.OnlyOnceLoggingDenyMeterFilter
message     Reached the maximum number of URI tags for 'http.server.requests'.
How can we preserve original uri tag? We combine camel with spring boot, so rest api layer is provided by spring boot but integration layer (messaging, http client etc.. ) is done by camel, so would love to keep camel route stats but keep spring boot reported metrics
{code}

basically
{code}
@Bean
    WebMvcTagsProvider webMvcTagsProvider() {
....
return Tags.concat(
                        super.getTags(request, response, handler, exception),
                        Tags.of(Tag.of(""uri"", uri))
                );
{code}
makes out of {{/users/\{id\}}} -> {{/users/1}}
by excluding 
{code}
@SpringBootApplication(exclude = {MicrometerTagsAutoConfiguration.class}) 
{code}

we get the original behavior. But the risk is that if future versions add more beans into MicrometerTagsAutoConfiguration they will be ignored

How can we preserve original uri tag or make it configurable?
may be regex/whitelist/blacklist on which it should apply? 




 ",davsclaus,Mustermann,Major,Resolved,Fixed,12/Mar/23 19:41,19/Mar/23 11:06
Bug,CAMEL-19150,13528577,camel-olingo4: queryParams option of read method does not work,"When using olingo4://read and providing the queryParams option, it seems the value is not handled correctly. The toString() value of the provided map is used as query parameters to the odata service.

There's some more detail in the original Camel Quarkus issue here:

https://github.com/apache/camel-quarkus/issues/4654",zhfeng,jamesnetherton,Minor,Resolved,Fixed,15/Mar/23 07:36,22/Mar/23 10:59
Bug,CAMEL-19151,13528590,The 'ignoreInvalidEndpoint' option isn't relevant for a static URI for WireTap component,"The 'ignoreInvalidEndpoint' option for the WireTap component has no effect on the non-dynamic endpoints.

The worst thing is that there is no warning that the set option is going to be ignored.

I'm not sure about the design decision here, but from the user point of view it's confusing and not what one can expect.

 

My use case:

I defined a route configuration that by default does some processing of the intercepted messages and also sends them to a preconfigured endpoint. In my design this endpoint is optional and a route for it could be defined only if the default processing isn't enough. 

But because the endpoint URI is static and the 'ignoreInvalidEndpoint' has no effect, I get the messages stuck in-flight.",davsclaus,Gomoliako,Minor,Resolved,Fixed,15/Mar/23 09:16,15/Mar/23 15:04
Bug,CAMEL-19155,13528637,Azure Service Bus component completes messages instead of abandoning on error,"The Azure Service Bus component currently invokes the \{{ServiceBusReceiverAsyncClient#complete}} method even if an error occurs during route processing, which can result in message loss. It should invoke 
{\{ServiceBusReceiverAsyncClient#abandon}} method on error to enable Service Bus to attempt redelivery and/or dead-letter the message as per its configuration.
 
We are currently using a dead letter error channel to work around this issue, but this solution is still not ideal, since the message could still be marked as completed if the publication to the dead letter channel fails.",,dylan.piergies,Major,Resolved,Fixed,15/Mar/23 13:35,23/Mar/23 08:55
Bug,CAMEL-19156,13528643,XML route configurations ignored without both XML IO and JAXB XML loaded,"XML route configurations are ignored unless both camel-xml-io-dsl-starter and camel-xml-jaxb-dsl-starter are loaded; XML routes are started without configurations.

To reproduce:
 * check out camel-spring-boot-examples, either main branch or camel-spring-boot-examples-3.20.0 tag
 * navigate to routes-configuration subdir
 * remove dependency on either DSL from pom.xml
 * run mvn spring-boot:run

Result:

{{2023-03-15 15:10:37.665 INFO 24848 — [ main] o.a.c.impl.engine.AbstractCamelContext : Routes startup (started:3)}}
{{2023-03-15 15:10:37.665 INFO 24848 — [ main] o.a.c.impl.engine.AbstractCamelContext : Started route1 (timer://java) (source: java:29)}}
{{2023-03-15 15:10:37.665 INFO 24848 — [ main] o.a.c.impl.engine.AbstractCamelContext : Started route2 (timer://xml)}}
{{2023-03-15 15:10:37.665 INFO 24848 — [ main] o.a.c.impl.engine.AbstractCamelContext : Started route3 (timer://yaml) (source: my-yaml-route.yaml:22)}}
{{2023-03-15 15:10:37.665 INFO 24848 — [ main] o.a.c.impl.engine.AbstractCamelContext : Routes configuration:}}
{{2023-03-15 15:10:37.665 INFO 24848 — [ main] o.a.c.impl.engine.AbstractCamelContext : route1 (javaError)}}
{{2023-03-15 15:10:37.666 INFO 24848 — [ main] o.a.c.impl.engine.AbstractCamelContext : route3 (yamlError)}}
{{2023-03-15 15:10:37.666 INFO 24848 — [ main] o.a.c.impl.engine.AbstractCamelContext : Apache Camel 3.20.0 (MyCamel) started in 1s43ms (build:80ms init:329ms start:634ms)}}



XML route is loaded and started, but without the configurations. No error or warning is logged about missing configuration.",davsclaus,jsz,Minor,Resolved,Fixed,15/Mar/23 14:15,24/Mar/23 09:38
Bug,CAMEL-19158,13528769,camel-core: ThrowExceptionProcessor may silently ignore exceptions in constructing the exception object,"Back in Camel 3.12, a change was made to ThrowExceptionProcessor: 

https://github.com/apache/camel/blob/fb0528706d7e8a7d0d6ba22dfb649cb31cf06bc8/core/camel-core-processor/src/main/java/org/apache/camel/processor/ThrowExceptionProcessor.java#L77-L82

If you happen to drop into the else clause, then no exception is set on the exchange and it's confusing to users as to why no exception was thrown in their route. This is potentially a common thing on Camel Quarkus native mode if the specific exception type was not registered for reflection.",jamesnetherton,jamesnetherton,Minor,Resolved,Fixed,16/Mar/23 09:57,16/Mar/23 13:26
Bug,CAMEL-19162,13528932,camel-ehcache - llegalStateException: Close not supported from UNINITIALIZED. When context.addRouteDefinition() called multiple times in route with Ehcache consumer,"In production, when starting the route, we also call _context.addRouteDefinition()_  so that context changes are picked up. Exception occurs on second stop/start of route with call  _context.addRouteDefinition()_ with  cache consumer definition added

Project with reproduction [^cache-camel-error.zip]

Test !image-2023-03-17-12-32-34-499.png!

Config 

!image-2023-03-17-12-33-14-402.png!

Exception StackTrace

 

 
{code:java}
org.apache.camel.RuntimeCamelException: java.lang.IllegalStateException: Close not supported from UNINITIALIZED
    at org.apache.camel.RuntimeCamelException.wrapRuntimeCamelException(RuntimeCamelException.java:51)
    at org.apache.camel.support.ChildServiceSupport.shutdown(ChildServiceSupport.java:120)
    at org.apache.camel.impl.engine.AbstractCamelContext.shutdownRouteService(AbstractCamelContext.java:3717)
    at org.apache.camel.impl.engine.AbstractCamelContext.removeRoute(AbstractCamelContext.java:1388)
    at org.apache.camel.impl.DefaultCamelContext.removeRoute(DefaultCamelContext.java:957)
    at org.apache.camel.impl.engine.AbstractCamelContext.removeRoute(AbstractCamelContext.java:1369)
    at org.apache.camel.impl.DefaultModel.removeRouteDefinition(DefaultModel.java:208)
    at org.apache.camel.impl.DefaultModel.removeRouteDefinitions(DefaultModel.java:197)
    at org.apache.camel.impl.DefaultModel.addRouteDefinitions(DefaultModel.java:175)
    at org.apache.camel.impl.DefaultModel.addRouteDefinition(DefaultModel.java:191)
    at org.apache.camel.impl.DefaultCamelContext.addRouteDefinition(DefaultCamelContext.java:350)
    at ru.factorts.cache.SpringFirstTest.testCache(SpringFirstTest.java:22)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
    at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
    at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
    at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
    at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
    at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
    at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at java.util.ArrayList.forEach(ArrayList.java:1249)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at java.util.ArrayList.forEach(ArrayList.java:1249)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
    at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
    at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
    at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:71)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: java.lang.IllegalStateException: Close not supported from UNINITIALIZED
    at org.ehcache.core.InternalStatus.close(InternalStatus.java:74)
    at org.ehcache.core.StatusTransitioner.close(StatusTransitioner.java:86)
    at org.ehcache.core.EhcacheBase.close(EhcacheBase.java:577)
    at java.util.concurrent.ConcurrentHashMap$ValuesView.forEach(ConcurrentHashMap.java:4707)
    at org.apache.camel.component.ehcache.EhcacheManager.stop(EhcacheManager.java:63)
    at org.apache.camel.component.ehcache.EhcacheEndpoint.doStop(EhcacheEndpoint.java:84)
    at org.apache.camel.support.service.BaseService.stop(BaseService.java:160)
    at org.apache.camel.support.service.ServiceHelper.stopService(ServiceHelper.java:162)
    at org.apache.camel.support.service.ServiceHelper.stopAndShutdownServices(ServiceHelper.java:257)
    at org.apache.camel.support.service.ServiceHelper.stopAndShutdownServices(ServiceHelper.java:215)
    at org.apache.camel.impl.engine.RouteService.doShutdown(RouteService.java:302)
    at org.apache.camel.support.ChildServiceSupport.shutdown(ChildServiceSupport.java:113)
    ... 78 more
 
{code}
 ",davsclaus,lukymiv,Minor,Resolved,Fixed,17/Mar/23 09:35,24/Mar/23 21:49
Bug,CAMEL-19169,13529078,camel-olingo2: queryParams option of read method does not work,There is a similar problem with olingo2. The origin report for olingo4 is CAMEL-19150,zhfeng,zhfeng,Major,Resolved,Fixed,18/Mar/23 13:15,20/Mar/23 12:20
Bug,CAMEL-19171,13529226,camel-http - Prevent duplicating slashes in generated URI,"After upgrading to HttpComponents v5, [Jenkins raised 2 regressions|https://ci-builds.apache.org/job/Camel/job/Camel%20JDK17/job/main/688/testReport/] which are {{VertxPlatformHttpProxyTest.testProxy}} and {{PlatformHttpProxyTest.testProxy}}. 

After a deeper investigation, it seems to be due to the fact that when the endpoint URI has a trailing slash and the path starts with a slash, the generated URI contains 2 slashes instead of only one which was accepted by HttpComponents v4 but it is rejected by HttpComponents v5.",nfilotto,nfilotto,Minor,Resolved,Fixed,20/Mar/23 09:28,20/Mar/23 11:03
Bug,CAMEL-19174,13529318,Jira component: duplicate messages with the new issues consumer,"When more than one JIRA issue fulfills the JQL and is available on a poll, it will result in  duplicates handled multiple times in the next poll. It depends on how many issues were created within the poll delay. Issues will be handled N times (N is the number of issues that were created at the same time).

How to reproduce:
- Make sure to return multiple issues for a JQL query
- Wait multiple polls and review the resulting issues being handled by the Camel route",christophd,christophd,Minor,Resolved,Fixed,20/Mar/23 21:24,24/Mar/23 05:23
Bug,CAMEL-19181,13529475,camel-springboot - mapstruct component is not autoconfigured automatically,"In a Camel Spring Boot application MapStruct component is not autoconfigured automatically, as you can see from the reproducer, even if application.properties contains _camel.component.mapstruct.mapper-package-name_ the component is not registered into the camel context, therefore the converter is not found and the following exception is raised:
_NoTypeConversionAvailableException: No type converter available to convert from type: org.jboss.fuse.tnb.universal.component.mapstruct.Car to the required type: org.jboss.fuse.tnb.universal.component.mapstruct.CarDto_

By uncommenting lines MyRouteBuilder:18 and MyRouteBuilder:19, the reproducer works, but I think that Camel Spring Boot should configure the component automatically.
",ldemasi,Federico Mariani,Major,Resolved,Fixed,21/Mar/23 16:09,23/Mar/23 13:45
Bug,CAMEL-19190,13529964,camel-vertx-websocket: sendToAll option may not discover connected host peers correctly,"While adding some more test coverage for vertx-websocket in Camel Quarkus, I noticed the logic to find connected peers on the local server when using the sendToAll option or the CamelVertxWebsocket.connectionKey header is a bit flawed. It can fail to find the correct peer, or incorrectly find matches for the wrong peers.

There's some more background in this Camel Quarkus issue:

https://github.com/apache/camel-quarkus/issues/4628
",jamesnetherton,jamesnetherton,Major,Resolved,Fixed,24/Mar/23 12:01,27/Mar/23 06:39
Bug,CAMEL-19198,13530068,Dynamic Router EIP component does not evaluate filters by order of priority attribute,"After switching the list of filters from a list to a map, the component no longer evaluates filter by order of their priority attribute.  Filter evaluation needs to ensure sorting by priority.",Steve973,Steve973,Minor,Resolved,Fixed,25/Mar/23 15:22,26/Mar/23 12:22
Bug,CAMEL-19199,13530096,Unable to start PLC4X route in camel-plc4x,"I am using [ModbusPal|https://plc4x.apache.org/users/getting-started/virtual-modbus.html] to create the Virtual modbus.

I am using the endpoint URI as:

 
{code:java}
plc4x:modbus-tcp://localhost:502?unitId=1&dataType=holding-register&addresses=1{code}
Following is the screenshot of Modbus holding registers:

!https://user-images.githubusercontent.com/62088117/227714761-e6486993-f44a-4482-a7be-e76e5a94b49c.png|width=354,height=350!

But when starting camel context it throws NullPointerException.

 
{code:java}
java.lang.NullPointerException
    at org.apache.camel.component.plc4x.Plc4XConsumer.startUnTriggered(Plc4XConsumer.java:89)
    at org.apache.camel.component.plc4x.Plc4XConsumer.doStart(Plc4XConsumer.java:81)
    at org.apache.camel.support.service.BaseService.start(BaseService.java:119) {code}
I debugged it and found the problem is with this line in PLC4XConsumer line 89:

 
{code:java}
for (Map.Entry<String, Object> tag : tags.entrySet()) {{code}
 

Here *tags* are null, so it might be that I am not configuring the endpoint correctly.

So, I tried by configuring the endpoint by creating an instance of Plc4XEndpoint and then configuring the tags there, it works like:
{code:java}
Map<String, Object> map = new HashMap<>();
            map.put(""value-1"", ""holding-register:1"");
            
            Plc4XEndpoint plc4xEndpoint = new Plc4XEndpoint(""plc4x:modbus-tcp://localhost:502"",
                    getContext().getComponent(""plc4x""));
            plc4xEndpoint.setTags(map);
            from(plc4xEndpoint).log(""value : ${body}""); {code}
But why it is not working if I configure the endpoint as just String?

Note:

I tried with below endpoint and it is also not working:
{code:java}
plc4x:modbus-tcp://localhost:502?tags={unitId=1&dataType=holding-register&addresses=1} {code}
 ",davsclaus,mdanish98,Minor,Resolved,Fixed,26/Mar/23 09:13,06/Jun/23 10:20
Bug,CAMEL-19220,13530630,camel-groovy - Avoid setting variables to initialize the binding,"The changes made for CAMEL-19212 cause runtime errors in native mode https://github.com/apache/camel-quarkus/issues/4712 of the following type: 


{noformat}
2023-03-29T09:15:06.5898355Z Caused by: org.codehaus.groovy.runtime.typehandling.GroovyCastException: Cannot cast object 'null' with class 'null' to class 'int'. Try 'java.lang.Integer' instead
2023-03-29T09:15:06.5901668Z 	at org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.castToNumber(DefaultTypeTransformation.java:186)
2023-03-29T09:15:06.5902449Z 	at org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation.intUnbox(DefaultTypeTransformation.java:92)
2023-03-29T09:15:06.5903768Z 	at org.apache.camel.quarkus.component.groovy.generated.Expression668axhkmpOfBwe2iixBEtT194721N7sqMJrOlgVZAJVxs61.run(org.apache.camel.quarkus.component.groovy.generated.Expression668axhkmpOfBwe2iixBEtT194721N7sqMJrOlgVZAJVxs61:5)
2023-03-29T09:15:06.5907252Z 	at org.apache.camel.language.groovy.GroovyExpression.evaluate(GroovyExpression.java:53)
2023-03-29T09:15:06.5907829Z 	at org.apache.camel.support.ExpressionSupport.matches(ExpressionSupport.java:36)
2023-03-29T09:15:06.5913250Z 	at org.apache.camel.processor.FilterProcessor.matches(FilterProcessor.java:89)
2023-03-29T09:15:06.5913967Z 	at org.apache.camel.processor.ChoiceProcessor.process(ChoiceProcessor.java:71)
2023-03-29T09:15:06.5914600Z 	at org.apache.camel.processor.errorhandler.RedeliveryErrorHandler$SimpleTask.run(RedeliveryErrorHandler.java:474)
2023-03-29T09:15:06.5916861Z 	at org.apache.camel.impl.engine.DefaultReactiveExecutor$Worker.schedule(DefaultReactiveExecutor.java:181)
2023-03-29T09:15:06.5917520Z 	at org.apache.camel.impl.engine.DefaultReactiveExecutor.scheduleMain(DefaultReactiveExecutor.java:59)
2023-03-29T09:15:06.5922296Z 	at org.apache.camel.processor.Pipeline.process(Pipeline.java:164)
2023-03-29T09:15:06.5922898Z 	at org.apache.camel.impl.engine.CamelInternalProcessor.process(CamelInternalProcessor.java:378)
2023-03-29T09:15:06.5926160Z 	at org.apache.camel.component.direct.DirectProducer.process(DirectProducer.java:96)
2023-03-29T09:15:06.5927374Z 	at org.apache.camel.impl.engine.SharedCamelInternalProcessor.process(SharedCamelInternalProcessor.java:213)
2023-03-29T09:15:06.5932016Z 	at org.apache.camel.impl.engine.SharedCamelInternalProcessor$1.process(SharedCamelInternalProcessor.java:110)
2023-03-29T09:15:06.5934733Z 	at org.apache.camel.impl.engine.DefaultAsyncProcessorAwaitManager.process(DefaultAsyncProcessorAwaitManager.java:81)
2023-03-29T09:15:06.5940964Z 	at org.apache.camel.impl.engine.SharedCamelInternalProcessor.process(SharedCamelInternalProcessor.java:107)
2023-03-29T09:15:06.5944226Z 	at org.apache.camel.support.cache.DefaultProducerCache.send(DefaultProducerCache.java:164)
2023-03-29T09:15:06.5944848Z 	at org.apache.camel.impl.engine.DefaultProducerTemplate.send(DefaultProducerTemplate.java:176)
2023-03-29T09:15:06.5950089Z 	at org.apache.camel.impl.engine.DefaultProducerTemplate.send(DefaultProducerTemplate.java:172)
2023-03-29T09:15:06.5950820Z 	at org.apache.camel.impl.engine.DefaultProducerTemplate.requestBody(DefaultProducerTemplate.java:413)
{noformat}
",nfilotto,nfilotto,Minor,Resolved,Fixed,29/Mar/23 14:00,29/Mar/23 16:15
Bug,CAMEL-19224,13530799,camel-azure - BlobConsumer does not use prefix,"When using prefix on azure endpoints the correct returned results should be all blobs starting with provided prefix, instead all blobs are retrieved. The issue is described in [Zulip chat thread|https://camel.zulipchat.com/#narrow/stream/257298-camel/topic/CamelAzureBlob.20endpoint.20prefix ]",,ipaulbogdan,Major,Resolved,Fixed,30/Mar/23 13:19,03/Apr/23 13:44
Bug,CAMEL-19242,13531287,camel-jbang - camel doc may not work on main,"~/workspace ❯ camel doc jms
java.lang.ClassNotFoundException: org.apache.camel.kamelets.catalog.KameletsCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)
	at org.apache.camel.main.download.DependencyDownloaderClassLoader.loadClass(DependencyDownloaderClassLoader.java:45)
	at org.apache.camel.dsl.jbang.core.commands.catalog.KameletCatalogHelper.loadKamelets(KameletCatalogHelper.java:151)
	at org.apache.camel.dsl.jbang.core.commands.catalog.KameletCatalogHelper.loadKameletModel(KameletCatalogHelper.java:168)
	at org.apache.camel.dsl.jbang.core.commands.catalog.CatalogDoc.doCall(CatalogDoc.java:130)",,davsclaus,Major,Resolved,Fixed,03/Apr/23 13:51,05/Apr/23 07:48
Bug,CAMEL-19247,13531430,camel-zeebe - Set Default Host and Port for Zeebe connection,The defaults for host and port of the zeebe service are not set.,,karstenreuter,Minor,Resolved,Fixed,04/Apr/23 11:01,05/Apr/23 04:31
Bug,CAMEL-19249,13531459,camel-salesforce: Creating blob data is broken,"This prevents creating Documents, Files (ContentVersion), etc.

Typical error message:

Cannot locate field VersionData on class org.apache.camel.component.salesforce.dto.generated.ContentVersion (through reference chain: org.apache.camel.component.salesforce.dto.generated.ContentVersion[""VersionData""]",jeremyross,jeremyross,Major,Resolved,Fixed,04/Apr/23 14:49,04/Apr/23 18:32
Bug,CAMEL-19250,13531477,Classes generated by camel-restdsl-openapi-plugin are not added to jar ,"{{swagger-codegen-maven-plugin}} executed by {{camel-restdsl-openapi-plugin}} during the model generation,  modifies the source list directory so the maven compiler plugin ignores the other classes generated by {{{}camel-restdsl-openapi-plugin{}}}.",ldemasi,ldemasi,Minor,Resolved,Fixed,04/Apr/23 15:56,05/Apr/23 09:48
Bug,CAMEL-19255,13531770,Jbang: jbang is not copying custom kamelets  to /kametets,"[Kamelet Component|https://camel.apache.org/components/3.20.x/kamelet-component.html] is expecting the default kamelet location to be classpath:/kamelets.

 
{code:java}
`jbang --debug -Dcamel.jbang.version=3.20.3 camel@apache/camel export --local-kamelet-dir=../kamelets` {code}
Jbang export copies kamelet files to src\main\resources\camel folder, so when running 'mvn quarkus:dev` gives an error that kamelet is not found.

 
{code:java}
java.io.FileNotFoundException: Resource not found: classpath:/kamelets/example.kamelet.yaml
        at org.apache.camel.dsl.yaml.YamlRoutesBuilderLoaderSupport.doLoadRouteBuilder(YamlRoutesBuilderLoaderSupport.java:76)
        at org.apache.camel.dsl.support.RouteBuilderLoaderSupport.loadRoutesBuilder(RouteBuilderLoaderSupport.java:102)
        at org.apache.camel.impl.engine.DefaultRoutesLoader.findRoutesBuilders(DefaultRoutesLoader.java:125)
        at org.apache.camel.spi.RoutesLoader.findRoutesBuilders(RoutesLoader.java:120)
        at org.apache.camel.spi.RoutesLoader.loadRoutes(RoutesLoader.java:75)
        at org.apache.camel.support.RouteTemplateHelper.loadRouteTemplateFromLocation(RouteTemplateHelper.java:106)
        at org.apache.camel.component.kamelet.KameletComponent$LifecycleHandler.createRouteForEndpoint(KameletComponent.java:403)
        at org.apache.camel.component.kamelet.KameletComponent$LifecycleHandler.onContextInitialized(KameletComponent.java:430)
        at org.apache.camel.impl.engine.AbstractCamelContext.doInit(AbstractCamelContext.java:3017)
        at org.apache.camel.quarkus.core.FastCamelContext.doInit(FastCamelContext.java:174)
        at org.apache.camel.support.service.BaseService.init(BaseService.java:83)
        at org.apache.camel.impl.engine.AbstractCamelContext.init(AbstractCamelContext.java:2679)
        at org.apache.camel.support.service.BaseService.start(BaseService.java:111)
        at org.apache.camel.impl.engine.AbstractCamelContext.start(AbstractCamelContext.java:2698)
        at org.apache.camel.impl.DefaultCamelContext.start(DefaultCamelContext.java:262)
        at org.apache.camel.quarkus.main.CamelMain.doStart(CamelMain.java:94)
        at org.apache.camel.support.service.BaseService.start(BaseService.java:119) {code}
 ",,msharma,Major,Resolved,Fixed,06/Apr/23 18:19,09/Apr/23 05:49
Bug,CAMEL-19256,13531808,camel-jdbc: leaks the statement in doCreateAndExecuteSqlStatement,It comes cross [https://github.com/apache/camel-quarkus/issues/4752] and I think it should close the statment in the finnal block if shouldCloseResources is true.,zhfeng,zhfeng,Major,Resolved,Fixed,07/Apr/23 02:18,14/Apr/23 12:13
Bug,CAMEL-19257,13531823,camel-jslt: Exception when using camel-jslt in Tomcat Servlet,"When a JSLT transformation is used in a route which is called from a Tomcat Servlet in Spring Boot, an exception is thrown after the Exchange is completely processed and the response is being returned to the client:

{{java.lang.IllegalStateException: getWriter() has already been called for this response}}
{{at org.apache.catalina.connector.Response.getOutputStream(Response.java:552) ~[tomcat-embed-core-9.0.70.jar:9.0.70]}}
{{at org.apache.catalina.connector.ResponseFacade.getOutputStream(ResponseFacade.java:210) ~[tomcat-embed-core-9.0.70.jar:9.0.70]}}
{{at org.apache.camel.http.common.DefaultHttpBinding.doWriteDirectResponse(DefaultHttpBinding.java:537) ~[camel-http-common-3.20.3.jar:3.20.3]}}
{{at org.apache.camel.http.common.DefaultHttpBinding.doWriteResponse(DefaultHttpBinding.java:439) ~[camel-http-common-3.20.3.jar:3.20.3]}}
{{at org.apache.camel.http.common.DefaultHttpBinding.writeResponse(DefaultHttpBinding.java:362) ~[camel-http-common-3.20.3.jar:3.20.3]}}
{{at org.apache.camel.http.common.CamelServlet.afterProcess(CamelServlet.java:349) ~[camel-http-common-3.20.3.jar:3.20.3]}}

This happens because the Response object is held in a header and getWriter() gets prematurely called when the headers are serialized in JsltEndpoint.extractVariables().",,jsz,Major,Resolved,Fixed,07/Apr/23 06:19,09/Apr/23 07:53
Bug,CAMEL-19259,13532174,camel-maven-plugin - Use configured main-class instead of KameletMain,"[INFO] --- camel-maven-plugin:3.20.3:run (default-cli) @ acme ---
[INFO] You can skip tests from the command line using: mvn camel:run -Dmaven.test.skip=true
[INFO] Kamelets YAML files detected in directory /Users/davsclaus/workspace/deleteme/java-jar/code/src/main/resources/kamelets
[INFO] Using org.apache.camel.main.KameletMain to initiate a CamelContext
[INFO] Starting Camel ...
[ERROR] *************************************
[ERROR] Error occurred while running main from: org.apache.camel.main.KameletMain
[ERROR]
java.lang.ClassNotFoundException: org.apache.camel.main.KameletMain",davsclaus,davsclaus,Minor,Resolved,Fixed,11/Apr/23 11:27,11/Apr/23 11:30
Bug,CAMEL-19263,13532352,oracle-database-source.kamelet doesn't create the correct JDBC connection String ,"Current value is 
{code:java}
value: 'jdbc:oracle:thin://{{serverName}}:{{serverPort}}/{{databaseName}}' {code}

Expected value is

{code:java}
value: 'jdbc:oracle:thin:@{{serverName}}:{{serverPort}}/{{databaseName}}'
{code}

 ",,msharma,Major,Closed,Fixed,12/Apr/23 13:52,12/Apr/23 14:16
Bug,CAMEL-19267,13532541,camel-jbang - Export to Quarkus with custom java processor may export to wrong folder,"Just a placeholder that to look into if this is a problem, where a custom java processor ends up in src/main/resources/camel instead of src/main/java",davsclaus,davsclaus,Major,Resolved,Fixed,13/Apr/23 16:48,16/Apr/23 11:19
Bug,CAMEL-19271,13532796,camel-jbang - trace with --latest should show last completed if no current inflight,Check if it works as expected as it appears that --latest waits for new incoming message instead of taking last complete,davsclaus,davsclaus,Minor,Resolved,Fixed,16/Apr/23 15:18,17/Apr/23 08:50
Bug,CAMEL-19281,13533264,Aws2- healthchecks not closing resources for awsClient,"As AWS2S3ConsumerHealthCheck and  Sqs2ConsumerHealthCheck working with 

SqsClient and S3Client, which extends AutoCloseble , meaning those have to be properly handled in terms of closing resources, we noticed that in our services invoking aws health checks leads to connection memory leak of *org.apache.http.impl.conn.PoolingHttpClientConnectionManager* , pls take a look",rhuanrocha,artemtraier,Major,Resolved,Fixed,19/Apr/23 15:50,23/Jun/23 10:00
Bug,CAMEL-19285,13533436,Kafka consumer can flood brokers if TLS handshake fails and pollOnError is set to RECONNECT,"The Kafka consumer does not respect reconnect backoff options when a TLS handshake fails if the consumer's {{pollOnError}} option is set to {{{}RECONNECT{}}}, resulting in reconnection attempts being made in a tight loop without delays, meaning that Camel applications consuming from Kafka topics can effectively mount a DDoS attack on the Kafka broker. This effect is amplified if concurrent consumers are in use, since each consumer thread is making its own connection attempts.

Naturally, we found this out the hard way, in production, when another team put in place a firewall rule to allow connections from our consumers. The amount of TLS handshake traffic generated was sufficient to overwhelm the broker, resulting in an outage.

I have created a small project to demonstrate the issue against a containerised Kafka broker here: [https://github.com/dylanpiergies/kafka-camel-flood-issue]

This issue does not occur when a connection fails for other reasons (e.g. connection refused, connection timeout); in these cases the reconnect backoff behaves as expected.",orpiske,dylan.piergies,Major,Resolved,Fixed,20/Apr/23 16:36,01/Jul/23 12:55
Bug,CAMEL-19293,13533803,camel-spring-ldap - base is set twice when using SB AutoConfiguration,"When using SB AutoConfiguration and the base is set by SB on the ldapTemplate, camel set the base twice.",Federico Mariani,Federico Mariani,Major,Resolved,Fixed,24/Apr/23 08:48,24/Apr/23 14:27
Bug,CAMEL-19295,13533883,Concurrency issues with dynamicMap in AbstractDynamicRegistry,"Hello everyone,

We have detected concurrency issues related to the dynamicMap attribute of the AbstractDynamicRegistry class. By default, dynamicMap is an object of the SimpleLRUCache class, which inherits from LinkedHashMap.

We have resolved the issue by creating our own LRUCacheFactory which returns a thread-safe synchronized map backed by the specified map.

Could you please review it?",rhuanrocha,adavila,Major,Resolved,Fixed,24/Apr/23 17:48,28/May/23 10:23
Bug,CAMEL-19296,13534014,Unable to init camel file with JBang for multi dot file name suffix - eg 'foo.camel.xml',"{code:java}
jbang run -Dcamel.jbang.version=3.20.3 camel@apache/camel init foo.camel.xml

> Error: Unsupported file type: camel.xml{code}
Tried also with latest camel version -Dcamel.jbang.version=4.0.0-M2",davsclaus,djelinek,Minor,Resolved,Fixed,25/Apr/23 14:51,26/May/23 08:22
Bug,CAMEL-19298,13534322,Snmp: version 3 is not supported for several actions for the component,"In snmp component, users can use versions 0.1 and 2 (using values from the [documentation|https://camel.apache.org/components/3.20.x/snmp-component.html#_endpoint_query_option_snmpVersion]).

Version 3 is not supported for several actions (like provide PDU or get next)
To support this version on component, different PDU class has to be used. (for example here https://github.com/apache/camel/blob/main/components/camel-snmp/src/main/java/org/apache/camel/component/snmp/SnmpProducer.java#L85)

There is a test coverage (creation in progress) in camel-quarkus, which could be used as a reproducer simulating missing scenarios - https://github.com/apache/camel-quarkus/issues/4843",jondruse,jondruse,Major,Resolved,Fixed,27/Apr/23 12:51,16/May/23 14:56
Bug,CAMEL-19339,13535823,karaf - ConnectionFactory not found when use camel-activemq,"Hello.

I use Apache Karaf 4.4.3 as an osgi container. When I try to use camel-activemq 3.20.4 I get an error while deploy bundle:

Caused by: java.lang.NoClassDefFoundError: javax/jms/ConnectionFactory
    at org.apache.camel.component.activemq.ActiveMQComponent.createConfiguration(ActiveMQComponent.java:260) ~[?:?]
    at org.apache.camel.component.jms.JmsComponent.<init>(JmsComponent.java:76) ~[?:?]
    at org.apache.camel.component.activemq.ActiveMQComponent.<init>(ActiveMQComponent.java:48) ~[?:?]
    at jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:?]
    at jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:?]
    at jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:?]
    at java.lang.reflect.Constructor.newInstance(Constructor.java:490) ~[?:?]
    at org.apache.aries.blueprint.utils.ReflectionUtils.newInstance(ReflectionUtils.java:369) ~[?:?]
    at org.apache.aries.blueprint.container.BeanRecipe.newInstance(BeanRecipe.java:839) ~[?:?]
    at org.apache.aries.blueprint.container.BeanRecipe.getInstanceFromType(BeanRecipe.java:350) ~[?:?]
    ... 28 more
Caused by: java.lang.ClassNotFoundException: javax.jms.ConnectionFactory not found by org.apache.camel.camel-activemq [121]
    at org.apache.felix.framework.BundleWiringImpl.findClassOrResourceByDelegation(BundleWiringImpl.java:1591) ~[?:?]
    at org.apache.felix.framework.BundleWiringImpl.access$300(BundleWiringImpl.java:79) ~[?:?]

In Karaf I use commands:
repo-add camel 3.20.4
feature:install camel
feature:install camel-activemq

My code and full log is in an attachment

On version 3.18.6 this code works",davsclaus,vladk,Minor,Resolved,Fixed,11/May/23 14:48,19/May/23 06:01
Bug,CAMEL-19342,13535925,Rest Inline Routes mixed with direct routes.,"Hello, 

For more information please follow Zulip thread:

[https://camel.zulipchat.com/#narrow/stream/257301-camel-spring-boot/topic/Inline.20Routes.20.28Default.20Model.29]

Here is my code:

[https://github.com/rodrigoserracoelho/rest-demo]

Do let me know if you need more info.

Thanks.

BR,

Rodrigo",davsclaus,coelhro,Major,Resolved,Fixed,12/May/23 07:11,13/May/23 07:21
Bug,CAMEL-19345,13536023,KameletDiscoveryTest fails to find routeTemplate,"This test binds a local RoutesBuilderLoaderSupport implementation to the kamelet.yaml extension as part of the test. This defines the routeTemplate.
The actual route is added directly to the context.
Due to the change in DefaultRoutesLoader.findRoutesBuilders() in commit [https://github.com/apache/camel/commit/5d868bdfd37e585582e5d7e3ac809f970e1dae28] related to CAMEL-19309, the custom RoutesBuilderLoader isn't invoked because the resource ""classpath:/kamelets/mySetBody.kamelet.yaml"" is checked for existence before checking for a matching loader.
Therefore the template is not defined.",davsclaus,klease78,Minor,Resolved,Fixed,12/May/23 21:35,16/May/23 18:12
Bug,CAMEL-19359,13536367,"camel-spring-boot - When camel start graceful shutdown, it still receive new request during graceful shutdown.","< camel graceful shutdown time = 20s, the service thread.sleep = 10s >
In my test case, at first I started the application, send the first request and then stop the JVM to run the camel graceful shutdown, in the console show that there have 2 infilght messages pending to complete, after about 4 sec, I send the second request, in theroy, camel shouldn't accept the second request, unfortunately, it still receive new request.

[TestCode|https://github.com/chen19980/GracefulShutdown-test]",davsclaus,chen19980,Minor,Resolved,Fixed,16/May/23 07:07,23/May/23 04:53
Bug,CAMEL-19364,13536560,Rest-openapi: lookup mechanism does not work after changes from Camel-18963,"I was trying to synchronize camel-quarkus with current Camel from `main` branch.

I noticed failures related to fix [https://github.com/apache/camel/commit/e2ad230d86fcbbc1e3c4ad5a9daf01aa3127eb42] in the Camel-rest-openapi component.

According to the doc, the url in component can be specified i.g. ""classpath:..."", ""bean:..."". At least these 2 options does not work.

It can be verified by a simple test:
{code:java}
    @Test
    public void testClasspath() throws Exception{
        Producer p =   context().getEndpoint(""rest-openapi:addPet?specificationUri=classpath:openapi.json"").createProducer();
        Assertions.assertNotNull(p);
    }
{code}",jondruse,jondruse,Minor,Resolved,Fixed,17/May/23 12:58,21/May/23 07:47
Bug,CAMEL-19371,13536750,RedeliveryErrorHandler's suppressed exceptions cause memory leak and logging issue,"Hi There,


I am a bit dubious about the behavior of RedeliveryErrorHandler's handleException() method in camel-core-processor bundle.

Chaining all exceptions in suppressedExceptions of Throwable during the redelivery process may not be the original intention, at least the comment below is suggesting that the goal was something else here.
{quote}
if (previous != null && previous != e) {
// a 2nd exception was thrown while handling a previous exception
// so we need to add the previous as suppressed by the new exception
// see also FatalFallbackErrorHandler
{quote}

The consequence of adding the previous exception to the suppressedExceptions list of the current one is building up an infinite chain of objects, besides a single log entry can be huge after a couple of retry attempts, see attached log.
I think the best would be to turn this feature on/off by a configuration parameter.

Thank you.",davsclaus,Meszaros,Minor,Resolved,Fixed,18/May/23 13:24,19/May/23 16:35
Bug,CAMEL-19373,13536845,spring-rabbitmq - Component does not Respect replyTimeout for InOut Exchanges,"Hi folks,

I think I found a bug in the spring-rabbitmq component. It seems that the *replyTimeout* settings (neither path nor spring properties) will be accepted by the component. I have create a [sample project|https://github.com/adamlukaszewski/spring-camel-rabbit-reply-timeout] for you to show the issue. The route is very simple:

{code}
public void configure() throws Exception {
        from(""timer:hello?repeatCount=1"")
            .transform(simple(""Random number ${random(0,100)}""))
            .log(LoggingLevel.ERROR,
                    "">>>>>>> Timeout of replay should happen after 10 secs (application.properties) or 15 secs as "" +
                            ""here configured"")
            .to(ExchangePattern.InOut, ""spring-rabbitmq:foo?routingKey=mykey&replyTimeout=15000"");
    }
{code}

I expect that the listener will throw a ""Reply timed out"" AmqpReplyTimeoutException after 15 seconds (or 10 seconds as defined in the [application.properties|http://application.properties/]). What is happening:

 !image.png!

You see that after 30 secs the exception occurred. It seems that we are hitting here the default configurations of Spring AMQ instead the setup configurations.

 !imageaa.png!",davsclaus,adam.lukaszewski,Minor,Resolved,Fixed,19/May/23 09:10,18/Jun/23 08:13
Bug,CAMEL-19381,13537304,Infinite loop creating processes with Camel JBang,"I used the command:
{noformat}
jbang camel@apache/camel run 'src/main/java/org/acme/timer/log/TimerRoute.java'
{noformat}

the file is coming from https://github.com/apache/camel-quarkus-examples/blob/ed3523e9c8d3343922eba3a34f6b248d90493782/timer-log/src/main/java/org/acme/timer/log/TimerRoute.java

there is this log repeating indefinitely:
{noformat}
Running integration with the following configuration:
    --camel-version=3.20.4
{noformat}

it creates a new CamelJBang on each time it writes the previous log block:
{noformat}
jps | grep CamelJBang
532586 CamelJBang
532743 CamelJBang
532666 CamelJBang
{noformat}

The main thread dump on them is:
{noformat}
Name: main
State: WAITING on java.lang.ProcessImpl@e90900f
Total blocked: 0  Total waited: 1

Stack trace: 
java.base@17.0.3/java.lang.Object.wait(Native Method)
java.base@17.0.3/java.lang.Object.wait(Object.java:338)
java.base@17.0.3/java.lang.ProcessImpl.waitFor(ProcessImpl.java:434)
app//org.apache.camel.dsl.jbang.core.commands.Run.runCamelVersion(Run.java:637)
app//org.apache.camel.dsl.jbang.core.commands.Run.run(Run.java:566)
app//org.apache.camel.dsl.jbang.core.commands.Run.doCall(Run.java:227)
app//org.apache.camel.dsl.jbang.core.commands.CamelCommand.call(CamelCommand.java:70)
app//org.apache.camel.dsl.jbang.core.commands.CamelCommand.call(CamelCommand.java:35)
app//picocli.CommandLine.executeUserObject(CommandLine.java:2041)
app//picocli.CommandLine.access$1500(CommandLine.java:148)
app//picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2461)
app//picocli.CommandLine$RunLast.handle(CommandLine.java:2453)
app//picocli.CommandLine$RunLast.handle(CommandLine.java:2415)
app//picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2273)
app//picocli.CommandLine$RunLast.execute(CommandLine.java:2417)
app//picocli.CommandLine.execute(CommandLine.java:2170)
app//org.apache.camel.dsl.jbang.core.commands.CamelJBangMain.run(CamelJBangMain.java:154)
app//main.CamelJBang.main(CamelJBang.java:36)
{noformat}
",davsclaus,apupier,Minor,Resolved,Fixed,23/May/23 09:44,09/Jun/23 13:57
Bug,CAMEL-19383,13537361,camel-jslt: allowTemplateFromHeader ignores header on subsequent exchanges,"When allowTemplateFromHeader is set, the endpoint will remember the first header expression that is set and use it for any subsequent exchange, even if it brings a different expression in its header.

That is:

{{template.sendBodyAndHeader(""direct:start"", TEST_BODY,}}
{{JsltConstants.HEADER_JSLT_STRING, "".foo"");}}

{{template.sendBodyAndHeader(""direct:start"", TEST_BODY,}}
{{JsltConstants.HEADER_JSLT_STRING, "".bar"");}}

will both use the expression .foo and the expression .bar will be ignored.

 

Also, if resource is set on the URI, it will ignore any expressions from header despite allowTemplateFromHeader, unless it was set on the first exchange, in which case it will be used for all exchanges, even those without the header set.",,jsz,Major,Resolved,Fixed,23/May/23 15:32,24/May/23 04:27
Bug,CAMEL-19387,13537614,camel-kafka - Cannot set custom azure credential provider,"https://github.com/apache/camel-kamelets-examples/issues/21

Setting this in yaml dsl / xml-dsl with additionalProperties.xxx should resolve custom #bean references",davsclaus,davsclaus,Major,Resolved,Fixed,25/May/23 08:18,25/May/23 08:22
Bug,CAMEL-19393,13537797,camel-kafka - Configuring kafka option should no longer all be string types,"Kafka Clients are now able to automatic convert to their expected type (int, boolean) and we should relax this in camel-kafka to allow configurations to be the type as-is.

Otherwise we can have problems like reported at
https://github.com/apache/camel-kamelets/issues/1492",davsclaus,davsclaus,Major,Resolved,Fixed,26/May/23 12:23,26/May/23 13:17
Bug,CAMEL-19399,13538003,camel-cxf - Prevent storing invalid entry in Converter cache on error,"If an error occurs while converting a value thanks to the {{CxfPayloadConverter}}, the value {{null}} is returned to let other fallback converters to try but if there are no such fallback converters, the cache of converters will keep the information that no converter exists which is not incorrect.

The behavior described above occurs when the unit test {{CxfPayloadConverterTest#testInvalidByteArrayToCxfPayload}} is called before {{CxfPayloadConverterTest#testByteArrayToCxfPayload}}, in that case, Camel believes that there is no converter for a byte array so it returns {{null}}. 

The corresponding failing test report https://ci-builds.apache.org/job/Camel/job/Camel%20JDK17/job/main/876/testReport/org.apache.camel.component.cxf.converter/CxfPayloadConverterTest/testByteArrayToCxfPayload/",nfilotto,nfilotto,Major,Resolved,Fixed,29/May/23 14:17,30/May/23 12:26
Bug,CAMEL-19401,13538016,Typo in kafka image name in ContainerLocalKafkaService,"Typo in kafka image name in [ContainerLocalKafkaService.java|https://github.com/apache/camel/blob/camel-3.20.x/test-infra/camel-test-infra-kafka/src/test/java/org/apache/camel/test/infra/kafka/services/ContainerLocalKafkaService.java#L28] in 3.20.5 (not in main branch though).


{code:java}
public static final String KAFKA3_IMAGE_NAME = ""confluentinc/cp-kafkai:7.3.2"";
{code}

should be 

{code:java}
public static final String KAFKA3_IMAGE_NAME = ""confluentinc/cp-kafka:7.3.2"";
{code}",nfilotto,Yullia.buzlukova,Major,Resolved,Fixed,29/May/23 17:16,29/May/23 19:29
Bug,CAMEL-19405,13538260,camel-jbang - camel get circuitbreaker may not work,"The cirucit breaker example does not output data when you do camel get cb
https://github.com/apache/camel-kamelets-examples/tree/main/jbang/circuit-breaker


",davsclaus,davsclaus,Minor,Resolved,Fixed,31/May/23 12:04,31/May/23 17:09
Bug,CAMEL-19415,13538771,camel-stax: using xtokenize might be NPE on xml default namespace,"Background:

camel-core use Woodstox to xtokenize the xml.

Woodstox has a property on WstxInputProperties.P_RETURN_NULL_FOR_DEFAULT_NAMESPACE

which has different value in different woodstox-core version.

---

Issue:

When using camel core to xtokenize xml, it will get the XML namespace prefix through javax.xml.stream.XMLStreamReader#getNamespacePrefix, which will use woodstox-core lib.

In org.apache.camel.support.XMLTokenExpressionIterator.XMLTokenIterator#createContextualToken with i mode,

It will then construct a namespace map and loop it .

In org.apache.camel.support.XMLTokenExpressionIterator line 404
sb.append(e.getKey().length() == 0 ? "" xmlns"" : "" xmlns:"")
As the e.getKey() has no null check, it might be null and resulting in NPE because of null.length()
---
Suggested Solution:
Add a null check at org.apache.camel.support.XMLTokenExpressionIterator line 404 e.getKey() to avoid NPE
i.e. sb.append((e.getKey() == null || e.getKey().length() == 0) ? "" xmlns"" : "" xmlns:"")",davsclaus,vincenttang,Major,Resolved,Fixed,05/Jun/23 13:14,12/Jun/23 01:45
Bug,CAMEL-19421,13538928,Camel-Jira: Use Files.createTempFile in FileConverter instead of creating File directly,,acosentino,acosentino,Major,Resolved,Fixed,06/Jun/23 13:29,07/Jun/23 07:59
Bug,CAMEL-19426,13539157,Spring-WS syntaxt and path properties inconsistency ,"In component catalog [https://github.com/apache/camel/blob/main/catalog/camel-catalog/src/generated/resources/org/apache/camel/catalog/components/spring-ws.json]

Component syntax is `spring-ws:type:lookupKey:webServiceEndpointUri`

But there are 4 path properties: type, lookupKey, webServiceEndpointUri and *expression*

Looks like we have missed something",davsclaus,marat.gubaidullin@gmail.com,Minor,Resolved,Fixed,07/Jun/23 22:51,09/Jun/23 09:39
Bug,CAMEL-19432,13539433,camel-azure-eventhubs: Providing a custom EventHubProducerAsyncClient has no effect,"The azure-eventhubs component has a {{producerAsyncClient}} option where you can pass a custom {{EventHubProducerAsyncClient}} instance to the component configuration.

Looking at the producer code, it seems that it gets ignored and a new {{EventHubProducerAsyncClient}} is created in {{doStart}}.

https://github.com/apache/camel/blob/main/components/camel-azure/camel-azure-eventhubs/src/main/java/org/apache/camel/component/azure/eventhubs/EventHubsProducer.java#L41

This was originally reported in the Camel Quarkus project:

https://github.com/apache/camel-quarkus/issues/4975 
",jamesnetherton,jamesnetherton,Minor,Resolved,Fixed,09/Jun/23 12:49,13/Jun/23 16:44
Bug,CAMEL-19443,13539854,camel-kamelet - Route templates should use route configured error handler,"See CAMEL-19411

There was also an underlying bug where the kamelet route template would be set a default error handler eager, instead from route configuration.",davsclaus,davsclaus,Major,Resolved,Fixed,13/Jun/23 12:58,13/Jun/23 15:33
Bug,CAMEL-19452,13539969,camel-jbang - Run with --open-api does not show log in console,this example does not show anything in the console: camel-kamelets-examples/jbang/open-api,davsclaus,davsclaus,Major,Resolved,Fixed,14/Jun/23 05:42,14/Jun/23 05:49
Bug,CAMEL-19457,13540127,camel-dynamic-router - InflightRepository size can be negative,"InflightRepository size can be negative，I think size should be a non negative integer。

!image-2023-06-15-10-51-29-124.png!",davsclaus,ggboy,Minor,Resolved,Fixed,15/Jun/23 02:53,18/Jun/23 19:48
Bug,CAMEL-19459,13540158,camel-report-maven-plugin fails to run due to missing class,*camel-report-maven-plugin* fails to run due to missing class: *org.eclipse.aether.spi.synccontext.SyncContextFactory*. Last working version: *4.0.0-M2*,davsclaus,alex-nt,Minor,Resolved,Fixed,15/Jun/23 07:56,17/Jun/23 11:34
Bug,CAMEL-19470,13540603,Exchange body is null when using Mongodb changeStreams with update operation,"When using the mongodb component as a ChangeStreams consumer, the fullDocument is null for update operations. It's normal if we refer to [mongodb documentation|https://www.mongodb.com/docs/manual/reference/operator/aggregation/changeStream/].

 

As a temporary fix, you can use the object id to get the modified document with a route like this: 
{code:java}
from(""mongodb:mongo?consumerType=changeStreams...."")
.to(""mongodb:mongo?operation=findById""){code}
 

 

The MongoDbChangeStreamsThread.initializeCursor() method should be changed to use the fullDocument option when creating the iterable.",,kgarrido,Minor,Resolved,Fixed,19/Jun/23 10:31,21/Jun/23 11:37
Bug,CAMEL-19476,13540741,rest-dsl - ClientRequestValidation accepted content-type may not validate correctly,https://camel.zulipchat.com/#narrow/stream/257301-camel-spring-boot/topic/rest.20endpoint.20properties.20to.20servlet.20component.20consumer,davsclaus,davsclaus,Minor,Resolved,Fixed,20/Jun/23 11:20,20/Jun/23 12:35
Bug,CAMEL-19486,13540997,The $repos placeholder used by Camel JBang is not resolved,"in the attached picture you can see I tried to set one repository as global and then run command with another repository which should mean to have both used

 

the global config list
{code:java}
» camel config list                                                                                                                                                                                                                                                                                                       
repos = https://packages.atlassian.com/maven-external {code}
 

when I execute eg camel jbang run, in the log there is
{code:java}
--repos=,https//....{code}
which indicates that ""$repos"" placeholder is empty probably...",davsclaus,djelinek,Minor,Resolved,Fixed,22/Jun/23 08:39,05/Jul/23 10:44
Bug,CAMEL-19509,13541586,"java.lang.NullPointerException: Cannot invoke ""org.apache.camel.model.FromDefinition.getLabel()"" because ""this.input"" is null","Running a file with content (tried with Camel JBang 3.20.6):

{noformat}
apiVersion: camel.apache.org/v1alpha1
kind: KameletBinding
metadata:
  name: ''
spec:
  source:
    ref:
      apiVersion: camel.apache.org/v1alpha1
      name: timer-source
      kind: Kamelet
  steps:
  - ref:
      apiVersion: camel.apache.org/v1alpha1
      name: log-action
      kind: Kamelet
{noformat}


leads to a NPE:
{noformat}
java.lang.NullPointerException: Cannot invoke ""org.apache.camel.model.FromDefinition.getLabel()"" because ""this.input"" is null
        at org.apache.camel.model.RouteDefinition.getLabel(RouteDefinition.java:171)
        at org.apache.camel.impl.PreconditionHelper.included(PreconditionHelper.java:51)
        at org.apache.camel.impl.DefaultCamelContext.includedRoute(DefaultCamelContext.java:1062)
        at org.apache.camel.impl.DefaultCamelContext.startRouteDefinitions(DefaultCamelContext.java:927)
        at org.apache.camel.impl.DefaultCamelContext.startRouteDefinitions(DefaultCamelContext.java:800)
        at org.apache.camel.impl.engine.AbstractCamelContext.doInit(AbstractCamelContext.java:3017)
        at org.apache.camel.support.service.BaseService.init(BaseService.java:83)
        at org.apache.camel.impl.engine.AbstractCamelContext.init(AbstractCamelContext.java:2688)
        at org.apache.camel.support.service.BaseService.start(BaseService.java:111)
        at org.apache.camel.impl.engine.AbstractCamelContext.start(AbstractCamelContext.java:2707)
        at org.apache.camel.impl.DefaultCamelContext.start(DefaultCamelContext.java:262)
        at org.apache.camel.main.KameletMain.doStart(KameletMain.java:280)
        at org.apache.camel.support.service.BaseService.start(BaseService.java:119)
        at org.apache.camel.dsl.jbang.core.commands.Run.runKameletMain(Run.java:784)
        at org.apache.camel.dsl.jbang.core.commands.Run.run(Run.java:592)
        at org.apache.camel.dsl.jbang.core.commands.Run.doCall(Run.java:232)
        at org.apache.camel.dsl.jbang.core.commands.CamelCommand.call(CamelCommand.java:73)
        at org.apache.camel.dsl.jbang.core.commands.CamelCommand.call(CamelCommand.java:36)
        at picocli.CommandLine.executeUserObject(CommandLine.java:2041)
        at picocli.CommandLine.access$1500(CommandLine.java:148)
        at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2461)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:2453)
        at picocli.CommandLine$RunLast.handle(CommandLine.java:2415)
        at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2273)
        at picocli.CommandLine$RunLast.execute(CommandLine.java:2417)
        at picocli.CommandLine.execute(CommandLine.java:2170)
        at org.apache.camel.dsl.jbang.core.commands.CamelJBangMain.run(CamelJBangMain.java:154)
        at main.CamelJBang.main(CamelJBang.java:36)
 {noformat}",davsclaus,apupier,Minor,Resolved,Fixed,27/Jun/23 14:40,27/Jun/23 15:25
Bug,CAMEL-19511,13541600,camel-spring-boot - Register properly auto-configurations,"While trying to figure out why no routes could even start in the reactive-streams example, I realized that it was due to the fact that the way [to register auto-configurations has changed|https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-3.0-Migration-Guide#auto-configuration-files], the code must be changed consequently",nfilotto,nfilotto,Trivial,Resolved,Fixed,27/Jun/23 15:58,28/Jun/23 12:30
